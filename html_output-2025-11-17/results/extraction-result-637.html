<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-637 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-637</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-637</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-b880c3d5c0116507e08ac33ff90d8371d75e333e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b880c3d5c0116507e08ac33ff90d8371d75e333e" target="_blank">Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems, and proposes a novel index associated with a dataset that roughly decides the feasibility of using such data for LLM-involved evaluation in academic development.</p>
                <p><strong>Paper Abstract:</strong> The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite uncommon from this trendy community. Briefly, they are: (i)-the minor but inevitable error occurrence in the user-generated query input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess poor consistency when processing semantically similar query input. In addition, as a side finding, we find that ChatGPT is still capable to yield the correct answer even when the input is polluted at an extreme level. While this phenomenon demonstrates the powerful memorization of the LLMs, it raises serious concerns about using such data for LLM-involved evaluation in academic development. To deal with it, we propose a novel index associated with a dataset that roughly decides the feasibility of using such data for LLM-involved evaluation. Extensive empirical studies are tagged to support the aforementioned claims.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e637.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e637.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (accessed via gpt-3.5-turbo API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-tuned generative language model accessed via API and used as the primary subject for robustness, consistency, and credibility experiments; evaluated on 12 QA/reasoning datasets with large-scale automated queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B*</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Large-scale automatic QA evaluation under adversarial perturbations and prompt/option-order variations to assess robustness, consistency, and dataset memorization (credibility).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Prompt phrasing (5 prompt variants tested), option ordering (6 randomized orders), character-level noise (repeat/delete/insert), word-level noise (insert/delete/replace), visual-similar character substitutions (10/50/90% ratios), model/API updates over time (noted in Limitations), tokenizer/tokenization effects, stochasticity in model outputs (reported as model randomness / low confidence in some models).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Error Rate (ER, %), Answer-Changed Rate (ACR, %), standard deviation of accuracy across prompts (std %), per-attack ER/ACR per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>On 39,253 samples overall baseline ER=40.19%. All attacks increased ER by >=2 percentage points. Word-level attacks increased ER ~10% more than character-level on average; e.g., word-delete ER up to 61.89% and corresponding ACR 49.8% (Table 6). Visual attacks: ER rises with substitution ratio (10%→42.47%, 50%→48.85%, 90%→55.88%). Consistency: standard deviation of accuracy across 5 prompts = 1.9%; across option-order permutations std = 1.13%. (Also reported earlier: a 3.2% average fluctuation claim for semantically identical inputs in summary.)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Standard deviation of accuracy across prompts and option orders, ACR (fraction of examples whose answer changed after perturbation), ER comparisons pre/post perturbation, RTI (dataset-level memorization index) applied to datasets used to evaluate model leakage/memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Model shows small prompt-order variance (std 1.9%) and low option-order bias (std 1.13%), but large answer changes under adversarial perturbations (ACR ≥ 27% for all attack classes; some datasets ACR >70% under strong attacks). Model outputs may change over time due to product updates (non-reproducibility noted in Limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>API/model parameter updates over time (model iteration); non-deterministic/stochastic sampling in API responses; prompt sensitivity; option-order sensitivity (small but present); tokenizer-induced sensitivity to word/visual perturbations; dataset memorization (answers sometimes preserved despite passage pollution).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Prompt standardization (fixed templates used), in-context learning (ICL) within a single chat session for consistent context, dataset credibility screening via RTI to avoid using memorized test samples, recommendations to avoid datasets with high RTI for LLM-involved evaluation, suggestion to explore adversarial training paradigms for LLMs (future work).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Overall >1,000,000 API queries across models; experiments used 39,253 distinct samples; prompt-variation tests used 5 prompts; option-order tests used 6 permutations; RTI tested by sweeping attack probability rho from 0.0 to 1.0 in steps of 0.1 per sample.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT is sensitive to realistic input perturbations (especially word-level and visual attacks) producing substantial output changes (ACR often >27% and ER increases up to ~20+ percentage points); however, across different prompts and option orders ChatGPT is relatively stable (std of accuracy ~1–2%), and many datasets appear memorized (RTI can indicate memorization), undermining evaluation reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e637.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e637.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (11.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source pre-trained LLM used as a comparative baseline; evaluated on the same adversarial QA setup to measure robustness and consistency differences from ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11.5B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Robustness and consistency evaluation on sampled QA datasets under the same attacks and prompt/option variations as ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Prompt phrasing, option ordering, character/word/visual perturbations, stochastic output tendencies (noted as higher randomness/low confidence relative to ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Error Rate (ER), Answer-Changed Rate (ACR), standard deviation of accuracy across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Higher prompt-driven variability than ChatGPT: average standard deviation across prompts reported as 11.9% (compared to ChatGPT's 1.9%). In example datasets LLaMA showed larger variance and a tendency to pick the first option in multiple-choice orders (indicating order bias).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Std. deviation of accuracy across prompts; empirical observation of order bias across 6 option orders.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>LLaMA exhibits larger inconsistency across prompts and option orders (std 11.9%) and exhibits a strong first-option bias, suggesting lower reproducibility/robustness for multiple-choice tasks without further instruction or tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Lack of instruction tuning (pretraining-only behavior), high output randomness/low confidence, prompt sensitivity and order bias, non-deterministic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Instruction tuning or prompt/instruction engineering suggested to reduce order bias and instability; no quantitative mitigation tested in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Reported per-table experiments on sampled datasets (examples: bAbi16 1000 samples / SenMaking 2021 samples used in cross-model comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLaMA (pretraining-only) shows substantially higher variability to prompt changes (std ~11.9%) and pronounced option-order bias, indicating that instruction tuning/in-context learning is important to achieve reproducible multi-choice behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e637.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e637.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPT (1.3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM used for comparative experiments; evaluated under the same adversarial perturbations to contrast robustness and variability with larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Robustness and consistency testing on sample QA tasks with word/character perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Prompt phrasing, character/word perturbations, option ordering, model randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Error Rate (ER), Answer-Changed Rate (ACR).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>OPT shows poor baseline capability on some datasets (e.g., 100% ER on SenMaking in the reported sample), and modest ACR/ER under attacks in sample comparisons (Table 9/10), indicating both low capability and variability in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>ER and ACR on sampled subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>OPT often produces very high ER (e.g., 100% on some sampled tasks), leaving little headroom for measuring fine-grained reproducibility improvements in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Low baseline task competence, high sensitivity to prompt/template choice, stochastic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Not tested; implication that stronger instruction tuning or model scaling would be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Per-table sampled experiments (e.g., 1000 samples for bAbi16, 2021 for SenMaking in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller models like OPT yield high error rates and are highly sensitive to task framing, making them less reproducible/robust in the evaluated QA/adversarial setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e637.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e637.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RTI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Training Index (RTI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset-level index introduced in this paper that estimates the relative probability that a dataset's examples were memorized by an LLM, by finding the minimum perturbation strength that causes the model to change its answer and averaging that threshold across the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to evaluated LLMs (ChatGPT, LLaMA, OPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / dataset credibility analysis</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Quantify dataset memorization/credibility for LLM evaluation by progressively applying word-level perturbations (auto-attacker g with probability rho from 0.0 to 1.0) and recording the minimal rho at which the model's answer changes; RTI is the dataset average of these thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Degree of perturbation (rho) applied via word-level auto-attacker; differences in model memorization stemming from training data overlap; input structure complexity (p length/alpha).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Per-example threshold r (minimum rho in 0.1 increments that flips model answer); RTI R_D = mean_x r_x over dataset D.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>RTI values vary by dataset (sampled 100 examples each): StrategyQA avg 0.456, AQuA 0.175, Creak 0.668, NoahQA 0.393, GSM8K 0.396, bAbi15 0.237, bAbi16 0.201, ECQA 0.454, ESNLI 0.346, QASC 0.442, QED 0.534, SenMaking 0.461 (Table 12). Lower RTI indicates dataset less memorized (harder for model to preserve answer under perturbation); higher RTI suggests memorization/more reliable recognition by model even under perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>RTI itself is used as a reproducibility/credibility indicator for whether evaluation on a given dataset is likely to reflect memorized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Datasets differ widely in RTI; e.g., AQuA low RTI (0.175) indicating low memorization, while Creak high RTI (0.668) indicating high memorization likelihood; authors recommend avoiding datasets with high RTI for LLM-involved evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>RTI is relative and cannot determine absolute memorization without ground-truth training-set exposure; cannot reverse-engineer actual training data; step granularity (rho increments of 0.1) limits resolution; RTI depends on attack type (word-level used).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use RTI as a screening tool to select evaluation datasets with lower memorization risk; compute baseline RTI scores across datasets to build reference levels; avoid datasets with high RTI when evaluating LLM-involved systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Per sampled dataset RTI computed by sweeping rho from 0.0 to 1.0 in steps of 0.1 for each sampled example (reported samples per dataset: typically 99–508 in Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RTI provides a simple, repeatable way to estimate relative dataset memorization by LLMs; RTI values vary substantially across datasets, and high RTI datasets risk producing misleading evaluation results because models can answer correctly even when input passages are heavily polluted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e637.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e637.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ER & ACR metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error Rate (ER) and Answer-Changed Rate (ACR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary quantitative metrics used in the paper: ER measures the model's error rate on the (clean or attacked) dataset; ACR measures the fraction of examples where the model's answer changed after an attack/perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to evaluated LLMs (ChatGPT, LLaMA, OPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / robustness measurement</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Quantify robustness and consistency by comparing model answers before and after structured perturbations (character/word/visual attacks) and prompt/option variations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Attack type and strength (character/word/visual; insertion/deletion/replacement; substitution ratios), prompt variants, option orders, dataset differences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>ER_D = |{x in D : f(x) != a}|/|D|; ACR_D = |{x in D : f(x) != f(x') }|/|D|, where x' is perturbed sample.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported examples: overall ChatGPT baseline ER=40.19% (39,253 samples). Word-delete ER up to 61.89% and ACR 49.8% (Table 6). Character-level deletion often raised ER by ~10 percentage points relative to insertion. All attacks led to at least 27% of outputs changing (ACR >=27%).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>ER and ACR used to quantify how reproducible original answers are under perturbation; also used within RTI procedure to find flip thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>High ACR under many attacks indicates original outputs are not robust/reproducible under realistic noise; ER increases quantify degraded accuracy under perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Dependence on specific attack choices and strengths; attack randomness (attacked words chosen with probability rho) introduces experimental variability; some datasets and models exhibit high baseline ER limiting interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Not directly tested as fixes; used as diagnostic metrics to evaluate mitigation proposals (RTI, prompt standardization, future adversarial training).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Computed over the full experimental runset (39,253 samples total) and per-dataset tables across different attack settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ER and ACR together reveal that many LLM outputs are brittle: structured, realistic perturbations (especially word-level) substantially increase error rates and frequently change model answers, undermining reproducibility of predictions under noisy/user-generated inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e637.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e637.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt & Options Attacks (Consistency tests)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-variant attack and Options-order attack (consistency evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two designed experiments measuring consistency: (1) replace prompt text with five semantically similar prompt templates to measure sensitivity to prompt phrasing; (2) randomize option order across six permutations to measure sensitivity to option ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to evaluated LLMs (ChatGPT, LLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation / prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assess internal consistency/stability of LLMs for semantically equivalent inputs by measuring accuracy variations across prompt templates and answer-option orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Prompt wording variants (5 templates), order of candidate options (6 permutations), in-context learning session configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation of accuracy across prompts; standard deviation across option orders; ACR where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>ChatGPT: std of accuracy across prompts = 1.9%, std across option orders = 1.13% (low sensitivity). LLaMA: std across prompts = 11.9% and strong first-option bias (high sensitivity). Earlier summary also reported ChatGPT average fluctuation of ~3.2% for semantically identical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Std. deviation of accuracy across prompt variants and option-order permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Prompt phrasing can induce non-negligible variability especially in pretraining-only models (LLaMA); ChatGPT is comparatively robust to prompt wording and option ordering but not immune to adversarial perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Prompt engineering choices and absence of standard prompt templates can reduce reproducibility; pretraining-only models require instruction tuning to reduce variability; multiple acceptable prompts in the wild lead to inconsistent user-facing behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use a fixed prompt template (authors adopted a single multiple-choice prompt template), in-context learning within same chat session, and recommend prompt standardization across evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Five prompt variants tested; six option orders tested; results aggregated across datasets and shown as boxplots (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt wording and option ordering are measurable sources of variability: well-tuned/instruction-tuned models (ChatGPT) show low variability, while pretraining-only models (LLaMA) show much higher variability and order bias; standardizing prompts and using instruction-tuned models improves consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the robustness of chatgpt: An adversarial and out-of-distribution perspective <em>(Rating: 2)</em></li>
                <li>Impact of adversarial training on robustness and generalizability of language models <em>(Rating: 1)</em></li>
                <li>Evaluating the robustness of neural language models to input perturbations <em>(Rating: 1)</em></li>
                <li>Interpreting the robustness of neural nlp models to textual perturbations <em>(Rating: 2)</em></li>
                <li>Adversarial glue: A multi-task benchmark for robustness evaluation of language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-637",
    "paper_id": "paper-b880c3d5c0116507e08ac33ff90d8371d75e333e",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "ChatGPT (gpt-3.5-turbo)",
            "name_full": "ChatGPT (accessed via gpt-3.5-turbo API)",
            "brief_description": "A large instruction-tuned generative language model accessed via API and used as the primary subject for robustness, consistency, and credibility experiments; evaluated on 12 QA/reasoning datasets with large-scale automated queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_size": "175B*",
            "scientific_domain": "Natural language processing / LLM evaluation",
            "experimental_task": "Large-scale automatic QA evaluation under adversarial perturbations and prompt/option-order variations to assess robustness, consistency, and dataset memorization (credibility).",
            "variability_sources": "Prompt phrasing (5 prompt variants tested), option ordering (6 randomized orders), character-level noise (repeat/delete/insert), word-level noise (insert/delete/replace), visual-similar character substitutions (10/50/90% ratios), model/API updates over time (noted in Limitations), tokenizer/tokenization effects, stochasticity in model outputs (reported as model randomness / low confidence in some models).",
            "variability_measured": true,
            "variability_metrics": "Error Rate (ER, %), Answer-Changed Rate (ACR, %), standard deviation of accuracy across prompts (std %), per-attack ER/ACR per dataset.",
            "variability_results": "On 39,253 samples overall baseline ER=40.19%. All attacks increased ER by &gt;=2 percentage points. Word-level attacks increased ER ~10% more than character-level on average; e.g., word-delete ER up to 61.89% and corresponding ACR 49.8% (Table 6). Visual attacks: ER rises with substitution ratio (10%→42.47%, 50%→48.85%, 90%→55.88%). Consistency: standard deviation of accuracy across 5 prompts = 1.9%; across option-order permutations std = 1.13%. (Also reported earlier: a 3.2% average fluctuation claim for semantically identical inputs in summary.)",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Standard deviation of accuracy across prompts and option orders, ACR (fraction of examples whose answer changed after perturbation), ER comparisons pre/post perturbation, RTI (dataset-level memorization index) applied to datasets used to evaluate model leakage/memorization.",
            "reproducibility_results": "Model shows small prompt-order variance (std 1.9%) and low option-order bias (std 1.13%), but large answer changes under adversarial perturbations (ACR ≥ 27% for all attack classes; some datasets ACR &gt;70% under strong attacks). Model outputs may change over time due to product updates (non-reproducibility noted in Limitations).",
            "reproducibility_challenges": "API/model parameter updates over time (model iteration); non-deterministic/stochastic sampling in API responses; prompt sensitivity; option-order sensitivity (small but present); tokenizer-induced sensitivity to word/visual perturbations; dataset memorization (answers sometimes preserved despite passage pollution).",
            "mitigation_methods": "Prompt standardization (fixed templates used), in-context learning (ICL) within a single chat session for consistent context, dataset credibility screening via RTI to avoid using memorized test samples, recommendations to avoid datasets with high RTI for LLM-involved evaluation, suggestion to explore adversarial training paradigms for LLMs (future work).",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Overall &gt;1,000,000 API queries across models; experiments used 39,253 distinct samples; prompt-variation tests used 5 prompts; option-order tests used 6 permutations; RTI tested by sweeping attack probability rho from 0.0 to 1.0 in steps of 0.1 per sample.",
            "key_findings": "ChatGPT is sensitive to realistic input perturbations (especially word-level and visual attacks) producing substantial output changes (ACR often &gt;27% and ER increases up to ~20+ percentage points); however, across different prompts and option orders ChatGPT is relatively stable (std of accuracy ~1–2%), and many datasets appear memorized (RTI can indicate memorization), undermining evaluation reliability.",
            "uuid": "e637.0",
            "source_info": {
                "paper_title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA (11.5B)",
            "brief_description": "An open-source pre-trained LLM used as a comparative baseline; evaluated on the same adversarial QA setup to measure robustness and consistency differences from ChatGPT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_size": "11.5B",
            "scientific_domain": "Natural language processing / LLM evaluation",
            "experimental_task": "Robustness and consistency evaluation on sampled QA datasets under the same attacks and prompt/option variations as ChatGPT.",
            "variability_sources": "Prompt phrasing, option ordering, character/word/visual perturbations, stochastic output tendencies (noted as higher randomness/low confidence relative to ChatGPT).",
            "variability_measured": true,
            "variability_metrics": "Error Rate (ER), Answer-Changed Rate (ACR), standard deviation of accuracy across prompts.",
            "variability_results": "Higher prompt-driven variability than ChatGPT: average standard deviation across prompts reported as 11.9% (compared to ChatGPT's 1.9%). In example datasets LLaMA showed larger variance and a tendency to pick the first option in multiple-choice orders (indicating order bias).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Std. deviation of accuracy across prompts; empirical observation of order bias across 6 option orders.",
            "reproducibility_results": "LLaMA exhibits larger inconsistency across prompts and option orders (std 11.9%) and exhibits a strong first-option bias, suggesting lower reproducibility/robustness for multiple-choice tasks without further instruction or tuning.",
            "reproducibility_challenges": "Lack of instruction tuning (pretraining-only behavior), high output randomness/low confidence, prompt sensitivity and order bias, non-deterministic sampling.",
            "mitigation_methods": "Instruction tuning or prompt/instruction engineering suggested to reduce order bias and instability; no quantitative mitigation tested in this paper.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Reported per-table experiments on sampled datasets (examples: bAbi16 1000 samples / SenMaking 2021 samples used in cross-model comparisons).",
            "key_findings": "LLaMA (pretraining-only) shows substantially higher variability to prompt changes (std ~11.9%) and pronounced option-order bias, indicating that instruction tuning/in-context learning is important to achieve reproducible multi-choice behavior.",
            "uuid": "e637.1",
            "source_info": {
                "paper_title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "OPT",
            "name_full": "OPT (1.3B)",
            "brief_description": "An open-source LLM used for comparative experiments; evaluated under the same adversarial perturbations to contrast robustness and variability with larger models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT",
            "model_size": "1.3B",
            "scientific_domain": "Natural language processing / LLM evaluation",
            "experimental_task": "Robustness and consistency testing on sample QA tasks with word/character perturbations.",
            "variability_sources": "Prompt phrasing, character/word perturbations, option ordering, model randomness.",
            "variability_measured": true,
            "variability_metrics": "Error Rate (ER), Answer-Changed Rate (ACR).",
            "variability_results": "OPT shows poor baseline capability on some datasets (e.g., 100% ER on SenMaking in the reported sample), and modest ACR/ER under attacks in sample comparisons (Table 9/10), indicating both low capability and variability in outputs.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "ER and ACR on sampled subsets.",
            "reproducibility_results": "OPT often produces very high ER (e.g., 100% on some sampled tasks), leaving little headroom for measuring fine-grained reproducibility improvements in this evaluation.",
            "reproducibility_challenges": "Low baseline task competence, high sensitivity to prompt/template choice, stochastic outputs.",
            "mitigation_methods": "Not tested; implication that stronger instruction tuning or model scaling would be needed.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Per-table sampled experiments (e.g., 1000 samples for bAbi16, 2021 for SenMaking in comparisons).",
            "key_findings": "Smaller models like OPT yield high error rates and are highly sensitive to task framing, making them less reproducible/robust in the evaluated QA/adversarial setting.",
            "uuid": "e637.2",
            "source_info": {
                "paper_title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "RTI",
            "name_full": "Relative Training Index (RTI)",
            "brief_description": "A dataset-level index introduced in this paper that estimates the relative probability that a dataset's examples were memorized by an LLM, by finding the minimum perturbation strength that causes the model to change its answer and averaging that threshold across the dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to evaluated LLMs (ChatGPT, LLaMA, OPT)",
            "model_size": null,
            "scientific_domain": "NLP evaluation / dataset credibility analysis",
            "experimental_task": "Quantify dataset memorization/credibility for LLM evaluation by progressively applying word-level perturbations (auto-attacker g with probability rho from 0.0 to 1.0) and recording the minimal rho at which the model's answer changes; RTI is the dataset average of these thresholds.",
            "variability_sources": "Degree of perturbation (rho) applied via word-level auto-attacker; differences in model memorization stemming from training data overlap; input structure complexity (p length/alpha).",
            "variability_measured": true,
            "variability_metrics": "Per-example threshold r (minimum rho in 0.1 increments that flips model answer); RTI R_D = mean_x r_x over dataset D.",
            "variability_results": "RTI values vary by dataset (sampled 100 examples each): StrategyQA avg 0.456, AQuA 0.175, Creak 0.668, NoahQA 0.393, GSM8K 0.396, bAbi15 0.237, bAbi16 0.201, ECQA 0.454, ESNLI 0.346, QASC 0.442, QED 0.534, SenMaking 0.461 (Table 12). Lower RTI indicates dataset less memorized (harder for model to preserve answer under perturbation); higher RTI suggests memorization/more reliable recognition by model even under perturbation.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "RTI itself is used as a reproducibility/credibility indicator for whether evaluation on a given dataset is likely to reflect memorized knowledge.",
            "reproducibility_results": "Datasets differ widely in RTI; e.g., AQuA low RTI (0.175) indicating low memorization, while Creak high RTI (0.668) indicating high memorization likelihood; authors recommend avoiding datasets with high RTI for LLM-involved evaluation.",
            "reproducibility_challenges": "RTI is relative and cannot determine absolute memorization without ground-truth training-set exposure; cannot reverse-engineer actual training data; step granularity (rho increments of 0.1) limits resolution; RTI depends on attack type (word-level used).",
            "mitigation_methods": "Use RTI as a screening tool to select evaluation datasets with lower memorization risk; compute baseline RTI scores across datasets to build reference levels; avoid datasets with high RTI when evaluating LLM-involved systems.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Per sampled dataset RTI computed by sweeping rho from 0.0 to 1.0 in steps of 0.1 for each sampled example (reported samples per dataset: typically 99–508 in Table 12).",
            "key_findings": "RTI provides a simple, repeatable way to estimate relative dataset memorization by LLMs; RTI values vary substantially across datasets, and high RTI datasets risk producing misleading evaluation results because models can answer correctly even when input passages are heavily polluted.",
            "uuid": "e637.3",
            "source_info": {
                "paper_title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ER & ACR metrics",
            "name_full": "Error Rate (ER) and Answer-Changed Rate (ACR)",
            "brief_description": "Two complementary quantitative metrics used in the paper: ER measures the model's error rate on the (clean or attacked) dataset; ACR measures the fraction of examples where the model's answer changed after an attack/perturbation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to evaluated LLMs (ChatGPT, LLaMA, OPT)",
            "model_size": null,
            "scientific_domain": "NLP evaluation / robustness measurement",
            "experimental_task": "Quantify robustness and consistency by comparing model answers before and after structured perturbations (character/word/visual attacks) and prompt/option variations.",
            "variability_sources": "Attack type and strength (character/word/visual; insertion/deletion/replacement; substitution ratios), prompt variants, option orders, dataset differences.",
            "variability_measured": true,
            "variability_metrics": "ER_D = |{x in D : f(x) != a}|/|D|; ACR_D = |{x in D : f(x) != f(x') }|/|D|, where x' is perturbed sample.",
            "variability_results": "Reported examples: overall ChatGPT baseline ER=40.19% (39,253 samples). Word-delete ER up to 61.89% and ACR 49.8% (Table 6). Character-level deletion often raised ER by ~10 percentage points relative to insertion. All attacks led to at least 27% of outputs changing (ACR &gt;=27%).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "ER and ACR used to quantify how reproducible original answers are under perturbation; also used within RTI procedure to find flip thresholds.",
            "reproducibility_results": "High ACR under many attacks indicates original outputs are not robust/reproducible under realistic noise; ER increases quantify degraded accuracy under perturbations.",
            "reproducibility_challenges": "Dependence on specific attack choices and strengths; attack randomness (attacked words chosen with probability rho) introduces experimental variability; some datasets and models exhibit high baseline ER limiting interpretability.",
            "mitigation_methods": "Not directly tested as fixes; used as diagnostic metrics to evaluate mitigation proposals (RTI, prompt standardization, future adversarial training).",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Computed over the full experimental runset (39,253 samples total) and per-dataset tables across different attack settings.",
            "key_findings": "ER and ACR together reveal that many LLM outputs are brittle: structured, realistic perturbations (especially word-level) substantially increase error rates and frequently change model answers, undermining reproducibility of predictions under noisy/user-generated inputs.",
            "uuid": "e637.4",
            "source_info": {
                "paper_title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Prompt & Options Attacks (Consistency tests)",
            "name_full": "Prompt-variant attack and Options-order attack (consistency evaluation)",
            "brief_description": "Two designed experiments measuring consistency: (1) replace prompt text with five semantically similar prompt templates to measure sensitivity to prompt phrasing; (2) randomize option order across six permutations to measure sensitivity to option ordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to evaluated LLMs (ChatGPT, LLaMA)",
            "model_size": null,
            "scientific_domain": "NLP evaluation / prompt sensitivity",
            "experimental_task": "Assess internal consistency/stability of LLMs for semantically equivalent inputs by measuring accuracy variations across prompt templates and answer-option orderings.",
            "variability_sources": "Prompt wording variants (5 templates), order of candidate options (6 permutations), in-context learning session configuration.",
            "variability_measured": true,
            "variability_metrics": "Standard deviation of accuracy across prompts; standard deviation across option orders; ACR where applicable.",
            "variability_results": "ChatGPT: std of accuracy across prompts = 1.9%, std across option orders = 1.13% (low sensitivity). LLaMA: std across prompts = 11.9% and strong first-option bias (high sensitivity). Earlier summary also reported ChatGPT average fluctuation of ~3.2% for semantically identical inputs.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Std. deviation of accuracy across prompt variants and option-order permutations.",
            "reproducibility_results": "Prompt phrasing can induce non-negligible variability especially in pretraining-only models (LLaMA); ChatGPT is comparatively robust to prompt wording and option ordering but not immune to adversarial perturbations.",
            "reproducibility_challenges": "Prompt engineering choices and absence of standard prompt templates can reduce reproducibility; pretraining-only models require instruction tuning to reduce variability; multiple acceptable prompts in the wild lead to inconsistent user-facing behavior.",
            "mitigation_methods": "Use a fixed prompt template (authors adopted a single multiple-choice prompt template), in-context learning within same chat session, and recommend prompt standardization across evaluations.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Five prompt variants tested; six option orders tested; results aggregated across datasets and shown as boxplots (Figure 4).",
            "key_findings": "Prompt wording and option ordering are measurable sources of variability: well-tuned/instruction-tuned models (ChatGPT) show low variability, while pretraining-only models (LLaMA) show much higher variability and order bias; standardizing prompts and using instruction-tuned models improves consistency.",
            "uuid": "e637.5",
            "source_info": {
                "paper_title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "rating": 2
        },
        {
            "paper_title": "Impact of adversarial training on robustness and generalizability of language models",
            "rating": 1
        },
        {
            "paper_title": "Evaluating the robustness of neural language models to input perturbations",
            "rating": 1
        },
        {
            "paper_title": "Interpreting the robustness of neural nlp models to textual perturbations",
            "rating": 2
        },
        {
            "paper_title": "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
            "rating": 1
        }
    ],
    "cost": 0.0177955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility</h1>
<p>Wentao $\mathbf{Y e}^{\mathbf{1}}$, Mingfeng $\mathbf{O u}^{\mathbf{1}}$, Tianyi $\mathbf{L i}^{\mathbf{1}}$, Yipeng Chen ${ }^{\mathbf{1}}$, Xuetao $\mathbf{M a}^{\mathbf{2}}$, Yifan $\mathbf{Y a n g g o n g}^{\mathbf{2}}$, Sai $\mathbf{W u}^{\mathbf{1}}$, Jie $\mathbf{F u}^{\mathbf{3}}$, Gang Chen ${ }^{\mathbf{1}}$, Haobo Wang ${ }^{\mathbf{1}}$, Junbo Zhao ${ }^{\mathbf{1}}$<br>${ }^{1}$ Zhejiang University<br>${ }^{2}$ ZhongHao XinYing (Hangzhou) Technology Co., Ltd.<br>${ }^{3}$ Beijing Academy of Artificial Intelligence</p>
<h4>Abstract</h4>
<p>The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite uncommon from this trendy community. Briefly, they are: (i)-the minor but inevitable error occurrence in the user-generated query input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess poor consistency when processing semantically similar query input. In addition, as a side finding, we find that ChatGPT is still capable to yield the correct answer even when the input is polluted at an extreme level. While this phenomenon demonstrates the powerful memorization of the LLMs, it raises serious concerns about using such data for LLM-involved evaluation in academic development. To deal with it, we propose a novel index associated with a dataset that roughly decides the feasibility of using such data for LLM-involved evaluation. Extensive empirical studies are tagged to support the aforementioned claims.</p>
<h2>1 Introduction</h2>
<p>In recent few months, the LLMs - in particular ChatGPT - have swept the world by causing huge influence on numerous domains extending from the computer science community. Assisted by the WebUI ${ }^{1}$, open-source models Touvron et al. [1], APIs ${ }^{2}$ or the ecosystem Plugins ${ }^{3}$, these LLMs successfully immerse into everyone's life, worldwide.</p>
<p>This success of LLMs is truly unprecedented, and even rare in the entire spectrum of development of the technology. This gives rise to various evaluation efforts surrounding large models[2-6], which encounter numerous challenges: (i)-The complexity of LLM's outputs necessitates heavy reliance</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of Evaluation Framework.
on human evaluation, hindering large-scale assessments; (ii)-Unlike traditional NLP models, current LLMs are widely deployed and exposed, posing additional potential risks hard to be detected through existed NLP capability evaluation; (iii)-The massive and unknown training data of LLMs present a pressing issue in selecting trustworthy evaluation data.</p>
<p>Driven by this, we fully consider the issues that are commonly encountered in relevant scenarios but have been rarely addressed in previous works. And we propose an automated workflow to conduct a systematic study of the LLMs covering three new terms of robustness, consistency, and credibility, as a pioneer attempt.</p>
<ul>
<li>the robustness of the LLM refers to the malicious queries that are made intentionally or unintentionally. Indeed, one can cast this problem as a conventional threat model of adversarial examples for NLP. Rather, in this work we delimit the robustness of LLMs against the conventional counterpart, in particular matching the threat model to the realistic deployment of the LLMs.</li>
<li>the consistency of the APIs. This is a new concept we intend to promote. Briefly, we try to quantitatively measure the distinction of the LLMs processing two semantically homogenous query inputs. To do that, we propose a novel threat model coupled with two LLM-adapted attacking means.</li>
<li>the credibility of the LLM evaluation. This measurement is perhaps most related to the academic community centered around the LLMs. This line is very much alerted by the following phenomenon: on certain datasets, ChatGPT is still able to spit out the correct answer, even when we pollute the passage information completely! On one hand, this "blind-eye" QA capacity of ChatGPT can be deemed as the LLM manages to memorize the provided datasets. Despite that, we urge the community to take caution when using these datasets to evaluate any potential compositional framework that composes an LLM within.</li>
</ul>
<p>In spite of the concise concepts we list above, there are several must-do steps to reach the evaluation of LLMs. Hereby, we briefly introduce our overall workflow.</p>
<p>Overall workflow First, we utilize the gpt-3.5-turbo API (the ChatGPT-level API), and the open-sourced LLaMA and OPT models as the main backbones for this study. Notably, to attain the statistically sound results, we conduct over one million queries (around 0.7 billion tokens ${ }^{4}$ on ChatGPT API) for each of the LLMs.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Then, to cope with the scaled query-response pairs, an automated interpreter is required. To do that, we devise a universal data primitive that composes of five parts: (prompt, $\mathbf{p}, q, o, a)$, where five symbols in order indicate prompt, passage, question, confusion options, and the corresponding answer. Upon 12 publicly available datasets, we manage to convert each data point to this form of primitive. Noted, the nature of the primitive is a QA form with the answers being a multi-option selection problem. By regularizing the output from the LLM, we manage to process the scaled responses parallelly.
At last, centered around the three aspects of LLM evaluation, we respectively propose two threat models and five attack schemes with an index number associated with the dataset itself. Without much detail, the two threat models point to robustness and consistency respectively. For the five attack schemes, unlike the conventional NLP adversarial attacks Jia and Liang [7], we purposely structurize them such that they correspond to several specific usages of the LLM API/models. The index - dubbed as RTI - is associated with the dataset. RTI provides a reference index to quantitatively measure the probability that the dataset has been memorized by the model. In that way, RTI can be viewed as a measure of the dataset's credibility, helping us decide whether to use this dataset in the evaluation.</p>
<p>Take-away lessons . With empirical study and analysis, our concerns can be summarized as follows.</p>
<ol>
<li>on the robustness of the LLMs, attacks - especially those that are closely correlated to the usage of LLMs - may pose a hazardous potential, such as the visual attacks for OCR-ed API input or typo error for ASR-ed input. Moreover, the developers using ChatGPT ought to be careful with the input contents, due to that some minor but structured changes may drastically shift the model/API output unexpectedly, e.g. by no less than $27 \%$ as per our test for ChatGPT.</li>
<li>from the consistency evaluation of the LLMs, we take ChatGPT as an example. Unfortunately, we find that with semantically identical query input, ChatGPT's accuracy of the responses at average fluctuates by $3.2 \%$ in our testing protocol. Further noted, the change to the input is mostly in the aspects of grammatical expression, writing habits and etc., which happens quite commonly considering the vast deployment of these LLMs.</li>
<li>RTI: RTI measures the relative probability that the dataset has been memorized. In other words, RTI is an index that characterizes if a provided dataset is suitable or credible for the LLM-involved evaluation process. We hope this metric can benefit the academic community towards developing a better suitable evaluation ecosystem in the era of LLMs.</li>
</ol>
<p>We open-source our datasets and samples at https://github.com/yyy01/LLMRiskEval_RCC for further development.</p>
<h1>2 Related Work</h1>
<h3>2.1 Adversarial Attack in NLP</h3>
<p>In traditional NLP tasks, there have been various works studying the adversarial attack models. Due to the non-differential nature of the text data, the attacks are mostly conducted in a block-box threat model. These attacks widely cover character-level manipulation [8-11], word-level manipulation [12-14], and sentence-level manipulation [15, 16]. Moreover, other works [17-22] are following a white-box fashion, requiring access to the target model's gradients, structure, or parameters. However, as the present ChatGPT version offers only interfaces, we can only employ black-box attacks for the evaluation.</p>
<h3>2.2 Adversarial Attacks on LLMs</h3>
<p>Previously, Altinisik et al. [23], Moradi and Samwald [24], Stolfo et al. [25], Zhang et al. [26] presented a variety of robustness assessing schemes for LLMs. This work generally resorts to using manageable perturbation on the input to the LLMs, such as typos, entities swap, negations, sentence insertion, etc.</p>
<p>Despite that, most - if not all - of these approaches are limited to smaller LLMs such as BERT [27], and XLNET [28]. Given the current terrain of rapidly developing and enlarging the LLMs, we argue that LLMs at the scale of ChatGPT need to be studied. Rather, a recent work [29] presents that ChatGPT demonstrates a consistent advantage in most adversarial classification and translation tasks.</p>
<h1>2.3 Threat Model</h1>
<p>The concept of the threat model originates from computer security. It refers to the process of identifying, evaluating, and managing threats that a system may suffer from. In NLP, threat models are widely used to help administrators identify possible threats, so as to take corresponding preventive actions. Brendel et al. [30], Neekhara et al. [31], Hambardzumyan et al. [32], Wallace et al. [33] and Madry et al. [34] have conducted various work on threat models, towards black-box systems or APIs. However, most of these methods more focus on smaller Machine Learning models, whose impact on LLMs is limited. With the popularity of ChatGPT, new threat models, more closely integrated with LLMs' application scenarios, are urgently required.</p>
<h3>2.4 LLMs' Evaluation: Reliable?</h3>
<p>Indeed, with the rise of ChatGPT and GPT-4, there have been various works coming to light in evaluating these LLMs on off-the-shelf datasets. To name a few, Zhong et al. [2], Qin et al. [3], Huang et al. [4], Kocoń et al. [5], Yang et al. [6] focus on developing additional model.
However, as we mentioned, the evaluation protocol when involving a powerful LLM requires further scrutiny. That is, our RTI index indicates a rough and relative rate that the fed testing samples were already memorized by the LLM. Consequently, the isolated evaluation on the external modules besides the LLM might be skeptical because we show that a ChatGPT is capable to answer questions without the help of any other information - for some datasets.</p>
<p>Indeed, our RTI is only capable to imply a relative score for the datasets, not the absolute one, without any further information about the dataset which the LLM was trained upon. We refer the reader to Section 3.6 for detailed information.</p>
<h2>3 Method, Data, and Desiderata</h2>
<p>In this work, we elaborate on the details of our workflow. In that, we begin with the data form construction that is catered to this particular task. In what follows, we detail the specific gauging mechanisms for evaluating the robustness, consistency, and credibility of the current LLMs. To wrap up, we formally provide the metric system for this study.</p>
<h3>3.1 Raw Dataset Curation</h3>
<p>We adopt 12 publicly available datasets for benchmarking. Noted, these datasets mainly contain tasks of mathematical deduction, logical reasoning, commonsense understanding and etc. The full setup is listed in Table 1.</p>
<p>Formally, for each individual data point among these datasets, we use a triplet form to represent it, as $(\mathbf{p}, q, a)$ where the three symbols in order indicate passage, question, and the corresponding answer respectively. Notably, data points in certain datasets - e.g. AQuA, Creak in Table 1 - do not involve a preset passage. For uniformity, we may still refer them to the triplet form while the $\mathbf{p}$ variable is proactively set to Null.</p>
<h3>3.2 Data Assembly</h3>
<p>As we mentioned, we ran overall more than a million queries on the chosen benchmarking LLMs. To cope with the scaled number of query responses, a must-do preliminary step is certainly to figure out an automatic interpreter that manages to assess each individual response with no explicit requirement of human scrutiny or intervention.</p>
<p>Table 1: Datasets statistics. Answer Type row refers to the types of answers given in datasets, where T/F means True or False, Number means Numerical value, Word means English words, Text means multi-words or sentences, and multi means multi-types.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Description</th>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Answer <br> Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">StrategyQA [35]</td>
<td style="text-align: left;">QA</td>
<td style="text-align: center;">train</td>
<td style="text-align: center;">2290</td>
<td style="text-align: center;">T/F</td>
</tr>
<tr>
<td style="text-align: left;">AQuA [36]</td>
<td style="text-align: left;">Algebra QA</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">Numbers</td>
</tr>
<tr>
<td style="text-align: left;">Creak[37]</td>
<td style="text-align: left;">Commonsense Reasoning</td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">1371</td>
<td style="text-align: center;">T/F</td>
</tr>
<tr>
<td style="text-align: left;">NoahQA[38]</td>
<td style="text-align: left;">Numerical Reasoning QA</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">10880</td>
<td style="text-align: center;">Multi</td>
</tr>
<tr>
<td style="text-align: left;">GSM8k[39]</td>
<td style="text-align: left;">Math Reasoning</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">6140</td>
<td style="text-align: center;">Text</td>
</tr>
<tr>
<td style="text-align: left;">bAbi15[40]</td>
<td style="text-align: left;">Deductive Reasoning</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Word</td>
</tr>
<tr>
<td style="text-align: left;">bAbi16[41]</td>
<td style="text-align: left;">Inductive Reasoning</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Word</td>
</tr>
<tr>
<td style="text-align: left;">QASC[42]</td>
<td style="text-align: left;">Multi-hop Reasoning</td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">926</td>
<td style="text-align: center;">Word</td>
</tr>
<tr>
<td style="text-align: left;">ECQA[43]</td>
<td style="text-align: left;">Commonsense Reasoning</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">2194</td>
<td style="text-align: center;">Word</td>
</tr>
<tr>
<td style="text-align: left;">e-SNLI[44]</td>
<td style="text-align: left;">Logical Relationship Reasoning</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">9824</td>
<td style="text-align: center;">Word</td>
</tr>
<tr>
<td style="text-align: left;">Sen-Making [45]</td>
<td style="text-align: left;">Commonsense Reasoning</td>
<td style="text-align: center;">test</td>
<td style="text-align: center;">2020</td>
<td style="text-align: center;">Text</td>
</tr>
<tr>
<td style="text-align: left;">QED [46]</td>
<td style="text-align: left;">QA with explanations</td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">1354</td>
<td style="text-align: center;">Text</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39253</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>That said, as a core component for the automatic interpreter, the raw form drawn from the original dataset requires amendment. To that regard, we design a template with a uniform data form that inherent is a multiple-option QA data primitive, showed in Table 3 where we demonstrate a set of samples. In particular, we represent the target template as quintuplet form (prompt, $\mathbf{p}, q, o, a)$. Here, we write down a set of confusing options that (i)-are not the correct option but (ii)-but similar to the ground-truth answer $a$. The parameter prompt denotes the prompts. Notably, $a$ won't be particularly long paragraphs and ChatGPT is able to output in our expected way under our setting of o. Essentially, this templated data form urges the LLM to correctly differentiate the correct answer $a$ against the others in $o$, based on the passage $\mathbf{p}$ and the connected question $q$. Thanks to the nature of this multiple-option form, we may simply scan the responses spit from the LLM for the index (e.g. A, B, C, D...) and pave the way for accurate calculation. Guided by good prompts $\mathbf{p}$, this auto-conversion mechanism empirically works well with very few exceptions.</p>
<p>In the implementation, we first build a converter to automatically fetch $(\mathbf{p}, q, a)$ part except for confusion options. For most datasets(as Appendix A shows) that provide passages and questions separately, we extract corresponding fields directly. Taking the example in Table 3, contexts of 'facts', 'question', and 'answer' fields correspond to $(\mathbf{p}, q, a)$ separately.
This above mechanism shall already suffice for the datasets that expectedly consist of a passage. For those that do not contain the passages, we alter the primitive by the following steps: (i)- we divide the question into two parts where the prior part involves and only involves the statement or the conditions, while the latter part is reduced to be the question. The process is automatically implemented by simple identification and sentence segmentation; (ii)-for the datasets where the questions are not decomposable, we simply set $\mathbf{p}$ to Null.</p>
<p>Confusion Option Generation We devise a generator that is dedicated to producing some confusion options $o$ according to answers $a$. As the core component of the primitive, the LLM is expected to choose $a$ against these confusion counterparts. From Table 1, we may summarize that the answers in these datasets primarily are categorized by the following four types: T/F(True or False), Number, Word, and Text. For $o$ in this primitive, a good confusion option ought to be maximally similar to the ground truth but can be differentiated by the common-sense understanding or reasoning capacity of the LLM. Samples of confusion options are shown in Table 2 In particular, for the distinct four categories, we devise the generated as follows:</p>
<ul>
<li>For datasets already provide a number of wrong options (e.g., AQuA, QASC, ECQA, and e-SNLI), we directly use these options as $o$.</li>
<li>
<p>For T/F binary answers, e.g., StrategyQA and Creak. While the built-in form of T/F should suffice for our primitive, we additionally incorporate a third option in $o$ as unable to determine to increase the difficulty.</p>
</li>
<li>
<p>For Numbered answers, e.g., NoahQA(part of queries), we randomize four numerical values to form $o$. Hence, the total option number is 5 .</p>
</li>
<li>For Word-ed answers, e.g., bAbi15, bAbi16, ECQA. We randomly select the other words of the same linguistic property, and upon the results, we run from the Part-of-Speech tagger, Brill [47], in passages and/or questions. The total number in $o$ for this case is also 5.</li>
<li>For Text-ed answers, e.g., GSM8K, . Basically, we alternate among the following three means: i)-delete, insert, or replace some words, ii)-change the correctness of the formula (if existed) or replace formulas of the current step with that of other steps and iii)-remove ground-truth $a$ and add an extra option 'None of the other options is correct'.</li>
</ul>
<p>Table 2: Confusion Options Generation Samples. For easier observation, we put all correct options in A-place. We actually randomize the order of the options as model's input.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Answer <br> Type</th>
<th style="text-align: left;">Original <br> Answer</th>
<th style="text-align: left;">Generated Options</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T/F</td>
<td style="text-align: left;">True</td>
<td style="text-align: left;">(A) True <br> (B) False <br> (C) Unable to determine</td>
</tr>
<tr>
<td style="text-align: center;">Numbers</td>
<td style="text-align: left;">36</td>
<td style="text-align: left;">(A) 36 <br> (B) 15 <br> (C) 17 <br> (D) 5 <br> (E) 7</td>
</tr>
<tr>
<td style="text-align: center;">Word</td>
<td style="text-align: left;">discovery</td>
<td style="text-align: left;">(A) discovery <br> (B) action <br> (C) reflection <br> (D) deciding <br> (E) thinking</td>
</tr>
<tr>
<td style="text-align: center;">Text</td>
<td style="text-align: left;">Janet sells 16-3-4=9 <br> duck eggs a day</td>
<td style="text-align: left;">(A) None of the other options is correct. <br> (B) Janet elude sells 16-3-4=9 duck eggs a axerophthol day. <br> (C) Janet sells 4-4-10=-10 duck eggs a day. <br> (D) Janet sells 11-11-15=28 duck eggs a day. <br> (E) He has to pay 3000-1000=2000.</td>
</tr>
</tbody>
</table>
<p>Prompt Construction and In-context Learning We manually construct a fixed template as prompt, whose format is:
Next, I will ask you a series of questions given a description, and you will have to choose one of several candidate options that you think is correct. The description is.
Based on this prompt template and In-context Learning(ICL, Brown et al. [48]), we divide a sample (prompt, $\mathbf{p}, q, o, a$ )into queries. And we give queries to the model step by step in the same chat process, serving as In-Context Learning of ChatGPT. As the 'Query' row in Table 3 shows, settings of queries contain i) A context directly connected by prompt and passage p; ii) A context composed by question $q$, options $o$ and other linking clauses that make sentences flow. iii) If a sample provides more than one question for a passage, we will form follow-up queries with each question and corresponding options in turn. After our verification, this template and ICL setting are (almost) universally feasible for constraining ChatGPT to answer the questions provided in our expected way.
We follow with evaluation methods surrounding robustness, consistency, and credibility. By this means, we hope to provide a throughout reference for ChatGPT's potential risks against real-world applications. Before that, we give some setups. For each dataset $D$, it is denoted as:</p>
<p>$$
D=\left{\mathbf{x}<em i="1">{i}\right}</em>}^{n}=\left{\left(\operatorname{prompt<em i="i">{i}, \mathbf{p}</em>
$$}, q_{i}, o_{i}, a_{i}\right)\right}_{i=1}^{n</p>
<p>where $\mathbf{x}_{i}$ denotes i-th sample in $D$, and the definition of (prompt, $\mathbf{p}, q, o, a)$ is showed in Section 3.2 .</p>
<p>Table 3: The construction of model input from the original sample. The sample in this case was taken from the dataset StrategyQA. Origin structure shows raw data form. And the target data form, divided into some parts, is shown in other structure rows in order.</p>
<table>
<thead>
<tr>
<th>Structure</th>
<th>Sample</th>
</tr>
</thead>
<tbody>
<tr>
<td>Origin</td>
<td>question": "Is Antarctica a good location for Groundhog Day?", <br> "answer": false, <br> "facts": [ <br> "Groundhog Day relies on a groundhog seeing their shadow.", <br> "Antarctica has an irregular sun pattern and some days have no sun rise or 24 <br> hour sunlight.", <br> "Antarctica has temperatures can range from -10 C to -60 C.", <br> "Groundhogs live in forests or woodlands with plenty of sunlight." <br> ]</td>
</tr>
<tr>
<td>Prompt</td>
<td>Next, I will ask you a series of questions given a description, and you will have to choose one of several candidate options that you think is correct. The description is</td>
</tr>
<tr>
<td>Paragraph</td>
<td>Groundhog Day relies on a groundhog seeing their shadow.Antarctica has an irregular sun pattern and some days have no sun rise or 24 hour sunlight.Antarctica has temperatures can range from -10 C to -60 C .Groundhogs live in forests or woodlands with plenty of sunlight.</td>
</tr>
<tr>
<td>Question</td>
<td>Is Antarctica a good location for Groundhog Day?</td>
</tr>
<tr>
<td>Options</td>
<td>(A) True <br> (B) False <br> (C) Unable to determine</td>
</tr>
<tr>
<td>Query</td>
<td>Query 1: ${$ Prompt $}+{$ Paragraph $}$ <br> Query 2: The first question is ${$ Question $}$, choose an answer from the following options: ${$ Options $}$.</td>
</tr>
<tr>
<td>Response</td>
<td>The answer would be (B) False. Antarctica is not a good location for Groundhog Day since the area has an irregular sun pattern, and some days could have no sun rise, or 24 -hour sunlight. Groundhogs rely on seeing their shadow to predict the weather, and the irregular patterns of sunrise and sunset would make the whole concept unworkable in Antarctica.</td>
</tr>
</tbody>
</table>
<h1>3.3 Gauge the Robustness</h1>
<p>The upfront property of the LLM we intend to gauge is its robustness. In a nutshell, we draw inspiration from the conventional NLP community - in particular, those research centered around adversarial examples on text. Borrowing the terms from Zhang et al. [26], a perturbed example input is defined as</p>
<p>$$
x^{\prime}=x+\delta ;|\delta| \leq \epsilon \wedge f(x, \theta) \neq f(x, \theta)
$$</p>
<p>where $\delta$ denotes the perturbation inserted into original sample $\mathbf{x}$ yielding its perturbed counterpart $\mathbf{x}^{\prime} . f(x, \theta)$ represents the model in a functional form parametrized by $\theta$.</p>
<p>Generally, in the conventional robustness assessment of NLP models, the $\epsilon$ is expected to be extremely small. In spite of that, we shift the extensive emphasis on minimizing the distance/difference from the adversarial examples to their clean counterparts. By contrast, we stress constructing the adversarial examples via more structural means that are more compatible with the realistic deployment of the LLMs, rather than random adversarial perturbation. For instance, with the release of the ChatGPT APIs, an (exponentially) increasing number of user-generated inputs would be fed to the API, followed by the distribution of the LLM's responses. In this work, we delve into it and gauge mistakes - that are common in user-generated content - such as character-level typos (e.g typing error), incorrect words (e.g. user input obtained from speech transcription), or visual flaws (e.g. user input yielded from OCR), etc. We believe this setup may simulate the possible outcomes for the particular deployment of the LLMs, being a better simulation than the study around conventional adversarial examples.</p>
<p>We carry out an adversarial attack(constructing adversarial examples) process with an auto-attacker $g$. Through this attacker, we can transfer the process to any dataset, as a part of the general automated evaluation system. Following conventional setting [49-51], we only perform adversarial attacks on $\mathbf{p}$ if $\mathbf{p}$ exists( $q$, otherwise). The result in a sample $\mathbf{x}$ set adversarial example $\mathbf{x}^{\prime}=\left(P, \mathbf{p}^{\prime}, q, o, a\right)$, and $\mathbf{p}^{\prime}$ and $\mathbf{p}$ is denoted as:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{p}^{\prime}=\left[\mathbf{w}<em 1="1">{1}^{<em>}, \cdots, \mathbf{w}_{n}^{</em>}\right] \
&amp; \mathbf{p}=\left[\mathbf{w}</em>\right]
\end{aligned}
$$}, \cdots, \mathbf{w}_{n</p>
<p>where $w$ or $\mathbf{w}^{<em>}$ denotes word in $\mathbf{p}$ or $\mathbf{p}^{\prime}$, and $\mathbf{w}^{</em>}$ can be further denoted as:</p>
<p>$$
\mathbf{w}<em i="i">{i}^{*}= \begin{cases}z \sim \mathcal{U}(0,1) \ \boldsymbol{g}\left(\mathbf{w}</em>
$$}\right) &amp; 0&lt;z&lt;\rho \ \mathbf{w}_{i} &amp; \text { Otherwise }\end{cases</p>
<p>where $\mathcal{U}$ is a uniform distribution, and $\rho$ is a preset probability between $[0,1]$, representing the probability of a word to be attacked.
Now, we go to the specific instantiations of auto-attacking function $g$ on each word $w$ from passage p. Noted again, we want to simulate the specific usage of LLMs. The robustness of the considered LLM for the following setup is gauged by the calculation of the accuracy alter by the perturbed word against the original ones. With the examples provided in Table 4, we define the following methods to complement the auto-attacking function $g$. Notably, all the attacks focus on each word $w$. While each $w$ maintains a probability $\rho$, mentioned above, of being altered.</p>
<ul>
<li>Word-level attacks. For each $w$ that should be altered, we randomly conduct one of the three operations(i.e., insertion, deletion, and replacement). Notably, for replacement and insertion, altered word $\mathbf{w}^{<em>}$ has a $50 \%$ chance to be a random word from the original passage p. Otherwise, $\mathbf{w}^{</em>}$ are synonyms of $w$ or random words from WordNetMiller [52]. Wordlevel attack simulates word errors in applications, e.g., missing or misidentifying words in speech recognition.</li>
<li>Character-level attacks. We randomly select characters of $w$, to be altered, according to a preset proportion. Subsequently, we perform one of the operations(i.e., repeat, insert, and delete) on selected characters. The settings of the proportion and inserted characters are listed in Appendix B. Character attack simulates those human natural errors, like typing or spelling mistakes, in user-generated contexts.</li>
<li>Visual-level attacks. Similar to character attacks, We select English letters in $w$, to be altered, by a preset ratio(see Appendix B for details). And we perform substitution of these letters with visual-similar characters. The data source of these characters is derived from previous work [11]. This attack simulates visual flaws in some real-world applications, e.g., OCR.</li>
</ul>
<p>Table 4: Attack Description. The context inside the red brackets refers to the Unicode of the new character.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Specific</th>
<th style="text-align: center;">Sample</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">George Washington died in 1799.CDs weren't invented until 1982.</td>
</tr>
<tr>
<td style="text-align: center;">Character Level</td>
<td style="text-align: center;">repeat</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Georggge Washington diiied iiin } 177799 . \text { CDs weren't innnvented until } \ &amp; 199982 . \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">delete</td>
<td style="text-align: center;">George Washington died in 1799.CDs weren't inted un 1982.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">insert</td>
<td style="text-align: center;">George Washington di@ed in 1799@.CDs weren't invented until 1982.</td>
</tr>
<tr>
<td style="text-align: center;">Word Level</td>
<td style="text-align: center;">insert</td>
<td style="text-align: center;">died George Washington died in 1799.CDs 1799.CDs weren't invented hoosier state until 1982.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">delete</td>
<td style="text-align: center;">invented.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">replace</td>
<td style="text-align: center;">George cook up cook up 1982 1799.CDs go bad invented until 1982.</td>
</tr>
<tr>
<td style="text-align: center;">Visual Attack</td>
<td style="text-align: center;">/</td>
<td style="text-align: center;">George $\quad$ Washin ${0260}$ ton ${0257}$ ied in 1799.CDs weren't ${0269}$ invented unti ${0625} 1982$.</td>
</tr>
</tbody>
</table>
<h1>3.4 Gauge the Consistency</h1>
<p>As we mentioned, we use the term consistency to measure the consistency of the LLMs when faced with user-generated inputs that convey similar semantics. Thereby, we conduct design two novel attacking methods that reflect two distinct aspects.</p>
<p>Prompt Attack In real-world applications, different users may have diverse expressions for the input of the same meaning. These inputs are semantically similar but in very different forms. It is of great importance for ChatGPT to maintain consistency against these inputs. To conduct the simulation, we select five statements, denoted as prompt ${ }^{\prime}$, synonymous with prompt(listed in Table 5). And we alternately replace prompt with different prompt ${ }^{\prime}$, creating an automated process. In this process, we test ChatGPT's output against different prompt ${ }^{\prime}$, to measure consistency for different prompts.</p>
<p>Table 5: Prompts for the multiple-option test.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Text</th>
<th style="text-align: left;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">""</td>
<td style="text-align: left;">Blank prompt</td>
</tr>
<tr>
<td style="text-align: left;">"Complete the description with an appropriate ending: "</td>
<td style="text-align: left;">Valid prompt of OPT</td>
</tr>
<tr>
<td style="text-align: left;">"You must choose the best answer from the following choices marked <br> (A), (B), (C), (D) or (E)."</td>
<td style="text-align: left;">CET examination</td>
</tr>
<tr>
<td style="text-align: left;">"To answer the following question according to the following infor- <br> mation."</td>
<td style="text-align: left;">Human-made</td>
</tr>
<tr>
<td style="text-align: left;">"Next, I will ask you a series of questions given a description, and you <br> will have to choose one of several candidate options that you think is <br> correct."</td>
<td style="text-align: left;">Human-made</td>
</tr>
</tbody>
</table>
<p>Options Attack For the other, we randomize the order of options - $a$ and $o$ combined - in Formula 1 in order to test LLMs' performance. It is apparent that this has no impact on the semantics of the data primitive itself, but empirically we find certain LLM exhibit weird behavior against this attack that we show in the experiments section. This attack method simulates some parallel grammatical structure, whose order does not affect reading, in actual usage.</p>
<h3>3.5 Gauge the Credibility: Relative Training Index.</h3>
<p>Indeed, this credibility gauging is admittedly a side-product of the empirical study of this work. That is when we conduct the aforementioned attack method in extreme strength and magnitude, LLMs are still able to perform accurately. Similar to the VQA benchmarking issue(Jabri et al. [53]), this cast serious concerns on the current benchmarking methods, especially when being used to assess LLM-involved methods. Thus, in spite of being a side-product, the importance of this section shall not be deprioritized nor overlooked. In particular, we present a novel indexing number - dubbed as Relative Training Index, - that measures the relative reliability of the provided datasets towards LLM-related evaluation.
Formally, the RTI is calculated by the following steps (specific complementation is showed in Algorithm 1)</p>
<ul>
<li>We adopt the word-level attack due to its commonality. As per Formula 4, we gradually increase the parameter $\rho$ from 0 to 1 by a stride of 0.1 . Within each strived point, we test the LLM towards its final accuracy.</li>
<li>We detect the output of the model for each $\rho$. During this process, we find the minimum $\rho$, denoted as $r$, that causes the model to change the original answer.</li>
<li>Finally, we calculate the expectation of $r$ for each x in dataset $D$ as the $R_{D}$ (RTI). RTI can be denoted as:</li>
</ul>
<p>$$
\mathbf{R}<em _mathbf_x="\mathbf{x">{D}=\mathbb{E}</em>
$$} \in D} r_{\mathbf{x}</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> Calculate <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathrm<span class="p">{</span>RTI<span class="p">}</span> <span class="err">\</span>mathbf<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>D<span class="p">}</span><span class="err">\</span><span class="p">)</span> of dataset <span class="err">\</span><span class="p">(</span>D<span class="err">\</span><span class="p">)</span>
Input<span class="p">:</span>
    <span class="err">\</span><span class="p">(</span><span class="ss">D</span><span class="o">=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span><span class="mi">1</span><span class="p">},</span> <span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span><span class="mi">2</span><span class="p">},</span> <span class="err">\</span>cdots<span class="p">,</span> <span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>n<span class="p">}</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span> <span class="err">\</span>cdot<span class="err">\</span><span class="p">)</span> dataset
    <span class="err">\</span><span class="p">(</span>g<span class="p">(</span>x<span class="p">,</span> <span class="err">\</span>rho<span class="p">):</span><span class="err">\</span><span class="p">)</span> auto-attacker on <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="k">with</span> probability <span class="err">\</span><span class="p">(</span><span class="err">\</span>rho<span class="err">\</span><span class="p">)</span>
    <span class="err">\</span><span class="p">(</span>f<span class="p">(</span>x<span class="p">,</span> <span class="err">\</span>theta<span class="p">):</span><span class="err">\</span><span class="p">)</span> model output on <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="k">with</span> parameter <span class="err">\</span><span class="p">(</span><span class="err">\</span>theta<span class="err">\</span><span class="p">)</span>
Output<span class="p">:</span>
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>D<span class="p">}:</span><span class="err">\</span><span class="p">)</span> RTI score of dataset <span class="err">\</span><span class="p">(</span>D<span class="err">\</span><span class="p">)</span>
    for <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span><span class="p">)</span> <span class="k">in</span> <span class="err">\</span><span class="p">(</span>D<span class="err">\</span><span class="p">)</span> do
        <span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="ss">rho</span><span class="o">=</span><span class="mf">0.1</span><span class="err">\</span><span class="p">)</span>
        <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span><span class="o">=</span>g<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>rho<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        while <span class="err">\</span><span class="p">(</span>f<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>theta<span class="err">\</span>right<span class="p">)</span><span class="o">=</span>f<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">},</span> <span class="err">\</span>theta<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> do
            <span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="ss">rho</span><span class="o">=</span><span class="err">\</span>rho<span class="o">+</span><span class="mf">0.1</span><span class="err">\</span><span class="p">)</span>
            <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">^</span><span class="p">{</span><span class="err">\</span>prime<span class="p">}</span><span class="o">=</span>g<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>rho<span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
        end while
        <span class="err">\</span><span class="p">(</span>r<span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span>_<span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="o">=</span><span class="err">\</span>rho<span class="err">\</span><span class="p">)</span>
    end for
    <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>D<span class="p">}</span><span class="o">=</span><span class="err">\</span>mathbb<span class="p">{</span>E<span class="p">}(</span>R<span class="p">)</span><span class="err">\</span><span class="p">)</span> where <span class="err">\</span><span class="p">(</span>R <span class="err">\</span>sim r<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}),</span> <span class="err">\</span>mathbf<span class="p">{</span>x<span class="p">}</span> <span class="err">\</span><span class="k">in</span> D<span class="err">\</span><span class="p">)</span>
    return <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>R<span class="p">}</span>_<span class="p">{</span>D<span class="p">}</span> <span class="p">;</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>We intend to assess the reliability of the normal NLP task evaluation that particularly involves an LLM. Intuitively, if the dataset was chosen to fill in the overall training set for the particular LLM, by proper training flow, the LLM should have absorbed and memorized the aspects of this dataset. In that case, incrementally developing an external module to couple the LLM for this dataset or its involved task - would be somewhat problematic. Put another way, we find that in some occasions, LLM can respond correctly even when the passage sample is $\mathbf{1 0 0 \%}$ polluted! This would evidently mean that the LLMs have already encoded the information and/or its answer into their parametric memorization. Hence, further evaluation of LLM-involved tasks on such occasions would very much be misleading. Further, we would admit that this study is just an implication of the reliability problem rather than fully explaining or resolving the puzzle - due to that, there is not a transparent path towards reverse-engineering on the original training set for such LLM (especially ChatGPT).
Put another way, indicated by RTI, the higher the score the lower reliability of the developed method is, and vice versa.
(Perhaps wild) claim. In hindsight, lower RTI shall encourage our cohorts to move away from the considered datasets in the era of the LLM.</p>
<h1>3.6 Metrics</h1>
<h3>3.6.1 Metrics on Robustness and Consistency.</h3>
<p>To finalize the gauging process on the popular LLMs, we devise two complementary quantitative measurements: the error rate( $\mathbf{E R}, \%$ ) and the answer-changing Rate( $\mathbf{A C R}, \%$ ). In particular, the error rate resembles conventional NLP evaluation where we judge the LLM based on its accuracy in correctly responding to the data primitive. On the other hand, the error-changing rate measures the proportion from the testing set that the model maintains the original output post to the attacks.
We write down the formulation of the two metrics as follows:</p>
<p>$$
\begin{gathered}
\mathbf{E R}<em i="i">{D}=\frac{\left|\left{\mathbf{x}</em>} \mid \mathbf{x<em i="i">{i} \in D \wedge f\left(\mathbf{x}</em> \
\mathbf{A C R}}, \theta\right) \neq a_{i}\right}\right|}{|D|<em i="i">{D}=\frac{\left|\left{\mathbf{x}</em>} \mid \mathbf{x<em i="i">{i} \in D \wedge f\left(\mathbf{x}</em>
\end{gathered}
$$}, \theta\right) \neq f\left(\mathbf{x}_{i}^{\prime}, \theta\right)\right}\right|}{|D|</p>
<p>where $|$ denotes a counter.</p>
<h1>3.7 Qualitative Pattern Analysis of Adversarial Sample</h1>
<p>Besides the quantitative measurements, we intend to thoroughly analyze the LLMs' behavior against our primitive. In that, we next introduce our qualitative pattern analysis scheme. We mostly follow previous work including Nguyen-Son et al. [54], Goodman et al. [55], Zheng et al. [56], Xue et al. [57], Wang et al. [58]. Likewise, upon gathering the LLMs' responses from the corresponding input - that includes both the clean and polluted counterpart - we aim to analyze which part/component of the input prompt may maximally and most probably cause the LLM to drift.
In that regard, we leverage tools like past-of-speech taggers, dependency parsing, phrase structure discovery ${ }^{5}$ 6, and intra-sentence positional analysis. We ground this portion of the analysis on the granularity of words, which covers all the attacking means as we mentioned before.
More specifically, we devise a separate mechanism for the above sources of information:</p>
<ul>
<li>For structure and intra-sentence analysis, we consider attacked samples $\mathbf{x}^{\prime}$ with unexpected output, where $f\left(\mathbf{x}^{\prime}, \theta\right) \neq f(\mathbf{x}, \theta)$. We count different categories' occurrence frequency of perturbed parts in $\mathbf{x}^{\prime}$. Categories $l$ here are position tags(e.g., head, tail, or middle positions) or structure tags(e.g., Noun Phrase, Noun Phrase). The frequency, denoted as $\mathbf{s}_{l}$, can be represented as:</li>
</ul>
<p>$$
\begin{gathered}
\mathbf{s}<em _mathbf_x="\mathbf{x">{l}=\sum</em>^{} \in I} \frac{1}{G(\mathbf{x})} \sum_{\mathbf{w} \in \mathbf{x}, \mathbf{w} \neq \mathbf{w<em>}} \operatorname{sgn}\left(\mathbf{w}^{</em>}, \mathbf{x}, \mathbf{x}^{\prime}, l\right) \
G(\mathbf{x})=\left|\left{\mathbf{w}<em i="i">{i} \mid \mathbf{w}</em>} \in \mathbf{x} \wedge \mathbf{w<em i="i">{i} \neq \mathbf{w}</em>^{<em>}\right}\right| \
\operatorname{sgn}\left(\mathbf{w}^{</em>}, \mathbf{x}, \mathbf{x}^{\prime}, l\right)=\left{\begin{array}{ll}
1 &amp; l\left(\mathbf{w}^{*}\right)=l \wedge f(\mathbf{x}, \theta) \neq f\left(\mathbf{x}^{\prime}, \theta\right) \
0 &amp; \text { otherwise }
\end{array}\right.
\end{gathered}
$$</p>
<p>where $l(\mathbf{w})$ is a function to get the category of the word $\mathbf{w}, \mathbf{s g n}$ is a function measuring whether the sample cause the LLM to drift, $G$ is a regularization function to balance attack times.</p>
<ul>
<li>And for past-of-speech and dependency parsing analysis, similarly, we evaluate them by calculating their contributions to the successful attacks. Similar to the above, we firstly formulate a vector $\mathbf{c}$ by counting the normalized frequency of each $l$ of categories set $L$ :</li>
</ul>
<p>$$
\mathbf{c}=\left[\frac{\left|\left{\mathbf{w} \mid \forall \mathbf{w} \in \mathbf{x}, \mathbf{w}^{*} \neq \mathbf{w} \wedge l(\mathbf{w})=l_{j}\right.\right.}{\left.\left|\left{\mathbf{w} \mid \forall \mathbf{w} \in \mathbf{x}, l(\mathbf{w})=l_{j}\right}\right.\right.}\right]<em j="j">{l</em>
$$} \in L</p>
<p>Then, by checking every attacked sample in $D$, we can extract a set $\left{\left(\mathbf{c}<em i="i">{i}, \operatorname{sgn}\left(\mathbf{x}</em>}, \mathbf{x<em i="1">{i}^{\prime}\right)\right)\right}</em>}^{n}$, where $\operatorname{sgn}\left(\mathbf{x<em i="i">{i}, \mathbf{x}</em>}^{\prime}\right)$ returns 1 if $f(\mathbf{x}, \theta) \neq f\left(\mathbf{x}^{\prime}, \theta\right)$, and 0 otherwise. Secondly, we train a Random Forest (RF) on this set, with $\mathbf{c<em i="i">{i}$ and $\operatorname{sgn}\left(\mathbf{x}</em>\right)$ as input feature and output target for training, respectively. Finally, we employ the trained RF model to assign an importance score to each category related to the success of attacks.}, \mathbf{x}_{i}^{\prime</p>
<h2>4 Experiment</h2>
<h3>4.1 Robustness Results</h3>
<h3>4.1.1 Main Results</h3>
<p>The robustness test results across adversarial attacks are listed in Table 6. And specific results of each dataset are listed in Table 7 and Table 8.</p>
<p>ChatGPT show low robustness. All attack methods have a negative impact on ChatGPT. Specifically, regardless of attack types, it leads to at least a $2 \%$ increase in the ER of ChatGPT's responses. Besides, as ACR results show, all attacks cause the model to change at least $27 \%$ of the output option. Furthermore, Robustness against character-level attacks is higher, and robustness against word-level attacks is lower among three levels. Taking ER as an instance, the average ER on word-level attacks is roughly $10 \%$ higher than the character-level. Simultaneously, ER on</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 6: Different level attack influence on ChatGPT. (The results are tested on all datasets with 39253 samples in total.)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Attack Type</th>
<th style="text-align: center;">Error Rate(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer Changed Rate(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Character Level</td>
<td style="text-align: center;">ori <br> 40.19</td>
<td style="text-align: center;">repeat <br> 44.59</td>
<td style="text-align: center;">delete <br> 51.89</td>
<td style="text-align: center;">insert <br> 41.63</td>
<td style="text-align: center;">repeat <br> 30.51</td>
<td style="text-align: center;">delete <br> 38.56</td>
<td style="text-align: center;">insert <br> 27.45</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Word Level</td>
<td style="text-align: center;">ori <br> 40.19</td>
<td style="text-align: center;">insert <br> 48.04</td>
<td style="text-align: center;">delete <br> 61.89</td>
<td style="text-align: center;">replace <br> 60.85</td>
<td style="text-align: center;">insert <br> 34.27</td>
<td style="text-align: center;">delete <br> 49.8</td>
<td style="text-align: center;">replace <br> 48.18</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Visual-level</td>
<td style="text-align: center;">ori <br> 40.19</td>
<td style="text-align: center;">$10 \%$ <br> 42.47</td>
<td style="text-align: center;">$50 \%$ <br> 48.85</td>
<td style="text-align: center;">$90 \%$ <br> 55.88</td>
<td style="text-align: center;">$10 \%$ <br> 28.19</td>
<td style="text-align: center;">$50 \%$ <br> 34.64</td>
<td style="text-align: center;">$90 \%$ <br> 41.94</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Different level attack influence of each dataset on ChatGPT, measured by error rate(\%).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Charater Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Word Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Visual Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">ori</td>
<td style="text-align: center;">repeat</td>
<td style="text-align: center;">delete</td>
<td style="text-align: center;">insert</td>
<td style="text-align: center;">insert</td>
<td style="text-align: center;">delete</td>
<td style="text-align: center;">replace</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$90 \%$</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">29.56</td>
<td style="text-align: center;">31.10</td>
<td style="text-align: center;">37.46</td>
<td style="text-align: center;">31.09</td>
<td style="text-align: center;">35.48</td>
<td style="text-align: center;">46.33</td>
<td style="text-align: center;">46.77</td>
<td style="text-align: center;">31.07</td>
<td style="text-align: center;">35.92</td>
<td style="text-align: center;">43.65</td>
</tr>
<tr>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">47.64</td>
<td style="text-align: center;">63.74</td>
<td style="text-align: center;">74.22</td>
<td style="text-align: center;">51.40</td>
<td style="text-align: center;">68.31</td>
<td style="text-align: center;">89.57</td>
<td style="text-align: center;">89.17</td>
<td style="text-align: center;">54.92</td>
<td style="text-align: center;">58.07</td>
<td style="text-align: center;">71.26</td>
</tr>
<tr>
<td style="text-align: center;">Creak</td>
<td style="text-align: center;">34.14</td>
<td style="text-align: center;">34.31</td>
<td style="text-align: center;">35.85</td>
<td style="text-align: center;">35.64</td>
<td style="text-align: center;">34.46</td>
<td style="text-align: center;">36.51</td>
<td style="text-align: center;">36.98</td>
<td style="text-align: center;">36.40</td>
<td style="text-align: center;">38.04</td>
<td style="text-align: center;">39.75</td>
</tr>
<tr>
<td style="text-align: center;">NoahQA</td>
<td style="text-align: center;">33.01</td>
<td style="text-align: center;">41.75</td>
<td style="text-align: center;">47.83</td>
<td style="text-align: center;">35.41</td>
<td style="text-align: center;">40.51</td>
<td style="text-align: center;">63.10</td>
<td style="text-align: center;">61.57</td>
<td style="text-align: center;">34.38</td>
<td style="text-align: center;">37.87</td>
<td style="text-align: center;">46.68</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">62.28</td>
<td style="text-align: center;">60.29</td>
<td style="text-align: center;">55.33</td>
<td style="text-align: center;">57.72</td>
<td style="text-align: center;">63.00</td>
<td style="text-align: center;">62.15</td>
<td style="text-align: center;">54.57</td>
<td style="text-align: center;">55.50</td>
<td style="text-align: center;">59.61</td>
</tr>
<tr>
<td style="text-align: center;">bAbi15</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">28.21</td>
<td style="text-align: center;">49.15</td>
<td style="text-align: center;">26.68</td>
<td style="text-align: center;">52.35</td>
<td style="text-align: center;">64.50</td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">31.35</td>
<td style="text-align: center;">45.15</td>
<td style="text-align: center;">59.35</td>
</tr>
<tr>
<td style="text-align: center;">bAbi16</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">52.97</td>
<td style="text-align: center;">62.15</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">60.20</td>
<td style="text-align: center;">61.65</td>
<td style="text-align: center;">61.10</td>
<td style="text-align: center;">56.80</td>
<td style="text-align: center;">60.50</td>
<td style="text-align: center;">66.25</td>
</tr>
<tr>
<td style="text-align: center;">ECQA</td>
<td style="text-align: center;">26.94</td>
<td style="text-align: center;">31.48</td>
<td style="text-align: center;">54.92</td>
<td style="text-align: center;">30.91</td>
<td style="text-align: center;">49.02</td>
<td style="text-align: center;">72.11</td>
<td style="text-align: center;">74.68</td>
<td style="text-align: center;">33.91</td>
<td style="text-align: center;">51.23</td>
<td style="text-align: center;">66.27</td>
</tr>
<tr>
<td style="text-align: center;">ESNLI</td>
<td style="text-align: center;">52.99</td>
<td style="text-align: center;">53.58</td>
<td style="text-align: center;">58.98</td>
<td style="text-align: center;">52.65</td>
<td style="text-align: center;">58.37</td>
<td style="text-align: center;">64.01</td>
<td style="text-align: center;">63.85</td>
<td style="text-align: center;">54.99</td>
<td style="text-align: center;">61.27</td>
<td style="text-align: center;">64.50</td>
</tr>
<tr>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;">20.52</td>
<td style="text-align: center;">23.01</td>
<td style="text-align: center;">53.07</td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">38.55</td>
<td style="text-align: center;">70.09</td>
<td style="text-align: center;">64.09</td>
<td style="text-align: center;">29.27</td>
<td style="text-align: center;">57.13</td>
<td style="text-align: center;">72.68</td>
</tr>
<tr>
<td style="text-align: center;">QED</td>
<td style="text-align: center;">23.54</td>
<td style="text-align: center;">27.21</td>
<td style="text-align: center;">45.52</td>
<td style="text-align: center;">28.98</td>
<td style="text-align: center;">37.79</td>
<td style="text-align: center;">58.67</td>
<td style="text-align: center;">58.63</td>
<td style="text-align: center;">29.70</td>
<td style="text-align: center;">44.06</td>
<td style="text-align: center;">56.20</td>
</tr>
<tr>
<td style="text-align: center;">SenMaking</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">23.42</td>
<td style="text-align: center;">34.22</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">31.89</td>
<td style="text-align: center;">59.30</td>
<td style="text-align: center;">50.12</td>
<td style="text-align: center;">26.18</td>
<td style="text-align: center;">41.17</td>
<td style="text-align: center;">49.04</td>
</tr>
</tbody>
</table>
<p>word-level attacks shows higher variance on different method types. Results in $\mathbf{A C R}$ indicator are similar. One possible reason is that word-level attacks caused larger changes in encoded vector by tokenizer[59] and word-embedding[60]. As model input, this change makes a greater impact on the model's semantic understanding.
For each attack level, we now proceed to a detailed analysis.</p>
<ul>
<li>Character-level. In this level, deletion is a more effective attack method, while insertion has less impact. ER on deletion is $10 \%$ higher than insertion, while $\mathbf{A C R}$ is $11 \%$ higher. But the difference is not significant between various methods at this level. Character-level</li>
</ul>
<p>Table 8: Different level attack influence of each dataset on ChatGPT, measured by answer changed rate(\%).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Charater Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Word Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Visual Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">repeat</td>
<td style="text-align: center;">delete</td>
<td style="text-align: center;">insert</td>
<td style="text-align: center;">insert</td>
<td style="text-align: center;">delete</td>
<td style="text-align: center;">replace</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$90 \%$</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">25.31</td>
<td style="text-align: center;">31.72</td>
<td style="text-align: center;">24.39</td>
<td style="text-align: center;">30.09</td>
<td style="text-align: center;">42.77</td>
<td style="text-align: center;">42.05</td>
<td style="text-align: center;">25.44</td>
<td style="text-align: center;">31.94</td>
<td style="text-align: center;">40.41</td>
</tr>
<tr>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">64.62</td>
<td style="text-align: center;">73.65</td>
<td style="text-align: center;">56.70</td>
<td style="text-align: center;">70.28</td>
<td style="text-align: center;">82.87</td>
<td style="text-align: center;">84.65</td>
<td style="text-align: center;">56.30</td>
<td style="text-align: center;">60.63</td>
<td style="text-align: center;">69.69</td>
</tr>
<tr>
<td style="text-align: center;">Creak</td>
<td style="text-align: center;">15.80</td>
<td style="text-align: center;">19.62</td>
<td style="text-align: center;">16.69</td>
<td style="text-align: center;">18.02</td>
<td style="text-align: center;">23.30</td>
<td style="text-align: center;">22.79</td>
<td style="text-align: center;">16.48</td>
<td style="text-align: center;">20.64</td>
<td style="text-align: center;">23.89</td>
</tr>
<tr>
<td style="text-align: center;">NoahQA</td>
<td style="text-align: center;">34.31</td>
<td style="text-align: center;">40.65</td>
<td style="text-align: center;">28.21</td>
<td style="text-align: center;">33.03</td>
<td style="text-align: center;">57.35</td>
<td style="text-align: center;">54.38</td>
<td style="text-align: center;">27.14</td>
<td style="text-align: center;">30.73</td>
<td style="text-align: center;">39.81</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">35.66</td>
<td style="text-align: center;">37.41</td>
<td style="text-align: center;">27.71</td>
<td style="text-align: center;">32.16</td>
<td style="text-align: center;">45.89</td>
<td style="text-align: center;">42.89</td>
<td style="text-align: center;">27.78</td>
<td style="text-align: center;">31.38</td>
<td style="text-align: center;">36.12</td>
</tr>
<tr>
<td style="text-align: center;">bAbi15</td>
<td style="text-align: center;">36.57</td>
<td style="text-align: center;">57.30</td>
<td style="text-align: center;">36.43</td>
<td style="text-align: center;">58.00</td>
<td style="text-align: center;">67.95</td>
<td style="text-align: center;">67.30</td>
<td style="text-align: center;">40.40</td>
<td style="text-align: center;">51.15</td>
<td style="text-align: center;">62.45</td>
</tr>
<tr>
<td style="text-align: center;">bAbi16</td>
<td style="text-align: center;">54.16</td>
<td style="text-align: center;">68.05</td>
<td style="text-align: center;">56.34</td>
<td style="text-align: center;">68.15</td>
<td style="text-align: center;">69.70</td>
<td style="text-align: center;">71.05</td>
<td style="text-align: center;">58.15</td>
<td style="text-align: center;">61.40</td>
<td style="text-align: center;">67.25</td>
</tr>
<tr>
<td style="text-align: center;">ECQA</td>
<td style="text-align: center;">16.44</td>
<td style="text-align: center;">42.13</td>
<td style="text-align: center;">16.13</td>
<td style="text-align: center;">38.47</td>
<td style="text-align: center;">63.81</td>
<td style="text-align: center;">68.69</td>
<td style="text-align: center;">20.31</td>
<td style="text-align: center;">38.58</td>
<td style="text-align: center;">56.06</td>
</tr>
<tr>
<td style="text-align: center;">ESNLI</td>
<td style="text-align: center;">31.32</td>
<td style="text-align: center;">35.87</td>
<td style="text-align: center;">31.30</td>
<td style="text-align: center;">34.92</td>
<td style="text-align: center;">38.59</td>
<td style="text-align: center;">38.77</td>
<td style="text-align: center;">32.01</td>
<td style="text-align: center;">35.91</td>
<td style="text-align: center;">38.24</td>
</tr>
<tr>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;">12.72</td>
<td style="text-align: center;">43.08</td>
<td style="text-align: center;">13.05</td>
<td style="text-align: center;">29.16</td>
<td style="text-align: center;">62.47</td>
<td style="text-align: center;">57.34</td>
<td style="text-align: center;">17.12</td>
<td style="text-align: center;">46.81</td>
<td style="text-align: center;">65.01</td>
</tr>
<tr>
<td style="text-align: center;">QED</td>
<td style="text-align: center;">16.56</td>
<td style="text-align: center;">39.00</td>
<td style="text-align: center;">16.41</td>
<td style="text-align: center;">28.19</td>
<td style="text-align: center;">53.03</td>
<td style="text-align: center;">52.25</td>
<td style="text-align: center;">20.37</td>
<td style="text-align: center;">36.86</td>
<td style="text-align: center;">51.99</td>
</tr>
<tr>
<td style="text-align: center;">SenMaking</td>
<td style="text-align: center;">19.23</td>
<td style="text-align: center;">30.25</td>
<td style="text-align: center;">18.85</td>
<td style="text-align: center;">28.77</td>
<td style="text-align: center;">55.34</td>
<td style="text-align: center;">46.29</td>
<td style="text-align: center;">22.09</td>
<td style="text-align: center;">36.07</td>
<td style="text-align: center;">44.29</td>
</tr>
</tbody>
</table>
<p>attacks are likely to be introduced by human natural errors, e.g., spelling mistakes and typo errors. More potential risks in some application scenarios are discussed in Section 5.</p>
<ul>
<li>Word-level. ER and ACR in word-level methods maintained at a relatively high level. Specifically, deletion operation has the strongest impact, with $61.8 \%$ in $\mathbf{E R}$ and $49.8 \%$ in ACR. Robustness against word-level attacks may pose potential risks in real-world applications, e.g., speech recognition. In these scenarios, input texts can usually ensure each word's completeness. However, the correctness of words among contexts cannot be always guaranteed.</li>
<li>Visual-level. The higher the proportion of transformations adopted on a single word is, the higher $\mathbf{E R}$ and $\mathbf{A C R}$ is. However, the model can still maintain around $40 \%$ accuracy. This case may be due to the rewriting of GPT Tokenizer considering visually similar characters. Visual robustness is crucial for the real-world application of LLMs in OCR and other scenarios.</li>
</ul>
<p>Also, we conduct comparative experiments on other LLMs (including LLaMA[1], OPT[61]). We choose datasets $D$ for ChatGPT with the best (SenMaking) and worst (bAbi-task 16) performance in ER as test benchmarks. Detailed model information and experimental results are listed in Table 9 and Table 10. Compared to other models, ChatGPT performs better on ER metric, but worse on ACR metric. The possible reason for the former is that other models themselves have poor baseline abilities. For the latter, we found that some models have high randomness and low confidence in their output options. We can indicate that models have not fully comprehended the intention to make multi-option.</p>
<p>Table 9: Attack influence on different LLMs, reported in error rate(\%)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">bAbi16</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SenMaking</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">samples</td>
<td style="text-align: center;">ori</td>
<td style="text-align: center;">word_replace</td>
<td style="text-align: center;">chr_delete</td>
<td style="text-align: center;">samples</td>
<td style="text-align: center;">ori</td>
<td style="text-align: center;">word_del</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT(175B*)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">55.40</td>
<td style="text-align: center;">61.10</td>
<td style="text-align: center;">62.15</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">59.30</td>
</tr>
<tr>
<td style="text-align: center;">OPT(1.3B)</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">90.00</td>
<td style="text-align: center;">89.45</td>
<td style="text-align: center;">88.87</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA(11.5B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">61.90</td>
<td style="text-align: center;">81.10</td>
<td style="text-align: center;">56.85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">56.46</td>
<td style="text-align: center;">64.77</td>
</tr>
</tbody>
</table>
<p>Table 10: Attack influence on different LLMs, reported in answer changed rate(\%)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">bAbi16</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SenMaking</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">samples</td>
<td style="text-align: center;">word_replace</td>
<td style="text-align: center;">chr_delete</td>
<td style="text-align: center;">samples</td>
<td style="text-align: center;">word_del</td>
<td style="text-align: center;">chr_insert</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT(175B*)</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">71.05</td>
<td style="text-align: center;">56.34</td>
<td style="text-align: center;">2021</td>
<td style="text-align: center;">55.34</td>
<td style="text-align: center;">18.85</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT(1.3B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">47.50</td>
<td style="text-align: center;">44.66</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA(11.5B)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">63.74</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">34.66</td>
<td style="text-align: center;">27.13</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>4.1.2 Attack Pattern Analysis</h1>
<p>From the perspective of part-of-speech (POS) and dependency relations: Following the methodology outlined in Section 3.6, we analyze the importance $c($ Formula 9) of different POS tags and dependency tags of perturbed words. As shown in Figure 2, it indicates that adversarial attacks on ChatGPT can be easier to succeed when targeting specific POS-tagged or dependency-tagged words. Specifically, the top five tags that lead to successful attacks are ['NOUN', 'ADP', 'VERB', 'DET', 'AUX'], and ['ROOT', 'nsubj', 'pobj', 'prep', 'det'] (the concept illustration of these tags is showed in the website ${ }^{8}$ ), respectively. One potential reason for this could be the fact that these tags play a crucial role in determining the meaning and structure of a sentence.</p>
<ul>
<li>Nouns, verbs, and adpositions in particular are fundamental in expressing critical concepts and relationships within a sentence, while determiners and auxiliaries impact tense and agreement.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>The 'ROOT' dependency is the center word in a sentence that connects all other words and their dependencies. Similarly, 'nsubj' and 'pobj' dependencies are related to subject and object nouns respectively. The 'prep' and 'det' dependencies are related to prepositions and determiners respectively, which are crucial for defining and differentiating between objects and concepts.
<img alt="img-1.jpeg" src="img-1.jpeg" /></li>
</ul>
<p>Figure 2: Attack pattern analysis on part-of-speech (left) and dependency relations (right). We evaluate the importance of different categories of attacked text for attack success through random forests. See text in Section 3.6 for details.</p>
<p>From the perspective of intra-sentence position and phrase structure: Following the methodology outlined in Section 3.6, we get the result as Table 11 and Figure 3 shows. It is easier to change original responses when attacks occur in words with a certain category $l$ (Formula 9). For parser structure tags, ['NP', 'VP', 'PP', 'WHNP', 'ADJP'] (concept illustration is showed in the website ${ }^{9}$ ) show the greatest impact. And for the position, the impact of attacking words in the middle is greater than attacking those at either end. Now we will conduct a detailed analysis separately.</p>
<ul>
<li>$l$ in ['Noun Phrase'(NP), 'verb phrase'(VP), 'preposition phrase'(PP)] shows a higher influence weight than any other categories on all datasets. Meanwhile, replacement at wordlevel is the most sensitive method to changes in the categories. One potential reason is that the language model generates the probability of the next word based on historical text. And nouns, verbs, and prepositional phrases can strongly affect the sentence structure. So that it will affect the prediction probability of correct words, and lead to a level of uncertainty in output.</li>
<li>The impact of attacking middle part words is significantly higher than attacking the head or tail words. The impact is evenly distributed within the middle parts and specific positions have little effect. And in some methods, attacking the end words may have a greater impact than attacking the head. Possible reasons for these situations are:</li>
</ul>
<p>1) A large number of words are involved in transitions in the middle. Since the length of the paragraph is relatively long.
2) different positions have different attention weights within the model.</p>
<p>Thus, perturbing these words may significantly impact the understanding of the original context or question. These patterns that are most susceptible to attacks can be identified. After that, future research can focus on developing more robust and safer LLMs to withstand these types of threats.</p>
<h1>4.2 Consistency Results</h1>
<p>Figure 4 present the consistency of ChatGPT across multiple runs on different $o$ and $a$ orders and prompt(Formula 1). The results are reported using box plots, and we use the standard deviation of accuracy to represent the consistency of the results.
Both ChatGPT and LLaMA are likely to experience performance inconsistency and instability when faced with different prompt. The average standard deviation under different prompts reaches a value of $\mathbf{1 1 . 9 \%}$ and $\mathbf{1 . 9 \%}$ on LLaMA and ChatGPT, respectively. Despite many efforts on prompt engineering, however, significant challenges still remain in achieving stable and reliable</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Attack pattern analysis on positions. We evaluate the importance of different positions (including head, tail, and relative positions of text) of attacked text for attack success through the frequency $s_{l}$ (Formula 9). See text in Section 3 for details.</p>
<p>Table 11: Parser Pattern Analysis</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Occurrence Frequency $s_{l}$</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Name</td>
<td>NP</td>
<td>VP</td>
<td>PP</td>
<td>WHNP</td>
<td>ADJP</td>
<td>ADVP</td>
<td>QP</td>
<td>PRT</td>
<td>WHPP</td>
<td></td>
<td>WHADJP</td>
</tr>
<tr>
<td>char_repeat</td>
<td>2846.95</td>
<td>1382.41</td>
<td>1071.72</td>
<td>89.78</td>
<td>91.68</td>
<td>71.75</td>
<td>29.76</td>
<td>22.87</td>
<td>5.79</td>
<td></td>
<td>8.47</td>
</tr>
<tr>
<td>char_delete</td>
<td>4297.89</td>
<td>1814.48</td>
<td>771.96</td>
<td>228.64</td>
<td>188.36</td>
<td>155.89</td>
<td>34.77</td>
<td>26.72</td>
<td>3.20</td>
<td></td>
<td>10.12</td>
</tr>
<tr>
<td>char_insert</td>
<td>2598.74</td>
<td>1313.87</td>
<td>950.05</td>
<td>90.10</td>
<td>87.16</td>
<td>69.68</td>
<td>20.80</td>
<td>20.82</td>
<td>2.98</td>
<td></td>
<td>5.86</td>
</tr>
<tr>
<td>word_insert</td>
<td>807.43</td>
<td>310.61</td>
<td>226.88</td>
<td>37.65</td>
<td>31.37</td>
<td>23.02</td>
<td>8.97</td>
<td>5.13</td>
<td>1.77</td>
<td></td>
<td>2.03</td>
</tr>
<tr>
<td>word_delete</td>
<td>7452.55</td>
<td>3359.09</td>
<td>2216.4</td>
<td>373.21</td>
<td>221.78</td>
<td>215.27</td>
<td>64.14</td>
<td>42.37</td>
<td>16.31</td>
<td></td>
<td>13.46</td>
</tr>
<tr>
<td>word_replace</td>
<td>8671.48</td>
<td>3537.06</td>
<td>2372.13</td>
<td>394.65</td>
<td>284.26</td>
<td>230.74</td>
<td>76.84</td>
<td>52.62</td>
<td>16.84</td>
<td></td>
<td>19.61</td>
</tr>
<tr>
<td>visual_10\%</td>
<td>3629.25</td>
<td>2026.04</td>
<td>1447.34</td>
<td>135.76</td>
<td>138.66</td>
<td>108.13</td>
<td>31.71</td>
<td>33.69</td>
<td>6.22</td>
<td></td>
<td>7.56</td>
</tr>
<tr>
<td>visual_50\%</td>
<td>4507.03</td>
<td>2483.14</td>
<td>1741.83</td>
<td>262.91</td>
<td>177.83</td>
<td>163.08</td>
<td>31.99</td>
<td>43.46</td>
<td>9.50</td>
<td></td>
<td>10.16</td>
</tr>
<tr>
<td>visual_90\%</td>
<td>5512.11</td>
<td>3088.87</td>
<td>2085.59</td>
<td>358.46</td>
<td>226.42</td>
<td>210.19</td>
<td>36.28</td>
<td>50.31</td>
<td>12.24</td>
<td></td>
<td>13.43</td>
</tr>
</tbody>
</table>
<p>LLM performance, particularly in user-oriented situations where the prompts or questions are highly varied.</p>
<p>ChatGPT represents low sensitivity to the $o$ and $a$ orders, suggesting its ability to comprehend the textual content of options proficiently rather than having a bias for specific option numbers. Specifically, the average standard deviation is only $\mathbf{1 . 1 3 \%}$ on ChatGPT. In comparison, despite having a low variance in accuracy (only $0.98 \%$ ), we found that llamas have a significant tendency to choose the first option among 6 different option orders, irrespective of the contents of the first option. This suggests that llamas may not possess the ability to perform on multiple-option tests. We infer that LLMs trained only through pre-training (like LLaMA) are inadequate in possessing this ability, and additional tuning (e.g., instruction or prompt tuning) is required to endow LLMs with this capability.</p>
<h1>4.3 Credibility Results</h1>
<p>If we set a higher probability parameter $z$ (see details in Section 3.3), the model is more likely to change the original options. Based on this conclusion from previous experiments, we hypothesize that this parameter may be related to whether the data has been trained. Therefore, we propose RTI, denoted as $\mathbf{R}_{D}$ on dataset $D$, indicator(see details in Section 3.5). Meanwhile, we conducted a sampling test on the aforementioned dataset, and the results are listed in Table 12.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The impact of prompts and option orders on accuracy. For the former, the results are obtained by testing under 5 different prompts for the multiple-option test (as listed in Table 5). While for the latter, the results are obtained by testing on 6 different option orders.</p>
<ul>
<li>Average RTI varies across different datasets. Therefore, it is possible to evaluate the difference between different datasets through RTI.</li>
<li>The more complex the $\mathbf{p}$ structure is and the more $\alpha$ (Formula 1) we input, the lower average RTI is. Normally, the more complex the sample $\mathbf{x}$ 's structure is, the more difficult it is for the model to fit $\mathbf{x}$. This theoretical result is consistent with actual results. For instance, AQuA is a very complex dataset in long-paragraph math reasoning. $\mathbf{R}_{\text {AQuA }}$ is in a low level as 0.175 , meaning that model has not fit this dataset well.</li>
<li>On the training and validation sets, average RTI is relatively higher compared to the test sets. This result is consistent with our intuition. Training set data can be effectively recognized (StrategyQA here), even if RTI rises up to even 0.45 .</li>
</ul>
<p>Thus, RTI has a certain value for assessing whether a dataset has been used in model training. It can provide a feasible quantitative reference for the reliability of the dataset used in various LLM evaluation studies.</p>
<h1>5 Discussion</h1>
<h3>5.1 LLM Application Security</h3>
<p>Under traditional application scenarios including chatbots and LLM-software integration, LLMs may also lead to a risk. For instance, Humans may make various natural errors (including typo errors, spelling mistakes, etc.), which may cause character-level interference to conditional text. Although LLM exhibits relatively high robustness under character-level adversarial attacks, there may still be significant fluctuations in the answer results, which may have a considerable impact on the user's experience and may also increase the user's checking cost.
Integrating models of other modalities into LLMs may introduce additional risks of adversarial attacks. One of the current trends in the application of Large Language Models (LLMs) is the</p>
<p>Table 12: RTI test</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RTI expectations on different methods</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Name</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">word_insert</td>
<td style="text-align: center;">word_delete</td>
<td style="text-align: center;">word_replace</td>
<td style="text-align: center;">average</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.456</td>
</tr>
<tr>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">$\underline{0.175}$</td>
</tr>
<tr>
<td style="text-align: center;">Creak</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">$\underline{\mathbf{0 . 6 6 8}}$</td>
</tr>
<tr>
<td style="text-align: center;">NoahQA</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.393</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">442</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: center;">bAbi15</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.237</td>
</tr>
<tr>
<td style="text-align: center;">bAbi16</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.201</td>
</tr>
<tr>
<td style="text-align: center;">ECQA</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.454</td>
</tr>
<tr>
<td style="text-align: center;">ESNLI</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.346</td>
</tr>
<tr>
<td style="text-align: center;">QASC</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.442</td>
</tr>
<tr>
<td style="text-align: center;">QED</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.534</td>
</tr>
<tr>
<td style="text-align: center;">SenMaking</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.461</td>
</tr>
</tbody>
</table>
<p>integration with models in other modalities, such as those based on structured data, images, speech, and other sensory input. While this integration can significantly leverage the capabilities of LLMs, it also opens up new avenues for adversarial attacks. Below, we provide some specific cases to illustrate the potential risks:</p>
<ul>
<li>For example, integrating optical character recognition (OCR) into LLMs enables machines to understand and interpret text images. However, it also creates vulnerabilities that can be exploited by visual-similarity character attacks that lead to incorrect interpretation of the text, which can have severe consequences, especially in industries such as finance or medicine, where even minor errors can cause significant harm. For instance, consider an OCR algorithm that misinterprets " 0 " (zero) for "O" (capital letter O) while processing a financial document. Such an error could lead to significant financial losses or legal implications. Similarly, consider the application in medical contexts, where errors due to OCR misinterpretation could lead to wrong prescriptions, misdiagnoses, or incorrect medical reports, endangering lives.</li>
<li>Integrating the speech recognition (equivalent to speech-to-text) system into LLM can help machines achieve more efficient human-computer interaction. The substantial of speech-to-text is to model acoustic features and classify them to achieve speech signal recognition. In this case, the model is usually able to effectively recognize complete individual words (whether correct or not) by pattern matching but may produce contextual errors, which correspond to several word-level attack forms in our experiment. From the result, we find that LLMs (e.g., ChatGPT) cannot provide enough robustness against word-level attacks, affecting their application in various fields. For instance, in automated driving vehicles equipped with intelligent voice recognition systems, if subject to model errors due to incorrect recognition, it could lead to extremely serious consequences and directly endanger the lives of passengers.
In conclusion, while integrating large language models into other systems possesses significant uncontrollability, researchers and developers must be aware of the potential harm and impact of adversarial attacks. Hence, it is essential to establish adequate safeguards mechanisms to thwart potential attacks.</li>
</ul>
<h1>5.2 LLM Evaluation Reliability</h1>
<p>Many evaluation methods based on open datasets are not reliable enough to generalize. Currently, most evaluation methods for ChatGPT measure its performance on some datasets using a certain metric, but this is not always reliable. Since ChatGPT has not publicly disclosed the training data it uses, it is difficult to determine whether the data used for evaluation has been memorized by the model during training. Additionally, we have found that ChatGPT can still complete the problem scenario and provide the correct answer in some extreme situations (such as deleting all paragraphs), indicating that these samples are likely to have been memorized by the model. There-</p>
<p>fore, using such samples to measure the performance of ChatGPT is not convincing. This problem requires more consideration by the whole community and RTI can provide some reference on relative training probability. Furthermore, we can calculate a series of RTI scores for different datasets and obtain baseline data through some statistical methods, which can be used as an absolute indicator to measure the data training situation. We hope this can be helpful to build a more reliable evaluation system.</p>
<h1>5.3 Adversarial Training Paradigm of LLM</h1>
<p>Further exploration is required to develop an adversarial training paradigm suitable for LLM. Existing research on adversarial robustness training for language models has predominantly focused on pre-training and fine-tuning stages. However, LLMs present unique challenges with their multistage training paradigm, involving the high costs of pre-training and in-context learning instead of fine-tuning, which hinders the transferability of traditional adversarial training techniques. To the best of our knowledge, no exploration has been conducted on effective adversarial training methods for LLM. Therefore, we believe that this is a promising direction for future research into LLM.</p>
<h2>6 Limitations</h2>
<p>Our evaluation work can provide a preliminary reference for LLM development and applications. Nevertheless, some limitations still exist as the following.</p>
<p>First, we evaluated based on gpt-3.5-turbo API, but the model parameters of ChatGPT have been continuously updated and iterated. ChatGPT may give different answers to the same question at different times, which may make the results not fully reproducible. However, we consider the operation relatively reasonable because, considering the enormous scale of inquiries, the local errors brought by model updates will not be so significant.</p>
<p>Second, our work mainly revolves around QA and reasoning datasets. But as we all know, ChatGPT and other LLMs can perform more basic tasks, including translation. However, the difficulty of a thorough evaluation is greatly increased by the cost growth brought by a wider range of datasets.</p>
<p>Third, as we mentioned, our input format uses a template for multiple-option, which is inspired by the instruction tuning of LLMs. However, we acknowledge that this structure may not be suitable for all other models. Additionally, using some other prompts may also improve the performance of the model.</p>
<p>Finally, We can use more datasets from more diverse fields to analyze more experimental results, to explore the reasons why ChatGPT presents corresponding performances. Additional experiments include ablation studies for different LLMs or modules, and different templates or prompts. We will complete these contents in subsequent work to make greater contributions to the evaluation community under the background of LLMs practice.</p>
<h2>References</h2>
<p>[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[2] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. arXiv preprint arXiv:2302.10198, 2023.
[3] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.
[4] Fan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736, 2023.</p>
<p>[5] Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. Chatgpt: Jack of all trades, master of none. arXiv preprint arXiv:2302.10724, 2023.
[6] Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. Exploring the limits of chatgpt for query or aspect-based text summarization, 2023.
[7] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems, 2017.
[8] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pages 50-56. IEEE, 2018.
[9] Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine translation. arXiv preprint arXiv:1711.02173, 2017.
[10] Georg Heigold, Günter Neumann, and Josef van Genabith. How robust are character-based word embeddings in tagging and mt against wrod scramlbing or randdm nouse? arXiv preprint arXiv:1704.04441, 2017.
[11] Steffen Eger, Gözde Gül Şahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. Text processing like humans do: Visually attacking and shielding nlp systems. arXiv preprint arXiv:1903.11508, 2019.
[12] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In MILCOM 2016-2016 IEEE Military Communications Conference, pages 49-54. IEEE, 2016.
[13] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998, 2018.
[14] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. arXiv preprint arXiv:1710.11342, 2017.
[15] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation with syntactically controlled paraphrase networks. arXiv preprint arXiv:1804.06059, 2018.
[16] Linyang Li, Yunfan Shao, Demin Song, Xipeng Qiu, and Xuanjing Huang. Generating adversarial examples in chinese texts using sentence-pieces. arXiv preprint arXiv:2012.14769, 2020.
[17] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. arXiv preprint arXiv:1812.05271, 2018.
[18] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. Word-level textual adversarial attacking as combinatorial optimization. arXiv preprint arXiv:1910.12196, 2019.
[19] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text classification. arXiv preprint arXiv:1712.06751, 2017.
[20] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert using bert. arXiv preprint arXiv:2004.09984, 2020.
[21] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125, 2019.
[22] Siddhant Garg and Goutham Ramakrishnan. Bae: Bert-based adversarial examples for text classification. arXiv preprint arXiv:2004.01970, 2020.</p>
<p>[23] Enes Altinisik, Hassan Sajjad, Husrev Taha Sencar, Safa Messaoud, and Sanjay Chawla. Impact of adversarial training on robustness and generalizability of language models. arXiv preprint arXiv:2211.05523, 2022.
[24] Milad Moradi and Matthias Samwald. Evaluating the robustness of neural language models to input perturbations. arXiv preprint arXiv:2108.12237, 2021.
[25] Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, and Mrinmaya Sachan. A causal framework to quantify the robustness of mathematical reasoning with language models. arXiv preprint arXiv:2210.12023, 2022.
[26] Yunxiang Zhang, Liangming Pan, Samson Tan, and Min-Yen Kan. Interpreting the robustness of neural nlp models to textual perturbations. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3993-4007, 2022.
[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[28] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.
[29] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095, 2023.
[30] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models, 2018.
[31] Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, and Farinaz Koushanfar. Adversarial reprogramming of text classification neural networks, 2019.
[32] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. Warp: Word-level adversarial reprogramming, 2021.
[33] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp, 2021.
[34] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks, 2019.
[35] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.
[36] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
[37] Yasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and Greg Durrett. Creak: A dataset for commonsense reasoning over entity knowledge. arXiv preprint arXiv:2109.01653, 2021.
[38] Qiyuan Zhang, Lei Wang, Sicheng Yu, Shuohang Wang, Yang Wang, Jing Jiang, and Ee-Peng Lim. Noahqa: Numerical reasoning with interpretable graph question answering dataset. arXiv preprint arXiv:2109.10604, 2021.
[39] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[40] Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra, Alexander Miller, Arthur Szlam, and Jason Weston. Evaluating prerequisite qualities for learning end-to-end dialog systems, 2016.</p>
<p>[41] Alexander L Gaunt, Matthew A Johnson, Maik Riechert, Daniel Tarlow, Ryota Tomioka, Dimitrios Vytiniotis, and Sam Webster. Ampnet: Asynchronous model-parallel training for dynamic neural networks. arXiv preprint arXiv:1705.09786, 2017.
[42] Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8082-8090, 2020.
[43] Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for commonsenseqa: New dataset and models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3050-3065, 2021.
[44] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31, 2018.
[45] Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. Does it make sense? and why? a pilot study for sense making and explanation. arXiv preprint arXiv:1906.00363, 2019.
[46] Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. Qed: A framework and dataset for explanations in question answering. Transactions of the Association for computational Linguistics, 9:790-806, 2021.
[47] E. Brill. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-565, 1995.
[48] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
[49] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.
[50] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840, 2021.
[51] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019.
[52] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38 (11):39-41, 1995.
[53] Allan Jabri, Armand Joulin, and Laurens van der Maaten. Revisiting visual question answering baselines. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision - ECCV 2016, pages 727-739, Cham, 2016. Springer International Publishing. ISBN 978-3-$319-46484-8$.
[54] Hoang-Quoc Nguyen-Son, Tran Phuong Thao, Seira Hidano, and Shinsaku Kiyomoto. Identifying adversarial sentences by analyzing text complexity. arXiv preprint arXiv:1912.08981, 2019.
[55] Dou Goodman, LV Zhonghou, et al. Fastwordbug: A fast method to generate adversarial text against nlp applications. arXiv preprint arXiv:2002.00760, 2020.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ https://stanfordnlp.github.io/CoreNLP/parse.html#description&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-893 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-893</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-893</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-28cbd1f2a472ba9a0330c97e7e6820b29883a69e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28cbd1f2a472ba9a0330c97e7e6820b29883a69e" target="_blank">True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments.</p>
                <p><strong>Paper Abstract:</strong> Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e893.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e893.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TWOSOME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>True knoWledge cOmeS frOM practicE (TWOSOME)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online framework that deploys an LLM (frozen LLaMA-7B) as an embodied decision-making agent by querying token-level joint probabilities over action prompts to form a behavior policy and then aligning the agent with environments via PPO using parameter-efficient LoRA updates for the actor and an MLP critic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TWOSOME (LLaMA-7B + LoRA actor + MLP critic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor: frozen LLaMA-7B decoder-only model augmented with LoRA adapters (trainable) used to score action-prompt token log-likelihoods; policy formed by softmax over action joint log-probabilities (token-level), optionally normalized by token- or word-count. Critic: 3-layer MLP attached to last transformer block, taking last token of observation prompt and outputting value estimates. Training: PPO updates LoRA parameters (actor) and MLP critic; no dropout in LoRA; sampled data used once per update. Actions are semantic action-prompts (macro-actions) mapped to environment primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Overcooked; VirtualHome</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual (symbolic) interface to embodied environments using high-level macro-actions. Overcooked: 7x7 grid kitchen, partial observability via 5x5 centered view; macro-actions (e.g., Get-Tomato, Go-Cutting-Board) that may execute multi-step primitives. VirtualHome: household simulation with rooms and object IDs, partially observable (current room only), macro-actions like Walk, Grab, SwitchOn. Both present sparse or shaped rewards and require multi-step procedures and navigation between rooms/cells.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>none (TWOSOME uses the LLM itself to score and plan; no external map, search engine, or separate tool chain is used within TWOSOME experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>LLM token log-probabilities / joint token likelihoods for action prompts (used to form action selection probabilities); textual observation prompts are inputs rather than external tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit textual belief encoded in observation prompts: the agent's 'belief' is represented by the concatenated observation prompt (which can include goal context and repeated nouns) and the LLM's conditional token probabilities over action prompts; there is no explicit separate belief-state memory, graphical belief, or probabilistic state estimator reported.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Belief is updated implicitly at each timestep by constructing a new observation prompt from the current (partially observable) raw observation and concatenating it with action prompts; the LLM recomputes token-level probabilities conditioned on the current prompt, thereby changing action rankings. The paper additionally leverages prompt-design (e.g., repeating preferred nouns in the prompt) to bias action likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy formation from LLM token scores (uses LLM likelihoods to score candidate high-level actions) combined with policy optimization (PPO) to align LLM preferences with environment returns; not explicit symbolic search or model-based internal simulator. Uses macro-actions as planning primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation is performed via high-level macro-actions (e.g., Walk-to-room, Go-Cutting-Board) whose implementations in the environment handle movement; TWOSOME selects among those macro-actions via LLM-scored probabilities rather than computing explicit paths (no A*/graph planning described).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>TWOSOME with word normalization (finetuned via PPO) achieved optimal or near-optimal performance across Overcooked and VirtualHome tasks (e.g., 100% success rates reported in Table 2 for multiple tasks); metrics are episode returns and success rates (dimensionless probabilities over trials).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>A non-finetuned LLM baseline (similar to SayCan) produced lower and more variable success rates (e.g., 0.36 ± 0.04 success on Tomato Salad vs TWOSOME word-norm 1.0 ± 0.0), demonstrating benefit from PPO finetuning; PPO (vanilla) also underperforms TWOSOME in partially observable or large-action-space tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scoring action prompts with LLM token joint probabilities yields always-valid action distributions and leverages LLM priors for exploration, but raw joint probabilities bias against longer prompts; normalizing by word-count (word normalization) corrects length bias and stabilizes training. Representing observations and goals textually (including repeating nouns in prompts) serves as an implicit belief and helps overcome partial observability by keeping target-related actions high-probability even when the object is not currently visible. Parameter-efficient PPO finetuning (LoRA actor, MLP critic) aligns LLMs to environments while preserving most language capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e893.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (Grounding language in robotic affordances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that multiplies LLM action affordance scores with a value model to select robot actions, enabling instruction-grounded planning without finetuning the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as I can, not as I say: Grounding language in robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LLM to score candidate natural-language action descriptions (affordances) and combines these scores with a learned value/feasibility function from the robot/policy to select actions; does not finetune the LLM in the original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robotic manipulation / embodied tasks (robot learning demos cited in paper); also used as a comparative baseline in the paper's household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Embodied robotic domains where actions must be grounded to robot capabilities; partially observable aspects are implicit in real-world settings but not elaborated in this paper's mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>LLM as affordance scorer (treated as an external model/tool relative to the robot's policy); value function / affordance estimator as a separate model.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual affordance likelihoods / scalar scores from LLM; scalar values from a learned value function.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Score-and-select: combine LLM-derived affordance scores with a value/feasibility function to choose actions (no explicit internal search or sequential planning algorithm described in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as a method that leverages LLM affordance scoring combined with a robot-value model to make grounded plans without finetuning the LLM; in this paper, TWOSOME's non-finetuned variant is described as similar to SayCan but TWOSOME then goes further by finetuning via PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e893.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning and Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that produces interleaved chain-of-thought style reasoning traces and actions from LLMs to improve decision-making and tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct-style LLM agents (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prompting method that elicits both reasoning traces and action outputs from an LLM, enabling the model to use intermediate reasoning and potentially interact with tools in a stepwise manner.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld and other text or simulation environments referenced in paper (e.g., TextWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based game environments (e.g., TextWorld) where agents operate based on textual observations and actions; such environments are often partially observable and require multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Potential to interact with external tools via generated action sequences or tool-invocation tokens (paper's mention highlights ReAct's chain-of-thought but does not detail specific tools).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual reasoning traces and action tokens; when used with tools, textual tool outputs would be used (not specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Chain-of-thought prompting that interleaves reasoning and actions to perform sequential decision-making; planning is implicit in generated token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of prompting that produces both reasoning and actions (ReAct) and applied to text-game style tasks; paper uses ReAct as related work on LLM-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e893.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that leverages a powerful LLM (GPT-4 in cited work) to learn and continually discover skills during embodied agent learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses a large LLM to generate plans, discover, and bootstrap skills continually in an open-ended embodied setting; relies on the LLM's inherent capabilities rather than finetuning smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Open-ended embodied simulation (e.g., Minecraft / large simulated worlds as in referenced works).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Complex open-world simulation environments requiring long-horizon planning, skill acquisition, and potentially partial observability; specifics are not given in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>LLM (GPT-4) used as core planning and skill-discovery tool; other tools not specified in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual plans and suggested skills/actions generated by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM-driven continual planning and skill discovery (open-ended), not described as explicit search-based planning in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a system that leverages very large LLMs to bootstrap skills and planning in embodied agents; used as contrast to TWOSOME which targets smaller LLMs by finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e893.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HuggingGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HuggingGPT (Solving AI tasks with ChatGPT and its friends in Huggingface)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses an LLM to coordinate and manage multiple specialized models and tools (from HuggingFace) to solve multimodal tasks by treating external models as tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HuggingGPT (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LLM as a controller to route inputs to and combine outputs from multiple external models (vision, language, etc.), effectively orchestrating external tools to solve tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multimodal AI task settings (not a text-game environment specifically in this paper's citation).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Multimodal tasks requiring combination of models (vision, language, etc.); not necessarily partially observable text environments per se in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Multiple external specialized models from HuggingFace (vision, language, tool models) invoked and composed under LLM control.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Multimodal model outputs (text, image features, classification outputs), orchestrated by LLM instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Tool orchestration via LLM controller; planning is enacted by composing tool calls and aggregating results rather than internal search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of using an LLM to manage other AI models and tools for complex tasks; cited to motivate LLMs-as-managers/tool-controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e893.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Socratic Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socratic Models: Composing zero-shot multimodal reasoning with language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework composing multiple pre-trained unimodal models (vision, audio, language) via LLM-mediated textual communication, enabling zero-shot multimodal reasoning by treating models as tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Socratic models: Composing zero-shot multimodal reasoning with language.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Socratic Models (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Compose and chain outputs of multiple pre-trained models via a language model interface so that models exchange textual outputs; effectively treats other models as external tools coordinated by an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General multimodal reasoning tasks (not specified as a partially observable text environment in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Multimodal contexts where model outputs are composed; the paper cites it as related work on composing models/tools.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Multiple pre-trained unimodal models (vision, audio, language) composed as tools.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual intermediate outputs, multimodal features, classification outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model composition / chaining mediated by LLM text; not described as explicit planning/search for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that LLMs can compose and manage other models/tools to accomplish multimodal tasks; relevant to the broader topic of tools+LLMs but not described in detail for partial-observability or navigation here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e893.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses LLM-generated internal monologues (structured textual internal state) to iteratively plan and monitor actions in embodied tasks, enabling closed-loop feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends LLM planning approaches by maintaining and using an internal textual 'monologue' (reasoning trace / plan) to close the loop between perception, planning, and action in embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Embodied planning and interactive tasks cited in related work (e.g., robot-like or simulated tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Embodied environments requiring monitoring of execution and adjustment of plans; partial observability implications are implicit but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>LLM-generated internal textual traces used as internal tool-like state; explicit external tools not described in the paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual internal monologue and action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Closed-loop LLM planning with internal monologue to iteratively adjust plans based on execution; described as related work for continuous monitoring and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of leveraging LLM internal reasoning traces to monitor and refine actions in embodied settings (closed-loop principle).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e893.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e893.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA: General robot manipulation with multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method using multimodal (language + vision) prompts to condition pre-trained policies for robot manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vima: General robot manipulation with multimodal prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VIMA (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conditions policies with multimodal textual/visual prompts to enable generalization in robot manipulation; cited as part of embodied-agent-with-LLM literature.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Robot manipulation / embodied environments (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Manipulation tasks that benefit from multimodal conditioning; partial observability is not detailed in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Multimodal pre-trained models as tools for perception and prompting (not elaborated here).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Multimodal features/textual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Prompt-conditioned policies (no explicit external planning tool described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned among related works that combine LLMs or multimodal prompts with policy learning for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as I can, not as I say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models. <em>(Rating: 2)</em></li>
                <li>HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface. <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language. <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Vima: General robot manipulation with multimodal prompts. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-893",
    "paper_id": "paper-28cbd1f2a472ba9a0330c97e7e6820b29883a69e",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "TWOSOME",
            "name_full": "True knoWledge cOmeS frOM practicE (TWOSOME)",
            "brief_description": "An online framework that deploys an LLM (frozen LLaMA-7B) as an embodied decision-making agent by querying token-level joint probabilities over action prompts to form a behavior policy and then aligning the agent with environments via PPO using parameter-efficient LoRA updates for the actor and an MLP critic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TWOSOME (LLaMA-7B + LoRA actor + MLP critic)",
            "agent_description": "Actor: frozen LLaMA-7B decoder-only model augmented with LoRA adapters (trainable) used to score action-prompt token log-likelihoods; policy formed by softmax over action joint log-probabilities (token-level), optionally normalized by token- or word-count. Critic: 3-layer MLP attached to last transformer block, taking last token of observation prompt and outputting value estimates. Training: PPO updates LoRA parameters (actor) and MLP critic; no dropout in LoRA; sampled data used once per update. Actions are semantic action-prompts (macro-actions) mapped to environment primitives.",
            "environment_name": "Overcooked; VirtualHome",
            "environment_description": "Textual (symbolic) interface to embodied environments using high-level macro-actions. Overcooked: 7x7 grid kitchen, partial observability via 5x5 centered view; macro-actions (e.g., Get-Tomato, Go-Cutting-Board) that may execute multi-step primitives. VirtualHome: household simulation with rooms and object IDs, partially observable (current room only), macro-actions like Walk, Grab, SwitchOn. Both present sparse or shaped rewards and require multi-step procedures and navigation between rooms/cells.",
            "is_partially_observable": true,
            "external_tools_used": "none (TWOSOME uses the LLM itself to score and plan; no external map, search engine, or separate tool chain is used within TWOSOME experiments).",
            "tool_output_types": "LLM token log-probabilities / joint token likelihoods for action prompts (used to form action selection probabilities); textual observation prompts are inputs rather than external tool outputs.",
            "belief_state_mechanism": "Implicit textual belief encoded in observation prompts: the agent's 'belief' is represented by the concatenated observation prompt (which can include goal context and repeated nouns) and the LLM's conditional token probabilities over action prompts; there is no explicit separate belief-state memory, graphical belief, or probabilistic state estimator reported.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Belief is updated implicitly at each timestep by constructing a new observation prompt from the current (partially observable) raw observation and concatenating it with action prompts; the LLM recomputes token-level probabilities conditioned on the current prompt, thereby changing action rankings. The paper additionally leverages prompt-design (e.g., repeating preferred nouns in the prompt) to bias action likelihoods.",
            "planning_approach": "Learned policy formation from LLM token scores (uses LLM likelihoods to score candidate high-level actions) combined with policy optimization (PPO) to align LLM preferences with environment returns; not explicit symbolic search or model-based internal simulator. Uses macro-actions as planning primitives.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Navigation is performed via high-level macro-actions (e.g., Walk-to-room, Go-Cutting-Board) whose implementations in the environment handle movement; TWOSOME selects among those macro-actions via LLM-scored probabilities rather than computing explicit paths (no A*/graph planning described).",
            "performance_with_tools": "TWOSOME with word normalization (finetuned via PPO) achieved optimal or near-optimal performance across Overcooked and VirtualHome tasks (e.g., 100% success rates reported in Table 2 for multiple tasks); metrics are episode returns and success rates (dimensionless probabilities over trials).",
            "performance_without_tools": "A non-finetuned LLM baseline (similar to SayCan) produced lower and more variable success rates (e.g., 0.36 ± 0.04 success on Tomato Salad vs TWOSOME word-norm 1.0 ± 0.0), demonstrating benefit from PPO finetuning; PPO (vanilla) also underperforms TWOSOME in partially observable or large-action-space tasks.",
            "has_tool_ablation": false,
            "key_findings": "Scoring action prompts with LLM token joint probabilities yields always-valid action distributions and leverages LLM priors for exploration, but raw joint probabilities bias against longer prompts; normalizing by word-count (word normalization) corrects length bias and stabilizes training. Representing observations and goals textually (including repeating nouns in prompts) serves as an implicit belief and helps overcome partial observability by keeping target-related actions high-probability even when the object is not currently visible. Parameter-efficient PPO finetuning (LoRA actor, MLP critic) aligns LLMs to environments while preserving most language capabilities.",
            "uuid": "e893.0",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (Grounding language in robotic affordances)",
            "brief_description": "A method that multiplies LLM action affordance scores with a value model to select robot actions, enabling instruction-grounded planning without finetuning the LLM.",
            "citation_title": "Do as I can, not as I say: Grounding language in robotic affordances.",
            "mention_or_use": "mention",
            "agent_name": "SayCan (as cited)",
            "agent_description": "Uses an LLM to score candidate natural-language action descriptions (affordances) and combines these scores with a learned value/feasibility function from the robot/policy to select actions; does not finetune the LLM in the original formulation.",
            "environment_name": "Robotic manipulation / embodied tasks (robot learning demos cited in paper); also used as a comparative baseline in the paper's household tasks.",
            "environment_description": "Embodied robotic domains where actions must be grounded to robot capabilities; partially observable aspects are implicit in real-world settings but not elaborated in this paper's mentions.",
            "is_partially_observable": null,
            "external_tools_used": "LLM as affordance scorer (treated as an external model/tool relative to the robot's policy); value function / affordance estimator as a separate model.",
            "tool_output_types": "Textual affordance likelihoods / scalar scores from LLM; scalar values from a learned value function.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Score-and-select: combine LLM-derived affordance scores with a value/feasibility function to choose actions (no explicit internal search or sequential planning algorithm described in this paper's summary).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Referenced as a method that leverages LLM affordance scoring combined with a robot-value model to make grounded plans without finetuning the LLM; in this paper, TWOSOME's non-finetuned variant is described as similar to SayCan but TWOSOME then goes further by finetuning via PPO.",
            "uuid": "e893.1",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning and Acting)",
            "brief_description": "A prompting paradigm that produces interleaved chain-of-thought style reasoning traces and actions from LLMs to improve decision-making and tool use.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "agent_name": "ReAct-style LLM agents (as cited)",
            "agent_description": "Prompting method that elicits both reasoning traces and action outputs from an LLM, enabling the model to use intermediate reasoning and potentially interact with tools in a stepwise manner.",
            "environment_name": "TextWorld and other text or simulation environments referenced in paper (e.g., TextWorld).",
            "environment_description": "Text-based game environments (e.g., TextWorld) where agents operate based on textual observations and actions; such environments are often partially observable and require multi-step reasoning.",
            "is_partially_observable": null,
            "external_tools_used": "Potential to interact with external tools via generated action sequences or tool-invocation tokens (paper's mention highlights ReAct's chain-of-thought but does not detail specific tools).",
            "tool_output_types": "Textual reasoning traces and action tokens; when used with tools, textual tool outputs would be used (not specified in this paper).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Chain-of-thought prompting that interleaves reasoning and actions to perform sequential decision-making; planning is implicit in generated token sequences.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as an example of prompting that produces both reasoning and actions (ReAct) and applied to text-game style tasks; paper uses ReAct as related work on LLM-based planning.",
            "uuid": "e893.2",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An approach that leverages a powerful LLM (GPT-4 in cited work) to learn and continually discover skills during embodied agent learning.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models.",
            "mention_or_use": "mention",
            "agent_name": "Voyager (as cited)",
            "agent_description": "Uses a large LLM to generate plans, discover, and bootstrap skills continually in an open-ended embodied setting; relies on the LLM's inherent capabilities rather than finetuning smaller models.",
            "environment_name": "Open-ended embodied simulation (e.g., Minecraft / large simulated worlds as in referenced works).",
            "environment_description": "Complex open-world simulation environments requiring long-horizon planning, skill acquisition, and potentially partial observability; specifics are not given in this paper beyond citation.",
            "is_partially_observable": null,
            "external_tools_used": "LLM (GPT-4) used as core planning and skill-discovery tool; other tools not specified in this paper's mention.",
            "tool_output_types": "Textual plans and suggested skills/actions generated by the LLM.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "LLM-driven continual planning and skill discovery (open-ended), not described as explicit search-based planning in this paper.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as a system that leverages very large LLMs to bootstrap skills and planning in embodied agents; used as contrast to TWOSOME which targets smaller LLMs by finetuning.",
            "uuid": "e893.3",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "HuggingGPT",
            "name_full": "HuggingGPT (Solving AI tasks with ChatGPT and its friends in Huggingface)",
            "brief_description": "A system that uses an LLM to coordinate and manage multiple specialized models and tools (from HuggingFace) to solve multimodal tasks by treating external models as tools.",
            "citation_title": "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface.",
            "mention_or_use": "mention",
            "agent_name": "HuggingGPT (as cited)",
            "agent_description": "Uses an LLM as a controller to route inputs to and combine outputs from multiple external models (vision, language, etc.), effectively orchestrating external tools to solve tasks.",
            "environment_name": "Multimodal AI task settings (not a text-game environment specifically in this paper's citation).",
            "environment_description": "Multimodal tasks requiring combination of models (vision, language, etc.); not necessarily partially observable text environments per se in this paper's mention.",
            "is_partially_observable": null,
            "external_tools_used": "Multiple external specialized models from HuggingFace (vision, language, tool models) invoked and composed under LLM control.",
            "tool_output_types": "Multimodal model outputs (text, image features, classification outputs), orchestrated by LLM instructions.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Tool orchestration via LLM controller; planning is enacted by composing tool calls and aggregating results rather than internal search.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned as an example of using an LLM to manage other AI models and tools for complex tasks; cited to motivate LLMs-as-managers/tool-controllers.",
            "uuid": "e893.4",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Socratic Models",
            "name_full": "Socratic Models: Composing zero-shot multimodal reasoning with language",
            "brief_description": "A framework composing multiple pre-trained unimodal models (vision, audio, language) via LLM-mediated textual communication, enabling zero-shot multimodal reasoning by treating models as tools.",
            "citation_title": "Socratic models: Composing zero-shot multimodal reasoning with language.",
            "mention_or_use": "mention",
            "agent_name": "Socratic Models (as cited)",
            "agent_description": "Compose and chain outputs of multiple pre-trained models via a language model interface so that models exchange textual outputs; effectively treats other models as external tools coordinated by an LLM.",
            "environment_name": "General multimodal reasoning tasks (not specified as a partially observable text environment in this paper).",
            "environment_description": "Multimodal contexts where model outputs are composed; the paper cites it as related work on composing models/tools.",
            "is_partially_observable": null,
            "external_tools_used": "Multiple pre-trained unimodal models (vision, audio, language) composed as tools.",
            "tool_output_types": "Textual intermediate outputs, multimodal features, classification outputs.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Model composition / chaining mediated by LLM text; not described as explicit planning/search for navigation.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as evidence that LLMs can compose and manage other models/tools to accomplish multimodal tasks; relevant to the broader topic of tools+LLMs but not described in detail for partial-observability or navigation here.",
            "uuid": "e893.5",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner Monologue: Embodied reasoning through planning with language models",
            "brief_description": "An approach that uses LLM-generated internal monologues (structured textual internal state) to iteratively plan and monitor actions in embodied tasks, enabling closed-loop feedback.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "mention_or_use": "mention",
            "agent_name": "Inner Monologue (as cited)",
            "agent_description": "Extends LLM planning approaches by maintaining and using an internal textual 'monologue' (reasoning trace / plan) to close the loop between perception, planning, and action in embodied tasks.",
            "environment_name": "Embodied planning and interactive tasks cited in related work (e.g., robot-like or simulated tasks).",
            "environment_description": "Embodied environments requiring monitoring of execution and adjustment of plans; partial observability implications are implicit but not detailed here.",
            "is_partially_observable": null,
            "external_tools_used": "LLM-generated internal textual traces used as internal tool-like state; explicit external tools not described in the paper's mention.",
            "tool_output_types": "Textual internal monologue and action sequences.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Closed-loop LLM planning with internal monologue to iteratively adjust plans based on execution; described as related work for continuous monitoring and refinement.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as an example of leveraging LLM internal reasoning traces to monitor and refine actions in embodied settings (closed-loop principle).",
            "uuid": "e893.6",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA: General robot manipulation with multimodal prompts",
            "brief_description": "A method using multimodal (language + vision) prompts to condition pre-trained policies for robot manipulation tasks.",
            "citation_title": "Vima: General robot manipulation with multimodal prompts.",
            "mention_or_use": "mention",
            "agent_name": "VIMA (as cited)",
            "agent_description": "Conditions policies with multimodal textual/visual prompts to enable generalization in robot manipulation; cited as part of embodied-agent-with-LLM literature.",
            "environment_name": "Robot manipulation / embodied environments (as cited).",
            "environment_description": "Manipulation tasks that benefit from multimodal conditioning; partial observability is not detailed in this paper's mention.",
            "is_partially_observable": null,
            "external_tools_used": "Multimodal pre-trained models as tools for perception and prompting (not elaborated here).",
            "tool_output_types": "Multimodal features/textual prompts.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Prompt-conditioned policies (no explicit external planning tool described in this paper).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned among related works that combine LLMs or multimodal prompts with policy learning for manipulation.",
            "uuid": "e893.7",
            "source_info": {
                "paper_title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances.",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models.",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models.",
            "rating": 2
        },
        {
            "paper_title": "HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface.",
            "rating": 2
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language.",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2
        },
        {
            "paper_title": "Vima: General robot manipulation with multimodal prompts.",
            "rating": 1
        }
    ],
    "cost": 0.01933525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments VIA REINFORCEMENT LEARNING</h1>
<p>Weihao Tan ${ }^{1}$, Wentao Zhang ${ }^{1}$, Shanqi Liu ${ }^{2}$, Longtao Zheng ${ }^{1}$, Xinrun Wang ${ }^{1,}$ Bo An ${ }^{1,3 *}$ ${ }^{1}$ Nanyang Technological University, Singapore ${ }^{2}$ Zhejiang University ${ }^{3}$ Skywork AI<br>{weihao001, wt.zhang, longtao001, xinrun.wang, boan}@ntu.edu.sg shanqiliu@zju.edu.cn</p>
<h4>Abstract</h4>
<p>Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decisionmaking environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>LLMs have demonstrated remarkable success in natural language generation and understanding (Brown et al., 2020; OpenAI, 2023). Recent studies show that LLMs can manage other AI models and tools to address complex multimodal tasks (Shen et al., 2023; Lu et al., 2023), assist or play sophisticated games, such as TextWorld (Yao et al., 2023), Handbi (Hu \&amp; Sadigh, 2023), and MineCraft (Wang et al., 2023a), or be deployed on robots for real-world interactions (Brohan et al., 2023; Driess et al., 2023). While LLMs can provide insightful suggestions for complex tasks, they often fail in solving simple decision-making tasks due to misalignment issues (Brohan et al., 2023).
There are two main misalignment issues leading to the failures of LLMs in decision-making
<img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Only tomato, lettuce (b) LLMs guide the agent and onion are provided in to put the lettuce on the the game. LLMs may cutting board, which choose to pick up addi- already contains tomato, tional ingredients, such as without knowing that each cucumber and pepper to cutting board can only cook the dish. contain one item at a time.</p>
<p>Figure 1: Two misalignment examples</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>tasks. i) LLMs may generate invalid actions. As the example shown in Figure 1a, LLMs may keep adding cucumber and pepper when asked to make a tomato and lettuce salad, while these ingredients are not provided. ii) LLMs may not know accurately the dynamic transitions of the environments, especially when some specific constraints are introduced in the environments. This incorrect estimation will make LLMs tend to choose actions that fit their learned common sense, resulting in the failure to solve domain-specific tasks. As the example shown in Figure 1b, LLMs may keep trying to put both tomato and lettuce on the same cutting board, without knowing that only one item can be placed on the board in this environment. Addressing these two misalignment issues requires careful alignment between LLMs and environments.</p>
<p>On the contrary, reinforcement learning (RL) learns agents' policies from scratch through trial and error in environments (Sutton \&amp; Barto, 2018), which ensures that RL agents are well aligned with environments. Most RL methods start from random policies, updated according to the return from the environments, which leads to poor sample efficiency as most policies have poor performance at the early stage of learning. One way to improve the sample efficiency is to incorporate the prior knowledge with the initialization of the policy and the exploration during training (Kumar et al., 2023). LLMs are ideal sources of prior knowledge for RL agents as LLMs are trained with enormous data from the corpus. Therefore, by leveraging RL to align LLMs with embodied environments to solve decision-making tasks, we can address the misalignment issues in LLMs and the sample efficiency issue in RL simultaneously.</p>
<p>Motivated by this idea, we propose True knoWledge cOmeS frOM practicE (TWOSOME), a general online framework that deploys LLMs as embodied agents to efficiently interact and align with environments via RL to solve decision-making tasks without requiring any prepared datasets or prior knowledge of the environments. Instead of letting LLMs directly generate actions, we use the loglikelihood scores of each token provided by LLMs to calculate the joint probabilities of each action and form valid behavior policies. This process eliminates the misalignment caused by invalid actions. Moreover, the LLM agents are optimized with proximal policy optimization (PPO) (Schulman et al., 2017) using rewards from the environments, which eliminates the misalignment caused by dynamic transitions. We observe that the formed behavior policies suffer a severe issue that longer actions tend to have lower joint probabilities, resulting in an unreasonable unbalance over the action distribution. To overcome this issue, we propose token normalization and word normalization in terms of the number of tokens and words in actions to rectify the unbalance. Furthermore, we design a novel architecture for efficient training, where both the actor and critic in RL methods share the same frozen LLaMA-7B model (Touvron et al., 2023), updated by parameter efficient finetuning methods, e.g., LoRA (Hu et al., 2022). During training, we observe that prompts for observations and actions can greatly influence the initial policies of LLM agents, therefore, we also summarize four principles for designing efficient prompts to enhance the reasoning ability of LLMs.</p>
<p>We first evaluate our methods on a classical RL decision-making environment, Overcooked, and a simulated physical environment, VirtualHome, with various tasks to show that our proposed word normalization method can remarkably improve stability and accelerate convergence during the training process. TWOSOME with word normalization exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in all tasks. Then, we test our trained TWOSOME agents in eight new unseen tasks and find that TWOSOME has superior generalization ability across unseen tasks. Finally, we evaluate our trained TWOSOME agents on traditional NLP benchmarks to demonstrate that under our framework, there is no significant loss of the LLMs' original ability during online PPO fine-tuning.</p>
<h1>2 Related Work</h1>
<p>In this section, we present a brief overview of related work. More discussions are in Appendix A.
Embodied Agents with LLMs. Recent methods use LLMs to assist planning and reasoning in robot learning (Brohan et al., 2023; Liang et al., 2022; Zeng et al., 2022) and simulation environments (Fan et al., 2022; Wang et al., 2023a; Yao et al., 2023). LLMs are also applied to help robot navigation (Parisi et al., 2022; Majumdar et al., 2020) and manipulation (Jiang et al., 2022; Ren et al., 2023; Khandelwal et al., 2022). Among them, ReAct (Yao et al., 2023) uses chain-of-thought prompting by generating both reasoning traces and action plans with LLMs. SayCan (Brohan et al., 2023) leverages the ability of LLMs to understand human instructions to make plans for completing</p>
<p>tasks without finetuning LLMs. Voyager (Wang et al., 2023a) leverages GPT-4 to learn and continually discover skills during learning. While these works exhibit promising results, they rely too heavily on the inherent capabilities of powerful LLMs, which are difficult to apply to smaller LLMs with weaker reasoning abilities. Concurrent to our work, GLAM (Carta et al., 2023) uses RL to ground LLMs. However, they focus on simple primitive actions in toy environments without rich semantics, resulting in underutilizing the capabilities of LLMs, and failing to observe the impact of prompt design and address the unbalance over action space.</p>
<p>Finetuning LLMs. Parameter-efficient finetuning (PEFT) can significantly reduce the number of parameters to tune LLMs while with minor loss of the performance (Ding et al., 2023). Prompt tuning (Lester et al., 2021) and prefix tuning (Li \&amp; Liang, 2021) prepend additional tunable tokens to the input or hidden layers, and adapter tuning (Houlsby et al., 2019) introduces the adapter to LLMs, where only the introduced tokens and adapters are finetuned. Recently, low-rank adaption (LoRA) (Hu et al., 2022) introduces low-rank matrices to approximate parameter updates. Reinforcement learning from human feedback (RLHF) is effective in finetuning LLMs (Ouyang et al., 2022), where the reward for RL is learned from human feedback. Another concurrent work (Xiang et al., 2023) leverages embodied environments to provide feedback to finetune LLMs. Different from our method, they use supervised learning to finetune LLMs with pre-collected embodied experiences by random sampling in environments instead of doing decision-making tasks from scratch.</p>
<h1>3 Preliminaries</h1>
<p>In this section, we provide a brief background on LLM and the RL problem formulation.
LLMs learn from text data using unsupervised learning. LLMs optimize the joint probabilities of variable-length symbol sequences as the product of conditional probabilities by $P(x)=$ $\prod_{i=1}^{n} P\left(s_{i} \mid s_{1}, \ldots, s_{i-1}\right)$, where $\left(s_{1}, s_{2}, \ldots, s_{n}\right)$ is variable length sequence of symbols.
LoRA is a parameter- and compute-efficient finetuning method that incorporates trainable rank decomposition matrices into each layer of an LLM. It allows indirect training of dense layers with weight matrix, $W_{0} \in \mathbb{R}^{d \times k}$ by optimizing the rank-decomposition matrices by $W_{0}+\Delta W=$ $W_{0}+B A$, where $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$, and the rank $r$ is much smaller than $d$ and $k$.
RL formulates decision-making problems as Markov Decision Processes (MDPs). An MDP is defined by the tuple $(S, A, T, R, \gamma)$, where $S$ is the state space, $A$ is the action space, $T$ is the transition dynamics, $R$ is the reward function and $\gamma$ is the discount factor. Agents select actions based on observations, aiming to maximize the expected discounted accumulative reward.
PPO is a state-of-the-art actor-critic RL method that optimizes the policy based on the accumulative reward with advantage function $A\left(s_{t}, a_{t}\right)=Q\left(s_{t}, a_{t}\right)-V\left(s_{t}\right)$, where $V\left(s_{t}\right)$ is the value function and $Q\left(s_{t}, a_{t}\right)$ is the action-value function. The objective of PPO can be expressed as:</p>
<p>$$
J(\theta)=\mathbb{E}<em _theta="\theta">{s, a}\left[\min \left(\frac{\pi</em>(s, a)\right)\right]
$$}(a \mid s)}{\pi_{\theta_{o l d}}(a \mid s)} A_{\pi_{\theta_{o l d}}}(s, a), \operatorname{clip}\left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{o l d}}(a \mid s)}, 1 \pm \epsilon\right) A_{\pi_{\theta_{o l d}}</p>
<p>where $A_{\pi_{\theta_{o l d}}}$ represents the advantage function of the policy before updating, and the clip operator controls the size of policy updates.
Aligning LLMs with Embodied Environments. We intend to deploy LLMs as interactive agents in embodied environments, where LLMs receive textual observations and generate textual actions executed in the environments. The textual observations and actions can be transferred from images or vectors by vision-language modes (VLMs) or scripts. Compared to primitive actions, high-level actions, such as macro-actions (Theocharous \&amp; Kaelbling, 2003) and learned skills (Konidaris et al., 2011) usually have richer semantics, benefiting LLMs to leverage their prior knowledge.</p>
<h2>4 TWOSOME</h2>
<p>In this section, we present TWOSOME, a general online framework to align LLMs with embodied environments via RL. We first describe how to deploy LLMs as embodied agents to generate valid actions. Secondly, we investigate the influence of lengths of action prompts on the generated policies and propose two normalization techniques, i.e., token normalization and word normalization, to al-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of how TWOSOME generates a policy using joint probabilities of actions. The color areas in the token blocks indicate the probabilities of the corresponding token in the actions.
leviate the unbalance over actions. Then, we design a parameter-efficient training method under the PPO framework. Finally, we summarize four principles for efficient prompt design for TWOSOME.</p>
<h1>4.1 VALID Policy GENERATION</h1>
<p>Actions directly generated by LLMs could be invalid in environments. This issue can be partially resolved by prompt design, i.e., adding the restrictions of actions in the prompt. However, most of the current LLMs cannot exactly follow the restrictions, especially for small and medium LLMs, i.e., less than 65B. Therefore, novel methods are required to let LLMs generate valid policies.</p>
<p>Instead of letting LLMs generate actions directly, TWOSOME queries the scores of all available actions from LLMs. These scores are used to determine the probabilities of executing the actions, which is similar to SayCan (Brohan et al., 2023). The process of the generation of policy is illustrated in Figure 2. Specifically, we associate a unique semantic prompt, e.g., pick up the tomato, to each action in the embodied environments, named action prompt, and observation prompt for the raw observation. The action prompt $a_{k} \in \mathcal{A}$ is a sequence of tokens $a_{k}=\left{w_{k}^{1}, \ldots, w_{k}^{N_{k}}\right}$ where $N_{k}$ is the length of the action prompt $k$. We note that the action prompts are not necessarily of the same length. At each step, the observation prompt $s \in \mathcal{S}$ is concatenated with each of the valid action prompts $a_{k}$, which forms the input of the LLM. We use the probabilities of tokens generated by the LLMs to compute the probability of the action prompt. Note that this probability differs from the probability of executing the action in the environment. For convenience, we call this probability token-level probability. The token-level probability of $a_{k}$ is</p>
<p>$$
P_{\text {token }}\left(a_{k} \mid s\right)=P\left(w_{k}^{1}, \ldots, w_{k}^{N_{k}} \mid s\right)=\prod_{i=1}^{N_{k}} P\left(w_{k}^{i} \mid s, w_{k}^{1}, \ldots, w_{k}^{i-1}\right)
$$</p>
<p>Normally, scores provided by LLMs are the loglikelihood of each token, namely the logits, $\log P_{\text {token }}$. We use softmax to normalize token-level probabilities over actions to get the policy:</p>
<p>$$
P\left(a_{k} \mid s\right)=\frac{\exp \left(\log P_{\text {token }}\left(a_{k} \mid s\right)\right)}{\sum_{a \in \mathcal{A}} \exp \left(\log P_{\text {token }}(a \mid s)\right)}
$$</p>
<p>There are two main advantages of this method: i) the generated policy is always valid for execution in the embodied environments and ii) leveraging the compositionability, our method can be applied to enormous actions representable by the vocabulary.</p>
<h3>4.2 ACTION PROMPT NORMALIZATION</h3>
<p>In this subsection, we will illustrate the issue in the method presented in the above section and present two normalization techniques to alleviate them.</p>
<p>Issue of Eq. (1). One key issue in Eq. (1) is that the probability of each token $P\left(w_{k}^{i} \mid\right)$ is always less than 1. Therefore, longer action prompts tend to have lower token-level probabilities, even though the longer action prompts may be more reasonable in the environment. For example, in Figure 2, pick up the tomato has more tokens than serve the dish but is the optimal action in terms of the given observation. A simple remedy for this issue is forcing all action prompts to have similar lengths, which is the case in GLAM (Carta et al., 2023) where all primitive actions have similar lengths. However, this will harm the applicability of the methods which makes the design of the</p>
<p>action prompts difficult, even impossible sometimes. Therefore, we propose two normalization techniques, token normalization and word normalization, to address this issue.</p>
<p>Token and Word Normalization. A simple idea to solve this issue is to normalize the token-level probabilities of the actions with the number of tokens, which can be defined as:</p>
<p>$$
\log P_{\text {token }}^{t n}\left(a_{k} \mid s\right)=\log P_{\text {token }}\left(a_{k} \mid s\right) / N_{k}
$$</p>
<p>We note that $\log P_{\text {token }}\left(a_{k} \mid s\right)$ will always be negative, the longer action prompts will have smaller negative values, therefore, dividing by the number of tokens will make the token-level probabilities of longer action prompts fall in the same magnitude with short action prompts. Another option is that instead of dividing the number of tokens, we can normalize the token-level probability by dividing the number of words, i.e.,</p>
<p>$$
\log P_{\text {token }}^{w n}\left(a_{k} \mid s\right)=\log P_{\text {token }}\left(a_{k} \mid s\right) / W_{k}
$$</p>
<p>where $W_{k}$ is the number of words in the action prompt $k$. For the example of pick up the tomato, $N_{k}=5$ while $W_{k}=4$.</p>
<p>Comparing the Two Normalizations. Though token normalization can eliminate the influence brought by the length of action prompts, it is slightly excessive. We observe that if a word is divided into several tokens, the first token usually has a relatively low probability, while the probabilities of the rest of the tokens tend to be remarkably high, which are often almost close to $100 \%$. For example, another important action, chop, in the Overcooked environment, is made up of two tokens, $c h$ and $o p$. The probabilities of $c h$ are usually in the range of $0-20 \%$, depending on the priority given by the LLM agent according to the observation, however, once $c h$ appears, the probability of $o p$ will boost to $90-99 \%$ in the next word since there are no other words starting with $c h$ in the observation prompts. The same phenomenon is also observed in tomao and dish as shown in Figure 2. LLMs discovered and learned this statistical law during the autoregressive training process. Thus, instead of normalizing the probabilities of actions according to the number of tokens, it is more reasonable to regard the several tokens made up of one word as an integrated symbol. Therefore, word normalization is a more suitable normalization, where the log probabilities of action get averaged according to the number of words in the action prompts.</p>
<h1>4.3 PARAMETER-EFFICIENT PPO FINETUNING</h1>
<p>In this section, we present the parameter-efficient finetuning method to align LLMs and embodied environments with the generated policies under the PPO framework. We first introduce the network design and then the training procedure.</p>
<p>Architecture. As shown in Figure 3, we add additional MLP layers to the last transformer block of LLaMA-7B model as the critic. The critic's MLPs use the last token of the observation prompt as input and output the estimated value of the observation prompt. On the other hand, the actor is formed by the frozen LLaMA-7B model with the augmentation of LoRA parameters. We also note that the dropout layers would bring additional training instabilities, because the randomness of the dropout may violate the KL divergence constraint in PPO, therefore, we do not use dropout in our LoRA modules. During training, only the MLPs for
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Parameter-efficient architecture the critic and the LoRA parameters for the actor are updated, which makes the training efficient. The LLaMA-7B model can also serve as the reference model to regularize the update of the parameters, which is not explored in this work. Though we focus on the decoder-only models, our method can be seamlessly extended to the encoder-decoder architecture, e.g., T5 (Raffel et al., 2020).</p>
<p>Training. The training procedure generally follows PPO (Schulman et al., 2017), which is demonstrated to be effective in finetuning LLMs with human feedback (Ouyang et al., 2022). We observe the training instability when updating the actor multiple times with the same data, which is also</p>
<p>observed in DeepSpeed Chat blog ${ }^{2}$. Therefore, every sampled data is discarded after training once. Given the fact that the newly added MLPs in the critic are initialized randomly, while the LoRA parameters in the actor are initialized as zero, i.e., the output of the actor is exactly the same as the LLaMA-7B model, which is reasonable, therefore, a larger learning rate for the critic and a smaller learning rate for the actor is preferred for stable training and fast convergence.</p>
<p>Inference. During inference, the critic is discarded and only the actor is needed. Furthermore, the alignment of the LLMs and embodied environments is fully encoded in the LoRA parameters, which is normally 20 times less than the LLMs, e.g., 4.2 M for LLaMA-7B. Therefore, the LoRA parameters can be a plug-and-play module of LLMs for generalizability across different environments.</p>
<h1>4.4 Prompt Design</h1>
<p>The prompts of observations and actions will significantly influence the generated policies by LLMs, which is a path orthogonal way to the finetuning for the alignment between LLMs and embodied environments. We summarize four principles for designing efficient prompts:</p>
<ul>
<li>Prompts of observations and actions should be cohesive for concatenation. Observation prompts end with you should and the next step is to, indicating the start of action prompts.</li>
<li>Articles, i.e., the, $a$, and an, are important for action prompts. Most action prompts consist of a verb and a noun, e.g., pick up the tomato. As LLMs are trained with high-quality corpus, they are sensitive to the articles. Therefore, pick up the tomato is better than pick up tomato, where the latter leads to the extremely low probability on tomato.</li>
<li>Preferred actions should appear in the observation prompt. As observed in (Xu et al., 2022; Fu et al., 2021), LLMs tend to assign higher probabilities to the repetitive tokens. Therefore, we can encourage LLMs to assign higher probability on preferred actions by emphasizing the nouns several times in the observation prompts. For example, if the observation prompt is I see a tomato. My task is to make a tomato salad. I should, then the tomato will have a relatively high probability.</li>
<li>The same action can have different action prompts under different observations. For example, in Overcooked, when the agent carries a bowl in hand, pick up the toamto can be replaced with put the tomato in the plate, where both action prompts have the same function in the environment, the latter one better fits with the context and thus has higher probability.</li>
</ul>
<p>The objective of the prompt design is to represent the observations and actions in the understandable manner of LLMs, thus improving the alignment between LLMs and embodied environments.</p>
<h2>5 EXPERIMENTS</h2>
<h3>5.1 EXPERIMENTS SETUP</h3>
<p>We deploy our methods on the LLaMA-7B model with half-precision and compare the performance among five methods: PPO (adapted from CleanRL (Huang et al., 2022a)), TWOSOME without finetuning (similar to SayCan (Brohan et al., 2023) with affordance value set to 1), TWOSOME without action prompt normalization (similar to GLAM (Carta et al., 2023) under decoder-only architecture and high-level actions) and TWOSOME with token normalization or word normalization. We mainly evaluate these methods in a typical decision-making environment, Overcooked, and a simulated physical household environment, VirtualHome.</p>
<p>Overcooked An agent is placed in the $7 \times 7$ Overcooked kitchen, aiming to make and serve a tomato salad and tomato-lettuce salad with the provided ingredients and tools in two tasks shown in Figure 4a and 4b. In the second task, we add an additional ingredient, onion as a disruptor, to show the robustness of our method. The agent needs to explore and learn the correct order to cook the dish with the provided macro-actions, such as Chop, Get-Tomato, and Go-Cutting-Board. The environment is partially observable. The agent only observes the objects within $5 \times 5$ square centered on the agent. The reward involves +0.2 for chopping a correct ingredient, +1 terminal reward for delivering the correct dish, -0.1 for delivering any wrong item, and -0.001 for every time step. The environment is adapted from Xiao et al. (2022) and Wu et al. (2021).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Experimental environments. Figure 4a and 4b show two tasks in Overcooked. Figure 4c and 4 d show two tasks in VirtualHome.</p>
<p>VirtualHome An agent is placed in a fully furnished household with various rich objects. Compared to Overcooked, this environment is more complex with a larger action space. The agent uses macro-actions to interact with the environment such as walk to the living room, turn on the TV and sit on the sofa. As shown in Figure 4c and 4d, we design two tasks. For the first task in Figure 4c, the agent needs to find the cold pancake on the table and heat it with the microwave in the kitchen. For the second task shown in Figure 4d, the agent plans to have some entertainment so it wants to find something to enjoy while watching TV. Thus, it needs to pick up chips and milk in the kitchen, bring them to the living room, turn on the TV, sit on the sofa and enjoy. The challenge arises when the agent already holding both the milk and chips, lacks an additional hand to turn on the TV. Consequently, it needs to learn to put at least one item on the nearby coffee table before operating the TV. Both tasks adopt a sparse reward setting, only when the task is finished, will the agent receive +1 reward. The environment is also partially observable. The agent can only see the objects in the current room. The environment is adapted from (Puig et al., 2018). A more detailed introduction of these two environments can be found in Appendix C. Our parameter-efficient framework enables us to complete all the experiments in one NVIDIA Tesla A100 40GB GPU. All the hyperparameters can be found in Appendix D.2. Finally, we provide the policy visualization and a detailed analysis of prompt design for all tasks in Appendix E and F.</p>
<h1>5.2 The Impact of Different Normalizations in TWOSOME</h1>
<p>Figure 5a shows the performance of finetuned TWOSOMEs. TWOSOME with word normalization succeeds in learning the optimal policy among all tasks, exhibiting great performance, and high sample efficiency over other methods, which is consistent with the analysis in Section 4.2. Except for the second Overcooked task, which is more difficult, TWOSOME without normalization learns quickly at the beginning but suffers a severe sudden drop in the the other three tasks. Without normalization, the unbalance over action distribution can accelerate the training process if the shorter action prompts happen to be reasonable actions but introduce extra training instability. TWOSOME with token normalization alleviates this instability slightly excessively, resulting in less data efficiency and slow convergence, as shown in the two overcooked tasks.</p>
<h3>5.3 TWOSOME vs. BASELINES</h3>
<p>Figure 5b shows the performance among the typical RL method, PPO, prompt tuning method, TWOSOME without finetuning, and our best method, TWOSOME with word normalization.</p>
<p>Comparison with Prompt Tuning. TWOSOME without finetuning fully relies on the capabilities of pre-trained LLMs with the designed prompts, demonstrating the LLM's decision-making ability</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison of performance among PPO and four TWOSOME variants: i) without finetuning, ii) without normalization, iii) with token normalization, and iv) with word normalization. PPO masks unrelated actions in both VirtualHome tasks. The results are averaged over 3 seeds. The non-trained method is averaged over 100 episodes.
and the difficulty of the task. For the second task, which is the most difficult task, the non-finetuned LLM agent fails to finish the task completely. For the other three tasks, the low total returns with large variations indicate that the non-finetuned LLM agent can only finish these tasks with low probabilities. Prompt tuning can improve LLMs in solving easy decision-making tasks but difficult to solve complex and long-horizontal tasks with more interferents in the environment. We conclude that all training methods including PPO and all TWOSOME variants surpass the performance of the prompt tuning method, emphasizing the necessity of aligning LLMs with environments via RL.</p>
<p>Comparison with PPO. TWOSOME with word normalization succeeds in learning the optimal policies in both Overcooked tasks using 10k and 80k sampled data, respectively. In contrast, PPO fails to learn the optimal policy and gets stuck in the suboptimal due to partial observability. For the more complex second task, PPO even needs more than 500K steps to converge. As for the two tasks in VirtualHome, we find that vanilla PPO cannot deal with the large action space and learns nothing, as shown in Appendix C.3.1. So we mask all the unrelated actions in VirtualHome for PPO. The masked PPO manages to learn the optimal policy in the task of Food Preparation but still fails in the task of Entertainment. The results demonstrate that by leveraging the power of LLM, our method can defeat the traditional RL methods in solving decision-making tasks with large action spaces.</p>
<p>All finetuned TWOSOME methods, no matter whether they are normalized, manage to overcome partial observability and achieve optimal performance in at least two seeds in the two Overcooked tasks. Even though the agent does not see the target object, e.g., tomato, it can still appear in the goal part of the observation prompt, such as your task is to make a salad consisting of tomato, maintaining the high probabilities of tomato related actions, while other unrelated objects like onion, not appeared in neither the observation nor the observation prompt, remain relatively low probabilities. This feature greatly facilitates the exploration process and helps agents to sample good actions.</p>
<h1>5.4 Open-VOCAbULARY TASK GENERALIZATION</h1>
<p>We also evaluate the generalization ability of TWOSOME in eight new unseen tasks. LLMs' openvocabulary feature enables TWOSOME to transfer learned skills to different tasks, while traditional RL agents do not have such ability. We compare the performance between our finetuned TWOSOME and non-tuned TWOSOME. Figure 6 shows that for the first four tasks, which are similar to the agent's original trained task, though non-tuned TWOSOME can somehow finish the tasks, fine-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Task Generalization Tests. TWOSOME without Finetuning and TWOSOME with word normalization are tested in eight unseen tasks. Each task is tested over 100 episodes. For the first four tasks, Cheese, Hamburger, Apple Pie and Pizza, we replace the pancake in the original Food Preparation task with the corresponding food to see whether the agent can still finish the task. The following Washing Plate and Laundry tasks are more different but still have similar procedures to Food Preparation (agent needs to put dishes or clothes into the dishwasher or washing machine). For the last two crossover tests, TWOSOME agent trained in Food Preparation is tested in Entertainment and TWOSOME trained in Entertainment is tested in Food Preparation.
tuned TWOSOME achieves perfect performance. Replacing objects has little impact on finetuned TWOSOME. For the following two more different tasks, Washing Plate and Laundry, there is an obvious drop in the success rate of non-tuned TWOSOME, while finetuned TWOSOME still manages to complete the task. For the last two crossover tasks, where agents are tested in a completely different task, TWOSOME still exhibits remarkable performance. Especially in the Entertainment task, where non-tuned TWOSOME can barely finish the task and finetuned TWOSOME still maintains a $97 \%$ success rate. Total returns of each task are provided in Appendix B.2.</p>
<h1>5.5 IMPACT OF ONLINE FinETUNING</h1>
<p>To investigate the impacts of online finetuning on the LLM's abilities, we evaluate the models trained by TWOSOME with word normalization in Virtualhome on widely used NLP benchmarks (Gao et al., 2021). The models trained in Food Preparation and Entertainment are named TWOSOME-FP and TWOSOMEE, respectively. The four benchmarks are</p>
<p>Table 1: Zero-shot performance on Language Model Evaluation Harness (Gao et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ARC_C</th>
<th style="text-align: center;">HellaSwag</th>
<th style="text-align: center;">PIQA</th>
<th style="text-align: center;">MMLU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA-7B-hf</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">TWOSOME-FP</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: left;">TWOSOME-E</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.30</td>
</tr>
</tbody>
</table>
<p>ARC_C, HellaSwag, PIQA and MMLU, which are also reported in Touvron et al. (2023). Results of the zero-shot performance are displayed in Table 1, which demonstrates that there is no significant loss of the ability of the language understanding of LLMs after the aligning with embodied environments, and even sometimes brings minor improvements. The full results are in Appendix B.3.</p>
<h2>6 DISCUSSION AND CONCLUSION</h2>
<p>In this paper, we propose TWOSOME, a general online framework for efficiently aligning LLMs with embodied environments via RL to solve decision-making tasks without requiring any prepared datasets or prior knowledge of environments and without significant loss of LLMs' original ability. Instead of letting LLMs generate actions directly, TWOSOME is more controllable and feasible with better interpretability, since the impact of tuning prompts can be clearly reflected by the change of action probabilities. TWOSOME with word normalization exhibits significantly better sample efficiency and performance compared to traditional RL methods and prompt tuning methods. Moreover, TWOSOME exhibits a remarkable generalization ability to transfer learned skills and knowledge to unseen tasks. However, our method suffers a major limitation that training a PPO agent from scratch is far faster and cheaper than finetuning an LLM. TWOSOME needs to feed all the valid actions to the LLMs for every action sampling, resulting in multiple times the amount of computation and a small batch size. We hope this work can provide a step toward the general autonomous agent, where LLMs can self-improve by interacting with the world and harvesting true knowledge from practice.</p>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>This work does not use any dataset. All the TWOSOME experimental code and environment code of Overcooked and VirtualHome are included in the Supplementary Materials. We also provide videos for each task recorded by our best agent in the Supplementary Materials. All the hyperparameters and network architecture we use can be found in Appendix D. 1 and D.2. We provide the policy visualization and a detailed analysis of prompt design for all tasks in Appendix E and F. Our parameter-efficient framework enables us to complete all the experiments in one NVIDIA Tesla A100 40GB GPU. Additional experimental results such as success rate and NLP benchmarks can be found in Appendix B. A detailed introduction to the Overcooked and VirtualHome environments, including observation space, action space, and macro actions we use can be found in Appendix C.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISGAward No: AISG2-GC-2023-009). This research is also supported by the National Research Foundation, Singapore under its Industry Alignment Fund - Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. The computational work for this work was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).</p>
<h2>REFERENCES</h2>
<p>Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive intelligence. arXiv preprint arXiv:2012.05672, 2020.</p>
<p>Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as I can, not as I say: Grounding language in robotic affordances. In Conference on Robot Learning, pp. 287-318, 2023.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pp. 1877-1901, 2020.</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. In Second Workshop on Language and Reinforcement Learning, 2022.</p>
<p>Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220-235, 2023.</p>
<p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. MineDojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.</p>
<p>Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition problem in text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. $12848-12856,2021$.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628 .</p>
<p>Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln-bert: A recurrent vision-and-language bert for navigation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1643-1653, 2021.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790-2799, 2019.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.</p>
<p>Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-AI coordination. arXiv preprint arXiv:2304.07297, 2023.</p>
<p>Shengyi Huang, Rousslan Fernand JulienDossa Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and João GM Araújo. CleanRL: High-quality single-file implementations of deep reinforcement learning algorithms. The Journal of Machine Learning Research, 23(1):1258512602, 2022a.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147, 2022b.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, 2022c.</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li FeiFei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.</p>
<p>Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. LILA: Languageinformed latent actions. In Conference on Robot Learning, pp. 1379-1390, 2021.</p>
<p>Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: CLIP embeddings for embodied AI. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 14809-14818, 2022.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.</p>
<p>George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. Autonomous skill acquisition on a mobile manipulator. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1468-1473, 2011.</p>
<p>Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline Qlearning on diverse multi-task data both scales and generalizes. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-making. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, 2021.
J. Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. ArXiv, abs/2209.07753, 2022.</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023.</p>
<p>Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI 16, pp. 259-274. Springer, 2020.</p>
<p>OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Kumar Gupta. The unsurprising effectiveness of pre-trained vision models for control. In International Conference on Machine Learning, 2022.</p>
<p>Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8494-8502, 2018.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.</p>
<p>Allen Z Ren, Bharat Govil, Tsung-Yen Yang, Karthik R Narasimhan, and Anirudha Majumdar. Leveraging language for accelerated learning of tool manipulation. In Conference on Robot Learning, pp. 1531-1541, 2023.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with ChatGPT and its friends in Huggingface. arXiv preprint arXiv:2303.17580, 2023.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: An autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Georgios Theocharous and Leslie Kaelbling. Approximate planning in pomdps with macro-actions. Advances in neural information processing systems, 16, 2003.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023b.</p>
<p>Sarah A Wu, Rose E Wang, James A Evans, Joshua B Tenenbaum, David C Parkes, and Max Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent collaboration. Topics in Cognitive Science, 13(2):414-432, 2021.</p>
<p>Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. arXiv preprint arXiv:2305.10626, 2023.</p>
<p>Yuchen Xiao, Weihao Tan, and Christopher Amato. Asynchronous actor-critic for multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. arXiv preprint arXiv:2206.02369, 2022.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Andy Zeng, Adrian S. Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter R. Florence. Socratic models: Composing zero-shot multimodal reasoning with language. ArXiv, abs/2204.00598, 2022.</p>
<p>Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. 2023.</p>
<h1>APPENDIX</h1>
<h2>A MORE Related Work</h2>
<p>Embodied Agent with LLMs. The successful integration of language as a semantically rich input for interactive decision-making highlights the crucial role of LLMs in facilitating interaction and decision-making processes (Abramson et al., 2020; Karamcheti et al., 2021; Li et al., 2022). LLMs are also applied in various environments to aid robot navigation (Parisi et al., 2022; Hong et al., 2021; Majumdar et al., 2020) and manipulation (Jiang et al., 2022; Ren et al., 2023; Khandelwal et al., 2022). Recently, there have been a large number of methods that utilize LLMs to enhance planning and reasoning capabilities in embodied agents. SayCan (Brohan et al., 2023) assesses the affordance of candidate actions by multiplying their probabilities under LLMs with a value function. Zeng et al. (2022) combine the LLM with a visual-language model and a pre-trained language-conditioned policy (Shridhar et al., 2022) to enable open vocabulary robotic tasks. Huang et al. (2022b) demonstrate that LLMs can be employed for planning and executing simple household tasks. They ground LLM-generated actions by comparing their embeddings with a predefined list of acceptable actions. To incorporate environment feedback, Inner Monologue (Huang et al., 2022c) extends SayCan using a closed-loop principle. This principle is also applied in related works such as (Yao et al., 2023; Huang et al., 2022c; Kim et al., 2023; Singh et al., 2022; Liang et al., 2022; Shinn et al., 2023; Wang et al., 2023b;a) to continuously monitor agent behaviors and refine and adjust plans accordingly for tasks such as computer automation, Minecraft, etc. Furthermore, there are approaches that prompt LLMs to generate temporal-abstracted actions (Zheng et al., 2023). Dasgupta et al. (2022) employ the LLM as a planner and success detector for an agent with their actor module necessitates pre-training with RL to enable the agent to follow natural language instructions. While these works demonstrate impressive results, they rely too heavily on the inherent capabilities of powerful LLMs, like GPT4 and PaLM (Chowdhery et al., 2022), which are difficult to apply to smaller LLMs with weaker reasoning abilities, like LLaMA-7B.</p>
<p>Concurrent to our work, GLAM (Carta et al., 2023) utilizes RL finetuning to achieve functional grounding of LLMs. However, they focus on simple primitive actions (turn left, turn right, go forward, etc.) evaluated in toy environments, BabyAI (Chevalier-Boisvert et al., 2018) with a much smaller encoder-decoder LLM, Flan-T5-780M. These primitive actions have a similar number of tokens and less meaningful semantics, resulting in underutilizing the capabilities of LLMs, and failing to observe the impact of prompt design and address the unbalance over action space, resulting in additional instability and poor robustness.</p>
<p>Finetuning LLMs. Parameter-efficient finetuning (PEFT) can significantly reduce the number of parameters to tune LLMs while with minor loss of the performance (Ding et al., 2023). Prompt tuning (Lester et al., 2021) and prefix tuning (Li \&amp; Liang, 2021) prepend additional tunable tokens to the input or hidden layers, and adapter tuning (Houlsby et al., 2019) introduces the adapter, i.e., the bottleneck layers, to LLMs, where only the introduced tokens and adapters are finetuned. Recently, low-rank adaption (LoRA) (Hu et al., 2022) introduces low-rank matrices to approximate parameter updates.</p>
<p>Reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) proves that RL is an effective way to make alignments with human preferences and proposes a novel perspective regarding LLMs as actors under actor-critic framework. Our work is also motivated by this method. While RLHF needs to train a reward model to simulate human preferences, these signals are naturally present in most RL environments, which makes it possible to align LLMs with certain environments.</p>
<p>Another concurrent work (Xiang et al., 2023) leverages VirtualHome to collect embodied experiences by random sampling and use supervised learning to finetune LLMs to these embodied knowledge. They finally apply a simple RL method, MCTS, to plan without training, instead of directly generating plans. It is worth pointing out that our method also applies their supervised learning process to learn the embodied knowledge and get more familiar with the environments. Our online PPO sampling is much more efficient and feasible than their random sampling, e.g. in the overcooked tasks, it is almost impossible to complete the task by random sampling. In addition, an online learning PPO is obviously better than a simple MCTS without training. Compared with this work, our method is a simpler end-to-end online framework, which can automatically acquire new knowledge and solve decision-making tasks by interacting with environments without any prepared datasets.</p>
<h1>B ADDITIONAL EXPERIMENTAL RESULTS</h1>
<h2>B. 1 SUCCESS Rate in OVERCOOKED and VIRTUALHOME</h2>
<p>Besides the total reward provided in 5b, here we also provide the final success rates of all methods in Table 2, which tells almost the same story. Our TWOSOME with word normalization method achieves a $100 \%$ success rate across all tasks, exhibiting great stability and remarkable performance.</p>
<p>Table 2: Final success Rate of different methods in Overcooked and VirtualHome. Results are averaged over three seeds. Each seed is run for 100 episodes. Norm is short for normalization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">PPO</th>
<th style="text-align: center;">TWOSOME</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">W/O Finetuning</td>
<td style="text-align: center;">W/O Norm</td>
<td style="text-align: center;">Token Norm</td>
<td style="text-align: center;">Word Norm</td>
</tr>
<tr>
<td style="text-align: center;">Tomato Salad</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.67 \pm 0.47$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: center;">Tomato-lettuce Salad</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$0 \pm 0$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: center;">Food Preparation</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$0.81 \pm 0.01$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
<tr>
<td style="text-align: center;">Entertainment</td>
<td style="text-align: center;">$0.33 \pm 0.47$</td>
<td style="text-align: center;">$0.45 \pm 0.05$</td>
<td style="text-align: center;">$0.67 \pm 0.47$</td>
<td style="text-align: center;">$1 \pm 0$</td>
<td style="text-align: center;">$\mathbf{1} \pm \mathbf{0}$</td>
</tr>
</tbody>
</table>
<h2>B. 2 Performance of TWOSOME and SayCan in Eight Unseen tasks</h2>
<p>Table 3 and 4 show the final success rate and total return of our best method TWOSOME with word normalization and our baseline, SayCan(non-finetuned TWSOME) in eight unseen tasks. Each task is run for 100 episodes. TWOSOME exhibits superior generalization across all tasks.</p>
<p>Table 3: Final success rate of TWOSOME and SayCan in eight unseen tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Cheese</th>
<th style="text-align: center;">Hamburger</th>
<th style="text-align: center;">Applepie</th>
<th style="text-align: center;">Pizza</th>
<th style="text-align: center;">Washing Plate</th>
<th style="text-align: center;">Laundry</th>
<th style="text-align: center;">Food Preparation</th>
<th style="text-align: center;">Entertainment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TWOSOME</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.04</td>
</tr>
</tbody>
</table>
<p>Table 4: Total return of TWOSOME and SayCan in eight unseen tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Cheese</th>
<th style="text-align: center;">Hamburger</th>
<th style="text-align: center;">Applepie</th>
<th style="text-align: center;">Pizza</th>
<th style="text-align: center;">Washing Plate</th>
<th style="text-align: center;">Laundry</th>
<th style="text-align: center;">Food Preparation</th>
<th style="text-align: center;">Entertainment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TWOSOME</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.01$</td>
<td style="text-align: center;">$0.77 \pm 0.02$</td>
<td style="text-align: center;">$0.76 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.24$</td>
<td style="text-align: center;">$0.43 \pm 0.14$</td>
</tr>
<tr>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">$0.18 \pm 0.17$</td>
<td style="text-align: center;">$0.21 \pm 0.17$</td>
<td style="text-align: center;">$0.23 \pm 0.17$</td>
<td style="text-align: center;">$0.17 \pm 0.18$</td>
<td style="text-align: center;">$0.18 \pm 0.17$</td>
<td style="text-align: center;">$0.08 \pm 0.13$</td>
<td style="text-align: center;">$0.20 \pm 0.16$</td>
<td style="text-align: center;">$0.01 \pm 005$</td>
</tr>
</tbody>
</table>
<h2>B. 3 Evaluation on NLP Benchmarks</h2>
<p>To investigate the impacts of the PPO online finetuning on the LLM's abilities, we evaluate the models trained by TWOSOME with word normalization in the virtual home tasks on widely used NLP benchmasrks (Gao et al., 2021). The models trained in Food Preparation and Entertainment are named TWOSOME-FP and TWOSOME-E, respectively. The four benchmarks are ARC_C, HellaSwag, PIQA and MMLU, which are also reported in (Touvron et al., 2023). The results on ARC_Challenge, HellaSwag and PIQA are displayed in Table 5 and the results of MMLU are displayed in Table 6. All results are calculated following the default configurations in (Gao et al., 2021).</p>
<p>Table 5: Zero-shot performance on Common Sense Reasoning tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA</th>
<th style="text-align: center;">TWOSOME-FP</th>
<th style="text-align: center;">TWOSOME-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ARC_C</td>
<td style="text-align: center;">$0.42 \pm 0.01$</td>
<td style="text-align: center;">$0.43 \pm 0.01$</td>
<td style="text-align: center;">$0.43 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">$0.57 \pm 0.00$</td>
<td style="text-align: center;">$0.58 \pm 0.00$</td>
<td style="text-align: center;">$0.58 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">$0.79 \pm 0.01$</td>
<td style="text-align: center;">$0.79 \pm 0.01$</td>
<td style="text-align: center;">$0.79 \pm 0.01$</td>
</tr>
</tbody>
</table>
<p>Table 6: Zero-shot performance on Massive Multitask Language Understanding (MMLU).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLaMA</th>
<th style="text-align: center;">TWOSOME-FP</th>
<th style="text-align: center;">TWOSOME-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">abstract_algebra</td>
<td style="text-align: center;">$0.33 \pm 0.05$</td>
<td style="text-align: center;">$0.30 \pm 0.05$</td>
<td style="text-align: center;">$0.28 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">anatomy</td>
<td style="text-align: center;">$0.38 \pm 0.04$</td>
<td style="text-align: center;">$0.39 \pm 0.04$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">astronomy</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">business_ethics</td>
<td style="text-align: center;">$0.37 \pm 0.05$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
<td style="text-align: center;">$0.32 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">clinical_knowledge</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
<td style="text-align: center;">$0.35 \pm 0.03$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">college_biology</td>
<td style="text-align: center;">$0.37 \pm 0.04$</td>
<td style="text-align: center;">$0.35 \pm 0.04$</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">college_chemistry</td>
<td style="text-align: center;">$0.28 \pm 0.05$</td>
<td style="text-align: center;">$0.30 \pm 0.05$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">college_computer_science</td>
<td style="text-align: center;">$0.25 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">college_mathematics</td>
<td style="text-align: center;">$0.34 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">college_medicine</td>
<td style="text-align: center;">$0.31 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">college_physics</td>
<td style="text-align: center;">$0.25 \pm 0.04$</td>
<td style="text-align: center;">$0.25 \pm 0.04$</td>
<td style="text-align: center;">$0.20 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">computer_security</td>
<td style="text-align: center;">$0.38 \pm 0.05$</td>
<td style="text-align: center;">$0.40 \pm 0.05$</td>
<td style="text-align: center;">$0.40 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">conceptual_physics</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">econometrics</td>
<td style="text-align: center;">$0.23 \pm 0.04$</td>
<td style="text-align: center;">$0.20 \pm 0.04$</td>
<td style="text-align: center;">$0.21 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">electrical_engineering</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">elementary_mathematics</td>
<td style="text-align: center;">$0.24 \pm 0.02$</td>
<td style="text-align: center;">$0.23 \pm 0.02$</td>
<td style="text-align: center;">$0.23 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">formal_logic</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">global_facts</td>
<td style="text-align: center;">$0.32 \pm 0.05$</td>
<td style="text-align: center;">$0.29 \pm 0.05$</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_biology</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_chemistry</td>
<td style="text-align: center;">$0.21 \pm 0.03$</td>
<td style="text-align: center;">$0.21 \pm 0.03$</td>
<td style="text-align: center;">$0.19 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_computer_science</td>
<td style="text-align: center;">$0.24 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
<td style="text-align: center;">$0.28 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_european_history</td>
<td style="text-align: center;">$0.40 \pm 0.04$</td>
<td style="text-align: center;">$0.38 \pm 0.04$</td>
<td style="text-align: center;">$0.41 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_geography</td>
<td style="text-align: center;">$0.28 \pm 0.03$</td>
<td style="text-align: center;">$0.29 \pm 0.03$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_government_and_politics</td>
<td style="text-align: center;">$0.30 \pm 0.03$</td>
<td style="text-align: center;">$0.30 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_macroeconomics</td>
<td style="text-align: center;">$0.28 \pm 0.02$</td>
<td style="text-align: center;">$0.26 \pm 0.02$</td>
<td style="text-align: center;">$0.24 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_mathematics</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
<td style="text-align: center;">$0.25 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_microeconomics</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_physics</td>
<td style="text-align: center;">$0.26 \pm 0.04$</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
<td style="text-align: center;">$0.28 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_psychology</td>
<td style="text-align: center;">$0.33 \pm 0.02$</td>
<td style="text-align: center;">$0.32 \pm 0.02$</td>
<td style="text-align: center;">$0.27 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_statistics</td>
<td style="text-align: center;">$0.22 \pm 0.03$</td>
<td style="text-align: center;">$0.19 \pm 0.03$</td>
<td style="text-align: center;">$0.17 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_us_history</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">high_school_world_history</td>
<td style="text-align: center;">$0.38 \pm 0.03$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
<td style="text-align: center;">$0.42 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">human_aging</td>
<td style="text-align: center;">$0.43 \pm 0.03$</td>
<td style="text-align: center;">$0.40 \pm 0.03$</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">human_sexuality</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
<td style="text-align: center;">$0.34 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">international_law</td>
<td style="text-align: center;">$0.47 \pm 0.05$</td>
<td style="text-align: center;">$0.43 \pm 0.05$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">jurisprudence</td>
<td style="text-align: center;">$0.36 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">logical_fallacies</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.35 \pm 0.04$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">machine_learning</td>
<td style="text-align: center;">$0.27 \pm 0.04$</td>
<td style="text-align: center;">$0.28 \pm 0.04$</td>
<td style="text-align: center;">$0.33 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">management</td>
<td style="text-align: center;">$0.28 \pm 0.04$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
<td style="text-align: center;">$0.22 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">marketing</td>
<td style="text-align: center;">$0.38 \pm 0.03$</td>
<td style="text-align: center;">$0.38 \pm 0.03$</td>
<td style="text-align: center;">$0.39 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">medical_genetics</td>
<td style="text-align: center;">$0.42 \pm 0.05$</td>
<td style="text-align: center;">$0.38 \pm 0.05$</td>
<td style="text-align: center;">$0.36 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">miscellaneous</td>
<td style="text-align: center;">$0.45 \pm 0.02$</td>
<td style="text-align: center;">$0.45 \pm 0.02$</td>
<td style="text-align: center;">$0.42 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">moral_disputes</td>
<td style="text-align: center;">$0.35 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.29 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">moral_scenarios</td>
<td style="text-align: center;">$0.24 \pm 0.01$</td>
<td style="text-align: center;">$0.24 \pm 0.01$</td>
<td style="text-align: center;">$0.24 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">nutrition</td>
<td style="text-align: center;">$0.33 \pm 0.03$</td>
<td style="text-align: center;">$0.33 \pm 0.03$</td>
<td style="text-align: center;">$0.30 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">philosophy</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">prehistory</td>
<td style="text-align: center;">$0.39 \pm 0.03$</td>
<td style="text-align: center;">$0.34 \pm 0.03$</td>
<td style="text-align: center;">$0.31 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">professional_accounting</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
<td style="text-align: center;">$0.28 \pm 0.03$</td>
<td style="text-align: center;">$0.26 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">professional_law</td>
<td style="text-align: center;">$0.28 \pm 0.01$</td>
<td style="text-align: center;">$0.28 \pm 0.01$</td>
<td style="text-align: center;">$0.29 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: center;">professional_medicine</td>
<td style="text-align: center;">$0.25 \pm 0.03$</td>
<td style="text-align: center;">$0.24 \pm 0.03$</td>
<td style="text-align: center;">$0.25 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">professional_psychology</td>
<td style="text-align: center;">$0.34 \pm 0.02$</td>
<td style="text-align: center;">$0.31 \pm 0.02$</td>
<td style="text-align: center;">$0.30 \pm 0.02$</td>
</tr>
<tr>
<td style="text-align: center;">public_relations</td>
<td style="text-align: center;">$0.35 \pm 0.05$</td>
<td style="text-align: center;">$0.33 \pm 0.04$</td>
<td style="text-align: center;">$0.32 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">security_studies</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
<td style="text-align: center;">$0.27 \pm 0.03$</td>
<td style="text-align: center;">$0.23 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">sociology</td>
<td style="text-align: center;">$0.41 \pm 0.03$</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
<td style="text-align: center;">$0.37 \pm 0.03$</td>
</tr>
<tr>
<td style="text-align: center;">us_foreign_policy</td>
<td style="text-align: center;">$0.42 \pm 0.05$</td>
<td style="text-align: center;">$0.41 \pm 0.05$</td>
<td style="text-align: center;">$0.41 \pm 0.05$</td>
</tr>
<tr>
<td style="text-align: center;">virology</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.36 \pm 0.04$</td>
<td style="text-align: center;">$0.33 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;">world_religions</td>
<td style="text-align: center;">$0.46 \pm 0.04$</td>
<td style="text-align: center;">$0.48 \pm 0.04$</td>
<td style="text-align: center;">$0.44 \pm 0.04$</td>
</tr>
</tbody>
</table>
<h1>C Details of Embodied Environments</h1>
<p>In this section, we present the details of the embodied environments Overcooked and VirtualHome.</p>
<h2>C. 1 OVERCOOKED</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Overcooked environments.</p>
<p>Goal. An agent is placed in the Overcooked kitchen and aims to cook a certain dish with the provided ingredients and tools and deliver it to the 'star' cell as soon as possible. Agents have to learn the correct procedure in terms of picking up raw vegetables, chopping them, and merging them in a bowl before delivering. Figure 7a and 7b show the recipes for making tomato salad and tomato-lettuce salad.</p>
<p>Observation Space. The environment is a $7 \times 7$ grid world involving ingredients, bowls, cutting boards, and a delivery counter. For the task of making tomato salad, there is only one tomato, one bowl, one cutting board, and one delivery counter available in the environment. For the task of making a tomato-lettuce salad, one tomato, one lettuce, one onion, one bowl, two cutting boards, and one delivery cell are provided in the environment. The onion serves as an interferent, though it does not appear in the recipe. The global state information consists of the positions of the agent and the above objects, and the status of each ingredient: chopped or unchopped. The environment is partially observable. For the observation, the agent only observes the positions and status of the entities within a $5 \times 5$ square centered on the agent. Other unseen objects are masked in the observation. And the initial positions of all the objects are known to agents. The symbolic raw observations are directly fed into PPO as input and converted to prompts with scripts to serve as LLMs' input.</p>
<p>Primitive-Action Space. Agents use five primitive actions: up, down, left, right, and stay to interact with the environment. Agents can move around and achieve picking, placing, chopping, and delivering by standing next to the corresponding cell and moving against it. All the macro-actions are based on these primitive actions.</p>
<p>Macro-Action Space. In this work, we mainly focus on using high-level macro-actions, since they usually have richer semantics. Every macro-action may take several time steps to complete. The main function of each macro-action and the corresponding termination conditions is exactly the same as the setting in (Xiao et al., 2022) despite that we only have one agent. Here we list and briefly describe all the macro-actions we use: Chop, chops a raw ingredient into pieces when the agent stands next to a cutting board with an unchopped ingredient on it. Get-Tomato, Get-Lettuce, Get-Onion, Get-Bowl, Go-Cutting-Board-1/2 and Deliver, which navigate the agent to the location of the corresponding object and execute the corresponding action.</p>
<p>In the first task of making tomato salad, Get-Tomato, Get-Bowl, Go-Cutting-Board-1 and Deliver are available. In the second task of making tomato-lettuce salad all the macro-actions are valid.</p>
<p>Dynamics: The transition in this task is deterministic. If an agent delivers any wrong item, the item will be reset to its initial position.</p>
<p>Reward: +0.2 for chopping a correct ingredient, +1 terminal reward for delivering the correct dish, -0.1 for delivering any wrong dish, and -0.001 for every timestep.</p>
<p>Episode Termination: Each episode terminates either when the agent successfully delivers the target dish to the delivery counter or reaches the maximal time steps, 200.</p>
<h1>C. 2 VirtualHome</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: VirtualHome environments.</p>
<p>Goal. An agent, represented as a humanoid avatar, is placed in a fully furnished household with various rich objects to interact with. Figure 8a and 8b show two tasks we design for the agent to complete. For the first task of food preparation, the agent needs to find the pancake on the table in the kitchen and put it in the microwave to heat. For the second task of entertainment, the agent needs to find and bring chips and milk, from the kitchen to the living room and succeed sitting on the sofa with the TV open and the milk and chips are nearby. The challenge arises when the agent, already holding both the milk and chips, lacks an additional hand to turn on the TV. Consequently, it needs to learn to put at least one item on the nearby coffee table before operating the TV.
Observation Space. The environment is also partially observable. The agent can only see the objects in the current room and does not the objects in the other room. The observation consists of a set of bool values, representing whether the agent sees the relative object, whether these objects are close to the agent and the status of the objects, such as whether the TV is turned on and whether the milk is on the coffee table. The symbolic raw observations are directly fed into PPO as input and converted to prompts with scripts to serve as LLMs' input.</p>
<h2>C. 3 Action SPace</h2>
<p>To simulate the daily activities, all the actions in VirtualHome are macro-actions. Every macroaction takes only time step for execution. Here, we list and briefly describe all the macro-actions as Table 7 .</p>
<p>Table 7: Actions in VirtualHome and their corresponding descriptions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[STANDUP]</td>
<td style="text-align: center;">Stand up</td>
</tr>
<tr>
<td style="text-align: center;">[SIT] <Object> <Object> (object_id)</td>
<td style="text-align: center;">Sit on the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <Object> (object_id)</td>
<td style="text-align: center;">Walk to the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <Object> (object_id)</td>
<td style="text-align: center;">Grab the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[OPEN] <Object> (object_id)</td>
<td style="text-align: center;">Open the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[CLOSE] <Object> (object_id)</td>
<td style="text-align: center;">Close the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHON] <Object> (object_id)</td>
<td style="text-align: center;">Switch/Turn on the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHOFF] <Object> (object_id)</td>
<td style="text-align: center;">Switch/Turn off the Object(object_id)</td>
</tr>
<tr>
<td style="text-align: center;">[PUTIN] <Object1> (object1_id) <Object2> (object2_id)</td>
<td style="text-align: center;">Put the Object1(object1_id) in the Object2(object2_id)</td>
</tr>
<tr>
<td style="text-align: center;">[PUTBACK] <Object1> (object1_id) <Object2> (object2_id)</td>
<td style="text-align: center;">Put the Object1(object1_id) on the Object2(object2_id)</td>
</tr>
</tbody>
</table>
<p>For the first task of heating pancake, there are four rooms and two items in the environment and their corresponding object id are as follows: livingroom(267), kitchen(11), bathroom(172), bedroom(210), pancake(62), microwave(109). For the second task of watching TV, there are four rooms and five items in the environment and their corresponding object id are as follows: livingroom(267), kitchen(11), bathroom(172), bedroom(210), chips(61), milk(46), TV(297), sofa(276), coffeetable(268). For their actions, corresponding action prompts and abbreviated labels, please refer to Table 8 and Table 9.</p>
<p>Only when the agent is close to the object, can the agent operate the object. For example, the agent needs to walk to the TV before turning it on.</p>
<p>Table 8: Actions, action prompts and abbreviated labels in the heating pancake task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Action Prompt</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[WALK] <livingroom> (267)</td>
<td style="text-align: center;">walk to the living room</td>
<td style="text-align: center;">Walk-Livingroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <kitchen> (11)</td>
<td style="text-align: center;">walk to the kitchen</td>
<td style="text-align: center;">Walk-Kitchen</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bathroom> (172)</td>
<td style="text-align: center;">walk to the bathroom</td>
<td style="text-align: center;">Walk-Bathroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bedroom> (210)</td>
<td style="text-align: center;">walk to the bedroom</td>
<td style="text-align: center;">Walk-Bedroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <pancake> (62)</td>
<td style="text-align: center;">reach for the pancake</td>
<td style="text-align: center;">Walk-Pancake</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <microwave> (109)</td>
<td style="text-align: center;">move to the microwave</td>
<td style="text-align: center;">Walk-Microwave</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <pancake> (62)</td>
<td style="text-align: center;">grab the pancake</td>
<td style="text-align: center;">Grab-Pancake</td>
</tr>
<tr>
<td style="text-align: center;">[OPEN] <microwave> (109)</td>
<td style="text-align: center;">open the microwave</td>
<td style="text-align: center;">Open-Microwave</td>
</tr>
<tr>
<td style="text-align: center;">[CLOSE] <microwave> (109)</td>
<td style="text-align: center;">close the microwave</td>
<td style="text-align: center;">Close-Microwave</td>
</tr>
<tr>
<td style="text-align: center;">[PUTIN] <pancake> (62) <microwave> (109)</td>
<td style="text-align: center;">put the pancake in the microwave</td>
<td style="text-align: center;">Putin-Pancake-Microwave</td>
</tr>
</tbody>
</table>
<p>Table 9: Actions, action prompts and abbreviated labels in the watching TV task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Action Prompt</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">[WALK] <livingroom> (267)</td>
<td style="text-align: center;">walk to the living room</td>
<td style="text-align: center;">Walk-Livingroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <kitchen> (11)</td>
<td style="text-align: center;">walk to the kitchen</td>
<td style="text-align: center;">Walk-Kitchen</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bathroom> (172)</td>
<td style="text-align: center;">walk to the bathroom</td>
<td style="text-align: center;">Walk-Bathroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <bedroom> (210)</td>
<td style="text-align: center;">walk to the bedroom</td>
<td style="text-align: center;">Walk-Bedroom</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <chips> (61)</td>
<td style="text-align: center;">reach for the chips</td>
<td style="text-align: center;">Walk-Chips</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <milk> (46)</td>
<td style="text-align: center;">reach for the milk</td>
<td style="text-align: center;">Walk-Milk</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <coffeetable> (268)</td>
<td style="text-align: center;">move to the coffee table</td>
<td style="text-align: center;">Walk-Coffeetable</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <TV> (297)</td>
<td style="text-align: center;">move to the TV</td>
<td style="text-align: center;">Walk-TV</td>
</tr>
<tr>
<td style="text-align: center;">[WALK] <sofa> (276)</td>
<td style="text-align: center;">move to the sofa</td>
<td style="text-align: center;">Walk-Sofa</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <chips> (61)</td>
<td style="text-align: center;">grab the chips</td>
<td style="text-align: center;">Grab-Chips</td>
</tr>
<tr>
<td style="text-align: center;">[GRAB] <milk> (46)</td>
<td style="text-align: center;">grab the milk</td>
<td style="text-align: center;">Grab-Milk</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHON] <TV> (297)</td>
<td style="text-align: center;">turn on the TV</td>
<td style="text-align: center;">Switchon-TV</td>
</tr>
<tr>
<td style="text-align: center;">[SWITCHOFF] <TV> (297)</td>
<td style="text-align: center;">turn off the TV</td>
<td style="text-align: center;">Switchoff-TV</td>
</tr>
<tr>
<td style="text-align: center;">[SIT] <sofa> (276)</td>
<td style="text-align: center;">take a seat on the sofa</td>
<td style="text-align: center;">Sit-Sofa</td>
</tr>
<tr>
<td style="text-align: center;">[STANDUP]</td>
<td style="text-align: center;">stand up from the sofa</td>
<td style="text-align: center;">Standup-Sofa</td>
</tr>
<tr>
<td style="text-align: center;">[PUTBACK] <chips> (61) <coffeetable> (268)</td>
<td style="text-align: center;">put the chips on the coffee table</td>
<td style="text-align: center;">Putback-Chips-Coffeetable</td>
</tr>
<tr>
<td style="text-align: center;">[PUTBACK] <milk> (46) <coffeetable> (268)</td>
<td style="text-align: center;">put the milk on the coffee table</td>
<td style="text-align: center;">Putback-Milk-Coffeetable</td>
</tr>
</tbody>
</table>
<p>Dynamics: The transition in this task is deterministic.
Reward: We adopt a sparse reward setting, where only when the task is finished will the agent receive +1 reward.</p>
<p>Episode Termination: Each episode terminates either when the agent successfully finishes the task or reaches the maximal time steps, 50 since every macro-action takes only time step for execution. For the task of heating pancake, the agent succeeds when it places the pancake in the microwave and closes the microwave. For the task of watching TV, the agent succeeds when it sits on the sofa with TV turned on and chips and milk are on the coffee table or hold in hand.</p>
<h1>C.3.1 Vanilla PPO VS PPO With Action Mask</h1>
<p>As mentioned in Section 5.3, in VirtualHome environment, vanilla PPO without action mask cannot learn anything for the two tasks due to the large action space. Figure 9 shows the performance of vanilla PPO without action mask and PPO with action mask.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance of PPO without action mask and PPO with action mask in VirtualHome tasks.</p>
<h1>D Training Details</h1>
<p>Our results are mainly generated on a single NVIDIA Tesla A100 and NVIDIA RTX A6000 GPU. For all domains, we use the same architecture, which is shown in Figure 10. Throughout all the experiments, we conducted training and testing using the half-precision float16.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Parameter efficient architecture</p>
<h2>D. 1 Network Architecture</h2>
<p>The same neural network architecture is applied to both the actor and critic networks across TWOSOME method. In the LLaMA-7B model, additional MLP layers are incorporated into the last transformer block to serve as the critic. The critic's MLPs take the last token of the observation prompt as input and output the estimated value of the observation prompt. The critic utilizes a 3 layer MLP structure, with the number of neurons in each layer being 1024, 512, and 1, respectively. ReLU is employed as the activation function. On the other hand, the actor consists of the frozen LLaMA-7B model with the augmentation of LoRA parameters. The policy optimizer uses AdamW with an epsilon value of $1 \mathrm{e}-5$ and a weight decay of 0 . The critic optimizer uses Adam with an epsilon value of $1 \mathrm{e}-5$ and the default weight decay of 0 . In our experiments, we found that the default epsilon value of $1 \mathrm{e}-8$ in the policy optimizer's AdamW can lead to unstable training and potential "nan" outputs in Lora.</p>
<p>In the PPO method, the critic network remains a 3-layer MLP architecture with 64 neurons in each layer and a single neuron as the final output. However, the main distinction from TWOSOME lies in the choice of activation function, which is Tanh. Meanwhile, the actor in the PPO method also adopts a 3-layer MLP architecture. The number of neurons in the first two layers is the same as the critic, which is 64 . The activation function used is Tanh. The final layer's output depends on the number of actions in the task. For example, in the Entertainment task, there are 17 actions, while in the Food Preparation task, there are 10 actions. The optimizer uses Adam with an epsilon value of $1 \mathrm{e}-5$ and a weight decay of 0 .</p>
<h2>D. 2 Hyperparameters for TWOSOME and PPO</h2>
<p>In following subsections, we first list the hyper-parameter candidates used for training TWOSOME via grid search in the corresponding task, and then show the hyper-parameter table with the parameters used by TWOSOME and PPO achieving the best performance. We choose the best performance of each method depending on its final convergence value and convergence efficiency.</p>
<p>Table 10: Overcooked hyper-parameter candidates of TWOSOME for grid search training</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning rate pair (policy,critic)</th>
<th style="text-align: center;">$\begin{gathered} \text { (1e-6, 5e-5), (5e-7, 5e-5), (5e-7, 1e-5) } \ \text { (3e-6, 5e-5), (1e-6, 1e-5) } \ (200,0.99),(50,0.95) \ 1 \mathrm{e} 5,5 \mathrm{e} 5 \end{gathered}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Max episode steps and gamma</td>
<td style="text-align: center;">First group parameters</td>
<td style="text-align: center;">Second group parameters</td>
</tr>
<tr>
<td style="text-align: center;">Total time steps</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of environments</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Number of steps per environment policy rollout</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of policy</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of critic</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Number of epochs to update policy</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Gradient checkpoint steps</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Surrogate clipping coefficient</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the entropy</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the value function</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">Maximum norm for the gradient clipping</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Target KL divergence threshold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Overcooked hyper-parameter used of TWOSOME in Tomato Salad task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning rate pair (policy,critic)</th>
<th style="text-align: center;">$(5 \mathrm{e}-7,1 \mathrm{e}-5)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Max episode steps and gamma</td>
<td style="text-align: center;">$(200,0.99)$</td>
</tr>
<tr>
<td style="text-align: center;">Total time steps</td>
<td style="text-align: center;">5 e 5</td>
</tr>
<tr>
<td style="text-align: center;">Number of environments</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of steps per environment policy rollout</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of policy</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of critic</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of epochs to update policy</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Gradient checkpoint steps</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Surrogate clipping coefficient</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the entropy</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the value function</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Maximum norm for the gradient clipping</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Target KL divergence threshold</td>
<td style="text-align: center;">0.02</td>
</tr>
</tbody>
</table>
<p>Table 12: Overcooked hyper-parameter used of TWOSOME in Tomato Lettuce Salad task</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning rate pair (policy,critic)</th>
<th style="text-align: center;">$(5 \mathrm{e}-7,1 \mathrm{e}-5)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Max episode steps and gamma</td>
<td style="text-align: center;">$(200,0.99)$</td>
</tr>
<tr>
<td style="text-align: center;">Total time steps</td>
<td style="text-align: center;">5 e 5</td>
</tr>
<tr>
<td style="text-align: center;">Number of environments</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of steps per environment policy rollout</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of policy</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">Number of mini-batches of critic</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Number of epochs to update policy</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Gradient checkpoint steps</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Surrogate clipping coefficient</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the entropy</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Coefficient of the value function</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Maximum norm for the gradient clipping</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;">Target KL divergence threshold</td>
<td style="text-align: center;">0.02</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/microsoft/DeepSpeedExamples/blob/master/ applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/README.md&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
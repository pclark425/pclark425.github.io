<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3799 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3799</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3799</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-90.html">extraction-schema-90</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <p><strong>Paper ID:</strong> paper-03651ef144d2f28583541f81056835a16aacfcac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/03651ef144d2f28583541f81056835a16aacfcac" target="_blank">VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners</a></p>
                <p><strong>Paper TL;DR:</strong> This work presents VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness, and evaluates VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers.</p>
                <p><strong>Paper Abstract:</strong> Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default. Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust. Existing work falls into two categories: rule-based and large language model (LLM)-based. While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language. On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness. In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness. VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages. VERT first uses the Web Assembly compiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to generate a readable candidate Rust program. This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds. We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks. Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone. In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers. Our results provide insights into the limitations of LLMs to write safe Rust.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3799",
    "paper_id": "paper-03651ef144d2f28583541f81056835a16aacfcac",
    "extraction_schema_id": "extraction-schema-90",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00484175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VERT: Verified Equivalent Rust Transpilation with Large Language Models as Few-Shot Learners</h1>
<p>AIDAN Z.H. YANG ${ }^{<em> \dagger}$, Carnegie Mellon University, USA<br>YOSHIKI TAKASHIMA ${ }^{</em> \dagger}$, Carnegie Mellon University, USA<br>BRANDON PAULSEN, Amazon Web Services, USA<br>JOSIAH DODDS, Amazon Web Services, USA<br>DANIEL KROENING, Amazon Web Services, USA</p>
<p>Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default. Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust. Existing work falls into two categories: rule-based and large language model (LLM)-based. While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language. On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness. In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness. VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages. VERT first uses the Web Assembly compiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to generate a readable candidate Rust program. This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds. We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks. Our results show that VERT significantly improves an LLM's ability to generate correct Rust transpilations. Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from $31 \%$ to $54 \%$ and bounded model-checking from $1 \%$ to $42 \%$ compared to using Claude alone. In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers. Our results provide insights into the limitations of LLMs to write safe Rust.</p>
<p>CCS Concepts: $\cdot$ Software and its engineering $\rightarrow$ Formal software verification; Model checking; $\cdot$ Computing methodologies $\rightarrow$ Natural language processing.</p>
<h2>1 INTRODUCTION</h2>
<p>Rust is a memory- and type-safe programming language that has performance on par with low-level languages like C. It is often referred to as a "safer C" because the Rust type checker can guarantee the absence of undefined behavior. Microsoft estimates that $70 \%$ of all their security bugs are due to memory-safety issues [16], which could be mostly or entirely eliminated if the code were written in Rust. Citing Rust's security benefits, Rust has been used in major open source projects such as Firecracker [8], and Linus Torvalds recently announced Rust will be a supported language for Linux kernel development [50].</p>
<p>Rust's security and performance benefits have fueled interest in automatically transpiling existing code written in other languages into Rust [2, 45]. Existing works on transpilation broadly fit into two categories: rule-based and large language model (LLM)-based. Rule-based approaches use</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Examples of transpilations from a source Go program using compile-then-lift, Anthropic's Claude LLM, and VERT.
hand-written rules and algorithms that translate a target program into a new language, typically in a statement-by-statement fashion. Rule-based approaches have human-understandable implementations that could, in theory, be proved correct. However, as we will show, they often result in unreadable and unidiomatic code that does not take full advantage of the target language's useful features, such as efficient native types.</p>
<p>On the other hand, LLM-based approaches train an LLM that takes a program in one language as input and attempt to output an equivalent program in the target language [43]. LLM-based approaches tend to produce code that is similar to their training data, and thus, if the model is trained on high quality, human written, code, the model will usually produce high quality, idiomatic transpilations [57]. However, these approaches come with no formal guarantees that the resulting code will maintain input-output equivalence with the original [34, 38, 55].</p>
<p>In this work, we focus on general, verified, and readable transpilation to Rust. By general, we mean that it can apply to most major languages. By verified, we mean that input-output equivalence of the final transpilation has been verified against the source program in some way. The verification techniques we experiment with are property-based testing (PBT), bounded model checking, and unbounded model checking. By readable, we mean that the transpilation resembles human-written code. While readability is a highly subjective measure, we show examples in Figure 1 that we believe makes this claim self-evident.</p>
<p>To the best of our knowledge, the only general rule-based approach for transpiling to Rust that could theoretically guarantee equivalence is to compile-then-lift. In this approach, we first compile the source program to an intermediate language like LLVM or Web Assembly, and then lift</p>
<p>the intermediate language to the target language (Rust in our case). For example, Web Assembly compilers exist for most major languages (e.g. C, C++, Rust, Java, Go, C#, PHP, Python, TypeScript, Zig, and Kotlin), and the recent work rWasm [11] can lift Web Assembly to Rust. While this approach is very general and, at least in theory, can guarantee equivalence, compile-then-lift approaches generally can only produce code that is as readable as the intermediate language itself, which for LLVM and Web Assembly is virtually unreadable [24, 41]. We give an example transpilation using a Web Assembly compiler and rWasm in Figure 1. As can been seen, rWasm produces Rust that looks like assembly rather than a high-level language.</p>
<p>There are also many works on transpiling without equivalence guarantees [35, 43, 45, 52, 53, 56], mainly using language models. While these approaches are also general and usually produce readable code, language models are notorious for outputting subtly incorrect code [55]. We show an example language model transpilation in Figure 1 using Anthropic's Claude-2, which is a state-of-the-art general purpose LLM. We can see the transpilation is far more readable than the result of the compile-then-lift approach. However, the LLM has changed a - to a + , which may escape human review. Such subtle errors may be difficult to debug and only manifest in corner cases.</p>
<p>To overcome these limitations, we combine rule-based and LLM-based transpilation with formal verification tools, and implement our approach in a tool VERT. Our algorithm takes a source program as input, and outputs a transpilation that is verified equivalent relative to a rule-based transpilation. Notably, VERT does not require any additional input beyond the source program. The main assumption of VERT is that the language of the source program has a Web Assembly compiler.</p>
<p>VERT first creates an oracle Rust transpilation by using the source language's Web Assembly compiler and rWasm, as previously described. This transpilation is equivalent by construction, but is unreadable. Next, we leverage an LLM to produce a candidate final transpilation, which is far more readable, but may have implementation errors, ranging from syntax errors to subtle logic errors. We then enter an iterative repair and verify process. We first attempt to fix compilation errors by applying a combination of hand-crafted rules and re-prompting the LLM to re-transpile the source program until the program compiles. Once compiled, we attempt to verify equivalence using one of the previously mentioned verification techniques. If verification succeeds, then we stop and output the program. However, if verification fails, which is usually the case, we re-prompt the LLM to transpile the program.</p>
<p>We evaluate VERT on 1,394 transpilation tasks with source languages in C++, C, and Go curated from prior work [45, 61]. We focus on C++ and C since these two languages are often used for similar tasks as Rust [27, 51]. We further evaluate on Go as Rust is often the transpilation target when cross-platform support is a hard requirement [26]. We experiment with three state-of-the-art LLMs as the underlying LLM for VERT, namely CodeLlama-2 [42], StarCoder [31], and Anthropic Claude-2 [4]. With Claude-2 as the underlying LLM, our results show that VERT can produce transpilations that pass bounded verification for $42 \%$ of these programs, and differential testing for $54 \%$ of these programs. Moreover, VERT improves the capabilities of existing LLMs - VERT is able to produce a verified transpilation $45 \%$ more often on average ( $43 \%$ for C++, $46 \%$ for C , and $43 \%$ for Go) compared to using an LLM alone.</p>
<p>To measure VERT's ability to produce non-trivial safe Rust, we gather and manually label an additional 14 programs that make significant use of pointers from the benchmarks of prior work on C to Rust transpilation [2, 21, 61]. Our results on these additional benchmarks show that VERT can produce Rust with relatively simple ownership models for small (less than 36 LoC) programs.</p>
<p>In summary, our main contributions are as follows.</p>
<ul>
<li>VERT. We propose and implement an end-to-end technique that can transpile any language that compiles to Wasm into human-readable Rust. Our data and tool are available for open-source. ${ }^{1}$</li>
<li>Verified Equivalence with Input Code We use Wasm generated from original input as a reference model and perform equivalence checking by automatically injecting test harnesses, allowing the user to verify that the LLM translation is free of hallucinations.</li>
<li>Empirical evaluation. We evaluated VERT on a set of real world programs and competitive programming solutions, which include 569 C++ programs, 520 C programs, and 305 Go programs. We perform an extensive evaluation of several LLMs directly (CodeLlama-2), with fine-tuning (StarCoder), and with instruction-tuned few-shot learning (Anthropic Claude-2) on different source code languages.</li>
</ul>
<h1>2 BACKGROUND</h1>
<p>We give a brief introduction of the key aspects of our tool. In particular: Rust, the rWasm compilation strategy, and auto-regressive large language models.</p>
<h3>2.1 Rust</h3>
<p>Rust is a systems programming language with a focus on performance, reliability, and safety. Rust's main goal is to eliminate memory safety errors through a memory-ownership mechanism. Rust's memory-ownership mechanism associates each value in memory with a unique owner variable, which guarantees safe static memory collection. In particular, when we want to create a variable that aliases a value (i.e., creating a new pointer to the value), we must transfer ownership to the new variable, either temporarily or permanently. Rust programs are challenging to write and synthesize as these ownership rules must be obeyed for the program to even compile. LLM-based Rust synthesis has the additional challenge that these typing rules are not found in popular languages that make up majority of the training dataset.</p>
<h3>2.2 Migrating to Rust</h3>
<p>Given the memory-safety properties of Rust, there is a strong incentive to migrate existing codebases to Rust. While several notable projects have been rewritten in Rust [60], the translation to Rust remains a challenge owing to the enormous manual effort. For C to Rust translation in particular, several tools have been developed to automatically translate C functions to Rust [2, 21, 61]. These tools use the semantic similarity between C and Rust and apply re-writing rules to generate Rust code. However, these re-write rules are specific to the source language, and do not scale to multiple languages, especially those whose semantics is not similar to Rust. To the best of our knowledge, no rule-based automatic translator exists from a garbage-collected language like Go or Java to Rust.</p>
<p>In contrast to transpilers, rWasm differs significantly in its intent. rWasm converts Web Assembly (Wasm) programs into Rust and leverages the memory-safety properties of safe Rust as a sandbox to eliminate the Wasm runtime overhead. Since many programming languages already target Wasm as an intermediate representation [5], we can leverage rWasm for multi-language support.</p>
<h3>2.3 Rust Testing and Verification</h3>
<p>To establish trust in the LLM-output, we perform equivalence verification of two Rust programs. We use existing tools that operate on Rust to prove equivalence between the LLM-generated and the rWasm-generated oracle Rust programs. We use bolero, a Rust testing and verification framework that can check properties using both Property-Based Testing (PBT) [22] and Bounded</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Model Checking [17, 18]. An example of a bolero harness for checking equivalence between an LLM-generated and a trusted reference program is given in Fig. 2.</p>
<div class="codehilite"><pre><span></span><code><span class="mi">1</span><span class="w"> </span><span class="err">#</span><span class="o">[</span><span class="n">test</span><span class="o">]</span>
<span class="mi">2</span><span class="w"> </span><span class="err">#</span><span class="o">[</span><span class="n">cfg_attr(kani, kani::proof)</span><span class="o">]</span>
<span class="mi">3</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="n">eq_check</span><span class="p">()</span><span class="w"> </span><span class="err">{</span>
<span class="mi">4</span><span class="w"> </span><span class="nl">bolero</span><span class="p">:</span><span class="err">:</span><span class="k">check</span><span class="err">!</span><span class="p">()</span>
<span class="mi">5</span><span class="w"> </span><span class="p">.</span><span class="n">with_type</span><span class="p">()</span>
<span class="mi">6</span><span class="w"> </span><span class="p">.</span><span class="n">cloned</span><span class="p">()</span>
<span class="mi">7</span><span class="w"> </span><span class="p">.</span><span class="n">for_each</span><span class="p">(</span><span class="o">|</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="p">(</span><span class="n">i32</span><span class="p">,</span><span class="w"> </span><span class="n">i32</span><span class="p">)</span><span class="o">|</span><span class="w"> </span><span class="mi">1</span><span class="n">lm_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">reference_fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">));</span>
<span class="mi">8</span><span class="w"> </span><span class="err">}</span>
</code></pre></div>

<p>Fig. 2. An example bolero harness checking equivalence between 2 functions.</p>
<p>Using a harness like the one in Fig. 2, bolero can check properties via two different methods. The first method is by random PBT. PBT works by randomly generating inputs to the function under test, running the harness with these inputs, and asserting the desired properties (e.g. equivalence between two functions). PBT repeatedly runs this procedure to check the property over the range of inputs. PBT is a valuable tool for catching implementation bugs, however it is generally infeasible to run PBT for long enough to exhaust all possible inputs to a program.
bolero performs model checking through Kani [48], a model-checker for Rust. When run with Kani, bolero produces symbolic inputs rather than random concrete inputs. Executing the harness with symbolic inputs, we can cover the entire space of inputs in one run and the model-checker ensures the property holds for all possible inputs. Since symbolic execution does not know how many times loops are run, Kani symbolically executes loops up to an user-provided bound. To prove soundness of this bound, Kani uses unwind checks asserting that loop iteration beyond the bound is not reachable. We say that a verification is bounded if unwind checks are turned off and exhaustiveness is not known. Conversely, a full verification includes unwind checks and thus is exhaustive with respect to all reachable executions. Even with full verification, the harness may not be able to cover enough values of unbounded types like vectors.</p>
<p>While Kani can prove properties of programs, complex programs can take too long to prove. Conversely, PBT runs exactly as quickly as the program runs, but is not exhaustive. Given the complementary properties of PBT and Kani, we allow users to use both tools, using bolero and marking the harness for both PBT (#[test]) and model-checking (kani::proof).</p>
<h1>2.4 Large Language Models</h1>
<p>Deep learning (DL) has recently shown promise for program generation [36, 47]. The DL models that can achieve the closest capabilities to human-written results are large language models (LLMs), such as GPT-4 [37]. LLMs train billions of parameters using a massive amount of training data. LLMs' effectiveness for code generation [13] suggests that LLMs are capable of performing specialized software engineering tasks, such as program language transpilation.</p>
<p>Most modern LLMs are attention-based models. Attention-based models use the Transformer architecture [49]. In a Transformer architecture model, tokens exchange information across all other tokens using an attention matrix. LLMs typically produce text in a left-to-right manner (auto-regressive model), producing each token given its prefix context. In a auto-regressive models, the learned attention matrix is partially masked out. Specifically, the generation of a new token depends only on its prefix context (i.e., tokens on the left), and its suffix context (i.e., tokens on the right) are hidden (i.e., masked out). After a model finishes training all trainable parameters in a</p>
<p>user specified time, a model can make predictions on future tokens on a sequence of tokens the model has never seen before.</p>
<h1>3 METHODOLOGY</h1>
<p>In this section, we describe the key ideas behind our universal transpilation technique. Figure 3 gives an overview of VERT's entire pipeline. The technique takes as input a source program, and outputs a verified equivalent Rust program. As shown in Figure 3, we parse the program into separate functions during the cleaning phase, then split the pipeline into two paths. The first path outputs an LLM-generated Rust transpilation. The second path produces rWasm Rust code that is compiled directly from the original program through Wasm. Finally, we create a test harness based on the original program to verify equivalence of the two paths' outputs, and only after a successful verification we output a correct and maintainable Rust transpilation. In the following sections, we describe each component of VERT in further detail.</p>
<h3>3.1 Program repair on LLM output</h3>
<p>LLMs often produce incorrect code. When prompting an LLM for Rust code, any slight mistake could cause the strict Rust compiler (rustc) to fail. Fortunately, rustc produces detailed error messages when compilation fails to guide the user to fix their program. We create an automatic repair system based on rustc error messages. For each error, we first classify the error into one of three main categories: syntax, typing, and domain specific.</p>
<p>As seen in Figure 4, an example syntax error generated by the LLM is the wrong closing delimiter. For rustc to successfully compile, all syntax errors must be resolved. We track the error code location (e.g., line 10 in Figure 4), and we use the rustc provided initial delimiter to guide our repair strategy. For this case, we know to use the right curly bracket $\overline{3}$ to replace $\overline{1}$ on line 10.</p>
<p>Typing error messages in Rust generally have a similar structure. In particular, error messages are usually of the form expected type a, found b. Figure 5 shows the LLM generate a pass-byreference variable $\&amp; 132$ while rustc expects a pass-by-value 132. Using the compiler message's error localization and suggestion line (characterized by the keyword help:), we replace the variable num by \&amp;num.</p>
<p>Finally, domain-specific errors are compilation errors that are specific to the program. The error messages for domain-specific errors do not share the same structure, and therefore we only use the rustc error message suggestion line to generate a repair. In Figure 6, which shows the error message for an immutable assignment, the suggestion line indicates that if the variable $x$ is converted to a mutable object, the immutable assignment error would be solved. Using this suggestion line, we replace $x$ by mut $x$, and observe that the program compiles. It is often the case that even with the error message suggestion line, we cannot generate a repair that fixes all errors. In these cases, we regenerate an LLM output using the error as part of the new prompt and restart the process. Our error-guided repair is significantly faster than the LLM generation (discussed in Section 4.2), so we only regenerate an LLM output after exhausting all rustc helper messages.</p>
<h3>3.2 Transpilation Oracle Generation</h3>
<p>Since the LLM output cannot be trusted on its own, we create an alternate trusted transpilation pipeline for generating a reference Rust program against which the LLM output is checked. The alternate pipeline does not need to produce maintainable code, but it needs to translate the source language into Rust using a reliably correct rule-based method. We use Wasm as the intermediate representation because many languages have compilers to Wasm, allowing it to serve as the common representation in the rule-based translation. Once the input programs are compiled to Wasm, we</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. VERT 's architecture, which takes as input a source program and produces a formally equivalent Rust program</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1</span>
<span class="mf">2</span><span class="err">}\</span><span class="n">mathrm</span><span class="err">{</span><span class="w"> </span><span class="kr">For</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">try</span><span class="w"> </span><span class="err">&#39;</span><span class="n">rustc</span><span class="w"> </span><span class="o">--</span><span class="nb">exp</span><span class="n">lain</span><span class="w"> </span><span class="n">E0433</span><span class="err">&#39;</span><span class="mf">.</span>
<span class="mf">3</span><span class="w"> </span><span class="n">error</span><span class="p">:</span><span class="w"> </span><span class="n">mismatched</span><span class="w"> </span><span class="n">closing</span><span class="w"> </span><span class="n">delimiter</span><span class="p">:</span><span class="w"> </span><span class="err">&#39;]&#39;</span>
<span class="w">    </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">roman_to_integer_test</span><span class="mf">.</span><span class="n">rs</span><span class="p">:</span><span class="mf">1</span><span class="p">:</span><span class="mf">33</span>
<span class="w">    </span><span class="mf">5</span><span class="w"> </span><span class="err">|</span>
<span class="mf">6</span><span class="w"> </span><span class="mf">1</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="kd">fn</span><span class="w"> </span><span class="n">roman_to_int</span><span class="p">(</span><span class="n">s</span><span class="p">:</span><span class="w"> </span><span class="err">&amp;</span><span class="n">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">i32</span><span class="w"> </span><span class="err">{</span>
<span class="mf">7</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="n">unclosed</span><span class="w"> </span><span class="n">delimiter</span>
<span class="mf">8</span><span class="w"> </span><span class="mf">...</span>
<span class="mf">9</span><span class="w"> </span><span class="mf">10</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="err">]</span><span class="p">);</span>
<span class="mf">10</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="n">mismatched</span><span class="w"> </span><span class="n">closing</span><span class="w"> </span><span class="n">delimiter</span>
</code></pre></div>

<p>Fig. 4. Syntax error</p>
<div class="codehilite"><pre><span></span><code><span class="n">error</span><span class="o">[</span><span class="n">E0308</span><span class="o">]</span><span class="err">:</span><span class="w"> </span><span class="n">mismatched</span><span class="w"> </span><span class="n">types</span><span class="c1">--&gt;</span>
<span class="n">rust_programs</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">uniquepaths</span><span class="p">.</span><span class="nl">rs</span><span class="p">:</span><span class="mi">11</span><span class="err">:</span><span class="mi">14</span>
<span class="mi">11</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="k">map</span><span class="p">.</span><span class="k">insert</span><span class="p">(</span><span class="n">num</span><span class="p">,</span><span class="k">index</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">i32</span><span class="p">);</span><span class="o">|</span><span class="w"> </span><span class="c1">------ ^^^ expected &#39;&amp;i32&#39;, found i32</span>
<span class="nl">help</span><span class="p">:</span><span class="w"> </span><span class="n">consider</span><span class="w"> </span><span class="n">borrowing</span><span class="w"> </span><span class="nl">here</span><span class="p">:</span>
</code></pre></div>

<p>Fig. 5. Mismatched type error</p>
<div class="codehilite"><pre><span></span><code><span class="n">error</span><span class="o">[</span><span class="n">E0384</span><span class="o">]</span><span class="err">:</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">assign</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">immutable</span><span class="w"> </span><span class="n">argument</span><span class="w"> </span><span class="s1">&#39;x&#39;</span>
<span class="w">    </span><span class="c1">--&gt; reverse_integer_test.rs:16:3</span>
<span class="w">    </span><span class="mi">3</span><span class="w"> </span><span class="o">|</span>
<span class="mi">4</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="nf">reverse</span><span class="p">(</span><span class="nl">x</span><span class="p">:</span><span class="w"> </span><span class="n">i32</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">i32</span><span class="w"> </span><span class="err">{</span>
<span class="mi">5</span><span class="w"> </span><span class="o">|-</span><span class="w"> </span><span class="nl">help</span><span class="p">:</span><span class="w"> </span><span class="n">consider</span><span class="w"> </span><span class="n">making</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">binding</span><span class="w"> </span><span class="nl">mutable</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;mut x&#39;</span>
<span class="mi">6</span><span class="w"> </span><span class="p">...</span>
<span class="mi">7</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
<span class="mi">8</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="o">^^^^^^^^^^</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">assign</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">immutable</span><span class="w"> </span><span class="n">argument</span>
<span class="mi">9</span>
<span class="mi">10</span><span class="w"> </span><span class="err">\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="nl">error</span><span class="p">:</span><span class="w"> </span><span class="n">aborting</span><span class="w"> </span><span class="n">due</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">errors</span><span class="err">}</span>
<span class="mi">11</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="k">try</span><span class="w"> </span><span class="s1">&#39;rustc --explain E0384&#39;</span><span class="p">.</span>
</code></pre></div>

<p>Fig. 6. Immutable assignment error</p>
<div class="codehilite"><pre><span></span><code><span class="k">func</span><span class="w"> </span><span class="n">callReverse</span><span class="p">()</span><span class="w"> </span><span class="nb nb-Type">int</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">result</span><span class="w"> </span><span class="p">:</span><span class="o">=</span><span class="w"> </span><span class="n">reverse</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="mi">3</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">321</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>Fig. 7. An entry point for the reverse function
use rWasm [11], a tool that translates from Wasm to Rust by embedding the Wasm semantics in Rust source code. While the original authors intended rWasm as a sandboxing tool that leverages the</p>
<div class="codehilite"><pre><span></span><code><span class="nt">fn</span><span class="w"> </span><span class="nt">func_4</span><span class="o">(&amp;</span><span class="nt">mut</span><span class="w"> </span><span class="nt">self</span><span class="o">,</span><span class="w"> </span><span class="o">)</span><span class="w"> </span><span class="nt">-</span><span class="o">&gt;</span><span class="w"> </span><span class="nt">Option</span><span class="o">&lt;</span><span class="nt">i32</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="err">//</span><span class="w"> </span><span class="err">...</span>
<span class="w">    </span><span class="err">let</span><span class="w"> </span><span class="err">mut</span><span class="w"> </span><span class="n">local_3</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">i32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">i32</span><span class="p">;</span>
<span class="w">    </span><span class="err">let</span><span class="w"> </span><span class="err">mut</span><span class="w"> </span><span class="n">local_4</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">i32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">i32</span><span class="p">;</span>
<span class="w">    </span><span class="err">v0</span><span class="w"> </span><span class="err">=</span><span class="w"> </span><span class="n">TaggedVal</span><span class="p">:</span><span class="o">:</span><span class="nf">from</span><span class="p">(</span><span class="mi">321</span><span class="n">i32</span><span class="p">);</span>
<span class="w">    </span><span class="err">//</span><span class="w"> </span><span class="n">mutant</span><span class="p">:</span><span class="w"> </span><span class="n">v0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TaggedVal</span><span class="o">::</span><span class="nf">from</span><span class="p">(</span><span class="mi">654</span><span class="n">i32</span><span class="p">);</span>
<span class="w">    </span><span class="err">local_3</span><span class="w"> </span><span class="err">=</span><span class="w"> </span><span class="err">v0.try_as_i32()?</span><span class="p">;</span>
<span class="w">    </span><span class="err">v0</span><span class="w"> </span><span class="err">=</span><span class="w"> </span><span class="n">TaggedVal</span><span class="p">:</span><span class="o">:</span><span class="nf">from</span><span class="p">(</span><span class="mi">123</span><span class="n">i32</span><span class="p">);</span>
<span class="w">    </span><span class="err">//</span><span class="w"> </span><span class="n">mutant</span><span class="p">:</span><span class="w"> </span><span class="n">v0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TaggedVal</span><span class="o">::</span><span class="nf">from</span><span class="p">(</span><span class="mi">456</span><span class="n">i32</span><span class="p">);</span>
<span class="w">    </span><span class="err">local_4</span><span class="w"> </span><span class="err">=</span><span class="w"> </span><span class="err">v0.try_as_i32()?</span><span class="p">;</span>
<span class="w">    </span><span class="err">//</span><span class="w"> </span><span class="err">...</span>
<span class="p">}</span>
</code></pre></div>

<p>Fig. 8. The difference between the original rWasm output and the mutated one (highlighted).
memory safety properties of safe Rust, we use it to generate trusted Rust code with same semantics as the original input.</p>
<h1>3.3 Mutation Guided Entry Point Identification</h1>
<p>Given the assembly-like output of rWasm, we must perform analysis to identify the entry point of the rWasm transpiled function. VERT provides the option for the user to manually identify the entrypoint, but we can find it automatically using a simple heuristic, such as a function call or a single test case. We note that this heuristic could be generated automatically using LLMs or search-based software testing and thus we can assume an entrypoint generator in the source language. VERT uses a function call in the source language with constant inputs to the function to be transpiled and an assertion on the output of that function. One such function is given in Fig. 7.</p>
<p>We leverage this function call to identify the input and output of the function. While one option for such analysis is to perform decompilation, we find that a mutation-guided approach is sufficient for our purposes. In Fig. 7, we know that the input is 123 and the output is 321 . Now, we wish to identify the equivalent constant in the rWasm output. While it is possible to just perform a linear scan of the rWasm output for this constant, that risks spurious matches, especially for simple types like i32. Instead, we guide this identification by leveraging the function call and mutating it. Suppose we swap 123 with 456 and 321 with 654 and re-transpile with rWasm. These constants will change, but the rest of the rWasm output remains the same. Taking the diff, we can identify inputs and outputs by what changed. The diff in the rWasm output is shown in Fig. 8.</p>
<h3>3.4 Equivalence Harness Generation</h3>
<p>In our final step, we generate harnesses to check for equivalence given the input and output locations. We define equivalence here in functional terms: for all inputs, running both functions yields no crashes and identical outputs. To check this property holds, we automatically generate a wrapper to the Wasm function and a harness where the LLM-synthesized and wrapped rWasm functions are called with the same inputs, and the outputs are asserted to be equal. To ensure this equivalence holds for all inputs, we leverage property-based testing with random inputs and model-checking with symbolic inputs. For the remainder of this section, we refer to both of them together as "the input."</p>
<p>The wrapper consists of two parts: input injection, and output checking. Since the original program runs with a set of constant inputs, we must replace these constant inputs with the inputs of the harness like input: i32. The challenge here is to inject the inputs into the middle of the</p>
<p>Rust code representing a Wasm module. Instead of replacing the parameters to the function, we use globals in Rust to inject the inputs right at the location where constants used to be. An example is given in Fig. 9, with func_4 being the Wasm equivalent of the test. VERT replaces constant inputs with global reads, generalizing the test and allowing us to vary the inputs fed into the Wasm-generated function. Note that, while this injection requires unsafe code, it is fine as this is only done in the oracle and the oracle is discarded once the equivalence is checked.</p>
<p>Now that we can feed various inputs into the Wasm function, we must also provide a way to assert that the output is equal. Recall that in Fig. 9, the output is compared to a baseline and 0 is returned if the check succeeds. Because of this comparison check, it is sufficient to inject the baseline value and then leverage the check to assert that the return value is 0 . Injection is done in the same way as the inputs.</p>
<p>We note that while this approach is sound, it may falsely identify some equivalent programs as faulty due to semantic differences between Rust and the target language, or between the target language and rWasm embedding. We note two cases where we permit the analyst to add assumptions. First, when the input type is an unsigned integer. In this case, we have a mismatch were Wasm has only signed integers. So the output of rWasm will represent unsigned integers by encoding it in signed. However, the true valid range will be smaller (u32 will become 164 to the full range of u32 values but the extra bits are not used). In this case, it is soundly permissible to assume that the values lie in the valid range of u32. Another case of valid assumptions occurs with strings: strings in C are ASCII while in Rust are Unicode. Therefore, a valid range of Rust strings will crash a C-derived Wasm module spuriously. We assume the string's range to valid ASCII only.</p>
<h1>3.5 Equivalence Checking</h1>
<p>With the equivalence checking harness built, we must now drive the harness and check that the equivalence property holds for all inputs. VERT provides 3 equivalence checking techniques with increasing levels of confidence and compute cost. This procedure is shown in Fig. 11. First, we run the equivalence-checking harness with PBT using bolero up to the time limit, generating random inputs and checking equivalence of the outputs. If the candidate diverges from the oracle, then PBT will return the diverging input as a counterexample. If no counterexample is found within the time limit, we say this candidate passes PBT.</p>
<p>If the PBT stage succeeds, we now perform bounded verification with Kani. In the bounded verification phase, we run Kani with an unwinding bound of $k$ and no unwind checks. This means that paths up to $k$ loop iterations is exhaustively explored, but any divergences between the candidate and the oracle with traces containing more than $k$ loop iterations are missed. We run this phase for 120 seconds, and terminate with 3 potential results. First, Kani returns with a counterexample that causes the oracle and the candidate to diverge or one of the two to crash. Second, Kani does not return with an answer within the time limit, which we also consider to be a failure as we cannot establish bounded equivalence. Finally, Kani verifies that, limited to executions with at most $k$ loop iteration, there are no divergences or crashes. We consider the third case alone to be successful. Bounded verification does not check whether $k$ is exhaustive.</p>
<p>If bounded verification succeeds, we perform full verification. VERT increases the unwinding bound until verification can be achieved with all checks enabled, including unwind checks that ensure the unwinding is fully exhaustive and the program cannot go beyond the code that was unwound. This ensures the equivalence and safety properties holds for every input the harness generates. Once again, three outcomes are possible. First, Kani can return a counterexample. Second, Kani can fail to return an answer within the time limit. Finally, Kani successfully verifies the harness. Again, we consider the third case alone to be successful. However, unlike with bounded verification,</p>
<div class="codehilite"><pre><span></span><code><span class="nx">static</span><span class="w"> </span><span class="nx">mut</span><span class="w"> </span><span class="nx">INPUT_1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="nx">static</span><span class="w"> </span><span class="nx">mut</span><span class="w"> </span><span class="nx">OUTPUT_1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kd">impl</span><span class="w"> </span><span class="nx">WasmModule</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">/// returns 0 if the output matches</span>
<span class="w">    </span><span class="kd">fn</span><span class="w"> </span><span class="nx">func_4</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">mut</span><span class="w"> </span><span class="kp">self</span><span class="p">,</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">Option</span><span class="p">&lt;</span><span class="kt">i32</span><span class="p">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// ...</span>
<span class="w">        </span><span class="kd">let</span><span class="w"> </span><span class="nx">mut</span><span class="w"> </span><span class="nx">local_3</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="kt">i32</span><span class="p">;</span>
<span class="w">        </span><span class="kd">let</span><span class="w"> </span><span class="nx">mut</span><span class="w"> </span><span class="nx">local_4</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">i32</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">0</span><span class="kt">i32</span><span class="p">;</span>
<span class="w">        </span><span class="nx">v0</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">TaggedVal</span><span class="o">::</span><span class="nx">from</span><span class="p">(</span><span class="nx">unsafe</span><span class="w"> </span><span class="p">{</span><span class="nx">INPUT_1</span><span class="p">});</span>
<span class="w">        </span><span class="nx">local_3</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">v0</span><span class="p">.</span><span class="nx">try_as_i32</span><span class="p">()?;</span>
<span class="w">        </span><span class="nx">v0</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">TaggedVal</span><span class="o">::</span><span class="nx">from</span><span class="p">(</span><span class="nx">unsafe</span><span class="w"> </span><span class="p">{</span><span class="nx">OUTPUT_1</span><span class="p">});</span>
<span class="w">        </span><span class="nx">local_4</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">v0</span><span class="p">.</span><span class="nx">try_as_i32</span><span class="p">()?;</span>
<span class="w">        </span><span class="c1">// ...</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
<span class="c1">// equivalence-checking harness.</span>
<span class="kd">fn</span><span class="w"> </span><span class="nx">equivalence</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">bolero</span><span class="o">::</span><span class="nx">check</span><span class="p">!()</span>
<span class="w">    </span><span class="p">.</span><span class="nx">for_each</span><span class="p">(</span><span class="o">|</span><span class="p">(</span><span class="nx">input</span><span class="p">:</span><span class="w"> </span><span class="kt">i32</span><span class="p">)</span><span class="o">|</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kd">let</span><span class="w"> </span><span class="nx">llm_fn_output</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">llm_generated_reverse</span><span class="p">();</span>
<span class="w">        </span><span class="nx">unsafe</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nx">INPUT_1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">input</span><span class="p">;</span>
<span class="w">            </span><span class="nx">OUTPUT_1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">llm_fn_output</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="kd">let</span><span class="w"> </span><span class="nx">mut</span><span class="w"> </span><span class="nx">wasm_module</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">WasmModule</span><span class="o">::</span><span class="nx">new</span><span class="p">();</span>
<span class="w">        </span><span class="nx">wasm_module</span><span class="p">.</span><span class="nx">_start</span><span class="p">().</span><span class="nx">unwrap</span><span class="p">();</span>
<span class="w">        </span><span class="nx">assert</span><span class="p">!(</span><span class="nx">wasm_module</span><span class="p">.</span><span class="nx">func_4</span><span class="p">().</span><span class="nx">unwrap</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">});</span>
<span class="p">}</span>
</code></pre></div>

<p>Fig. 9. Equivalence-checking harness for func_4.
successful full verification guarantees that the translation is without any error. If the oracle crashes at any point in the equivalence checking, VERT provides the user with a counterexample which can be used to diagnose the crash in the original program.</p>
<p>We support complex types through their primitive parts. Given a struct or enum, that Kani or PBT does not initially support, we construct values of that type by abstracting the primitive parameters of that type and any required discriminants for enums. For types of finite size, this is sufficient. However, we provide bounded support for handling vector types. The challenge here is to vary the length of the vector in the rWasm output, which is done by having a fixed-length vector of varying inputs and then pruning the length down to the actual length dynamically. Our approach is sound and complete for primitive types, and by extension, any type that comprises solely of primitive types such as tuples of primitives. For unbounded types like vectors, hashmaps and user-defined types containing such, VERT synthesizes harnesses that generate inputs up to the size encountered in the sample function call. As a limitation, any divergences that require bigger vector than encountered will be missed.</p>
<h1>3.6 Few-shot Learning</h1>
<p>The main focus of this work is on verifying the output of LLMs for program transpilation, and not LLM prompt engineering. Therefore, we keep the prompts simple and short. Complicated</p>
<p>1 {Original code}
2 ...
3 Safe Rust refactoring of above code in {language}, with code only, no comments
4 Use the same function name, same argument and return types.
5 Make sure the output program can compile on its own.
6 // If there exists counter examples from prior failed equivalence checking
7 Test that outputs from inputs {counter_examples} are equivalent to source program.</p>
<p>Fig. 10. LLM Prompt template.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 11. Evaluation procedure.
and repeated querying of the same prompts do not provide additional benefits on the accuracy of outputs for small sized models, and too expensive for an average practitioner for industry sized models (i.e., Anthropic Claude). To achieve few-shot learning on our transpilation queries, each failed transpilation attempt provides its equivalence checking counter examples as a few-shot learning example for future transpilation attempts.</p>
<p>Figure. 10 shows our template for few shot learning. We start with querying the LLM to refactor the source code into safe Rust. Although we filter for safe Rust LLM output, we experimentally found that asking the LLM to always produce safe Rust gives more accurate results. We prompt the LLM to use the same argument and return types as the original, and can compile without external dependencies. Finally, we collect the counter examples from prior failed equivalence checks as part of the prompt. Specifically, we ask the LLM to consider the specific inputs that caused a test or verification failure from the previous iterations. We observed that providing specific inputs as information to the LLM results in subtle bug fixes within the program output.</p>
<h1>4 EVALUATION</h1>
<p>In this section, we present our approach and results for the following research questions.
RQ1. How does VERT perform vs. using the respective LLM by itself? We evaluate our technique's performance on a benchmark dataset, showing that VERT significantly increases the number of verified equivalent transpilations vs. using the LLM by itself.
RQ2. How does each component of VERT's approach impact its performance? We conduct an ablation analysis, which shows that our prompting and error-guided refinement helps produce more well-typed and more correct programs. We further measure the runtime performance of each part of VERT, showing that time costs of error-guided refinement is reasonable and VERT spends most of the time in verification.</p>
<p>RQ3. Does VERT produce safe, readable, and idiomatic Rust transpilations? To evaluate VERT's ability to produce safe Rust, we collect programs from real world C projects that make use of pointers. In addition, we report on the frequency of linter warnings of transpilations produced by VERT, and compare lines of code between the translations produced by VERT, rWasm, and CROWN [61], a rule-based C to Rust transpiler. VERT's transpilations do not produce linter warnings, and has far fewer lines of code than the other approaches.
RQ4. How extensible is VERT to future verifiers? While we use Kani [48] for most of the benchmarks to leverage automation, we encountered a large number of timeouts. We show that VERT is able to work with multiple verifiers by using Verus [30] and show that manual verification is possible albeit costly.</p>
<h1>4.1 Setup</h1>
<p>4.1.1 LLMs. We use the following LLMs to generate the candidate transpilations in VERT:</p>
<ul>
<li>TransCoder-IR [45]: A language model trained by low-level compiler intermediate representations (IR) for the specific purpose of programming language translation. TransCoder-IR improves upon the TransCoder model [43] by incorporating IR into the training data and decompiling into IR as a training target. Both TransCoder and TransCoder-IR are trained on roughly 2.8 million repositories from GitHub ${ }^{2}$. Since TransCoder-IR's input is the original code alone and no prompt is taken, we do not perform error-guided few-shot prompting. To the best of our knowledge, TransCoder-IR is the only LLM-based general transpilation tool for Rust. Therefore, we use TransCoder-IR as baseline for our evaluation.</li>
<li>CodeLlama-2 [42]: A 13B parameter model initialized from Llama-2 [46], then further fine-tuned on 500 billion tokens of code data.</li>
<li>StarCoder Fine-tuned [31]: A 15.5B parameter model trained on 1 trillion tokens sourced from The Stack [28]. StarCoder prompted achieves the highest HumanEval [13] score of 40.8 over comparable open-source LLMs, such as LLaMA-65B [46] with a score 23.7 and CodeGen-16B [36] with a score of 29.3. To investigate the effectiveness of Rust fine-tuning on prior LLMs, we fine-tune StarCoder for transpilation using LeetCode problems that have solutions for Rust, $\mathrm{C}, \mathrm{C}++$, and Go. In total, we collect solutions in each language for 94 LeetCode problems. We fine-tune the LLM to take C, C++, or Go solution as the input, and produce the corresponding Rust solution as output.</li>
<li>Anthropic Claude-2 [4]: A production-grade, proprietary LLM accessible through Anthropic's APIs with roughly 130 billion parameters. Claude-2 costs about $\$ 0.0465$ per thousand tokens.
4.1.2 LLM Fine-tuning. The availability of Rust code in open source is scarce as compared code written in most other programming languages. Incoder [23] estimates that Rust is only the 20th most common language in their training database, which is a 159 GB code corpus taken from Github BigQuery ${ }^{3}$. Due to the lack of Rust data available on open-source, we opt to not train an LLM targeted at Rust code generation. Instead, we directly use an off-the-shelf industry grade LLM, and also fine-tune on a separate open-source pretrained LLM. Specifically, we use Anthropic Claude-2 ${ }^{4}$ for the industry grade LLM, and StarCoder [31] for the pretrained LLM.</li>
</ul>
<p>We use light weight and parameter efficient adapter layers [25, 33, 58] for fine-tuning StarCoder. Instead of retraining StarCoder entirely, we taken the final hidden states of StarCoder and add adapter layers at the end using small amounts of data. We collect 94 LeetCode type question solutions in C, C++, Go and Rust. Although there are existing code bases for all four languages, we</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>find that LeetCode has the most consistent translation between other languages and Rust. We were able to collect 94 LeetCode questions of which have a direct translation between all 3 languages. For each LeetCode type question, we have a corresponding source program (written in Go, C, or C++ ), and a target program (written in Rust). We encode all code words into tokens using the GPT-2 tokenizer. We fine-tune with 4 Transformer layers, 300 total epochs, and a final model dimension of 6144 .
4.1.3 Benchmark selection. We draw our benchmarks from two sources. Our first source is the benchmark set from TransCoder-IR [45], which is primarily made up of competitive program solutions. In total, this benchmark set contains 852 C++ programs, 698 C programs, and 343 Go programs. We choose this dataset to avoid potential data-leakage (i.e., LLM memorization) [9] in our evaluation. We note that the Rust programs produced by TransCoder-IR were released after June 2022, which is the training data cutoff date of our chosen LLMs [4, 31, 42]. We select programs from the TransCoder-IR dataset that can directly compile to Wasm using rWasm. After filtering, we collect a benchmark set of 569 C++ programs, 506 C programs, and 341 Go programs. These types of benchmarks are common for evaluating LLMs' coding ability. However, the programs themselves often do not make extensive use of pointers, so they do not adequately challenge VERT's ability to generate safe Rust.</p>
<p>To provide insight into VERT's ability to write safe rust, we gather 14 additional pointermanipulating C programs from prior work on C to Rust transpilation [2, 21, 61]. We note, however, that the benchmarks in these prior works use open-source programs written before our chosen LLM's training data cutoff (June 2022). To avoid LLM data-leakage, we select and customize snippets from these C projects to transpile to Rust. We manually label the input output pairs for each snippet for verifying equivalence on the transpiled Rust programs. Many of the benchmarks we select involve multiple functions. The explicit goal when selecting benchmarks from these projects is to discover the limitations of VERT in terms of writing safe Rust, therefore we gather benchmarks of increasing complexity in terms of the number of pointer variables, and the number of functions in the benchmark. We present several complexity metrics for the benchmarks and discuss them in more detail in Section 4.2.</p>
<p>In total, we evaluate our approach on $\mathbf{5 6 9} \mathbf{C}++$ programs, $\mathbf{5 2 0} \mathbf{C}$ programs, and $\mathbf{3 4 1}$ Go programs.
4.1.4 Evaluation Metrics. Neural machine translation (NMT) approaches use metrics that measure token similarity between the expected output and the actual output produced by the LLM, in which a higher score indicates the two outputs have many tokens in common. While these approaches are often meaningful when applied to natural language, for programming languages, small differences in the expected output and actual output could result in different compilation or run-time behavior. Conversely, two programs that share very few tokens (and hence have a very low text similarity score) could have identical compilation or run-time behavior.</p>
<p>For programming languages, metrics based off of passing tests have been proposed. Roziere et al. [43] and Szafraniec et al. [45] use the computational accuracy (CA) metric, which counts a translation as correct if it passes a series of unit tests. However, there is no accepted standard for the number of required passing tests when using the CA metric. Furthermore, the CA metric does not take into account the quality or coverage of the unit tests.</p>
<p>To improve upon the metrics used for prior NMT approaches and remove the overhead of writing high-coverage unit tests, we use formal methods to measure the correctness of the output. We insert the LLM-generated code and rWasm-generated code in an equivalence-checking harness that asserts equal inputs lead to equal outputs. An example of such a harness is given in Figure 9. Our full procedure is shown in Fig. 11. Since the three metrics used are significantly slower than checking a series of unit tests, we set a time limit for our metrics. For all three metrics, we set a 120</p>
<p>seconds limit. For PBT, no counterexamples within 120 seconds counts as success. For Bounded and Full verification, success requires establishing verified equivalence within 120 seconds. If any of the three verification step fails, VERT terminates.
4.1.5 Environment. The experiments for all benchmarks were run on an Ubuntu 22 instance with 32 Intel Xeon 2.30 GHz processors, 240GB RAM, and 4 Tesla V100 GPUs.</p>
<h1>4.2 Results</h1>
<p>We present results on the TransCoder-IR benchmarks in Table 1. We present VERT operating in three different modes. Single-shot means that VERT uses the LLM once to create a single candidate transpilation, and then proceeds directly to verification. If verification fails, then VERT does not attempt to regenerate. Few-shot repair means that, if verification fails, then VERT will prompt the LLM to regenerate the transpilation repeatedly. In each iteration, we apply the syntactic repair described in Section 3.1 to the output of the LLM. Finally, Few-shot repair \&amp; counter examples means that we use counter examples produced by previous failed verification attempts as part of the LLM's few-shot learning, as described in Section 3.6. Few-shot repair \&amp; counter examples only works for instruction-tuned models. We re-prompt the LLM up to 20 times for few-shot modes. For each LLM and each mode of VERT, we report the number of transpilations that compiled and that passed the various verification modes. As seen in Table 1, we only perform single-shot for Transcoder-IR (baseline) to replicate results from prior work. We perform few-shot repair on CodeLlama2 and StarCoder fine-tuned to investigate the effectiveness of few-shot and rule-based repair on open-source, non-instruction tuned LLMs. Finally, we perform single-shot, few-shot repair, and few-shot repair with counter examples with Anthropic Claude-2 to investigate how each part of VERT impacts an instruction-tuned LLM's ability to perform Rust transpilation.</p>
<h2>RQ1. How does VERT perform vs. using the respective LLM by itself?</h2>
<p>As seen in table 1, VERT with Claude-2 compiles for $76 \%$ more programs for C++, $75 \%$ for C, and $82 \%$ for Go as compared to baseline (i.e., Transcoder-IR). VERT with Claude-2 can pass PBT for $49 \%$ more programs for C++, $37 \%$ for C, and $56 \%$ for Go as compared to baseline. VERT with Claude-2 can pass bounded verification for $40 \%$ more programs for C++, $37 \%$ for C, and $47 \%$ for Go as compared to baseline. For passing full verification, VERT with Claude-2 can transpile 19 C++ programs, 15 C programs, and 9 Go programs. Transcoder-IR cannot pass full verification on any of the tested programs. VERT with both CodeLlama2 and StarCoder fine-tuned also improve upon baseline on number of programs passing compilation, PBT, bounded verification, and full verification. We observe that few-shot learning with rule-based repair on general code-based LLMs can perform more accurate Rust transpilations than an LLM trained with transpilation as its main target.</p>
<p>To confirm that VERT yields a statistically significant improvement over baseline, we perform a Wilcoxon rank test [54], which indicates if the metric performance between VERT and baseline are statistically different. We use the Wilcoxon signed-rank test to see if the statistically significant difference is also positive (i.e., our approach is different and better as measured by our three metrics). We observe Wilcoxon signed-rank p-values ranging from $1 \times 10^{-5}$ to $4 \times 10^{-5}$ for PBT, bounded-verification, and full-verification.</p>
<h2>RQ1 Summary</h2>
<p>VERT with CodeLlama2, StarCoder fine-tuned, and Anthropic Claude-2 can produce more PBT, bounded verification, and full verification passing Rust transpilations than baseline. In particular, VERT with Claude-2 can pass bounded verification for $40 \%$ more programs for C++, $37 \%$ for C, and $47 \%$ for Go as compared to baseline.</p>
<p>Table 1. VERT performance across with different LLMs and modes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM</th>
<th style="text-align: center;">Source <br> Lang</th>
<th style="text-align: center;">Technique</th>
<th style="text-align: center;">Compiled</th>
<th style="text-align: center;">PBT</th>
<th style="text-align: center;">Bounded-ver.</th>
<th style="text-align: center;">Full-ver.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transcoder <br> IR <br> (Baseline)</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{C}++ \ &amp; (569) \end{aligned}$</td>
<td style="text-align: center;">Single-shot</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \mathrm{C} \ (520) \end{gathered}$</td>
<td style="text-align: center;">Single-shot</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \hline \mathrm{Go} \ (341) \end{gathered}$</td>
<td style="text-align: center;">Single-shot</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">CodeLlama2 $13 B$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{C}++ \ &amp; (569) \end{aligned}$</td>
<td style="text-align: center;">Few-shot repair</td>
<td style="text-align: center;">307</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \mathrm{C} \ (520) \end{gathered}$</td>
<td style="text-align: center;">Few-shot repair</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \hline \mathrm{Go} \ (341) \end{gathered}$</td>
<td style="text-align: center;">Few-shot repair</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">StarCoder fine-tuned $15.5 B$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{C}++ \ &amp; (569) \end{aligned}$</td>
<td style="text-align: center;">Few-shot repair</td>
<td style="text-align: center;">253</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \hline \mathrm{C} \ (520) \end{gathered}$</td>
<td style="text-align: center;">Few-shot repair</td>
<td style="text-align: center;">179</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \hline \mathrm{Go} \ (341) \end{gathered}$</td>
<td style="text-align: center;">Few-shot repair</td>
<td style="text-align: center;">134</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Anthropic <br> Claude-2 <br> 130B</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{C}++ \ &amp; (569) \end{aligned}$</td>
<td style="text-align: center;">Single-shot <br> Few-shot repair <br> Few-shot repair \&amp; counter examples (VERT)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 240 \ &amp; 539 \ &amp; 539 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 55 \ &amp; 292 \ &amp; 295 \end{aligned}$</td>
<td style="text-align: center;">$\begin{gathered} 6 \ 41 \ 233 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 2 \ 19 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \mathrm{C} \ (520) \end{gathered}$</td>
<td style="text-align: center;">Single-shot <br> Few-shot repair <br> Few-shot repair \&amp; counter examples (VERT)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 239 \ &amp; 339 \ &amp; 339 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 49 \ &amp; 195 \ &amp; 209 \end{aligned}$</td>
<td style="text-align: center;">$\begin{gathered} 6 \ 29 \ 193 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 4 \ 15 \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{gathered} \text { Go } \ \text { (341) } \end{gathered}$</td>
<td style="text-align: center;">Single-shot <br> Few-shot repair <br> Few-shot repair \&amp; counter examples (VERT)</td>
<td style="text-align: center;">$\begin{aligned} &amp; 126 \ &amp; 276 \ &amp; 317 \end{aligned}$</td>
<td style="text-align: center;">$\begin{gathered} 26 \ 157 \ 195 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 3 \ 39 \ 159 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 0 \ 4 \ 9 \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>RQ2. How does each component of VERT impact its performance? Table 1 shows the transpilation results across CodeLlama-2 and StarCoder fine-tuned in a few-shot setting. We observe that VERT with CodeLlama-2 and StarCoder fine-tuned improve over Transcoder slightly for compilable Rust translations. Since Rust is an underrepresented language in all LLMs trained on</p>
<p>Table 2. VERT's average runtime per component for a Single-program translation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component type</th>
<th style="text-align: left;">Component</th>
<th style="text-align: right;">Time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Transcoder-IR</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">CodeLlama-2</td>
<td style="text-align: right;">43</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Starcoder fine-tuned</td>
<td style="text-align: right;">45</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Anthropic Claude</td>
<td style="text-align: right;">30</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">rustc</td>
<td style="text-align: right;">$&lt;1$</td>
</tr>
<tr>
<td style="text-align: left;">Rust compilation</td>
<td style="text-align: left;">Error guided repair</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">rwasm</td>
<td style="text-align: right;">$&lt;1$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">PBT</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td style="text-align: left;">Testing and verification</td>
<td style="text-align: left;">Bounded-ver.</td>
<td style="text-align: right;">52</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Full-ver.</td>
<td style="text-align: right;">$\mathbf{6 7}$</td>
</tr>
</tbody>
</table>
<p>GitHub open-source repositories and The Stack dataset [28], we see that light-weight fine-tuning on a small dataset shows immediate improvement. In particular, we observe that StarCoder fine-tuned has fewer transpilations than CodeLlama-2 passing compilation, but more transpilations than CodeLlama-2 passing bounded verification. Fine-tuning with Rust code has an immediate impact on transpilation accuracy. StarCoder's results are limited by its ability to pass compilation, even with VERT's rustc error guided program repair in place. VERT with StarCoder fine-tuned compiles $47 \%$ fewer programs for C++, $41 \%$ fewer for C, and $63 \%$ fewer programs for Go as compared to VERT with Claude-2. While adding fine-tuning on Rust syntax increases the number of compilable translation generated, we observe that an industry-grade LLM with more trainable parameters and a larger training dataset performs significantly better for our metrics.</p>
<p>We observe that VERT using few-shot plus repair with either StarCoder fine-tuned or Claude-2 yields better transpilation across all our three languages and three metrics. In particular, few-shot plus repair with Claude-2 passes $43 \%$ more PBT checks for C++, $46 \%$ more for C, and $43 \%$ more for Go as compared to single-shot with Claude-2. Table 1 does not show single-shot results for CodeLlama-2 and StarCoder fine-tuned as we observed no transpilations passing PBT. Few-shot plus repair with Claude-2 passes $6 \%$ more bounded-verification checks for C++, $4 \%$ more for C, and $12 \%$ more for Go as compared to single-shot with Claude-2. We find that the few-shot prompting for Claude-2 yields a greater improvement over single-shot compared to our repair technique. For C++ and C in particular, few-shot and repair with Claude-2 does not provide any additional passes on bounded verification nor full verification as compared to only few-shot with Claude-2. We observe that few-shot learning with counter examples of failed previous verification attempts provides the largest improvements on both bounded-verification and full-verification. Modern LLMs that are instruction-tuned can learn to generate more correct program when given specific test failures in few-shot settings.</p>
<p>Table 2 shows the average runtime of each of VERT's components across our entire evaluation dataset. We observe that in the non-timeout failure cases (i.e., Kani does not establish equivalence within 120s), Kani's full verification (full-ver.) uses an average of 67 seconds per program. Kani's bounded verification uses an average of 52 seconds per program, and Bolero's property testing uses an average of 25 seconds per program. Of the LLMs, both CodeLlama-2 and StarCoder use about 3 seconds per each prompt attempt, and Anthropic Claude-2 about 2 seconds. Not counting the failure cases (i.e., the LLM does not generate any program that can pass equivalence after 20 attempts), we observe an average of 15 tries before the LLM can achieve compilation. Transcoder-IR</p>
<p>uses 8 seconds on average per transpilation, which we prompt only one time as the baseline of our evaluation.</p>
<h1>RQ2 Summary</h1>
<p>Our ablation study shows that fine-tuning an LLM with Rust yields a higher accuracy of transpiled programs, as seen by a higher number of programs passing PBT and bounded verification by StarCoder fine-tuned compared to CodeLlama2. However, few-shot learning with counter examples provides the largest improvements on transpilation accuracy. Finally, we observe that VERT spends most of its runtime in verification.</p>
<p>RQ3. Does VERT produce safe, readable, and idiomatic Rust transpilations? To measure VERT's ability to generate safe Rust, we use VERT few-shot + repair with Claude-2 to transpile the 14 C programs described previously. Table 3 presents the results of VERT on these 14 benchmarks as well as several metrics that provide a rough idea of the complexity of the benchmarks. Specifically, we present the number of pointer variables, function definitions, LoC, and the number of structs defined in each benchmark. The avl_<em> benchmarks are taken from a library that implements an AVL tree. The brotli_</em> benchmarks from the Brotli compression library. The buffer_<em> benchmarks allocate and resize a buffer respectively. The ht_</em> benchmarks compute a hash key, and create a hash table, respectively, the libcsv_* benchmarks initialize a struct with pointer variables, and retrieve members from the struct. libtree determines if an array of pointers to int64s is sorted. urlparser parses a url.</p>
<p>VERT can produce transpilations for 7 of the 14 C programs that pass PBT, and 2 of those can pass bounded verification. Two benchmarks cannot pass compilation due to Rust's borrow checker (ht_create and urlparser). In particular, VERT was unable to generate safe Rust on ht_create due to transferring a variable into byte representation in two lines of code.</p>
<p>The results show that VERT tends to struggle as the programs get larger, have more pointer variables, and also on programs with multiple functions. Still on smaller programs, the LLM can still determine basic ownership needs. For example, it can determine if a variable or parameter reference needs a mut permission. On the avl_insert benchmark, the LLM successfully assigns ownership to the newly created node. To evaluate the readability, we compare lines of code in the transpilations produced by VERT, rWasm, and CROWN [61], the rule-based C to Rust transpiler. After running rustfmt [44], the official formatter of Rust, CROWN's output is more than 5x larger than VERT, and rWasm's output more than 10x as large. Given the strong negative association between LoC and code readability [12], we conclude that VERT's outputs are more readable than CROWN and rWasm.</p>
<p>To evaluate the idiomaticity, we run Clippy ${ }^{5}$, the official linter of Rust, on VERT's transpilations. Clippy checks Rust code for potential issues in the categories of correctness (e.g. checking if an unsigned int is greater than 0), performance (e.g. unnecessarily using a Box or collection type), stylistic (e.g. not following naming conventions, unnecessary borrowing), and conciseness (e.g. unnecessary type casting or parentheses). On average, Clippy produces 10.9 warnings per function for CROWN, and 372 warnings per function for rWasm. Clippy does not produce any warnings on VERT's transpilations, thus we conclude that they are reasonably idiomatic.</p>
<p>VERT targets the broader and more difficult problem of general, verified translation to Rust, whereas CROWN only targets unsafe to safe rust (after running C2Rust [2]) without verification. For the 14 programs, both VERT and CROWN output safe Rust. However, VERT 's output is more Rust-native than CROWN's, using standard Rust types while CROWN and C2Rust use C-foreign</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3. Benchmark information and results on the 14 C pointer manipulation programs. The symbols $\checkmark, \times$, and $\ominus$ indicate pass, fail (with counterexample), and timeout.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Structs</th>
<th style="text-align: center;">Functions</th>
<th style="text-align: center;">Pointer <br> variables</th>
<th style="text-align: center;">LOC</th>
<th style="text-align: center;">Compiled</th>
<th style="text-align: center;">PBT</th>
<th style="text-align: center;">Bounded <br> Ver.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">avl_minvalue</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">avl_insert</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\ominus$</td>
</tr>
<tr>
<td style="text-align: left;">avl_rotate</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\ominus$</td>
</tr>
<tr>
<td style="text-align: left;">avl_delete</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">brotli_parse_int</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\ominus$</td>
</tr>
<tr>
<td style="text-align: left;">brotli_filesize</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">buffer_new</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\ominus$</td>
</tr>
<tr>
<td style="text-align: left;">buffer_resize</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">ht_hashkey</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">ht_create</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">libcsv_get_opts</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">libcsv_init</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: left;">libtree</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\ominus$</td>
</tr>
<tr>
<td style="text-align: left;">urlparser</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">158</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
</tbody>
</table>
<p>types/functions. VERT 's lack of reliance on C-foreign functions is a qualitative strength. VERT 's output is more self-contained and reviewable to Rust programmers [7]. VERT can catch buggy C API calls in the input program instead of translating the incorrect API calls to Rust libc :: calls that remain buggy. Finally, we note that CROWN assumes correctness on their output, and only runs deterministic test suites on 6 example benchmarks (corresponding to $4 / 14$ of our selected pointer benchmarks). VERT performs three layers of equivalence checking on all its output.</p>
<h1>RQ3 Summary</h1>
<p>VERT can produce transpilations for 7 of the 14 C programs that pass PBT, and 2 of those can pass bounded verification. VERT tends to struggle as programs have more pointer variables, or have multiple functions. However, VERT is far more readable than prior work. VERT produces 5X less LoC than CROWN, 10x less LoC than rWasm, and its transpiled Rust programs do not show any linter warnings.</p>
<p>RQ4. How extensible is VERT to future verifiers? We observe in Table 1 that few transpiled Rust programs can pass full-verification with Kani, which is a bounded model checker (BMC). Full-verification using a BMC results in complete unrolling of a program, which does not scale to programs that loop over their inputs. We consider using Verus [30] as the verifier instead of Kani. Given that the verification failures are due to Kani unrolling loops, we use Verus to specify loop invariants and algebraic specifications for proving equivalence.</p>
<p>VERT handles multiple verifiers for the equivalence checking step. This is useful when different verifiers have different strengths and weaknesses. For example, Kani is a bounded model checker so loops are difficult to verify. Verus is a autoactive verifier [30], so it can verify loops more effectively via invariants, but at the cost of lower automation. To understand the need for an autoactive verifier, we run lightweight analysis using regex matching on our benchmarks that Kani failed to verify. $86 \%$ of the benchmarks had explicit iteration over an input. Furthermore, this is an undercount because it ignores loops that happen in API calls and library functions like strcpy.</p>
<p>Table 4. We manually verify 5 timeout cases in RQ4 using Verus. LoC is the lines of code while Spec. Loc is the lines of specification. No Spec. LoC is given when verification is not successful. The symbols $\checkmark$ and indicate pass and feature limitation respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Code LoC</th>
<th style="text-align: center;">Spec. LoC</th>
<th style="text-align: center;">Verus Ver.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">avl_insert</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">avl_rotate</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">brotli_parse_int</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">buffer_new</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">libtree</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Given the manual effort required to manually verify each of the LLM's outputs, we limit ourselves to the 5 cases in the CROWN benchmark where PBT succeeded but bounded Kani failed. The results are show in the Table 4. We succeed in verifying equivalence for 3 of the 5 failed benchmarks while we fail to verify AVL benchmarks due to Verus's limitations on returning mutable pointers. In each of the successful cases, checking equivalence required fully specifying and verifying the resulting programs. Given that, the spec to code ratio varied substantially depending on the benchmark. For brotli_new the program simply allocates a large buffer that caused Kani to time out. So the specification is a one line, and would have probably passed with Kani using the new contract feature or with a lower buffer size. libtree benchmark is a function that, given a list of integers, checks if it is sorted. This one required a quantifier-based specification to assert an invariant over the range of the array that has checked as sorted. While the spec size is not substantial, the presence of quantifiers over unbounded arrays will have been difficult to specify with Kani. The final and heaviest benchmark is brotli_parseint, which required 92 lines of specification and supporting proof. This function parses an integer from an array of chars, and specifying that required recursive specs involving exponentiation that would be difficult with Kani. The sheer size of the spec also stems from proofs required to show that the spec is well-behaved while recursing over the array. Overall ratio of spec to code is 2.6 .</p>
<p>We go over brotli_parseint detail to describe the supporting specification proofs for equivalence. We give the specification used to prove brotli_parseint function correct in Fig. 12. The main specification is right_parse, which defines parsing of the array from right to left upto tthe given index. The induction proof right_parse_continues extends this specification over the array of char. We also prove the absence of arithmetic overflows in the output program by relating the maximum array length to the size of the parsed integer. We omit the precise specification for conciseness.</p>
<p>We give the LLM's output and supporting specifications in Fig. 13. We use the ensures to specify that a valid vector parses correctly and invalid or out-of-range values are not returned. The main loop verifies through the invariant that the integer parsed out is correct up to the current index of the array. For this to hold, we also trivially specify that the vector remains valid through the loop. Finally, we put an invariant that the integer parsed is below $10^{i}$, which we use to show no integer overflow occurs. The rest of the requirements dispatches naturally through the conditional cases. We note that full specification, while possible, is not compatible with the automatic nature of the tool. We expect that further improvements, such as automatic invariant synthesis through LLMs [59] might make this approach more amenable to users who have neither the specification nor the formal methods expertise to use tools like Verus.</p>
<div class="codehilite"><pre><span></span><code><span class="n">spec</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="n">right_parse</span><span class="p">(</span><span class="nl">s</span><span class="p">:</span><span class="w"> </span><span class="o">&amp;</span><span class="n">Vec</span><span class="o">&lt;</span><span class="nc">char</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="nl">upto</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="err">:</span><span class="w"> </span><span class="nc">int</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">bool</span>
<span class="w">    </span><span class="n">decreases</span><span class="w"> </span><span class="n">upto</span><span class="p">,</span>
<span class="err">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">upto</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">upto</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="nf">len</span><span class="p">()</span><span class="w"> </span><span class="err">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">upto</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="err">{</span>
<span class="w">            </span><span class="p">(</span><span class="k">value</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="p">(</span><span class="n">s</span><span class="o">[</span><span class="n">upto - 1</span><span class="o">]</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">u32</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="s1">&#39;0&#39;</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">u32</span><span class="p">))</span><span class="w"> </span><span class="o">&amp;&amp;</span>
<span class="w">                </span><span class="n">right_parse</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">upto</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="p">)</span>
<span class="w">            </span><span class="err">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="k">value</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="err">}</span>
<span class="w">    </span><span class="err">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="err">{</span><span class="k">false</span><span class="err">}</span>
<span class="err">}</span>
<span class="n">proof</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="n">right_parse_continues</span><span class="p">(</span><span class="nl">s</span><span class="p">:</span><span class="w"> </span><span class="o">&amp;</span><span class="n">Vec</span><span class="o">&lt;</span><span class="nc">char</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="nl">upto</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="err">:</span><span class="w"> </span><span class="nc">int</span><span class="p">)</span>
<span class="w">    </span><span class="n">requires</span>
<span class="w">        </span><span class="n">valid_vector</span><span class="p">(</span><span class="n">s</span><span class="p">),</span>
<span class="w">        </span><span class="n">right_parse</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">upto</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="p">),</span>
<span class="w">        </span><span class="mi">0</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">upto</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="nf">len</span><span class="p">(),</span>
<span class="w">    </span><span class="n">ensures</span>
<span class="w">        </span><span class="n">right_parse</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">upto</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="k">value</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">((</span><span class="n">s</span><span class="o">[</span><span class="n">upto</span><span class="o">]</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">u32</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="s1">&#39;0&#39;</span><span class="w"> </span><span class="k">as</span>
<span class="w">                </span><span class="n">u32</span><span class="p">));</span>
<span class="err">{</span><span class="w"> </span><span class="cm">/* Verus figures out by definition you</span>
<span class="cm">can append a digit by parsing one more char. */</span><span class="err">}</span>
</code></pre></div>

<p>Fig. 12. Specification of brotli_parseint. We prove this spec on both the LLM and reference programs.</p>
<h1>RQ3 Summary</h1>
<p>We manually verify 5 programs with Verus that previously timed-out using Kani. We succeeded in verifying equivalence for 3 of the 5 failed benchmarks by leveraging loop invariants.</p>
<h2>5 RELATED WORK</h2>
<h3>5.1 Language model transpilers</h3>
<p>Recent advances in language modelling using code as training data have shown that LLMs can perform code completion [20] and generate code based on natural language [40] with impressive effectiveness. Large Language Models (LLMs) have raised performance on these tasks using significantly more trainable parameter and training data [13]. Transcoder and Transcoder-IR [43, 45] use unsupervised machine translation to train neural transcompilers. As both Transcoder versions are trained on program translation pairing, they perform better on program translation tasks than similar sized but generic auto-regressive LLMs.</p>
<p>However, recent work shows that LLMs can generate buggy and vulnerable programs [10, 13, 39]. Transcoder-IR [45] show in their evaluation that a significant portion of their translated code do not compile, especially for target programming languages that are underrepresented in their training data (e.g., Rust). Our work helps both generate memory-safe code and establish equivalence, to safely harness the code output of an LLM.</p>
<h3>5.2 Rust transpilers</h3>
<p>Prior Rust transpilers convert C/C++ to Rust. C2Rust [2] automatically converts large-scale C programs to Rust while preserving C semantics. Citrus [3] and Bindgen [1] both generate Rust FFI bindings from C libraries, and produce Rust code without preserving C semantics. Bosamiya et al. [11] embedded WebAssembly (Wasm) semantics in safe Rust code for the Rust compiler to emit safe and executable Rust code. Bosamiya et al. [11] implemented all stages of their tool in safe Rust,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://doc.rust-lang.org/clippy/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-739 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-739</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-739</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-250626996</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2207.08457v2.pdf" target="_blank">A Meta-Reinforcement Learning Algorithm for Causal Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Causal discovery is a major task with the utmost importance for machine learning since causal structures can enable models to go beyond pure correlation-based inference and significantly boost their performance. However, finding causal structures from data poses a significant challenge both in computational effort and accuracy, let alone its impossibility without interventions in general. In this paper, we develop a meta-reinforcement learning algorithm that performs causal discovery by learning to perform interventions such that it can construct an explicit causal graph. Apart from being useful for possible downstream applications, the estimated causal graph also provides an explanation for the data-generating process. In this article, we show that our algorithm estimates a good graph compared to the SOTA approaches, even in environments whose underlying causal structure is previously unseen. Further, we make an ablation study that shows how learning interventions contribute to the overall performance of our approach. We conclude that interventions indeed help boost the performance, efficiently yielding an accurate estimate of the causal structure of a possibly unseen environment.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e739.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e739.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-Causal Discovery (MCD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-reinforcement-learning algorithm that learns a policy to perform budgeted interventions and to maintain/update an epistemic causal graph estimate online, producing fast causal-structure estimates at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Meta Causal Discovery (MCD)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MCD models a causal-discovery algorithm as an RL policy (trained with ACER) that at each timestep either issues an intervention (listening action do(X_i = c) or a non-action) or edits the current epistemic graph (structure actions: add/delete/reverse an edge). Observations include current variable values, a one-hot intervention target, an encoding of the agent's current epistemic model, and time-to-horizon; an LSTM in the policy aggregates history so the policy can reason over sequences of interventions/observations. Training reward is negative directed Structural Hamming Distance (dSHD) at episode end, plus a small intervention bonus. The policy is trained across many synthetic SCM environments (linear additive with Gaussian noise) so that at test time a frozen policy can rapidly (milliseconds) infer new graphs with few interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic SCM virtual lab (episodic interactive SCM environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Each episode samples an SCM (a DAG plus functional relations and noise). The environment is interactive and open to active experimentation: the agent chooses interventions (do(X_i = c)) and receives samples from the resulting post-interventional distribution; episodes have a fixed horizon (limits interventions). Observations are sampled per timestep from the SCM and the agent updates its epistemic model online.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit active interventions to disambiguate spurious observational correlations/observational equivalence; aggregation of intervention outcomes via an LSTM to accumulate evidence; training-time randomization of noise variances to reduce varsortability-related spurious signals; structural editing actions to remove edges contradicted by interventional data.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Observational equivalence (structures indistinguishable from observational data), varsortability-induced spurious orderings, confounding implied by observational correlations that are resolved by interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detects spurious/incorrect edges by comparing post-interventional samples to the expectations under the current epistemic model: the policy inspects changes in marginal/post-interventional distributions (encoded in observations) and, via LSTM-aggregated history, infers which edges are inconsistent with interventional outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Direct intervention (do(X_i = c)) to break parent-dependencies; if post-interventional observations contradict current edge hypotheses the agent applies structure-actions (delete/reverse/add) to revise the epistemic graph; final correctness is reinforced via dSHD reward.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Learned policy (ACER) actively selects which variable to intervene on (listening actions) and when to stop, balancing horizon-limited intervention budget; the policy conditions on history, current epistemic graph encoding, variable values and remaining time to choose informative interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On the 3-variable synthetic SCM test set (best model, frozen weights): mean dSHD = 1.28, median dSHD = 1.0, std = 0.66; inference latency ≈ 23 ms per estimation (3 vars) and ≈ 30 ms (4 vars); average ~17 interventions per episode in 3-variable experiments (horizon 20).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to the observational-only variant (MCD-O): MCD-O mean dSHD = 2.6, median = 3.0, std = 1.44 on same 3-variable test set; experiments show MCD (with interventions) is significantly better (Wilcoxon p << 0.025).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active, learned intervention selection is the principal mechanism MCD uses to detect and refute spurious observational correlations and observational equivalences; interventions substantially improve structural accuracy (dSHD) versus an observational-only variant, while using far fewer interventional samples than baselines; training across many SCMs yields a fast, generalizable causal-discovery policy that requires a small intervention budget.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e739.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCD-O</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MCD observational-only variant (MCD-O)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of MCD trained/operated without intervention actions (observational data only), sharing the same architecture but constrained to collect pure observational samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MCD-O (observational-only)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Same meta-RL architecture and epistemic-graph representation as MCD but intervention/listening actions disabled (agent only collects observational samples). The policy still updates a graph estimate via structure actions but has no capacity to perform do-interventions to resolve ambiguities.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic SCM virtual lab (observational-only sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic SCM environment where the agent is restricted to non-intervention observations (I = ∅), so structure must be inferred solely from observational distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Observational confounding and observational equivalence remain unresolved without interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Measured on 3-variable test SCMs: mean dSHD = 2.6, median = 3.0, std = 1.44 (worse than MCD which uses interventions). Comparable to NOTEARS in tests (no significant difference vs NOTEARS).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Removing interventions degrades structural recovery; MCD-O does not outperform strong observational baselines (NOTEARS), confirming that the main robustness to spurious observational signals in MCD comes from active interventions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e739.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGs with NO TEARS: Continuous Optimization for Structure Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuous optimization (score-based) method for DAG structure learning using an acyclicity constraint, applied to observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAGs with NO TEARS: Continuous Optimization for Structure Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Optimization-based algorithm that formulates structure learning as a continuous constrained optimization problem enforcing acyclicity via a smooth constraint; in this paper used as an observational-data benchmark (trained on sampled observational data only).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic SCM benchmark datasets (observational samples)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive evaluation: NOTEARS receives observational sample sets drawn from SCMs (no active interventions by the method in the experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NOTEARS is an observational-only baseline; in the paper it underperforms the intervention-enabled MCD, especially in low-sample or few-intervention regimes, illustrating the value of active experimentation for refuting spurious observational signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e739.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENCO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient neural causal discovery without acyclicity constraints (ENCO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural (neuro-causal) approach to causal discovery that can integrate observational and interventional data and avoids explicit acyclicity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient neural causal discovery without acyclicity constraints</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ENCO</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A neural-network-based causal discovery method that jointly estimates graph structure and parameters and can incorporate observational and interventional samples; used as a benchmark here with both observational and interventional sample sets provided to the algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic SCM benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>ENCO is applied offline to datasets sampled from SCMs, including observational and post-interventional samples (but ENCO itself is not used to actively select which interventions to run in the paper's experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ENCO can ingest interventional data (unlike NOTEARS) but in the paper's experiments its performance showed unexpected anomalies (authors note ENCO performed worse than expected and behaved oddly when sample counts were reduced); nevertheless ENCO is treated as an interventional-data-capable benchmark against which MCD is compared.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e739.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DCDI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Causal Discovery from Interventional Data (DCDI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A differentiable/optimization-based approach for causal discovery that can incorporate interventional data using parameterized generative models (e.g., deep sigmoidal flows).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Differentiable causal discovery from interventional data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DCDI</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A differentiable causal-discovery method that trains generative models (e.g., deep flows) and can be trained on both observational and interventional datasets; in the paper used as a benchmark with interventional samples provided.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic SCM benchmark with interventional samples</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Offline application to sampled datasets (interventional data provided explicitly), not actively choosing interventions in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DCDI can integrate interventional data but in the paper it requires substantially more interventional samples than MCD to attain comparable structural accuracy; in low-sample regimes its performance drops relative to MCD.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e739.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dasgupta et al. (2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal reasoning from meta-reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior meta-reinforcement-learning study showing that meta-RL agents can learn causal reasoning behaviors from experience; cited as related work with a similar meta-RL flavor but not focused primarily on structure discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal reasoning from meta-reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Meta-RL causal reasoning (Dasgupta et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Meta-RL setup where an agent learns to perform causal reasoning (interventions and inference) from experience; in this paper cited as related work and described as similar in spirit but with a different primary task.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as demonstrating meta-RL can produce causal-reasoning behaviors; not evaluated or used directly in experiments of this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e739.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scherrer et al. (2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning neural causal models with active interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-learning approach that chooses interventions efficiently to estimate causal structure; cited in this paper as closely related work on active selection of interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning neural causal models with active interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Active intervention selection (Scherrer et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Develops neural causal modeling combined with active selection of interventions to improve structure learning efficiency; cited as related work that actively chooses interventions to estimate structure from data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Active selection of intervention targets to maximally inform structure learning (paper cited as doing efficient intervention selection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a work that focuses on learning with active interventions; relevant for approaches aiming to refute spurious correlations via designed experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e739.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Amirinezhad et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning of causal structures with deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-based active-learning method that learns a heuristic to choose the next intervention target, but uses a predefined graph-updating procedure and does not model variable values/distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active learning of causal structures with deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RL intervention-heuristic (Amirinezhad et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses RL to learn a heuristic for selecting next intervention targets (active learning), but the graph-update rules are hand-designed and the method does not incorporate the actual variable values or distributions into the update decisions (cited by the paper as complementary but less expressive than MCD).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Learned heuristic to pick next intervention target; does not adapt graph updates based on observed post-interventional value distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as similar in setup (active intervention selection with RL) but limited by predefined graph-update rules and ignoring variable values/distributions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e739.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nair et al. (2019)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal induction from visual observations for goal directed tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A work using causal induction from visual observations to support goal-directed tasks; cited as related research that connects visual/interactive observations to causal inference but typically learns structures in a supervised manner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal induction from visual observations for goal directed tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal induction from visual observations (Nair et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses visual observational data to induce causal relations for goal-directed tasks and combines this with RL; cited as related work where structure learning is typically done in supervised ways rather than meta-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as work connecting visual observation to causal induction; relevant to interactive/virtual-lab settings where distractors may appear in raw perceptual inputs, but the paper does not detail distractor-handling here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e739.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e739.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tigas et al. (2022)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interventions, where and how? experimental design for causal models at scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study on experimental design at scale for causal models that develops strategies for choosing interventions efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interventions, where and how? experimental design for causal models at scale</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Experimental design for interventions (Tigas et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Develops methods for large-scale experimental design to choose where and how to intervene to learn causal structure efficiently; cited as related work on intervention design and active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Focus on experimental design choices for which interventions to perform at scale; related to MCD's goal of minimizing expensive interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as relevant literature on intervention design and active selection of experiments to improve robustness of causal discovery under constrained budgets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal reasoning from meta-reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning neural causal models with active interventions <em>(Rating: 2)</em></li>
                <li>Active learning of causal structures with deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Causal induction from visual observations for goal directed tasks <em>(Rating: 2)</em></li>
                <li>Interventions, where and how? experimental design for causal models at scale <em>(Rating: 2)</em></li>
                <li>Efficient neural causal discovery without acyclicity constraints <em>(Rating: 2)</em></li>
                <li>Differentiable causal discovery from interventional data <em>(Rating: 2)</em></li>
                <li>DAGs with NO TEARS: Continuous Optimization for Structure Learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-739",
    "paper_id": "paper-250626996",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "MCD",
            "name_full": "Meta-Causal Discovery (MCD)",
            "brief_description": "A meta-reinforcement-learning algorithm that learns a policy to perform budgeted interventions and to maintain/update an epistemic causal graph estimate online, producing fast causal-structure estimates at test time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Meta Causal Discovery (MCD)",
            "method_description": "MCD models a causal-discovery algorithm as an RL policy (trained with ACER) that at each timestep either issues an intervention (listening action do(X_i = c) or a non-action) or edits the current epistemic graph (structure actions: add/delete/reverse an edge). Observations include current variable values, a one-hot intervention target, an encoding of the agent's current epistemic model, and time-to-horizon; an LSTM in the policy aggregates history so the policy can reason over sequences of interventions/observations. Training reward is negative directed Structural Hamming Distance (dSHD) at episode end, plus a small intervention bonus. The policy is trained across many synthetic SCM environments (linear additive with Gaussian noise) so that at test time a frozen policy can rapidly (milliseconds) infer new graphs with few interventions.",
            "environment_name": "Synthetic SCM virtual lab (episodic interactive SCM environments)",
            "environment_description": "Each episode samples an SCM (a DAG plus functional relations and noise). The environment is interactive and open to active experimentation: the agent chooses interventions (do(X_i = c)) and receives samples from the resulting post-interventional distribution; episodes have a fixed horizon (limits interventions). Observations are sampled per timestep from the SCM and the agent updates its epistemic model online.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit active interventions to disambiguate spurious observational correlations/observational equivalence; aggregation of intervention outcomes via an LSTM to accumulate evidence; training-time randomization of noise variances to reduce varsortability-related spurious signals; structural editing actions to remove edges contradicted by interventional data.",
            "spurious_signal_types": "Observational equivalence (structures indistinguishable from observational data), varsortability-induced spurious orderings, confounding implied by observational correlations that are resolved by interventions.",
            "detection_method": "Detects spurious/incorrect edges by comparing post-interventional samples to the expectations under the current epistemic model: the policy inspects changes in marginal/post-interventional distributions (encoded in observations) and, via LSTM-aggregated history, infers which edges are inconsistent with interventional outcomes.",
            "downweighting_method": null,
            "refutation_method": "Direct intervention (do(X_i = c)) to break parent-dependencies; if post-interventional observations contradict current edge hypotheses the agent applies structure-actions (delete/reverse/add) to revise the epistemic graph; final correctness is reinforced via dSHD reward.",
            "uses_active_learning": true,
            "inquiry_strategy": "Learned policy (ACER) actively selects which variable to intervene on (listening actions) and when to stop, balancing horizon-limited intervention budget; the policy conditions on history, current epistemic graph encoding, variable values and remaining time to choose informative interventions.",
            "performance_with_robustness": "On the 3-variable synthetic SCM test set (best model, frozen weights): mean dSHD = 1.28, median dSHD = 1.0, std = 0.66; inference latency ≈ 23 ms per estimation (3 vars) and ≈ 30 ms (4 vars); average ~17 interventions per episode in 3-variable experiments (horizon 20).",
            "performance_without_robustness": "Compared to the observational-only variant (MCD-O): MCD-O mean dSHD = 2.6, median = 3.0, std = 1.44 on same 3-variable test set; experiments show MCD (with interventions) is significantly better (Wilcoxon p &lt;&lt; 0.025).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Active, learned intervention selection is the principal mechanism MCD uses to detect and refute spurious observational correlations and observational equivalences; interventions substantially improve structural accuracy (dSHD) versus an observational-only variant, while using far fewer interventional samples than baselines; training across many SCMs yields a fast, generalizable causal-discovery policy that requires a small intervention budget.",
            "uuid": "e739.0"
        },
        {
            "name_short": "MCD-O",
            "name_full": "MCD observational-only variant (MCD-O)",
            "brief_description": "A variant of MCD trained/operated without intervention actions (observational data only), sharing the same architecture but constrained to collect pure observational samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "MCD-O (observational-only)",
            "method_description": "Same meta-RL architecture and epistemic-graph representation as MCD but intervention/listening actions disabled (agent only collects observational samples). The policy still updates a graph estimate via structure actions but has no capacity to perform do-interventions to resolve ambiguities.",
            "environment_name": "Synthetic SCM virtual lab (observational-only sampling)",
            "environment_description": "Interactive episodic SCM environment where the agent is restricted to non-intervention observations (I = ∅), so structure must be inferred solely from observational distributions.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Observational confounding and observational equivalence remain unresolved without interventions.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "Measured on 3-variable test SCMs: mean dSHD = 2.6, median = 3.0, std = 1.44 (worse than MCD which uses interventions). Comparable to NOTEARS in tests (no significant difference vs NOTEARS).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Removing interventions degrades structural recovery; MCD-O does not outperform strong observational baselines (NOTEARS), confirming that the main robustness to spurious observational signals in MCD comes from active interventions.",
            "uuid": "e739.1"
        },
        {
            "name_short": "NOTEARS",
            "name_full": "DAGs with NO TEARS: Continuous Optimization for Structure Learning",
            "brief_description": "A continuous optimization (score-based) method for DAG structure learning using an acyclicity constraint, applied to observational data.",
            "citation_title": "DAGs with NO TEARS: Continuous Optimization for Structure Learning",
            "mention_or_use": "use",
            "method_name": "NOTEARS",
            "method_description": "Optimization-based algorithm that formulates structure learning as a continuous constrained optimization problem enforcing acyclicity via a smooth constraint; in this paper used as an observational-data benchmark (trained on sampled observational data only).",
            "environment_name": "Synthetic SCM benchmark datasets (observational samples)",
            "environment_description": "Non-interactive evaluation: NOTEARS receives observational sample sets drawn from SCMs (no active interventions by the method in the experiments).",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "NOTEARS is an observational-only baseline; in the paper it underperforms the intervention-enabled MCD, especially in low-sample or few-intervention regimes, illustrating the value of active experimentation for refuting spurious observational signals.",
            "uuid": "e739.2"
        },
        {
            "name_short": "ENCO",
            "name_full": "Efficient neural causal discovery without acyclicity constraints (ENCO)",
            "brief_description": "A neural (neuro-causal) approach to causal discovery that can integrate observational and interventional data and avoids explicit acyclicity constraints.",
            "citation_title": "Efficient neural causal discovery without acyclicity constraints",
            "mention_or_use": "use",
            "method_name": "ENCO",
            "method_description": "A neural-network-based causal discovery method that jointly estimates graph structure and parameters and can incorporate observational and interventional samples; used as a benchmark here with both observational and interventional sample sets provided to the algorithm.",
            "environment_name": "Synthetic SCM benchmark",
            "environment_description": "ENCO is applied offline to datasets sampled from SCMs, including observational and post-interventional samples (but ENCO itself is not used to actively select which interventions to run in the paper's experiments).",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "ENCO can ingest interventional data (unlike NOTEARS) but in the paper's experiments its performance showed unexpected anomalies (authors note ENCO performed worse than expected and behaved oddly when sample counts were reduced); nevertheless ENCO is treated as an interventional-data-capable benchmark against which MCD is compared.",
            "uuid": "e739.3"
        },
        {
            "name_short": "DCDI",
            "name_full": "Differentiable Causal Discovery from Interventional Data (DCDI)",
            "brief_description": "A differentiable/optimization-based approach for causal discovery that can incorporate interventional data using parameterized generative models (e.g., deep sigmoidal flows).",
            "citation_title": "Differentiable causal discovery from interventional data",
            "mention_or_use": "use",
            "method_name": "DCDI",
            "method_description": "A differentiable causal-discovery method that trains generative models (e.g., deep flows) and can be trained on both observational and interventional datasets; in the paper used as a benchmark with interventional samples provided.",
            "environment_name": "Synthetic SCM benchmark with interventional samples",
            "environment_description": "Offline application to sampled datasets (interventional data provided explicitly), not actively choosing interventions in the experiments.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "DCDI can integrate interventional data but in the paper it requires substantially more interventional samples than MCD to attain comparable structural accuracy; in low-sample regimes its performance drops relative to MCD.",
            "uuid": "e739.4"
        },
        {
            "name_short": "Dasgupta et al. (2019)",
            "name_full": "Causal reasoning from meta-reinforcement learning",
            "brief_description": "A prior meta-reinforcement-learning study showing that meta-RL agents can learn causal reasoning behaviors from experience; cited as related work with a similar meta-RL flavor but not focused primarily on structure discovery.",
            "citation_title": "Causal reasoning from meta-reinforcement learning",
            "mention_or_use": "mention",
            "method_name": "Meta-RL causal reasoning (Dasgupta et al.)",
            "method_description": "Meta-RL setup where an agent learns to perform causal reasoning (interventions and inference) from experience; in this paper cited as related work and described as similar in spirit but with a different primary task.",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as demonstrating meta-RL can produce causal-reasoning behaviors; not evaluated or used directly in experiments of this paper.",
            "uuid": "e739.5"
        },
        {
            "name_short": "Scherrer et al. (2021)",
            "name_full": "Learning neural causal models with active interventions",
            "brief_description": "An active-learning approach that chooses interventions efficiently to estimate causal structure; cited in this paper as closely related work on active selection of interventions.",
            "citation_title": "Learning neural causal models with active interventions",
            "mention_or_use": "mention",
            "method_name": "Active intervention selection (Scherrer et al.)",
            "method_description": "Develops neural causal modeling combined with active selection of interventions to improve structure learning efficiency; cited as related work that actively chooses interventions to estimate structure from data.",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Active selection of intervention targets to maximally inform structure learning (paper cited as doing efficient intervention selection).",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as a work that focuses on learning with active interventions; relevant for approaches aiming to refute spurious correlations via designed experiments.",
            "uuid": "e739.6"
        },
        {
            "name_short": "Amirinezhad et al. (2022)",
            "name_full": "Active learning of causal structures with deep reinforcement learning",
            "brief_description": "An RL-based active-learning method that learns a heuristic to choose the next intervention target, but uses a predefined graph-updating procedure and does not model variable values/distributions.",
            "citation_title": "Active learning of causal structures with deep reinforcement learning",
            "mention_or_use": "mention",
            "method_name": "RL intervention-heuristic (Amirinezhad et al.)",
            "method_description": "Uses RL to learn a heuristic for selecting next intervention targets (active learning), but the graph-update rules are hand-designed and the method does not incorporate the actual variable values or distributions into the update decisions (cited by the paper as complementary but less expressive than MCD).",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Learned heuristic to pick next intervention target; does not adapt graph updates based on observed post-interventional value distributions.",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as similar in setup (active intervention selection with RL) but limited by predefined graph-update rules and ignoring variable values/distributions.",
            "uuid": "e739.7"
        },
        {
            "name_short": "Nair et al. (2019)",
            "name_full": "Causal induction from visual observations for goal directed tasks",
            "brief_description": "A work using causal induction from visual observations to support goal-directed tasks; cited as related research that connects visual/interactive observations to causal inference but typically learns structures in a supervised manner.",
            "citation_title": "Causal induction from visual observations for goal directed tasks",
            "mention_or_use": "mention",
            "method_name": "Causal induction from visual observations (Nair et al.)",
            "method_description": "Uses visual observational data to induce causal relations for goal-directed tasks and combines this with RL; cited as related work where structure learning is typically done in supervised ways rather than meta-RL.",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Mentioned as work connecting visual observation to causal induction; relevant to interactive/virtual-lab settings where distractors may appear in raw perceptual inputs, but the paper does not detail distractor-handling here.",
            "uuid": "e739.8"
        },
        {
            "name_short": "Tigas et al. (2022)",
            "name_full": "Interventions, where and how? experimental design for causal models at scale",
            "brief_description": "A study on experimental design at scale for causal models that develops strategies for choosing interventions efficiently.",
            "citation_title": "Interventions, where and how? experimental design for causal models at scale",
            "mention_or_use": "mention",
            "method_name": "Experimental design for interventions (Tigas et al.)",
            "method_description": "Develops methods for large-scale experimental design to choose where and how to intervene to learn causal structure efficiently; cited as related work on intervention design and active learning.",
            "environment_name": null,
            "environment_description": null,
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Focus on experimental design choices for which interventions to perform at scale; related to MCD's goal of minimizing expensive interventions.",
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as relevant literature on intervention design and active selection of experiments to improve robustness of causal discovery under constrained budgets.",
            "uuid": "e739.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal reasoning from meta-reinforcement learning",
            "rating": 2,
            "sanitized_title": "causal_reasoning_from_metareinforcement_learning"
        },
        {
            "paper_title": "Learning neural causal models with active interventions",
            "rating": 2,
            "sanitized_title": "learning_neural_causal_models_with_active_interventions"
        },
        {
            "paper_title": "Active learning of causal structures with deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "active_learning_of_causal_structures_with_deep_reinforcement_learning"
        },
        {
            "paper_title": "Causal induction from visual observations for goal directed tasks",
            "rating": 2,
            "sanitized_title": "causal_induction_from_visual_observations_for_goal_directed_tasks"
        },
        {
            "paper_title": "Interventions, where and how? experimental design for causal models at scale",
            "rating": 2,
            "sanitized_title": "interventions_where_and_how_experimental_design_for_causal_models_at_scale"
        },
        {
            "paper_title": "Efficient neural causal discovery without acyclicity constraints",
            "rating": 2,
            "sanitized_title": "efficient_neural_causal_discovery_without_acyclicity_constraints"
        },
        {
            "paper_title": "Differentiable causal discovery from interventional data",
            "rating": 2,
            "sanitized_title": "differentiable_causal_discovery_from_interventional_data"
        },
        {
            "paper_title": "DAGs with NO TEARS: Continuous Optimization for Structure Learning",
            "rating": 2,
            "sanitized_title": "dags_with_no_tears_continuous_optimization_for_structure_learning"
        }
    ],
    "cost": 0.0204615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Meta-Reinforcement Learning Algorithm for Causal Discovery Erman Acar Vincent François-Lavet
2023</p>
<p>Andreas Sauter 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>A Sauter@vu Nl 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>Erman Acar@uva Nl 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>Vincent Francoislavet@vu 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>Nl 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>Mihaela Van Der Schaar 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>Dominik Janzing 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>Cheng Zhang 
Vrije Universiteit Amsterdam
Universiteit van Amsterdam
Vrije Universiteit Amsterdam</p>
<p>A Meta-Reinforcement Learning Algorithm for Causal Discovery Erman Acar Vincent François-Lavet</p>
<p>Proceedings of Machine Learning Research
TBD2023Causal DiscoveryReinforcement LearningMeta-Learning
Uncovering the underlying causal structure of a phenomenon, domain or environment is of great scientific interest, not least because of the inferences that can be derived from such structures. Unfortunately though, given an environment, identifying its causal structure poses significant challenges. Amongst those are the need for costly interventions and the size of the space of possible structures that has to be searched. In this work, we propose a meta-reinforcement learning setup that addresses these challenges by learning a causal discovery algorithm, called Meta-Causal Discovery, or MCD. We model this algorithm as a policy that is trained on a set of environments with known causal structures to perform budgeted interventions. Simultaneously, the policy learns to maintain an estimate of the environment's causal structure. The learned policy can then be used as a causal discovery algorithm to estimate the structure of environments in a matter of milliseconds. At test time, our algorithm performs well even in environments that induce previously unseen causal structures. We empirically show that MCD estimates good graphs compared to SOTA approaches on toy environments and thus constitutes a proof-of-concept of learning causal discovery algorithms.</p>
<p>Context and Contribution</p>
<p>Many scientific questions, from "Why did this apple fall on my head?" to "Does more physical activity reduce the risk of cardiovascular diseases?", aim at answering questions about causal effects. The field of causality offers a framework to formalize these questions. Although causality has been researched for decades (Glymour et al., 1991;Spirtes et al., 2000;Pearl and Mackenzie, 2018), it has recently gained momentum in the context of machine learning (ML)  and, more specifically, reinforcement learning (RL).</p>
<p>Causal models carry the promise to allow ML models to go beyond correlation-based inference through capabilities such as counterfactual reasoning and reasoning about actions. While the inference power of causal models is impressive, estimating causal structure from data, also called causal discovery, poses several challenges. One big challenge lies in the fact that some causal structures cannot be distinguished from observational data alone (Hauser and Bühlmann, 2012). This issue can be mitigated by assigning values to variables independently from their causes (Pearl, 1993;Hauser and Bühlmann, 2012;Bareinboim et al., 2022), a process called intervention. Unfortunately, when confronted with real-world environments, performing interventions such as randomized controlled trials can be resource expensive or even impossible. Therefore, a large body of research exists on intervention design, or put it differently, on how to minimize the number of interventions needed to estimate the causal model.</p>
<p>With the successful application of RL algorithms to many domains (François-Lavet et al., 2018;Plaat et al., 2021;Moerland et al., 2023), the opportunity to use RL as a tool for causal discovery has opened as well. RL methods allow for sampling the environment as opposed to learning from a static data set of pre-collected observations. This interactive learning setting for data collection allows for estimating causal structures step-by-step considering always the newest data sample. This can be beneficial for online decisions e.g. on which variable to intervene based on how informative an intervention is for estimating a causal structure. Furthermore, an RL setting allows us to sample data from an environment, without being restricted to a fixed set of samples. This allows for better exploration of the data from an environment.</p>
<p>In this work, we show that it is possible to learn an algorithm (hence the term meta-learning) for causal discovery, called Meta Causal Discovery (MCD). More specifically, we sketch a metareinforcement learning model that estimates the causal structure of an environment with a given set of variables. The model is allowed to perform interventions with a limited budget to aid this process. The model simultaneously learns to perform informative interventions and to infer the updates to the structural model based on the resulting observations. During test time, the weights of the model are frozen; therefore, the model learns the causal structure only by utilizing its current network activations. Our work contributes to common challenges in causal discovery through the following capabilities:</p>
<p>• Providing good estimates of the ground-truth causal structure compared to the SOTA.</p>
<p>• Performing causal discovery in a matter of milliseconds.</p>
<p>• Integrating observational and interventional data for causal discovery.</p>
<p>• Limiting the number of interventions through a single hyper-parameter.</p>
<p>We will start by introducing necessary notations (Section 2). We will then proceed to provide an overview of the relevant literature on causal discovery leading to a discussion of the common challenges of the task (Section 3). Next, we describe our approach and our model in detail (Section 4). In a toy experiment, we show that our approach can learn to use interventions to distinguish causal structures (Section 5). We will conclude with experiments on the accuracy of our model w.r.t. SOTA approaches and a rigorous discussion thereof (Sections 6, 7, and 8). Figure 1 presents an overview of our approach. The implementation can be found at https://github.com/ sa-and/MCD</p>
<p>Preliminaries and Notation</p>
<p>Causal relationships can formally be expressed in terms of a structural causal model (SCM). We define an SCM S as a tuple (X , U, F, P) where X = {X 1 , . . . , X |X | } is the set of endogenous variables; U = {U 1 , . . . , U |U | } is the set of exogenous variables; F = {f 1 , . . . , f |X | } is the set of functions whose elements are defined as structural equations in the form of X i ← f i (.); P = {P 1 , . . . , P |U | } is a set of pairwise independent distributions where U i ∼ P i . Every SCM induces a graph structure G in which each node represents a random variable. For all nodes W i ∈ {X ∪ U}, X j ∈ X , the induced graph G has a directed edge (W i , X j ) iff W i is an input of f j . This  Figure 1: The learning setup of our approach. Green arrows describe processes that happen at every time step. Red arrows describe processes that happen at the end of each episode. At every time step, an observation is constructed. Based on this, our policy either performs an intervention on the environment or updates its estimate of the environment's causal structure. At the end of each episode, the estimated structure is compared to the ground truth, and the negative difference is provided as a reward. Then a new environment is sampled from the training set. During test time, the learned policy can be applied to previously unseen environments.</p>
<p>implies that every exogenous variable U i is a root node in G. In this work, we restrict ourselves to SCMs that induce a directed acyclic graph (DAG). An intervention on a variable X i ∈ X is defined as replacing the corresponding structural equation X i ← f i (.) with X i ← x for some value x, which we denote as do(X i = x). Intervening on a variable makes it independent of its parents and removes its incoming edges in G. The model is causal in the sense that one can derive the distribution of a subset X ⊆ X of variables following an intervention on a set of variables, called intervention target, I ⊆ X \ X . We call the resulting distribution over X post-interventional. When no intervention is performed (I = ∅) we call the resulting distribution an observational distribution.</p>
<p>We characterize our RL learning problem by a state-space S, an observation space Ω, an actionspace A, a reward function r(s) : S → R, and a policy π(h) : Ω t → A, where h is the history of observations up to time step t. We define an episode e as the state-action sequence from the beginning to the end of the estimation. We will refer to the length of the episode as horizon H. The value function V π (h) : Ω t → R defines the expected, discounted cumulative reward following a deterministic policy π, with discount factor γ. The objective of the RL agent is to find the optimal policy π * that maximizes the value function for all observations which can be expressed as π * (h) = argmax π V π (h), ∀h ∈ Ω t . We describe our approach as a meta-learning setup, since we use RL, to learn a policy that learns a causal structure of an environment.</p>
<p>Related Work</p>
<p>Due to its relevance in many applications, causal discovery research has gained momentum in the last years leading to an impressive body of work (Vowels et al., 2021). Score-based causal discovery approaches search the space of DAGs via metrics that indicate how well the graph fits the data. This is often done greedily over the space of classes of graphs in which the graphs can only be distinguished via interventions (Meek, 1997;Chickering, 2002;Hauser and Bühlmann, 2012;Ramsey et al., 2017), or over permutations of node orderings Yang et al., 2018). Constraint-based approaches leverage the statistical independence patterns in the data to constrain the possible output graphs (Glymour et al., 1991;Spirtes et al., 2000). These constraints can even be expressed as propositional formulas and then solved with answer-set programming (Hyttinen et al., 2014). RL offers an alternative way of searching the space of DAGs by using the reward to navigate toward good graph generators . Note that many algorithms rely on strong assumptions on the class of causal relations e.g. linear additive noise models (Bühlmann et al., 2014;Shimizu et al., 2006). This makes these algorithms interesting for theoretical analysis but it also restricts their application potential in practice.</p>
<p>Since the number of possible DAGs grows super-exponentially in the number of nodes (Robinson, 1977), most score-and constraint-based approaches suffer from long run times. A recent line of research tackles this problem by deploying optimization-based algorithms. These algorithms work e.g. with constraint optimization (Zheng et al., 2018;Brouillard et al., 2020) but also by learning causal graph neural networks (Goudet et al., 2018;Yu et al., 2019;Ton et al., 2021) or variational auto-encoders (Yang et al., 2021). For neuro-causal models, advances are also made in the theoretical analysis of their identifiability (Xia et al., 2021). A similar approach is taken by works that sample both the graph structures and the functional parameters from posterior distributions (Ke et al., 2019;Lippe et al., 2021;Scherrer et al., 2021;Lei et al., 2022). This improves learning efficiency, not only of the structures but also of the functional relations of the causal mechanisms. While optimization-based approaches can reduce the run-time for structure learning by avoiding a combinatorial explosion, they can still take significant time to learn the causal structure. In our work, we are shifting the computational complexity to training time to circumvent long runtimes at test time.</p>
<p>Another common challenge amongst most causal discovery algorithms is the integration of observational and interventional data. Although integrating frameworks exist (Mooij et al., 2020), only a fraction of causal discovery algorithms successfully jointly consider interventional and observational data (Vowels et al., 2021). A promising direction for the seamless integration of interventional data is by means of RL. We argue that this is partly due to the implicit connections between interventions and actions in any RL framework, and partly because RL can easily be combined with deep-learning models. Our work distinguishes itself from these closely related works in different ways. While Dasgupta et al. (2019) developed an algorithm that is similar to ours, their primary task was not causal discovery. Nair et al. (2019), Gasse et al. (2021), Lei et al. (2022), and Mendez-Molina et al. (2022) put a strong focus on using causal structures to aid RL while learning the structures is done in a supervised manner. Similarly, Scherrer et al. (2021) and Tigas et al. (2022) develop an active learning algorithm that chooses interventions more efficiently to estimate the structure from this data. Amirinezhad et al. (2022) have a similar setup and task but restrict RL to learn a heuristic function for choosing the next intervention target. Furthermore, they do not take into account the values and distributions of the random variables. Their graph-updating procedure is pre-defined, whereas in our approach the update rules are learned.</p>
<p>Meta-Reinforcement Learning Setup</p>
<p>Actions</p>
<p>We implement two types of discrete actions. The first type performs an intervention on the current environment SCM. This enables the policy to choose a (post-interventional) distribution to sample from. We will refer to this kind of action as listening action. All, except for one, of the listening actions are intervention actions that intervene on exactly one variable (i.e., |I| = 1). For each endogenous variable X ∈ X , we provide an action do(X = c) for a constant c. We argue that c should be chosen in a way that makes it easy to distinguish the post-interventional distribution from the observational distribution i.e. it should be unlikely that samples from the post-interventional distribution come from the observational distribution. A future expansion of our work could include learning a good c. The intervention actions amount to a total of n actions for n nodes. There is one additional listening action which we call the non-action. When the non-action is taken, the agent does not intervene (i.e., I = ∅). This action accounts for the collection of purely observational data.</p>
<p>The second type of action is responsible for maintaining the current causal structure estimate of the environment, which we call the epistemic model. We will refer to these actions as structureactions. Each structure action can either add, delete or reverse an edge of the epistemic model. Whenever a delete or reverse action is applied to an edge that is not present in the current model, the action is ignored. This is effectively equivalent to performing the non-action. The same holds when the add action is applied to an edge that is already in the epistemic model. We do not make any further restrictions, for instance, w.r.t. acyclicity for the structure actions.</p>
<p>For a graph with n nodes, there are n(n − 1) possible edges, and hence there are 3n(n − 1) structure actions. Together with the listening actions we have n + 1 + 3n(n − 1) actions. Therefore, the size of the action space is quadratic in the size of nodes.</p>
<p>Observation Space</p>
<p>In this Section, we describe how observations o ∈ Ω are constructed. Each environment is completely defined by an SCM. At each time step t, the exogenous variables U are sampled. The functions F are then evaluated according to the topological ordering of their corresponding nodes in G. This results in a sample of the endogenous variables X and makes up the first part o V t of the observation vector.</p>
<p>The second part of the observation, o A t is a one-hot vector that indicates the intervention target. If the i-th element of o A t is 1, then there is an intervention on X i . The third part of the observation, o G t encodes the current epistemic model as a vector. Each value of this vector represents an undirected edge in the graph. The edges in the vector are ordered lexicographically. The value 0 encodes that there is no edge between the two nodes. The value 0.5 encodes that there is an edge going from the lexicographically smaller node to the bigger node of the undirected edge. And the value 1 encodes that there is an edge in the opposite direction. For example, a 3-node graph X 0 → X 2 → X 1 would be encoded as o G t = [0, 0.5, 1]. The last element in the observation, o T t , encodes the time until the end of an episode normalized to 1 as
o T t = t H , where H is the horizon. The complete observation o t , is the concatenation of o V t , o A t , o G t ,
and o T t as shown in Figure 2. Taken together, the size of one observation is 2n + n(n − 1)/2 + 1 with n endogenous variables and hence quadratic in the size of the graph. The input to our policy is the history of observations from the beginning of the episode h t = o 0:t which we will approximate by implementing a recurrent policy (see Section 4.4).</p>
<p>Rewards and Episodes</p>
<p>Our task is to find the causal structure of the environment, i.e., the DAG that corresponds to the graph induced by the SCM of the environment. Therefore, we compare the epistemic model to the true causal structure of the environment. The quantification of this comparison serves as the reward for our algorithm. We count the edge differences between the two graphs. This ensures that generating a model that has more edges in common with the true DAG will be preferred over one which has fewer edges in common. It further gives a strong focus on causal discovery as opposed to scores based on causal inference. Specifically, we use a variant of the Structural Hamming Distance (SHD) (Tsamardinos et al., 2006). In this variant, we take two directed graphs and count how many of the edges need to be removed or added to transform the first graph into the second graph. This results in a metric that simply counts the distinguishing edges of two directed graphs. We will refer to this metric as directed SHD or dSHD. Given a predicted directed graph G P = (V, E P ) and a target, directed graph G T = (V, E T ), we define the dSHD as dSHD(E P , E T ) =| E P \E T | + | E T \E P |.</p>
<p>For each episode, we set a finite horizon H. The estimation of the epistemic model is complete when H −1 actions were taken. Dynamically determining the end of the estimations is left for future research. Note that when a small episode length is chosen, fewer samples can be collected by the agent. This might impact how well the agent is informed on which updates to make to the epistemic model. At the same time, H should not be set too large since additional learning complexity might be introduced. At the beginning of each episode, an SCM is sampled from the training set and the epistemic model of the agent is reset to a random DAG, to further introduce randomness.</p>
<p>The reward is calculated by taking the negative dSHD between the generated DAG and the true causal graph at the end of each episode. Every other step receives a bonus of 0.1 if an intervention action is performed. This will lead to an algorithm that performs more interventions. However, our main parameter to reduce interventions is the horizon, which puts a hard cap on the maximum number of interventions. We introduced the bonus for interventions because it worked well as a reward-shaping tool. The resulting value function for a history of observations h and a policy π is then defined as
V π (h) = E h∼π −γ H−t dSHD(E H Epi , E Env ) | h 0 = h + E h∼π γ t 0.11 I (h) | h 0 = h(1)
where E H Epi are the edges of the epistemic model at the end of an episode, E Env are the edges of the ground truth causal graph and 1 I (h) is the function that indicates whether there is an intervention in the latest observation in h. An optimal policy on this value function will construct an epistemic model that corresponds to the DAG induced by the causal structure of the current environment. The pseudocode of our overall learning setup can be seen in Algorithm 1. Algorithm 1: Pseudocode of our MCD learning algorithm. The returned policy π can be applied to new environments without re-training.</p>
<p>Input</p>
<p>Learning Algorithm and Policy Network</p>
<p>We use the Actor-Critic with Experience Replay (ACER) (Wang et al., 2016) algorithm to learn our policy. We choose this algorithm because of its sample-efficient off-policy method, its (potentially) easy extension to continuous action spaces, and because it worked well after preliminary experiments. We use a discount factor γ = 0.99, a buffer size of 500000, and a constant learning rate. All other parameters are according to the standard values of Stable-Baselines (Hill et al., 2018, version 2.10.1). The architecture of our policy network is sketched in Figure 2. Both, the actor-network and the critic-network are fully-connected multi-layer perceptrons (MLP). They are preceded by a shared MLP for feature extraction and a single LSTM layer (Hochreiter and Schmidhuber, 1997). We introduce the LSTM layer to allow the policy to estimate the history of observations as described in Section 2. With the LSTM, our policy can access an aggregated version of previous variableintervention pairs. This can help to disambiguate post-interventional distributions and has been shown to work in similar research (Dasgupta et al., 2019). The exact amounts of layers and their sizes are specified for each experiment.</p>
<p>Learning to Intervene</p>
<p>To test whether our approach can learn to perform the right interventions to identify causal models under optimal conditions we develop a toy example. To this end, we construct a simple setup in which two observationally equivalent, yet interventionally different environments have to be distinguished. This means that the causal structures of the two environments can only be distinguished by intervening on them (Bareinboim et al., 2022). Observing the values of the variables is not enough for distinguishing their structure. For this experiment, we disable the bonus reward for performing interventions. Thus, if the policy should distinguish the two environments, it has to learn that interventions are needed and that certain structures can be inferred from those interventions. The two environments are governed by SCMs with 3 endogenous variables X 1 , X 2 , X 3 , and structures G 1 : X 1 ← X 0 → X 2 and G 2 : X 0 → X 1 → X 2 . In both environments, the root node X 0 follows a normal distribution with X 0 ∼ N (µ = 0, σ = 0.1). The nodes X 1 and X 2 take the values of their parents in the corresponding graph. The resulting observational distributions P G 1 (X 0 , X 1 , X 2 ) and P G 2 (X 0 , X 1 , X 2 ) are equivalent and so are the post-interventional distributions after interventions on X 0 or X 2 . For an intervention on X 1 , P G 1 (X 0 , X 2 | do(X 1 = x)) = P G 2 (X 0 , X 2 | do(X 1 = x)). Hence the two SCMs can only be distinguished by intervening on X 1 . The details for the training setup can be found in Appendix A.1. The algorithm is trained and eval-uated in both environments, where at the beginning of each episode, one of the two environments is picked at random. This setup allows us to investigate whether, given enough training time and data, our approach can learn to distinguish observationally equivalent environments.</p>
<p>After training, we observe that the mean dSHD between the generated epistemic models and the ground truth graphs is 0.0 with a standard derivation of 0.0. This is a perfect reproduction of the two environments in all cases. This indicates that our policy has indeed learned to use the right intervention to find the true causal structure. For further testing, we apply the converged policy 10 times to each of the two environments and qualitatively analyze the behavior. What the resulting 20 episodes have in common is that, towards the beginning of each episode, they tend to delete edges that do not overlap in the two environments. Then an intervention on X 1 is performed. Depending on the outcome of the intervention, either G 1 or G 2 is ultimately generated. This can also be seen in the two hand-picked example episodes in Figure 3.  Figure 3: Illustration of two sample episodes after training with the respective causal environments G 1 and G 2 . Each step shows the current epistemic model, the current values of the three random variables, and the action which is chosen by the policy based on those observations. Interventions and their effects are highlighted in green. In steps 7-9 neither the epistemic model nor the resulting action changes.</p>
<p>These results show that our learned policy learns to use the intervention on X 1 to distinguish between the two environments. Thus, our approach is capable of learning to use interventions in an active manner and generating the appropriate graph from the resulting observations. If this can be successfully learned in more complex environments, our learned policy could potentially be used to discover new rules of causal structure estimation. Furthermore, these results suggest that the model has learned to only perform interventions that are relevant to causal discovery as opposed to random interventions.</p>
<p>Learning a Causal Discovery Algorithm</p>
<p>In this section, we investigate whether the agent can learn a causal discovery algorithm with the meta-learning setup described in Algorithm 1. More specifically, we train our policy on a set of SCMs where we sample a new SCM at every episode at random. If learning such a policy is successful, the policy accurately estimates the causal structure of these environments, even when the weights are frozen and the ground truth structure is unknown. We hypothesize that the learned policy is a causal discovery algorithm in itself, and can thus successfully be applied even in environments with previously unseen causal structures.</p>
<p>We test this by running the learned policy, with frozen weights, on a test set of SCMs and compare it to the SOTA causal discovery algorithms ENCO (Lippe et al., 2021), DCDI (Brouillard et al., 2020), NOTEARS (Zheng et al., 2018), and a baseline that generates a random DAG. Note that ENCO and DCDI can both integrate observational and interventional data while NOTEARS only uses observational data.</p>
<p>Following the widely adopted practice, we test our approach on SCMs that have an additive linear causal model with independent Gaussian noise. Although this choice limits the applicability to real-world environments, it provides a good means for comparison to other approaches. To make our approach more general, a variant could be learned in which the training SCMs have more general functional relations. It is known that environments with linear additive functions and Gaussian noise can suffer from varsortability where good results can be achieved by ordering the variables by the variance of their observational distribution (Reisach et al., 2021;Kaiser and Sipos, 2021). To make our approach less prone to this error, we randomly sample the variance of each Gaussian noise we use.</p>
<p>Given a structure G = (V, E) and X = V , we model our SCM environments as follows. Each exogenous variable U i follows a distribution P i = N (µ = 0, σ = Σ) where Σ is sampled from U nif orm([0; 0.5]) for every U i . For each endogenous variable X i , we model f i as
f i (P a G X , U i ) =   Y ∈P a G X W Y   + U i(2)
where P a G X are the parents of X in G, and W ∼ U nif orm([−1; 1]) represents a random weight for each causal effect of a parent to a child.</p>
<p>We randomly generate a test set of 7 DAGs with 3 variables and 200 DAGs with 4 variables. We generate 10 SCMs following Equation 2 for each of these graphs. During training, we generate a random ground truth DAG at the beginning of each episode. If this random DAG is in the test set, we discard it and sample a new random DAG. This process is repeated until the sampled DAG is not in the test set to ensure that our model has never seen the causal structures in the test set. When a training DAG is found, we generate an SCM as described above as our current environment. We chose to generate the training set this way, so our training set covers as much of the space of DAGs as possible. The training details for our policy can be found in Appendix A.2. We will refer to the best model that is found during training as best model.</p>
<p>To compare our approach, we used the following setup for the benchmarks. For NOTEARS (Zheng et al., 2020) we sampled 10000 samples from the observational distribution of each SCM. For ENCO (Lippe et al., 2021) we sampled 10000 samples from the observational distribution and 3333 samples from each post-interventional distribution (one per variable) and trained for 50 epochs.</p>
<p>For DCDI (Brouillard et al., 2020) we took 3333 samples from each post-interventional distribution as well and trained the deep sigmoidal flow model version of the algorithm for 50000 iterations. We use the original implementation from the authors of the corresponding papers. For each of the algorithms, we computed the dSHD between the predicted DAG and the ground truth DAG. Table  1 shows the results of running our best model (with frozen weights) and the benchmarks on the first 50SCMs in the test set. Firstly, Table 1 shows that our approach outperforms the random baseline, suggesting that MCD learns to estimate the environment's causal structure beyond randomly orienting edges. The means over the resulting dSHDs suggest that our approach compares favorably to the benchmarks. To investigate this difference in more detail, we performed a one-sided Wilcoxon signed-rank test between the estimates from our policy and the estimates from DCDI, ENCO, and NOTEARS. To correct for performing 3 comparisons, we consider a significance level of 1.7% instead of 5%. In the 3 variable case as well as the 4 variable case we can conclude that the dSHDs from our method are significantly lower (all p-values &lt; 1.7%) than the ones from any of the other algorithms. Please note that the results for ENCO are somewhat unexpected. Refer to Appendix B for an elaboration on the issue.</p>
<p>We note that each of these estimations of MCD takes an average of 23ms in the 3 variable case and 30ms in the 4 variable case on a consumer-grade notebook. This is in contrast to the SOTA, which can take minutes for one estimation. We attribute this performance to the fact that one estimation of MCD only takes H forward passes through the policy network, and that the computational complexity of our approach is shifted completely to training time.</p>
<p>We conclude that with our approach a causal discovery algorithm can be learned that interactively performs interventions and updates its structure estimate. Our algorithm not only compares favorably to the SOTA w.r.t. to the dSHD to the ground truth graph but is also computationally quick in deriving the estimate making it interesting for a variety of applications.</p>
<p>Contribution of Interventions</p>
<p>To empirically investigate the effect of interventions on the performance of our algorithm, we perform an ablation study. To this end, we train a variant of our policy (MCD-O) which is based on purely observational data, i.e. we disallow the use of interventions. We then compare our results to the results of MCD and NOTEARS, which also works on purely observational data and the random baseline of the previous section. We decided to compare MCD-O to NOTEARS, to investigate if, even in the observational setting, the learned policy constitutes a good causal discovery algorithm. We train our model with the same parameters as in Section 6 and measure the dSHD on the first 50 3-variable SCMs in the test set with the best model of the training run and frozen weights. We perform a Wilcoxon signed-rank test to evaluate whether there is a significant difference between the model that uses interventions and the one that does not. We also test whether there is a difference between NOTEARS and our approach when no interventions are allowed.</p>
<p>The statistics of MCD-O applied once on the first 50 test SCMs are as follows: mean=2.6, median=3.0, std=1.44. When comparing this to the version which uses interventions (mean=1.28, median=1.0, std=0.66, see Table 1), we can see the importance that interventions have on the overall performance of MCD. This is confirmed by performing a Wilcoxon signed-rank test between the results of MCD and MCD-O indicating that MCD is significantly better (with p &lt;&lt; 0.025). When comparing MCD-O with NOTEARS, we do not observe any significant difference in a two-sided Wilcoxon signed-rank test (p ∼ 0.4). In other words, while MCD-O does not provide an improvement over NOTEARS, it still constitutes a valid alternative approach. These results confirm that introducing interventions results in the hypothesized edge over the purely observational version of our model.</p>
<p>Aspects of Intervention Design</p>
<p>As argued in Section 1, MCD provides an approach to restrict the number of interventions needed to accurately discover the causal structure of an environment. The upper bound of interventions that the learned policy will perform is the horizon of an episode (20-30 in our experiments). Compared to the interventions used in the benchmarks (up to 10000 samples from the observational distribution and up to 3333 samples from the interventional distributions), this is a significant improvement considering the comparably good performance of MCD.</p>
<p>Empirically we see that our best model from section 6 performs an average of 17 interventions in the 3-variable environments. On average, 64% of the interventions were on the first variable and 36% on the third variable. No interventions were performed on the second variable in any of the runs. To investigate this behavior, we ran checkpoints of the model of earlier steps of the training and found that the model is performing interventions on the second variable in those checkpoints. We hypothesize that the model learns to not do this intervention because of the composition of the training SCMs.</p>
<p>To have a better comparison of the performance of MCD in a context where interventional samples are hard to obtain, we re-evaluate our approach w.r.t. SOTA. We run the benchmarks described in section 6 again, with a different number of samples to see how they perform under comparable sample sizes. For DCDI, we take 17 samples from each post-interventional distribution. For ENCO we take 17 samples from each post-interventional distribution and 4 samples from the observational distribution. For NOTEARS we take 20 observational samples. As before, we run the benchmarks on the first 50 SCMs in the test set of 3 variables and report the statistics of the dSHD to the ground truth graph. Table 2 shows the statistics over the dSHD obtained from running the corresponding algorithms on the 3 variable test SCMs. As expected, we see an increase in the performance gap between MCD and DCDI and MCD and NOTEARS. This indicates that their ability to perform well when few interventions are provided is limited for DCDI and NOTEARS. The better performance of MCD  w.r.t. ENCO can also still be seen, although ENCO seems to perform slightly better in this lowsample regime (see Appendix B for a discussion).</p>
<p>These investigations show that SOTA causal discovery algorithms rely on many samples to improve their prediction accuracy. At the same time, MCD generates more accurate DAGs w.r.t. the SOTA, while using only a fraction of samples. Furthermore, when confronted with a sample size that is similar to MCD, DCDI, and NOTEARS quickly drop in performance. Overall, we conclude that MCD uses interventions in an efficient way which makes it perform well even when the budget for interventions is low. The low number of interventions needed for MCD promises to make it more applicable than SOTA, especially in domains in which interventions are costly.</p>
<p>Conclusion</p>
<p>This paper presents an approach to learning a causal discovery algorithm. In our RL setting, we learn a policy that simultaneously learns to perform informative interventions and update an estimate of the causal structure of the environment. Once the policy is learned, it can be used to perform causal discovery even on environments whose structure it has not encountered during training in a matter of milliseconds. This is partly because of its ability to integrate interventional and observational data. By limiting the episode length, we put an upper bound on the number of interventions that can be performed by MCD, making it more suitable for applications where interventions are costly.</p>
<dl>
<dt>We acknowledge that our approach needs modifications to scale to realistic environments with more variables. The explosion of the action-and state-space that this would imply prompts considerations about better encodings. A further problem in a potential real-world setting is the availability of a large amount of data-generating models for training. To perform well on all the possible causal relations in the real world, the class of training SCMs would need to be significantly expanded. An alternative approach would be to make MCD transferable to SCM classes other than linearadditive SCMs. We argue that also an extension to a scenario in which the variables are learned from raw input would lead to even better applicability since hand-crafted variables often introduce sub-optimalities w.r.t. task performance.</dt>
<dd>
<p>test set of DAGs D, number of training episodes N , horizon H Output: causal discovery policy π 1 initialize policy π ; // (Section 4.4) 2 for ep ← 0; ep &lt; N ; ep ← ep + 1 do 3 generate random SCM S with G / ∈ D ; update([S.sample() + a.onehot() +Ĝ.encode() + [ steps H ]]) ; // Update observation history (Section 4.2) 14 if steps == H − 1 then 15 r ← r − dSHD(Ĝ, G) ; // CompareĜ to true G (</p>
</dd>
</dl>
<p>Figure 2 :
2An overview of our observations and policy network. The observation consists of the environment's current variable values, the intervention target, the encoding of the current epistemic model, and the remaining time as a fraction of the horizon. The actor and the critic share a fully connected feed-forward network with an additional LSTM layer that approximates the history of observations.</p>
<ul>
<li>Training :
TrainingTraining 
set of 
environments </li>
</ul>
<p>Agent Model </p>
<p>Policy update </p>
<p>Test: </p>
<p>Observation </p>
<p>. . . </p>
<p>Agent Policy </p>
<p>Structure 
Actions </p>
<p>= </p>
<p>Reward </p>
<p>Each Step </p>
<p>Each Episode </p>
<p>Observation </p>
<p>Agent Policy </p>
<p>Agent Model </p>
<p>Intervention 
Actions 
Structure 
Actions </p>
<p>Intervention 
Actions </p>
<p>Random 
Choice </p>
<p>? 
? </p>
<p>? </p>
<p>Table 2 :
2Statistics over the dSHD obtained from predicting the causal structure of the 3 variable test SCMs by algorithm. The samples sizes for DCDI, ENCO, and NOTEARS are reduced to approximately match the number of samples used by MCD.
© 2023 A. Sauter, E. Acar &amp; V. François-Lavet.
AcknowledgmentsWe thank Aske Plaat and the anonymous reviewers, whose suggestions helped improve the final version of this paper significantly. Furthermore, we thank Philip Lippe for taking the time to share insight on how to tune ENCO.This research was partially funded by the Hybrid Intelligence Center, a 10-year programme funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research, https://hybrid-intelligence-centre.nl, grant number 024.004.022Appendix A. Training DetailsA.1. Learning to InterveneFor the experiment in section 5 the policy network has a fully connected layer of size 30, followed by an LSTM layer of size 30. The actor-network has one fully connected layer of size 30, and the critic-network has one fully connected layer of size 10. The length of each episode was set to 10 and the model was trained for 5 million training steps. As intervention actions we provided do(X i = 0) and do(X i = 5) for each X i ∈ X . For all other parameters, the default values were used.Appendix B. Results for ENCOThe results in Section 6 and 8 raised some questions about the correctness of our comparison. More specifically, two anomalies for the ENCO(Lippe et al., 2021)benchmark emerged. Firstly, in all runs, ENCO seems to perform worse than the reports in the original paper might suggest. Secondly, when decreasing the number of samples in the training set, ENCO improves in performance. Neither of these behaviors is expected or can be easily explained.We investigated these issues in more detail and contacted one of the authors of the original paper for a sanity check of the code that bridges our data to their implementation. Even with his help, we spotted no faults. We want to invite any reader to check the publicly available code (see Section 1). Furthermore, we asked the author for hints to tune ENCO for better performance which we also incorporated, but that led to no significant change in performance.We argue that these surprising results for the benchmark still do not undermine the claims made in this paper. The improvement in performance w.r.t. the SOTA is not the core contribution of this paper and merely indicates that our novel approach performs empirically well. That being said, future research could look into the cause of these anomalies of ENCO when presented with the environments described in Section 6.
Active learning of causal structures with deep reinforcement learning. Amir Amirinezhad, Saber Salehkaleybar, Matin Hashemi, Neural Networks. 154Amir Amirinezhad, Saber Salehkaleybar, and Matin Hashemi. Active learning of causal structures with deep reinforcement learning. Neural Networks, 154:22-30, 2022.</p>
<p>On pearl's hierarchy and the foundations of causal inference. Elias Bareinboim, D Juan, Duligur Correa, Thomas Ibeling, Icard, Probabilistic and Causal Inference: The Works of Judea Pearl. Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. On pearl's hierarchy and the foundations of causal inference. In Probabilistic and Causal Inference: The Works of Judea Pearl, pages 507-556. 2022.</p>
<p>Differentiable causal discovery from interventional data. Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, Alexandre Drouin, Advances in Neural Information Processing Systems. 33Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexan- dre Drouin. Differentiable causal discovery from interventional data. Advances in Neural Infor- mation Processing Systems, 33:21865-21877, 2020.</p>
<p>Cam: Causal additive models, high-dimensional order search and penalized regression. Peter Bühlmann, Jonas Peters, Jan Ernest, The Annals of Statistics. 426Peter Bühlmann, Jonas Peters, and Jan Ernest. Cam: Causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 42(6):2526-2556, 2014.</p>
<p>Optimal structure identification with greedy search. David Maxwell, Chickering , Journal of machine learning research. 3David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507-554, 2002.</p>
<p>Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, Zeb Kurth-Nelson, arXiv:1901.08162Causal reasoning from meta-reinforcement learning. arXiv preprintIshita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal reasoning from meta-reinforcement learning. arXiv preprint arXiv:1901.08162, 2019.</p>
<p>An introduction to deep reinforcement learning. Foundations and Trends® in Machine Learning. Vincent François-Lavet, Peter Henderson, Riashat Islam, G Marc, Joelle Bellemare, Pineau, 11Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, Joelle Pineau, et al. An introduction to deep reinforcement learning. Foundations and Trends® in Machine Learning, 11(3-4):219-354, 2018.</p>
<p>Causal reinforcement learning using observational and interventional data. Maxime Gasse, Damien Grasset, Guillaume Gaudron, Pierre-Yves Oudeyer, arXiv:2106.14421arXiv preprintMaxime Gasse, Damien Grasset, Guillaume Gaudron, and Pierre-Yves Oudeyer. Causal reinforce- ment learning using observational and interventional data. arXiv preprint arXiv:2106.14421, 2021.</p>
<p>Causal inference. Clark Glymour, Peter Spirtes, Richard Scheines, Erkenntnis. 351-3Clark Glymour, Peter Spirtes, and Richard Scheines. Causal inference. Erkenntnis, 35(1-3):151- 189, 1991.</p>
<p>Learning functional causal models with generative neural networks. In Explainable and interpretable models in computer vision and machine learning. Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, Michele Sebag, SpringerOlivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David Lopez-Paz, and Michele Sebag. Learning functional causal models with generative neural networks. In Explain- able and interpretable models in computer vision and machine learning, pages 39-80. Springer, 2018.</p>
<p>Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. Alain Hauser, Peter Bühlmann, The Journal of Machine Learning Research. 131Alain Hauser and Peter Bühlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research, 13 (1):2409-2464, 2012.</p>
<p>. Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselinesAshley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Rad- ford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github. com/hill-a/stable-baselines, 2018.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 98Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.</p>
<p>Constraint-based causal discovery: conflict resolution with answer set programming. Antti Hyttinen, Frederick Eberhardt, Matti Järvisalo, Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence. the Thirtieth Conference on Uncertainty in Artificial IntelligenceAntti Hyttinen, Frederick Eberhardt, and Matti Järvisalo. Constraint-based causal discovery: con- flict resolution with answer set programming. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, pages 340-349, 2014.</p>
<p>Unsuitability of notears for causal graph discovery. Marcus Kaiser, Maksim Sipos, arXiv:2104.05441arXiv preprintMarcus Kaiser and Maksim Sipos. Unsuitability of notears for causal graph discovery. arXiv preprint arXiv:2104.05441, 2021.</p>
<p>Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schölkopf, C Michael, Mozer, arXiv:1910.01075Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprintNan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schölkopf, Michael C Mozer, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.</p>
<p>Causal discovery for modular world models. Anson Lei, Bernhard Schölkopf, Ingmar Posner, NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI). 2022Anson Lei, Bernhard Schölkopf, and Ingmar Posner. Causal discovery for modular world models. In NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI), 2022.</p>
<p>Efficient neural causal discovery without acyclicity constraints. Phillip Lippe, Taco Cohen, Efstratios Gavves, International Conference on Learning Representations. Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclic- ity constraints. In International Conference on Learning Representations, 2021.</p>
<p>Graphical Models: Selecting causal and statistical models. Christopher Meek, Carnegie Mellon UniversityPhD thesisChristopher Meek. Graphical Models: Selecting causal and statistical models. PhD thesis, PhD thesis, Carnegie Mellon University, 1997.</p>
<p>Causal discovery and reinforcement learning: A synergistic integration. Arquimides Mendez-Molina, Eduardo F Morales, L Enrique Sucar, International Conference on Probabilistic Graphical Models. PMLRArquimides Mendez-Molina, Eduardo F Morales, and L Enrique Sucar. Causal discovery and re- inforcement learning: A synergistic integration. In International Conference on Probabilistic Graphical Models, pages 421-432. PMLR, 2022.</p>
<p>Model-based reinforcement learning: A survey. Foundations and Trends® in Machine Learning. Joost Thomas M Moerland, Aske Broekens, Plaat, M Catholijn, Jonker, 16Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based rein- forcement learning: A survey. Foundations and Trends® in Machine Learning, 16(1):1-118, 2023.</p>
<p>Joint causal inference from multiple contexts. M Joris, Sara Mooij, Tom Magliacane, Claassen, Journal of Machine Learning Research. 21Joris M Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. Journal of Machine Learning Research, 21:1-108, 2020.</p>
<p>Causal induction from visual observations for goal directed tasks. Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei, arXiv:1910.01751arXiv preprintSuraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Causal induction from visual observations for goal directed tasks. arXiv preprint arXiv:1910.01751, 2019.</p>
<p>Bayesian analysis in expert systems: Comment: graphical models, causality and intervention. Judea Pearl, Statistical Science. 83Judea Pearl. Bayesian analysis in expert systems: Comment: graphical models, causality and inter- vention. Statistical Science, 8(3):266-269, 1993.</p>
<p>The Book of Why: The New Science of Cause and Effect. Judea Pearl, Dana Mackenzie, Basic Books, Inc., USA0465097601st editionJudea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect. Basic Books, Inc., USA, 1st edition, 2018. ISBN 046509760X.</p>
<p>Causal discovery with continuous additive noise models. J Peters, Mooij, B Janzing, Schölkopf, Journal of Machine Learning Research. 151J Peters, JM Mooij, D Janzing, and B Schölkopf. Causal discovery with continuous additive noise models. Journal of Machine Learning Research, 15(1):2009-2053, 2014.</p>
<p>High-accuracy model-based reinforcement learning, a survey. Aske Plaat, Walter Kosters, Mike Preuss, arXiv:2107.08241arXiv preprintAske Plaat, Walter Kosters, and Mike Preuss. High-accuracy model-based reinforcement learning, a survey. arXiv preprint arXiv:2107.08241, 2021.</p>
<p>A million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images. International journal of data science and analytics. Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, Clark Glymour, 3Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour. A million vari- ables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images. Inter- national journal of data science and analytics, 3(2):121-129, 2017.</p>
<p>Beware of the simulated dag! causal discovery benchmarks may be easy to game. Alexander Reisach, Christof Seiler, Sebastian Weichwald, Advances in Neural Information Processing Systems. 34Alexander Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag! causal discovery benchmarks may be easy to game. Advances in Neural Information Processing Sys- tems, 34:27772-27784, 2021.</p>
<p>Counting unlabeled acyclic digraphs. Robert W Robinson, Combinatorial mathematics V. SpringerRobert W Robinson. Counting unlabeled acyclic digraphs. In Combinatorial mathematics V, pages 28-43. Springer, 1977.</p>
<p>Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard Schölkopf, C Michael, Yoshua Mozer, Stefan Bengio, Nan Rosemary Bauer, Ke, arXiv:2109.02429Learning neural causal models with active interventions. arXiv preprintNino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab, Bernhard Schölkopf, Michael C Mozer, Yoshua Bengio, Stefan Bauer, and Nan Rosemary Ke. Learning neural causal models with active interventions. arXiv preprint arXiv:2109.02429, 2021.</p>
<p>Toward causal representation learning. Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, Yoshua Bengio, Proceedings of the IEEE. 1095Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612-634, 2021.</p>
<p>A linear non-gaussian acyclic model for causal discovery. Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen, Michael Jordan, Journal of Machine Learning Research. 710Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.</p>
<p>Consistency guarantees for permutation-based causal inference algorithms. Liam Solus, Yuhao Wang, Lenka Matejovicova, Caroline Uhler, arXiv:1702.03530arXiv preprintLiam Solus, Yuhao Wang, Lenka Matejovicova, and Caroline Uhler. Consistency guarantees for permutation-based causal inference algorithms. arXiv preprint arXiv:1702.03530, 2017.</p>
<p>Causation, prediction, and search. Peter Spirtes, N Clark, Richard Glymour, David Scheines, Heckerman, MIT pressPeter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.</p>
<p>Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schölkopf, arXiv:2203.02016Yarin Gal, and Stefan Bauer. Interventions, where and how? experimental design for causal models at scale. arXiv preprintPanagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Schölkopf, Yarin Gal, and Stefan Bauer. Interventions, where and how? experimental design for causal models at scale. arXiv preprint arXiv:2203.02016, 2022.</p>
<p>Meta learning for causal direction. Jean-François Ton, Dino Sejdinovic, Kenji Fukumizu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Jean-François Ton, Dino Sejdinovic, and Kenji Fukumizu. Meta learning for causal direction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9897-9905, 2021.</p>
<p>The max-min hill-climbing bayesian network structure learning algorithm. Ioannis Tsamardinos, Laura E Brown, Constantin F Aliferis, Machine learning. 651Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. Machine learning, 65(1):31-78, 2006.</p>
<p>D'ya like dags? a survey on structure learning and causal discovery. J Matthew, Vowels, Richard Necati Cihan Camgoz, Bowden, ACM Computing Surveys. 2021Matthew J Vowels, Necati Cihan Camgoz, and Richard Bowden. D'ya like dags? a survey on structure learning and causal discovery. ACM Computing Surveys (CSUR), 2021.</p>
<p>Permutation-based causal inference algorithms with interventions. Yuhao Wang, Liam Solus, Karren Yang, Caroline Uhler, Advances in Neural Information Processing Systems. 30Yuhao Wang, Liam Solus, Karren Yang, and Caroline Uhler. Permutation-based causal inference algorithms with interventions. Advances in Neural Information Processing Systems, 30, 2017.</p>
<p>Sample efficient actor-critic with experience replay. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, arXiv:1611.01224arXiv preprintZiyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016.</p>
<p>The causal-neural connection: Expressiveness, learnability, and inference. Kai-Zhan Kevin Muyuan Xia, Yoshua Lee, Elias Bengio, Bareinboim, Advances in Neural Information Processing Systems. A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman VaughanKevin Muyuan Xia, Kai-Zhan Lee, Yoshua Bengio, and Elias Bareinboim. The causal-neural con- nection: Expressiveness, learnability, and inference. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=hGmrNwR8qQP.</p>
<p>Characterizing and learning equivalence classes of causal DAGs under interventions. Karren Yang, Abigail Katcoff, Caroline Uhler, PMLRProceedings of the 35th International Conference on Machine Learning. Jennifer Dy and Andreas Krausethe 35th International Conference on Machine Learning80Karren Yang, Abigail Katcoff, and Caroline Uhler. Characterizing and learning equivalence classes of causal DAGs under interventions. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Pro- ceedings of Machine Learning Research, pages 5541-5550. PMLR, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/yang18a.html.</p>
<p>Causalvae: Disentangled representation learning via neural structural causal models. Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, Jun Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentangled representation learning via neural structural causal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9593-9602, June 2021.</p>
<p>Dag-gnn: Dag structure learning with graph neural networks. Yue Yu, Jie Chen, Tian Gao, Mo Yu, International Conference on Machine Learning. PMLRYue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. In International Conference on Machine Learning, pages 7154-7163. PMLR, 2019.</p>
<p>DAGs with NO TEARS: Continuous Optimization for Structure Learning. Xun Zheng, Bryon Aragam, Pradeep Ravikumar, Eric P Xing, Advances in Neural Information Processing Systems. Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Con- tinuous Optimization for Structure Learning. In Advances in Neural Information Processing Systems, 2018.</p>
<p>. Xun Zheng, Bryon Aragam, Alexandre Drouin, Ignavier Ng, 2020Xun Zheng, Bryon Aragam, Alexandre Drouin, and Ignavier Ng. notears. https://github. com/xunzheng/notears, 2020.</p>
<p>Causal discovery with reinforcement learning. Shengyu Zhu, Ignavier Ng, Zhitang Chen, International Conference on Learning Representations. Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal discovery with reinforcement learning. In International Conference on Learning Representations, 2019.</p>
<p>Learning a Causal Discovery Algorithm The following configuration for the policy network of the experiment in Section 6 worked best after preliminary experiments for the 3-variable (4-variable) environments: One (two) fully connected layer(s) of size 30 (64) followed by an LSTM layer of size 30 (128). A , Its outputs are fed into a fully connected layer of size 30 (32) for the actor-network and one of size 10 (32) for the critic-network. For this experiment, we set the horizon to 20. As intervention actions we provide do(X i = 5) for each X i ∈ X. We chose this value since it is unlikely to come from any of the noise distributionsA.2. Learning a Causal Discovery Algorithm The following configuration for the policy network of the experiment in Section 6 worked best after preliminary experiments for the 3-variable (4-variable) environments: One (two) fully connected layer(s) of size 30 (64) followed by an LSTM layer of size 30 (128). Its outputs are fed into a fully connected layer of size 30 (32) for the actor-network and one of size 10 (32) for the critic-network. For this experiment, we set the horizon to 20. As intervention actions we provide do(X i = 5) for each X i ∈ X . We chose this value since it is unlikely to come from any of the noise distributions.</p>            </div>
        </div>

    </div>
</body>
</html>
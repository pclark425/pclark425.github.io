<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2979 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2979</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2979</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-ccb1ccc4deacc4fb18000f0e1ce24329548963ae</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ccb1ccc4deacc4fb18000f0e1ce24329548963ae" target="_blank">Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The ablation and exploratory studies detail how the design beats the counterparts and provide a promising update on the $\texttt{ObtainDiamond}$ grand challenge with the MC-Planner approach.</p>
                <p><strong>Paper Abstract:</strong> We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose"$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and $\underline{S}$elect"($\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of the plan execution process and providing self-$\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2979.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2979.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Describe, Explain, Plan and Select</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive LLM-based planner that (1) describes execution events, (2) asks the LLM to self-explain failures, (3) replans, and (4) selects efficient sub-goals via a learned horizon-predictive selector to solve long-horizon open-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DEPS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DEPS uses an event-triggered Descriptor to summarize the agent state into text, an LLM that acts both as Explainer (self-explanation of plan failures) and Planner (re-generates plans), and a learned Horizon-Predictive Selector that ranks parallel candidate sub-goals by estimated remaining steps; plans are executed by a goal-conditioned controller.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>code-davinci-02 (Codex) for Minecraft; text-davinci-03 for ALFWorld; gpt-3.5-turbo for Tabletop (paper uses different LLMs per environment)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFWorld; MC-TextWorld (text-only Minecraft benchmark); Minecraft (visual/open-world environment)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>ALFWorld: text-aligned interactive environment where agents find and manipulate objects using textual state descriptions and candidate language-conditioned actions; MC-TextWorld: text-only Minecraft benchmark preserving crafting and mining rules; Minecraft (visual) is an open-ended embodied environment with long-horizon, compositional goals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-history / event-level episodic descriptions + experience-based horizon memory (selector)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>At each failure/event the Descriptor f_DESC(s_{t-1}) produces a distilled textual event description d_t (inventory, biome, GPS, outcome); the LLM explainer f_EX(d_t) produces an explanation e_t; the prompt is updated as p_t = CONCAT(p_{t-1}, d_t, e_t) so the LLM receives accumulated dialogue/context (history of descriptions + explanations + past plans). Separately, the Selector learns a horizon predictor μ(h|s,g) (a neural network with a goal-sensitive Impala CNN backbone) trained on offline trajectories to estimate remaining steps to each candidate goal and rank them.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Recency-based accumulation in the LLM prompt (concatenation of prior prompt, event descriptions, and explanations); selector retrieves horizon estimates via the learned μ network from current visual/state observations and goal embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Bounded by the LLM context window / max tokens and capped re-planning rounds (paper notes Codex practical max rounds ≈ 7–8 due to token limits); no explicit unlimited external store reported.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Distilled event-level descriptions (d_t) including inventory, biome, GPS, execution outcomes; LLM explanations of plan failures (e_t); previous plan fragments; implicitly, a history of prior re-planning rounds in the prompt. The Selector's learned model encodes experience (horizon estimates) from offline trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (text): DEP (describe+explain+plan) average success 76.0% (Table 6); Minecraft (visual open world): DEPS average success 48.56% across 71 tasks (Table 2); MC-TextWorld (text-only Minecraft): per-table results up to 100% on some meta-tasks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ALFWorld: GPT zero-shot baseline 10.0% average, GPT+RP 52.0%, Inner Monologue 30.0% (Table 6). Minecraft: GPT zero-shot baseline average 15.42%, Inner Monologue 21.64%, Code-as-Policies 25.77%, DEP (without Selector) 39.36% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>On Minecraft tasks, DEPS (with selector + iterative describe/explain/plan) improved average success from DEP (w/o Selector) 39.36% to DEPS 48.56% (+9.2 percentage points, ≈+23% relative); compared to GPT baseline (15.42%) the absolute gain is +33.14pp. In ALFWorld, adding re-planning/self-explanation (DEP) improved from GPT 10.0% to 76.0% (+66pp). Selector ablations show improvements of +22.3%, +29.2%, +32.6% compared to a fixed plan under 2/3/4 parallel goals in one-round tests (Figure 4 discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Appending event-level descriptions and LLM self-explanations into the prompt (iterative prompt-history) allows the LLM to locate planning errors and re-plan, substantially improving success on long-horizon, open-world tasks; a learned horizon predictor (selector) that uses offline experience to rank parallel sub-goals further improves efficiency and success in limited-horizon episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Memory/prompt history is constrained by LLM context length and token limits (practical re-planning rounds capped, Codex ≈7–8 rounds); performance still capped by low-level controller reliability and episode length; planning bottleneck and reliance on privately-held LLMs are noted limitations; paper also notes some fundamental planning pathologies (e.g., dead ends) may not be fully addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper compares the horizon-predictive Selector (experience-based) to vision-language similarity selectors (CLIP, MineCLIP) and fixed/random ordering: horizon-predictive Selector outperforms CLIP and MineCLIP and yields substantial gains (+22–33% in one-round scenarios). No other explicit memory architectures are compared.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2979.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2979.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive LLM-based hierarchical planning approach that augments LLM planning with environmental feedback via scene descriptors and success detectors to iteratively plan in embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue (IM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>IM uses an LLM planner and incorporates feedback (scene descriptions and success detection) into the prompt to produce next goals or re-plan during execution, forming an interactive planning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>text-davinci-03 (used in the paper's reproduced baseline experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFWorld; Minecraft; Tabletop Manipulation (used as baselines across these environments)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>ALFWorld is a text-aligned interactive environment requiring object finding and manipulation; Minecraft is an open-world visual game with long-horizon crafting/mine goals; Tabletop is a compact manipulation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>feedback-driven episodic memory implemented as appended scene descriptors and success/failure signals in the LLM prompt</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not detailed in this paper; described as passing scene descriptors and success detection outputs back to the LLM planner as feedback (i.e., per-step feedback appended to prompt), enabling iterative planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit via appended prompt history / feedback (recency-based); explicit retrieval mechanism not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Per-step feedback including scene descriptions and success/failure indicators; these are used by the LLM to inform subsequent planning decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as baseline results in the paper: ALFWorld average 30.0% (Table 6); Minecraft average 21.64% across meta-tasks (Table 2); Tabletop average 56.4% (Table A.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla GPT zero-shot (no iterative feedback) baselines: ALFWorld 10.0% (Table 6); Minecraft 15.42% (Table 2) -- these provide a rough comparison of with/without interactive feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Incorporating feedback (scene descriptors and success detectors) helps planners in compact/non-open-ended domains, but IM still suffers accumulative planning error and low success on long-horizon open-world tasks like Minecraft compared to DEPS.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>According to the paper, IM can suffer from accumulative planning error especially for long-horizon tasks in open worlds; it often fails to correct planning errors sufficiently via simple feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper reports that DEPS's describe+explain+plan approach plus selector outperforms IM on open-world Minecraft; no direct multi-memory-type comparison within IM is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2979.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2979.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT+RP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT with Re-Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot LLM planner (GPT) augmented with the ability to re-plan when given execution feedback, used as an experimental variant in ALFWorld comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT+RP (GPT with Re-Planning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard GPT zero-shot planner augmented with a re-planning loop that ingests feedback and regenerates plans (i.e., a simple iterative prompt update / re-planning mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>text-davinci-03 (used in ALFWorld experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFWorld (text-based)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>ALFWorld: text-aligned interactive environment where tasks require finding objects often by opening receptacles and performing interactions (heating, cleaning, placing).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>iterative prompt-updates / re-planning (prompt-history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not specified in detail; described as augmented with re-planning ability so feedback can be provided and new plans produced (implementation detail left to reproduction).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Likely recency-based append of feedback to prompt (not detailed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Execution feedback used for re-planning (textual state descriptions); exact stored contents not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld: GPT+RP average success 52.0% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ALFWorld: GPT zero-shot average success 10.0% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>+42.0 percentage points on ALFWorld average (10.0% → 52.0%) as reported in Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Adding re-planning (simple iterative feedback) substantially improves GPT performance on ALFWorld, showing that even simple prompt-history re-planning is beneficial in text-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Implementation details and limits (e.g., context window) not described; no deeper architectural memory described beyond re-planning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared empirically in ALFWorld: GPT+RP (52.0%) outperforms GPT (10.0%) and IM (30.0%) but DEPS (76.0%) outperforms GPT+RP; no structural memory-type comparison beyond empirical numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2979.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2979.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited concurrent work that proposes an autonomous agent architecture with dynamic memory and self-reflection to improve performance via experience-based memory updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as a concurrent work on planning with LLMs that uses dynamic memory and self-reflection; the paper cites it in related-work context but does not use or evaluate it.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic memory / self-reflection (as stated in citation title)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as a concurrent approach that leverages dynamic memory and self-reflection; no experimental or architectural details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2979.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2979.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-robotic-brain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM as a robotic brain: Unifying egocentric memory and control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited concurrent work proposing to unify egocentric memory and control with LLMs for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llm as a robotic brain: Unifying egocentric memory and control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM as a robotic brain</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited in related work as another line of research aiming to integrate egocentric (agent-centered) memory into LLM-based control architectures; the current paper does not use or evaluate it.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>egocentric memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Mentioned as concurrent work that unifies egocentric memory with control; no experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Llm as a robotic brain: Unifying egocentric memory and control <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 1)</em></li>
                <li>ProgPrompt: Generating situated robot task plans using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2979",
    "paper_id": "paper-ccb1ccc4deacc4fb18000f0e1ce24329548963ae",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "DEPS",
            "name_full": "Describe, Explain, Plan and Select",
            "brief_description": "An interactive LLM-based planner that (1) describes execution events, (2) asks the LLM to self-explain failures, (3) replans, and (4) selects efficient sub-goals via a learned horizon-predictive selector to solve long-horizon open-world tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DEPS",
            "agent_description": "DEPS uses an event-triggered Descriptor to summarize the agent state into text, an LLM that acts both as Explainer (self-explanation of plan failures) and Planner (re-generates plans), and a learned Horizon-Predictive Selector that ranks parallel candidate sub-goals by estimated remaining steps; plans are executed by a goal-conditioned controller.",
            "base_llm_model": "code-davinci-02 (Codex) for Minecraft; text-davinci-03 for ALFWorld; gpt-3.5-turbo for Tabletop (paper uses different LLMs per environment)",
            "base_llm_size": null,
            "text_game_name": "ALFWorld; MC-TextWorld (text-only Minecraft benchmark); Minecraft (visual/open-world environment)",
            "text_game_description": "ALFWorld: text-aligned interactive environment where agents find and manipulate objects using textual state descriptions and candidate language-conditioned actions; MC-TextWorld: text-only Minecraft benchmark preserving crafting and mining rules; Minecraft (visual) is an open-ended embodied environment with long-horizon, compositional goals.",
            "uses_memory": true,
            "memory_type": "prompt-history / event-level episodic descriptions + experience-based horizon memory (selector)",
            "memory_architecture": "At each failure/event the Descriptor f_DESC(s_{t-1}) produces a distilled textual event description d_t (inventory, biome, GPS, outcome); the LLM explainer f_EX(d_t) produces an explanation e_t; the prompt is updated as p_t = CONCAT(p_{t-1}, d_t, e_t) so the LLM receives accumulated dialogue/context (history of descriptions + explanations + past plans). Separately, the Selector learns a horizon predictor μ(h|s,g) (a neural network with a goal-sensitive Impala CNN backbone) trained on offline trajectories to estimate remaining steps to each candidate goal and rank them.",
            "memory_retrieval_mechanism": "Recency-based accumulation in the LLM prompt (concatenation of prior prompt, event descriptions, and explanations); selector retrieves horizon estimates via the learned μ network from current visual/state observations and goal embedding.",
            "memory_capacity": "Bounded by the LLM context window / max tokens and capped re-planning rounds (paper notes Codex practical max rounds ≈ 7–8 due to token limits); no explicit unlimited external store reported.",
            "what_is_stored_in_memory": "Distilled event-level descriptions (d_t) including inventory, biome, GPS, execution outcomes; LLM explanations of plan failures (e_t); previous plan fragments; implicitly, a history of prior re-planning rounds in the prompt. The Selector's learned model encodes experience (horizon estimates) from offline trajectories.",
            "performance_with_memory": "ALFWorld (text): DEP (describe+explain+plan) average success 76.0% (Table 6); Minecraft (visual open world): DEPS average success 48.56% across 71 tasks (Table 2); MC-TextWorld (text-only Minecraft): per-table results up to 100% on some meta-tasks (Table 3).",
            "performance_without_memory": "ALFWorld: GPT zero-shot baseline 10.0% average, GPT+RP 52.0%, Inner Monologue 30.0% (Table 6). Minecraft: GPT zero-shot baseline average 15.42%, Inner Monologue 21.64%, Code-as-Policies 25.77%, DEP (without Selector) 39.36% (Table 2).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "On Minecraft tasks, DEPS (with selector + iterative describe/explain/plan) improved average success from DEP (w/o Selector) 39.36% to DEPS 48.56% (+9.2 percentage points, ≈+23% relative); compared to GPT baseline (15.42%) the absolute gain is +33.14pp. In ALFWorld, adding re-planning/self-explanation (DEP) improved from GPT 10.0% to 76.0% (+66pp). Selector ablations show improvements of +22.3%, +29.2%, +32.6% compared to a fixed plan under 2/3/4 parallel goals in one-round tests (Figure 4 discussion).",
            "key_findings_about_memory": "Appending event-level descriptions and LLM self-explanations into the prompt (iterative prompt-history) allows the LLM to locate planning errors and re-plan, substantially improving success on long-horizon, open-world tasks; a learned horizon predictor (selector) that uses offline experience to rank parallel sub-goals further improves efficiency and success in limited-horizon episodes.",
            "memory_limitations": "Memory/prompt history is constrained by LLM context length and token limits (practical re-planning rounds capped, Codex ≈7–8 rounds); performance still capped by low-level controller reliability and episode length; planning bottleneck and reliance on privately-held LLMs are noted limitations; paper also notes some fundamental planning pathologies (e.g., dead ends) may not be fully addressed.",
            "comparison_with_other_memory_types": "Paper compares the horizon-predictive Selector (experience-based) to vision-language similarity selectors (CLIP, MineCLIP) and fixed/random ordering: horizon-predictive Selector outperforms CLIP and MineCLIP and yields substantial gains (+22–33% in one-round scenarios). No other explicit memory architectures are compared.",
            "uuid": "e2979.0",
            "source_info": {
                "paper_title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "IM",
            "name_full": "Inner Monologue",
            "brief_description": "An interactive LLM-based hierarchical planning approach that augments LLM planning with environmental feedback via scene descriptors and success detectors to iteratively plan in embodied environments.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "mention_or_use": "use",
            "agent_name": "Inner Monologue (IM)",
            "agent_description": "IM uses an LLM planner and incorporates feedback (scene descriptions and success detection) into the prompt to produce next goals or re-plan during execution, forming an interactive planning loop.",
            "base_llm_model": "text-davinci-03 (used in the paper's reproduced baseline experiments)",
            "base_llm_size": null,
            "text_game_name": "ALFWorld; Minecraft; Tabletop Manipulation (used as baselines across these environments)",
            "text_game_description": "ALFWorld is a text-aligned interactive environment requiring object finding and manipulation; Minecraft is an open-world visual game with long-horizon crafting/mine goals; Tabletop is a compact manipulation benchmark.",
            "uses_memory": true,
            "memory_type": "feedback-driven episodic memory implemented as appended scene descriptors and success/failure signals in the LLM prompt",
            "memory_architecture": "Not detailed in this paper; described as passing scene descriptors and success detection outputs back to the LLM planner as feedback (i.e., per-step feedback appended to prompt), enabling iterative planning.",
            "memory_retrieval_mechanism": "Implicit via appended prompt history / feedback (recency-based); explicit retrieval mechanism not described in this paper.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Per-step feedback including scene descriptions and success/failure indicators; these are used by the LLM to inform subsequent planning decisions.",
            "performance_with_memory": "Reported as baseline results in the paper: ALFWorld average 30.0% (Table 6); Minecraft average 21.64% across meta-tasks (Table 2); Tabletop average 56.4% (Table A.2).",
            "performance_without_memory": "Vanilla GPT zero-shot (no iterative feedback) baselines: ALFWorld 10.0% (Table 6); Minecraft 15.42% (Table 2) -- these provide a rough comparison of with/without interactive feedback.",
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Incorporating feedback (scene descriptors and success detectors) helps planners in compact/non-open-ended domains, but IM still suffers accumulative planning error and low success on long-horizon open-world tasks like Minecraft compared to DEPS.",
            "memory_limitations": "According to the paper, IM can suffer from accumulative planning error especially for long-horizon tasks in open worlds; it often fails to correct planning errors sufficiently via simple feedback.",
            "comparison_with_other_memory_types": "Paper reports that DEPS's describe+explain+plan approach plus selector outperforms IM on open-world Minecraft; no direct multi-memory-type comparison within IM is provided.",
            "uuid": "e2979.1",
            "source_info": {
                "paper_title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "GPT+RP",
            "name_full": "GPT with Re-Planning",
            "brief_description": "A zero-shot LLM planner (GPT) augmented with the ability to re-plan when given execution feedback, used as an experimental variant in ALFWorld comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GPT+RP (GPT with Re-Planning)",
            "agent_description": "Standard GPT zero-shot planner augmented with a re-planning loop that ingests feedback and regenerates plans (i.e., a simple iterative prompt update / re-planning mechanism).",
            "base_llm_model": "text-davinci-03 (used in ALFWorld experiments)",
            "base_llm_size": null,
            "text_game_name": "ALFWorld (text-based)",
            "text_game_description": "ALFWorld: text-aligned interactive environment where tasks require finding objects often by opening receptacles and performing interactions (heating, cleaning, placing).",
            "uses_memory": true,
            "memory_type": "iterative prompt-updates / re-planning (prompt-history)",
            "memory_architecture": "Not specified in detail; described as augmented with re-planning ability so feedback can be provided and new plans produced (implementation detail left to reproduction).",
            "memory_retrieval_mechanism": "Likely recency-based append of feedback to prompt (not detailed in paper).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Execution feedback used for re-planning (textual state descriptions); exact stored contents not detailed.",
            "performance_with_memory": "ALFWorld: GPT+RP average success 52.0% (Table 6).",
            "performance_without_memory": "ALFWorld: GPT zero-shot average success 10.0% (Table 6).",
            "has_ablation_study": null,
            "memory_improvement_magnitude": "+42.0 percentage points on ALFWorld average (10.0% → 52.0%) as reported in Table 6.",
            "key_findings_about_memory": "Adding re-planning (simple iterative feedback) substantially improves GPT performance on ALFWorld, showing that even simple prompt-history re-planning is beneficial in text-based tasks.",
            "memory_limitations": "Implementation details and limits (e.g., context window) not described; no deeper architectural memory described beyond re-planning capability.",
            "comparison_with_other_memory_types": "Compared empirically in ALFWorld: GPT+RP (52.0%) outperforms GPT (10.0%) and IM (30.0%) but DEPS (76.0%) outperforms GPT+RP; no structural memory-type comparison beyond empirical numbers.",
            "uuid": "e2979.2",
            "source_info": {
                "paper_title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A cited concurrent work that proposes an autonomous agent architecture with dynamic memory and self-reflection to improve performance via experience-based memory updates.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Mentioned as a concurrent work on planning with LLMs that uses dynamic memory and self-reflection; the paper cites it in related-work context but does not use or evaluate it.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "dynamic memory / self-reflection (as stated in citation title)",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as a concurrent approach that leverages dynamic memory and self-reflection; no experimental or architectural details provided in this paper.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2979.3",
            "source_info": {
                "paper_title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "LLM-as-robotic-brain",
            "name_full": "LLM as a robotic brain: Unifying egocentric memory and control",
            "brief_description": "A cited concurrent work proposing to unify egocentric memory and control with LLMs for embodied agents.",
            "citation_title": "Llm as a robotic brain: Unifying egocentric memory and control",
            "mention_or_use": "mention",
            "agent_name": "LLM as a robotic brain",
            "agent_description": "Cited in related work as another line of research aiming to integrate egocentric (agent-centered) memory into LLM-based control architectures; the current paper does not use or evaluate it.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "egocentric memory",
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Mentioned as concurrent work that unifies egocentric memory with control; no experimental details provided in this paper.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2979.4",
            "source_info": {
                "paper_title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Llm as a robotic brain: Unifying egocentric memory and control",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 1
        },
        {
            "paper_title": "ProgPrompt: Generating situated robot task plans using large language models",
            "rating": 1
        }
    ],
    "cost": 0.020714999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</h1>
<p>Zihao Wang ${ }^{1,2}$, Shaofei Cai ${ }^{1,2}$, Guanzhou Chen ${ }^{3}$, Anji Liu ${ }^{1}$, Xiaojian Ma ${ }^{4}$, Yitao Liang ${ }^{1,5 *}$ Team CraftJarvis<br>${ }^{1}$ Institute for Artificial Intelligence, Peking University<br>${ }^{2}$ School of Intelligence Science and Technology, Peking University<br>${ }^{3}$ School of Computer Science, Beijing University of Posts and Telecommunications<br>${ }^{4}$ Computer Science Department, University of California, Los Angeles<br>${ }^{5}$ Beijing Institute for General Artificial Intelligence (BIGAI)<br>{zhwang, caizhaofei}@stu.pku.edu.cn, rayment@bupt.edu.cn<br>liuanji@cs.ucla.edu, xiaojian.ma@ucla.edu, yitaol@pku.edu.cn</p>
<h4>Abstract</h4>
<p>We investigate the challenge of task planning for multi-task embodied agents in open-world environments. ${ }^{2}$ Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "Describe, Explain, Plan and Select" (DEPS), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated plan by integrating description of the plan execution process and providing selfexplanation of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal selector, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the ObtainDiamond grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</p>
<h2>1 Introduction</h2>
<p>Developing multi-task agents that can accomplish a vast and diverse suite of tasks in complex domains has been viewed as one of the key milestones towards generally capable artificial intelligence [36, 1, $5,10,25]$. To enable such capabilities, earlier works have suggested employing a hierarchical goal execution architecture [2, 4], where a planner generates action plans that would then be executed by low-level goal-conditioned controllers. This architecture has been delivering promising progress in</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Planning success rates plummet in open worlds due to new challenges.
many robotics domains, including table-top and mobile manipulation [46, 4], 2D shape drawing [20] and table rearrangement [17]. However, whether such success can be transferred to a more open-ended world with unlimited exploration areas and internet-scale knowledge remains open [14, 10, 13, 12, 19].
To understand the gap, we run Inner Monologue [17], a general and competitive hierarchical goal execution model on a typical open-world domain Minecraft [18, 14, 10] and two classical robotic environments ALFWorld [41] and Tabletop environments [40, 4]. The algorithm uses a Large Language Model (LLM) based planner that contains domain-specific knowledge for all three environments. In all environments, we use either an Oracle goal-conditioned controller or a learned one. Results are shown in the bar plot in Figure 1. First, even when the Oracle controller is used, the success rate of executing Minecraft tasks is much less than that of the other environments. Next, the task failure rate becomes even higher in Minecraft when the learned controller is substituted. Both failures originate from unique challenges brought by open-world environments, which we identify in the following.
First, compared to canonical environments (e.g., Atari [29] and robotic control suite [40]), open worlds have highly abundant object types with complex dependency and relation. As a result, groundtruth plans typically involve a long sequence of sub-goals with strict dependencies. As Figure 1 challenge #1 suggests, it requires at least 13 sub-goals executed in proper order to obtain a diamond in Minecraft, while in Tabletop a task is typically no more than a few consecutive sub-goals.
Another challenge brought by the complicated tasks in an open-ended world is the feasibility of the produced plans. Consider the example shown in Figure 1 (challenge #2). To craft a bed in Minecraft, the fastest way is by either slaughtering a sheep to obtain wool, which can be used to craft beds, or collecting beds from a village. However, since no sheep or village is reachable by the agent within 3 minutes of gameplay, to craft a bed efficiently, the agent should choose to slaughter a spider and use materials (e.g., string) it drops to craft wool, and then a bed. That is, when dealing with a task that can be completed by executing multiple possible sequences of sub-goals, the planner should be able to select the best route based on the current state of the agent. However, the complex and diverse state distribution of open-world environments makes state awareness hard to achieve.
To tackle these problems, we propose "Describe, Explain, Plan and Select" (DEPS), an interactive planning approach based on Large Language Models (LLMs) to alleviate the aforementioned issues. The key to tackling the first challenge is to effectively adjust the generated plan upon failure. Specifically, whenever the controller fails to complete a sub-goal, a descriptor will summarize the current situation as text and send it back to the LLM-based planner. We then prompt the LLM as an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan using information from the descriptor and explainer. To improve the feasibility of generated plans conditioned on the current state, which is the second identified challenge, we use a learned goalselector to choose the most accessible sub-task based on the proximity to each candidate sub-goal.
Our experiments are conducted on 71 tasks in open-ended Minecraft without any demonstration. Given the goal-conditioned controller for atom sub-tasks (i.e., mine log and mine stone), our zero-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our proposed interactive planner architecture.
shot ${ }^{3}$ LLM-based planner can finish all tasks within a limited horizon (3000-12000 steps for different tasks). We find DEPS outperforms all language planner baselines by nearly doubling the overall success rate, with the same initial state and goal-conditioned controller. Our ablation and exploratory studies then explain how our approach beats the counterparts and becomes the first planning-based agent that accomplishes the challenging ObtainDiamond task. DEPS does not require any planning training for the environment. Additionally, DEPS achieves between on-par and more than $50 \%$ relative improvement over existing or concurrent LLM-based planning methods on non-open-ended robotics domains such as ALFWorld [41] and Tabletop environments [40].</p>
<h1>2 Background</h1>
<p>We aim to develop an agent capable of solving long-horizon goal-reaching tasks using image observations and language goals. To accomplish this, we propose a combined approach involving goal-conditioned policies (termed controllers) and a planner. The goal-conditioned policies are trained to complete sub-goals, while the planner decomposes long-horizon tasks into a series of $K$ short-horizon sub-goals, $g_{1}, \ldots, g_{K}$, to be executed by the controller. At each time step $t$, the goal-conditioned policy $\pi\left(a_{t} \mid s_{t}, g_{k}\right)$ generates an action $a_{t}$ based on the current state $s_{t}$ and the specified sub-goal $g_{k}$.</p>
<p>Planning with Large Language Models Previous works have shown that LLMs such as InstructGPT [32] and Codex [8] can be used as zero-shot planners to generate sub-goal sequences for various tasks in embodied environments [16, 42]. Formally, given the task description $T$ as prompt $p$, LLM acts as a planner to decode $T$ into $K$ sub-goals, $g_{1}, \ldots, g_{K}$, which are then executed one by one by the low-level controller $\pi\left(a_{t} \mid s_{t}, g_{k}\right)$ to accomplish the task.
However, the above pipeline suffers from both challenges identified in Section 1. Regarding the first challenge, the probability of generating a flawless plan directly from the task description decreases significantly as the required number of sub-goals increases. Moreover, even when the LLM generates a correct plan, it is very likely that the plan is highly inefficient given the agent's current state (challenge #2). Prior works mostly focus on solving the first challenge by providing environmental feedback to the LLM through affordance functions [4], success detector [20] or scene descriptor [17]. However, although these approaches work well on many non-open-ended domains, they still suffer from high failure rates in open-world environments.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Towards Reliable Planning in Embodied Open-World Environments</h1>
<p>In this section, we first give an overview of our proposed interactive planning framework "Descibe, Explain, Plan, and Select" (DEPS) for solving complex and long-horizon tasks in open-world environments (Sec. 3.1). Next, in Section 3.2, we elaborate how DEPS iteratively refines its plan to combat the first identified challenge. Section 3.3 introduces the selector module that is used to identify efficient plans in response to the second identified challenge.</p>
<h3>3.1 DEPS Overview</h3>
<p>As demonstrated in Figure 2, our agent (DEPS) consists of an event-triggered Descriptor, a Large Language Model (LLM) as Explainer and Planner, a goal Selector based on horizon prediction and a goal-conditioned controller. In the following, we use Minecraft as a running example to better elaborate our agent. Note that DEPS can be directly applied to other (non-)open-ended tasks.</p>
<p>We take a large language model (LLM) as a zero-shot planner of the agent to complete tasks. Given a goal command (e.g., ObtainDiamond) as task $T$, the LLM-based planner decomposes this high-level task into a sequence of sub-goals $\left{g_{1}, \ldots, g_{K}\right}$, as the initial plan $P_{0}$. The goals are instructions in natural language, such as mine oak wood (in Minecraft), find two cups (in ALFWorld), put block A on top of block B (in Tabletop Manipulation).</p>
<p>As described in Section 2, a controller is then invoked to execute the provided sub-goals sequentially through a goal-conditioned policy $\pi(a \mid s, g)$. However, the initial plan provided by the planner often contains errors, which results in execution failures of the controller. For example, the goal can not be finished only with a wooden pickaxe $\boldsymbol{\mathcal { P }}$ as shown in Figure 2. When failure pops up, the descriptor will summarize the current state $s_{t}$ and execution outcome of the most recent goal into text $d_{t}$ and send it to the LLM. The LLM will first try to locate the errors in the previous plan $P_{t-1}$ by self-explanation, e.g., the goal need to be executed with a stone pickaxe $\boldsymbol{\mathcal { P }}$. Then it will re-plan the current task $T$ and generate a revised plan $P_{t}$ according to the explanation. In this process, the LLM is also treated as an explainer in addition to the planner role. The Descriptor, Explainer, and Planner will be detailed in Section 3.2.</p>
<p>$$
\begin{aligned}
\text { Description } &amp; : d_{t}=f_{\mathrm{DESC}}\left(s_{t-1}\right) \
\text { Explanation } &amp; : e_{t}=f_{\mathrm{EX}}\left(d_{t}\right) \
\text { Prompt } &amp; : p_{t}=\operatorname{CONCAT}\left(p_{t-1}, d_{t}, e_{t}\right) \
\text { Plan } &amp; : P_{t}=f_{\mathrm{LM}}\left(p_{t}\right) \
\text { Goal } &amp; : g_{t} \sim f_{\mathrm{S}}\left(\mathrm{P}<em t-1="t-1">{t}, s</em>\right) \
\text { Action } &amp; : a_{t} \sim \pi\left(a_{t} \mid s_{t-1}, g_{t}\right)
\end{aligned}
$$</p>
<p>As shown in Equation (1), DEPS will iteratively update the plan $P_{t}$ until the task is finished, where $f_{\text {DESC }}$ is the descriptor model, $f_{\mathrm{LM}}$ denotes the language model as explainer and planner, $f_{\mathrm{S}}$ is the selector model, $\pi$ is goal-conditioned policies from the controller.</p>
<p>To filter out inefficient plans, the selector is trained to predict the number of time steps remaining to achieve every goal $g_{k}$ in a set of parallel goals given the current state $s_{t}$. When the generated plan contains alternative routes, the selector uses this information to choose a suitable goal as the current goal $g_{t}$. For example, the horizon predicted by the selector of goal acacia tree is less than goal oak tree in Savanna biome, which leads to chop acacia tree as current goal $g_{t}$.</p>
<h3>3.2 Describe, Explain and Plan with LLM Generates Executable Plans</h3>
<p>Current LLM-based planners usually query the LLM once at the beginning of every episode and use the output plan throughout the episode [16, 42]. However, as demonstrated by Figure 1, such one-shot planning methods often fail on long-horizon tasks that require many sub-goals. This is caused by two major issues. First, since the correct plan for long-horizon tasks needs to respect various complex preconditions, it is extremely hard for the LLM to generate a flawless plan directly from the task instructions, resulting in failure when simply following the initial plan. Additionally, due to the unpredictable transition dynamics, some incidents may happen during the execution and make the initial plan non-executable. To remedy these problems, existing methods introduce feedback (e.g.,</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># step 1: mine 3 logs</span>
<span class="w">    </span><span class="n">mine</span><span class="p">(</span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># step 2: craft 12 planks from 3 logs</span>
<span class="w">    </span><span class="n">craft</span><span class="p">(</span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;planks&quot;</span><span class="p">:</span><span class="mi">12</span><span class="p">},</span><span class="w"> </span><span class="n">materials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span><span class="mi">3</span><span class="p">},</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># step 3: craft 4 sticks from 2 planks</span>
<span class="w">    </span><span class="n">craft</span><span class="p">(</span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;stick&quot;</span><span class="p">:</span><span class="mi">4</span><span class="p">},</span><span class="w"> </span><span class="n">materials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;planks&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># step 4: craft 1 crafting_table from 4 planks</span>
<span class="w">    </span><span class="n">craft</span><span class="p">(</span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;crafting_table&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="n">materials</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;planks&quot;</span><span class="p">:</span><span class="mi">4</span><span class="p">},</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># step 5: craft 1 wooden axe from 3 planks and 2 sticks on crafting table</span>
<span class="w">    </span><span class="n">craft</span><span class="p">(</span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;wooden_axe&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;planks&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;stick&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;crafting_table&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="s2">&quot;wooden_axe&quot;</span>
</code></pre></div>

<p>from success detector or scene descriptor) to reflect on the results of previous executions [17, 20, 4]. However, merely informing the LLM whether a sub-goal is completed is often insufficient to correct the planning error.</p>
<p>To remedy this, we propose "describe, explain and plan", a new interactive planning method to generate more executable and explainable plans. We start with rewriting the prompt into an interactive dialogue format as in ChatGPT [32] so that subsequent feedback can be passed to the LLM effectively. The produced plan is also augmented with the preconditions and effects of each goal. The structured prompt improves the readability and interpretability of the plan and facilitates error-locating when the execution fails later, as demonstrated in Prompt 1.</p>
<p>The descriptor will then collect the feedback generated by the agent during the execution of the task. The feedback can be practically obtained either by a person (human feedback [4]), or by a pre-trained vision-language model CLIP [35]. While the previous type of feedback needs intensive human involvement, the latter from the pre-trained model needs to be fine-tuned for the specific domain, which decreases the automation and generalization of the agent. On the contrary, Minecraft returns the 'info' and other high-level observations (such as biome, GPS, and compass), we can easily translate the unstructured information into structured language. Therefore we take the symbolic information available in the game and translate it into feedback description $d_{t}$ in this work. To avoid carrying unrelated information in the prompt, we further distill plan-related messages (e.g., inventory information, biome) as final event-level description $d_{t}$ as demonstrated in Figure 2.</p>
<p>Notably, we also treat the LLM as an explainer to explain why the previous plans $P_{t-1}$ failed. Specifically, by analyzing the current state from description $d_{t}$ and precondition of current goal $g_{t}$, the explainer can identify the reason why the current goal cannot be executed successfully. As shown in Figure 2, the reason may be the current goal requires the use of an iron pickaxe, but the tool is not prepared in advance, or the current goal requires the use of 3 planks, but the currently available planks are not enough. To implement this, we provide few-shot demonstrations to the LLM as in chain-of-thoughts prompting [45], as shown in Prompt 1. Finally, the LLM goes back to its role as a planner and re-plans the task with the explicit explanation of existing bugs in the previous plan $P_{t-1}$, ultimately generating an updated plan $P_{t}$ according to the explanation.</p>
<h1>3.3 Horizon-Predictive Selector Yields Efficient Plans</h1>
<p>Due to the abundance of objects and the compositional nature of their functionalities, there often exist multiple feasible plans to complete a task, i.e., there are usually multiple paths for the completion of a particular goal. However, despite the feasibility of all such plans, most of them are highly inefficient to execute in the current episode. For example, as shown in Figure 2, obtaining a wood can be done by chopping oak trees $\boldsymbol{\Theta}$, birch trees $\boldsymbol{\Theta}$, or acacia trees $\boldsymbol{\Theta}$. But only oak trees are available in the plains biome. So the planner needs to choose oak trees since it is more efficient, as the agent does not need to travel to another biome.</p>
<p>On the other hand, there is no strict sequential requirement for some goals in the plan $P_{t}$, i.e., $g_{i}, g_{j} \sim P_{t}$ enjoy the same precondition, which means $g_{i}$ and $g_{j}$ can be executed in any order. As shown in Figure 1, the choice of different paths (sequences) may affect the execution efficiency of the plan $P_{t}$ as one goal might be closer to the agent. Always choosing the closer goal to execute first could yield more efficient plans and improve the final success rate under a limited episode length. Moreover, the dynamic nature of open-world environments further amplifies the impact of efficient</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Selection Demonstration from "Selector". Given parallel sub-goals, i.e. candidate skills, our Selector will determine the sequence in which to carry out these sub-goals based on their current proximity to the agent and modify the original plan produced by the LM planner.
plans on the success rate. For example, in Minecraft, if the agent chooses to execute a further goal like collect wood first, the much closer target sheep may disappear and be hard to find again.</p>
<p>In order to improve the efficiency of our plans, we propose to use a selector that selects the most efficient path with the highest execution success rate as the final plan. Specifically, we design a state-aware selector to choose the nearest goal under state $s_{t}$ as the current goal $g_{t}$ from the candidate goal sets in plan $P_{t}$. It predicts the goal distribution $p\left(g_{t} \mid s_{t}, P_{t}\right)$ under the current state $s_{t}$ and plan $P_{t}$, where $g_{t} \in G_{t}, G_{t}$ describes all current executable goals in $P_{t}$. A straight way to implement the selector is to leverage the semantic similarity between the current state and the goal text using a vision-language model (VLM) such as CLIP [35]. Nevertheless, this may not exactly reflect the difficulty of completing the goal since VLM lacks practical experience. For example, an "oak tree" in front of the agent could lead to high semantic similarity for the "chopping tree" goal, but it may be inefficient to achieve this goal if a canyon is in the middle between the agent and the oak tree.</p>
<p>To mitigate this, we implement a horizon-predictive selector that embeds practical task experience to accurately rank the goals based on their efficiency and feasibility. Here, we define the horizon of a goal $h_{t}(g):=T_{g}-t$ as the remaining time steps to complete the given goal, where $T_{g}$ is the time of completing goal $g$. This metric accurately reflects how quickly we can achieve the given goal from the current state. To estimate the horizon, we learn a neural network $\mu$ to fit the offline trajectories by minimizing the entropy loss $-\log \mu\left(h_{t}(g) \mid s_{t}, g\right)$, where $h_{t}$ is the ground-truth horizon in trajectories of completing goal $g$. Therefore, the goal distribution can be formulated as follows:</p>
<p>$$
f\left(g_{t} \mid s_{t}, P_{t}\right)=\frac{\exp \left(-\mu\left(g_{t}, s_{t}\right)\right)}{\sum_{g \in G_{t}} \exp \left(-\mu\left(g, s_{t}\right)\right)}
$$</p>
<p>We set goal-sensitive Impala CNN [6] as the backbone of the selector. In practice, the horizon predictive selector can be jointly trained with the controller policies and share the backbone parameters [6].</p>
<h1>4 Experiments</h1>
<p>This section analyzes and evaluates our proposed "describe, explain, plan, and select" (DEPS) method. To minimize performance variation caused by the low-level controller, we standardize all experiments with one controller learned by behavior cloning. We refer to the details of this controller in Appendix C. In Section 4.1, we introduce our testing environments and our evaluation task set, consisting of the hardest 71 tasks from MCU SkillForgeChain [22]. In Section 4.2, we report our performance in the context of existing LLM-based planners. Ablation studies are conducted in Section 4.3. Finally, we pay close attention to the hardest task, ObtainDiamond, which is long-hailed as a major challenge in the community. The experiments on ALFWorld and Tabletop Manipulation environments are shown in Appendix A.</p>
<h3>4.1 Experimental Setup</h3>
<p>Environment and Task Setting We first evaluate our proposed method in Minecraft, a popular open-world environment with both challenges discussed in Section 1. For better reflecting the performance of DEPS, we choose three Minecraft environments with different versions for better</p>
<p>evaluation, including Minedojo [10] with Minecraft 1.11.2, MineRL [3] with Minecraft 1.16.5, and MC-TextWorld [22] with Minecraft 1.19.2. Rules and items have something different in the above three Minecraft environments, which can better evaluate the dynamic and interactive planning abilities of DEPS.</p>
<p>Table 1: Attributes of 8 meta tasks covering Task101: We evaluate the algorithm on Minecraft Task101. We group the consisted 71 task into 8 different meta groups, with each focusing on testing a different aspect of our proposed method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Number</th>
<th style="text-align: center;">Example Task</th>
<th style="text-align: center;">Max. Steps</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Given Tool</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MT1</td>
<td style="text-align: center;">Basic</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">Make a wooden door.</td>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT2</td>
<td style="text-align: center;">Tool (Simple)</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Make a stone pickaxe.</td>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT3</td>
<td style="text-align: center;">Hunt and Food</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Cook the beef.</td>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT4</td>
<td style="text-align: center;">Dig-Down</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Mine coal.</td>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT5</td>
<td style="text-align: center;">Equipment</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Equip the leather helmet.</td>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT6</td>
<td style="text-align: center;">Tool (Complex)</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Make shears and bucket.</td>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT7</td>
<td style="text-align: center;">IronStage</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Obtain an iron sword.</td>
<td style="text-align: center;">6000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
<tr>
<td style="text-align: left;">MT8</td>
<td style="text-align: center;">Challenge</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Obtain a diamond!</td>
<td style="text-align: center;">12000</td>
<td style="text-align: center;">Empty</td>
<td style="text-align: center;">Axe</td>
</tr>
</tbody>
</table>
<p>We choose 71 tasks from the Minecraft Universe Benchmark SkillForgeChain [22] for evaluation. These tasks are related to items that can be obtained in the Minecraft overworld. To better present the results, we divide the 71 Minecraft tasks into 8 meta groups according to the ingredients and function of the tasks, i.e., MT1-MT8. The instruction for every task is written in natural language, e.g., make a wooden door in MT1 (Basic group) and obtain a diamond in MT8 (Challenge group), as illustrated in Table 1. Considering how long it typically takes human players to complete each task as a ballpark [14], we set different maximum episode steps for different meta tasks from 3000 (for easiest Basic tasks) to 12000 (for the hardest Challenge tasks). The names, number of required skills, and functions of all tasks are listed in Appendix B. We give an empty inventory for every task in Survival mode and require the agent to obtain every item from the environment by itself. Note that our agent will be summoned in different environments randomly for each evaluation. Biomes and initial positions are also different each time. Following the previous work [18], we take the success rate as the evaluation metric.</p>
<p>Baselines We compare DEPS with other language-based planners, including GPT as Zero-shot Planner(GPT) [16], ProgPrompt(PP) [42], Chain-of-Thought(CoT) [45], Inner Monologue(IM) [17], and Code as Policies(CaP) [20]. For all baseline models, we use the same demonstration example in the prompt, the same LM model from OpenAI, and the same controller in all tasks for a fair comparison. Since these methods were not originally experimented with Minecraft, we reproduce them to conform to the Minecraft specification based on prompt and feedback template design. All planner methods access the LLM model through OpenAI API (text-davinci-03 model [32] for GPT, CoT, and IM, and code-davinci-02 model [8] for PP, CaP, and Ours). All hyperparameters of LLM (including the temperature and best_of, etc.) are kept as default. We also list the full prompt of all different methods in Appendix G.</p>
<h1>4.2 Main Results</h1>
<p>Every task is executed 30 times and the average results in Minedojo [10] for every meta task are listed in Table 2. Our approach achieves the best performance with all meta tasks. As the complexity of the task increases from MT1-MT8, the planner usually needs to give more accurate task steps (i.e., longer goal sequence) to achieve the final task. Therefore the success rate of all agents decreases with the reasoning steps increasing. Starting from MT6, almost all existing LLM-based planners fail (nearly 0 success rate). DEP (w/o Selector) already consistently beats existing LLM-based planners in all meta tasks with a significant margin. This validates that "describe, explain and plan" can estimate the reason for current plan failure and correct the original flawed plans. Due to the limited maximum episode length and restricted control success rate for a hard goal (e.g., Mine diamond with iron_pickaxe), the final success rate is still capped.</p>
<p>Table 2: Success rates of DEPS and existing LLM planners on Minecraft Task101. The full task-by-task list is in Appendix F.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>MT1</th>
<th>MT2</th>
<th>MT3</th>
<th>MT4</th>
<th>MT5</th>
<th>MT6</th>
<th>MT7</th>
<th>MT8</th>
<th>AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT<em>[16, 32]</em></td>
<td>$25.85 \pm 24.8$</td>
<td>$47.88 \pm 31.5$</td>
<td>$10.78 \pm 14.6$</td>
<td>$7.14 \pm 9.0$</td>
<td>$1.98 \pm 5.9$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>15.42</td>
</tr>
<tr>
<td>PP<em>[42]</em></td>
<td>$30.61 \pm 23.6$</td>
<td>$40.09 \pm 30.6$</td>
<td>$17.13 \pm 19.1$</td>
<td>$16.00 \pm 17.3$</td>
<td>$3.21 \pm 4.9$</td>
<td>$0.47 \pm 1.3$</td>
<td>$0.60 \pm 2.2$</td>
<td>$0.0 \pm 0.0$</td>
<td>16.88</td>
</tr>
<tr>
<td>CoT<em>[45]</em></td>
<td>$40.24 \pm 30.8$</td>
<td>$55.21 \pm 26.8$</td>
<td>$6.82 \pm 11.6$</td>
<td>$4.76 \pm 8.2$</td>
<td>$1.73 \pm 5.2$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>18.89</td>
</tr>
<tr>
<td>IM<em>[17]</em></td>
<td>$46.89 \pm 31.4$</td>
<td>$53.73 \pm 20.8$</td>
<td>$3.64 \pm 6.9$</td>
<td>$18.41 \pm 17.4$</td>
<td>$4.57 \pm 7.4$</td>
<td>$0.64 \pm 1.7$</td>
<td>$1.02 \pm 2.5$</td>
<td>$0.0 \pm 0.0$</td>
<td>21.64</td>
</tr>
<tr>
<td>CaP<em>[20]</em></td>
<td>$60.08 \pm 17.3$</td>
<td>$60.11 \pm 20.24$</td>
<td>$8.72 \pm 9.7$</td>
<td>$20.33 \pm 21.0$</td>
<td>$2.84 \pm 4.6$</td>
<td>$0.63 \pm 1.3$</td>
<td>$0.60 \pm 2.2$</td>
<td>$0.0 \pm 0.0$</td>
<td>25.77</td>
</tr>
<tr>
<td>DEP</td>
<td>$75.70 \pm 10.4$</td>
<td>$66.13 \pm 13.4$</td>
<td>$45.69 \pm 16.2$</td>
<td>$43.35 \pm 20.2$</td>
<td>$15.93 \pm 13.9$</td>
<td>$5.71 \pm 3.7$</td>
<td>$4.60 \pm 7.1$</td>
<td>$0.50 \pm 0.5$</td>
<td>39.36</td>
</tr>
<tr>
<td>DEPS</td>
<td>$79.77 \pm 8.5$</td>
<td>$79.46 \pm 10.6$</td>
<td>$62.40 \pm 17.9$</td>
<td>$53.32 \pm 29.3$</td>
<td>$29.24 \pm 27.3$</td>
<td>$13.80 \pm 8.0$</td>
<td>$12.56 \pm 13.3$</td>
<td>$0.59 \pm 0.5$</td>
<td>48.56</td>
</tr>
</tbody>
</table>
<p>In addition, selector also greatly improves the final task success rate of the agent (from DEP w/o Selector to DEPS). Hard meta tasks usually require the completion of multiple sub-goals (up to dozens of goals), thus bringing more flexibility and providing more candidate goals for the Selector. At the same time, as the agent conducts experiments with limited episode length, it also places high demands on the efficiency of the plan. Therefore, the Selector brings a significant improvement on efficiency-sensitive tasks such as MT7 (up to $\mathbf{+ 2 . 7}$ times success rate).</p>
<p>Robustness on different controller and different Minecraft versions We also evaluate DEPS on MineRL [3] and MC-Textworld [22]. Note that DEPS is a planning method, which needs to equip the goal-conditioned controller for interacting with the Minecraft environments. We choose MC-Controller [6] and Steve-1 [21] as controllers to interact with Minedojo and MineRL, respectively. These two methods are all control policies that perceive visual partial observations and produce mouse and keyboard actions. While MC-Textworld is a text world, which only keeps the Minecraft crafting recipes and mining rules. So MC-Textworld does not require the controller. The DEPS results of the task set MT1-MT8 on different Minecraft environments are shown in Table 3. The results report that DEPS can generate effective plans in various Minecraft environments. The results on MC-Textworld [22] also show that the performance drops on more difficult task sets from MT6 to MT8 are mainly from the controller limitation.</p>
<p>Table 3: Success rates of DEPS under different Minecraft environments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Version</th>
<th style="text-align: center;">Controller</th>
<th style="text-align: center;">MT1</th>
<th style="text-align: center;">MT2</th>
<th style="text-align: center;">MT3</th>
<th style="text-align: center;">MT4</th>
<th style="text-align: center;">MT5</th>
<th style="text-align: center;">MT6</th>
<th style="text-align: center;">MT7</th>
<th style="text-align: center;">MT8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MineDojo [10]</td>
<td style="text-align: center;">1.11 .2</td>
<td style="text-align: center;">$[6]$</td>
<td style="text-align: center;">79.77</td>
<td style="text-align: center;">79.46</td>
<td style="text-align: center;">62.40</td>
<td style="text-align: center;">53.32</td>
<td style="text-align: center;">29.24</td>
<td style="text-align: center;">13.80</td>
<td style="text-align: center;">12.56</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">MineRL [3]</td>
<td style="text-align: center;">1.16 .5</td>
<td style="text-align: center;">$[21]$</td>
<td style="text-align: center;">84.05</td>
<td style="text-align: center;">80.32</td>
<td style="text-align: center;">24.25</td>
<td style="text-align: center;">36.21</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">17.22</td>
<td style="text-align: center;">16.79</td>
<td style="text-align: center;">1.84</td>
</tr>
<tr>
<td style="text-align: center;">MC-Textworld [22]</td>
<td style="text-align: center;">1.19 .2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">90.00</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;">56.25</td>
<td style="text-align: center;">64.71</td>
<td style="text-align: center;">57.14</td>
<td style="text-align: center;">69.57</td>
<td style="text-align: center;">50.00</td>
</tr>
</tbody>
</table>
<h1>4.3 Ablation Study</h1>
<p>We conduct ablation experiments to investigate the number of candidate executable goals for different Selector models and the specific impact of the rounds of DEPS.</p>
<h3>4.3.1 Ablation on Selector</h3>
<p>We verify the robustness of our proposed Selector under different parallel goals. The agent is asked to complete 2,3 , and 4 candidate goals (the precondition is consistent for all goals), respectively. The goals of the task correspond to different kinds of mobs or materials.
We report the final success rate of our method (DEP) with different selector implementations, including using a fixed sequence of goals, a random sequence of goals, and selecting a goal based on MineCLIP [10], CLIP [35], and our horizon-predictive Selector (HPS). As Figure 4 shows, in one round of parallel candidate goals, an improvement of $\Delta=+22.3 \%,+29.2 \%,+32.6 \%$ is obtained using our horizon-predictive Selector compared to not any selector (i.e., fixed plan), respectively.
At a limited episode length, e.g., 1000 steps, goal-model shows a greater advantage, which proves that goal-model can improve the execution efficiency of the plan in embodied environments. In addition, compared to using vision-language models such as CLIP [35] and MineCLIP [10] as a goal model, horizon-predictive has the best performance due to better estimation of the horizon information. The curve trend also demonstrates that agents with Selector scale up under large amounts of goals in an open-world environment.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The success rates of DEPS with different selectors under varying numbers of parallel goals and maximum episode lengths.</p>
<p>Table 4: Success Rate of DEPS under different maximum rounds of re-planning. Round 0 represents the vanilla Planner w/o the re-planning process. $\infty$ represents the re-planning process will not end until task success or reaching the maximum horizon, which is still limited by the maximum tokens of LLMs. The maximum number of rounds for Codex is around 7-8 rounds.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rounds</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">$\infty$</th>
<th style="text-align: center;">$\Delta$ <br> $(0 \rightarrow \infty)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT1</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">+51.2</td>
</tr>
<tr>
<td style="text-align: center;">MT2</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">+42.4</td>
</tr>
<tr>
<td style="text-align: center;">MT3</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">+47.3</td>
</tr>
<tr>
<td style="text-align: center;">MT4</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">+37.4</td>
</tr>
<tr>
<td style="text-align: center;">MT5</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">+26.0</td>
</tr>
<tr>
<td style="text-align: center;">MT6</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">+13.3</td>
</tr>
<tr>
<td style="text-align: center;">MT7</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">+12.0</td>
</tr>
<tr>
<td style="text-align: center;">MT8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">+0.6</td>
</tr>
</tbody>
</table>
<h1>4.3.2 Ablation on Re-Planning Rounds</h1>
<p>We evaluate our agent on all tasks with increasing maximum rounds of DEPS. The round is defined as a cycle of interactive LLM-based planning with description, explanation, and planning and selecting, i.e., an updated plan. All tasks for every maximum round are executed 30 times and the average success rate is reported in Table 4. We take the vanilla LLM planner as the baseline, in which the model takes the initially generated plan as the final execution plan, without involving any description, re-planning, or self-explanation processes during the task execution. Our results in the previous subsection utilize the maximum rounds possible under maximum tokens capped by OpenAI. We also report the success rate increment from vanilla planner to DEPS of every meta task in column $\Delta$ in Table 4. This set of experiments demonstrates that DEPS can iteratively improve its plan in open-world environments. More description, self-explanation, and re-planning rounds produce better results, especially for hard tasks.</p>
<h3>4.4 ObtainDiamond Challenge</h3>
<p>Mining diamonds in the open-world game Minecraft, i.e. MT8 in Table 2, has been a long-standing challenge for the community [14]. It is challenging because mining diamonds from scratch in Minecraft involves acquiring a sequence of difficult-to-obtain items that require complex planning on goals like mining, inventory management, crafting with and without a crafting table, tool use, smelting iron ingot in a furnace, and mining at the lowest depths. We take the ObtainDiamond task as a bonus experiment to show the capabilities of our zero-shot planner on complex tasks in embodied environments. Previous methods' success rates on this challenge further vouch for its difficulty. [43, 34] leverages domain-secific reward functions and RL fine-tuning to achieve $\smile 0.1 \%$ success rate in 15 minutes of game play. VPT further boosts the success rate to $20 \%$ within 20 minutes of play through pre-training on collects $\smile 70 \mathrm{k}$ hours human demonstrations and finetuning with human-designed reward function [3]. DreamerV3 is trained from scratch to collect diamonds in a modified Minecraft environment (easier to break blocks) with world models to achieve a success rate of $2 \%$ [15].</p>
<p>Our DEPS manages to achieve on-par performance in this grand challenge; our agent achieves a $0.59 \%$ success rate within 10 minutes of gameplay. Note our method does not specifically fine-tune for this challenge. It is designed to be multi-task in its nature. Furthermore, considering our planner operates with demonstration prompts on a fixed Large Language Model, it can be straightforwardly adapted to other open-ended environments with modifications.</p>
<h2>5 Related Works</h2>
<p>Task planning with LLMs There have been some methods leveraging the large language model to generate action plans for high-level tasks in embodied environments [46, 9, 11]. [16] decompose natural language commands into sequences of executable actions by text completion and semantic</p>
<p>translation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances from value functions [4]. For better executing the plan in embodied environments, some methods use an object detector describing the initial environment into the language prompt to produce environment-suitable plans and adopt success detectors to check that each step is executed successfully [17, 20]. [42] and [20] use the pythonic-style prompt to produce more executable plans. However, all of the above methods assume that the initial plan from the LLM is correct. When there are bugs in the initial plan, it's difficult for the agent to finish the task successfully.</p>
<p>Interactive Planning with LLMs Inner Monologue [17] pilots the front of interactive planning with LLMs, which introduces the feedback (including success detection and scene description) to the planner. However, we found it could still suffer from accumulative planning error, especially in long-horizon open-world tasks. Rather, our "Describe, Explain, Plan and Select" (DEPS) method can produce more reliable plans by leveraging chain-of-thought thinking and explanation to locate the errors in previous plans. Moreover, we also propose a goal Selector to further improve the efficiency of the plan, thereby yielding much better performances. Readers are encouraged to refer to the comparative results in Section 4.2 between DEPS and these prior arts. There are also some concurrent works on planning with LLMs [39, 26, 23, 33, 47].</p>
<p>Agents in Minecraft Some previous works have employed the hierarchical architecture to solve long-horizon tasks in Minecraft [30, 27, 24]. Recently, based on the internet-scale corpus, [10] pre-trains a language-conditioned reward function and learns multi-task MineAgent. [3] collects a vast amount of human demonstrations to train a behavior cloning agent. More recently, [15] utilized a learned world model to distill a policy that can efficiently explore in Minecraft. There are also some works focus on learning goal-conditioned policies for better instruction-following [6, 7, 21]. While these efforts all focus on improving the low-level controller. Rather, the planner in our architecture emphasizes applying domain knowledge to propose and arrange the sub-goals. It significantly influences the complexity and breadth of tasks that the agent can handle. Moreover, our planner is zero-shot, making it possible to generalize to other long-horizon open worlds.</p>
<h1>6 Limitations</h1>
<p>Albeit the impressive results of our approach, we believe there are at least two major limitations within our approach. First of all, our framework relies on privately-held LLMs like GPT-3 and ChatGPT, which makes it less accessible to those who cannot afford or access the service. However, we're fully committed to ensuring a more democratized method and will explore using open-sourced models including OPT [48] and BLOOM [38]. Another issue is the explicit step-by-step planning in our system. Although it brings us superior performances over the baselines, the planning bottleneck can also prevent our model from being further scaled up. A more appealing approach will be amortizing the planning within an end-to-end trainable goal-conditioned policy, which is worth exploring next. Furthermore, some previous fundamental challenges in planning (e.g., dead ends) may not prevalent in our adopted environments and hence could be inadvertently overlooked by our paper. We are dedicated to addressing more fundamental challenges present in building a multi-task generalist agent in our series of following work.</p>
<h2>7 Conclusion</h2>
<p>We investigate the problem of planning in open worlds. We identify two major challenges unique to these environments: 1) long-term planning requires precise and multi-step reasoning, and 2) planning efficiency could be compromised since canonical planners do not take the agent's proximity to parallel goals/subtasks into consideration. We propose "Describe, Explain, Plan and Select" (DEPS), an interactive approach based on Large Language Models (LLMs) to tackle them both. Our experiments in the challenging Minecraft domain verify the advantages of our approach over counterparts by marking the milestone of robustly accomplishing 70+ Minecraft tasks and nearly doubling the overall performances. DEPS also is the first planning-based agent that can reach the diamond in this game.</p>
<h1>Acknowledgements</h1>
<p>This work is funded in part by the National Key R\&amp;D Program of China #2022ZD0160301, a grant from CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441, #CCF-1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI. We thank Dai Zhixiang from NVIDIA and Xu Hongming from BIGAI on training LLMs and infrastructure supports, respectively.</p>
<h2>References</h2>
<p>[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 1
[2] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, 2017. 1
[3] B. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv:2206.11795, 2022. 7, 8, 9, 10
[4] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference on Robot Learning, 2022. 1, 2, 3, 5, 10, 21
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1, 3, 15, 21, 22
[6] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023. $6,8,10,21,24$
[7] S. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions by watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023. 10
[8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 3, 7, 21, 22
[9] I. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus. Collaborating with language models for embodied reasoning. In NeurIPS Foundation Models for Decision Making Workshop, 2022. 9
[10] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems Datasets and Benchmarks, 2022. 1, 2, 7, 8, 10, 18, 21, 26
[11] R. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023. 9
[12] W. H. Guss, M. Y. Castro, S. Devlin, B. Houghton, N. S. Kuno, C. Loomis, S. Milani, S. P. Mohanty, K. Nakata, R. Salakhutdinov, J. Schulman, S. Shiroshita, N. Topin, A. Ummadisingu, and O. Vinyals. The minerl 2020 competition on sample efficient reinforcement learning using human priors. arXiv: Learning, 2021. 2
[13] W. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. Mohanty, D. P. Liebana, R. Salakhutdinov, N. Topin, et al. Neurips 2019 competition: the minerl competition on sample efficient reinforcement learning using human priors. arXiv preprint arXiv:1904.10079, 2019. 2
[14] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019. 2, 7, 9, 24
[15] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. 9, 10</p>
<p>[16] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ICML, 2022. 3, 4, 7, 8, 9, 15, 16, 17, 21, $24,25,26,28$
[17] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. 2, 3, 5, 7, 8, 10, 15, 16, 17, 24, 25, 26, 30, 34, 35
[18] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence experimentation. In Ijcai, pages 4246-4247. Citeseer, 2016. 2, 7
[19] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, W. Hong, Z. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin, Y. Belousov, O. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons learned. neural information processing systems, 2022. 2
[20] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022. 2, 3, 5, 7, 8, 10, 25, 26, 31
[21] S. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for text-to-behavior in minecraft. arXiv preprint arXiv:2306.00937, 2023. 8, 10
[22] H. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023. 6, 7, 8, 18
[23] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023. 10
[24] Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sampleefficient hierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021. 10
[25] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. 1
[26] J. Mai, J. Chen, B. Li, G. Qian, M. Elhoseiny, and B. Ghanem. Llm as a robotic brain: Unifying egocentric memory and control. arXiv preprint arXiv:2304.09349, 2023. 10
[27] H. Mao, C. Wang, X. Hao, Y. Mao, Y. Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sampleefficient hierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third International Conference, DAI 2021, Shanghai, China, December 17-18, 2021, Proceedings 3, pages 38-51. Springer, 2022. 10
[28] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. 23
[29] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 2
[30] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pages 2661-2670. PMLR, 2017. 10
[31] OpenAI. Gpt-4 technical report, 2023. 21, 22
[32] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. 3, 5, 7, 8
[33] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023. 10
[34] V. P. Patil, M. Hofmarcher, M.-C. Dinu, M. Dorfer, P. M. Blies, J. Brandstetter, J. A. ArjonaMedina, and S. Hochreiter. Align-rudder: Learning from few demonstrations by reward redistribution. ICML, 2020. 9
[35] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 5, 6, 8</p>
<p>[36] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022. 1
[37] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 23
[38] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. 10
[39] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023. 10
[40] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning. PMLR, 2022. 2, 3, 15, 16, 17, 23
[41] M. Shridhar, X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. 2, 3, 15, 16
[42] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022. 3, 4, 7, 8, 10, 25, 26, 28
[43] A. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V. Davydov, and A. I. Panov. Forgetful experience replay in hierarchical reinforcement learning from expert demonstrations. Knowledge-Based Systems, 218:106844, 2021. 9
[44] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 22
[45] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. 36th Conference on Neural Information Processing Systems (NeurIPS 2022), 2022. 5, 7, 8, 15, 25, 26, 29
[46] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. 2, 9
[47] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, et al. Proagent: Building proactive cooperative ai with large language models. arXiv preprint arXiv:2308.11339, 2023. 10
[48] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 10</p>
<h1>Appendix</h1>
<h2>Contents</h2>
<p>A Additional Experiments ..... 15
A. 1 ALFWorld ..... 15
A.1.1 Tasks ..... 15
A.1.2 Results ..... 15
A. 2 Tabletop Manipulation ..... 16
A.2.1 Tasks ..... 16
A.2.2 Results ..... 17
B Minecraft Task Details ..... 18
C DEPS Implementation Details ..... 20
C. 1 Controller ..... 20
C. 2 LLM as Planner ..... 21
C. 3 LLM as Explainer ..... 22
C. 4 Other modules ..... 23
D Comparison with other LLM-based Planners ..... 23
E Discussion on ObtainDiamond Task ..... 24
F Success Rates of ALL Tasks in Minecraft ..... 25
G Prompt for Different Tasks and Different Methods ..... 27
G. 1 Prompt for Minecraft Tasks ..... 27
G.1.1 DEPS ..... 27
G.1.2 Baselines ..... 28
G. 2 Prompt for ALFWorld Tasks ..... 31
G.2.1 DEPS ..... 31
G.2.2 Baselines ..... 33
G. 3 Prompt for Tabletop Manipulation Tasks ..... 34
G.3.1 DEPS ..... 34
G.3.2 Baselines ..... 35
H Full Dialogue ..... 35</p>
<h1>A Additional Experiments</h1>
<p>Additional experiments are conducted on the ALFWorld [41] and Tabletop Manipulation environments [40] to showcase the generalization capabilities of DEPS.</p>
<h2>A. 1 ALFWorld</h2>
<p>ALFWorld [41] is an interactive learning environment that aligns text and embodiment, allowing agents to acquire abstract, text-based policies in TextWorld, and subsequently execute goals from the ALFRED benchmark in a visually rich environment.</p>
<h2>A.1.1 Tasks</h2>
<p>The ALFWorld framework contains six types (namely Pick \&amp; Place, Examine in Light, Clean \&amp; Place, Heat \&amp; Place, Cool \&amp; Place, Pick Two \&amp; Place) of tasks with various difficulty levels. Tasks involve first finding a particular object, which often requires the agent to open and search receptacles like drawers or cabinets. Subsequently, all tasks other than Pick \&amp; Place require some interaction with the object such as heating (place the object in a microwave and start it) or cleaning (wash the object in a sink). To complete the task, the object must be placed in the designated location. We sample 10 tasks from ALFWorld randomly and list all the task names, types, and the number of receptacles in Table 5. We classify them into 6 groups based on their functionality. For all tasks, the maximum number of steps is set as 50 .</p>
<p>Table 5: Task list in ALFWorld.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Group</th>
<th style="text-align: center;">No.</th>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Number of Receptacles</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pick \&amp; Place</td>
<td style="text-align: center;">1</td>
<td style="text-align: left;">put some soapbottle on garbagecan</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: left;">put a tissuebox in dresser</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">put some soapbar on drawer</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: left;">Clean \&amp; Place</td>
<td style="text-align: center;">4</td>
<td style="text-align: left;">put a clean soapbar in bathtubbasin</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: left;">clean some tomato and put it in fridge</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: left;">Cool \&amp; Place</td>
<td style="text-align: center;">6</td>
<td style="text-align: left;">put a cool tomato in countertop</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: left;">put a cool bread in countertop</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: left;">Heat \&amp; Place</td>
<td style="text-align: center;">8</td>
<td style="text-align: left;">heat some cup and put it in cabinet</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: left;">Pick Two \&amp; Place</td>
<td style="text-align: center;">9</td>
<td style="text-align: left;">find two cup and put them in cabinet</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: left;">Examine in Light</td>
<td style="text-align: center;">10</td>
<td style="text-align: left;">look at mug under the desklamp</td>
<td style="text-align: center;">18</td>
</tr>
</tbody>
</table>
<p>We select the GPT as Zero-Shot Planner (GPT) [16] and Inner Monologue (IM) [17] as baseline methods. For the Inner Monologue, the planning goal is the next goal among all candidate goals. For the GPT and DEP, which produce the full plan at once, the planning goal is the full plan (a goal sequence). Then the plan will be executed step-by-step, i.e., the current goal will be given to the controller and select suitable action according to the current state. The goal termination module is also employed with the LLM. For better demonstrate the effectiveness of self-explanation in DEP, we also augment the zero-shot planner with re-planning ability (GPT+RP). All planner methods access the LLM model through OpenAI API (text-davinci-03 model [5]). Since ALFWorld is a text world, the environment will be given a literal description and candidate language-conditioned actions for each state, so the controller under ALFWorld is also LLM-based. Chain-of-Thought [45] is also employed in the controller for better decision-making. All prompts for planner and controller in ALFWorld are listed in Section G.2.</p>
<h2>A.1.2 Results</h2>
<p>Each task is executed five times, and the average results for each task group are presented in Table 6. BUTLER is the a training-based method, the results are sourced from [41]. Re-planning is a crucial capability in complex and exploratory environments. The short-horizon planning approach (IM) with</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Planning in the ALFWorld experiments.
re-planning capability outperforms the long-horizon planning approach (GPT) without re-planning capability with a large margin. Furthermore, the long-horizon planning method augmented with re-planning capability (GPT+RP) achieves superior performance ranging from $10 \%$ (GPT) to $52 \%$. DEP further enhances the feasibility of planning with descriptions and self-explanation. Notably, all planning methods fail on Place Two \&amp; Place tasks, which is attributable to LLM's lack of requisite knowledge for this task. It is worth investigating how to effectively incorporate the distinctive knowledge of an environment into LLM.</p>
<p>Table 6: Success rates of tasks in ALFWorld.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Group</th>
<th style="text-align: center;">BUTLER [41]</th>
<th style="text-align: center;">GPT [16]</th>
<th style="text-align: center;">GPT+RP</th>
<th style="text-align: center;">IM [17]</th>
<th style="text-align: center;">DEP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pick \&amp; Place</td>
<td style="text-align: center;">$46.0 \%$</td>
<td style="text-align: center;">$33.3 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$33.3 \%$</td>
<td style="text-align: center;">$93.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Clean \&amp; Place</td>
<td style="text-align: center;">$39.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$10.0 \%$</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$50.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Cool \&amp; Place</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Heat \&amp; Place</td>
<td style="text-align: center;">$74.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$80.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Pick Two \&amp; Place</td>
<td style="text-align: center;">$24.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Examine in Light</td>
<td style="text-align: center;">$22.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">$37.0 \%$</td>
<td style="text-align: center;">$10.0 \%$</td>
<td style="text-align: center;">$52.0 \%$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$76.0 \%$</td>
</tr>
</tbody>
</table>
<h1>A. 2 Tabletop Manipulation</h1>
<p>The Tabletop Manipulation experiments are conducted on a Universal Robot UR5e with a suction gripper in the simulated environments [40].</p>
<h2>A.2.1 Tasks</h2>
<p>The assessment of all methods is conducted in five seen tasks, as illustrated in Table 7, wherein the seen tasks are employed for training the CLIPort [40] as the controller. The task involves a robotic arm equipped with a gripper, which is tasked with rearranging a number of blocks and bowls on a table to achieve a desired configuration specified via natural language (e.g., "putting the blocks in the bowls with matching colors").</p>
<p>Table 7: Task list in CLIPort.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">No</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Assembling Kits</td>
<td style="text-align: center;">Put the objects in the corresponding holes.</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Towers of Hanoi</td>
<td style="text-align: center;">Move the rings to the darker brown side.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Put Block in Bowl</td>
<td style="text-align: center;">Match the blocks and the bowls.</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Packing Shapes</td>
<td style="text-align: center;">Pack the objects in the brown box.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Stack Block Pyramid</td>
<td style="text-align: center;">Stack the blocks into a pyramid.</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Planning in the Tabletop Manipulation experiments.</p>
<p>We utilized Inner Monologue (IM) [17] and Zero-shot Planner (GPT) [16] as planning baselines, in addition to comparing with a multi-task CLIPort policy directly trained on long-horizon task instructions (i.e., without utilizing LLM for planning). As CLIPort is a single-step policy that does not spontaneously terminate during policy rollout, we report CLIPort evaluations with Oracle termination (i.e., repeat until the Oracle indicates task completion) and fixed-step termination (i.e., repeat for $k$ steps). For Inner Monologue, which directly produces the next-step goal and terminates when the LLM ceases to generate new steps, we similarly set the maximum number of steps to be $k$ for practical considerations. For the zero-shot planner [16] and our DEP, which produce the full plan at once, they are augmented with the LLM-based termination. DEP also involves the description, explanation, and re-planning process. The same $k$ step is suitable for these two methods. In practice, $k$ is set as 15 . The prompts for all methods are listed in Section G.3. We use the checkpoints provided by CLIPort as the controller and all planner methods access the ChatGPT (as LLM) through OpenAI API (gpt-3.5-turbo model). Each task is evaluated 5 times with different seeds.</p>
<h1>A.2.2 Results</h1>
<p>The results of each method are listed in Table 8. All LLM-based planning methods perform well on tabletop rearrangement tasks. Given the compact nature of the tabletop environment, the performance gap among the various LLM-planning methods is not as pronounced as in the open-ended Minecraft. This observation underscores the robust generalization capabilities of LLM-based planning methods across diverse environments.</p>
<p>Table 8: Success rates for various methods across different tasks in Tabletop Manipulation environment.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">CLIPort [40] +oracle</th>
<th style="text-align: center;">GPT [16]</th>
<th style="text-align: center;">IM [17]</th>
<th style="text-align: center;">DEP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Assembling Kits</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Towers of Hanoi</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Put Block in Bowl</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$82.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Packing Shapes</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Stack Block Pyramid</td>
<td style="text-align: center;">$80.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
<td style="text-align: center;">$40.0 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: center;">$76.0 \%$</td>
<td style="text-align: center;">$80.0 \%$</td>
<td style="text-align: center;">$56.4 \%$</td>
<td style="text-align: center;">$80.0 \%$</td>
</tr>
</tbody>
</table>
<h1>B Minecraft Task Details</h1>
<p>To fully validate the multitask planning and execution capability of our agent, we choose over 70 tasks from the Minecraft Universe Benchmark [22] as the set of evaluation tasks. These tasks are related to items that can be obtained in the Minecraft overworld. These tasks are also a subset of MineDojo [10] programmatic tasks. Minedojo exists some programmatic tasks sharing the same object item given different conditions (e.g., obtain wool given shear or obtain wool given nothing). Minedojo expands the richness of the same tasks (sharing the same Minecraft item as an object) by giving different initial conditions (e.g., obtain wool given shears or obtain wool given nothing). We keep only the 71 hardest conditions (i.e. given nothing) as tasks.
We list all task names, objects, and their required skills number for planning from Table 9 to Table 16. Object item is used as the basis for the successful completion of the task. These objects cannot be obtained directly from the environment, and usually require multiple goals (i.e., reasoning steps) to be constructed. Here we only consider the number of required goal types, and multiple identical goals are unified into 1 reasoning step. Note that the reasoning steps for each task are not fixed, and as the initial state of the agent and the biome is in change, more reasoning steps may be required to complete it, we only report the most basic case here.
As shown in Figure 4, for each task, a relaxed (longer) maximum episode steps will increase the success rate of the task. To fully test the efficiency of our method, we set an upper limit on the episode length for each task. Since different tasks have different difficulty levels, we double the average completion time of human players for different meta-tasks as the upper limit of the episode. The play time are computed as corresponding maximum steps (i.e., Max. Steps in Table 1) of episode length at 20 Hz .</p>
<p>Table 9: Task details on MT1 Basic set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT1 <br> Basic</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">CraftPlanks</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">planks</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Obtain a plank.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">CraftSticks</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">stick</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stick.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">CraftWoodenSlab</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">wooden_slab</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden slab.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">CraftWoodenPressure</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">wooden_pressure</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden pressure plate.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">CraftBowl</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">bowl</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a bowl.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">CraftWoodenButton</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">wooden_button</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden button.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">CraftChest</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">chest</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a chest.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">CraftOakStairs</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">oak_stairs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an oak stair.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">CraftSign</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">sign</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a sign.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">CraftFence</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">fence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a fence.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">CraftFenceGate</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">fence_gate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a fence gate.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">CraftBoat</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">boat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a boat.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">CraftTrapdoor</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">trapdoor</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a trap door.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">CraftWoodenDoor</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">door</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a door.</td>
</tr>
</tbody>
</table>
<p>Table 10: Task details on MT2 Tool (Simple) set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT2 <br> Tool <br> (Simple)</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">CraftCraftingTable</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">crafting_table</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Obtain a crafting table.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">CraftWoodenPickaxe</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">wooden_pickaxe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden pickaxe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">CraftWoodenAxe</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">wooden_axe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden axe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">CraftWoodenHoe</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">wooden_hoe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden hoe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">CraftWoodenSword</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">wooden_sword</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden sword.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">CraftWoodenShovel</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">wooden_shovel</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a wooden shovel.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">CraftFurnace</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">furnace</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a furnace.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">CraftStonePickaxe</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_pickaxe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone pickaxe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">CraftStoneAxe</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_axe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone axe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">CraftStoneHoe</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_hoe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone hoe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">CraftStoneShovel</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_shovel</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone shovel.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">CraftStoneSword</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_sword</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone sword.</td>
</tr>
</tbody>
</table>
<p>Table 11: Task details on MT3 Hunt and Food set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT3 <br> Hunt <br> $\&amp;$ <br> Food</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">CraftBed</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">bed</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Obtain a bed.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">CraftPainting</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">painting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a painting.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">CraftCarpet</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">carpet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a carpet.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">CraftItemFrame</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">item_frame</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an item frame.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">CookPorkchop</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">cooked_porkchop</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cook the porkchop.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">CookBeef</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">cooked_beef</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cook the beef.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">CookMutton</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">cooked_mutton</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cook the mutton.</td>
</tr>
</tbody>
</table>
<p>Table 12: Task details on MT4 Dig-Down set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT4 <br> Dig-Down</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">CraftStoneStairs</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_stairs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone stair.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">CraftStoneSlab</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_slab</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone slab.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">CraftArmorStand</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">armor_stand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an armor stand.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">CraftCobblestoneWall</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">cobblestone_wall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a cobblestone wall.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">CraftQuartzBlock</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">quartz_block</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a quartz block.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">CraftStoneBrick</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">stone_brick</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone brick.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">SmeltStone</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">stone</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Smelt a stone.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">CraftTorch</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">torch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone brick.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">ObtainCoal</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">coal</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mine coal.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">CraftStoneBrickStairs</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">stonebrick_stairs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone brick.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">CraftStonePressurePlate</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">stone_pressure_plate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone brick.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">CraftStoneButton</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">stone_button</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone brick.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">CraftLever</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">level</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a stone brick.</td>
</tr>
</tbody>
</table>
<p>Table 13: Task details on MT5 Equipment set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT5 <br> Equipment</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">EquipLeatherBoots</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">leather_boots</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Equip the leather boot.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">EquipLeatherChestplate</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">leather_chestplate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the leather chestplate.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">EquipLeatherHelmet</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">leather_helmet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the leather helmet.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">EquipLeatherLeggings</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">leather_leggings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the leather leggings.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">EquipShield</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">shield</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the shield.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">EquipIronChestplate</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_chestplate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the iron chestplate.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">EquipIronLeggings</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_leggings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the iron leggings.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">EquipIronHelmet</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_helmet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the iron helmet.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">EquipIronBoots</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_boots</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equip the iron boots.</td>
</tr>
</tbody>
</table>
<p>Table 14: Task details on MT6 Tool (Complex) set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT6 <br> Tool <br> (Complex)</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">CraftBucket</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">bucket</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Obtain a bucket.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">CraftShears</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">shears</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Make shears.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">CraftIronPickaxe</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_pickaxe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron pickaxe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">CraftIronAxe</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_axe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron axe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">CraftIronHoe</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_hoe</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron hoe.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">CraftIronShovel</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_shovel</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron shovel.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">CraftIronSword</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_sword</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron sword.</td>
</tr>
</tbody>
</table>
<p>Table 15: Task details on MT7 Iron-Stage set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Meta-Task</th>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Task Name</th>
<th style="text-align: center;">Required Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial Inventory</th>
<th style="text-align: center;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MT7 <br> Iron-Stage</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">CraftIronBars</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_bars</td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Obtain an iron bar.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">CraftIronNugget</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_nugget</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron nugget.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">CraftMinecart</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">minecart</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a minecart.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">CraftHopper</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">hopper</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a hopper.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">CraftHopperMinecart</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">hopper_minecart</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a hopper minecart.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">CraftFurnaceMinecart</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">furnace_minecart</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a furnace minecart.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">CraftCauldron</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">cauldron</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a cauldron.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">CraftChestMinecart</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">chest_minecart</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a chest minecart.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">CraftIronDoor</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_door</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron door.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">CraftIronTrapdoor</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">iron_trapdoor</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain an iron trapdoor.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">CraftTripwireHook</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">tripwire_hook</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a tripwire hook.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">CraftHWPressurePlate</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">heavy_weighted_plate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a heavy weighted plate.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">CraftRail</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">rail</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Obtain a rail.</td>
</tr>
</tbody>
</table>
<p>Table 16: Task details on MT8 Challenge set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta-Task</th>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">Task Name</th>
<th style="text-align: center;">Required <br> Skills</th>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Initial <br> Inventory</th>
<th style="text-align: left;">Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Challenge <br> MT8</td>
<td style="text-align: left;">76</td>
<td style="text-align: left;">ObtainDiamond</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">diamond</td>
<td style="text-align: center;">null</td>
<td style="text-align: left;">Obtain a diamond.</td>
</tr>
</tbody>
</table>
<h1>C DEPS Implementation Details</h1>
<p>We study three different implementations of DEPS for each of the experimental settings. While each version incorporates description and self-explanation to improve planning of LLM, there are differences in the internal components of each system, as seen in Table 17.</p>
<p>Table 17: Comparison between different versions of DEPS implemented in three different environments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Minecraft</th>
<th style="text-align: left;">ALFWorld</th>
<th style="text-align: left;">Tabletop Manipulation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: left;">code-davinci-02</td>
<td style="text-align: left;">text-davinci-03</td>
<td style="text-align: left;">gpt-3.5-turbo</td>
</tr>
<tr>
<td style="text-align: left;">Controller</td>
<td style="text-align: left;">Behavior Cloning Learned</td>
<td style="text-align: left;">LLM-based</td>
<td style="text-align: left;">CLIPort</td>
</tr>
<tr>
<td style="text-align: left;">Descriptor</td>
<td style="text-align: left;">Inventory Description</td>
<td style="text-align: left;">Env Support</td>
<td style="text-align: left;">heuristics</td>
</tr>
<tr>
<td style="text-align: left;">Explainer</td>
<td style="text-align: left;">LLM-based</td>
<td style="text-align: left;">LLM-based</td>
<td style="text-align: left;">LLM-based</td>
</tr>
<tr>
<td style="text-align: left;">Selector</td>
<td style="text-align: left;">Horizon Prediction Module</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
</tbody>
</table>
<h2>C. 1 Controller</h2>
<p>As the name implies, tasks in Minecraft are usually related to mine and craft goals. Mine goals require the agent to collect raw materials from the environment using the appropriate tools. Craft goals ask the agent to synthesize using existing materials. Any raw material used requires the agent to collect through suitable tools (e.g., diamonds can only be collected by an iron pickaxe or a better pickaxe). So a task usually requires dozens of step-by-step mine and craft goals, as the required skills in Table 9. Note that the successful execution of a task needs to satisfy certain exact numerical constraints due to the presence of strict generation recipes in the environment (e.g., a log can craft 4 planks, so harvesting 6 planks requires at least 2 logs). When the number of materials collected is not enough, the goal cannot be completed successfully. When more materials are collected than actually needed, the execution success rate of the task could also be reduced because the plan can not be finished under the maximum action steps.</p>
<p>Table 18: The success rate of different skill/goal with imitation learning controller.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: left;">Skill Description</th>
<th style="text-align: center;">Success Rate</th>
<th style="text-align: center;">Episode Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: left;">Mine 1 oak wood</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Mine birch wood</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Mine 1 cobblestone with pickaxe</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Mine 1 stone with pickaxe</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Mine 1 seed</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: left;">Mine 1 leaves with shears</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: left;">Mine 1 dirt</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: left;">Mine 1 iron ore with stone pickaxe</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">3000</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: left;">Mine 3 iron ore with stone pickaxe</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">3000</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: left;">Mine 1 diamond with iron pickaxe</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">12000</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: left;">Mine 1 diamond with stone pickaxe</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">12000</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: left;">Kill 1 sheep with axe</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: left;">Kill 1 cow with axe</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: left;">Kill 1 chicken with axe</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: left;">Kill 1 pig with axe</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: left;">Kill 1 llama</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: left;">Equip tool on mainland</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;">17-261</td>
<td style="text-align: left;">Craft w/o crafting_table</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Craft w/ crafting_table</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">600</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">Smelt w/ furnace</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">600</td>
</tr>
</tbody>
</table>
<p>We designed the agent's skill space based on these goals, as shown in Table 18, with a total of 262 goals. Every goal is designed with an objective item (e.g., 1 minecraft: cobblestone for skill "Mine 1 cobblestone with pickaxe"), which is used to evaluate the achievement of the goal. The skill, as a goal-conditioned policy $\pi(a \mid s, g)$ for decision-making, maps the current state $s$ and goal $g$ to action $a$. The goal is specified as natural language instructions here, which is similar to [4].
When training the controller, we adopt the observation space provided by MineDoJo [10], which includes an RGB camera view, yaw/pitch angle, GPS location, and the type of $3 \times 3$ blocks surrounding the agent. We discretize the original multi-discrete action space provided by MineDojo into 42 discrete actions. We use the proposed imitation learning method proposed by [6] in training. To be specific, a modified goal-sensitive Impala CNN is used as the backbone network. The success rate under a fixed episode length of every skill is listed in Table 18.</p>
<h1>C. 2 LLM as Planner</h1>
<p>DEPS relies on Large Language Models (LLMs) to generate language-based plans. In our Minecraft experiment, we chose Codex [8] as the LLM Planner because it can accept longer input tokens and is cost-effective. However, DEPS is compatible with various types of LLMs. Therefore, we used GPT3 [5] and ChatGPT as LLM Planners in the ALFWorld and Tabletop Manipulation experiments, respectively. Due to the effective planning and error correction performance of DEPS, the initial plan generated by the LLM has little impact on the final performance of the Agent. We also conduct ablation experiments on
even if the initial plan generated by the LLM has low accuracy, DEPS can generate a final feasible plan through self-explanation and re-planning. Therefore, we conducted ablation experiments on LLM in Minecraft.
We choose Codex [8], ChatGPT, GPT3 [5], and recent GPT-4 [31] as Planners. We used Vanilla Planner [16] as baselines and excluded the re-planning process. Given the same prompt with DEPS, the performance of baseline models reflects the planning ability of different LLMs. The success rate of baseline and DEPS on different LLMs are reported in Table 19.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Similar to $[5,16]$, "zero-shot" here means no gradient updates are performed. However we provide some related demonstrations as prompts during inference time.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
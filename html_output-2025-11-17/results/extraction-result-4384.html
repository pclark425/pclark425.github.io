<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4384 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4384</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4384</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-266999747</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.06945v2.pdf" target="_blank">Knowledge-Centric Templatic Views of Documents</a></p>
                <p><strong>Paper Abstract:</strong> Authors seeking to communicate with broader audiences often share their ideas in various document formats, such as slide decks, newsletters, reports, and posters. Prior work on document generation has generally tackled the creation of each separate format to be a different task, leading to fragmented learning processes, redundancy in models and methods, and disjointed evaluation. We consider each of these documents as templatic views of the same underlying knowledge/content, and we aim to unify the generation and evaluation of these templatic views. We begin by showing that current LLMs are capable of generating various document formats with little to no supervision. Further, a simple augmentation involving a structured intermediate representation can improve performance, especially for smaller models. We then introduce a novel unified evaluation framework that can be adapted to measuring the quality of document generators for heterogeneous downstream applications. This evaluation is adaptable to a range of user defined criteria and application scenarios, obviating the need for task specific evaluation metrics. Finally, we conduct a human evaluation, which shows that people prefer 82% of the documents generated with our method, while correlating more highly with our unified evaluation framework than prior metrics in the literature.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4384.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4384.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Templatic Views</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Centric Templatic Views of Documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified LLM-powered two-step approach that extracts a structured intermediate representation (JSON) from an input scientific paper and then prompts an LLM to generate diverse templatic views (slides, posters, blog posts) from that representation; includes a style-parameter mechanism and a template-adaptable evaluation framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-Centric Templatic Views</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-step, LLM-centered pipeline: (1) Intermediate Representation Generation — an LLM is prompted to extract the most important information from a source scientific paper and format it into a predefined JSON schema (panels, headings, salient sentences, metadata). (2) Templatic View Generation — the same or another LLM is prompted (with a short style parameter describing the desired template) to turn the JSON representation into a LaTeX-formatted target document (slide deck, poster, or blog post). The pipeline emphasizes minimal prompt engineering, uses style parameters to adapt to different templates, and forces LaTeX compilation to surface formatting errors as part of evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt35-16k (gpt-3.5 16k), gpt4-32k (GPT-4 32k), Mistral-7B, Mixtral (as reported in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Structured prompting with a fixed JSON schema (intermediate representation extraction); text/layout extraction via Azure Document Layout/OCR for input preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Two-step structured generation: generate structured representation then generate templated document; multi-step reasoning via intermediate representation (analogous to chain-of-thought/content-planning prompting). No explicit multi-document synthesis procedure is proposed — synthesis is per input document into multiple templatic views.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on existing datasets: sampled 1,000 examples from DOC2PPT, 505 accessible blog references from LongSumm, and 85 papers from Paper-Poster (full dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific papers (Computer Science domain datasets used for evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Templatic documents (LaTeX outputs): slide decks, scientific posters, blog-style summaries (templatic views of single input papers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Template-Adaptable Evaluation (TAE) framework using similarity metrics as components (ROUGE-L, METEOR, BLEU, BERTScore) plus ordering penalty (Spearman rank correlation) and length penalty; human evaluation (3 annotators per document, preference + degree + reasons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Human evaluators preferred documents generated with the intermediate representation 82% of the time (majority vote; 71% unanimous). Automatic evaluation via the TAE framework shows JSON intermediate representation yields the best TAE F1 scores across most similarity measures (ROUGE-L, METEOR, BLEU, BERTScore) and models; TAE variants correlated more highly with human judgments than prior standard metrics (except an inconclusive result for BERTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No intermediate representation (direct prompting from source paper to target template) and variants with other intermediate formats (text-only or model-chosen representation); also comparisons across LLM sizes (Mistral-7B, Mixtral, GPT4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Structured JSON intermediate representation generally outperformed skipping the intermediate step and outperformed an unstructured text intermediate; gains were more pronounced for smaller models. Human preference strongly favored the intermediate representation (82%). Exact numeric TAE F1s vary by template and metric (reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generating a structured intermediate representation before final generation improves output quality (formatting and salient information) and correlates better with human preference; the approach is model-agnostic, requires little prompt engineering, and style parameters make the system adaptable to multiple templatic views. Gains are larger for smaller LLMs, indicating structured prompting helps resource-constrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Method focuses on single-document transformations (not explicit multi-document synthesis/theory construction); limited to textual content (no multimodal generation beyond text extraction from images); evaluation only in the scientific domain; potential data leakage into LLM training data; LLM risks (hallucination, copyright/PII issues) remain; some outputs require LaTeX compilation fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper reports that structured intermediate representations produce larger relative gains for smaller models (e.g., Mistral-7B) and observes robust performance when scaling models up to GPT-4 (gpt4-32k); the approach was applied across datasets of different sizes (tens to thousands of papers), but no explicit large-scale multi-paper synthesis scaling law is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Centric Templatic Views of Documents', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4384.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4384.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-Adaptable Evaluation (TAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified precision-recall style evaluation framework that composes a quality similarity measure, an ordering penalty, and a length penalty to evaluate templatic document generation across heterogeneous templates using standard similarity metrics as components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Template-Adaptable Evaluation (TAE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TAE defines Precision = Q_P * O_P * L and Recall = Q_R * O_R * L, where Q_P / Q_R are quality similarity measures (user-chosen: ROUGE, BERTScore, etc.), O_P / O_R are order penalties computed by constructing one-to-one alignments between generated and reference 'panels' and computing Spearman rank correlation, and L is a non-reflexive length penalty. The framework is modular: any underlying similarity metric can be plugged into Q, allowing uniform evaluation across slides, posters, blogs, or other templatic views.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not an LLM-based synthesis technique; it synthesizes multiple metric components and order/length penalties into a single adaptable evaluation score.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied in experiments to the datasets used in the paper (DOC2PPT sample ~1,000, LongSumm ~505, Paper-Poster 85); designed to generalize to arbitrary numbers of documents/panels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Document generation evaluation across templatic views (scientific documents in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evaluation scores (precision, recall, F1) adaptable to templates; supports per-panel and whole-document evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Integrates conventional similarity metrics (ROUGE-L, METEOR, BLEU, BERTScore) as the Q component; adds order penalty via Spearman rank correlation and length penalty via an exponential function of panel count differences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>TAE variants correlate more highly with human judgments than standard computations of the underlying metrics in most cases (paper reports improved Pearson correlations between TAE-based metric deltas and human preference scores), though BERTScore results were not statistically significant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard use of ROUGE/BLEU/BERTScore/METEOR computed only on extracted text without template-aware penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>TAE-augmented metrics achieved higher correlation with human preferences than the standard (text-only) computation for most metrics; exact correlation numbers are reported in the paper's Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating structure (panels), order penalties, and length normalization into evaluation better captures human notions of document quality across diverse templatic views; TAE is flexible and allows reuse of existing metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>TAE is restricted to textual evaluation and panel-like templates (definition of panel may be domain-dependent); non-reflexive length penalty can be gamed by over-generation if reflexivity changed; does not evaluate factual correctness per se.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to scale across template types and dataset sizes; no explicit empirical scaling laws presented beyond experiments on datasets of tens-to-thousands of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Centric Templatic Views of Documents', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4384.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4384.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method for LLMs that elicits intermediate reasoning steps (multi-step chain of internal reasoning) to improve performance on complex tasks; referenced as analogous motivation for generating intermediate representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A prompt-engineering technique where the LLM is encouraged to produce intermediate reasoning steps (an explicit chain of reasoning) before arriving at a final answer; in this paper, the idea motivates generating structured intermediate representations to improve downstream document generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Intermediate-step elicitation via prompting; not an extraction schema per se but a multi-step reasoning prompt that can surface intermediate facts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-step reasoning that can be used to combine information internally in the model; the paper cites this method as conceptually similar to their two-step extraction-then-generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM prompting / reasoning tasks; cited as prior work in LLM reasoning literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Intermediate reasoning chains; improved final answers/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as prior work demonstrating that eliciting intermediate steps can improve LLM performance on complex tasks; motivates structured intermediate representations for document generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Known to require careful prompting and may increase verbosity/hallucination if not managed; the paper does not experimentally evaluate chain-of-thought itself but uses the concept as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Prior literature indicates benefits scale with model size and prompting strategies; this paper reports that structured intermediate representations help smaller models relatively more, conceptually aligning with chain-of-thought findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Centric Templatic Views of Documents', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4384.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4384.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Content-Planning Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Content Planning Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that first generates a content plan (outline or structured plan) which an LLM then realizes into final text; cited as related work and conceptually similar to the paper's intermediate-representation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Content Planning Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-step prompting approach: (1) prompt the model to produce a content plan or outline of salient points, (2) prompt the model to expand that plan into the final document. The authors cite this as analogous prior work showing benefits to structured intermediate planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Outline/content-plan generation via prompting; can be seen as unstructured or semi-structured extraction of salient points.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical generation: plan-then-realize; useful for multi-paragraph or long-form outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text generation; related to long-document and structured generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Content plans/outlines and realized documents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior evidence that planning/intermediate representations can improve final output; the present paper validates that structured JSON representations provide similar benefits in templatic document generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Effectiveness depends on plan quality and prompt design; not evaluated directly in this paper beyond conceptual analogy.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>General literature suggests planning benefits are model-dependent; the paper reports structured representations aid smaller models more.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Centric Templatic Views of Documents', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4384.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4384.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongSumm (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LongSumm (Long form summarization dataset for scientific papers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of long-form blog-style summaries of scientific papers; used in this paper as an evaluation dataset where the authors prompt LLMs to generate blog posts from scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Overview and insights from the shared tasks at scholarly document processing 2020: CL-SciSumm, LaySumm and LongSumm</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LongSumm (dataset used for blog generation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A publicly available dataset containing scientific papers paired with long-form blog-style summaries; in this work, the authors used LongSumm (the training split/public entries) to evaluate LLM-generated blog templatic views by prompting LLMs to produce blog posts from input papers and scoring with TAE.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Experiments used gpt35-16k and gpt4-32k for generation over the LongSumm set (505 accessible references used).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Input text extracted via Azure Document Layout tool; then LLMs prompted to produce a JSON intermediate representation or directly generate blog LaTeX from paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-paper summarization into blog-style output; not an explicit multi-paper synthesis method.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used ~505 accessible examples from the LongSumm dataset (out of 531 public blog posts; 26 inaccessible).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific papers (computer science) -> blog summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Blog-style long-form summaries (LaTeX formatted in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>TAE with underlying similarity metrics (ROUGE-L, METEOR, BLEU, BERTScore); also human evaluation and leaderboard comparisons (blind test set ROUGE-1 reported for GPT-4 submissions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On the LongSumm blind test (22 papers) the paper reports competitive leaderboard placement (second) for GPT-4-based runs; in their full evaluations JSON intermediate representations generally improved TAE F1 and human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No-intermediate-representation generation (direct prompting) and variants that used text-only or model-chosen representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>JSON intermediate representation improved automatic TAE scores and was preferred by humans in the majority of cases; specific per-metric numbers are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even for blog-style outputs (less structured templates), the intermediate representation can help or be neutral; structured prompts yielded overall improvements across templates, with the largest gains observed on more structured templates (slides, posters).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LongSumm blind test has few examples; some links inaccessible; dataset is single-paper summarization, not multi-paper theory synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Evaluated at the scale of several hundred documents; paper notes leaderboard placement demonstrating LLMs' capability in long-document generation but does not quantify scaling laws for multi-paper synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Centric Templatic Views of Documents', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Doc2ppt: Automatic presentation slides generation from scientific documents <em>(Rating: 2)</em></li>
                <li>Learning to generate posters of scientific papers <em>(Rating: 2)</em></li>
                <li>Overview and insights from the shared tasks at scholarly document processing 2020: CL-SciSumm, LaySumm and LongSumm <em>(Rating: 2)</em></li>
                <li>Qlarify: Bridging scholarly abstracts and papers with recursively expandable summaries <em>(Rating: 2)</em></li>
                <li>Talk-Summ: A dataset and scalable annotation method for scientific paper summarization based on conference talks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4384",
    "paper_id": "paper-266999747",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Templatic Views",
            "name_full": "Knowledge-Centric Templatic Views of Documents",
            "brief_description": "A unified LLM-powered two-step approach that extracts a structured intermediate representation (JSON) from an input scientific paper and then prompts an LLM to generate diverse templatic views (slides, posters, blog posts) from that representation; includes a style-parameter mechanism and a template-adaptable evaluation framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge-Centric Templatic Views",
            "system_description": "A two-step, LLM-centered pipeline: (1) Intermediate Representation Generation — an LLM is prompted to extract the most important information from a source scientific paper and format it into a predefined JSON schema (panels, headings, salient sentences, metadata). (2) Templatic View Generation — the same or another LLM is prompted (with a short style parameter describing the desired template) to turn the JSON representation into a LaTeX-formatted target document (slide deck, poster, or blog post). The pipeline emphasizes minimal prompt engineering, uses style parameters to adapt to different templates, and forces LaTeX compilation to surface formatting errors as part of evaluation.",
            "llm_model_used": "gpt35-16k (gpt-3.5 16k), gpt4-32k (GPT-4 32k), Mistral-7B, Mixtral (as reported in experiments)",
            "extraction_technique": "Structured prompting with a fixed JSON schema (intermediate representation extraction); text/layout extraction via Azure Document Layout/OCR for input preprocessing.",
            "synthesis_technique": "Two-step structured generation: generate structured representation then generate templated document; multi-step reasoning via intermediate representation (analogous to chain-of-thought/content-planning prompting). No explicit multi-document synthesis procedure is proposed — synthesis is per input document into multiple templatic views.",
            "number_of_papers": "Evaluated on existing datasets: sampled 1,000 examples from DOC2PPT, 505 accessible blog references from LongSumm, and 85 papers from Paper-Poster (full dataset).",
            "domain_or_topic": "Scientific papers (Computer Science domain datasets used for evaluation).",
            "output_type": "Templatic documents (LaTeX outputs): slide decks, scientific posters, blog-style summaries (templatic views of single input papers).",
            "evaluation_metrics": "Template-Adaptable Evaluation (TAE) framework using similarity metrics as components (ROUGE-L, METEOR, BLEU, BERTScore) plus ordering penalty (Spearman rank correlation) and length penalty; human evaluation (3 annotators per document, preference + degree + reasons).",
            "performance_results": "Human evaluators preferred documents generated with the intermediate representation 82% of the time (majority vote; 71% unanimous). Automatic evaluation via the TAE framework shows JSON intermediate representation yields the best TAE F1 scores across most similarity measures (ROUGE-L, METEOR, BLEU, BERTScore) and models; TAE variants correlated more highly with human judgments than prior standard metrics (except an inconclusive result for BERTScore).",
            "comparison_baseline": "No intermediate representation (direct prompting from source paper to target template) and variants with other intermediate formats (text-only or model-chosen representation); also comparisons across LLM sizes (Mistral-7B, Mixtral, GPT4).",
            "performance_vs_baseline": "Structured JSON intermediate representation generally outperformed skipping the intermediate step and outperformed an unstructured text intermediate; gains were more pronounced for smaller models. Human preference strongly favored the intermediate representation (82%). Exact numeric TAE F1s vary by template and metric (reported in paper tables).",
            "key_findings": "Generating a structured intermediate representation before final generation improves output quality (formatting and salient information) and correlates better with human preference; the approach is model-agnostic, requires little prompt engineering, and style parameters make the system adaptable to multiple templatic views. Gains are larger for smaller LLMs, indicating structured prompting helps resource-constrained models.",
            "limitations_challenges": "Method focuses on single-document transformations (not explicit multi-document synthesis/theory construction); limited to textual content (no multimodal generation beyond text extraction from images); evaluation only in the scientific domain; potential data leakage into LLM training data; LLM risks (hallucination, copyright/PII issues) remain; some outputs require LaTeX compilation fixes.",
            "scaling_behavior": "Paper reports that structured intermediate representations produce larger relative gains for smaller models (e.g., Mistral-7B) and observes robust performance when scaling models up to GPT-4 (gpt4-32k); the approach was applied across datasets of different sizes (tens to thousands of papers), but no explicit large-scale multi-paper synthesis scaling law is presented.",
            "uuid": "e4384.0",
            "source_info": {
                "paper_title": "Knowledge-Centric Templatic Views of Documents",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "TAE",
            "name_full": "Template-Adaptable Evaluation (TAE)",
            "brief_description": "A unified precision-recall style evaluation framework that composes a quality similarity measure, an ordering penalty, and a length penalty to evaluate templatic document generation across heterogeneous templates using standard similarity metrics as components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Template-Adaptable Evaluation (TAE)",
            "system_description": "TAE defines Precision = Q_P * O_P * L and Recall = Q_R * O_R * L, where Q_P / Q_R are quality similarity measures (user-chosen: ROUGE, BERTScore, etc.), O_P / O_R are order penalties computed by constructing one-to-one alignments between generated and reference 'panels' and computing Spearman rank correlation, and L is a non-reflexive length penalty. The framework is modular: any underlying similarity metric can be plugged into Q, allowing uniform evaluation across slides, posters, blogs, or other templatic views.",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": "Not an LLM-based synthesis technique; it synthesizes multiple metric components and order/length penalties into a single adaptable evaluation score.",
            "number_of_papers": "Applied in experiments to the datasets used in the paper (DOC2PPT sample ~1,000, LongSumm ~505, Paper-Poster 85); designed to generalize to arbitrary numbers of documents/panels.",
            "domain_or_topic": "Document generation evaluation across templatic views (scientific documents in experiments).",
            "output_type": "Evaluation scores (precision, recall, F1) adaptable to templates; supports per-panel and whole-document evaluation.",
            "evaluation_metrics": "Integrates conventional similarity metrics (ROUGE-L, METEOR, BLEU, BERTScore) as the Q component; adds order penalty via Spearman rank correlation and length penalty via an exponential function of panel count differences.",
            "performance_results": "TAE variants correlate more highly with human judgments than standard computations of the underlying metrics in most cases (paper reports improved Pearson correlations between TAE-based metric deltas and human preference scores), though BERTScore results were not statistically significant.",
            "comparison_baseline": "Standard use of ROUGE/BLEU/BERTScore/METEOR computed only on extracted text without template-aware penalties.",
            "performance_vs_baseline": "TAE-augmented metrics achieved higher correlation with human preferences than the standard (text-only) computation for most metrics; exact correlation numbers are reported in the paper's Table 3.",
            "key_findings": "Incorporating structure (panels), order penalties, and length normalization into evaluation better captures human notions of document quality across diverse templatic views; TAE is flexible and allows reuse of existing metrics.",
            "limitations_challenges": "TAE is restricted to textual evaluation and panel-like templates (definition of panel may be domain-dependent); non-reflexive length penalty can be gamed by over-generation if reflexivity changed; does not evaluate factual correctness per se.",
            "scaling_behavior": "Designed to scale across template types and dataset sizes; no explicit empirical scaling laws presented beyond experiments on datasets of tens-to-thousands of examples.",
            "uuid": "e4384.1",
            "source_info": {
                "paper_title": "Knowledge-Centric Templatic Views of Documents",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting method for LLMs that elicits intermediate reasoning steps (multi-step chain of internal reasoning) to improve performance on complex tasks; referenced as analogous motivation for generating intermediate representations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Thought Prompting",
            "system_description": "A prompt-engineering technique where the LLM is encouraged to produce intermediate reasoning steps (an explicit chain of reasoning) before arriving at a final answer; in this paper, the idea motivates generating structured intermediate representations to improve downstream document generation.",
            "llm_model_used": null,
            "extraction_technique": "Intermediate-step elicitation via prompting; not an extraction schema per se but a multi-step reasoning prompt that can surface intermediate facts.",
            "synthesis_technique": "Multi-step reasoning that can be used to combine information internally in the model; the paper cites this method as conceptually similar to their two-step extraction-then-generation pipeline.",
            "number_of_papers": null,
            "domain_or_topic": "General LLM prompting / reasoning tasks; cited as prior work in LLM reasoning literature.",
            "output_type": "Intermediate reasoning chains; improved final answers/outputs.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Referenced as prior work demonstrating that eliciting intermediate steps can improve LLM performance on complex tasks; motivates structured intermediate representations for document generation.",
            "limitations_challenges": "Known to require careful prompting and may increase verbosity/hallucination if not managed; the paper does not experimentally evaluate chain-of-thought itself but uses the concept as motivation.",
            "scaling_behavior": "Prior literature indicates benefits scale with model size and prompting strategies; this paper reports that structured intermediate representations help smaller models relatively more, conceptually aligning with chain-of-thought findings.",
            "uuid": "e4384.2",
            "source_info": {
                "paper_title": "Knowledge-Centric Templatic Views of Documents",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Content-Planning Prompting",
            "name_full": "Content Planning Prompting",
            "brief_description": "A prompting technique that first generates a content plan (outline or structured plan) which an LLM then realizes into final text; cited as related work and conceptually similar to the paper's intermediate-representation approach.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Content Planning Prompting",
            "system_description": "Two-step prompting approach: (1) prompt the model to produce a content plan or outline of salient points, (2) prompt the model to expand that plan into the final document. The authors cite this as analogous prior work showing benefits to structured intermediate planning.",
            "llm_model_used": null,
            "extraction_technique": "Outline/content-plan generation via prompting; can be seen as unstructured or semi-structured extraction of salient points.",
            "synthesis_technique": "Hierarchical generation: plan-then-realize; useful for multi-paragraph or long-form outputs.",
            "number_of_papers": null,
            "domain_or_topic": "General text generation; related to long-document and structured generation tasks.",
            "output_type": "Content plans/outlines and realized documents.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as prior evidence that planning/intermediate representations can improve final output; the present paper validates that structured JSON representations provide similar benefits in templatic document generation.",
            "limitations_challenges": "Effectiveness depends on plan quality and prompt design; not evaluated directly in this paper beyond conceptual analogy.",
            "scaling_behavior": "General literature suggests planning benefits are model-dependent; the paper reports structured representations aid smaller models more.",
            "uuid": "e4384.3",
            "source_info": {
                "paper_title": "Knowledge-Centric Templatic Views of Documents",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LongSumm (dataset)",
            "name_full": "LongSumm (Long form summarization dataset for scientific papers)",
            "brief_description": "A dataset of long-form blog-style summaries of scientific papers; used in this paper as an evaluation dataset where the authors prompt LLMs to generate blog posts from scientific papers.",
            "citation_title": "Overview and insights from the shared tasks at scholarly document processing 2020: CL-SciSumm, LaySumm and LongSumm",
            "mention_or_use": "use",
            "system_name": "LongSumm (dataset used for blog generation evaluation)",
            "system_description": "A publicly available dataset containing scientific papers paired with long-form blog-style summaries; in this work, the authors used LongSumm (the training split/public entries) to evaluate LLM-generated blog templatic views by prompting LLMs to produce blog posts from input papers and scoring with TAE.",
            "llm_model_used": "Experiments used gpt35-16k and gpt4-32k for generation over the LongSumm set (505 accessible references used).",
            "extraction_technique": "Input text extracted via Azure Document Layout tool; then LLMs prompted to produce a JSON intermediate representation or directly generate blog LaTeX from paper text.",
            "synthesis_technique": "Single-paper summarization into blog-style output; not an explicit multi-paper synthesis method.",
            "number_of_papers": "Used ~505 accessible examples from the LongSumm dataset (out of 531 public blog posts; 26 inaccessible).",
            "domain_or_topic": "Scientific papers (computer science) -&gt; blog summaries.",
            "output_type": "Blog-style long-form summaries (LaTeX formatted in experiments).",
            "evaluation_metrics": "TAE with underlying similarity metrics (ROUGE-L, METEOR, BLEU, BERTScore); also human evaluation and leaderboard comparisons (blind test set ROUGE-1 reported for GPT-4 submissions).",
            "performance_results": "On the LongSumm blind test (22 papers) the paper reports competitive leaderboard placement (second) for GPT-4-based runs; in their full evaluations JSON intermediate representations generally improved TAE F1 and human preference.",
            "comparison_baseline": "No-intermediate-representation generation (direct prompting) and variants that used text-only or model-chosen representations.",
            "performance_vs_baseline": "JSON intermediate representation improved automatic TAE scores and was preferred by humans in the majority of cases; specific per-metric numbers are in the paper tables.",
            "key_findings": "Even for blog-style outputs (less structured templates), the intermediate representation can help or be neutral; structured prompts yielded overall improvements across templates, with the largest gains observed on more structured templates (slides, posters).",
            "limitations_challenges": "LongSumm blind test has few examples; some links inaccessible; dataset is single-paper summarization, not multi-paper theory synthesis.",
            "scaling_behavior": "Evaluated at the scale of several hundred documents; paper notes leaderboard placement demonstrating LLMs' capability in long-document generation but does not quantify scaling laws for multi-paper synthesis.",
            "uuid": "e4384.4",
            "source_info": {
                "paper_title": "Knowledge-Centric Templatic Views of Documents",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Doc2ppt: Automatic presentation slides generation from scientific documents",
            "rating": 2,
            "sanitized_title": "doc2ppt_automatic_presentation_slides_generation_from_scientific_documents"
        },
        {
            "paper_title": "Learning to generate posters of scientific papers",
            "rating": 2,
            "sanitized_title": "learning_to_generate_posters_of_scientific_papers"
        },
        {
            "paper_title": "Overview and insights from the shared tasks at scholarly document processing 2020: CL-SciSumm, LaySumm and LongSumm",
            "rating": 2,
            "sanitized_title": "overview_and_insights_from_the_shared_tasks_at_scholarly_document_processing_2020_clscisumm_laysumm_and_longsumm"
        },
        {
            "paper_title": "Qlarify: Bridging scholarly abstracts and papers with recursively expandable summaries",
            "rating": 2,
            "sanitized_title": "qlarify_bridging_scholarly_abstracts_and_papers_with_recursively_expandable_summaries"
        },
        {
            "paper_title": "Talk-Summ: A dataset and scalable annotation method for scientific paper summarization based on conference talks",
            "rating": 1,
            "sanitized_title": "talksumm_a_dataset_and_scalable_annotation_method_for_scientific_paper_summarization_based_on_conference_talks"
        }
    ],
    "cost": 0.017402749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge-Centric Templatic Views of Documents
27 Sep 2024</p>
<p>Isabel Cachola icachola@cs.jhu.edu 
Johns Hopkins University</p>
<p>Silviu Cucerzan silviu@microsoft.com 
Microsoft</p>
<p>Allen Herring allenh@microsoft.com 
Microsoft</p>
<p>Vuksan Mijovic vmijovic@microsoft.com 
Microsoft</p>
<p>Erik Oveson erikov@microsoft.com 
Microsoft</p>
<p>Sujay Kumar Jauhar 
Microsoft</p>
<p>Luisa Bentivogli 
Beatrice Savoldi beatrice.savoldi@unitn.it 
Matteo Negri negri@fbk.eu 
Mattia Antonino 
Di Gangi digangi@fbk.eu 
Marco Turchi turchi@fbk.eu 
Roldano Cattoni cattoni@fbk.eu 
Fondazione Bruno Kessler </p>
<p>Microsoft Research</p>
<p>University of Trento</p>
<p>Knowledge-Centric Templatic Views of Documents
27 Sep 20244E5CCE239B18E639464AB9F75E9DB610arXiv:2401.06945v2[cs.CL]
Authors seeking to communicate with broader audiences often share their ideas in various document formats, such as slide decks, newsletters, reports, and posters.Prior work on document generation has generally tackled the creation of each separate format to be a different task, leading to fragmented learning processes, redundancy in models and methods, and disjointed evaluation.We consider each of these documents as templatic views of the same underlying knowledge/content, and we aim to unify the generation and evaluation of these templatic views.We begin by showing that current LLMs are capable of generating various document formats with little to no supervision.Further, a simple augmentation involving a structured intermediate representation can improve performance, especially for smaller models.We then introduce a novel unified evaluation framework that can be adapted to measuring the quality of document generators for heterogeneous downstream applications.This evaluation is adaptable to a range of user defined criteria and application scenarios, obviating the need for task specific evaluation metrics.Finally, we conduct a human evaluation, which shows that people prefer 82% of the documents generated with our method, while correlating more highly with our unified evaluation framework than prior metrics in the literature.</p>
<p>Introduction</p>
<p>Sharing information is vital for communication and discourse across domains, as it allows for knowledge to be disseminated to a wider audience.This is often done by users through documents in multiple formats that nevertheless share some underlying knowledge.A product manager may need to create a requirements spec, a product pitch deck, and an announcement newsletter for the same project.</p>
<p>Likewise, a person on the job market may create a resume, a cover letter, and a personal website.We consider these documents to be templatic views of the same underlying knowledge.This is equally true for the scientific domain, in which researchers create documents in multiple formats to effectively communicate and showcase their work, -such as through academic papers, conference talks, social media posts, poster presentations, and non-technical blog posts.Sharing knowledge in multiple formats broadens the audience and can help bridge the information gap between domain experts, researchers in adjacent fields, and even the general public, leading to greater understanding, collaborations and accelerated progress (Bornmann and Mutz, 2014).</p>
<p>Past work on document generation has focused on developing generation and evaluation methods specific to a single document type (Fu et al., 2021;Qiang et al., 2016;Chandrasekaran et al., 2020).Narrow, custom methods tailored to individual document types are, nevertheless, time consuming to engineer and manage over the long term.For example, in an enterprise setting, it's common to have dozens of occupation-and task-specific documents, each with their own template.Additionally, specific trained methods require data that may be expensive to acquire, or even be unavailable entirely.Meanwhile, LLMs have recently shown great success in long document generation (Radford et al., 2019;Brown et al., 2020), indicating that this fragmentation of methods may no longer be necessary.Thus, our goal is to unify methods for both generating and evaluating templatic views of documents, allowing system designers and engineers to manage and adapt to a range of document types and domains easily and efficiently.</p>
<p>We begin by showing that LLMs are capable of diverse, structured document generation, requiring very little instructional guidance to do so effectively.Additionally, a few minor augmentations ... etc.</p>
<p>Figure 1: Visualization of our method to unify the generation and evaluation of templatic views of documents.Given an input document, we prompt the LLM to generate an intermediate representation.We can use the representation to prompt the model to generate a templatic view of the input document.We then evaluate the generations using our unified evaluation framework.The LLM represented in the figure is the same model.</p>
<p>to the prompt -such as a structured, intermediate representation, and simple stylistic descriptionscan further improve downstream performance, especially for smaller, less resource intensive models.These findings have important implications on the deployment and scaling of unified, real-world AIassisted document authoring systems.</p>
<p>In similar vein, we then introduce Template Adaptable Evaluation (TAE), departing from prior work's task specific evaluation methods (Zhang* et al., 2020;Qiang et al., 2016;Wang et al., 2015).TAE is a unified precision-recall style framework for automatic evaluation that is highly customizable, allowing users to easily integrate existing text-based metrics from the literature into its formulation and tailor it to their specific use case.Additionally, this framework allows developers to compare performance across document types, without needing to develop an evaluation metric for each individual template.</p>
<p>We evaluate our unified approach for templatic view generation and evaluation on 3 types of documents: slides, posters, and blog posts (Fu et al., 2021;Qiang et al., 2016;Chandrasekaran et al., 2020).Our experiments demonstrate that using a structured intermediate representation leads to improvements in performance across tasks, with greater gains for smaller language language models.In our human evaluation to validate both our unified document generation method and evaluation metric, we show that annotators prefer the output yielded by the structure-aware generation process 82% of the time and that our evaluation metric correlates more highly with human preference than other popular metrics.We release our code1 to support future research.</p>
<p>Related Work</p>
<p>There are several areas of related research in NLP that are relevant to the problems of document transformation and evaluation.</p>
<p>Document summarization has been explored in a number of domains, including news (See et al., 2017), literature (Sciré et al., 2023), law (Deroy et al., 2023), and dialogue (Chen et al., 2021).In the scientific domain, summarization of scientific papers has taken the form of long form summaries (Chandrasekaran et al., 2020), abstract generation (Cohan and Goharian, 2015), conference talks (Lev et al., 2019), and query based summaries (Fok et al., 2023).These summaries can be either extractive (Sefid and Giles, 2022) or abstractive (Chandrasekaran et al., 2020).</p>
<p>Although the tasks of slide and poster generation have generally been considered separate from scientific summarization, they are related in that both tasks require taking an input article, then organizing and abstracting the information to generate a new document.Past work has developed methods for slide generation from papers (Hu and Wan, 2015;Li et al., 2021;Hu and Wan, 2015;Fu et al., 2021), from code (Wang et al., 2023a), or based on a query (Sun et al., 2021).Poster generation has been explored in the form of content extraction for posters (Xu and Wan, 2021), interactive generation (Wang et al., 2015), or full content generation using graphical models (Qiang et al., 2016).To the best of our knowledge, our work is the first to create a unified method capable of generating a diverse range of templatic views of a source document.</p>
<p>Large Language Models (LLMs), which are central to our approach, have shown impressive capabilities in a variety of tasks (Radford et al., 2019;Brown et al., 2020).Based on the transformer architecture (Vaswani et al., 2017), LLMs have shown emergent abilities in tasks such as arithmetic and question answering (Wei et al., 2022a).Similar to chain of thought prompting (Wei et al., 2022b) and content planning prompting (Wang et al., 2023b), we show that by generating an intermediate representation of an input document can improve performance over simply prompting the model to generate the final document from the original input.</p>
<p>As past work has tackled generation of templatic views as separate tasks, methods for automatic evaluation of different document types is fragmented.LongSumm, the shared task introduced by Chandrasekaran et al. ( 2020), uses ROUGE to evaluate model performance (Lin, 2004).Fu et al. (2021) introduced Slide Level ROUGE to evaluate slide generation, a variant that contains a penalty for the number of slides.Qiang et al. (2016) used a trained regressor.For summarization, many automatic evaluation metrics have been introduced such as BERTScore (Zhang* et al., 2020), UniEval (Zhong et al., 2022), BARTScore (Yuan et al., 2021), BLANC (Vasilyev et al., 2020), and MoverScore (Zhao et al., 2019).However, these metrics are intended for a simple input documentsummary setup, and do not take into account factors that affect the quality of other types of documents (e.g.structure).Our work is the first to introduce template adaptable evaluation, allowing uniform comparison of performance across template types.</p>
<p>Data</p>
<p>We begin by describing the data used in this paper.There is no existing dataset that includes multiple views of a single document.Instead, we evaluate our unified method, described in §4, on 3 existing datasets: DOC2PPT, LongSumm, and Paper-Poster (Fu et al., 2021;Chandrasekaran et al., 2020;Qiang et al., 2016).These datasets are chosen because they each involve generating a different view of a document.Although our method is not specific to the scientific domain, it is one of the few domains with abundantly available public data of multiple templatic views2 .The three datasets and their associated generation tasks are described below.</p>
<p>Slide Generation.We use use the DOC2PPT dataset (Fu et al., 2021), which contains 5.8K scientific papers in Computer Science and their respective slide decks.As Fu et al. (2021) do not release data splits or code, we randomly sample 1K examples from this dataset for evaluation.The slides are provided as an image for each slide.We use the Azure OCR tool to extract the text from each slide3 .Blog Generation.We use the LongSumm dataset (Chandrasekaran et al., 2020), which includes blog posts of scientific papers in the Computer Science domain.Since our approach requires no training or supervision, we use the entire training split from Longsumm as our evaluation set.Of the 531 publicly released blog posts in this set, we could only access 505, with the other 26 including broken links or being behind a paywall.</p>
<p>Notably, while Longsumm includes a blind test set of 22 papers, this test set only consists of inputs without their reference outputs, thus making it impossible to compute our custom evaluation metric (see §5).In the interest of completeness and comparison to prior work, we do, however submit runs from our systems to the leader board and report the results of this blind test set in Appendix D.</p>
<p>Poster Generation.We use the Paper-Poster dataset (Qiang et al., 2016), which consists of a dataset of 85 papers in Computer Science and Biology, and their respective scientific posters; two examples containing corrupted PDFs are excluded.Although Qiang et al. (2016) release data splits, they do not release code or results for comparison.Given the small size of the dataset, we use it in its entirety for more robust results.While the authors uses the source files to extract the text of posters for evaluation, they only release the PDF formats.To preprocess the reference posters, we found that automatic tools to extract text from documents did not handle the visual layout of posters well, so we manually extracted the text of the posters in this dataset.Note that this process was only done to obtain evaluation scores, and that our unsupervised generation method is capable of creating target documents without the need for reference data.</p>
<p>For all 3 datasets, we use the Azure Document Layout tool to extract the text of the input papers. 4 Unified LLM-powered Generation of Templatic Views</p>
<p>The most straightforward way to transform documents between templatic views using LLMs, is to simply prompt the system to generate the target view given the input.However, similar to chain of thought prompting (Wei et al., 2022b), we hypothesize that first generating a structured, intermediate representation of an input document and then reasoning over that representation will result in better generations than directly prompting the model.Our goal is to evaluate the capabilites of LLMs to generate long, structured documents, and experiment with how structured prompting can improve performance.We experiment with a simple general two-step process: first generate an intermediate representation, then generate the templatic view.These steps are described in greater detail below, and the process is visualized in Figure 1.</p>
<p>Intermediate Representation Generation.In this work, we set the intermediate representation to be a JSON consisting of a structured layout of the most important parts of the input.We provide the input document to the model along with a template of the representation and prompt it to extract the most important information from the input document, and format it in the given JSON structure.The exact prompts and JSON structure can be found in Appendix §A.While our experiments use a JSON intermediate representation, note that other formats that provide structure to the input text could be employed (e.g.XML or Markdown).Rather than trying to optimize for the best representation format, our goal is to show that this chain of extraction approach along with structured augmentation to prompts can aid the quality of generations from LLMs.We leave exploration of different formats and other prompt optimization to future work.</p>
<p>Templatic View Generation.We then feed the generated representation as input back into the LLM, prompting the model to generate the final output document, represented as a LaTeX document.</p>
<p>For each templatic view, the prompt to generate the final LaTeX document takes a short description of the desired output, which we refer to as a style parameter.For example, the style parameter for slide generation is as follows: "Slides should include a title page.Following slides should contain an informative slide title and short, concise bullet points.Longer slides should be broken up into multiple slides."The use of style parameters makes our method adaptable to new templatic views; the user only needs to write a short description of the template style.Both the generation of the intermediate representations and the final documents require little to no prompt engineering.The prompts and style parameters can be found in Appendix §A.</p>
<p>Template Adaptable Evaluation</p>
<p>Prior work on document generation has treated the evaluation of different templatic views as separate tasks.Thus, our goal is to develop a framework of automatic evaluation that is template adaptable.</p>
<p>This not only allows us to compare performance across diverse datasets, it also removes the requirement of designing and maintaining individual metrics for each template.In order to generalize to multiple templates, we introduce the concept of panels.A panel is a unit of organization within a document type, for which the placement and ordering of the panel is important to the overall flow of information in the document.</p>
<p>For example, we consider panels to be each slide in a slide deck and each section on a poster.We consider the entirety of a blog post to be a single panel.Although we test our method on the tasks of slide, blog, and poster generation, the concept of panels is not limited to these document types.For example, each post on a social media thread could be considered a panel, or each page on a website.</p>
<p>We aim to unify the evaluation of templatic views by integrating prior metrics into a template adaptable precision-recall framework, which we refer to as Template-Adaptable Evaluation (TAE).TAE is not a new individual metric, but rather an evaluation framework that allows generalization to new templates.For example, TAE can even be used with ROUGE to evaluate poster generation.</p>
<p>The general TAE formulation is as follows:
Precision = Q P × O P × L Recall = Q R × O R × L (1)
in which Q P is the precision measure of quality ( §5.1), O P is the precision penalty for order ( §5.2), and L is the non-reflexive penalty for length ( §5.3).</p>
<p>Similarly, Q R is the recall quality measure and O R is the recall penalty for order.The precision-recall formulation allows evaluators to decide which measure is most important to them, or calculate an overall F-measure score.</p>
<p>Quality Measure</p>
<p>For the TAE precision score, we calculate the average similarity between the generated panels and their most similar reference panel as follows:
Q P = 1 | S| S max sim (S, Si ) (2)
in which S is the set of reference panels and S</p>
<p>is the set of generated panels.For the similarity metric, the user can choose a metric that best matches their use case, such as ROUGE, BERT-Score, or a custom trained regressor (Lin, 2004;Zhang* et al., 2020).For example, a user might choose ROUGE if they want a similarity metric that focuses on exact word overlap, or BERTScore to measure broader semantic similarity.Similar to precision, the TAE recall score is calculated as the average similarity between the reference panels and their most similar generated panel:
Q R = 1 |S| S max sim ( S, S i ) (3)
By splitting the evaluation of quality into precision and recall, we can evaluate both the content of the slides that were generated as well as the coverage of this content against some reference.</p>
<p>Order Penalty</p>
<p>Broadly, the goal of the ordering penalty is to measure the similarity of the order of information in reference and generated panels, independent of other factors.Unfortunately, because the cardinality of panels in the two outputs is not necessarily the same, a direct one-to-one mapping to compare ordering is not feasible.Instead, a panel in one set can align to multiple references in the other, or none at all -as depicted in Figure 2. Intuitively, our solution is to virtually replicate (resp.drop) panels that have multiple (resp.zero) alignments in the reference set so that a one-to-one mapping of ordering, can in fact be computed.</p>
<p>Formally, assume S and S are sequences of reference and generated panels respectively.We use for the precision ordering penalty.We first use the similarity measure to map each generated panel to its most similar reference document.This mapping is used to calculate the precision quality score QP .We then use the mappings to create a one-to-one alignment from the generated to the reference panels, which we use to calculate the precision ordering penalty (OP ).By creating a one-to-one alignment, we are able to represent inversions in the ordering.This process is reflexive, and panels not accounted for in the precision ordering penalty are accounted for in the recall ordering penalty.</p>
<p>the maximum similarity scores calculated in §5.1 to align the panels across sets.</p>
<p>For the precision ordering penalty, we define the following operation λ P (s) = s δ P (s, s), where
δ P (s, s) = 1, iff s → s 0, otherwise
Intuitively, this captures the cardinality of the alignment of a panel in S with panels in S.Then, using this operation we can replace every s ∈ S with λ P (s) copies, leading to an identical cardinality for both S and S, and subsequent one-to-one mapping between their corresponding panels.</p>
<p>Then, to operationalize a penalty score for the two sets of ordered panels we associate them with ranks in both sets and use a rank correlation metric to compute the degree of agreement.Specifically, rank assignment is done as follows: panels in S are simply assigned ranks in order of appearance 1 through N -we call this SP ranking ; meanwhile panels in S are assigned the identical rank to their one-to-one aligned panel in S and SP ranking -we refer to these rankings as S P</p>
<p>ranking .An example of this process can be found in Figure 2. The final ordering penalty is computed using Spearman's rank correlation (Szmidt and Kacprzyk, 2010):
O P = Spearman(S P ranking , SP ranking ) + 1 2 (4)
where we perform a linear transformation to map the original range of the correlation coefficient [-1, 1] to the desired range [0, 1].</p>
<p>Similarly, for the recall ordering penalty, we map the reference panels to the generated panels, calculated as λ R (s) = s δ R (s, s).O R is calculated similar to O P , using the recall rankings.</p>
<p>Length Penalty</p>
<p>Finally, we compute a length penalty for both the recall and precision scores.Similar to Fu et al. (2021), this is done as follows:
L = e −abs(|S|−| S|) |S|(5)
We chose to keep L non-reflexive, because in the reverse case -as | S| → ∞, L → 1 -the metric could be cheated by over-generating.</p>
<p>Results</p>
<p>As mentioned in §3, past work on Doc2PPT and Paper-Posters do not release code, making it difficult to do a direct comparison.They also do not report any baselines to compare against.Meanwhile, Longsumm's blind test does not allow us to compute our custom metric, although we do report the leaderboard results in Appendix D. Notably, with almost no prompt engineering our LLM-based system places second on this leaderboard.We argue that for the investigation in this paper, direct comparison to prior non-LLM baselines is not only unfair to those approaches, but not particularly insightful.Therefore, similar to Wei et al. (2022b), we focus on variants of our LLM-based method and treat them as baselines.Example outputs of each template type can be found in Appendix E.</p>
<p>We conduct experiments with the following settings: (1) No Representation -this is the default setting of going directly from the source document to the target document.We skip the intermediate generation step, passing the full paper as input.We experiment both with and without the style parameters.( 2 For each template, we experiment with different representations (Rep) and whether or not we include the style parameters (Style).We report the TAE F1 scores as calculated in §5, using ROUGE-L (R-L), METEOR (M), BLEU (B), and BERTScore (BERTS) as the similarity metrics.</p>
<p>the JSON structure.(4) JSON Representationthis is the full JSON structure for the intermediate generation step.We experiment both with and without the style parameters.We use gpt35-16k in our main set of experiments.We truncate text that is too long for the input window and use a temperature of 0.0 as standard.5</p>
<p>Results of automatic evaluation</p>
<p>In Table 1, we report the TAE F1 scores as described in §5, using ROUGE-L (Lin, 2004), ME-TEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002), and BERTScore (Zhang* et al., 2020) for the similarity measure.As seen in the results, by most measures, generating a JSON intermediate representation yields the best performance.</p>
<p>We see that using the text representation generally degrades the performance over providing the structured JSON representation, indicating that structure is important for downstream performance in addition to abstractive filtering of information.Additionally, the text representation performs better than skipping the intermediate step altogether for both the poster and slide generation task, but not the blog generation task.This is likely because posters and slides have more inherent structure than blog posts, which can be relatively free-form.</p>
<p>Finally, we see that allowing the model to choose its own representation format degrades performance over providing our pre-defined JSON structure.However, we see that in most cases, providing a representation generated without a JSON structure still performs better than skipping the intermediate generation step altogether (while maintaining the same style parameter setting).This indicates that even without a pre-defined structure, the intermediate step is still valuable for performance.</p>
<p>Experiments with additional models.We conduct a subset of our experiments on Mistral-7B (Jiang et al., 2023), Mixtral (Jiang et al., 2024), and GPT4 (gpt4-32k), comparing the JSON representation to skipping the intermediate step.We maintain the same style parameters in both settings.</p>
<p>In Table 2  While annotators largely preferred documents generated with an intermediate representation, the most common reasons for preference are better formatting and information content.We exclude the "Other" count as it was only selected once.</p>
<p>resentation do not strictly score higher on most measures is the posters generated with Mixtral and GPT4.Upon closer inspection, the references in this dataset are very verbose, averaging 391 tokens.Our method produces generally less verbose posters, averaging 265 total tokens compared to 345 tokens produced by the baseline.We hypothesize that by editing the style parameters to include information about verbosity and length, we can improve performance on posters in the future.</p>
<p>Human evaluation</p>
<p>After showing that LLMs benefit from intermediate structured representations in document transformations, we investigate whether our proposed evaluation framework aligns better with human judgment than previously proposed metrics.We sample 100 documents each from DOC2PPT and LongSumm, and use the entirety of the Paper-Poster dataset in this study.We present annotators with 2 versions of each document, one generated with the intermediate representation and one without.Both versions use gpt4-32k, as the best performing model.The annotators are provided with the original paper and the intended document type (blog, slide deck, or poster), and are asked the following questions: (1) Which document do you prefer?(2) On a scale of 1-3, to what degree do you prefer your selection?(3) Why do you prefer your selection?For question 3, annotators are also provided with a multi-select checklist of reasons for their preference: (1) quality of the content, (2) formatting, (3) document style matching the intended document types, (4) information represented in the document, and ( 5) other (along with a free text box).The full instructions, including the reasons provided and examples, can be found in Appendix §B.</p>
<p>If the models do not produce LaTeX and instead produce only text, we wrap the text with \begin{document} and \end{document}.</p>
<p>We force the compilation of the outputs with the command: pdflatex --interaction=nonstopmode <filename.tex>.Occasionally, this forced compilation leads to oddly formatted documents, but we consider this to be a part of the performance of the method and present the documents with no further changes.Each document is annotated by 3 different annotators.We employed 4 annotators from India, sourced via a third-party agency, to carry out the human evaluation of our task based on a guideline document containing task-specific instructions, guidance, and annotated examples.</p>
<p>They were compensated at a rate of $11.98 USD per hour for the total time spent working on the task, including a training round of annotation.</p>
<p>Which method do humans prefer?The documents generated with an intermediate representation were preferred by 82%, based on majority vote (71% unanimously).The annotator agreement score was 0.51 with Krippendorff's alpha, indicating that while this is a subjective and specialized task, even non-expert annotators agree to a moderate degree.A visualization of the reasons the annotators preferred their selection can be found in Figure 3.It can be seen that while annotators largely preferred the documents generated with an intermediate representation, the most common reasons for preference are better formatting and better information content.This indicates that the structure provided by the intermediate representation makes it easier for the model to format the final document well.Additionally, the intermediate representation only includes the most salient information from the original text, resulting in higher quality of information content.Finally, we see a fairly even distribution across different templatic views for the reasons of preference, indicating humans prefer the documents generated with the intermediate representation across different document types.</p>
<p>Which metric correlates better with humans?</p>
<p>We test whether our metric, as described in §5, correlates better with human preference compared to prior evaluation metrics in the literature.For each annotation, given the degree of the preference d (Appendix §B Q. 2) we convert value to a score  3: Correlation of evaluation metrics with human judgement.We compare each metric computed using the TAE framework versus the standard computation.*Indicates the correlation is not statistically significant (p &gt; 0.01).
P (d) → [1, 2, 3] if d is slight, moderate, or
strong, respectively.If the annotator the document generated without an intermediate representation, we take −P (d) instead.This allows us to measure if the metric captures directionality of preference along with degree. in parallel, we compute the automatic score m for each metric, then calculate S = m(with rep) − m(skip rep) where m is the metric we are evaluating (e.g ROUGE).If a human annotator prefers a document generated without the intermediate step, we'd expect a good metric to assign a higher score to that document as well, resulting in both S and P (d) being negative (and positive in the opposite case).Using this intuition we assign an affinity score of a metric with respect to human evaluation as the Pearson correlation (Freedman et al., 2007) of S and P (d).</p>
<p>Since prior metrics are not designed to account for the structure of documents, we compute them by extracting only the text of both the generated and reference documents.The correlations with human judgement for each metric to its respective TAE variants can be found in Table 3.As we can see from the results, evaluations using our template adaptable framework correlate more highly with human judgement, except in the case of BERTScore.In the latter case the results are not statistically significant, and we hypothesize that the open-domain nature of BERT embeddings are poorly suited to represent the semantic similarity of scientific text.</p>
<p>Conclusion</p>
<p>In many domains, people choose to disseminate information across different modalities and formats for better communication to broader audiences.We proposed a unified view of document transformation and evaluation.We showed that LLMs are capable of templatic document generation with minimal supervision, and that a structured, intermediate representation can improve performance, particularly for smaller models.We also introduced a flexible precision-recall framework for automatic evaluation that easily integrates existing evaluation metrics into a unified system and allows for comparison across diverse datasets without additional task specific metric design.Finally, we conducted a human evaluation and showed that annotators prefer the documents generated using the intermediate representation 82% of the time and that our evaluation framework correlates better with human preference than standard evaluation metrics.</p>
<p>Limitations</p>
<p>Although our methods are not domain specific, we only evaluated them in the scientific domain, due to the availability of public data.Additionally, our framework is limited to textual content.In future work we plan to explore the application of our unified framework for generation and evaluation on document views in other domains, as well as incorporating multi-modal models and content generation.Finally, it is possible that some of our test data has leaked into the training data of the models with which we experimented.This limitation is not unique to our work and exists for our baselines in addition to our methods.</p>
<p>Ethics</p>
<p>The potential risks of our work are similar to those of other work in downstream applications of LLMs.LLM generated documents can potential generate copy-righted material (Carlini et al., 2020), personally-identifiable information (Lukas et al., 2023), or factually incorrect text (Manakul et al., 2023).The use of LLMs to generate documents may violate some academic dishonesty policies (Zdravkova et al., 2023).Our system is intended to be used in collaboration with human writers.Users should edit the generations, checking for factual inconsistencies and other potential errors.Our work is intended to save users time that might be spent repeating information across multiple documents, so they can focus on content creation.Therefore, we believe the benefits of our work outweigh the potential risks.</p>
<p>A Prompt details</p>
<p>The prompts and intermediate representation template used can be found in Table 4 and Figure 4, respectively.We note that the specific structure provided to the prompt is not inherent to our method, and a different structure could be provided depending on the input document and domain.For the tasks we evaluate in this paper, we use the following style parameters:</p>
<p>• Slides: "Slides should include a title page.</p>
<p>Following slides should contain an informative slide title and short, concise bullet points.Longer slides should be broken up into multiple slides."</p>
<p>• Posters: "Posters should include a title section at the top.Each panel should include a heading and short, concise bullet points of the most important take-aways from that section."</p>
<p>• Blogs: "Blogs should include paragraphs introducing the topic, a summary of the input document, and important takeaways.The blog should be more readable to a general audience than the input document."• The quality of the content -The text is grammatical and understandable.E.g.Document A contains major grammatical errors while Document B only contains minor errors.</p>
<p>B Annotation Instructions</p>
<p>• The formatting -The formatting is reasonable and matches the formatting of the intended document type.E.g.A poster contains panels and each panel contains a header and body text.</p>
<p>• The style -The document matches the style of the intended document type.E.g.Shorter, bulleted sentences in a slide deck.</p>
<p>• Information represented in the document -The document contains sufficient information to represent the input document.E.g.A blog post represents the most important sections from the input document.</p>
<p>The above criteria are non-exhaustive.Not all criteria must be met, and you may use other relevant criteria to make your decision.You are not rating the document for factual correctness, 6 and only need to refer to the corresponding scientific article if it will aid in making your preference.You can answer this question with either Document A or Document B.</p>
<p>Question 2 -On a scale of 1-3, to what degree do you prefer your selection?In this question you will rate the degree to which you prefer your selection, on the following scale:</p>
<ol>
<li>
<p>Small preference -The documents are similar in quality and only contain minor differences that affect my preference.2. Moderate preference -I clearly prefer one document but the differences are not major.</p>
</li>
<li>
<p>Strong preference -I have a strong preference for one document and the differences between the documents are major.</p>
</li>
</ol>
<p>Question 3 -Why do you prefer your selection?(You may select more than one property)
□ Formatting □ Information □ Quality □ Style □ Other (free text)</p>
<p>B.2 Edge cases</p>
<p>For most edge cases, it is up to your discretion on how to best handle the case.However, below are a few examples of how you could consider certain edge cases:</p>
<p>Example 1: Slides 1-5 of Document A are higher quality but slides 6-10 of Document B are higher quality.You could reason that the first slides represent the most important information, and choose Document A. However, since Document B contained higher quality slides for another portion of the document, you could rate your degree of preference as "Small preference."</p>
<p>Example 2: Document A more closely matches the style of the intended document type, but Document B contains more relevant information to the source document.you could reason that information content is more important than style, and prefer Document B.</p>
<p>Example 3: Document A contains more relevant information than Document B, but also contains major formatting errors, such as text being cut off from the document.</p>
<p>You could reason that although Document A contains more relevant information, the major formatting errors are significant enough to prefer Document B.</p>
<p>Example 4: Neither document matches the style or formatting of the intended document type.Since neither document matches the style or formatting of the intended document type, you could consider other criteria, such as quality of content or the information represented.</p>
<p>C Temperature Experiments</p>
<p>We experiment with the temperature of the generations to see how temperature affects perfor-mance.We randomly sample 100 documents each from LongSumm and Doc2PPT for the blog and poster generation tasks, respectively.We use the entirety of the Paper-Poster dataset, since it contains less than 100 examples.We use gpt35-16k and experiment with the temperatures [0.0, 0.25, 0.5, 0.75, 1.0].The results of this experiment can be found in Table 5.As we can see from the results, there seems to be little consistency across the different types in which temperature performs the best.</p>
<p>D Longsumm Blind Test Set Results</p>
<p>We submit the final documents from GPT4, the best performing model overall, to the Longsumm blind test set evaluation.We compare the documents generated with and without the intermediate step.</p>
<p>We see that without the intermediate representation we get a Rouge-1 score of 46.8 while the results generated without the intermediate representation received a Rouge-1 score of 46.4.We note that this blind test set of 22 papers is significantly smaller than the evaluation data (505 papers) we used in the main body of this paper.Despite not designing a task specific method, we place second on the leaderboard, showing the powerful capabilities of LLMs in long document generation.</p>
<p>E Example Outputs</p>
<p>We provide examples of the outputs generated with and without the intermediate representation below.The documents in all examples are generated with GPT4 (gpt4-32k).Figure 5 includes example slide generations, Figure 6 includes example blog generations, and Figure 7 includes example poster generations.Introduction</p>
<p>The need to address gender fairness and gender bias in natural language processing tasks is a growing concern.Gender bias arises from the extent through which each language formally expresses the female or male gender of a referred human entity.Machines tend to reproduce the linguistic asymmetries present in the real-world data they are trained on.</p>
<p>The study presents the first systematic analysis aimed to assess speech translation performance on gender translation.The MuST-SHE benchmark MuST-SHE is a multilingual, natural benchmark allowing for a fine-grained analysis of gender bias in machine translation and speech translation.It comprises approximately 1,000 (audio, transcript, translation) triplets annotated with qualitatively differentiated and balanced gender-related phenomena.The dataset was created and annotated by an expert linguist with a background in translation studies.</p>
<p>Experimental Setting The study compares an End2End system with two cascade systems (Cascade and Cascade+tag).The evaluation method acknowledges and adapts previous related works to go beyond them and make BLEU scores informative about gender.The study implements a new evaluation method that removes unrelated factors that may affect the overall performance of a system to soundly estimate gender bias.Conclusion Translating gender is still an issue in speech translation and current technologies are affected by gender bias to variable extent.The study encourages the community to start its rescue from MuST-SHE and the findings discussed in this paper.</p>
<p>Introduction</p>
<p>In the world of robotics and artificial intelligence, one of the key challenges is designing autonomous agents that can perform tasks with well-defined goals and objectives.While computers and robots often outperform humans in tasks requiring computational speed, precise manipulation, and exact timing, it can be difficult to design reward functions and objectives that lead to desired behaviors.This is where inverse reinforcement learning (IRL) techniques come into play.IRL techniques can infer the intrinsic reward function of a user from demonstrations, which is particularly useful when goals or rewards are difficult for a human to specify.</p>
<p>The Problem with Existing IRL Methods</p>
<p>However, a critical flaw of existing IRL methods is their inability to significantly outperform the demonstrator.This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice.</p>
<p>A New Approach: T-REX</p>
<p>In a recent paper, we introduced a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer highquality reward functions from a set of potentially poor demonstrations.The goal of our work is to achieve improvements over a suboptimal demonstrator in highdimensional reinforcement learning tasks without requiring a hand-specified reward function or supervision during policy learning.</p>
<p>Figure 2 :
2
Figure 2: Example of the process of obtaining the rankings</p>
<p>Figure 3 :
3
Figure 3: Reasons annotators preferred each document.</p>
<p>(a) Document generated without intermediate representation.This example is not cropped.(b) Document generated with intermediate representation.This example is cropped for space and includes an additional 4 slides that are not included for space.</p>
<p>Figure 5 :
5
Figure 5: The above documents are example slides generated by GPT4 (gpt4-32k) with and without the intermediate representation.We can see that without the intermediate step, the model did not generate a true slide deck.</p>
<p>1</p>
<p>(b) Document generated with the intermediate representation.This example is cropped for space and includes an additional page of text.</p>
<p>Figure 6 :
6
Figure 6: The above documents are example blog posts generated by GPT4 (gpt4-32k) with and without the intermediate representation.We can see that without the intermediate representation, the model did not properly format the LaTeX file for compilation.</p>
<p>(a) Document generated without the intermediate representation.This example is cropped for space and includes an additional 3 slides.(b) Document generated with the intermediate representation.This example is cropped for space and includes an additional 4 slides.</p>
<p>Figure 7 :
7
Figure 7: The above documents are example posters generated by GPT4 (gpt4-32k) with and without the intermediate representation.We found that GPT4 often generates slide decks in place of posters.We can see that the document generated without the intermediate representation contains more verbose panels and includes less formatting.</p>
<p>Table 1 :
1
) Own Representation -we do not pass a JSON structure to the intermediate generation step, and allow the model to choose its own structure.(3) Text Representation -we extract the text from the intermediate representation, discarding Evaluation results using GPT3.5 (gpt35-16k).
Similarity MeasureRep.Style R-LMBBERTSNone×5.06.40.331.6SlidesNone Own Text JSON✓ ✓ ✓ ×5.1 6.5 7.3 4.26.0 7.1 8.0 6.00.4 1.2 1.4 0.331.7 36.1 36.4 31.4JSON✓7.48.41.536.9None×26.6 19.63.082.5None✓25.1 17.72.382.8BlogsOwn Text✓ ✓23.9 19.2 25.4 19.32.3 2.582.2 82.5JSON×28.3 25.35.082.3JSON✓25.4 19.62.882.4None×8.110.31.035.6PostersNone Own Text JSON✓ ✓ ✓ ×10.1 11.6 12.8 12.6 11.3 11.7 14.2 16.81.9 2.9 2.1 4.039.5 52.8 45.9 52.8JSON✓15.5 14.5 15.353.3</p>
<p>Given the input text, extract the document title and authors.For each section in the given input text, extract the most important sentences.
Prompt FunctionPromptGenerate the intermediate representation"Format the output using the following JSON template:\n <SURe STRUCTURE>\n\nInput: <INPUT DOCUMENT>\nOutput:""Summarize the following input in a <TEMPLATE TYPE>style.Style parameters: <STYLE PARAMETERS>Generate LaTeX documentFormat the output document as a latex file:\nInput: <INPUT DOCUMENT>\n\nOutput:"</p>
<p>Table 4 :
4
Prompts used to generate the intermediate representation and final LaTeX document.The JSON structure is pictured in Figure 4.</p>
<p>You could consider if Document A contains sufficient information to represent the input document, such as representing the most important sections.If yes, then you could prefer Document A. If not, then
Simularity MeasureTemp R-L MBBERTS0.07.38.21.6 35.1Slides0.25 0.5 0.757.2 7.0 7.08.3 8.3 8.01.5 35.4 1.5 36.4 1.2 35.51.07.48.21.4 35.80.025.3 19.9 2.7 82.7Blogs0.25 0.5 0.7525.5 19.8 2.7 82.6 26.2 20.9 3.2 82.8 25.4 19.9 2.7 82.61.024.8 19.9 2.6 82.70.013.5 15.3 3.4 53.5Posters0.25 0.5 0.7513.0 14.8 3.4 53.1 12.5 14.0 2.7 52.2 12.0 13.9 3.0 50.81.011.6 11.9 2.4 50.3Table 5: Results of the temperature hyperparameterexperiments. We use ROUGE-L (R-L), METEOR (M),BLEU (B) and BERTScore (BERTS) as our similaritymeasures.
https://github.com/microsoft/ knowledge-centric-templatic-views
We acknowledge that scientific writing does have structural regularities that may influence unified document generation. Due to the lack of other available datasets we leave exploration of other domains to future work
.3 https://learn.microsoft.com/en-us/azure/ ai-services/computer-vision/overview-ocr
https://learn.microsoft.com/en-us/azure/ ai-services/document-intelligence/concept-layout
A detailed evaluation of the temperature hyper-parameter is included in Appendix §C
The annotators are non-experts and do not have the background to determine factual correctness of scientific information. Instead, they are encouraged to use the original paper to understand if the information presented in the documents represent the information in the paper, to the best of their understanding.
Daniel S.Brown, Wonjoon Goo, Prabhat Nagarajan, Scott Niekum   December 11, 2023 <br />
'Introduction Inverse Reinforcement Learning (IRL) is a method used in machine learning where an agent learns to perform tasks by observing a demonstrator.However, a significant limitation of existing IRL methods is their inability to outperform the demonstrator.This is because IRL typically seeks a reward fun Summary of the Input Document A recent paper by Daniel S. Brown and colleagues introduces a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of ranked demonstrations to infer high-quality reward functions from a set of potent Important Takeaways T-REX has several advantages.First, rather than imitating suboptimal demonstrations, it allows us to identify features that are correlated with rankings, in a manner that can be extrapolated beyond the demonstrations.Second, when learning features directly from high-dimensional data, this The authors evaluated T-REX on a variety of standard Atari and MuJoCo benchmark tasks.Their experiments show that T-REX can extrapolate well, achieving performance that is often more than twice as high as the best-performing demonstration, as well as outperforming state-of-the-art imitation learning algo Conclusion T-REX is a promising new approach to IRL that can significantly outperform the demonstrator without additional external knowledge.This makes it a valuable tool for tasks where the demonstrator is suboptimal, and the goal is to exceed the demonstrator's performance.(a) Document generated without the intermediate representation.This example is not cropped.Extrapolating Beyond Suboptimal Demonstrations: A New Approach to InverseReinforcement Learning
Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Satanjeev Banerjee, Alon Lavie, IEEvaluation@ACL. Lutz Bornmann and Rüdiger Mutz. 2005. 201466Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Dawn Xiaodong Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, USENIX Security Symposium. 2020Extracting training data from large language models</p>
<p>Overview and insights from the shared tasks at scholarly document processing 2020: CL-SciSumm, LaySumm and LongSumm. Muthu Kumar Chandrasekaran, Guy Feigenblat, Eduard Hovy, Abhilasha Ravichander, Michal Shmueli-Scheuer, Anita De Waard, 10.18653/v1/2020.sdp-1.24Proceedings of the First Workshop on Scholarly Document Processing. the First Workshop on Scholarly Document ProcessingOnline. Association for Computational Linguistics2020</p>
<p>DialogSum: A real-life scenario dialogue summarization dataset. Yulong Chen, Yang Liu, Liang Chen, Yue Zhang, 10.18653/v1/2021.findings-acl.449Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Scientific article summarization using citation-context and article's discourse structure. Arman Cohan, Nazli Goharian, Conference on Empirical Methods in Natural Language Processing. 2015</p>
<p>How ready are pre-trained abstractive models and llms for legal case judgement summarization?. Aniket Deroy, Kripabandhu Ghosh, Saptarshi Ghosh, ArXiv, abs/2306.012482023</p>
<p>Qlarify: Bridging scholarly abstracts and papers with recursively expandable summaries. Raymond Fok, Joseph Chee Chang, Tal August, Amy X Zhang, Daniel S Weld, ArXiv, abs/2310.075812023</p>
<p>. David Freedman, Robert Pisani, Roger Purves, Statistics. 2007international student edition</p>
<p>. R Pisani, Purves, WW Norton &amp; CompanyNew York4th edn</p>
<p>Doc2ppt: Automatic presentation slides generation from scientific documents. Tsu-Jui Fu, William Yang Wang, Daniel J Mcduff, Yale Song, AAAI Conference on Artificial Intelligence. 2021</p>
<p>Ppsgen: Learningbased presentation slides generation for academic papers. Yue Hu, Xiaojun Wan, IEEE Transactions on Knowledge and Data Engineering. 272015</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Emma Bou Casas, Florian Hanna, Gianna Bressand, Guillaume Lengyel, Guillaume Bour, L Lample, Renard Elio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, ArXiv, abs/2401.04088Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2024</p>
<p>Devendra Singh Chaplot, Diego de Las Casas. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, ArXiv, abs/2310.06825Mistral 7b. Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2023</p>
<p>Talk-Summ: A dataset and scalable annotation method for scientific paper summarization based on conference talks. Guy Lev, Michal Shmueli-Scheuer, Jonathan Herzig, Achiya Jerbi, David Konopnicki, 10.18653/v1/P19-1204Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Towards topic-aware slide generation for academic papers with unsupervised mutual learning. Da-Wei Li, Danqing Huang, Tingting Ma, Chin-Yew Lin, AAAI Conference on Artificial Intelligence. 2021</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Annual Meeting of the Association for Computational Linguistics. 2004</p>
<p>Analyzing leakage of personally identifiable information in language models. Nils Lukas, A Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella, -B' Eguelin, 2023 IEEE Symposium on Security and Privacy (SP). 2023</p>
<p>Selfcheckgpt: Zero-resource blackbox hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark John, Francis Gales, ArXiv, abs/2303.088962023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Annual Meeting of the Association for Computational Linguistics. 2002</p>
<p>Learning to generate posters of scientific papers. Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal, ArXiv, abs/1604.012192016</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Echoes from alexandria: A large resource for multilingual book summarization. Alessandro Sciré, Simone Conia, Simone Ciciliano, Roberto Navigli, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Get to the point: Summarization with pointergenerator networks. Abigail See, Peter J Liu, Christopher D Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Scibertsum: Extractive summarization for scientific documents. Athar Sefid, C Lee Giles, 10.1007/978-3-031-06555-2_46DAS 2022Document Analysis Systems: 15th IAPR International Workshop. La Rochelle, France; Berlin, HeidelbergSpringer-Verlag2022. May 22-25, 2022</p>
<p>D2S: Document-to-slide generation via query-based text summarization. Edward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, Nancy X R Wang, 10.18653/v1/2021.naacl-main.111Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>The spearman rank correlation coefficient between intuitionistic fuzzy sets. Eulalia Szmidt, Janusz Kacprzyk, 10.1109/IS.2010.55483992010 5th IEEE International Conference Intelligent Systems. 2010</p>
<p>Fill in the BLANC: Human-free quality estimation of document summaries. Oleg Vasilyev, Vedant Dharnidharka, John Bohannon, 10.18653/v1/2020.eval4nlp-1.2Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems. the First Workshop on Evaluation and Comparison of NLP SystemsOnline. Association for Computational Linguistics2020</p>
<p>Slide4n: Creating presentation slides from computational notebooks with human-ai collaboration. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin ; Xuye, Oujing Liu, Ali Liu, Tengfei Neshati, Min Ma, J Zhu, Zhao, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2017. 2023a30Advances in Neural Information Processing Systems</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>iposter: Interactive poster generation based on topic structure and slide presentation. Yuanyuan Wang, Yukiko Kawai, Kazutoshi Sumiya, Transactions of The Japanese Society for Artificial Intelligence. 302015</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Transactions on Machine Learning Research. Survey Certification. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc2022b35</p>
<p>Neural content extraction for poster generation of scientific papers. Sheng Xu, Xiaojun Wan, ArXiv, abs/2112.085502021</p>
<p>BARTScore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. 2021</p>
<p>Integration of large language models into higher education: A perspective from learners. Katerina Zdravkova, Fisnik Dalipi, Fredrik Ahlgren, 2023 International Symposium on Computers in Education (SIIE). 2023</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, Conference on Empirical Methods in Natural Language Processing. 2019</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Peng Liu, Chenguang Zhu, Ji Heng, Jiawei Han, Conference on Empirical Methods in Natural Language Processing. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
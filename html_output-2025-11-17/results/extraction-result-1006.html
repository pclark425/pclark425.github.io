<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1006 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1006</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1006</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-82a8b7164f377c139e848986a2980a6678aa6cd3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/82a8b7164f377c139e848986a2980a6678aa6cd3" target="_blank">Online Bayesian Goal Inference for Boundedly-Rational Planning Agents</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Experiments are presented showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.</p>
                <p><strong>Paper Abstract:</strong> People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent's goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent's goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1006.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1006.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SIPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Inverse Plan Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online sequential Monte Carlo algorithm introduced in this paper that inverts a boundedly-rational, search-based planner to perform Bayesian inference over an agent's goals and partial plans using probabilistic programs embedded with PDDL domain models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sequential Inverse Plan Search (SIPS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SIPS maintains a particle approximation (SMC) to the posterior over goals, plans, states and actions. Each particle contains a hypothesized goal and a partial plan expressed over a PDDL world model; particles are extended online by calling a base planner (a stochastic A* variant) only when a hypothesized plan does not cover the next timestep. SIPS uses resampling and two Metropolis-Hastings rejuvenation kernels (heuristic-driven goal proposals and error-driven replanning) implemented via the Gen probabilistic programming system to preserve hypothesis diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (predicate-based) with support for probabilistic transitions via PPDDL; belief-state (particle) representation over goals/plans/states</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>World represented as predicate-based facts, relations and numeric fluents (PDDL). Actions are modeled as PDDL operators with preconditions and effects; transitions in experiments are primarily deterministic but the architecture supports stochastic transitions via Probabilistic PDDL (PPDDL). Observations are noisy: Boolean predicates may be corrupted and numeric fluents receive Gaussian noise. SIPS maintains a belief (particle set) over latent states, partial plans and goals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>epistemic uncertainty over goals/plans/states; aleatoric/observation noise; stochasticity in planning/search (search noise and stochastic planner budget)</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Sequential Monte Carlo (particle filter) over goals/plans/states, probability distributions for planner budget (negative binomial), stochastic A* successor sampling (softmax over -f(s,g)/gamma), Metropolis-Hastings rejuvenation via data-driven proposals, resampling</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>A* (stochastic variant as base planner) integrated inside an SMC inference loop; planning for baselines uses value iteration for BIRL and standard A* for PRP comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Posterior probability of true goal P(g_true|o) at trajectory quartiles (Q1-Q3), Top-1 accuracy, runtime metrics (startup C0, marginal MC, average AC seconds), number of states visited</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (Doors, Keys & Gems, non-optimal dataset): P(g_true|o) Q1=0.37, Q2=0.51, Q3=0.61; Top-1 at Q3 = 0.74; runtime C0=3.30s, MC=0.70s, AC=0.86s (from Table 1 in paper). Reported performance varies by domain (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to Bayesian IRL (value-iteration based) and PRP: on Doors, Keys & Gems SIPS Q3=0.61 vs BIRL-oracle Q3=0.42 and BIRL-unbiased Q3=0.33; SIPS AC=0.86s vs BIRL-unbiased AC=154s and BIRL-oracle AC=7.01s; PRP can achieve high Top-1 but at much higher runtime in many domains (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablation isolating 'with vs without' uncertainty modeling; the paper reports robustness experiments to parameter mismatch (r, q, gamma, heuristics) showing SIPS remains effective under many mismatches, and reports results with and without rejuvenation (rejuvenation improves handling of highly sub-optimal/failed plans but increases compute).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SIPS demonstrates that a probabilistic symbolic world model (PDDL/PPDDL) embedded in probabilistic programs together with SMC belief tracking can perform accurate, online goal inference even from sub-optimal and failed trajectories. Modeling uncertainty at the level of planning (search noise and limited search budget) rather than at the action-selection level yields more human-like inference dynamics and often better accuracy and runtime than value-iteration based Bayesian IRL baselines. The architecture does not use large language models and is not applied to text-based interactive environments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1006.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PDDL - Planning Domain Definition Language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic language for expressing planning domains in terms of predicates, relations, numeric fluents, actions (preconditions/effects) and goals; embedded into the paper's probabilistic programs to represent states and transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PDDL - the Planning Domain Definition Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDDL embedding (via PDDL.jl / Plinf.jl)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PDDL is used to represent states, goals and transition operators (preconditions/effects) inside the probabilistic programs; this enables reuse of general-purpose symbolic planners (A*, other heuristic planners) as modeling components. Numeric fluents and predicate facts are supported; observation noise is applied to these representations for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL (symbolic predicate-based), with explicit mention/support for PPDDL (probabilistic PDDL) for stochastic transitions</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>States: sets of predicate facts and numeric fluents; Actions: PDDL operators with preconditions/effects; Transitions: typically deterministic in experiments but the system can represent stochastic transitions via PPDDL; Goals: PDDL goal specifications or reward functions expressed over predicates</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>observation noise on predicates and numeric fluents; optional probabilistic transitions via PPDDL</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Observation model: Bernoulli corruption of Boolean predicates and Gaussian noise on numeric fluents; optional PPDDL-style probabilistic transition operators</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Symbolic planners (A* and other heuristic search planners) are used as sub-components operating over PDDL encodings</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding PDDL into the probabilistic-programming model enables use of fast general-purpose symbolic planners as modeling components and allows representing goals/states compactly; stochastic transitions could be represented using PPDDL if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1006.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPDDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic PDDL (PPDDL / Ppddl1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension to PDDL for expressing planning domains with probabilistic (stochastic) effects and transitions; cited in the paper as supported by the architecture for representing stochastic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ppddl1. 0: An extension to pddl for expressing planning domains with probabilistic effects.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PPDDL (Probabilistic PDDL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PPDDL extends PDDL action operators with probabilistic effects, allowing specification of stochastic transition dynamics in symbolic domains. The paper notes support for stochastic transitions 'as in Probabilistic PDDL' though deterministic transitions were used in most experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PPDDL (probabilistic symbolic transition model)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Actions can have probabilistic outcomes; states remain predicate-based facts and numeric fluents; supports modeling aleatoric transition uncertainty explicitly in the domain definition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>aleatoric (stochastic) transition uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Probabilistic transition operators specified in domain (PPDDL)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Classical symbolic planners may be used; the paper does not detail specialized probabilistic planners but notes that the architecture can support stochastic transitions expressed in PPDDL</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PPDDL is acknowledged as the representation for stochastic domain dynamics; the paper's architecture supports PPDDL-style stochastic transitions though experiments primarily used deterministic transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1006.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gen probabilistic programming system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose probabilistic programming system used to implement the boundedly-rational agent models, SIPS inference, and custom inference kernels (data-driven proposals and involutive rejuvenation) for efficient Bayesian inference over plans and goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gen: a general-purpose probabilistic programming system with programmable inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gen (probabilistic programming system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Gen provides primitives for probabilistic programs and customizable inference; SIPS and the boundedly-rational agent programs (UPDATE-PLAN, SELECT-ACTION) are implemented in Gen. Gen's support for data-driven proposals and involutive MCMC kernels enables the MH rejuvenation moves used in SIPS.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>probabilistic program embedding PDDL world models (so PDDL + probabilistic program hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Gen models the stochastic generative process over goals, plans, actions and states; the PDDL-based environment model is embedded into Gen programs so that planners and transition models can be called from within the probabilistic program.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>epistemic uncertainty over latent variables (goals, plans), aleatoric observation noise, stochastic planning/search</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Supports SMC, MCMC (involutive kernels), data-driven proposals; implements the particle filter and MH rejuvenation used by SIPS.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Hosts calls to planning algorithms (e.g., stochastic A*), but is itself the probabilistic inference engine coordinating SMC/MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gen enabled the implementation of the probabilistic agent models and the custom inference moves (data-driven proposals and involutive kernels) that make SIPS computationally efficient; it is central to composing symbolic PDDL models with probabilistic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1006.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stochastic A*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic (probabilistic) A* search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified A* planner used as the base planner inside the boundedly-rational agent model; successor expansions are sampled probabilistically with a softmax over estimated f(s,g) to model search noise/sub-optimal search behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stochastic A* (probabilistic search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instead of deterministically expanding the most promising successor, the planner samples successor states with probability proportional to exp(-f(s,g)/gamma), where f = path cost + heuristic and gamma controls search noise. Search is limited by a sampled budget (negative binomial) so partial plans may be returned.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>operates over PDDL states and actions (symbolic world model)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Search explores predicate-based states defined in PDDL; transitions are as specified by the PDDL operator semantics; stochasticity is introduced in the successor selection process and via sampling of search budget.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>planning/search stochasticity (modeling sub-optimal planning decisions) and tied to epistemic uncertainty over selected partial plans</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Softmax sampling of successors controlled by noise parameter gamma; negative binomial distribution for sampled node budget eta</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>A* variant (heuristic search) with stochastic successor expansion</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Modeling search as stochastic and resource-limited (sampled budget) provides a mechanism to capture boundedly-rational/suboptimal planning behavior and enables the observer (via SIPS) to explain backtracking and failed plans as outcomes of limited/stochastic planning rather than purely action noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1006.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMC belief tracker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Monte Carlo (particle filter) belief state over goals/plans/states</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The core inference mechanism in SIPS: a particle-based belief state maintained online over goals, partial plans and latent states, with resampling and MH rejuvenation moves to preserve diversity and reintroduce pruned hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SMC particle filter for goal/plan inference</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Particles each encode (g, p_1:t, s_1:t, a_1:t). At each timestep particles are extended by sampling next state and, if needed, calling UPDATE-PLAN to extend the partial plan. Weights updated via observation likelihood P(o_t | s_t). When effective sample size falls below threshold, resample and apply rejuvenation MCMC moves (heuristic-driven goal proposals and error-driven replanning).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>belief-state over PDDL symbolic world model and probabilistic program latent variables</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Belief encodes distributions over predicate-based PDDL states, partial plan sequences and goals; transitions follow PDDL operators and the agent generative model (stochastic A*/budget)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>epistemic uncertainty over goals/plans and uncertainty about latent states; observation noise</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Particle filtering (weights, resampling), Metropolis-Hastings rejuvenation with data-driven proposals (softmax heuristic proposals, replanning proposals), effective-sample-size thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Used to invert a planner (stochastic A*) â€” the SMC loop performs inference rather than planning itself</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Same as SIPS (posterior metrics and runtime); SMC is the algorithm used to produce those results</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>See SIPS entry (SMC is core to achieving reported Q1-Q3 posterior and runtime numbers across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against Bayesian IRL (value-iteration based particle-less inference) and PRP; SMC-based SIPS is faster and more accurate in many large/sparse domains.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper reports experiments with and without rejuvenation; rejuvenation increases robustness to highly sub-optimal/failed plans but increases runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Particle-based belief tracking over symbolic PDDL states/plans enables online inference of goals from sub-optimal or failed behavior; data-driven rejuvenation proposals are important to recover pruned but later-plausible goals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1006.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plinf.jl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plinf.jl (Plinf package for Bayesian inverse planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Julia implementation of the paper's modeling and inference architecture that integrates Gen with a PDDL interpreter (PDDL.jl) to perform SIPS and the experiments; code is publicly available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Plinf.jl (implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Plinf.jl ties together Gen (for probabilistic programs/inference), PDDL.jl (PDDL interpreter), and implementations of SIPS, stochastic A*, and utilities for running the reported experiments and baselines (BIRL, PRP).</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>PDDL embedding / probabilistic-program hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Uses PDDL.jl to represent predicate-based states and actions; Gen to represent probabilistic generative processes and inference over them.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>observation noise, planning/search stochasticity, uncertainty over goals/plans</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>SMC, MH rejuvenation, stochastic planner budget modeling</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Invokes A* (and stochastic A*) implemented over PDDL.jl; orchestrates SIPS inference</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code release demonstrates practical integration of probabilistic programming and symbolic PDDL planning for Bayesian inverse planning; enables reproducibility of experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1006.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Inverse Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach used in the paper: computes a Boltzmann-rational policy for each candidate goal via value iteration and evaluates likelihood of observed actions under that policy to produce a posterior over goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian inverse reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Inverse Reinforcement Learning (BIRL) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BIRL treats goals as (indicator) reward functions and computes value functions Q(s,a) with value iteration; likelihood of actions is Boltzmann in Q (P(a|s,g) âˆ exp(alpha * Q(s,a))). Two variants were implemented: unbiased asynchronous VI sampling states uniformly and an oracle VI variant that samples states from observed trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>MDP / value-function based (not explicitly PDDL although environments were instantiated as PDDL Gym environments)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Uses an MDP formalism with value functions computed by (asynchronous) value iteration; transitions assumed known and typically deterministic in experiments; uncertainty modeled as Boltzmann action noise (alpha parameter).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>action-selection (Boltzmann) noise modeling, but not planning/search uncertainty or belief-state tracking over goals</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>Boltzmann policy P(a|s,g) with temperature alpha; posterior over goals computed exactly given computed policies</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Value iteration (asynchronous variants used for large state spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Same posterior metrics (P(g_true|o) quartiles), Top-1 accuracy, runtime (VI iterations, seconds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (Doors, Keys & Gems): unbiased BIRL Q3=0.33 (flat posterior due to VI failing to converge), oracle BIRL Q3=0.42; runtime for unbiased BIRL is large (C0 ~ 3326s, AC ~154s) whereas oracle BIRL is faster but still often slower than SIPS on some domains (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to SIPS and PRP: SIPS outperforms unbiased BIRL in speed and accuracy in most complex domains; oracle BIRL can be competitive but assumes advance access to trajectories (not online).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Value-iteration based Bayesian IRL that models action noise rather than planning/search noise performs worse at explaining sub-optimal and failed plans and is computationally expensive on large/sparse domains; this motivates modeling planning-level uncertainty (as in SIPS).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1006.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1006.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan Recognition as Planning (PRP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline benchmark (adapted from prior work) that approximates plan likelihoods using the cost difference from optimal plans and requires computing optimal partial plans for every goal at each timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Probabilistic plan recognition using off-the-shelf classical planners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Plan Recognition as Planning (PRP) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PRP uses classical planners to compute optimal plans to each goal and models likelihood of observed partial plan p as proportional to exp(-beta*(|p|-|p*|)), where p* is the optimal plan. At each timestep it computes a partial plan to each goal consistent with observations and evaluates these cost differences to form a posterior over goals. PRP is accurate on many trajectories but computationally costly and unable to account for irreversible failures in the agent's behavior model.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>classical symbolic planners over PDDL (deterministic planning), heuristic cost-based likelihood model</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Relies on optimal plan computation under a PDDL domain for each candidate goal; represents plans and costs but not stochastic planning behavior or belief states.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>Classical optimal planning (A*/heuristic planners) to compute optimal and partial plans for each goal</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Top-1 accuracy, posterior over goals, runtime</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PRP often achieves high Top-1 accuracy but with much higher runtime; e.g., in some domains PRP's Top-1 = 1.00 but AC and C0 can be many times larger than SIPS (see tables); PRP fails to model irreversible failures and can default to uniform posteriors upon failure.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to SIPS and BIRL: PRP can have higher Top-1 accuracy in many cases but is far more computationally expensive and less robust to failed/irreversible plans.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PRP provides a useful offline accuracy benchmark but its assumptions (always-plan-exists, cost-based likelihood) make it ill-suited for explaining irreversible failures and for online computation at scale; contrasts motivate SIPS' online incremental plan extension and probabilistic planning-level modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Bayesian Goal Inference for Boundedly-Rational Planning Agents', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ppddl1. 0: An extension to pddl for expressing planning domains with probabilistic effects. <em>(Rating: 2)</em></li>
                <li>PDDL - the Planning Domain Definition Language <em>(Rating: 2)</em></li>
                <li>Gen: a general-purpose probabilistic programming system with programmable inference <em>(Rating: 2)</em></li>
                <li>Probabilistic plan recognition using off-the-shelf classical planners <em>(Rating: 2)</em></li>
                <li>Plan recognition as planning <em>(Rating: 2)</em></li>
                <li>Probabilistic programs for inferring the goals of autonomous agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1006",
    "paper_id": "paper-82a8b7164f377c139e848986a2980a6678aa6cd3",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "SIPS",
            "name_full": "Sequential Inverse Plan Search",
            "brief_description": "An online sequential Monte Carlo algorithm introduced in this paper that inverts a boundedly-rational, search-based planner to perform Bayesian inference over an agent's goals and partial plans using probabilistic programs embedded with PDDL domain models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Sequential Inverse Plan Search (SIPS)",
            "system_description": "SIPS maintains a particle approximation (SMC) to the posterior over goals, plans, states and actions. Each particle contains a hypothesized goal and a partial plan expressed over a PDDL world model; particles are extended online by calling a base planner (a stochastic A* variant) only when a hypothesized plan does not cover the next timestep. SIPS uses resampling and two Metropolis-Hastings rejuvenation kernels (heuristic-driven goal proposals and error-driven replanning) implemented via the Gen probabilistic programming system to preserve hypothesis diversity.",
            "world_model_type": "PDDL (predicate-based) with support for probabilistic transitions via PPDDL; belief-state (particle) representation over goals/plans/states",
            "world_model_description": "World represented as predicate-based facts, relations and numeric fluents (PDDL). Actions are modeled as PDDL operators with preconditions and effects; transitions in experiments are primarily deterministic but the architecture supports stochastic transitions via Probabilistic PDDL (PPDDL). Observations are noisy: Boolean predicates may be corrupted and numeric fluents receive Gaussian noise. SIPS maintains a belief (particle set) over latent states, partial plans and goals.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "epistemic uncertainty over goals/plans/states; aleatoric/observation noise; stochasticity in planning/search (search noise and stochastic planner budget)",
            "uncertainty_method": "Sequential Monte Carlo (particle filter) over goals/plans/states, probability distributions for planner budget (negative binomial), stochastic A* successor sampling (softmax over -f(s,g)/gamma), Metropolis-Hastings rejuvenation via data-driven proposals, resampling",
            "planning_algorithm": "A* (stochastic variant as base planner) integrated inside an SMC inference loop; planning for baselines uses value iteration for BIRL and standard A* for PRP comparisons",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Posterior probability of true goal P(g_true|o) at trajectory quartiles (Q1-Q3), Top-1 accuracy, runtime metrics (startup C0, marginal MC, average AC seconds), number of states visited",
            "performance_value": "Example (Doors, Keys & Gems, non-optimal dataset): P(g_true|o) Q1=0.37, Q2=0.51, Q3=0.61; Top-1 at Q3 = 0.74; runtime C0=3.30s, MC=0.70s, AC=0.86s (from Table 1 in paper). Reported performance varies by domain (see paper tables).",
            "baseline_comparison": "Compared to Bayesian IRL (value-iteration based) and PRP: on Doors, Keys & Gems SIPS Q3=0.61 vs BIRL-oracle Q3=0.42 and BIRL-unbiased Q3=0.33; SIPS AC=0.86s vs BIRL-unbiased AC=154s and BIRL-oracle AC=7.01s; PRP can achieve high Top-1 but at much higher runtime in many domains (see tables).",
            "has_ablation_uncertainty": false,
            "ablation_results": "No explicit ablation isolating 'with vs without' uncertainty modeling; the paper reports robustness experiments to parameter mismatch (r, q, gamma, heuristics) showing SIPS remains effective under many mismatches, and reports results with and without rejuvenation (rejuvenation improves handling of highly sub-optimal/failed plans but increases compute).",
            "key_findings": "SIPS demonstrates that a probabilistic symbolic world model (PDDL/PPDDL) embedded in probabilistic programs together with SMC belief tracking can perform accurate, online goal inference even from sub-optimal and failed trajectories. Modeling uncertainty at the level of planning (search noise and limited search budget) rather than at the action-selection level yields more human-like inference dynamics and often better accuracy and runtime than value-iteration based Bayesian IRL baselines. The architecture does not use large language models and is not applied to text-based interactive environments in this paper.",
            "uuid": "e1006.0",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "PDDL",
            "name_full": "PDDL - Planning Domain Definition Language",
            "brief_description": "A symbolic language for expressing planning domains in terms of predicates, relations, numeric fluents, actions (preconditions/effects) and goals; embedded into the paper's probabilistic programs to represent states and transitions.",
            "citation_title": "PDDL - the Planning Domain Definition Language",
            "mention_or_use": "use",
            "system_name": "PDDL embedding (via PDDL.jl / Plinf.jl)",
            "system_description": "PDDL is used to represent states, goals and transition operators (preconditions/effects) inside the probabilistic programs; this enables reuse of general-purpose symbolic planners (A*, other heuristic planners) as modeling components. Numeric fluents and predicate facts are supported; observation noise is applied to these representations for inference.",
            "world_model_type": "PDDL (symbolic predicate-based), with explicit mention/support for PPDDL (probabilistic PDDL) for stochastic transitions",
            "world_model_description": "States: sets of predicate facts and numeric fluents; Actions: PDDL operators with preconditions/effects; Transitions: typically deterministic in experiments but the system can represent stochastic transitions via PPDDL; Goals: PDDL goal specifications or reward functions expressed over predicates",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "observation noise on predicates and numeric fluents; optional probabilistic transitions via PPDDL",
            "uncertainty_method": "Observation model: Bernoulli corruption of Boolean predicates and Gaussian noise on numeric fluents; optional PPDDL-style probabilistic transition operators",
            "planning_algorithm": "Symbolic planners (A* and other heuristic search planners) are used as sub-components operating over PDDL encodings",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Embedding PDDL into the probabilistic-programming model enables use of fast general-purpose symbolic planners as modeling components and allows representing goals/states compactly; stochastic transitions could be represented using PPDDL if needed.",
            "uuid": "e1006.1",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "PPDDL",
            "name_full": "Probabilistic PDDL (PPDDL / Ppddl1.0)",
            "brief_description": "An extension to PDDL for expressing planning domains with probabilistic (stochastic) effects and transitions; cited in the paper as supported by the architecture for representing stochastic dynamics.",
            "citation_title": "Ppddl1. 0: An extension to pddl for expressing planning domains with probabilistic effects.",
            "mention_or_use": "mention",
            "system_name": "PPDDL (Probabilistic PDDL)",
            "system_description": "PPDDL extends PDDL action operators with probabilistic effects, allowing specification of stochastic transition dynamics in symbolic domains. The paper notes support for stochastic transitions 'as in Probabilistic PDDL' though deterministic transitions were used in most experiments.",
            "world_model_type": "PPDDL (probabilistic symbolic transition model)",
            "world_model_description": "Actions can have probabilistic outcomes; states remain predicate-based facts and numeric fluents; supports modeling aleatoric transition uncertainty explicitly in the domain definition.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "aleatoric (stochastic) transition uncertainty",
            "uncertainty_method": "Probabilistic transition operators specified in domain (PPDDL)",
            "planning_algorithm": "Classical symbolic planners may be used; the paper does not detail specialized probabilistic planners but notes that the architecture can support stochastic transitions expressed in PPDDL",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "PPDDL is acknowledged as the representation for stochastic domain dynamics; the paper's architecture supports PPDDL-style stochastic transitions though experiments primarily used deterministic transitions.",
            "uuid": "e1006.2",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Gen",
            "name_full": "Gen probabilistic programming system",
            "brief_description": "A general-purpose probabilistic programming system used to implement the boundedly-rational agent models, SIPS inference, and custom inference kernels (data-driven proposals and involutive rejuvenation) for efficient Bayesian inference over plans and goals.",
            "citation_title": "Gen: a general-purpose probabilistic programming system with programmable inference",
            "mention_or_use": "use",
            "system_name": "Gen (probabilistic programming system)",
            "system_description": "Gen provides primitives for probabilistic programs and customizable inference; SIPS and the boundedly-rational agent programs (UPDATE-PLAN, SELECT-ACTION) are implemented in Gen. Gen's support for data-driven proposals and involutive MCMC kernels enables the MH rejuvenation moves used in SIPS.",
            "world_model_type": "probabilistic program embedding PDDL world models (so PDDL + probabilistic program hybrid)",
            "world_model_description": "Gen models the stochastic generative process over goals, plans, actions and states; the PDDL-based environment model is embedded into Gen programs so that planners and transition models can be called from within the probabilistic program.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "epistemic uncertainty over latent variables (goals, plans), aleatoric observation noise, stochastic planning/search",
            "uncertainty_method": "Supports SMC, MCMC (involutive kernels), data-driven proposals; implements the particle filter and MH rejuvenation used by SIPS.",
            "planning_algorithm": "Hosts calls to planning algorithms (e.g., stochastic A*), but is itself the probabilistic inference engine coordinating SMC/MCMC",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Gen enabled the implementation of the probabilistic agent models and the custom inference moves (data-driven proposals and involutive kernels) that make SIPS computationally efficient; it is central to composing symbolic PDDL models with probabilistic inference.",
            "uuid": "e1006.3",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Stochastic A*",
            "name_full": "Stochastic (probabilistic) A* search",
            "brief_description": "A modified A* planner used as the base planner inside the boundedly-rational agent model; successor expansions are sampled probabilistically with a softmax over estimated f(s,g) to model search noise/sub-optimal search behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Stochastic A* (probabilistic search)",
            "system_description": "Instead of deterministically expanding the most promising successor, the planner samples successor states with probability proportional to exp(-f(s,g)/gamma), where f = path cost + heuristic and gamma controls search noise. Search is limited by a sampled budget (negative binomial) so partial plans may be returned.",
            "world_model_type": "operates over PDDL states and actions (symbolic world model)",
            "world_model_description": "Search explores predicate-based states defined in PDDL; transitions are as specified by the PDDL operator semantics; stochasticity is introduced in the successor selection process and via sampling of search budget.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "planning/search stochasticity (modeling sub-optimal planning decisions) and tied to epistemic uncertainty over selected partial plans",
            "uncertainty_method": "Softmax sampling of successors controlled by noise parameter gamma; negative binomial distribution for sampled node budget eta",
            "planning_algorithm": "A* variant (heuristic search) with stochastic successor expansion",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Modeling search as stochastic and resource-limited (sampled budget) provides a mechanism to capture boundedly-rational/suboptimal planning behavior and enables the observer (via SIPS) to explain backtracking and failed plans as outcomes of limited/stochastic planning rather than purely action noise.",
            "uuid": "e1006.4",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "SMC belief tracker",
            "name_full": "Sequential Monte Carlo (particle filter) belief state over goals/plans/states",
            "brief_description": "The core inference mechanism in SIPS: a particle-based belief state maintained online over goals, partial plans and latent states, with resampling and MH rejuvenation moves to preserve diversity and reintroduce pruned hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SMC particle filter for goal/plan inference",
            "system_description": "Particles each encode (g, p_1:t, s_1:t, a_1:t). At each timestep particles are extended by sampling next state and, if needed, calling UPDATE-PLAN to extend the partial plan. Weights updated via observation likelihood P(o_t | s_t). When effective sample size falls below threshold, resample and apply rejuvenation MCMC moves (heuristic-driven goal proposals and error-driven replanning).",
            "world_model_type": "belief-state over PDDL symbolic world model and probabilistic program latent variables",
            "world_model_description": "Belief encodes distributions over predicate-based PDDL states, partial plan sequences and goals; transitions follow PDDL operators and the agent generative model (stochastic A*/budget)",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "epistemic uncertainty over goals/plans and uncertainty about latent states; observation noise",
            "uncertainty_method": "Particle filtering (weights, resampling), Metropolis-Hastings rejuvenation with data-driven proposals (softmax heuristic proposals, replanning proposals), effective-sample-size thresholding",
            "planning_algorithm": "Used to invert a planner (stochastic A*) â€” the SMC loop performs inference rather than planning itself",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Same as SIPS (posterior metrics and runtime); SMC is the algorithm used to produce those results",
            "performance_value": "See SIPS entry (SMC is core to achieving reported Q1-Q3 posterior and runtime numbers across domains)",
            "baseline_comparison": "Compared against Bayesian IRL (value-iteration based particle-less inference) and PRP; SMC-based SIPS is faster and more accurate in many large/sparse domains.",
            "has_ablation_uncertainty": false,
            "ablation_results": "Paper reports experiments with and without rejuvenation; rejuvenation increases robustness to highly sub-optimal/failed plans but increases runtime.",
            "key_findings": "Particle-based belief tracking over symbolic PDDL states/plans enables online inference of goals from sub-optimal or failed behavior; data-driven rejuvenation proposals are important to recover pruned but later-plausible goals.",
            "uuid": "e1006.5",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Plinf.jl",
            "name_full": "Plinf.jl (Plinf package for Bayesian inverse planning)",
            "brief_description": "A Julia implementation of the paper's modeling and inference architecture that integrates Gen with a PDDL interpreter (PDDL.jl) to perform SIPS and the experiments; code is publicly available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Plinf.jl (implementation)",
            "system_description": "Plinf.jl ties together Gen (for probabilistic programs/inference), PDDL.jl (PDDL interpreter), and implementations of SIPS, stochastic A*, and utilities for running the reported experiments and baselines (BIRL, PRP).",
            "world_model_type": "PDDL embedding / probabilistic-program hybrid",
            "world_model_description": "Uses PDDL.jl to represent predicate-based states and actions; Gen to represent probabilistic generative processes and inference over them.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "observation noise, planning/search stochasticity, uncertainty over goals/plans",
            "uncertainty_method": "SMC, MH rejuvenation, stochastic planner budget modeling",
            "planning_algorithm": "Invokes A* (and stochastic A*) implemented over PDDL.jl; orchestrates SIPS inference",
            "planning_integrates_uncertainty": true,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Code release demonstrates practical integration of probabilistic programming and symbolic PDDL planning for Bayesian inverse planning; enables reproducibility of experiments in the paper.",
            "uuid": "e1006.6",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "BIRL",
            "name_full": "Bayesian Inverse Reinforcement Learning",
            "brief_description": "A baseline approach used in the paper: computes a Boltzmann-rational policy for each candidate goal via value iteration and evaluates likelihood of observed actions under that policy to produce a posterior over goals.",
            "citation_title": "Bayesian inverse reinforcement learning",
            "mention_or_use": "use",
            "system_name": "Bayesian Inverse Reinforcement Learning (BIRL) baseline",
            "system_description": "BIRL treats goals as (indicator) reward functions and computes value functions Q(s,a) with value iteration; likelihood of actions is Boltzmann in Q (P(a|s,g) âˆ exp(alpha * Q(s,a))). Two variants were implemented: unbiased asynchronous VI sampling states uniformly and an oracle VI variant that samples states from observed trajectories.",
            "world_model_type": "MDP / value-function based (not explicitly PDDL although environments were instantiated as PDDL Gym environments)",
            "world_model_description": "Uses an MDP formalism with value functions computed by (asynchronous) value iteration; transitions assumed known and typically deterministic in experiments; uncertainty modeled as Boltzmann action noise (alpha parameter).",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "action-selection (Boltzmann) noise modeling, but not planning/search uncertainty or belief-state tracking over goals",
            "uncertainty_method": "Boltzmann policy P(a|s,g) with temperature alpha; posterior over goals computed exactly given computed policies",
            "planning_algorithm": "Value iteration (asynchronous variants used for large state spaces)",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Same posterior metrics (P(g_true|o) quartiles), Top-1 accuracy, runtime (VI iterations, seconds)",
            "performance_value": "Example (Doors, Keys & Gems): unbiased BIRL Q3=0.33 (flat posterior due to VI failing to converge), oracle BIRL Q3=0.42; runtime for unbiased BIRL is large (C0 ~ 3326s, AC ~154s) whereas oracle BIRL is faster but still often slower than SIPS on some domains (see paper tables).",
            "baseline_comparison": "Compared to SIPS and PRP: SIPS outperforms unbiased BIRL in speed and accuracy in most complex domains; oracle BIRL can be competitive but assumes advance access to trajectories (not online).",
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Value-iteration based Bayesian IRL that models action noise rather than planning/search noise performs worse at explaining sub-optimal and failed plans and is computationally expensive on large/sparse domains; this motivates modeling planning-level uncertainty (as in SIPS).",
            "uuid": "e1006.7",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "PRP",
            "name_full": "Plan Recognition as Planning (PRP)",
            "brief_description": "An offline benchmark (adapted from prior work) that approximates plan likelihoods using the cost difference from optimal plans and requires computing optimal partial plans for every goal at each timestep.",
            "citation_title": "Probabilistic plan recognition using off-the-shelf classical planners",
            "mention_or_use": "use",
            "system_name": "Plan Recognition as Planning (PRP) baseline",
            "system_description": "PRP uses classical planners to compute optimal plans to each goal and models likelihood of observed partial plan p as proportional to exp(-beta*(|p|-|p*|)), where p* is the optimal plan. At each timestep it computes a partial plan to each goal consistent with observations and evaluates these cost differences to form a posterior over goals. PRP is accurate on many trajectories but computationally costly and unable to account for irreversible failures in the agent's behavior model.",
            "world_model_type": "classical symbolic planners over PDDL (deterministic planning), heuristic cost-based likelihood model",
            "world_model_description": "Relies on optimal plan computation under a PDDL domain for each candidate goal; represents plans and costs but not stochastic planning behavior or belief states.",
            "uses_llm": false,
            "llm_role": null,
            "llm_model_name": null,
            "uncertainty_modeling": false,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "Classical optimal planning (A*/heuristic planners) to compute optimal and partial plans for each goal",
            "planning_integrates_uncertainty": false,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": "Top-1 accuracy, posterior over goals, runtime",
            "performance_value": "PRP often achieves high Top-1 accuracy but with much higher runtime; e.g., in some domains PRP's Top-1 = 1.00 but AC and C0 can be many times larger than SIPS (see tables); PRP fails to model irreversible failures and can default to uniform posteriors upon failure.",
            "baseline_comparison": "Compared to SIPS and BIRL: PRP can have higher Top-1 accuracy in many cases but is far more computationally expensive and less robust to failed/irreversible plans.",
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "PRP provides a useful offline accuracy benchmark but its assumptions (always-plan-exists, cost-based likelihood) make it ill-suited for explaining irreversible failures and for online computation at scale; contrasts motivate SIPS' online incremental plan extension and probabilistic planning-level modeling.",
            "uuid": "e1006.8",
            "source_info": {
                "paper_title": "Online Bayesian Goal Inference for Boundedly-Rational Planning Agents",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ppddl1. 0: An extension to pddl for expressing planning domains with probabilistic effects.",
            "rating": 2
        },
        {
            "paper_title": "PDDL - the Planning Domain Definition Language",
            "rating": 2
        },
        {
            "paper_title": "Gen: a general-purpose probabilistic programming system with programmable inference",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic plan recognition using off-the-shelf classical planners",
            "rating": 2
        },
        {
            "paper_title": "Plan recognition as planning",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic programs for inferring the goals of autonomous agents",
            "rating": 2
        }
    ],
    "cost": 0.024670749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Online Bayesian Goal Inference for Boundedly-Rational Planning Agents</h1>
<p>Tan Zhi-Xuan, Jordyn L. Mann, Tom Silver Joshua B. Tenenbaum, Vikash K. Mansinghka<br>Massachusetts Institute of Technology<br>{xuan, jordynm,tslvr,jbt,vkm}@mit.edu</p>
<h4>Abstract</h4>
<p>People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent's goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent's goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.</p>
<h2>1 Introduction</h2>
<p>Everyday experience tells us that it is impossible to plan ahead for everything. Yet, not only do humans still manage to achieve our goals by piecing together partial and approximate plans, we also appear to account for this cognitive strategy when inferring the goals of others, understanding that they might plan and act sub-optimally, or even fail to achieve their goals. Indeed, even 18-month old infants seem capable of such inferences, offering their assistance to adults after observing them execute failed plans [1]. How might we understand this ability to infer goals from such plans? And how might we endow machines with this capacity, so they might assist us when our plans fail?
While there has been considerable work on inferring the goals and desires of agents, much of this work has assumed that agents act optimally to achieve their goals. Even when this assumption is relaxed, the forms of sub-optimality considered are often highly simplified. In inverse reinforcement learning, for example, agents are assumed to either act optimally [2] or to exhibit Boltzmann-rational action noise [3], while in the plan recognition literature, longer plans are assigned exponentially decreasing probability [4]. None of these approaches account for the difficulty of planning itself, which may lead agents to produce sub-optimal or failed plans. This not only makes them ill-equipped to infer goals from such plans, but also saddles them with a cognitively implausible burden: If inferring an agent's goals requires knowing the optimal solution to reach each goal, then an observer would need to compute the optimal plan or policy for all of those goals in advance [5]. Outside of the simplest problems and domains, this is deeply intractable.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our architecture performing online Bayesian goal inference via Sequential Inverse Plan Search. In (a), an agent exhibits a sub-optimal plan to acquire the blue gem, backtracking to pick up the key required for the second door. In (b), an agent exhibits a failed plan to acquire the blue gem, myopically using up its first key to get closer to the gem instead of realizing that it needs to collect the bottom two keys. In both cases, our method not only manages to infer the correct goal by the end, but also captures sharp human-like shifts in its inferences at key points, such as (a.ii) when the agent picks up a key unnecessary for the red gem, (a.ii) when the agent starts to backtrack, (b.iii) when the agent ignores the door to the red gem, or (b.iv) when the agent unlocks the first door to the blue gem.</p>
<p>In this paper, we present a unified modeling and inference architecture (Figure 2) that addresses both of these limitations. In contrast to prior work that models agents as actors that are noisily rational, we model agents as planners that are boundedly rational with respect to how much they plan, interleaving resource-limited plan search with plan execution. This allows us to perform online Bayesian inference of plans and goals even from highly sub-optimal trajectories involving backtracking or irreversible failure (Figure 1). We do so by modeling agents as probabilistic programs (Figure 3), comprised of goal priors and domain-general planning algorithms (Figure 2i), and interacting with a symbolic environment model (Figure 2ii). Inference is then performed via Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo (SMC) algorithm that exploits the replanning assumption of our agent models, incrementally inferring partial plans while limiting computational cost (Figure 2iii).
Our architecture delivers both accuracy and speed by being built in Gen, a general-purpose probabilistic programming system that supports customized inference using data-driven proposals and involutive rejuvenation kernels [6, 7, 8], alongside an embedding of the Planning Domain Definition Language [9, 10], enabling the use of fast general-purpose planners [11] as modeling components. We evaluate our approach against a Bayesian inverse reinforcement learning baseline [12] on a wide variety of planning domains that exhibit compositional task structure and sparse rewards (e.g. Figure 1), achieving high accuracy on many domains, often with orders of magnitude less computation.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our modeling and inference architecture is comprised of: (i) A programmatic model of a boundedly rational planning agent, implemented in the Gen probabilistic programming system; (ii) An environment model specified in the Planning Domain Definition Language (PDDL), facilitating support for a wide variety of planning domains and state-of-the-art symbolic planners; (iii) Sequential Inverse Plan Search (SIPS), a novel SMC algorithm that exploits the replanning assumption of our agent model to reduce computation, extending hypothesized plans only as new observations arrive.</p>
<h1>2 Related Work</h1>
<p>Inverse reinforcement learning (IRL). A long line of work has shown how to learn reward functions as explanations of goal-directed agent behavior via inverse reinforcement learning [2, 13, 12, 14]. However, most such approaches are too costly for online settings of complex domains, as they require solving the underlying Markov Decision Process (MDP) for every posited goal or reward function, and for all possible initial states [15, 5]. Our approach instead assumes that agents are online model-based planners. This greatly reduces computation time, while also better reflecting humans' intuitive understanding of other agents.
Bayesian theory-of-mind (BToM). Computational models of humans' intuitive theory-of-mind posit that we understand other's actions by Bayesian inference of their likely goals and beliefs. These models, largely built upon the same MDP formalism used in IRL, have been shown to make predictions that correspond closely with human inferences [16, 17, 18, 19, 20, 21, 22]. Some recent work also models agents using probabilistic programs [23, 24]. Our research extends this line of work by explicitly modeling an agent's partial plans, or intentions [25]. This allows our architecture to infer final goals from instrumental subgoals produced as part of a plan, and to account for sub-optimality in those plans, thereby enriching the range of mental inferences that BToM models can explain.
Plan recognition as planning (PRP). Our work is related to the literature on plan recognition as planning, which performs goal and plan inference by using classical satisficing planners to model plan likelihoods given a goal [26, 4, 27, 28, 29, 30]. However, because these approaches use a heuristic likelihood model that assumes goals are always achievable, they are unable to infer likely goals when irreversible failures occur. In contrast, we model agents as online planners who may occasionally execute partial plans that lead to dead ends.
Online goal inference. Several recent papers have extended IRL to an online setting, but these have either focused on maximum-likelihood estimation in 1D state spaces [31, 32], or utilize an expensive value iteration subroutine that is unlikely to scale [33]. In contrast, we develop a sequential Monte Carlo algorithm that exploits the online nature of the agent models in order to perform incremental plan inference with limited computation cost.
Inferences from sub-optimal behavior. We build upon a growing body of research on inferring goals and preferences while accounting for human sub-optimality [3, 24, 34, 35, 36, 37], introducing a model of boundedly-rational planning as resource-limited search. This reflects a natural principle of resource rationality under which agents are less likely to engage in costly computations [38, 39]. Unlike prior models of myopic agents which assign zero reward to future states beyond some time horizon [34, 36], our approach accounts for myopic planning in domains with instrumental subgoals and sparse rewards.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) One realization of our agent and environment model.</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span><span class="w"> </span><span class="k">UPDATE</span><span class="o">-</span><span class="k">PLAN</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">g</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">parameters</span><span class="err">:</span><span class="w"> </span><span class="n">PLANNER</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="o">&gt;</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">LENGTH</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">notin</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="o">[</span><span class="n">t</span><span class="o">]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">eta</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">NEGATIVE</span><span class="err">}</span><span class="o">-</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">BINOMIAL</span><span class="err">}</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">sim</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">PLANNER</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">g</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">gamma</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">eta</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">APPEND</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">p</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="n">model</span>
<span class="w">        </span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="n">Samples</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">P</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">g</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="n">model</span><span class="w"> </span><span class="k">SELECT</span><span class="o">-</span><span class="k">ACTION</span><span class="p">(</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">t</span><span class="o">]</span><span class="err">\</span><span class="nf">left</span><span class="o">[</span><span class="n">s_{t}\right</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="k">end</span><span class="w"> </span><span class="n">model</span>
<span class="w">    </span><span class="p">(</span><span class="n">ii</span><span class="p">)</span><span class="w"> </span><span class="n">Samples</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">P</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">a_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">mid</span><span class="w"> </span><span class="n">s_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">p_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>(b) Boundedly-rational agent programs.</p>
<p>Figure 3: We model agents as boundedly rational planners that interleave search and execution of partial plans as they interact with the environment. In (a) we depict one possible realization of this model, where the agent initially samples a search budget $\eta_{1}$ and searches for a plan $p_{1}$ that is two actions long. At $t=2$, no additional planning needs to be done, so $p_{2}$ is copied from $p_{1}$, as denoted by the dashed lines. The agent then replans at $t=3$ from state $s_{3}$, sampling a new search budget $\eta_{3}$ and an extended plan $p_{3}$ with three more actions. We formally specify this agent model using probabilistic programs, with pseudo-code shown in (b). UPDATE-PLAN samples extended plans $p_{t}$ given previous plans $p_{t-1}$, while SELECT-ACTION selects an action $a_{t}$ according the current plan $p_{t}$.</p>
<h1>3 Boundedly-Rational Planning Agents</h1>
<p>In order to account for sub-optimal behavior due to resource-limited planning, observers need to model not only an agent's goals and actions, but also the plans they form to achieve those goals. As such, we model agents and their environments as generative processes of the following form:</p>
<p>$$
\begin{aligned}
\text { Goal prior: } &amp; g \sim P(g) \
\text { Plan update: } &amp; p_{t} \sim P\left(p_{t} \mid s_{t}, p_{t-1}, g\right) \
\text { Action selection: } &amp; a_{t} \sim P\left(a_{t} \mid s_{t}, p_{t}\right) \
\text { State transition: } &amp; s_{t+1} \sim P\left(s_{t+1} \mid s_{t}, a_{t}\right) \
\text { Observation noise: } &amp; o_{t+1} \sim P\left(o_{t+1} \mid s_{t+1}\right)
\end{aligned}
$$</p>
<p>where $g, p_{t}, a_{t}, s_{t}$ are the agent's goals, the internal state of the agent's plan, the agent's action, and the environment's state at time $t$ respectively. For the purposes of goal inference, observers also assume that each state $s_{t}$ might be subject to observation noise, producing an observed state $o_{t}$.</p>
<p>This generative process, depicted in Figure 3a, extends the standard model of MDP agents by modeling plans and plan updates explicitly, allowing us to represent not only agents that act according to some precomputed policy $a_{t} \sim \pi\left(a_{t} \mid s_{t}\right)$, but also agents that compute and update their plans $p_{t}$ on-the-fly. We describe each component of this process in greater detail below.</p>
<h3>3.1 Modeling Goals, States and Observations</h3>
<p>To represent states, observations, goals, and distributions over goals in a general and flexible manner, our architecture embeds the Planning Domain Definition Language (PDDL) [9, 10], representing states $s_{t}$ and goals $g$ in terms of predicate-based facts, relations, and numeric expressions (Figure 2ii). State transitions $P\left(s_{t} \mid s_{t-1}, a_{t-1}\right)$ are modeled by transition operators that specify the preconditions and effects of actions. While we focus on deterministic transitions in this paper, we also support stochastic transitions, as in Probabilistic PDDL [40]. Given this representation, an observer's prior over goals $P(g)$ can be specified as a probabilistic program over PDDL goal specifications, including numeric reward functions, as well as sets of goal predicates (e.g. has (gem)), equivalent to indicator reward functions. Observation noise $P\left(o_{t+1} \mid s_{t+1}\right)$ can also be modeled by corrupting each Boolean predicate with some probability, and adding continuous (e.g. Gaussian) noise to numeric fluents.</p>
<h1>3.2 Modeling Sub-Optimal Plans and Actions</h1>
<p>To model sub-optimal plans, the basic insight we follow is that agents like ourselves are boundedly rational: we attempt to plan to achieve our goals efficiently, but are limited by our cognitive resources. The primary limitation we consider is that full-horizon planning is often costly or intractable. Instead, it may often make sense to form partial plans towards promising intermediate states, execute them, and replan from there. We model this by assuming that agents only search for a plan up to some budget $\eta$, before executing a partial plan to a promising state found during search. We operationalize $\eta$ as the maximum number of nodes expanded (i.e., states explored), which we treat as a random variable sampled from a negative binomial distribution:</p>
<p>$$
\eta \sim \text { NEGATIVE-BINOMIAL }(r, q)
$$</p>
<p>The parameters $r$ (maximum failure count) and $q$ (continuation probability) characterize the persistence of a planner who may choose to give up after expanding each node. When $r&gt;1$, this distribution peaks at medium values of $\eta$, then decreases exponentially, modeling agents that are unlikely to form extremely long plans, which are costly, or extremely short plans, which are unhelpful.
This model also assumes access to a planning algorithm capable of producing partial plans. While we support any such planner as a sub-component, in this work we focus on A<em> search due to its ability to support domain-general heuristics that can guide search in human-like ways [11, 41]. We also modify A</em> so that search is stochastic, modeling agent sub-optimality during search. In particular, instead of always expanding the most promising successor state, we sample successor $s$ with probability:</p>
<p>$$
P_{\text {expand }}(s) \propto \exp (-f(s, g) / \gamma)
$$</p>
<p>where $\gamma$ is a noise parameter controlling the randomness of search, and $f(s, g)=c(s)+h(s, g)$ is the estimated total plan cost, i.e. the sum of the path cost $c(s)$ so far with the heuristic goal distance $h(s, g)$. On termination, we simply return the most recently selected successor state, which is likely to have low total plan cost $f(s, g)$ if the heuristic $h(s, g)$ is informative and the noise $\gamma$ is low.
We incorporate these limitations into a model of how a boundedly rational planning agent interleaves search and execution, specified by the probabilistic programs UPDATE-PLAN and SELECT-ACTION in Figure 3b. At each time $t$, the agent may reach the end of its last made plan $p_{t-1}$ or encounter a state $s_{t}$ not anticipated by the plan, in which case it will call the base planner (probabilistic A*) with a randomly sampled node budget $\eta$. The partial plan produced is then used to extend the original plan. Otherwise, the agent will simply continue executing its original plan, performing no additional computation. Note that by replanning when the unexpected occurs, the agent automatically handles some amount of stochasticity, as well as errors in its environment model.</p>
<h2>4 Online Bayesian Goal Inference</h2>
<p>Having specified our model, we can now state the problem of Bayesian goal inference. We assume that an observer receives a sequence of potentially noisy state observations $o_{1: t}=\left(o_{1}, \ldots, o_{t}\right)$. Given the observations up to timestep $t$ and a set of possible goals $\mathcal{G}$, the observer's aim is to infer the agent's goal $g \in \mathcal{G}$ by computing the posterior:</p>
<p>$$
P\left(g \mid o_{1: t}\right) \propto P(g) \sum_{\substack{s_{1}: t \ a_{1: t} \ p_{1: t}}} \prod_{\tau=0}^{t-1} P\left(o_{\tau+1} \mid s_{\tau+1}\right) P\left(s_{\tau+1} \mid s_{\tau}, a_{\tau}\right) P\left(a_{\tau} \mid s_{\tau}, p_{\tau}\right) P\left(p_{\tau} \mid s_{\tau}, p_{\tau-1}, g\right)
$$</p>
<p>Computing this posterior exactly is intractable, as it requires marginalizing over all the random latent variables $s_{\tau}, a_{\tau}$, and $p_{\tau}$. Instead, we develop a sequential Monte Carlo procedure, shown in Algorithm 1, to perform approximate inference in an online manner, using samples from the posterior $P\left(g \mid o_{1: t-1}\right)$ at time $t-1$ to inform sampling from the posterior $P\left(g \mid o_{1: t}\right)$ at time $t$. We call this algorithm Sequential Inverse Plan Search (SIPS), because it sequentially inverts a search-based planning algorithm, inferring sequences of partial plans that are likely given the observations, and consequently the likely goals.
As in standard particle filtering schemes, we first sample a set of particles or hypotheses $i \in[1, k]$, with corresponding weights $w_{i}$ (lines 3-5). Each particle corresponds to a particular plan $p_{\tau}^{i}$ and goal $g^{i}$. As each new observation $o_{\tau}$ arrives, we extend the particles (lines 12-14) and reweight them by their likelihood of producing that observation (line 15). The collection of weighted particles thus approximates the full posterior over the unobserved variables in our model, including the agent's plans and goals. We describe several key features of this algorithm below.</p>
<p>Algorithm 1 Sequential Inverse Plan Search (SIPS) for online Bayesian goal inference
procedure $\operatorname{SIPS}\left(s_{0}, o_{1: t},\right)$
2: parameters: $k$, number of particles; $c$, resampling threshold
3: $w^{i} \leftarrow 1$ for $i \in[1, k]$
$\triangleright$ Initialize particle weights
4: $s_{0}^{i}, p_{0}^{i}, a_{0}^{i} \leftarrow s_{0},[]$, no-op for $i \in[1, k]$
$\triangleright$ Initialize states, plans and actions
5: $g^{i} \sim$ GOAL-PRIOR $()$ for $i \in[1, k]$
$\triangleright$ Sample $k$ particles from goal prior
6: for $\tau \in[1, t]$ do
7: if EFFECTIVE-SAMPLE-SIZE $\left(w^{1}, \ldots, w^{k}\right) / k&lt;c$ then
$\triangleright$ Resample and rejuvenate
8: $g^{i}, s_{1: \tau}^{i}, p_{1: \tau}^{i}, a_{1: \tau}^{i} \sim$ RESAMPLE $\left(\left[g^{i}, s_{1: \tau}, p_{1: \tau}, a_{1: \tau}\right]^{1: k}\right)$ for $i \in[1, k]$
9: $g^{i}, s_{1: \tau}^{i}, p_{1: \tau}^{i}, a_{1: \tau}^{i} \sim$ REJUVENATE $\left(g^{i}, o_{1: \tau}, s_{1: \tau}^{i}, p_{1: \tau}^{i}, a_{1: \tau}^{i}\right)$ for $i \in[1, k]$
10: end if
11: for $i \in[1, k]$ do
$\triangleright$ Extend each particle to timestep $\tau$
12: $s_{\tau}^{i} \sim P\left(s_{\tau} \mid s_{\tau-1}^{i}, a_{\tau-1}^{i}\right)$
$\triangleright$ Sample state transition
13: $p_{\tau}^{i} \sim$ UPDATE-PLAN $\left(p_{\tau} \mid s_{\tau}^{i}, p_{\tau-1}^{i}, g^{i}\right)$
$\triangleright$ Extend plan if necessary
14: $a_{\tau}^{i} \sim$ SELECT-ACTION $\left(a_{\tau} \mid s_{\tau}^{i}, p_{\tau}^{i}\right)$
$\triangleright$ Select action
15: $w^{i} \leftarrow w^{i} \cdot P\left(o_{\tau} \mid s_{\tau}^{i}\right)$
$\triangleright$ Update particle weight
16: end for
17: end for
18: $\tilde{w}^{i} \leftarrow w^{i} / \sum_{j=1}^{k} w^{j}$ for $i \in[1, k]$
$\triangleright$ Normalize particle weights
19: return $\left[\left(g^{1}, w^{1}\right), \ldots,\left(g^{k}, w^{k}\right)\right]$
$\triangleright$ Return weighted goal particles
20: end procedure
21:
22: procedure REJUVENATE $\left(g, o_{1: \tau}, s_{1: \tau}, p_{1: \tau}, a_{1: \tau}\right)$
$\triangleright$ Metropolis-Hasting rejuvenation move
23: parameters: $p_{g}$, goal rejuvenation probability
24: if BERNOULLI $\left(p_{g}\right)$ then
$\triangleright$ Heuristic-driven goal proposal
25: $g^{\prime} \sim Q(g):=\operatorname{SOFTMAX}\left(\left[h\left(o_{\tau}, g\right)\right.\right.$ for $\left.g \in \mathcal{G}\right]$ )
$\triangleright$ Propose $g_{0}^{\prime}$ based on est. distance to $o_{\tau}$
26: $s_{1: \tau}^{\prime}, p_{1: \tau}^{i}, a_{1: \tau}^{\prime} \sim P\left(s_{1: \tau}, p_{1: \tau}, a_{1: \tau} \mid g\right)$
$\triangleright$ Sample trajectory under new goal $g$
27: $\alpha \leftarrow Q(g) / Q\left(g^{\prime}\right)$
$\triangleright$ Compute proposal ratio
28: else
29: $\quad t_{<em>} \sim Q\left(t_{</em>} \mid s_{1: \tau}, o_{1: \tau}\right)$
$\triangleright$ Error-driven replanning proposal
30: $\quad s_{t_{<em>} \cdot \tau}^{\prime}, p_{t_{</em>} \cdot \tau}^{\prime}, a_{t_{<em>} \cdot \tau}^{\prime} \sim Q\left(s_{t_{</em>} \cdot \tau}, p_{t_{<em>} \cdot \tau}, a_{t_{</em>} \cdot \tau} \mid o_{t_{<em>} \cdot \tau}\right)$
$\triangleright$ Propose new plan sequence $p_{t_{</em>} \cdot \tau}^{\prime}$
31: $\quad \alpha \leftarrow Q\left(s_{t_{<em>} \cdot \tau}, p_{t_{</em>} \cdot \tau}, a_{t_{<em>} \cdot \tau} \mid o_{t_{</em>} \cdot \tau}\right) / Q\left(s_{t_{<em>} \cdot \tau}^{\prime}, p_{t_{</em>} \cdot \tau}^{\prime}, a_{t_{<em>} \cdot \tau}^{\prime} \mid o_{t_{</em>} \cdot \tau}\right)$
$\triangleright$ Compute proposal ratio
32: $\quad \alpha \leftarrow \alpha \cdot Q\left(t_{<em>} \mid s_{1: \tau}^{\prime}, o_{1: \tau}\right) / Q\left(t_{</em>} \mid s_{1: \tau}, o_{1: \tau}\right)$
$\triangleright$ Reweight by auxiliary proposal ratio
33: end if
34: $\quad \alpha \leftarrow \alpha \cdot P\left(o_{1: \tau} \mid s_{1: \tau}^{\prime}\right) / P\left(o_{1: \tau} \mid s_{1: \tau}\right)$
$\triangleright$ Compute acceptance ratio
35: return $g_{0}^{\prime}, s_{1: \tau}^{\prime}, p_{1: \tau}^{\prime}, a_{1: \tau}^{\prime}$ if BERNOULLI $(\min (\alpha, 1))$ else $g_{0}, s_{1: \tau}, p_{1: \tau}, a_{1: \tau}$
36: end procedure
$\triangleright$ Accept or reject proposals</p>
<h1>4.1 Online Extension of Hypothesized Partial Plans</h1>
<p>A key aspect that makes SIPS a genuinely online algorithm is the modeling assumption that agents also plan online. This obviates the need for the observer to precompute a complete plan or policy for each of the agent's possible goals in advance, and instead defers such computation to the point where the agent reaches a time $t$ that the observer's hypothesized plans do not yet reach. In particular, for each particle $i$, the corresponding plan hypothesis $p_{t-1}^{i}$ is extended (Algorithm 1, line 13) by running the UPDATE-PLAN procedure in Figure 3b.i, which only performs additional computation if $p_{t-1}^{i}$ does not already contain a planned action for time $t$ and state $s_{t}$. This means that at any given time $t$, only a small number of plans require extension, limiting the number of expensive planning calls.</p>
<h3>4.2 Managing Hypothesis Diversity via Resampling and Rejuvenation</h3>
<p>We also introduce resampling and rejuvenation steps into SIPS in order to ensure particle diversity. Whenever the effective sample size falls below a threshold $c$ (line 7), we resample the particles (line 8), thereby pruning low-weight hypotheses. We then rejuvenate by applying a mixture of two data-driven Metropolis-Hastings kernels to each particle. The first kernel uses a heuristic-driven goal proposal (lines 25-27), proposing goals $\tilde{g} \in \mathcal{G}$ which are close in heuristic distance $h\left(o_{\tau}, \tilde{g}\right)$ to the last observed state $o_{\tau}$. This allows SIPS to reintroduce goals that were pruned, but later become more likely. The second kernel uses an error-driven replanning proposal (lines 29-32), which samples a time close to the divergence point between the hypothesized and observed trajectories, and then proposes to replan from that time, thereby constructing a new sequence of hypothesized partial plans that are less likely to diverge from the observations. Despite the complexity of these proposals, acceptance ratios are automatically calculated via Gen's support for involutive kernels [8]. Collectively, these steps help to ensure that hypotheses are both diverse and likely given the observations.</p>
<h1>5 Experiments</h1>
<p>We conducted several sets of experiments that demonstrate the human-likeness, accuracy, speed, and robustness of our approach. We first present experiments demonstrating the novel capacity of SIPS to infer goals from sub-optimal trajectories involving backtracking and failure (Figure 1). Comparing these inferences against human goal inferences shows that SIPS is more human-like than baseline approaches (Figure 4). We also evaluate the accuracy and speed of SIPS on a variety of planning domains (Table 1a), showing that it outperforms Bayesian IRL baselines. Finally, we present robustness experiments showing that SIPS can infer goals even when the data-generating model differs from the model assumed by the algorithm (Table 1b).</p>
<h3>5.1 Domains</h3>
<p>We validate our approach on domains with varying degrees of complexity, both in terms of the size of the state space $|\mathcal{S}|$ and the number of possible goals $|\mathcal{G}|$. All domains are characterized by compositional structure and sparse rewards, posing a challenge for standard MDP-based approaches.
Taxi $(|\mathcal{G}|=3,|\mathcal{S}|=125)$ : A benchmark domain used in hierarchical reinforcement learning [42], where a taxi has to transport a passenger from one location to another in a gridworld.
Doors, Keys, \&amp; Gems $(|\mathcal{G}|=3,|\mathcal{S}| \sim 10^{5})$ : A domain in which an agent must navigate a maze with doors, keys, and gems (Figure 1). Each key can be used once to unlock a door, allowing the agent to acquire items behind that door. Goals correspond to acquiring one out of three colored gems.
Block Words $(|\mathcal{G}|=5,|\mathcal{S}| \sim 10^{5})$ : A Blocks World variant adapted from [4] where blocks are labeled with letters. Goals correspond to block towers that spell one of a set of five English words.
Intrusion Detection $(|\mathcal{G}|=20,|\mathcal{S}| \sim 10^{30})$ : A cybersecurity-inspired domain drawn from [4], where an agent might perform a variety of attacks on a set of servers. There are 20 possible goals, each corresponding to a set of attacks (e.g. cyber-vandalism or data-theft) on up to 10 servers.</p>
<h3>5.2 Baselines</h3>
<p>We implemented Bayesian IRL (BIRL) baselines by running value iteration to compute a Boltzmanrational policy $\pi\left(a_{t} \mid s_{t}, g\right)$ for each possible goal $g \in \mathcal{G}$. Following the setting of early Bayesian theory-of-mind approaches [18], we treated goals as indicator reward functions, and assumed a uniform prior $P(g)$ over goals. Inference was then performed by exact computation of the posterior over reward functions, using the policy as the likelihood for observed actions. Unless otherwise stated, we used a discount factor of 0.9 , and Boltzmann noise parameter $\alpha=1$.
Due to the exponentially large state space of many of our domains, standard value iteration (VI) often failed to converge even after $10^{6}$ iterations. As such, we implemented two variants of BIRL that use asynchronous VI, sampling states instead of fully enumerating them. The first, unbiased BIRL, uses uniform random sampling of the state space up to 250,000 iterations, sufficient for convergence in the Block Words domain. The second, oracle BIRL, assumes oracular access to the full set of observed trajectories in advance, and performing biased sampling of states that appear in those trajectories. Although inapplicable in practice for online use, this ensures that the computed policy is able to reach the goal in all cases, making it a useful benchmark for comparison.</p>
<h3>5.3 Human-Like Goal Inference from Sub-optimal and Failed Plans</h3>
<p>To investigate the novel human-like capabilities of our approach, we performed a set of qualitative experiments on a set of trajectories designed to exhibit notable sub-optimality or failure. The experiments were performed on the Doors, Keys \&amp; Gems domain because it allows for irreversible failures. Two illustrative examples are shown in Figure 1, and more are provided in the supplement. In Figure 1a, SIPS accurately infers goals from a sub-optimal plan with backtracking, initially placing more posterior mass on the yellow gem when the agent acquires the first key (panel ii), but then switching to the blue gem once the agent backtracks to the second key (panel iv). In Figure 1b, SIPS remains uncertain about all three goals when the first key is acquired (panel ii), but discards the red gem as a possibility when the agent walks past the door (panel iii), and finally converges upon the blue gem when the agent myopically unlocks the first door required to access that gem (panel iv).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Average human goal inferences over time ( $\pm 1$ std.) for the sub-optimal trajectory in Figure 1a, compared to (b) inferences made by SIPS and (c) oracle BIRL. We omit unbiased BIRL because unbiased VI fails to converge for this domain, producing a flat posterior. In (d) we show a scatterplot of mean human inferences against algorithm inferences across all trajectories.</p>
<p>In addition to posterior convergence, the inferences made by SIPS display human-like changes at key timepoints. We quantified this human-likeness by collecting human goal inferences on ten trajectories (six sub-optimal or failed) in a pilot study with $N=8$ subjects. Human inferences were collected every six timesteps, and a comparison against SIPS and the oracle BIRL baseline is shown in Figure 4. For the trajectory in in Figure 1a, human inferences (Figure 4a) display extremely similar qualitative trends as SIPS (Figure 4b, $r=0.99$ ). Oracle BIRL correlates less well (Figure 4c, $r=0.80$ ), assigning high probability to the yellow gem even after the agent backtracks at $t \geq 18$. This is because Boltzmann action noise assigns significant likelihood to the undoing of past actions. As Figure 4d shows, SIPS also correlates more strongly with mean human inferences across the dataset. Inferences made SIPS (yellow) hew closely to the diagonal, achieving a correlation of $r=0.89$, indicating that the agent model assumed by SIPS is similar to humans' theory-of-mind. In contrast, inferences made by BIRL (blue) are much more diffuse, achieving a correlation of only $r=0.51$.</p>
<h1>5.4 Accuracy, Speed and Robustness of Inference</h1>
<p>To evaluate accuracy and speed, we ran each inference method on a dataset of optimal and nonoptimal agent trajectories for each domain, assuming a uniform prior over goals. The optimal trajectories were generated using A* search with an admissible heuristic for each possible goal in the domain. Non-optimal trajectories were generated using the replanning agent model in Figure 3b, with parameters $r=2, q=0.95, \gamma=0.1$. We found that with matched model parameters, SIPS achieved good performance with 10 particles per goal without the use of rejuvenation moves, so we report those results here. Further experimental details and parameters can be found in the supplement.
We summarize the results of these experiments in Table 1a, with additional results in the supplement. Our method greatly outperforms the unbiased BIRL baseline in both accuracy and speed in three out of four domains, with an average runtime (AC) often several orders of magnitude smaller. This is largely because unbiased VI fails to converge except for the highly restricted Taxi domain. In contrast, SIPS requires far less initial computation, albeit with higher marginal cost due its online generation of partial plans. In fact, it achieves comparable accuracy and speed to the oracle BIRL baseline, sometimes with less computation (e.g. in Doors, Keys \&amp; Gems). SIPS also produces higher estimates of the goal posterior $P\left(g_{\text {true }} \mid o\right)$. This is a reflection of the underlying agent model, which assumes randomness at the level of planning instead of acting. As a result, even a few observations can provide substantial evidence that a particular plan and goal was chosen.
Given the specific assumptions made by our agent model, a reasonable question is whether inference is robust to plans generated by other agent models or actual humans. To address this, we also performed a series of robustness experiments for two domains (Table 1b) on data generated by mismatched model parameters $r, q, \gamma$, mismatched planning heuristics $h$, Boltzmann-rational RL agents, optimal agents, and 5 pilot human subjects ( 30 trajectories per subject).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Runtime</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">$P\left(g_{\text {true }} \mid o\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\left|\log _{2} \cdot\right|$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{C}_{0}$ (s)</td>
<td style="text-align: center;">MC (s)</td>
<td style="text-align: center;">AC (s)</td>
<td style="text-align: center;">N</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Taxi <br> (3 Goals)</td>
<td style="text-align: center;">SIPS (ours)</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">1.80</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">1429</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (unbiased)</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">2.22</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (oracle)</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">1.63</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">2500</td>
</tr>
<tr>
<td style="text-align: center;">Doors, Keys \&amp; Gems (3 Goals)</td>
<td style="text-align: center;">SIPS (ours)</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">3.30</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">2099</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (unbiased)</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">3326</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">154</td>
<td style="text-align: center;">250000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (oracle)</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">7.01</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;">Block Words (5 Goals)</td>
<td style="text-align: center;">SIPS (ours)</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">2.46</td>
<td style="text-align: center;">4.15</td>
<td style="text-align: center;">2506</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (unbiased)</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">687</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">250000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (oracle)</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">10000</td>
</tr>
<tr>
<td style="text-align: center;">Intrusion Detection (20 Goals)</td>
<td style="text-align: center;">SIPS (ours)</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">375</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">13321</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (unbiased)</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">18038</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">1069</td>
<td style="text-align: center;">250000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL (oracle)</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">10000</td>
</tr>
</tbody>
</table>
<p>(a) Accuracy and runtime of goal inference across domains and inference methods. We quantify accuracy at the 1st, 2nd and 3rd quartiles (Q1-Q3) of each observed trajectory via the posterior probability of the true goal $P\left(g_{\text {true }} \mid o\right)$, and the fraction of problems where $g_{\text {true }}$ is top-ranked (Top-1). We measure runtime in terms of the start-up cost $\left(\mathrm{C}_{0}\right)$, marginal cost per timestep (MC), and average cost per timestep (AC) in seconds. We also report the total number of states visited ( N ) during either search or value iteration as a platform-independent measure. Excluding the oracle baseline, the best metrics are bolded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Persistence $(r)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Persistence $(q)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RL <br> $v r=50$</th>
<th style="text-align: center;">Optimal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$2^{*}$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">$0.95^{*}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Doors, Keys, Gems</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">Block Words</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Search Noise. $(\gamma)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Heuristic $(h)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Humans</td>
</tr>
<tr>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$0.1^{*}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">Mh.*</td>
<td style="text-align: center;">Mz.</td>
<td style="text-align: center;">GC.</td>
<td style="text-align: center;">$h_{\text {add }}{ }^{*}$</td>
<td style="text-align: center;">$n=5$</td>
</tr>
<tr>
<td style="text-align: center;">Doors, Keys, Gems</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;">Block Words</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.73</td>
</tr>
</tbody>
</table>
<p>(b) Robustness to model mismatch. Top-1 accuracy of SIPS at the third time quartile (Q3), evaluated on data generated by mismatched parameters, Boltzmann-rational RL agents, optimal agents, and humans. We ran SIPS assuming $r=2, q=0.95, T=10$. For Doors, Keys, Gems, we assumed a Manhattan (Mh.) heuristic against a maze distance (Mz.) heuristic. For Block Words, we assumed $h_{\text {add }}$ against the naive goal count (GC.) heuristic. Matched parameters are starred $\left({ }^{*}\right)$.</p>
<p>Table 1: Accuracy, runtime, and robustness of inference.</p>
<p>As Table 1b shows, SIPS is relatively robust to data generated by these other models and parameters. Although performance can degrade with mismatch, this is partly due to the difficulty of inference from highly random behavior (e.g. $q=0.8, h=\mathrm{GC}$.). On the other hand, when mismatched parameters are more optimal, performance can improve (e.g. $h=\mathrm{Mz}$.). Importantly, SIPS also does well on human data, showing robustness even when the planner is unknown. While our boundedly rational agent model cannot possibly capture all aspects of human planning, these experiments suggest that it is serves as a reasonable approximation, similar to our intuitive theories of other people's minds.</p>
<h1>6 Limitations and Future Work</h1>
<p>In this paper, we demonstrated an architecture capable of online inference of goals and plans, even when those plans might fail. However, important limitations remain. First, we considered only finite sets of goals, but the space of goals that humans pursue is easily infinite. Relatedly, we assume that these goals are final, instead of accounting for the hierarchical and instrumental nature of goals and plans. A promising next step would thus be to express hierarchies of goals and plans as probabilistic grammars or programs [43, 44, 45], capturing both the infinitude and structure of the motives we attribute to each other [46, 47]. Second, unlike the domains considered here, the environments we operate in often involve stochastic dynamics and infinite action spaces [48, 49]. A natural extension would be to integrate Monte Carlo Tree Search or sample-based motion planners into our architecture as modeling components [23], potentially parameterized by learned heuristics [50]. With hope, our architecture might then approach the full complexity of problems that we face everyday, whether one is stacking blocks as a kid, finding the right keys for the right doors, or writing a research paper.</p>
<h1>7 Broader Impact</h1>
<p>We embarked upon this research in the belief that, as increasingly powerful autonomous systems become embedded in our society, it may eventually become necessary for them to accurately understand our goals and values, so as to robustly act in our collective interest. Crucially, this will require such systems to understand the ways in which humans routinely fail to achieve our goals, and not take that as evidence that those goals were never desired. Due to our manifold cognitive limitations, gaps emerge between our goals and our intentions, our intentions and our actions, our beliefs and our conclusions, and our ideals and our practices. To the extent that we would like machines to aid us in actualizing the goals and ideals we most value, rather than those we appear to be acting towards, it will be critical for them to understand how, when, and why those gaps emerge. This aspect of the value alignment problem has thus far been under-explored [51]. By performing this research at the intersection of cognitive science and AI, we hope to lay some of the conceptual and technical groundwork that may be necessary to understand our boundedly-rational behavior.
Of course, the ability to infer the goals of others, and to do so online and despite failures, has many more immediate uses, each of them with its own set of benefits and risks. Perhaps the most straightforwardly beneficial are assistive use cases, such as smart user interfaces [52], intelligent personal assistants, and collaborative robots, which may offer to aid a user if that user appears to be pursuing a sub-optimal plan. However, even those use cases come with the risk of reducing human autonomy, and care should be taken so that such applications ensure the autonomy and willing consent of those being aided [53].
More concerning however is the potential for such technology to be abused for manipulative, offensive, or surveillance purposes. While the research presented in this paper is nowhere near the level of integration that would be necessary for active surveillance or manipulation, it is highly likely that mature versions of similar technology will be co-opted for such purposes by governments, militaries, and the security industry [54, 55]. Although detecting and inferring "suspicious intent" may not seem harmful in its own right, these uses need to be considered within the broader context of society, especially the ways in which marginalized peoples are over-policed and incarcerated [56]. Given these risks, we urge future research on this topic to consider seriously the ways in which technology of this sort will most likely be used, by which institutions, and whether those uses will tend to lead to just and beneficial outcomes for society as a whole. The ability to infer and understand the motives of others is a skill that can be wielded to both great benefit and great harm. We ought to use it wisely.</p>
<h2>8 Code Availability</h2>
<p>Code for the architecture and experiments presented in this paper is available at https://github. com/ztangent/Plinf.jl/tree/neurips-2020-experiments, as part of the Plinf.jl package for Bayesian inverse planning.</p>
<h2>9 Acknowledgements</h2>
<p>This work was funded in part by the DARPA Machine Common Sense program (Award ID: 03052300001); philanthropic gifts from the Aphorism Foundation and from the Siegel Family Foundation; and financial support from the MIT-IBM Watson AI Lab and the Intel Probabilistic Computing Center. Tom Silver is supported by an NSF Graduate Research Fellowship.</p>
<h2>References</h2>
<p>[1] Felix Warneken and Michael Tomasello. Altruistic helping in human infants and young chimpanzees. Science, 311(5765):1301-1303, 2006.
[2] Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 663-670, 2000 .
[3] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.</p>
<p>[4] Miguel RamÃ­rez and Hector Geffner. Probabilistic plan recognition using off-the-shelf classical planners. In Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.
[5] Bernard Michini and Jonathan P How. Improving the efficiency of bayesian inverse reinforcement learning. In 2012 IEEE International Conference on Robotics and Automation, pages 3651-3656. IEEE, 2012.
[6] Marco F Cusumano-Towner, Feras A Saad, Alexander K Lew, and Vikash K Mansinghka. Gen: a general-purpose probabilistic programming system with programmable inference. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 221-236. ACM, 2019.
[7] Marco F Cusumano-Towner and Vikash K Mansinghka. Using probabilistic programs as proposals. arXiv preprint arXiv:1801.03612, 2018.
[8] Marco Cusumano-Towner, Alexander K Lew, and Vikash K Mansinghka. Automating involutive mcmc using probabilistic and differentiable programming. arXiv preprint arXiv:2007.09871, 2020.
[9] Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. PDDL - the Planning Domain Definition Language, 1998.
[10] Maria Fox and Derek Long. PDDL2. 1: An extension to PDDL for expressing temporal planning domains. Journal of artificial intelligence research, 20:61-124, 2003.
[11] Blai Bonet and HÃ©ctor Geffner. Planning as heuristic search. Artificial Intelligence. 2001 Jun; 129 (1-2): 5-33., 2001.
[12] Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7, pages 2586-2591, 2007.
[13] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004.
[14] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement learning. In Advances in neural information processing systems, pages 39093917, 2016.
[15] Daniel S Brown and Scott Niekum. Deep bayesian reward learning from preferences. arXiv preprint arXiv:1912.04472, 2019.
[16] Noah D Goodman, Chris L Baker, Elizabeth Baraff Bonawitz, Vikash K Mansinghka, Alison Gopnik, Henry Wellman, Laura Schulz, and Joshua B Tenenbaum. Intuitive theories of mind: A rational approach to false belief. In Proceedings of the twenty-eighth annual conference of the cognitive science society, volume 6. Cognitive Science Society Vancouver, 2006.
[17] Chris L Baker, Joshua B Tenenbaum, and Rebecca R Saxe. Goal inference as inverse planning. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 29, 2007.
[18] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. Action understanding as inverse planning. Cognition, 113(3):329-349, 2009.
[19] Chris Baker, Rebecca Saxe, and Joshua Tenenbaum. Bayesian theory of mind: Modeling joint belief-desire attribution. In Proceedings of the Annual Meeting of the Cognitive Science Society, 33 (33), 2011.
[20] Julian Jara-Ettinger, Hyowon Gweon, Laura E Schulz, and Joshua B Tenenbaum. The naÃ¯ve utility calculus: Computational principles underlying commonsense psychology. Trends in cognitive sciences, 20(8):589-604, 2016.
[21] Chris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. Rational quantitative attribution of beliefs, desires and percepts in human mentalizing. Nature Human Behaviour, $1(4): 1-10,2017$.
[22] Julian Jara-Ettinger, Laura Schulz, and Josh Tenenbaum. The naive utility calculus as a unified, quantitative framework for action understanding. PsyArXiv, 2019.
[23] Marco F Cusumano-Towner, Alexey Radul, David Wingate, and Vikash K Mansinghka. Probabilistic programs for inferring the goals of autonomous agents. arXiv preprint arXiv:1704.04977, 2017.
[24] Iris R Seaman, Jan-Willem van de Meent, and David Wingate. Nested reasoning about autonomous agents using probabilistic programs. arXiv, pages arXiv-1812, 2018.</p>
<p>[25] Michael Bratman. Intention, plans, and practical reason, volume 10. Harvard University Press Cambridge, MA, 1987.
[26] Miquel RamÃ­rez and Hector Geffner. Plan recognition as planning. In Twenty-First International Joint Conference on Artificial Intelligence, 2009.
[27] Shirin Sohrabi, Anton V Riabov, and Octavian Udrea. Plan recognition as planning revisited. In IJCAI, pages 3258-3264, 2016.
[28] Daniel HÃ¶ller, Gregor Behnke, Pascal Bercher, and Susanne Biundo. Plan and goal recognition as htn planning. In 2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI), pages 466-473. IEEE, 2018.
[29] Gal A Kaminka, Mor Vered, and Noa Agmon. Plan recognition in continuous domains. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[30] Mor Vered, Ramon Fraga Pereira, Mauricio Cecilio Magnaguagno, Felipe Meneguzzi, and Gal A Kaminka. Online goal recognition as reasoning over landmarks. In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[31] Pratiksha Thaker, Joshua B Tenenbaum, and Samuel J Gershman. Online learning of symbolic concepts. Journal of Mathematical Psychology, 77:10-20, 2017.
[32] Ryan Self, Michael Harlan, and Rushikesh Kamalapurkar. Online inverse reinforcement learning for nonlinear systems. In 2019 IEEE Conference on Control Technology and Applications (CCTA), pages 296-301. IEEE, 2019.
[33] Nicholas Rhinehart and Kris Kitani. First-person activity forecasting from video with online inverse reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 2018.
[34] Owain Evans and Noah D Goodman. Learning the preferences of bounded agents. In NIPS Workshop on Bounded Optimality, volume 6, 2015.
[35] Owain Evans, Andreas StuhlmÃ¼ller, and Noah Goodman. Learning the preferences of ignorant, inconsistent agents. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
[36] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca D Dragan. On the feasibility of learning, rather than assuming, human biases for reward inference. arXiv preprint arXiv:1906.09624, 2019.
[37] Stuart Armstrong and SÃ¶ren Mindermann. Occam's razor is insufficient to infer the preferences of irrational agents. In Advances in Neural Information Processing Systems, pages 5598-5609, 2018.
[38] Thomas L Griffiths, Falk Lieder, and Noah D Goodman. Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic. Topics in cognitive science, 7(2):217-229, 2015.
[39] Mark K Ho, David Abel, Jonathan D Cohen, Michael L Littman, and Thomas L Griffiths. The efficiency of human cognition reflects planned information processing. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.
[40] HÃ¥kan LS Younes and Michael L Littman. Ppddl1. 0: An extension to pddl for expressing planning domains with probabilistic effects. Techn. Rep. CMU-CS-04-162, 2:99, 2004.
[41] Hector Geffner. Heuristics, planning and cognition. Heuristics, Probability and Causality. A Tribute to Judea Pearl. College Publications, 2010.
[42] Thomas G Dietterich. The maxq method for hierarchical reinforcement learning. In ICML, volume 98, pages 118-126. Citeseer, 1998.
[43] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
[44] Feras A Saad, Marco F Cusumano-Towner, Ulrich Schaecthle, Martin C Rinard, and Vikash K Mansinghka. Bayesian synthesis of probabilistic programs for automatic data modeling. Proceedings of the ACM on Programming Languages, 3(POPL):1-32, 2019.
[45] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv preprint arXiv:2006.08381, 2020.</p>
<p>[46] Chris Cundy and Daniel Filan. Exploring hierarchy-aware inverse reinforcement learning. arXiv preprint arXiv:1807.05037, 2018.
[47] Leslie Pack Kaelbling and TomÃ¡s Lozano-PÃ©rez. Hierarchical task and motion planning in the now. In 2011 IEEE International Conference on Robotics and Automation, pages 1470-1477. IEEE, 2011.
[48] Caelan Reed Garrett, TomÃ¡s Lozano-PÃ©rez, and Leslie Pack Kaelbling. Strips planning in infinite domains. arXiv preprint arXiv:1701.00287, 2017.
[49] Caelan Reed Garrett, TomÃ¡s Lozano-PÃ©rez, and Leslie Pack Kaelbling. PDDLStream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning. In Proceedings of the International Conference on Automated Planning and Scheduling, volume 30, pages $440-448,2020$.
[50] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
[51] Stuart Russell. Human compatible: Artificial intelligence and the problem of control. Penguin, 2019.
[52] Eric J Horvitz, John S Breese, David Heckerman, David Hovel, and Koos Rommelse. The lumiere project: Bayesian user modeling for inferring the goals and needs of software users. arXiv preprint arXiv:1301.7385, 2013.
[53] Vasanth Sarathy, Thomas Arnold, and Matthias Scheutz. When exceptions are the norm: Exploring the role of consent in hri. ACM Transactions on Human-Robot Interaction (THRI), $8(3): 1-21,2019$.
[54] Shoshana Zuboff. Big other: surveillance capitalism and the prospects of an information civilization. Journal of Information Technology, 30(1):75-89, 2015.
[55] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artificial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018.
[56] Andrew Guthrie Ferguson. The rise of big data policing: Surveillance, race, and the future of law enforcement. NYU Press, 2019.</p>
<h1>Supplemental Material <br> Online Bayesian Goal Inference for Boundedly-Rational Planning Agents</h1>
<p>Tan Zhi-Xuan, Jordyn L. Mann, Tom Silver Joshua B. Tenenbaum, Vikash K. Mansinghka<br>Massachusetts Institute of Technology<br>{xuan, jordynm,tslvr,jbt,vkm}@mit.edu</p>
<h2>A Experimental Details</h2>
<p>Below we provide experimental details for each of the inference methods described in the main text. We have also performed additional experiments using a baseline adapted from the plan recognition as planning (PRP) literature [1], which we include below as a useful offline benchmark.</p>
<h2>A. 1 Sequential Inverse Plan Search</h2>
<p>We conducted experiments using two main variants of Sequential Inverse Plan Search (SIPS), the first using data-driven rejuvenation, as described in the main text, and the second without. Rejuvenation is necessary for the results shown in Figure 1 of the main text, and for highly sub-optimal and failed plans more generally. However, rejuvenation is also hard to tune, and can increase runtime due to the need to replan. We thus report results without rejuvenation in our quantitative experiments.
Parameters for qualitative experiments are given in each of the corresponding figures in section B.1. For the quantitative experiments, we used SIPS with 10 particles per possible goal (e.g., 50 particles for the Block Words domain), with a resampling threshold of $c=1 / 4$. For the underlying agent model, we assumed search noise of $\gamma=0.1$ and persistence parameters of $r=2$ and $q=0.95$ (giving an average search budget of 38 nodes). We varied the search heuristic $h$ to suit the type of domain: For the gridworld-based domains (Taxi; Doors, Keys \&amp; Gems), we used a Manhattan distance heuristic to the goal. For the other domains (Block Words; Intrusion Detection), we used the $h_{\text {add }}$ heuristic introduced by the HSP algorithm [2] as a generalized relaxed-distance heuristic.
SIPS also requires the specification of an observation model $P(o \mid s)$, in order to score the likelihood of a hypothesized state trajectory $\hat{s}<em t="t">{1}, \ldots, \hat{s}</em>$. We defined this observation model by adding zero-mean Gaussian noise with $\sigma=0.25$ for each numeric variable in the state (e.g., the agent's position in a gridworld), and Bernoulli corruption noise with $p=0.05$ for each Boolean variable in the state (e.g. whether block A is on top of block B).
All SIPS experiments were performed using Plinf. jl, a Julia implementation of our modeling and inference architecture that integrates the Gen probabilistic programming system with PDDL.jl, a Julia interpreter for the Planning Domain Definition Language [3]. Experiments were run on a 1.9 GHz Intel Core i7 processor with 16 GB RAM.}$ given the observed states $o_{1}, \ldots, o_{t</p>
<h2>A. 2 Bayesian Inverse Reinforcement Learning</h2>
<p>Bayesian Inverse Reinforcement Learning (BIRL) requires computing an approximate value function $Q(s, a)$ offline and a posterior over goals online using the likelihood $P(a \mid s, g)=\frac{1}{Z} e^{\alpha \cdot Q(s, a)}$, where $Z$ is the partition function and $\alpha$ is an optimality parameter. For the quantitative experiments, we used $\alpha=1$ which we found to perform well in preliminary trials. For qualitative comparisons, however, we used $\alpha=5$, as this choice produced results in range more similar to human inferences. To approximate the value function, we used value iteration (VI) with a discount factor of 0.9 .</p>
<p>As discussed in the main text, several of the domains considered in this work have state spaces that are too large to enumerate, making standard VI intractable. We therefore used asynchronous VI, sampling states instead of fully enumerating them, for 250,000 iterations for the unbiased baseline (BIRL-U). Preliminary experiments suggested that running for up to 1,000,000 iterations did not appreciably improve results. Taxi, which has a far smaller state space than the other domains, was run with 10,000 iterations, which was consistently sufficient for convergence. For the oracle baseline (BIRL-O), 2500 iterations were sufficient to reach convergence for the Taxi domain, and 10,000 iterations for the other domains.</p>
<p>All BIRL experiments were written in Python and run on a 2.9 GHz Intel Core i9 processor with 32 GB RAM. We made use of the PDDLGym library [4] for instantiating the PDDL planning problems as OpenAI Gym environments. To perform asynchronous VI efficiently, we implemented state samplers and valid action generators for each domain. The unbiased version of BIRL (BIRL-U) uses these state samplers to sample states within asynchronous VI. For the oracle baseline (BIRL-O), which has access to the test-time trajectories, we instead sampled one state uniformly at random from the states visited across all test-time trajectories.</p>
<h1>A. 3 Plan Recognition as Planning</h1>
<p>We adapted the plan recognition as planning (PRP) approach described in [1] as an offline benchmark that achieves high accuracy at the cost of substantially more runtime (up to 30 times) than SIPS. In the PRP approach, we use a heuristic approximation to the likelihood of a plan $p$ given a goal $g$ :</p>
<p>$$
P(p \mid g) \propto e^{-\beta\left(\left|p\left|-\left|p_{t}^{*}\right|\right.\right.}
$$</p>
<p>where $p_{t}^{g}$ is an optimal plan to the goal $g,|p|$ denotes the length of the plan $p$, and $\beta$ is a noise parameter. This likelihood function model agent rationality by placing exponentially less probability on costlier plans, where larger values of $\beta$ correspond to more optimality.
In order to perform inference using this likelihood model, we first compute the optimal plan $p_{t}^{g}$ for each possible goal $g$ in a domain. At each timestep $t$, we then construct a plan $p_{t}^{g}$ to each goal $g$ consistent with the observations so far, by computing an optimal partial plan $p_{t}^{+}$from the current observed state $o_{t}$ to $g$, and then concatenating it with the initial sequence of actions $p_{t}^{-}:=a_{1}, \ldots, a_{t-1}$ taken by the agent, giving $p_{t}^{g}=\left[p_{t}^{-}, p_{t}^{+}\right]$. Under the additional approximation that $p_{t}^{g}$ is the only plan consistent with the observation sequence $o_{1}, \ldots, o_{t}$, we can then compute the goal posterior as</p>
<p>$$
P\left(g \mid o_{1}, \ldots, o_{t}\right) \simeq \frac{e^{-\beta\left(\left|p_{t}^{g}\left|-\left|p_{t}^{*}\right|\right.\right)}}{\sum_{g^{\prime} \in \mathcal{G}} e^{-\beta\left(\left|p_{t}^{g^{\prime}}\left|-\left|p_{t}^{g^{\prime}}\right|\right.\right.}}
$$</p>
<p>The main limitation of this approach is that it requires computation of an optimal partial plan $p_{t}^{+}$for every goal $g$ at every timestep $t$, which scales poorly with the number of goals and timesteps per trajectory, especially when the observed trajectory leads the agent further and further away from most of the goals under consideration. This is contrast to SIPS, which performs incremental computation by extending partial plans from previous timesteps. In addition, due to the assumption that there always exists a plan from the current observed state $o_{t}$ to every goal $g$, the PRP approach is unable to account for irreversible failures. This is shown in our qualitative comparisons.
Nonetheless, because PRP still achieves high accuracy on many sub-optimal trajectories (at the expense of considerably more computation, especially on domains with many goals), we include it here as a benchmark for accuracy. All PRP experiments were performed on the same machine as the SIPS experiments, using the implementation of A* search provided by Plinf.jl.</p>
<h2>B Additional Results</h2>
<h2>B. 1 Qualitative Comparisons for Sub-Optimal \&amp; Failed Plans</h2>
<p>Here we present detailed qualitative comparisons of the goal inferences made for sub-optimal and failed plans in the Doors, Keys \&amp; Gems domain. Figures S1 and S2 show the inferences made for two sub-optimal trajectories, while Figures S3 and S4 show the inferences made for two trajectories with irreversible failures. We omit the unbiased Bayesian IRL baseline (BIRL-U), because it is unable to solve the underlying Markov Decision Process in any of these examples, leading to a uniform posterior over goals over the entire trajectory.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure S1: Goal inferences made by SIPS, BIRL-O, and PRP for the sub-optimal trajectory shown in Figure 1(a) of the main text. Predicted future trajectories in panels (i)-(iv) are made by SIPS. For SIPS, we used 30 particles per goal, search noise $\gamma=0.1$, persistence parameters $r=2$, $q=0.95$, and a Manhattan distance heuristic to the goal. Rejuvenation moves were used, with a goal rejuvenation probability of $p_{g}=0.25$. For BIRL-O, we used $\alpha=5$. For PRP, we used $\beta=1$.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure S2: Goal inferences made by SIPS, BIRL-O, and PRP for another sub-optimal trajectory. Predicted future trajectories in panels (i)-(iv) are made by SIPS. For SIPS, we used 30 particles per goal, search noise $\gamma=0.1$, persistence parameters $r=2, q=0.95$, and a Manhattan distance heuristic to the goal. Rejuvenation moves were used, with a goal rejuvenation probability of $p_{g}=0.25$. For BIRL-O, we used $\alpha=5$. For PRP, we used $\beta=1$.</p>
<h1>B.1.1 Sub-Optimal Plans</h1>
<p>Figure S1 shows how the inferences produced by SIPS are more human-like, compared to the BIRL and PRP baselines. In particular, SIPS adjusts its inferences in a human-like manner, initially remaining uncertain between the 3 gems (panel i), placing more posterior mass on the yellow gem when the agent acquires the first key (panel ii), increasing that posterior mass when agent appears to ignore the second key and unlock the first door (panel iii), but then switching to the blue gem once the agent backtracks towards the second key (panel iv).
While the inferences produced by BIRL display similar trends, they are much more gradual, because BIRL assumes noise at the level of acting instead of planning. In addition, the agent model underlying BIRL leads to strange artifacts, such as the rise in probability of the red gem when $t&lt;9$. This is because Boltzmann action noise places lower probability $P(a \mid g)$ on an action $a$ that leads to a goal $g$ which is further away, due to the value function $V_{g}$ associated with that goal $g$ being smaller due to time discounting. As a result, when $t&lt;9$, BIRL computes that $P(\operatorname{right} \mid \mathrm{red})&gt;P(\text { right } \mid \text { yellow })$ and $P($ right $\mid$ blue $)$, leading to the red gem being inferred as the most likely goal.
Finally, PRP exhibits both over-confidence in the yellow gem and slow recovery towards the blue gem. This is due to the assumption that the likelihood of a plan $p$ to some goal $g$ is exponentially decreasing in its cost difference from the optimal plan $p_{<em>}^{g}$. Between $t=10$ and $t=20$, all plans consistent with the observations to the blue gem are considerably longer than the optimal plan $p_{</em>}^{\text {blue }}$. As a result, PRP gives very low probability to the blue gem. This effect continues for many timesteps after the agent starts to backtrack ( $t=17$ to $t=24$ ), indicating that the PRP modeling assumptions are inadequate for plans with substantial backtracking.
Similar dynamics can be observed for the trajectory in Figure S2. The BIRL baseline performs especially poorly, placing high probability on the yellow gem even when the agent backtracks to collect the second key ( $t=19$ to $t=22$ ). This again is due to the assumption of action noise instead of planning noise, making it much more likely under the BIRL model that an agent would randomly walk back towards the second key. The PRP baseline exhibits the same issues with over-confidence and slow recovery described earlier, placing so little posterior mass on the blue gem from $t=17$ to $t=20$ that it even considers the red gem to be more likely. In contrast, our method, SIPS, immediately converges to the blue gem once backtracking occurs at $t=20$.</p>
<h2>B.1.2 Failed Plans</h2>
<p>The differences between SIPS and the baseline methods are even more striking for trajectories with irreversible failures. As shown in Figure S3, SIPS accurately infers that the blue gem is the most likely goal when the agent ignores the two keys at the bottom, instead turning towards the first door guarding the blue gem at $t=19$. This inference also remains stable after $t=21$, when the agent irreversibly uses up its key to unlock that door. SIPS is capable of such inferences because the search for partial plans is biased towards promising intermediate states. Since the underlying agent model assumes a relaxed distance heuristic that considers states closer to the blue gem as promising, the model is likely to produce partial plans that lead spatially toward the blue gem, even if those plans myopically use up the agent's only key.
In contrast, both BIRL and PRP fail to infer that the blue gem is the goal. BIRL initially places increasing probability on the red gem, due to Boltzmann action noise favoring goals which take less time to reach. While this probability decreases slightly as the agent detours from the optimal plan to the red gem, it remains the highest probability goal even after the agent uses up its key at $t=21$. The posterior over goals stops changing after that, because there are no longer any any possible paths to a goal. PRP exhibits a different failure mode. While it does not suffer from the artifacts due to Boltzmann action noise, it completely fails to account for the possibility that an agent might make a failed plan. As a result, the probability of the blue gem does not increase even after the agent turns towards it at $t=19$. Furthermore, once failure occurs at $t=21$, PRP ends up defaulting to a uniform distribution over the three gems, even though it had previously eliminated the red gem as a possibility.
The inferences in Figure S4 display similar trends. Once again, SIPS accurately infers that the blue gem is the goal, even slightly in advance of failure (panel iii). In contrast, BIRL wrongly infers that the red gem is the most likely, while PRP erroneously defaults to inferring upon failure that the only remaining acquirable gem (yellow) is the goal.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure S3: Goal inferences made by SIPS, BIRL-O, and PRP for the failed trajectory shown in Figure 1(b) of the main text. Predicted future trajectories in panels (i)-(iv) are made by SIPS. For SIPS, we used 30 particles per goal, search noise $\gamma=0.1$, persistence parameters $r=2, q=0.95$, and a maze-distance heuristic (i.e. distance to the goal, ignoring doors). Rejuvenation moves were used with $p_{g}=0.25$. For BIRL-O, we used $\alpha=5$. For PRP, we used $\beta=1$.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure S4: Goal inferences made by SIPS, BIRL-O, and PRP for another failed trajectory. Predicted future trajectories in panels (i)-(iv) are made by SIPS. For SIPS, we used 30 particles per goal, search noise $\gamma=0.1$, persistence parameters $r=2, q=0.95$, and a Manhattan distance heuristic to the goal. Rejuvenation moves were used, with a goal rejuvenation probability of $p_{g}=0.25$. For BIRL-O, we used $\alpha=5$. For PRP, we used $\beta=1$.</p>
<h1>B. 2 Accuracy \&amp; Speed</h1>
<p>Here we present quantitative comparisons of the accuracy and speed of each inference method. Tables S1 and S2 show the accuracy results for the optimal and sub-optimal datasets respectively. $P\left(g_{\text {true }} \mid o\right)$ represents the posterior probability of the true goal, while Top-1 represents the fraction of problems where $g_{\text {true }}$ is top-ranked. Accuracy metrics are reported at the first (Q1), second (Q2), and third (Q3) quartiles of each observed trajectory. The corresponding standard deviations (taken across the dataset) are shown to the right of each accuracy mean.
Tables S3 and S4 show the runtime results for the optimal and sub-optimal datasets respectively. Runtime is reported in terms of the start-up cost $\left(\mathrm{C}_{0}\right)$, marginal cost per timestep (MC), and average cost per timestep (AC), all measured in seconds. The corresponding standard deviations are shown to the right of each runtime mean. The total number ( N ) of states visited (during either plan search or value iteration) are also reported as a platform-independent cost metric.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">$P\left(g_{\text {true }} \mid o\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Top-1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q3</td>
</tr>
<tr>
<td style="text-align: center;">Taxi <br> (3 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">$\pm 0.27$</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">$\pm 0.22$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.47$</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">$\pm 0.33$</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Doors, Keys \&amp; Gems (3 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">$\pm 0.35$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">$\pm 0.33$</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">$\pm 0.08$</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">$\pm 0.47$</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$\pm 0.30$</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Block Words (5 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$\pm 0.27$</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\pm 0.46$</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$\pm 0.05$</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">$\pm 0.10$</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">$\pm 0.01$</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\pm 0.44$</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$\pm 0.34$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">$\pm 0.28$</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">$\pm 0.26$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Intrusion Detection (20 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">$\pm 0.38$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">$\pm 0.01$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$\pm 0.02$</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$\pm 0.13$</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">$\pm 0.01$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
</tbody>
</table>
<p>Table S1: Inference accuracy on the dataset of optimal trajectories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">$P\left(g_{\text {true }} \mid o\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Top-1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q3</td>
</tr>
<tr>
<td style="text-align: center;">Taxi <br> (3 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">$\pm 0.38$</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$\pm 0.42$</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">$\pm 0.51$</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$\pm 0.51$</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.47$</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$\pm 0.29$</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$\pm 0.48$</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">$\pm 0.23$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Doors, Keys \&amp; Gems (3 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">$\pm 0.37$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$\pm 0.44$</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$\pm 0.44$</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">$\pm 0.02$</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">$\pm 0.04$</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$\pm 0.49$</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$\pm 0.32$</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">$\pm 0.40$</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">$\pm 0.31$</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">$\pm 0.47$</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">Block Words (5 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">$\pm 0.33$</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">$\pm 0.28$</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">$\pm 0.31$</td>
<td style="text-align: center;">0.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$\pm 0.04$</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">$\pm 0.48$</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$\pm 0.03$</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">$\pm 0.06$</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">$\pm 0.09$</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\pm 0.44$</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">$\pm 0.42$</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">$\pm 0.18$</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">$\pm 0.24$</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">$\pm 0.17$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Intrusion Detection (20 Goals)</td>
<td style="text-align: center;">SIPS</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">$\pm 0.43$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">$\pm 0.50$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">$\pm 0.41$</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-U</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BIRL-O</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">$\pm 0.01$</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">$\pm 0.04$</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">$\pm 0.07$</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">$\pm 0.22$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRP</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">$\pm 0.01$</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">$\pm 0.003$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">$\pm 0.00$</td>
<td style="text-align: center;">1.00</td>
</tr>
</tbody>
</table>
<p>Table S2: Inference accuracy on the dataset of suboptimal trajectories.</p>
<p>In terms of accuracy alone, it can be seen that the PRP baseline generally achieves the highest metrics, with SIPS and BIRL-O performing comparably, and with BIRL-U completely incapable of making accurate inferences except in the Taxi domain. As demonstrated by the qualitative comparisons however, these metrics alone maybe misleading, failing to show how inferences of each method really evolve over time. In particular, while the PRP baseline is routinely able to achieve the highest Top-1 accuracy, this may not correspond to a suitably calibrated posterior over goals, nor might it capture the sharp human-like changes over time that SIPS appears to display. It should also be noted that most of the domains considered do not allow for irreversible failures. As such, the distinctive capability of SIPS to infer goals despite failed plans is not captured by the results in Table S2.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Domain</td>
<td>Method</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(3 Goals)</td>
<td>SIPS</td>
<td>14.7</td>
<td>$\pm 6.73$</td>
<td>2.19</td>
<td>$\pm 0.95$</td>
<td>3.08</td>
<td>$\pm 1.24$</td>
<td>1220</td>
<td>$\pm 405$</td>
</tr>
<tr>
<td></td>
<td>BIRL-U</td>
<td>2.22</td>
<td>$\pm 0.06$</td>
<td>0.002</td>
<td>$\pm 0.0007$</td>
<td>0.17</td>
<td>$\pm 0.03$</td>
<td>10000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td></td>
<td>BIRL-O</td>
<td>0.56</td>
<td>$\pm 0.02$</td>
<td>0.002</td>
<td>$\pm 0.0006$</td>
<td>0.04</td>
<td>$\pm 0.01$</td>
<td>2500</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td></td>
<td>PRP</td>
<td>13.2</td>
<td>$\pm 2.19$</td>
<td>6.21</td>
<td>$\pm 1.52$</td>
<td>6.73</td>
<td>$\pm 1.50$</td>
<td>6830</td>
<td>$\pm 2090$</td>
</tr>
<tr>
<td>Doors,</td>
<td>SIPS</td>
<td>3.17</td>
<td>$\pm 1.10$</td>
<td>0.72</td>
<td>$\pm 0.21$</td>
<td>0.84</td>
<td>$\pm 0.25$</td>
<td>2100</td>
<td>$\pm 1140$</td>
</tr>
<tr>
<td>Keys \&amp;</td>
<td>BIRL-U</td>
<td>3280</td>
<td>$\pm 173$</td>
<td>0.13</td>
<td>$\pm 0.14$</td>
<td>181</td>
<td>$\pm 184$</td>
<td>250000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td>Gems</td>
<td>BIRL-O</td>
<td>142</td>
<td>$\pm 13.0$</td>
<td>0.13</td>
<td>$\pm 0.14$</td>
<td>8.00</td>
<td>$\pm 8.24$</td>
<td>10000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td>(3 Goals)</td>
<td>PRP</td>
<td>5.32</td>
<td>$\pm 2.21$</td>
<td>3.12</td>
<td>$\pm 1.58$</td>
<td>3.24</td>
<td>$\pm 1.67$</td>
<td>5970</td>
<td>$\pm 3350$</td>
</tr>
<tr>
<td>Block</td>
<td>SIPS</td>
<td>21.1</td>
<td>$\pm 4.84$</td>
<td>1.67</td>
<td>$\pm 0.61$</td>
<td>3.62</td>
<td>$\pm 0.85$</td>
<td>2380</td>
<td>$\pm 1110$</td>
</tr>
<tr>
<td>Words</td>
<td>BIRL-U</td>
<td>687</td>
<td>$\pm 273$</td>
<td>0.15</td>
<td>$\pm 0.05$</td>
<td>69.5</td>
<td>$\pm 31.2$</td>
<td>250000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td>(5 Goals)</td>
<td>BIRL-O</td>
<td>19.5</td>
<td>$\pm 0.59$</td>
<td>0.12</td>
<td>$\pm 0.03$</td>
<td>2.11</td>
<td>$\pm 0.51$</td>
<td>10000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td></td>
<td>PRP</td>
<td>25.6</td>
<td>$\pm 11.3$</td>
<td>26.5</td>
<td>$\pm 7.90$</td>
<td>26.3</td>
<td>$\pm 7.50$</td>
<td>3980</td>
<td>$\pm 1410$</td>
</tr>
<tr>
<td>Intrusion</td>
<td>SIPS</td>
<td>325</td>
<td>$\pm 24.9$</td>
<td>12.0</td>
<td>$\pm 1.40$</td>
<td>30.0</td>
<td>$\pm 3.00$</td>
<td>14100</td>
<td>$\pm 343$</td>
</tr>
<tr>
<td>Detection</td>
<td>BIRL-U</td>
<td>18000</td>
<td>$\pm 2050$</td>
<td>0.01</td>
<td>$\pm 0.07$</td>
<td>1130</td>
<td>$\pm 230$</td>
<td>250000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td>(20 Goals)</td>
<td>BIRL-O</td>
<td>100</td>
<td>$\pm 11.7$</td>
<td>0.02</td>
<td>$\pm 0.00$</td>
<td>5.80</td>
<td>$\pm 0.86$</td>
<td>10000</td>
<td>$\pm 0$</td>
</tr>
<tr>
<td></td>
<td>PRP</td>
<td>246</td>
<td>$\pm 5.12$</td>
<td>381</td>
<td>$\pm 108$</td>
<td>374</td>
<td>$\pm 102$</td>
<td>75700</td>
<td>$\pm 20800$</td>
</tr>
</tbody>
</table>
<p>Table S3: Inference runtime on the dataset of optimal trajectories.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Domain</td>
<td>Method</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(3 Goals)</td>
<td>SIPS</td>
<td>12.2</td>
<td>$\pm 7.75$</td>
<td>1.61</td>
<td>$\pm 0.74$</td>
<td>2.29</td>
<td>$\pm 1.05$</td>
<td>1530</td>
<td>$\pm 1110$</td>
</tr>
<tr>
<td></td>
<td>BIRL-U</td>
<td>2.22</td>
<td>$\pm 0.06$</td>
<td>0.003</td>
<td>$\pm 0.0004$</td>
<td>0.16</td>
<td>$\pm 0.04$</td>
<td>10000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td></td>
<td>BIRL-O</td>
<td>2.17</td>
<td>$\pm 0.05$</td>
<td>0.002</td>
<td>$\pm 0.0003$</td>
<td>0.15</td>
<td>$\pm 0.04$</td>
<td>2500</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td></td>
<td>PRP</td>
<td>13.3</td>
<td>$\pm 3.26$</td>
<td>7.33</td>
<td>$\pm 2.61$</td>
<td>7.74</td>
<td>$\pm 2.56$</td>
<td>8840</td>
<td>$\pm 5800$</td>
</tr>
<tr>
<td>Doors,</td>
<td>SIPS</td>
<td>3.40</td>
<td>$\pm 1.18$</td>
<td>0.69</td>
<td>$\pm 0.24$</td>
<td>0.87</td>
<td>$\pm 0.31$</td>
<td>2100</td>
<td>$\pm 1140$</td>
</tr>
<tr>
<td>Keys \&amp;</td>
<td>BIRL-U</td>
<td>3360</td>
<td>$\pm 66.0$</td>
<td>0.11</td>
<td>$\pm 0.06$</td>
<td>133</td>
<td>$\pm 68.7$</td>
<td>250000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td>Gems</td>
<td>BIRL-O</td>
<td>155</td>
<td>$\pm 3.31$</td>
<td>0.11</td>
<td>$\pm 0.06$</td>
<td>6.27</td>
<td>$\pm 3.31$</td>
<td>10000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td>(3 Goals)</td>
<td>PRP</td>
<td>4.65</td>
<td>$\pm 1.58$</td>
<td>3.04</td>
<td>$\pm 1.56$</td>
<td>3.11</td>
<td>$\pm 1.56$</td>
<td>6150</td>
<td>$\pm 3680$</td>
</tr>
<tr>
<td>Block</td>
<td>SIPS</td>
<td>20.6</td>
<td>$\pm 5.79$</td>
<td>2.86</td>
<td>$\pm 1.12$</td>
<td>4.41</td>
<td>$\pm 1.77$</td>
<td>2570</td>
<td>$\pm 810$</td>
</tr>
<tr>
<td>Words</td>
<td>BIRL-U</td>
<td>687</td>
<td>$\pm 273$</td>
<td>0.33</td>
<td>$\pm 0.13$</td>
<td>60.6</td>
<td>$\pm 34.0$</td>
<td>250000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td>(5 Goals)</td>
<td>BIRL-O</td>
<td>23.5</td>
<td>$\pm 1.76$</td>
<td>0.01</td>
<td>$\pm 0.001$</td>
<td>2.12</td>
<td>$\pm 0.86$</td>
<td>10000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td></td>
<td>PRP</td>
<td>40.5</td>
<td>$\pm 22.7$</td>
<td>38.9</td>
<td>$\pm 16.1$</td>
<td>38.9</td>
<td>$\pm 15.7$</td>
<td>5660</td>
<td>$\pm 4860$</td>
</tr>
<tr>
<td>Intrusion</td>
<td>SIPS</td>
<td>400</td>
<td>$\pm 29.7$</td>
<td>3.90</td>
<td>$\pm 1.04$</td>
<td>26.6</td>
<td>$\pm 2.06$</td>
<td>12900</td>
<td>$\pm 3020$</td>
</tr>
<tr>
<td>Detection</td>
<td>BIRL-U</td>
<td>18000</td>
<td>$\pm 2050$</td>
<td>1.12</td>
<td>$\pm 3.83$</td>
<td>1040</td>
<td>$\pm 163$</td>
<td>250000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td>(20 Goals)</td>
<td>BIRL-O</td>
<td>96.9</td>
<td>$\pm 10.4$</td>
<td>0.02</td>
<td>$\pm 0.002$</td>
<td>5.60</td>
<td>$\pm 0.77$</td>
<td>10000</td>
<td>$\pm 0.00$</td>
</tr>
<tr>
<td></td>
<td>PRP</td>
<td>281</td>
<td>$\pm 2.48$</td>
<td>332</td>
<td>$\pm 25.8$</td>
<td>330</td>
<td>$\pm 24.7$</td>
<td>51900</td>
<td>$\pm 960$</td>
</tr>
</tbody>
</table>
<p>Table S4: Inference runtime on the dataset of suboptimal trajectories.</p>
<p>Once runtime is taken into account, it becomes clear that SIPS achieves the best balance between speed and accuracy due to its use of incremental computation. In contrast, BIRL-U requires orders of magnitude more initial computation while still failing to produce meaningful inferences, while PRP requires up to 30 times more computation per timestep. This is especially apparent on the Intrusion Detection domain, which has a large number of goals, requiring PRP to compute a large number of optimal plans at each timestep. Even the BIRL-O baseline, which assumes oracular access to the dataset of observed trajectories during value iteration, is slower than SIPS on the Doors, Keys \&amp; Gems domain in terms of average runtime. Overall, these results imply that SIPS is the only method suitable for online usage on the full range of domains we consider.</p>
<h1>B. 3 Robustness to Parameter Mismatch</h1>
<p>Tables S5 and S6 present additional results for the robustness experiments described in the main text, showing how different settings of model parameters fare against each other. Each column corresponds to a parameter value assumed by SIPS, and each row corresponds to the true parameter for the boundedly rational agent model used to generate the data. Within each sub-table, unspecified parameters default to $\gamma=0.1, r=2, q=0.95, h=$ Manhattan (for Doors, Keys, Gems) and $h=h_{\text {add }}$ (for Block Words).</p>
<p>It can be seen that SIPS fares reasonably well against mismatched parameters, with degradation partly driven by mismatch itself, but also partly by increased randomness when the data-generating parameters lead to less optimal agent behavior. The effect of noisy behavior is especially apparent in Table S6(d): data generated by agents using the highly uninformative goal count heuristic (which simply the counts the number of goal predicates yet to be satisfied as a distance metric) is highly random. This results in very poor inferences (Top-1 at Q3 = 0.37), even when SIPS correctly assumes the same heuristic. Nonetheless, mismatched heuristics do lead to poorer performance, raising the open question of whether observers need good models of others' planning heuristics in order to accurately infer their goals.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$q$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">h</td>
<td style="text-align: center;">Mh.</td>
<td style="text-align: center;">Mz.</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\pi}{4}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">Mh.</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">Mz.</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">(a) Persistence $(r)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(b) Persistence $(q)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(c) Search noise $(\gamma)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(d) Heuristic $(h)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table S5: Robustness to parameter mismatch for the Doors, Keys, Gems domain. The metric shown is the top-1 accuracy of SIPS at the third time quartile (Q3). $h=$ Mh. refers to Manhattan distance, while $h=$ Mz. refers to maze distance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Assumed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$r$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$q$</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">h</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">$h_{\text {add }}$</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{\pi}{4}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">GC.</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">$h_{\text {add }}$</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">(a) Persistence $(r)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(b) Persistence $(q)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(c) Search noise $(\gamma)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">(d) Heuristic $(h)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table S6: Robustness to parameter mismatch for the Blocks World domain. The metric shown is the top-1 accuracy of SIPS at the third time quartile (Q3). $h=$ GC. refers to the goal count heuristic, while $h=h_{\text {add }}$ refers to the additive delete-relaxation heuristic.</p>
<h2>C Human Studies</h2>
<p>As described in the main text, we conducted two sets of pilot studies with human subjects, the first to measure human goal inferences for comparison, and the second to collect human-generated plans for robustness experiments. These studies were approved under MIT's IRB (COUHES no.: 0812003014).</p>
<h2>C. 1 Human Inferences</h2>
<p>Data was collected from $N=5$ pilot subjects in the MIT population. Each subject was given access to a web interface that would present trajectories of an agent in the Doors, Keys \&amp; Gems domain, and that would ask for goal inference judgements at every 6th timestep, as well as the first and last timestep. Subjects could select which gem they believed to be the most likely goal of the agent, and then were allowed to adjust sliders indicating how likely the other goals were in comparison. These relative probability ratings were normalized, and recorded. Excerpts from this interface are shown in Figure S5. Subjects were shown a series of 10 trajectories, out of which 4 were optimal trajectories, and 6 exhibited notable suboptimality or failure.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2512 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2512</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2512</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-52071511</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1802.03793v4.pdf" target="_blank">Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking</a></p>
                <p><strong>Paper Abstract:</strong> The first step of many research projects is to define and rank a short list of candidates for study. In the modern rapidity of scientific progress, some turn to automated hypothesis generation (HG) systems to aid this process. These systems can identify implicit or overlooked connections within a large scientific corpus, and while their importance grows alongside the pace of science, they lack thorough validation. Without any standard numerical evaluation method, many validate general-purpose HG systems by rediscovering a handful of historical findings, and some wishing to be more thorough may run laboratory experiments based on automatic suggestions. These methods are expensive, time consuming, and cannot scale. Thus, we present a numerical evaluation framework for the purpose of validating HG systems that leverages thousands of validation hypotheses. This method evaluates a HG system by its ability to rank hypotheses by plausibility; a process reminiscent of human candidate selection. Because HG systems do not produce a ranking criteria, specifically those that produce topic models, we additionally present novel metrics to quantify the plausibility of hypotheses given topic model system output. Finally, we demonstrate that our proposed validation method aligns with real-world research goals by deploying our method within MOLIERE, our recent topic-driven HG system, in order to automatically generate a set of candidate genes related to HIV-associated neurodegenerative disease (HAND). By performing laboratory experiments based on this candidate set, we discover a new connection between HAND and Dead Box RNA Helicase 3 (DDX3).Reproducibility: code, validation data, and results can be found at sybrandt.com/2018/validation.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2512.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2512.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOLIERE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOLIERE (Automatic Biomedical Hypothesis Generation System)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, topic-model-driven hypothesis generation (HG) system that builds a multi-layer knowledge network from MEDLINE, extracts a focused subcorpus for a given query pair (a, c), produces topic models (PLDA+) and uses word/phrase embeddings (FastText) to surface implicit connections between entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MOLIERE: Automatic Biomedical Hypothesis Generation System.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOLIERE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MOLIERE constructs a multi-layered knowledge network (papers, terms, phrases, links) from MEDLINE; given a query pair (a, c) it induces a relevant subnetwork, extracts titles/abstracts, builds a topic model using PLDA+ (with ToPMine n-gram preprocessing), represents topics as probability distributions and weighted centroids, and uses a 500-dimensional FastText embedding space to compute multiple similarity and network metrics over topics and query entities. It supports nearest-neighbor topic networks induced from topic centroids and graph analytics (shortest paths, centrality, modularity) to characterize possible explanatory 'stories' between a and c.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>topic-model + embedding-based hypothesis generation (knowledge-network backed)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine / biomedical literature mining (genes, diseases, drugs)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Given entities a and c, MOLIERE induces a focused subcorpus from a multi-layer knowledge network, runs PLDA+ topic modeling on the subcorpus (100 topics in experiments), and exposes the topic model as the candidate explanation for an a–c connection. Implicit hypotheses are represented as the produced topic model (topics linking concepts), which are then evaluated by scoring metrics to rank plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>MOLIERE itself produces topic models; plausibility is assessed by computed metrics that compare embeddings of a and c to topic centroids and topic word distributions (metrics: embedding Euclidean distance L2, cosine similarity CSIM, TOPICCORR, BESTCENTRCSIM, BESTCENTRL2, BESTTOPPERWORD, and topic-network-based metrics such as TOPWALKLENGTH, TOPWALKBTWN, TOPWALKEIGEN, TOPNETCCOEF, TOPNETMOD). These metric scores are combined (POLYMULTIPLE) to produce a ranking score indicating hypothesis plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>The system separates hypothesis generation (topic models) from plausibility scoring; novelty is addressed externally via cut-date-based validation (see validation_mechanism). Plausibility is prioritized by metric scoring (POLYMULTIPLE) which weights signals that favor established semantic closeness and structured topic-network explanations; there is no internal mechanism that explicitly optimizes a novelty-vs-plausibility tradeoff within MOLIERE.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Set of embedding- and topic-based metrics: CSIM(a,c) (cosine similarity), L2(a,c) (Euclidean distance), TOPICCORR(a,c,T) (cosine between vectors of topic similarities), BESTCENTRCSIM (max over topics of avg cosine similarity to a and c), BESTCENTRL2 (distance of topic centroid to midpoint normalized by radius), BESTTOPPERWORD (max over topics of min(TOPICSIM(a,Ti), TOPICSIM(c,Ti))), TOPWALKLENGTH (shortest path length in topic-centroid NN graph), TOPWALKBTWN (avg betweenness centrality along shortest a∼c path), TOPWALKEIGEN (avg eigenvector centrality along path), TOPNETCCOEF (clustering coefficient), TOPNETMOD (network modularity), and the composite POLYMULTIPLE polynomial combination of top metrics. Validation metric: ROC AUC measured on ranking published vs noise predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Large-scale computational validation: split corpus by cut-date (2010), label unordered entity pairs whose first co-occurrence appears after cut-date via SemMedDB as 'published' positives and randomly sampled UMLS pairs not in SemMedDB as 'noise' negatives, run MOLIERE on both sets, score each hypothesis with metrics (including POLYMULTIPLE) and compute ROC curves and AUC; additionally, experimental laboratory validation: selection of top-ranked genes connecting HAND to genes and follow-up wet-lab experiments (DDX3 inhibitor protecting neurons) to verify a generated hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Open-source code and validation data/results published (sybrandt.com/2018/validation), use of public corpora and resources (MEDLINE, SemMedDB, UMLS, HUGO gene list), explicit cut-date protocol for training data, and deterministic metric definitions; dataset versions and cut-year specified (e.g., 2017 MEDLINE, 2017AB UMLS, 2016 SemMedDB) to facilitate replication.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Indirect detection via validation: using the noise set and published set from SemMedDB, metrics (and ROC AUC separation) serve to detect and demote 'noisy' or spurious (potentially hallucinated) a–c pairs relative to published connections; no explicit post-generation hallucination detection algorithm beyond ranking against noise.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Primary quantitative evaluation uses ROC AUC; no explicit p-values or confidence intervals reported for classifier separations in the main results.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SemMedDB (predicates extracted from MEDLINE), MEDLINE titles/abstracts, UMLS (entity vocabulary), HUGO gene list for case study; cut-date protocol used to construct published/highly-cited/noise sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key reported results: POLYMULTIPLE composite ROC AUC = 0.834 (published vs noise, PvN) and 0.874 (highly-cited vs noise, HCvN). Individual metrics: L2 distance AUC 0.783 (PvN) and 0.809 (HCvN); CSIM AUC 0.709/0.703; TOPICCORR AUC 0.609/0.496; BESTCENTRCSIM AUC 0.719/0.742; BESTTOPPERWORD AUC 0.686/0.731; BESTCENTRL2 AUC 0.578/0.587. Topic-network metrics performed worse than simple L2 but provided interpretability signals (e.g., TOPWALKBTWN negative indicator).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Yes — MOLIERE (with the ranking metrics) was used to rank ~30,000 gene queries vs HAND; selecting top candidates and focusing on genes without prior HAND links yielded the selection of DDX3, and laboratory experiments showed that a DDX3 inhibitor blocks Tat-dependent stress granule formation and protects neurons from combined Tat/cocaine toxicity, supporting a novel connection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Scalability and performance constraints (original system required huge memory and runtime; experiments ran on 100 instances to sample subsets), sensitivity of some metrics (e.g., TOPICCORR) to number of topics, inability to incorporate verb/relationship type (MOLIERE uses unordered entity pairs and ignores predicate verbs), possible noise/incorrect predicates in SemMedDB used for validation, interpretability limitations of topic models (some topics not intuitively connected), and lack of formal uncertainty quantification or explicit hallucination prevention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2512.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POLYMULTIPLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>POLYMULTIPLE composite ranking metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned polynomial combination of multiple embedding- and topic-network-based metrics designed to score and rank topic-model-derived hypotheses by plausibility, optimized to maximize ROC AUC distinguishing published vs noise predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>POLYMULTIPLE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An ensemble scoring function formed as a polynomial combination of top-performing metrics (L2, BESTCENTRL2, BESTTOPPERWORD, TOPICCORR, TOPWALKBTWN, TOPNETCCOEF etc.) with learned coefficients α_i and exponents β_i; parameters were optimized by stochastic black-box search across ~1,000,000 samples to maximize AUC on the validation ranking task. Input features are scaled to [0,1] before polynomial combination.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>metric-ensemble / learned scoring function</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical literature mining (general-purpose across domains in principle)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator; it scores hypotheses produced by topic-model HG systems (e.g., MOLIERE) to produce a plausibility ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Combines multiple metrics capturing vector-space proximity (L2), topic centroid proximity, per-topic word similarities, topic-model correlation, and network-centrality measures into a single composite score that correlates with known published predicate plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>By weighting different metrics (some that favor closeness, some network structure), the polynomial implicitly balances signals that may correspond to established (plausible) vs more novel links; no explicit novelty penalty or objective was used.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Composite POLYMULTIPLE score; validation measured by ROC AUC on published vs noise ranking (0.834 PvN, 0.874 HCvN).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Trained via black-box optimization to maximize ROC AUC on the SemMedDB-derived published vs noise validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Parameter search procedure and feature definitions detailed; code and data released with the paper (sybrandt.com/2018/validation).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same validation sets derived from SemMedDB, MEDLINE, UMLS; PvN and HCvN evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>POLYMULTIPLE achieved ROC AUC = 0.834 (published vs noise) and 0.874 (highly-cited vs noise), outperforming any single metric tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed individual metrics (e.g., L2 AUC 0.783, BESTCENTRCSIM 0.719), indicating benefit of combining heterogeneous signals.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Used as the ranking criterion to select candidate genes in the HAND case study that led to experimental follow-up (DDX3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Optimization was black-box and potentially prone to local minima; learned weights depend on chosen validation set and number of topics; method requires careful feature scaling and significant computation to search parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2512.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topic-driven ranking metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topic- and embedding-based hypothesis plausibility metrics (CSIM, L2, TOPICCORR, BESTCENTRCSIM, BESTCENTRL2, BESTTOPPERWORD, TOPWALK*, TOPNET*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of eleven quantitative metrics that characterize the relationship between query entities and topic-model output using embeddings and graph measures to score hypothesis plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Topic-driven ranking metrics (ensemble components)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Metrics include: CSIM (cosine similarity between entity embeddings), L2 (Euclidean distance between embeddings), TOPICCORR (cosine similarity between vectors of topic similarities for a and c), BESTCENTRCSIM (best topic centroid averaged cosine to a & c), BESTCENTRL2 (best centroid proximity to midpoint normalized), BESTTOPPERWORD (best topic where both a and c have high TOPICSIM), and topic-network metrics derived from NN graph of centroids: TOPWALKLENGTH, TOPWALKBTWN, TOPWALKEIGEN, TOPNETCCOEF, TOPNETMOD. TOPICSIM(x, Ti) is computed as the probability-weighted cosine between x's embedding and each word embedding in topic Ti.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>feature set for plausibility scoring (embedding- and graph-based)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator; these metrics operate on output topic models to quantify how well topics explain an a–c linkage.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Each metric evaluates a different intuition: proximity in embedding space (L2, CSIM), topic-level mutual similarity (TOPICCORR, BESTTOP*), and structural centrality/connectivity in topic-centroid networks (TOPWALK*, TOPNET*). Scores are intended to correlate with published (plausible) connections.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>The metrics themselves are the hypothesis quality features; their discriminative power is measured by ROC AUC versus the SemMedDB validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Each metric individually and in combination was evaluated by computing rankings over published and noise sets and measuring ROC AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Precise formulas and computation procedures provided; relies on public embeddings (FastText trained on MEDLINE) and topic models (PLDA+).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Metrics serve to demote noisy (likely spurious) hypotheses by assigning lower plausibility scores, indirectly aiding detection of hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluated on SemMedDB-derived published/noise sets (PvN and HCvN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Individual metric AUCs reported: L2 0.783/0.809 (PvN/HCvN), CSIM 0.709/0.703, TOPICCORR 0.609/0.496, BESTCENTRCSIM 0.719/0.742, BESTTOPPERWORD 0.686/0.731, BESTCENTRL2 0.578/0.587; topic-network measures variable but generally below L2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Some individual metrics (notably L2) outperformed more complex topic-network metrics; ensemble combination improved over all individuals.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some metrics sensitive to number of topics (e.g., TOPICCORR degrades with large topic counts), several metrics underperform on highly-cited/hard bridging links, and topic-model interpretability issues remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2512.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemMedDB validation method</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SemMedDB-based cut-date published vs noise validation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, automated validation framework that treats predicates (unordered entity pairs) first appearing after a cut-date in SemMedDB as positive (published) hypotheses and randomly sampled absent UMLS pairs as negative (noise) hypotheses, using ROC AUC on rankings to evaluate HG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SemMedDB cut-date validation framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure: choose a cut-year and train HG system on corpus up to that year; label unordered entity pairs whose first SemMedDB co-occurrence occurs after the cut-date as 'published' positives; sample 'noise' negatives from UMLS pairs not present in SemMedDB; optionally create 'highly-cited' subset by filtering published pairs whose first paper has >100 citations (via Semantic Scholar). Evaluate HG system by its ability to rank published above noise using ROC AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>computational validation protocol / dataset-driven benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical literature and hypothesis-generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty proxy: label as 'novel' those a–c pairs whose first co-occurrence predates the cut-year; this uses temporal novelty — connections absent before cut-date are treated as novel discoveries for the purposes of validation.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is assessed by whether HG system ranks SemMedDB 'published' pairs above noise pairs (measured by ROC AUC).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>ROC AUC of ranked published vs noise lists (PvN) and ranked highly-cited vs noise (HCvN); AUC values used to quantify quality.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational benchmarking across thousands/millions of pairs; optionally combined with downstream experimental validation of top-ranked novel candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Uses public SemMedDB, MEDLINE, UMLS versions and an explicit cut-date; code and data released to reproduce validation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>By construction, 'noise' pairs serve to model nonsensical or spurious links; separation between published and noise in rankings helps detect likely hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SemMedDB predicates, MEDLINE titles/abstracts, UMLS vocabulary, Semantic Scholar citation metadata for highly-cited subset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to report HG system AUCs (see MOLIERE/POLYMULTIPLE numbers); provides large-scale quantitative validation instead of small expert-based checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Validation labels may include incorrect or incidental published predicates; noise set could contain real-but-undiscovered connections; reliance on SemMedDB quality and citation matching (title-based matching to Semantic Scholar) can introduce errors; temporal labelling is a proxy for novelty but not a guarantee of scientific importance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2512.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARROWSMITH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arrowsmith (ABC model literature-based discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early literature-based discovery system implementing the ABC model: given A and C, find intermediate B terms such that A→B and B→C, thereby suggesting an implicit A→C hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using arrowsmith: a computerassisted approach to formulating and assessing scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Arrowsmith / ABC systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ABC/co-occurrence approach: run queries specifying two keywords (a and c), identify lists of intermediate linking terms b that co-occur with each (A-B and B-C), and present candidate linking terms as potential explanations; historically validated by rediscovering Swanson's findings.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>ABC / co-occurrence based literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical literature-based discovery</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates hypotheses by enumerating linking terms B that connect A and C through known co-occurrences in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Historically via replication of Swanson's discoveries and by comparing to new UMLS/OMIM entries; no systematic large-scale temporal validation described here.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Often relies on expert evaluation, or precision/recall computed versus curated 'gold' term sets in some evaluations (LitLinker-style).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Precision/recall on curated gold standards in some works; otherwise expert validation.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Replication of known discoveries (M1), expert review, and small-scale statistical evaluations in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MEDLINE/UMLS in historical studies (context dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Historically responsible for notable rediscoveries such as Raynaud's–fish oil and others referenced in the HG literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ABC approach can be limited by combinatorial explosion of B-terms, potential for spurious co-occurrence-based links, requires expert filtering or additional scoring to prioritize candidates, and may miss multi-step or non-term-based explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2512.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioLDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioLDA (topic-model-driven relationship mining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A topic-model-based system applied to biomedical literature to find complex biological relationships by deriving latent topics over documents and identifying topics that link biological entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finding complex biological relationships in recent pubmed articles using bio-lda.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioLDA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applies LDA-like topic modeling to biomedical text (PubMed) to discover latent topical structures that can reveal relationships among genes, drugs, and diseases; similar in spirit to MOLIERE's topic-driven approach but constructed independently.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>topic-model-based HG</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Derives topics over PubMed articles and inspects topic-term co-occurrences to suggest relationships among biological entities.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Often relies on expert analysis and interpretation of topic-word distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>PubMed articles</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Topic models can produce topics unrelated to queries; lack of direct ranking criteria for hypotheses and reliance on expert interpretation limit scalability without additional metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2512.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Watson for Drug Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IBM Watson for Drug Discovery (as used in Bakkar et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial AI/recommender system applied to biomedical literature and data to prioritize candidate entities for follow-up; cited as a system that aided discovery of RNA-binding proteins linked to ALS via recommendation and expert follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Watson for Drug Discovery (recommender-style literature mining)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A commercial AI platform that integrates literature mining, knowledge graph construction, and recommendation algorithms to suggest candidate biological entities for further investigation; in cited work, used to select RNA-binding proteins for expert evaluation leading to discovery of new ALS connections.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-graph / recommender / literature-mining platform</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine / neurodegenerative disease research</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Combines document co-occurrence networks, learned connection patterns, and recommender-system methods to propose candidate entities related to a target condition.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Often validated through downstream experimental or domain-expert follow-up; platform ranks candidates by internal scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Expert scientists examine and experimentally follow up on recommended candidates; cited study uncovered five new RBPs linked to ALS using such process.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Cited study reports identification of five RBPs not previously linked to ALS following system recommendations and domain scientist validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on domain experts for candidate triage and experimental validation; commercial platform details often not fully disclosed in public literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2512.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2512.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist (Soldatova & Rzhetsky)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Scientist / automated experiment-running systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated systems that not only generate hypotheses but also plan and execute experiments (within constrained domains) to test them, exemplified by Soldatova & Rzhetsky's work; capable of selecting high-value experiments and running them in an automated lab for closed-loop discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation of research hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist / automated laboratory-driven HG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Systems that encode hypotheses as formal logical statements, select experiments that maximally reduce uncertainty or test critical implications, and (in some implementations) automatically execute assays in laboratory hardware to confirm or refute hypotheses; initial implementations were constrained to small-scale (e.g., yeast) experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural-symbolic / automated-experimentation (robotic lab integration)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology (laboratory-scale experimental science), bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates formal logical hypotheses from literature/data and ranks experiments based on expected value of information or informativeness; can automatically select and run experiments to test top hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Direct experimental validation through robotic execution of assays; selects statements that would be most valuable if proven true.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Work referenced discusses selecting experiments with high expected value, implying formal uncertainty/utility computations, though specific methods are context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Prior robot-scientist efforts have demonstrated autonomous discovery in constrained laboratory settings (e.g., yeast), validating HG outputs with real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Scope limited by cost and physical constraints of automated labs; feasible only for narrow experimental domains and small organism/model systems in initial implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MOLIERE: Automatic Biomedical Hypothesis Generation System. <em>(Rating: 2)</em></li>
                <li>Semmeddb: a pubmed-scale repository of biomedical semantic predications. <em>(Rating: 2)</em></li>
                <li>Finding complex biological relationships in recent pubmed articles using bio-lda. <em>(Rating: 2)</em></li>
                <li>Using arrowsmith: a computerassisted approach to formulating and assessing scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis. <em>(Rating: 2)</em></li>
                <li>Representation of research hypotheses. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2512",
    "paper_id": "paper-52071511",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "MOLIERE",
            "name_full": "MOLIERE (Automatic Biomedical Hypothesis Generation System)",
            "brief_description": "An open-source, topic-model-driven hypothesis generation (HG) system that builds a multi-layer knowledge network from MEDLINE, extracts a focused subcorpus for a given query pair (a, c), produces topic models (PLDA+) and uses word/phrase embeddings (FastText) to surface implicit connections between entities.",
            "citation_title": "MOLIERE: Automatic Biomedical Hypothesis Generation System.",
            "mention_or_use": "use",
            "system_name": "MOLIERE",
            "system_description": "MOLIERE constructs a multi-layered knowledge network (papers, terms, phrases, links) from MEDLINE; given a query pair (a, c) it induces a relevant subnetwork, extracts titles/abstracts, builds a topic model using PLDA+ (with ToPMine n-gram preprocessing), represents topics as probability distributions and weighted centroids, and uses a 500-dimensional FastText embedding space to compute multiple similarity and network metrics over topics and query entities. It supports nearest-neighbor topic networks induced from topic centroids and graph analytics (shortest paths, centrality, modularity) to characterize possible explanatory 'stories' between a and c.",
            "system_type": "topic-model + embedding-based hypothesis generation (knowledge-network backed)",
            "scientific_domain": "biomedicine / biomedical literature mining (genes, diseases, drugs)",
            "hypothesis_generation_method": "Given entities a and c, MOLIERE induces a focused subcorpus from a multi-layer knowledge network, runs PLDA+ topic modeling on the subcorpus (100 topics in experiments), and exposes the topic model as the candidate explanation for an a–c connection. Implicit hypotheses are represented as the produced topic model (topics linking concepts), which are then evaluated by scoring metrics to rank plausibility.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "MOLIERE itself produces topic models; plausibility is assessed by computed metrics that compare embeddings of a and c to topic centroids and topic word distributions (metrics: embedding Euclidean distance L2, cosine similarity CSIM, TOPICCORR, BESTCENTRCSIM, BESTCENTRL2, BESTTOPPERWORD, and topic-network-based metrics such as TOPWALKLENGTH, TOPWALKBTWN, TOPWALKEIGEN, TOPNETCCOEF, TOPNETMOD). These metric scores are combined (POLYMULTIPLE) to produce a ranking score indicating hypothesis plausibility.",
            "novelty_plausibility_balance": "The system separates hypothesis generation (topic models) from plausibility scoring; novelty is addressed externally via cut-date-based validation (see validation_mechanism). Plausibility is prioritized by metric scoring (POLYMULTIPLE) which weights signals that favor established semantic closeness and structured topic-network explanations; there is no internal mechanism that explicitly optimizes a novelty-vs-plausibility tradeoff within MOLIERE.",
            "hypothesis_quality_metrics": "Set of embedding- and topic-based metrics: CSIM(a,c) (cosine similarity), L2(a,c) (Euclidean distance), TOPICCORR(a,c,T) (cosine between vectors of topic similarities), BESTCENTRCSIM (max over topics of avg cosine similarity to a and c), BESTCENTRL2 (distance of topic centroid to midpoint normalized by radius), BESTTOPPERWORD (max over topics of min(TOPICSIM(a,Ti), TOPICSIM(c,Ti))), TOPWALKLENGTH (shortest path length in topic-centroid NN graph), TOPWALKBTWN (avg betweenness centrality along shortest a∼c path), TOPWALKEIGEN (avg eigenvector centrality along path), TOPNETCCOEF (clustering coefficient), TOPNETMOD (network modularity), and the composite POLYMULTIPLE polynomial combination of top metrics. Validation metric: ROC AUC measured on ranking published vs noise predicates.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Large-scale computational validation: split corpus by cut-date (2010), label unordered entity pairs whose first co-occurrence appears after cut-date via SemMedDB as 'published' positives and randomly sampled UMLS pairs not in SemMedDB as 'noise' negatives, run MOLIERE on both sets, score each hypothesis with metrics (including POLYMULTIPLE) and compute ROC curves and AUC; additionally, experimental laboratory validation: selection of top-ranked genes connecting HAND to genes and follow-up wet-lab experiments (DDX3 inhibitor protecting neurons) to verify a generated hypothesis.",
            "reproducibility_measures": "Open-source code and validation data/results published (sybrandt.com/2018/validation), use of public corpora and resources (MEDLINE, SemMedDB, UMLS, HUGO gene list), explicit cut-date protocol for training data, and deterministic metric definitions; dataset versions and cut-year specified (e.g., 2017 MEDLINE, 2017AB UMLS, 2016 SemMedDB) to facilitate replication.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Indirect detection via validation: using the noise set and published set from SemMedDB, metrics (and ROC AUC separation) serve to detect and demote 'noisy' or spurious (potentially hallucinated) a–c pairs relative to published connections; no explicit post-generation hallucination detection algorithm beyond ranking against noise.",
            "hallucination_rate": null,
            "statistical_significance_testing": "Primary quantitative evaluation uses ROC AUC; no explicit p-values or confidence intervals reported for classifier separations in the main results.",
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "SemMedDB (predicates extracted from MEDLINE), MEDLINE titles/abstracts, UMLS (entity vocabulary), HUGO gene list for case study; cut-date protocol used to construct published/highly-cited/noise sets.",
            "performance_metrics": "Key reported results: POLYMULTIPLE composite ROC AUC = 0.834 (published vs noise, PvN) and 0.874 (highly-cited vs noise, HCvN). Individual metrics: L2 distance AUC 0.783 (PvN) and 0.809 (HCvN); CSIM AUC 0.709/0.703; TOPICCORR AUC 0.609/0.496; BESTCENTRCSIM AUC 0.719/0.742; BESTTOPPERWORD AUC 0.686/0.731; BESTCENTRL2 AUC 0.578/0.587. Topic-network metrics performed worse than simple L2 but provided interpretability signals (e.g., TOPWALKBTWN negative indicator).",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Yes — MOLIERE (with the ranking metrics) was used to rank ~30,000 gene queries vs HAND; selecting top candidates and focusing on genes without prior HAND links yielded the selection of DDX3, and laboratory experiments showed that a DDX3 inhibitor blocks Tat-dependent stress granule formation and protects neurons from combined Tat/cocaine toxicity, supporting a novel connection.",
            "limitations": "Scalability and performance constraints (original system required huge memory and runtime; experiments ran on 100 instances to sample subsets), sensitivity of some metrics (e.g., TOPICCORR) to number of topics, inability to incorporate verb/relationship type (MOLIERE uses unordered entity pairs and ignores predicate verbs), possible noise/incorrect predicates in SemMedDB used for validation, interpretability limitations of topic models (some topics not intuitively connected), and lack of formal uncertainty quantification or explicit hallucination prevention.",
            "uuid": "e2512.0",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "POLYMULTIPLE",
            "name_full": "POLYMULTIPLE composite ranking metric",
            "brief_description": "A learned polynomial combination of multiple embedding- and topic-network-based metrics designed to score and rank topic-model-derived hypotheses by plausibility, optimized to maximize ROC AUC distinguishing published vs noise predicates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "POLYMULTIPLE",
            "system_description": "An ensemble scoring function formed as a polynomial combination of top-performing metrics (L2, BESTCENTRL2, BESTTOPPERWORD, TOPICCORR, TOPWALKBTWN, TOPNETCCOEF etc.) with learned coefficients α_i and exponents β_i; parameters were optimized by stochastic black-box search across ~1,000,000 samples to maximize AUC on the validation ranking task. Input features are scaled to [0,1] before polynomial combination.",
            "system_type": "metric-ensemble / learned scoring function",
            "scientific_domain": "biomedical literature mining (general-purpose across domains in principle)",
            "hypothesis_generation_method": "Not a generator; it scores hypotheses produced by topic-model HG systems (e.g., MOLIERE) to produce a plausibility ranking.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Combines multiple metrics capturing vector-space proximity (L2), topic centroid proximity, per-topic word similarities, topic-model correlation, and network-centrality measures into a single composite score that correlates with known published predicate plausibility.",
            "novelty_plausibility_balance": "By weighting different metrics (some that favor closeness, some network structure), the polynomial implicitly balances signals that may correspond to established (plausible) vs more novel links; no explicit novelty penalty or objective was used.",
            "hypothesis_quality_metrics": "Composite POLYMULTIPLE score; validation measured by ROC AUC on published vs noise ranking (0.834 PvN, 0.874 HCvN).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Trained via black-box optimization to maximize ROC AUC on the SemMedDB-derived published vs noise validation set.",
            "reproducibility_measures": "Parameter search procedure and feature definitions detailed; code and data released with the paper (sybrandt.com/2018/validation).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same validation sets derived from SemMedDB, MEDLINE, UMLS; PvN and HCvN evaluation sets.",
            "performance_metrics": "POLYMULTIPLE achieved ROC AUC = 0.834 (published vs noise) and 0.874 (highly-cited vs noise), outperforming any single metric tested.",
            "comparison_with_baseline": "Outperformed individual metrics (e.g., L2 AUC 0.783, BESTCENTRCSIM 0.719), indicating benefit of combining heterogeneous signals.",
            "validated_on_real_science": true,
            "novel_discoveries": "Used as the ranking criterion to select candidate genes in the HAND case study that led to experimental follow-up (DDX3).",
            "limitations": "Optimization was black-box and potentially prone to local minima; learned weights depend on chosen validation set and number of topics; method requires careful feature scaling and significant computation to search parameter space.",
            "uuid": "e2512.1",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Topic-driven ranking metrics",
            "name_full": "Topic- and embedding-based hypothesis plausibility metrics (CSIM, L2, TOPICCORR, BESTCENTRCSIM, BESTCENTRL2, BESTTOPPERWORD, TOPWALK*, TOPNET*)",
            "brief_description": "A suite of eleven quantitative metrics that characterize the relationship between query entities and topic-model output using embeddings and graph measures to score hypothesis plausibility.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Topic-driven ranking metrics (ensemble components)",
            "system_description": "Metrics include: CSIM (cosine similarity between entity embeddings), L2 (Euclidean distance between embeddings), TOPICCORR (cosine similarity between vectors of topic similarities for a and c), BESTCENTRCSIM (best topic centroid averaged cosine to a & c), BESTCENTRL2 (best centroid proximity to midpoint normalized), BESTTOPPERWORD (best topic where both a and c have high TOPICSIM), and topic-network metrics derived from NN graph of centroids: TOPWALKLENGTH, TOPWALKBTWN, TOPWALKEIGEN, TOPNETCCOEF, TOPNETMOD. TOPICSIM(x, Ti) is computed as the probability-weighted cosine between x's embedding and each word embedding in topic Ti.",
            "system_type": "feature set for plausibility scoring (embedding- and graph-based)",
            "scientific_domain": "biomedical literature mining",
            "hypothesis_generation_method": "Not a generator; these metrics operate on output topic models to quantify how well topics explain an a–c linkage.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Each metric evaluates a different intuition: proximity in embedding space (L2, CSIM), topic-level mutual similarity (TOPICCORR, BESTTOP*), and structural centrality/connectivity in topic-centroid networks (TOPWALK*, TOPNET*). Scores are intended to correlate with published (plausible) connections.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "The metrics themselves are the hypothesis quality features; their discriminative power is measured by ROC AUC versus the SemMedDB validation set.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Each metric individually and in combination was evaluated by computing rankings over published and noise sets and measuring ROC AUC.",
            "reproducibility_measures": "Precise formulas and computation procedures provided; relies on public embeddings (FastText trained on MEDLINE) and topic models (PLDA+).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Metrics serve to demote noisy (likely spurious) hypotheses by assigning lower plausibility scores, indirectly aiding detection of hallucinations.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Evaluated on SemMedDB-derived published/noise sets (PvN and HCvN).",
            "performance_metrics": "Individual metric AUCs reported: L2 0.783/0.809 (PvN/HCvN), CSIM 0.709/0.703, TOPICCORR 0.609/0.496, BESTCENTRCSIM 0.719/0.742, BESTTOPPERWORD 0.686/0.731, BESTCENTRL2 0.578/0.587; topic-network measures variable but generally below L2.",
            "comparison_with_baseline": "Some individual metrics (notably L2) outperformed more complex topic-network metrics; ensemble combination improved over all individuals.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Some metrics sensitive to number of topics (e.g., TOPICCORR degrades with large topic counts), several metrics underperform on highly-cited/hard bridging links, and topic-model interpretability issues remain.",
            "uuid": "e2512.2",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "SemMedDB validation method",
            "name_full": "SemMedDB-based cut-date published vs noise validation framework",
            "brief_description": "A large-scale, automated validation framework that treats predicates (unordered entity pairs) first appearing after a cut-date in SemMedDB as positive (published) hypotheses and randomly sampled absent UMLS pairs as negative (noise) hypotheses, using ROC AUC on rankings to evaluate HG systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SemMedDB cut-date validation framework",
            "system_description": "Procedure: choose a cut-year and train HG system on corpus up to that year; label unordered entity pairs whose first SemMedDB co-occurrence occurs after the cut-date as 'published' positives; sample 'noise' negatives from UMLS pairs not present in SemMedDB; optionally create 'highly-cited' subset by filtering published pairs whose first paper has &gt;100 citations (via Semantic Scholar). Evaluate HG system by its ability to rank published above noise using ROC AUC.",
            "system_type": "computational validation protocol / dataset-driven benchmarking",
            "scientific_domain": "biomedical literature and hypothesis-generation evaluation",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": "Novelty proxy: label as 'novel' those a–c pairs whose first co-occurrence predates the cut-year; this uses temporal novelty — connections absent before cut-date are treated as novel discoveries for the purposes of validation.",
            "plausibility_assessment_method": "Plausibility is assessed by whether HG system ranks SemMedDB 'published' pairs above noise pairs (measured by ROC AUC).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "ROC AUC of ranked published vs noise lists (PvN) and ranked highly-cited vs noise (HCvN); AUC values used to quantify quality.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational benchmarking across thousands/millions of pairs; optionally combined with downstream experimental validation of top-ranked novel candidates.",
            "reproducibility_measures": "Uses public SemMedDB, MEDLINE, UMLS versions and an explicit cut-date; code and data released to reproduce validation experiments.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "By construction, 'noise' pairs serve to model nonsensical or spurious links; separation between published and noise in rankings helps detect likely hallucinations.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "SemMedDB predicates, MEDLINE titles/abstracts, UMLS vocabulary, Semantic Scholar citation metadata for highly-cited subset.",
            "performance_metrics": "Used to report HG system AUCs (see MOLIERE/POLYMULTIPLE numbers); provides large-scale quantitative validation instead of small expert-based checks.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Validation labels may include incorrect or incidental published predicates; noise set could contain real-but-undiscovered connections; reliance on SemMedDB quality and citation matching (title-based matching to Semantic Scholar) can introduce errors; temporal labelling is a proxy for novelty but not a guarantee of scientific importance.",
            "uuid": "e2512.3",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "ARROWSMITH",
            "name_full": "Arrowsmith (ABC model literature-based discovery)",
            "brief_description": "An early literature-based discovery system implementing the ABC model: given A and C, find intermediate B terms such that A→B and B→C, thereby suggesting an implicit A→C hypothesis.",
            "citation_title": "Using arrowsmith: a computerassisted approach to formulating and assessing scientific hypotheses.",
            "mention_or_use": "mention",
            "system_name": "Arrowsmith / ABC systems",
            "system_description": "ABC/co-occurrence approach: run queries specifying two keywords (a and c), identify lists of intermediate linking terms b that co-occur with each (A-B and B-C), and present candidate linking terms as potential explanations; historically validated by rediscovering Swanson's findings.",
            "system_type": "ABC / co-occurrence based literature-based discovery",
            "scientific_domain": "biomedical literature-based discovery",
            "hypothesis_generation_method": "Generates hypotheses by enumerating linking terms B that connect A and C through known co-occurrences in the literature.",
            "novelty_assessment_method": "Historically via replication of Swanson's discoveries and by comparing to new UMLS/OMIM entries; no systematic large-scale temporal validation described here.",
            "plausibility_assessment_method": "Often relies on expert evaluation, or precision/recall computed versus curated 'gold' term sets in some evaluations (LitLinker-style).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Precision/recall on curated gold standards in some works; otherwise expert validation.",
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Replication of known discoveries (M1), expert review, and small-scale statistical evaluations in prior literature.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "MEDLINE/UMLS in historical studies (context dependent)",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Historically responsible for notable rediscoveries such as Raynaud's–fish oil and others referenced in the HG literature.",
            "limitations": "ABC approach can be limited by combinatorial explosion of B-terms, potential for spurious co-occurrence-based links, requires expert filtering or additional scoring to prioritize candidates, and may miss multi-step or non-term-based explanations.",
            "uuid": "e2512.4",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "BioLDA",
            "name_full": "BioLDA (topic-model-driven relationship mining)",
            "brief_description": "A topic-model-based system applied to biomedical literature to find complex biological relationships by deriving latent topics over documents and identifying topics that link biological entities.",
            "citation_title": "Finding complex biological relationships in recent pubmed articles using bio-lda.",
            "mention_or_use": "mention",
            "system_name": "BioLDA",
            "system_description": "Applies LDA-like topic modeling to biomedical text (PubMed) to discover latent topical structures that can reveal relationships among genes, drugs, and diseases; similar in spirit to MOLIERE's topic-driven approach but constructed independently.",
            "system_type": "topic-model-based HG",
            "scientific_domain": "biomedical literature mining",
            "hypothesis_generation_method": "Derives topics over PubMed articles and inspects topic-term co-occurrences to suggest relationships among biological entities.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Often relies on expert analysis and interpretation of topic-word distributions.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "PubMed articles",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Topic models can produce topics unrelated to queries; lack of direct ranking criteria for hypotheses and reliance on expert interpretation limit scalability without additional metrics.",
            "uuid": "e2512.5",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Watson for Drug Discovery",
            "name_full": "IBM Watson for Drug Discovery (as used in Bakkar et al.)",
            "brief_description": "A commercial AI/recommender system applied to biomedical literature and data to prioritize candidate entities for follow-up; cited as a system that aided discovery of RNA-binding proteins linked to ALS via recommendation and expert follow-up.",
            "citation_title": "Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis.",
            "mention_or_use": "mention",
            "system_name": "Watson for Drug Discovery (recommender-style literature mining)",
            "system_description": "A commercial AI platform that integrates literature mining, knowledge graph construction, and recommendation algorithms to suggest candidate biological entities for further investigation; in cited work, used to select RNA-binding proteins for expert evaluation leading to discovery of new ALS connections.",
            "system_type": "knowledge-graph / recommender / literature-mining platform",
            "scientific_domain": "biomedicine / neurodegenerative disease research",
            "hypothesis_generation_method": "Combines document co-occurrence networks, learned connection patterns, and recommender-system methods to propose candidate entities related to a target condition.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Often validated through downstream experimental or domain-expert follow-up; platform ranks candidates by internal scoring.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Expert scientists examine and experimentally follow up on recommended candidates; cited study uncovered five new RBPs linked to ALS using such process.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Cited study reports identification of five RBPs not previously linked to ALS following system recommendations and domain scientist validation.",
            "limitations": "Relies on domain experts for candidate triage and experimental validation; commercial platform details often not fully disclosed in public literature.",
            "uuid": "e2512.6",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Robot Scientist (Soldatova & Rzhetsky)",
            "name_full": "Robot Scientist / automated experiment-running systems",
            "brief_description": "Automated systems that not only generate hypotheses but also plan and execute experiments (within constrained domains) to test them, exemplified by Soldatova & Rzhetsky's work; capable of selecting high-value experiments and running them in an automated lab for closed-loop discovery.",
            "citation_title": "Representation of research hypotheses.",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist / automated laboratory-driven HG",
            "system_description": "Systems that encode hypotheses as formal logical statements, select experiments that maximally reduce uncertainty or test critical implications, and (in some implementations) automatically execute assays in laboratory hardware to confirm or refute hypotheses; initial implementations were constrained to small-scale (e.g., yeast) experiments.",
            "system_type": "neural-symbolic / automated-experimentation (robotic lab integration)",
            "scientific_domain": "biology (laboratory-scale experimental science), bioinformatics",
            "hypothesis_generation_method": "Generates formal logical hypotheses from literature/data and ranks experiments based on expected value of information or informativeness; can automatically select and run experiments to test top hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Direct experimental validation through robotic execution of assays; selects statements that would be most valuable if proven true.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Work referenced discusses selecting experiments with high expected value, implying formal uncertainty/utility computations, though specific methods are context-dependent.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Prior robot-scientist efforts have demonstrated autonomous discovery in constrained laboratory settings (e.g., yeast), validating HG outputs with real experiments.",
            "limitations": "Scope limited by cost and physical constraints of automated labs; feasible only for narrow experimental domains and small organism/model systems in initial implementations.",
            "uuid": "e2512.7",
            "source_info": {
                "paper_title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MOLIERE: Automatic Biomedical Hypothesis Generation System.",
            "rating": 2,
            "sanitized_title": "moliere_automatic_biomedical_hypothesis_generation_system"
        },
        {
            "paper_title": "Semmeddb: a pubmed-scale repository of biomedical semantic predications.",
            "rating": 2,
            "sanitized_title": "semmeddb_a_pubmedscale_repository_of_biomedical_semantic_predications"
        },
        {
            "paper_title": "Finding complex biological relationships in recent pubmed articles using bio-lda.",
            "rating": 2,
            "sanitized_title": "finding_complex_biological_relationships_in_recent_pubmed_articles_using_biolda"
        },
        {
            "paper_title": "Using arrowsmith: a computerassisted approach to formulating and assessing scientific hypotheses.",
            "rating": 2,
            "sanitized_title": "using_arrowsmith_a_computerassisted_approach_to_formulating_and_assessing_scientific_hypotheses"
        },
        {
            "paper_title": "Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis.",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_in_neurodegenerative_disease_research_use_of_ibm_watson_to_identify_additional_rnabinding_proteins_altered_in_amyotrophic_lateral_sclerosis"
        },
        {
            "paper_title": "Representation of research hypotheses.",
            "rating": 1,
            "sanitized_title": "representation_of_research_hypotheses"
        }
    ],
    "cost": 0.021912499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking
5 Dec 2018</p>
<p>Justin Sybrandt 
Clemson University School of Computing Clemson
USA</p>
<p>Michael Shtutman shtutmanm@sccp.sc.edu 
University of South Carolina Drug Discovery and Biomedical Sciences Columbia
USA</p>
<p>Ilya Safro isafro@clemson.edu 
Clemson University School of Computing Clemson
USA</p>
<p>Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking
5 Dec 2018C3E8F168BF5D9B919E632474EAA850D7arXiv:1802.03793v4[cs.IR]Literature Based DiscoveryHypothesis GenerationScientific Text MiningApplied Data Science
The first step of many research projects is to define and rank a short list of candidates for study.In the modern rapidity of scientific progress, some turn to automated hypothesis generation (HG) systems to aid this process.These systems can identify implicit or overlooked connections within a large scientific corpus, and while their importance grows alongside the pace of science, they lack thorough validation.Without any standard numerical evaluation method, many validate generalpurpose HG systems by rediscovering a handful of historical findings, and some wishing to be more thorough may run laboratory experiments based on automatic suggestions.These methods are expensive, time consuming, and cannot scale.Thus, we present a numerical evaluation framework for the purpose of validating HG systems that leverages thousands of validation hypotheses.This method evaluates a HG system by its ability to rank hypotheses by plausibility; a process reminiscent of human candidate selection.Because HG systems do not produce a ranking criteria, specifically those that produce topic models, we additionally present novel metrics to quantify the plausibility of hypotheses given topic model system output.Finally, we demonstrate that our proposed validation method aligns with real-world research goals by deploying our method within MOLIERE, our recent topic-driven HG system, in order to automatically generate a set of candidate genes related to HIVassociated neurodegenerative disease (HAND).By performing laboratory experiments based on this candidate set, we discover a new connection between HAND and Dead Box RNA Helicase 3 (DDX3).Reproducibility: code, validation data, and results can be found at sybrandt.com/2018/validation.</p>
<p>I. INTRODUCTION</p>
<p>In the early stages of a research project, biomedical scientists often perform "candidate selection," wherein they select potential targets for future study [1].For instance, when exploring a certain cancer, scientists may identify a few dozen genes on which to experiment.This process relies on the background knowledge and intuitions held by each researcher, and higher-quality candidate lists often lead to more efficient research results.However, the rate of scientific progress has been increasing steadily [2], and occasionally scientists miss important findings.for instance, was the case regarding the missing connection between Raynaud's Syndrome and fish oil [3], and in the case of five genes recently linked to Amyotrophic Lateral Sclerosis [4].Hypothesis Generation (HG) systems allow scientists to leverage the cumulative knowledge contained across millions of papers, which lead to both above findings, among many others.The importance of these systems rises alongside the pace of scientific output; an abundance of literature implies an abundance of overlooked connections.While many propose techniques to understand potential connections [5], [6], [7], [8], [9], few automated validation techniques exist [10] for general-purpose HG systems (not designed for specific sub-domains or types of queries such as OHSUMED [11] or BioCreative datasets).Often, subject-matter experts assist in validation by running laboratory experiments based on HG system output.This process is expensive, time consuming, and does not scale beyond a handful of validation examples.</p>
<p>HG systems are hard to validate because they attempt to uncover novel information, unknown to even those constructing or testing the system.For instance, how are we to distinguish a bizarre generated hypothesis that turns out to produce important results from one that turns out to be incorrect?Furthermore, how can we do so at scale or across fields?While there are verifiable models for novelty in specific contexts, each is trained to detect patterns similar to those present in a training set, which is conducive to traditional crossvalidation.Some examples include using non-negative matrix factorization to uncover protein-protein interactions [12], or to discover mutational cancer signatures [13].However, HG is unlike the above examples as it strives to detect novel patterns that are a) absent from a dataset, b) may be wholly unknown or even currently counterintuitive, and c) not necessarily outliers as in traditional data mining.</p>
<p>Our contribution: In this paper we propose novel hypothesis ranking methods and a method to validate HG systems that does not require expert input and allows for large validation sets.This method judges a system by its ability to rank hypotheses by plausibility, similarly to how a human scientist must rank potential research directions during candidate selection.We start by dividing a corpus based on a "cut date," and provide a system only information that was priorly available.Then, we identify predicates (clauses consisting of subject, verb, and object) whose first co-occurrence in a sentence is after the cut date.Because typical corpora contain only titles and abstracts, these recently introduced connections represent significant findings that were not previously formulated, thus we can treat them as surrogates for plausible hypotheses from the perspective of the system under evaluation.To provide implausible hypotheses, we randomly generate predicates that do not occur in the corpus as a whole.Then, the HG system must rank both the plausible and implausible predicates together by evaluating the predicted connection strength between each predicate's subject and object.The system's evaluation is based on the area under this ranking's Receiver Operating Characteristic (ROC) curve, wherein the highest area under curve (AUC) of 1 represents a ranking that places all plausible connections above the implausible, and the lowest AUC of 0.5 represents an even mixture of the two.</p>
<p>We note that many HG systems do not typically produce a ranking criteria for potential hypotheses.Particularly, we find that those systems that produce topic model output, such as MOLIERE [6] or BioLDA [5], lack this criteria, but present promising results through expert analysis.Therefore, we additionally developed a number of novel metrics for topicdriven HG systems that quantify the plausibility of potential connections.These metrics leverage word embeddings [14] to understand how the elements of a hypothesis relate to its resulting LDA topic model [15].Through our experiments, described below, we identify that a polynomial combination of five different metrics allows for the highest-scoring ranking (0.834).This result is especially significant given that the main validation methods available, to both MOLIERE and other similar systems (see survey in [6]), were expert analysis and replicating the results of others [10].Still, while the systems mentioned above focus on the medical domain, we note that neither our metrics, nor our validation methodology, are domain specific.</p>
<p>To demonstrate that our proposed validation process and new metrics apply to real-world applications, we present a case study wherein our techniques validate an open-source HG system as well as identify a novel gene-disease connection.We modify MOLIERE to support our new metrics, and we perform our validation process.This system is trained on MEDLINE [16], a database containing over 27 million papers (titles and abstracts) maintained by the National Library of Health.We use SemMedDB [17], a database of predicates extracted from MEDLINE, in order to identify the set of "published" (plausible) and "noise" (implausible) hypotheses.This database represents its connections in terms of codified entities provided by the Unified Medical Language System (UMLS), which enables our experimental procedure to be both reproducible and directly applicable to many other medical HG systems.This evaluation results in an ROC AUC of 0.834, and when limiting the published set to only predicates occurring in papers that received over 100 citations, this rises to 0.874.Then, we generate hypotheses, using up-to-date training data, which attempt to connect HIV-associated neurodegenerative disease (HAND) to over 30,000 human genes.From there, we select the top 1,000 genes based on our ranking metrics as a large and rudimentary "candidate set."By performing laboratory experiments on select genes within our automatically generated set, we discover a new relation between HAND and Dead Box RNA Helicasee 3 (DDX3).Thus, demonstrating the practical utility of our proposed validation and ranking method.</p>
<p>II. TECHNICAL BACKGROUND</p>
<p>Extracting Information from Hypothesis Generation Systems Swanson and Smalheiser created the first HG system Arrowsmith [18], and in doing so outlined the ABC model for discovery [19].Although this approach has limitations [20], its conventions and intuitions remain in modern approaches [9].</p>
<p>In the ABC model, users run queries by specifying two keywords a and c.From there, the goal of a HG system is to discover some entity b such that there are known relationships "a → b" and "b → c," which allow us to infer the relationship between a and c.Because many connections may require more than one element b to describe, researchers apply other techniques, such as topic models in our case, to describe these connections.</p>
<p>We center this work around the MOLIERE HG system [6].Once a user queries a and c, the system identifies a relevant region within its multi-layered knowledge network, which consists of papers, terms, phrases, and various types of links.The system then extracts abstracts and titles from this region and creates a sub-corpus upon which we generate a topic model (Note that in [21] we address trade-offs of using full text).This topic model describes groups of related terms, which we study to understand the quality of the a-to-c connection.Previously, these results were compared biased on those words that cooccur with high probability in prominent topics.Without clear metrics, or a validation framework, experts could only help evaluate a select handful of a, c pairs.</p>
<p>Word and Phrase Embedding</p>
<p>The method of finding dense vector representations of words is often referred to as "word2vec."In reality, this umbrella term references two different algorithms, the Continuous BOW (CBOW) method and the Skip-Gram method [14].Both rely on shallow neural networks in order to learn vectors through word-usage patterns.</p>
<p>MOLIERE uses FastText [22], a similar tool under the word2vec umbrella, to find high-quality embeddings of medical entities.By preprocessing MEDLINE text with the automatic phrase mining technique ToPMine [23], we improve these embeddings while finding multi-word medical terms such as "lung cancer" or "benign tumor."We see in Figure 1 that FastText clusters similar biological terms, an observation we later leverage to derive a number of metrics.</p>
<p>Topic Models Latent Dirichlet Allocation (LDA) [15], the classical topic modeling method, groups keywords based on their document co-occurrence rates in order to describe the set of trends that are expressed across a corpus.A topic is simply a probability distribution over a vocabulary, and each document from the input corpus is assumed to be a mixture of these topics.For instance, a topic model derived from New York Times articles would likely find one topic containing words such as "computer," "website," and "Internet," while another topic may contain words such as "money," "market," and "stock."</p>
<p>In the medical domain, some use topic models to understand trends across scientific literature.We look for groupings of entities such as genes, drugs, and diseases, which we then analyze to find novel connections.While LDA is the classical algorithm, MOLIERE uses a parallel technique, PLDA+ [24] to quickly find topics from documents related to a and c.Additionally, because MOLIERE preprocess's MEDLINE articles with ToPMine, its resulting topic models include both words and phrases.This often leads to more interpretable results, as a topic containing an n-gram, such as "smoking induced asthma," is typically easier to understand than a topic containing each unigram listed separately with different probabilities.</p>
<p>We additionally can use the probabilities of each word to represent a topic within an embedding space created with word2vec.For instance, we can take a weighted average over the embeddings for each topic to describe each topics's "center."Additionally, we can simply treat each topic as a weighted point cloud for the purposes of typical similarity metrics.We leverage both representations later in our metrics.</p>
<p>III. VALIDATION METHODOLOGY</p>
<p>In order to unyoke automatic HG from expert analysis, we propose a method that any system can leverage, provided it can rank its proposed connections.A successful system ought to rank published connections higher than those we randomly created.We train a system given historical information, and create the "published," "highly-cited," and "noise" query sets.We pose these connections to an HG system, and rank its outputs in order to plot ROC curves, which determine whether published predicates are preferred to noise.Through the area under these ROC curves, a HG system demonstrates its quality at a large scale without expert analysis.</p>
<p>Our challenge starts with the Semantic Medical Database (SemMedDB) [17] that contains predicates extracted from MEDLINE defined on the set of UMLS terms [16].For instance, predicate "C1619966 TREATS C0041296" represents a discovered fact "abatacept treats tuberculosis."Because MOLIERE does not account for word order or verb, we look for distinct unordered word-pairs a-c instead.In Section VIII, we discuss how we may improve MOLIERE to include this unused information.</p>
<p>From there, we select a "cut year."Using the metadata associated with each predicate, we note the date each unordered pair was first published.For this challenge, we train MOLIERE using only information published before the cut year.We then identify the set of SemMedDB unordered pairs a-c first published after the cut year provided a and c both occur in that year's UMLS release.This "published set" of pairs represent new connections between existing entities, from the perspective of the HG system.We select 2010 as the cut year for our study in order to create a published set of over 1 million pairs.(Due to practical limitations, our evaluation consists of a randomly chosen subset of 4,319 pairs.)</p>
<p>Additionally, we create a set of "highly-cited" pairs by filtering the published set by citation count.We use data from SemMedDB, MEDLINE, and Semantic Scholar to identify 1,448 pairs from the published set that first occur in a paper cited over 100 times.We note that this set is closer to the number of landmark discoveries since the cut-date, given that the published set is large and likely contains incidental or incorrect connections.</p>
<p>To provide negative examples, we generate a "noise set" of pairs by sampling the cut-year's UMLS release, storing the pair only if it does not occur in SemMedDB.These pairs represent nonsensical connections between UMLS elements.Although it is possible that we may stumble across novel findings within the noise set, we assume this will occur infrequently enough to not affect our results.We generate two noise pair sets of equal size to both the published and highlycited sets.</p>
<p>We run a-c queries from each set through MOLIERE and create two ranked lists: published vs. noise (PvN) (8,638 total pairs) and highly-cited vs. noise (HCvN) (2,896 total pairs).After ranking each set, we generate ROC curves [25], which allow us to judge the quality of an HG system.If more published predicates occur earlier in the ranking than noise, the ROC area will be close to 1; otherwise it will be closer to 0.5.</p>
<p>IV. NEW RANKING METHODS FOR TOPIC MODEL DRIVEN HYPOTHESES</p>
<p>Because many HG systems do not currently produce a ranking criteria, such as those systems that instead return topic models [6], [5], we propose here a number of metrics to numerically evaluate the plausibility of potential connections.We implement these metrics within MOLIERE [6].This system is open source, and already leverages word embeddings in order to produce topic model output for potential connectionsall of which are properties our metrics exploit.Put simply, MOLIERE takes as input two keywords (a and c), and produces a topic model (T ) that describes the structure of relevant documents.</p>
<p>While these metrics are proposed in the context of validation, another extremely important use case is that of the one-tomany query.Often during candidate selection, scientists may Fig. 2. The above depicts two queries, a-c 1 and a-c 2 , where a-c 1 is a published connection and a-c 2 is a noise connection.We see topics for each query represented as diamonds via CENTR(T i ).Although both queries lead to topics which are similar to a, c 1 , or c 2 , we find that the the presence of some topic which is similar to both objects of interest may indicate the published connection.</p>
<p>have a large list of initial potential targets -such as 30,000 genes in the human genome -that they wish to consider.For this, one may run a large set of queries between some disease a, and each target c i .However, without a ranking criteria, the analysis of each a-c i connection is left to experts, which is untenable for most practical purposes.</p>
<p>To begin, we note a key intuition underpinning the following metrics, depicted in Figure 2.Not only are related objects grouped in a word embedding space, but the distances between words are also meaningful.For this reason we hypothesize, and later show through validation experiments, that one can estimate the strength of an a-c connection by comparing the distance of topics to the embeddings of each a, c, and their midpoint.Note, we use (x) to map a text object x into this embedding space, as described in [14].But, because not all hypotheses or topic models exhibit the same features, we quantify this "closeness" in eleven ways, and then train a polynomial to weight the relevance of each proposed metric.</p>
<p>A. Similarity Between Query Words</p>
<p>As a baseline, we first consider two similarity metrics that do not include topic information: cosine similarity (CSIM) and Euclidean distance (L 2 ):
CSIM(a, c) = (a) • (c) || (a)|| 2 ×|| (c)|| 2 , L 2 (a, c) = || (a) − (c)|| 2 ,
where a and c are the two objects of interest, and (x) is an embedding function (see Section II).Note that when calculating ROC curves for the L 2 metric, we will sort in reverse, meaning smaller distances ought to indicate published predicates.</p>
<p>These metrics indicate whether a and c share the same cluster with respect to the embedding space.Our observation is that this can be a good indication that a and c are of the same kind, or are conceptually related.This cluster intuition is shared by others studying similar embedding spaces [26].</p>
<p>B. Topic Model Correlation</p>
<p>The next metric attempts to uncover whether a and c are mutually similar to the generated topic model.This metric starts by creating vectors v(a, T ) and v(c, T ) which express each object's similarity to topic model T = {T i } k i=1 derived from an a − c query.We do so by calculating the weighted cosine similarity TOPICSIM(x, T i ) between each topic T i and each object x ∈ {a, c}, namely,
TOPSIM(x, T i ) = (w,p)∈Ti p • CSIM(x, w),
where a probability distribution over terms in T i is represented as word-probability pairs (w, p).This metric results in a value in the interval [-1, 1] to represent the weighted similarity of x with T i .The final similarity vectors v(a, T ) and v(c, T ) in R k are defined below.
∀x ∈ {a, c} v(x, T ) =      TOPSIM(x, T 1 ) TOPSIM(x, T 2 ) . . . TOPSIM(x, T k )     
Finally, we can see how well T correlates with both a and c by taking another cosine similarity
TOPICCORR(a, c, T ) = v(a, T ) • v(c, T ) ||v(a, T )|| 2 ×||v(c, T )|| 2 ∈ [−1, 1].
If TOPICCORR(a, c, T ) is close to 1, then topics that are similar or dissimilar to a are also similar or dissimilar to c.Our preliminary results show that if some explanation of the a−c connection exists within T , then many T i ∈ T will likely share these similarity relationships.</p>
<p>C. Similarity of Best Topic Centroid</p>
<p>While the above metric attempts to find a trend within the entire topic model T , this metric attempts to find just a single topic T i ∈ T that is likely to explain the a−c connection.This metric is most similar to that depicted in Figure 2.Each T i is represented in the embedding space by taking a weighted centroid over its word probability distribution.We then rate each topic by averaging its similarity with both queried words.The score for the overall hypothesis is simply the highest score among the topics.</p>
<p>We define the centroid of T i as
CENTR(T i ) = (w,p)∈Ti (w) • p,
and then compare it to both a and c through cosine similarity and Euclidean distance.When comparing with CSIM, we highly rank T i 's with centroids located within the arc between (a) and (c).Because our embedding space identifies dimensions that help distinguish different types of objects, and because we trained a 500-dimensional embedding space, cosine similarity intuitively finds topics that share similar characteristics to both objects of interests.We define the best centroid similarity for CSIM as
BESTCENTRCSIM(a, c, T ) = max Ti∈T CSIM(a, T i ) + CSIM(c, T i ) 2 .
What we lose in the cosine similarity formulation is that clusters within our embedding space may be separate with respect to Euclidean distance but not cosine similarity.In order to evaluate the effect of this observation, we also formulate the best centroid metric with L 2 distance.In this formulation we look for topics that occur as close to the midpoint between (a) and (c) as possible.We express this score as a ratio between that distance and the radius of the sphere with diameter from (a) to (c).In order to keep this metric in a similar range to the others, we limit its range to [0, 1], namely, for the midpoint m = ( (a) + (c))/2.
BESTCENTRL 2 (a, c, T ) = max Ti∈T 1 − CENTR(T i ) − m 2 m 2</p>
<p>D. Cosine Similarly of Best Topic Per Word</p>
<p>In a similar effort to the above centroid-based metric, we attempt to find topics which are related to a and c, but this time on a per-word (or phrase) basis using TOPICSIM(x, T i ) from Section IV-B.Now instead of looking across the entire topic model, we attempt to identify a single topic which is similar to both objects of interest.We do so by rating each topic by the lower of its two similarities, meaning the best topic overall will be similar to both query words.</p>
<p>BESTTOPPERWORD(a, c, T ) = max</p>
<p>Ti∈T min TOPSIM(a, T i ), TOPSIM(c, T i )</p>
<p>E. Network of Topic Centroids</p>
<p>A majority of the above metrics rely on a single topic to describe the potential connection between a and c, but as Smalheizer points out in [27], a hypothesis may be best described as a "story" -a series of topics in our case.To model semantic connections between topics, we induce a nearest-neighbors network N from the set of vectors V = (a) ∪ (b) ∪ {CENTR(T i )|T i ∈ T } which form the set of nodes for N .In this case, we set the number of neighbors per node to the smallest value (that may be different for each query) such that there exists a path from a to c.Using this topic network, we attempt to model the semantic differences between published and noise predicates using network analytic metrics.</p>
<p>We depict two such networks in Figure 3, and observe that the connectivity between a and c from a published predicate is substantially stronger and more structured.In order to quantify this observed difference, we measure the average betweenness and eigenvector centrality [28] of nodes along a shortest path from a to c (denoted by a ∼ c) within N to reflect possible information flow between T i ∈ T .This shortest path represents the series of links between key concepts present within our dataset that one might use to explain the relationship between a and c.We expect the connection linking a and c to be stronger if that path is more central to the topic network.Below we define metrics to quantify the differences in these topic networks.Such network analytic metrics are widely applied in semantic knowledge networks [29].TOPWALKLENGTH(a, c, T ): Length of shortest path a ∼ c TOPWALKBTWN(a, c, T ): Avg. a ∼ c betweenness centrality TOPWALKEIGEN(a, c, T ): Avg. a ∼ c eigenvalue centrality Fig. 3. Above depicts two topic networks as described in Section IV-E.In this visualization, longer edges correspond to dissimilar neighbors.In red are objects a and c, which we queried to create these topic models.We observe that the connectivity between a and c from the published predicate is much higher than in the noisy example.</p>
<p>TOPNETCCOEF(a, c, T ): Clustering coefficient of N TOPNETMOD(a, c, T ): Modularity of N</p>
<p>F. Combination of Multiple Metrics</p>
<p>Each of the above methods are based on different assumptions regarding topic model or embedding space properties exhibited by published connections.To leverage each metric's strengths, we combined the top performing ones from each category into the following POLYMULTIPLE method.We explored polynomial combinations in the form of i α i x βi i for ranges of α i ∈ [−1, 1] and β i ∈ [1, 3] after scaling each x i to the [0, 1] interval.Through a blackbox optimization technique, we searched over one-million parameter combinations.In doing so we maximize for the AUC of our validation curve by sampling each α i and β i from their respective domains.We perform this search stochastically, sampling from parameter space and limiting our search space as we find stable local-minima.Our results represent the best parameter values determined after one-million parameter samples.</p>
<p>POLYMULTIPLE(a, c, T
) = α 1 •L β1 2 +α 2 •BESTCENTERL β2 2 +α 3 •BESTTOPPERWORD(a, c, T ) β3 +α 4 •TOPCORR(a, c, T ) β4 α 5 •TOPWALKBTWN(a, c, T ) β5 +α 6 •TOPNETCCOEF(a, c, T ) β6</p>
<p>V. RESULTS AND LESSONS LEARNED</p>
<p>As described in Section III, our goal is to distinguish publishable connections from noise.We run MOLIERE to generate topic models related to published, noise, and highlycited pairs.Using this information, we plot ROC curves in Figures 4 and 5, and summarize the results in Table I.These plots represent an analysis of 8,638 published vs. noise (PvN) pairs and 2,896 (HCvN) pairs (half of each set are noise).Unfortunately, no alternative general-purpose query HG systems that perform in a reasonable time are freely available for the comparison with our ranking methods.Topic Model Correlation metric (see Section IV-B) is a poorly performing metric with an ROC area of 0.609 (PvN) and 0.496 (HCvN).The core issue of this method is its sensitivity to the number of topics generated, and given that we generate 100 topics per pair, we likely drive down performance through topics which are unrelated to the query.In preliminary testing, we observe this intuition for queries with only 20 topics, but also find the network-biased metrics are less meaningful.In Section VIII we overview a potential way to combine multiple topic models in our analysis.</p>
<p>Surprisingly, this metric is less able to distinguish highlycited pairs, which we suppose is because highly-cited connections often bridge very distant concepts [30] and likely results in more noisy topic models.Additionally, we may be able to limit this noise by tuning the number of topics returned from a query, as described in Section VIII.L 2 -based metrics exhibit even more surprising results.BESTCENTRL 2 performs poorly, with an ROC area of 0.578 (PvN) and 0.587 (HCvN), while the much simpler L 2 metric is exceptional, scoring a 0.783 (PvN) and 0.809 (HCvN).We note that if two words are related, they are more likely to be closer together in our vector space.We evaluate topic centroids based on their closeness to the midpoint between a and c, normalized by the distance between them, so if that distance is small, the radius from the midpoint is small as well.Therefore, it would seem that the distance between a and c is a better connection indication, and that the result of the centroid measurement is worse if this distance is small.CSIM-based metrics are more straightforward.The simple CSIMmetric scores a 0.709 (PvN) and 0.703 (HCvN), which is interestingly consistent given that the L 2 metric increases in ROC area given highly-cited pairs.The BESTTOPICPER-WORD metric only scores a 0.686 (PvN), but increases substantially to 0.731 (HCvN).The topic centroid method BESTCENTROIDCSIM is the best cosine-based metric with an ROC area of 0.719 (PvN) and 0.742 (HCvN).This result is evidence that our initial hypothesis described in Figure 2 holds given cosine similarity, but as stated above, does not hold for Euclidean distance.Topic network metrics are all outperformed by simple L 2 , but we see interesting properties from their results that help users to interpret generated hypotheses.For instance, we see that TOPICWALKBTWN is a negative indicator while TOP-ICWALKEIGEN is positive.Looking at the example in Figure 3 we see that a and c are both far from the center of the network, connected to the rest of the topics through a very small number of high-betweenness nodes.In contrast, we see that in the network created from a published pair, the path from a to c is more central.We also see a denser clustering for the noise pair network, which is echoed by the fact that TOPICNETCCOEF and TOPICNETMOD are both negative indicators.Lastly, we see that TOPICWALKLENGTH performs the best out of these network approaches, likely because it is most similar to the simple L 2 or CSIM metrics.Combination of metrics, POLYMULTIPLE, significantly outperforms all others with ROC areas of 0.834 (PvN) and 0.874 (HCvN).This is unsurprising because each other metric makes a different assumption about what sort of topic or vector configuration best indicates a published pair.When each is combined, we see not only better performance, but their relative importances.By studying the coefficients of our polynomial we observe that the two L 2 -based metrics are most important, followed by the topic network methods, and finally by TOPICWALKCORR and BESTTOPICPERWORD.Unsurprisingly, the coefficient signs correlate directly with whether each metric is a positive or negative indication as summarized in Table I.Additionally, the ordering of importance roughly follows the same ordering as the ROC areas.</p>
<p>VI. CASE-STUDY: HAND AND DDX3 CANDIDATE SELECTION</p>
<p>Our proposed validation method is rooted in the process of candidate selection.To demonstrate our method's applicability to real-world scenarios, we applied the above methods to a series of queries surrounding Human Immunodeficiency Virus -associated dementia (or HIV-associated neurodegenerative disease, HAND).HAND is one of the most common and clinically important complications of HIV infection [31].The brain-specific effects of HIV are of great concern because the HIV-infected population is aging and unfortunately revealing new pathologies [32], [33].About 50% of HIV-infected patients are at risk of developing HAND, which might be severely worsened by abusing drugs such as cocaine, opioids and amphetamines [34], [35].</p>
<p>We generated over 30,000 queries, each between HAND and a gene from the HUGO Gene Nomenclature Committee dataset [36].The network that generated these results consisted of the 2017 MEDLINE dataset, the 2017AB UMLS release, and the 2016 SemMedDB release (latest at the time).We trained FastText using all of the available titles and abstracts, about 27 million in total, and selected a dimensionality of 500 for our word embeddings.Our results consist of each diseasegene query ranked by our POLYMULTIPLE metric.</p>
<p>Based on this ranking we select the first ˜1000 genes for further analysis.We observe that many of the top genessuch as APOE-4, T-TAU, and BASE1, which occur in our top five -are known to be linked to dementia.So to direct our search to yet-unknown connections, we select those genes that have no previous connection to HAND, but still ranked highly overall.This process limits our search to those proteins Fig. 4. The above ROC curves show the ability for each of our proposed methods to distinguish the MOLIERE results of published pairs from noise.We use our system to generate hypotheses regarding 8,638 pairs, half from each set, on publicly available data released prior to 2,015.We only show the best performing metrics from Section IV-E for clarity.Fig. 5.The above ROC curves show the ability for each of our proposed methods to distinguish the MOLIERE results of highly-cited pairs from noise.We identify 1,448 pairs who first occur in papers with over 100 citations published after our cut date.To plot the above ROC curve, we also select an random subset of equal size from the noise pairs.that have known selective compounds, which were often tested animal models or clinical trials.</p>
<p>From this candidate set we selected Dead Box RNA Helicase 3 (DDX3).We tested the activity of a DDX3 inhibitor on the tissue culture model of HAND, which is widely used for the analysis combine neurotoxicity of HIV proteins and drugs of abuse.Here we tested the effect of the DDX3 inhibition on combined toxicity of most toxic HIV protein, Trans-Activator of Transcription (Tat).The mouse cortical neurons had been treated with HIV Tat followed by the addition of cocaine.The combination of Tat and cocaine kills more than 70% of the neurons, while the inhibitor protects the neurons from Tat/cocaine toxicity (Figure 6).</p>
<p>Based on the analysis, we formulate following hypothesis: Exposing neurons with Tat protein causes internal stress and results in the formation of Stress-Granules (SGs)the structures in cytoplasm formed by multiple RNAs and proteins.These gel-like structures sequester cellular RNA from translation, and the formation of SGs requires enzymatically active Dead Box RNA Helicase 3. The formation of SGs also allows the neurons to wait out the stress.However, prolonged stress associated with HIV-Tat treatment leads to the formation of pathological stress granules, which are denser and have a different composition relative to "normal" ones.Additional exposure to cocaine further exaggerates the "pathological" SGs and eventually causes neuronal death.The hypothesis, initially generated with MOLIERE, led to the following finding: Treatment with a DDX3-specific inhibitor blocks the enzymatic activity of the DDX3.This lack of enzymatic activity, in turn, blocks Tat-dependent stress granules from formating and protects neurons from the combined toxicity of Tat and cocaine.In Figure 6, we demonstrate the hypothesis scheme.Thus, the application of the automated HG system pointed to a new avenue for anti-HAND therapy and to the prototype of a small molecule for drug development.</p>
<p>VII. RELATED WORK AND PROPOSED VALIDATION</p>
<p>The HG community struggles to validate its systems in a number of ways.Yetisgen-Yildiz and Pratt, in their chapter "Evaluation of Literature-Based Discovery Systems," outline four such methods (M1-M4) [10], [37].M1: Replicate Swanson's Experiments.Swanson, during his development of ARROWSMITH [18], worked alongside medical researchers to uncover a number of new connections.These connections include the link between Raynaud's Disease and Fish Oil [3], the link between Alzheimer's Disease and Estrogen [38] and the link between Migraine and Magnesium [39].As discussed in [37], a number of projects have centered their validation effort around Swanson's results [40], [41], [42], [43], [44].These efforts always rediscover a number of findings using information before Swanson's discovery date, and occasionally apply additional metrics such as precision and recall in order to quantify their results [25].</p>
<p>While limiting discussion to Swanson's discoveries reduces the domain of discovery drastically, at its core this method builds confidence in a new system through its ability to find known connections.We expand on this idea by validating automatically and on a massive scale, freeing our discourse from a single researcher's findings.M2: Statistical Evaluation.Hristovski et al. validate their system by studying a number of relationships and note their confidence and support with respect to the MEDLINE document set [45].Then, they can generate potential relationships for the set of new connections added to UMLS [46] or OMIM [47].By limiting their method to association rules, Hristovski et al. note that they can validate their system by predicting UMLS connections using data available prior to their publications.Therefore, this method is similar to our own, but we notice that restricting discussion to only UMLS gene-disease connections results in a much smaller set than the predicate information present with SemMedDB.</p>
<p>Pratt et al. provide additional statistical validation for their system LitLinker [44].This method also calculates precision and recall, but this time focusing on their B-set of returned results.Their system, like ARROWSMITH [18], returns a set of intermediate terms which may connect two queried entities.Pratt et al. run LitLinker for a number of diseases on which they establish a set of "gold standard" terms.Their method is validated based on its ability to list those gold-standard terms within its resulting B-sets.This approach requires careful selection of a (typically small) set of gold-standard terms, and is limited to "ABC" systems like ARROWSMITH, which are designed to identify term lists [20].M3: Incorporating Expert Opinion.This ranges from comparisons between system output and expert output, such as the analysis done on the Manjal system [42], to incorporating expert opinion into gold-standard terms for LitLinker [44], to running actual experiments on potential results by Wren et al. [48].Expert opinion is at the heart of many recent systems [5], [6], [7], [8], including the previous version of our own.This process is both time consuming and risks introducing significant bias into the validation.</p>
<p>Spangler incorporates expert knowledge in a more sophisticated manner through the use of visualizations [9], [49].This approach centers around visual networks and ontologies produced automatically, which allows experts to see potential new connections as they relate to previously established information.This view is shared by systems such as DiseaseConnect [7] which generates sub-networks of ONIM and GWAS related to specific queries.Although these visualizations allow users to quickly understand query results, they do not lend themselves to a numeric and massive evaluation of system performance.</p>
<p>BioCreative, a set of challenges focused on assessing biomedical text mining, is the largest endeavor of its kind, to the best of our knowledge [50].Each challenge centers around a specific task, such as mining chemical-protein interactions, algorithmically identifying medical terms, and constructing causal networks from raw text.Although these challenges are both useful and important, their tasks fall under the umbrella of information retrieval (and not HG) because their tasks compare expert analysis with software results given the same text.M4: Publishing in the Medical Domain.This method is exceptionally rare and expensive.The idea is to take prevalent potential findings and pose them to the medical research community for another group to attempt.Swanson and Smalheiser rely on this technique to solidify many of their early results, such as that between magnesium deficiency and neurologic disease [51].</p>
<p>Bakkar et al. take a similar approach in order to demonstrate the efficacy of Watson for Drug Discovery [4], [49] To do so, this work begins by identifying 11 RNA-binding proteins (RBPs) known to be connected to Amyotrophic Lateral Aclerosis (ALS).Then, the automated system uses a recommender system to select RPBs that exhibit similar connection patterns within a large document co-occurrence network.Domain scientists then explore a set of candidates selected by the computer system, and uncover five RPBs that were previously unrelated to ALS.</p>
<p>An alternative to the domain-scientist approach is taken by Soldatova and Rzhetsky wherein a "robot scientist" automatically runs experiments posed by their HG system [52], [53].This system uses logical statements to represent their hypotheses, so new ideas can be posed through a series of implications.Going further, their system even identifies statements that would be the most valuable if proven true [30].However, the scope of experiments that a robot scientist can undertake is limited; in their initial paper, the robot researcher is limited to small-scale yeast experiments.Additionally, many groups cannot afford the space and expense that an automated lab requires.</p>
<p>VIII. DEPLOYMENT CHALLENGES AND OPEN PROBLEMS</p>
<p>Validation Size.Our proposed validation challenge involves ranking millions of published and noise query pairs.However, in Section V we show our results on a randomly sampled subset of our overall challenge set.This was necessary due to performance limitations of MOLIERE, a system which initially required a substantial amount of time and memory to process even a single hypothesis.To compute these results, we ran 100 instances of MOLIERE, each on a 16 core, 64 GB RAM machine connected to a ZFS storage system.Unfortunately, performance limitations within ZFS created a bottleneck that both limited our results and drastically reduced cluster performance overall.Thus, our results represent a set of predicates that we evaluated in a limited time period.System Optimizations.While performing a keyword search, most network-centered systems are either I/O or memory bound simply because they must load and traverse large networks.In the case of MOLIERE, we initially spent hours trying to find shortest paths or nearby abstracts.But, we found a way to leverage our embedding space and our parallel file system in order to drastically improve query performance.In brief, one can discover a relevant knowledge-network region by inducing a subnetwork on a and c and expanding that selection by adding i th order neighbors until a and c are connected.From our experiments, i rarely exceeds 4.This increases performance because, given a parallel file system and p processors, identifying the subnetwork from an edge list file is in order O(ni/p).The overall effect reduced the wall-clock runtime of a single query from about 12 hours to about 5-7 minutes.Additionally, we reduced the memory requirement for a single query from over 400GB to under 16GB.Highly-Cited Predicates.Identifying highly-cited predicates requires that we synthesize information across multiple data sources.Although SemMedDB contains MEDLINE references for each predicate, neither contains citation information.For this, we turn to Semantic Scholar because not only do they track citations of medical papers, but they allow a free bulk download of metadata information (many other potential sources either provide a very limited API or none at all).In order to match Semantic Scholar data to MEDLINE citation, it is enough to match titles.This process allows us to get citation information for many MEDLINE documents, which in turn allow us to select predicates whose first occurrence was in highly-cited papers.We explored a number of thresholds for what constitutes "highly cited" and selected 100 because it was a round number and selected a sizable predicate set.Because paper citations follow a power-law distribution, any change drastically effects the size of this set.We note that the set of selected predicates was also limited by the quality of data in Semantic Scholar, and that the number of citations identified this was appeared to be substantially lower than that reported by other methods.Quality of Predicates.Through our above methods we learned that careful ranking methods can distinguish between published and noise predicates, but there is a potential inadequacy in this method.Potentially, some predicates that occur within our published say may be untrue.Additionally, it is possible that a noise predicate may be discovered to be true in the future.If MOLIERE ranks the published predicate which is untrue below the noise predicate which is, the result would be a lower ROC area.This same phenomena is addressed by Yetisgen-Yildiz and Pratt when they discuss the challenges present in validating literature-based discovery systems [37] -if a HG systems goal is to identify novel findings, then it should find different connections than human researchers.</p>
<p>We show through our results that despite an uncertain validation set, there are clearly core differences between publishable results and noise, which are evident at scale.Although there may be some false positives or negatives, we see through our meaningful ROC curves that they are far outnumbered by more standard predicates.Comparison with ABC Systems.Additionally, we would like to explore how our ranking methods apply to traditional ABC systems.Although there are clear limitations to these systems [20], many of the original systems such as AR-ROWSMITH follow the ABC pattern.These systems typically output a list of target terms and linking terms, which could be thought of as a topic.If we were to take a pre-trained embedding space, and treated a set of target terms like a topic, we could likely use our methods from Section IV to validate any ABC system.Verb Prediction.We noticed, while processing SemMedDB predicates, that we can improve MOLIERE if we utilize verbs.SemMedDB provides a handful of verb types, such as "TREATS," "CAUSES," or "INTERACTS WITH," that suggest a concrete relationship between the subject and object of a sentence.MOLIERE currently outputs a topic model that can be interpreted using our new metrics, but does not directly state what sort of connection may exist between a and c.Thus we would like to explore accurately predicting these verb types given only topic model information.</p>
<p>Interpretability of Hypotheses remains one of the major problems in HG systems.Although topic-driven HG partially resolve this issue by producing readable output, we still observe many topic models T (i.e., hypotheses) whose T i ∈ T are not intuitively connected with each other.While the proposed ranking is definitely helpful for understanding T , it still does not fully resolve the interpretability problem.One of our current research directions is to tackle it using text summarization techniques.Scope.While we focus on biomedical science, any field that is accurately described by entities that act on one another benefits from our network and text mining methods.For instance, economic entities, such as governments or the upper/lower class, interact via actions such as regulation or boycott.Similarly, patent law consists of inventions and the components that comprise them.Mathematics, in contrast, is not served by this representation -the algebra does not act on other math entities.Here automatic theorem proving is better equipped to generate hypotheses.We are presently unsure if the same is true for computer science.</p>
<p>Fig. 1 .
1
Fig. 1.The above diagram shows a 2-D representation of the embeddings for over 8 thousand UMLS keywords within MOLIERE.We used singular value decomposition to reduce the dimensionality of these vectors from 500 to 2.</p>
<p>Fig. 6 .
6
Fig. 6.Scheme of the hypothesis of Stress-Granule dependent mechanism of neuroprotection by DDX3 inhibitor.Neurons are curved figures.Treatment with HIV-Tat leads to DDX3-dependent formation of SGs (A), which transform from "normal" to "pathological" (B).The addition of cocaine further enlarges the SGs and leads to the death of the neurons (C).Treatment with DDX3 specific inhibitor blocks DDX3 enzymatic activity and Tat-dependent SG formation (D) and protects the neurons from cocaine-induced death (E).</p>
<p>Decision-making in product portfolios of pharmaceutical research and development-managing streams of innovation in highly regulated markets. A Jekunen, development and therapy. 820092014Drug design</p>
<p>Global scientific output doubles every nine years. R Van Noorden, Nature News Blog. 2014</p>
<p>Fish oil, raynaud's syndrome, and undiscovered public knowledge. D R Swanson, Perspectives in biology and medicine. 3011986</p>
<p>Artificial intelligence in neurodegenerative disease research: use of IBM Watson to identify additional RNA-binding proteins altered in amyotrophic lateral sclerosis. N Bakkar, T Kovalik, I Lorenzini, S Spangler, A Lacoste, K Sponaugle, P Ferrante, E Argentinis, R Sattler, R Bowser, Acta neuropathologica. 13522018</p>
<p>Finding complex biological relationships in recent pubmed articles using bio-lda. H Wang, Y Ding, J Tang, X Dong, B He, J Qiu, D J Wild, PloS one. 63e172432011</p>
<p>MOLIERE: Automatic Biomedical Hypothesis Generation System. J Sybrandt, M Shtutman, I Safro, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '17. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '17New York, NY, USAACM2017</p>
<p>Diseaseconnect: a comprehensive web server for mechanism-based disease-disease connections. C.-C Liu, Y.-T Tseng, W Li, C.-Y Wu, I Mayzus, A Rzhetsky, F Sun, M Waterman, J J Chen, P M Chaudhary, Nucleic acids research. 42W12014</p>
<p>Undiscovered public knowledge. D R Swanson, The Library Quarterly. 5621986</p>
<p>Accelerating Discovery: Mining Unstructured Information for Hypothesis Generation. S Spangler, 2015CRC Press37</p>
<p>Literature-based discovery. P Bruza, M Weeber, 2008Springer Science &amp; Business Media</p>
<p>Ohsumed: an interactive retrieval evaluation and new large test collection for research. W Hersh, C Buckley, T Leone, D Hickam, SIGIR94. Springer1994</p>
<p>Ensemble non-negative matrix factorization methods for clustering protein-protein interactions. D Greene, G Cagney, N Krogan, P Cunningham, Bioinformatics. 24152008</p>
<p>Signatures of mutational processes in human cancer. L B Alexandrov, S Nik-Zainal, D C Wedge, S A Aparicio, S Behjati, A V Biankin, G R Bignell, N Bolli, A Borg, A.-L Børresen-Dale, Nature. 50074632013</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.37812013</p>
<p>Latent dirichlet allocation. D M Blei, A Y Ng, M I Jordan, Journal of machine Learning research. 3Jan. 2003</p>
<p>PubMed. Coordinators Ncbi Resource, 2017</p>
<p>Semmeddb: a pubmed-scale repository of biomedical semantic predications. H Kilicoglu, D Shin, M Fiszman, G Rosemblat, T C Rindflesch, Bioinformatics. 28232012</p>
<p>Using arrowsmith: a computerassisted approach to formulating and assessing scientific hypotheses. N R Smalheiser, D R Swanson, Computer methods and programs in biomedicine. 5731998</p>
<p>An interactive system for finding complementary literatures: A stimulus to scientific discovery. D R Swanson, N R Smalheiser, 10.1016/S0004-3702(97)00008-8S0004-3702(97)00008-8Artif. Intell. 912Apr. 1997</p>
<p>Literature-based discovery: Beyond the ABCs. N R Smalheiser, Journal of the Association for Information Science and Technology. 6322012</p>
<p>Are abstracts enough for hypothesis generation. J Sybrandt, A Carrabba, A Herzog, I Safro, abs/1804.05942CoRR. 2018</p>
<p>A Joulin, E Grave, P Bojanowski, M Douze, H Jégou, T Mikolov, arXiv:1612.03651Fasttext.zip: Compressing text classification models. 2016arXiv preprint</p>
<p>Scalable topical phrase mining from text corpora. A El-Kishky, Y Song, C Wang, C R Voss, J Han, Proceedings of the VLDB Endowment. the VLDB Endowment20148</p>
<p>Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing. Z Liu, Y Zhang, E Y Chang, M Sun, ACM Transactions on Intelligent Systems and Technology (TIST). 23262011</p>
<p>J Han, J Pei, M Kamber, Data mining: concepts and techniques. Elsevier2011</p>
<p>Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification. P Wang, B Xu, J Xu, G Tian, C.-L Liu, H Hao, Neurocomputing. 1742016</p>
<p>[rediscovering don swanson: The past, present and future of literaturebased discovery. </p>
<p>Networks: an introduction. M Newman, 2010Oxford university press</p>
<p>Principles of semantic networks: Explorations in the representation of knowledge. J F Sowa, 2014Morgan Kaufmann</p>
<p>Choosing experiments to accelerate collective discovery. A Rzhetsky, J G Foster, I T Foster, J A Evans, Proceedings of the National Academy of Sciences. 112472015</p>
<p>Neuronal toxicity in hiv cns disease. J Kovalevich, D Langford, Future virology. 772012</p>
<p>Hiv and neurocognitive dysfunction. S Spudich, Current HIV/AIDS Reports. 1032013</p>
<p>Neurologic diseases in hiv-infected patients. M Bilgrami, P Okeefe, Handbook of clinical neurology. Elsevier2014121</p>
<p>Epidemiologic links between drug use and hiv epidemics: an international perspective. C Beyrer, A L Wirtz, S Baral, A Peryskina, F Sifakis, JAIDS Journal of Acquired Immune Deficiency Syndromes. 552010</p>
<p>Cocaine and hiv-1 interplay in cns: cellular and molecular mechanisms. S Buch, H Yao, M Guo, T Mori, B Mathias-Costa, V Singh, P Seth, J Wang, T.-P Su, Current HIV research. 1052012</p>
<p>Hgnc database. Jan 2017</p>
<p>Evaluation of literature-based discovery systems. M Yetisgen-Yildiz, W Pratt, Literature-based discovery. Springer2008</p>
<p>Linking estrogen to alzheimer's disease an informatics approach. N R Smalheiser, D R Swanson, Neurology. 4731996</p>
<p>Migraine and magnesium: eleven neglected connections. D R Swanson, Perspectives in biology and medicine. 3141988</p>
<p>Inferring undiscovered public knowledge by using text mining-driven graph model. G E Heo, K Lee, M Song, Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics. the ACM 8th International Workshop on Data and Text Mining in Bioinformatics2014</p>
<p>Automatically identifying candidate treatments from existing medical literature. C Blake, W Pratt, AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases. 2002</p>
<p>Text mining: generating hypotheses from MEDLINE. P Srinivasan, Journal of the American Society for Information Science and Technology. 5552004</p>
<p>A semantic-based approach for mining undiscovered public knowledge from biomedical literature. X Hu, G Li, I Yoo, X Zhang, X Xu, Granular Computing, 2005 IEEE International Conference on. IEEE20051</p>
<p>Litlinker: capturing connections across the biomedical literature. W Pratt, M Yetisgen-Yildiz, Proceedings of the 2nd international conference on Knowledge capture. the 2nd international conference on Knowledge captureACM2003</p>
<p>Using literature-based discovery to identify disease candidate genes. D Hristovski, B Peterlin, J A Mitchell, S M Humphrey, International journal of medical informatics. 7422005</p>
<p>UMLS reference manual. 2009</p>
<p>Online mendelian inheritance in man (omim), a knowledgebase of human genes and genetic disorders. A Hamosh, A F Scott, J S Amberger, C A Bocchini, V A Mckusick, Nucleic acids research. 33Database2005</p>
<p>Knowledge discovery by automated identification and ranking of implicit relationships. J D Wren, R Bekeredjian, J A Stewart, R V Shohet, H R Garner, Bioinformatics. 2032004</p>
<p>Automated hypothesis generation based on mining scientific literature. S Spangler, A D Wilkins, B J Bachman, M Nagarajan, T Dayaram, P Haas, S Regenbogen, C R Pickering, A Comer, J N Myers, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACM2014</p>
<p>Overview of biocreative: critical assessment of information extraction for biology. L Hirschman, A Yeh, C Blaschke, A Valencia, BMC Bioinformatics. 61S1May 2005</p>
<p>. 10.1186/1471-2105-6-S1-S1</p>
<p>Assessing a gap in the biomedical literature: Magnesium deficiency and neurologic disease. N Smalheiser, D Swanson, Neuroscience research communications. 1511994</p>
<p>Representation of research hypotheses. L N Soldatova, A Rzhetsky, Journal of biomedical semantics. 22S92011</p>
<p>The big mechanism program: Changing how science is done. A Rzhetsky, 2016</p>            </div>
        </div>

    </div>
</body>
</html>
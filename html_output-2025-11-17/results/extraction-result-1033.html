<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1033 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1033</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1033</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-252992562</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.10243v2.pdf" target="_blank">CLUTR: Curriculum Learning via Unsupervised Task Representation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement Learning (RL) algorithms are often known for sample inefficiency and difficult generalization. Recently, Unsupervised Environment Design (UED) emerged as a new paradigm for zero-shot generalization by simultaneously learning a task distribution and agent policies on the generated tasks. This is a non-stationary process where the task distribution evolves along with agent policies; creating an instability over time. While past works demonstrated the potential of such approaches, sampling effectively from the task space remains an open challenge, bottlenecking these approaches. To this end, we introduce CLUTR: a novel unsupervised curriculum learning algorithm that decouples task representation and curriculum learning into a two-stage optimization. It first trains a recurrent variational autoencoder on randomly generated tasks to learn a latent task manifold. Next, a teacher agent creates a curriculum by maximizing a minimax REGRET-based objective on a set of latent tasks sampled from this manifold. Using the fixed-pretrained task manifold, we show that CLUTR successfully overcomes the non-stationarity problem and improves stability. Our experimental results show CLUTR outperforms PAIRED, a principled and popular UED method, in the challenging CarRacing and navigation environments: achieving 10.6X and 45\% improvement in zero-shot generalization, respectively. CLUTR also performs comparably to the non-UED state-of-the-art for CarRacing, while requiring 500X fewer environment interactions.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1033.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1033.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLUTR (CarRacing student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Learning via Unsupervised Task Representation Learning — Student (CarRacing domain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pixel-based continuous-control RL student (protagonist) trained under CLUTR's curriculum; uses PPO for policy learning and benefits from a pretrained recurrent VAE task manifold produced by CLUTR's teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLUTR Student (Protagonist)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A simulated driving agent trained with Proximal Policy Optimization (PPO) in a pixel-based CarRacing environment; the curriculum is produced by a CLUTR teacher that samples latent task vectors from a pretrained recurrent VAE decoder and optimizes a regret objective.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CarRacing (Bézier-curve tracks; F1 benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A pixel-based closed-loop racing task where each track is generated by up to 12 Bézier control points encoded as integers on a 10x10 grid; observations are 96×96×3 RGB images, actions are 3D continuous (steer, gas, brake), dense reward (1000/L per new polygon visited) with step penalty; test F1 benchmark contains 20 real-world modeled tracks that are out-of-distribution relative to the 12-control-point generator.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Track representation complexity: up to 12 control points (integer sequence length 12), number of track polygons L (reward scaled by 1000/L), high-dimensional pixel observations (96×96×3), continuous 3D action space; complexity operationalized via track geometry and visual richness.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (pixel-input continuous control; up to 12 control points; complex track geometry; OOD F1 tracks are more complex than generated training tracks)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Task variation measured by number of procedurally-generated track instances used for training and VAE pretraining (1,000,000 random tracks for VAE), and by the F1 benchmark size (20 held-out test tracks representing distribution shift).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (1M random tracks used to train VAE; 20 significantly out-of-distribution test tracks in F1 benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic return on test tracks (zero-shot generalization returns); also sample-efficiency measured in environment timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mean return (flexible regret): 468 ± 21; Mean return (standard regret): 276 ± 24; Compared to PAIRED: CLUTR achieves 10.6× higher returns (standard regret) and ≈82% higher returns (flexible regret); CLUTR (flexible) is comparable to attention-based SOTA while requiring ~500× fewer environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed qualitatively: CLUTR's latent manifold lets the teacher navigate task variation more stably, producing curricula that span task difficulty and avoid over-simplification; simultaneous learning of task manifold and curriculum (high non-stationarity) degrades performance, so decoupling (pretrained fixed VAE) improves stability and generalization. The paper frames a trade-off: adaptive generators that must learn the manifold while generating curricula face instability when environment complexity and variation evolve together.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via unsupervised latent task-representation pretraining (recurrent VAE) plus regret-optimizing teacher (CLUTR); student trained with PPO on generated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot generalization to 20 unseen F1 tracks: CLUTR (flexible regret) achieves strong generalization, outperforming PAIRED on all 20 tracks (and matching/outperforming attention-based SOTA on 9/20 tracks); CLUTR shows substantially better generalization despite far fewer environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>VAE: 1M randomly generated tracks (1M gradient steps). Student/teacher training: flexible-regret experiments run for 2M timesteps (CLUTR and PAIRED); standard-regret experiments run for 5M timesteps. CLUTR claims ~500× fewer environment interactions than attention-based SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decoupling task representation (pretrained recurrent VAE) from curriculum learning yields more stable curriculum generation, higher sample efficiency, and much better zero-shot generalization in a high-complexity pixel-based continuous-control domain; finetuning the decoder with regret causes substantial performance drops; sorting task encodings reduces combinatorial explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTR: Curriculum Learning via Unsupervised Task Representation Learning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1033.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1033.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAIRED (CarRacing student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAIRED — Protagonist/Antagonist regret-maximizing curriculum (baseline; CarRacing domain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive-teacher UED baseline where a teacher RL agent constructs environment parameters to maximize student regret; student agents trained with standard RL on the generated parametrized tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent complexity and zero-shot transfer via unsupervised environment design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PAIRED Student (Protagonist)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated driving agent trained with PPO as part of the PAIRED multi-agent framework (protagonist vs antagonist), with a teacher policy that generates environment parameters sequentially and receives regret as reward.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CarRacing (Bézier-curve tracks; F1 benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same CarRacing setup as above: procedurally generated tracks from up to 12 Bézier control points, pixel observations, continuous 3D action space, dense per-polygon reward.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Track geometry complexity (up to 12 control points), pixel observation dimensionality (96×96×3), continuous control action space; teacher constructs environment parameter vector sequentially (combinatorial parameter permutations).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Training variation limited to teacher-generated parametrizations of control points; compared against CLUTR's VAE-based sampling of 1M tracks and F1 20-track test set.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high in principle, but in practice PAIRED struggled to explore effective regions of task space and often generated over-simplified curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic return on held-out F1 tracks (zero-shot generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mean return (PAIRED flexible regret): 257 ± 16; Mean return (PAIRED standard regret): ~26 ± 19 (very poor in standard-regret setting). PAIRED underperforms CLUTR substantially in both regret variants.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>PAIRED must learn the task manifold and curriculum simultaneously; paper reports this co-learning causes non-stationarity and instability, leading to poor curricula (e.g., over-simplified tasks) and degraded generalization. The combination of complex parametrization (high complexity) and large permutation-invariant parameter spaces exacerbates the combinatorial explosion for PAIRED.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive teacher curriculum learning (PAIRED) where the teacher is an RL agent trained to maximize regret; students trained with PPO on generated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Poor zero-shot generalization on F1 benchmark with standard regret; improved with flexible regret but still much worse than CLUTR (PAIRED flexible mean 257 vs CLUTR flexible 468). PAIRED often generates degenerate oversimplified tasks during training, harming learning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Same training budgets used for fair comparison: flexible-regret experiments 2M timesteps; standard-regret 5M timesteps. Original PAIRED results in prior work used far more timesteps (reported original baseline after 3B timesteps), indicating sample inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simultaneously learning task-manifold and curriculum creates severe instability and combinatorial challenges for PAIRED; PAIRED frequently generates overly-simple tasks during training and is sample-inefficient compared to CLUTR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTR: Curriculum Learning via Unsupervised Task Representation Learning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1033.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1033.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLUTR (MiniGrid student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Learning via Unsupervised Task Representation Learning — Student (MiniGrid navigation domain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A partially-observable grid-world navigation RL student (protagonist) trained under CLUTR; student trained with PPO and evaluated on zero-shot generalization to held-out navigation grids.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CLUTR Student (Protagonist)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A partially-observable MiniGrid navigation agent trained with PPO under a CLUTR-generated curriculum; the CLUTR teacher samples latent grid encodings from a pretrained recurrent VAE decoder and optimizes regret to generate training tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual grid-world)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MiniGrid partially-observable navigation (15×15 grid; 13×13 active area)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Discrete grid navigation tasks where each task is encoded as a sequence of up to 52 integers: first 50 denote obstacle locations (allowing duplicates to represent fewer obstacles), followed by goal and start locations; partially observable 5×5×3 local view plus direction; sparse reward (only upon reaching goal); number of obstacles varies from 0 to 50.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Grid complexity measured by number of obstacles (0–50), grid size effectively 13×13 active area (169 locations), partial observability (5×5 view + direction), and sparse reward structure (sensitive credit assignment).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium–high (sparse-reward, partial observability, up to 50 obstacles creates substantial combinatorial complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Task variation measured by number of procedurally-generated grid instances used to train VAE (flexible-regret: 10,000,000 random grids; standard-regret: 1,000,000 grids), plus the held-out testset of 16 unseen navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (10M VAE training grids for flexible experiments; wide obstacle-count distribution 0–50)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Solve rate (%) on 16 unseen navigation tasks (zero-shot generalization); also sample-efficiency (solve-rate over training timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Flexible regret: CLUTR solves 58% of unseen grids vs PAIRED 43% (≈35% relative improvement). Standard regret: CLUTR solves 64% vs PAIRED 44% (≈45% relative improvement). CLUTR outperforms PAIRED on 13/16 (flexible) and 14/16 (standard) test grids.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that CLUTR's pretrained latent manifold provides a neighborhood structure that helps the teacher navigate variations in task instances (obstacle placements) to produce smoother curricula; PAIRED often degenerates by generating either overly-hard or overly-simple tasks, indicating poor handling of the trade-off between complexity and variation. Sorting obstacle locations for VAE training reduces combinatorial explosion and improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning with pretrained recurrent VAE (latent-task manifold) + regret-optimizing teacher (CLUTR); students trained with PPO. VAE trained on sorted obstacle sequences to reduce permutation combinatorics.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot generalization to 16 held-out navigation tasks: CLUTR substantially improves solve rates over PAIRED (58% vs 43% flexible; 64% vs 44% standard), and converges faster (better sample efficiency). CLUTR generates a broader range of task difficulties during training rather than degenerating to trivial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training budgets: flexible-regret experiments 250M timesteps; standard-regret experiments 500M timesteps (5 independent runs). VAE pretraining used 10M random grids for flexible experiments and 1M for standard.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining a VAE on a large, sorted dataset of generated grids and freezing the decoder yields a latent task manifold that allows the teacher to produce more useful curricula, improving zero-shot generalization and sample efficiency; sorting VAE training data mitigates combinatorial explosion; allowing decoder finetuning reduces performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTR: Curriculum Learning via Unsupervised Task Representation Learning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1033.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1033.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAIRED (MiniGrid student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAIRED — Protagonist/Antagonist regret-maximizing curriculum (baseline; MiniGrid domain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline adaptive-teacher UED method where a sequential teacher RL agent constructs grid parameters to maximize the regret between antagonist and protagonist; used as a primary baseline in MiniGrid navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent complexity and zero-shot transfer via unsupervised environment design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PAIRED Student (Protagonist)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>MiniGrid navigation agent trained with PPO under PAIRED where the teacher sequentially generates parameter vectors (obstacle placements, goal, start) to maximize antagonist-protagonist regret; students and antagonist are standard RL agents.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual grid-world)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MiniGrid partially-observable navigation (15×15 grid; 13×13 active area)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as CLUTR MiniGrid description: up to 50 obstacles, sparse reward, partial observability; PAIRED's teacher operates on raw parameter sequences and suffers from permutation-invariance combinatorics unless special handling is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of obstacles (0–50), sequence length 52 (first 50 obstacles + goal + agent), partial observability, sparse rewards; teacher must generate sequences parameter-by-parameter, leading to long-horizon credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium–high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Teacher-generated variation over obstacle placements; compared against CLUTR's VAE-sampled variation (1M/10M datasets). PAIRED's practical exploration suffers from combinatorial explosion in the permutation-invariant parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high in principle but sample-inefficient in practice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Solve rate (%) on 16 unseen navigation tasks (zero-shot generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Flexible regret: PAIRED solves 43% of unseen grids vs CLUTR 58%. Standard regret: PAIRED solves 44% vs CLUTR 64%. PAIRED underperforms CLUTR across most test tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>PAIRED's simultaneous manifold-and-curriculum learning leads to non-stationarity and instability, making it struggle to find an effective balance across complexity and variation; it often collapses to overly-simple curricula or generates arbitrarily complex grids early that are unsolvable.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adaptive teacher (PAIRED) generating parameter sequences; students trained with PPO. No separate latent manifold pretraining in baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PAIRED shows inferior zero-shot generalization compared to CLUTR across the 16 held-out MiniGrid tasks and exhibits less sample-efficient learning and degenerate curricula behavior during training.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Experiments run for 250M (flexible) and 500M (standard) timesteps; original reported PAIRED baselines in prior work sometimes used far larger budgets (e.g., 3B timesteps) highlighting practical sample inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PAIRED's approach of jointly learning a task manifold while generating curricula leads to instability and combinatorial difficulties in permutation-invariant parameter spaces; it underperforms CLUTR when evaluated for zero-shot generalization and sample efficiency in MiniGrid.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLUTR: Curriculum Learning via Unsupervised Task Representation Learning', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>Replay-guided adversarial environment design <em>(Rating: 2)</em></li>
                <li>Prioritized Level Replay <em>(Rating: 2)</em></li>
                <li>Robust PLR <em>(Rating: 1)</em></li>
                <li>Domain Randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1033",
    "paper_id": "paper-252992562",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "CLUTR (CarRacing student)",
            "name_full": "Curriculum Learning via Unsupervised Task Representation Learning — Student (CarRacing domain)",
            "brief_description": "A pixel-based continuous-control RL student (protagonist) trained under CLUTR's curriculum; uses PPO for policy learning and benefits from a pretrained recurrent VAE task manifold produced by CLUTR's teacher.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CLUTR Student (Protagonist)",
            "agent_description": "A simulated driving agent trained with Proximal Policy Optimization (PPO) in a pixel-based CarRacing environment; the curriculum is produced by a CLUTR teacher that samples latent task vectors from a pretrained recurrent VAE decoder and optimizes a regret objective.",
            "agent_type": "simulated agent",
            "environment_name": "CarRacing (Bézier-curve tracks; F1 benchmark)",
            "environment_description": "A pixel-based closed-loop racing task where each track is generated by up to 12 Bézier control points encoded as integers on a 10x10 grid; observations are 96×96×3 RGB images, actions are 3D continuous (steer, gas, brake), dense reward (1000/L per new polygon visited) with step penalty; test F1 benchmark contains 20 real-world modeled tracks that are out-of-distribution relative to the 12-control-point generator.",
            "complexity_measure": "Track representation complexity: up to 12 control points (integer sequence length 12), number of track polygons L (reward scaled by 1000/L), high-dimensional pixel observations (96×96×3), continuous 3D action space; complexity operationalized via track geometry and visual richness.",
            "complexity_level": "high (pixel-input continuous control; up to 12 control points; complex track geometry; OOD F1 tracks are more complex than generated training tracks)",
            "variation_measure": "Task variation measured by number of procedurally-generated track instances used for training and VAE pretraining (1,000,000 random tracks for VAE), and by the F1 benchmark size (20 held-out test tracks representing distribution shift).",
            "variation_level": "high (1M random tracks used to train VAE; 20 significantly out-of-distribution test tracks in F1 benchmark)",
            "performance_metric": "Mean episodic return on test tracks (zero-shot generalization returns); also sample-efficiency measured in environment timesteps.",
            "performance_value": "Mean return (flexible regret): 468 ± 21; Mean return (standard regret): 276 ± 24; Compared to PAIRED: CLUTR achieves 10.6× higher returns (standard regret) and ≈82% higher returns (flexible regret); CLUTR (flexible) is comparable to attention-based SOTA while requiring ~500× fewer environment interactions.",
            "complexity_variation_relationship": "Discussed qualitatively: CLUTR's latent manifold lets the teacher navigate task variation more stably, producing curricula that span task difficulty and avoid over-simplification; simultaneous learning of task manifold and curriculum (high non-stationarity) degrades performance, so decoupling (pretrained fixed VAE) improves stability and generalization. The paper frames a trade-off: adaptive generators that must learn the manifold while generating curricula face instability when environment complexity and variation evolve together.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via unsupervised latent task-representation pretraining (recurrent VAE) plus regret-optimizing teacher (CLUTR); student trained with PPO on generated tasks.",
            "generalization_tested": true,
            "generalization_results": "Zero-shot generalization to 20 unseen F1 tracks: CLUTR (flexible regret) achieves strong generalization, outperforming PAIRED on all 20 tracks (and matching/outperforming attention-based SOTA on 9/20 tracks); CLUTR shows substantially better generalization despite far fewer environment interactions.",
            "sample_efficiency": "VAE: 1M randomly generated tracks (1M gradient steps). Student/teacher training: flexible-regret experiments run for 2M timesteps (CLUTR and PAIRED); standard-regret experiments run for 5M timesteps. CLUTR claims ~500× fewer environment interactions than attention-based SOTA.",
            "key_findings": "Decoupling task representation (pretrained recurrent VAE) from curriculum learning yields more stable curriculum generation, higher sample efficiency, and much better zero-shot generalization in a high-complexity pixel-based continuous-control domain; finetuning the decoder with regret causes substantial performance drops; sorting task encodings reduces combinatorial explosion.",
            "uuid": "e1033.0",
            "source_info": {
                "paper_title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "PAIRED (CarRacing student)",
            "name_full": "PAIRED — Protagonist/Antagonist regret-maximizing curriculum (baseline; CarRacing domain)",
            "brief_description": "An adaptive-teacher UED baseline where a teacher RL agent constructs environment parameters to maximize student regret; student agents trained with standard RL on the generated parametrized tracks.",
            "citation_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "mention_or_use": "use",
            "agent_name": "PAIRED Student (Protagonist)",
            "agent_description": "Simulated driving agent trained with PPO as part of the PAIRED multi-agent framework (protagonist vs antagonist), with a teacher policy that generates environment parameters sequentially and receives regret as reward.",
            "agent_type": "simulated agent",
            "environment_name": "CarRacing (Bézier-curve tracks; F1 benchmark)",
            "environment_description": "Same CarRacing setup as above: procedurally generated tracks from up to 12 Bézier control points, pixel observations, continuous 3D action space, dense per-polygon reward.",
            "complexity_measure": "Track geometry complexity (up to 12 control points), pixel observation dimensionality (96×96×3), continuous control action space; teacher constructs environment parameter vector sequentially (combinatorial parameter permutations).",
            "complexity_level": "high",
            "variation_measure": "Training variation limited to teacher-generated parametrizations of control points; compared against CLUTR's VAE-based sampling of 1M tracks and F1 20-track test set.",
            "variation_level": "high in principle, but in practice PAIRED struggled to explore effective regions of task space and often generated over-simplified curricula.",
            "performance_metric": "Mean episodic return on held-out F1 tracks (zero-shot generalization).",
            "performance_value": "Mean return (PAIRED flexible regret): 257 ± 16; Mean return (PAIRED standard regret): ~26 ± 19 (very poor in standard-regret setting). PAIRED underperforms CLUTR substantially in both regret variants.",
            "complexity_variation_relationship": "PAIRED must learn the task manifold and curriculum simultaneously; paper reports this co-learning causes non-stationarity and instability, leading to poor curricula (e.g., over-simplified tasks) and degraded generalization. The combination of complex parametrization (high complexity) and large permutation-invariant parameter spaces exacerbates the combinatorial explosion for PAIRED.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive teacher curriculum learning (PAIRED) where the teacher is an RL agent trained to maximize regret; students trained with PPO on generated tasks.",
            "generalization_tested": true,
            "generalization_results": "Poor zero-shot generalization on F1 benchmark with standard regret; improved with flexible regret but still much worse than CLUTR (PAIRED flexible mean 257 vs CLUTR flexible 468). PAIRED often generates degenerate oversimplified tasks during training, harming learning.",
            "sample_efficiency": "Same training budgets used for fair comparison: flexible-regret experiments 2M timesteps; standard-regret 5M timesteps. Original PAIRED results in prior work used far more timesteps (reported original baseline after 3B timesteps), indicating sample inefficiency.",
            "key_findings": "Simultaneously learning task-manifold and curriculum creates severe instability and combinatorial challenges for PAIRED; PAIRED frequently generates overly-simple tasks during training and is sample-inefficient compared to CLUTR.",
            "uuid": "e1033.1",
            "source_info": {
                "paper_title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CLUTR (MiniGrid student)",
            "name_full": "Curriculum Learning via Unsupervised Task Representation Learning — Student (MiniGrid navigation domain)",
            "brief_description": "A partially-observable grid-world navigation RL student (protagonist) trained under CLUTR; student trained with PPO and evaluated on zero-shot generalization to held-out navigation grids.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CLUTR Student (Protagonist)",
            "agent_description": "A partially-observable MiniGrid navigation agent trained with PPO under a CLUTR-generated curriculum; the CLUTR teacher samples latent grid encodings from a pretrained recurrent VAE decoder and optimizes regret to generate training tasks.",
            "agent_type": "simulated agent (virtual grid-world)",
            "environment_name": "MiniGrid partially-observable navigation (15×15 grid; 13×13 active area)",
            "environment_description": "Discrete grid navigation tasks where each task is encoded as a sequence of up to 52 integers: first 50 denote obstacle locations (allowing duplicates to represent fewer obstacles), followed by goal and start locations; partially observable 5×5×3 local view plus direction; sparse reward (only upon reaching goal); number of obstacles varies from 0 to 50.",
            "complexity_measure": "Grid complexity measured by number of obstacles (0–50), grid size effectively 13×13 active area (169 locations), partial observability (5×5 view + direction), and sparse reward structure (sensitive credit assignment).",
            "complexity_level": "medium–high (sparse-reward, partial observability, up to 50 obstacles creates substantial combinatorial complexity)",
            "variation_measure": "Task variation measured by number of procedurally-generated grid instances used to train VAE (flexible-regret: 10,000,000 random grids; standard-regret: 1,000,000 grids), plus the held-out testset of 16 unseen navigation tasks.",
            "variation_level": "high (10M VAE training grids for flexible experiments; wide obstacle-count distribution 0–50)",
            "performance_metric": "Solve rate (%) on 16 unseen navigation tasks (zero-shot generalization); also sample-efficiency (solve-rate over training timesteps).",
            "performance_value": "Flexible regret: CLUTR solves 58% of unseen grids vs PAIRED 43% (≈35% relative improvement). Standard regret: CLUTR solves 64% vs PAIRED 44% (≈45% relative improvement). CLUTR outperforms PAIRED on 13/16 (flexible) and 14/16 (standard) test grids.",
            "complexity_variation_relationship": "Paper reports that CLUTR's pretrained latent manifold provides a neighborhood structure that helps the teacher navigate variations in task instances (obstacle placements) to produce smoother curricula; PAIRED often degenerates by generating either overly-hard or overly-simple tasks, indicating poor handling of the trade-off between complexity and variation. Sorting obstacle locations for VAE training reduces combinatorial explosion and improves performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning with pretrained recurrent VAE (latent-task manifold) + regret-optimizing teacher (CLUTR); students trained with PPO. VAE trained on sorted obstacle sequences to reduce permutation combinatorics.",
            "generalization_tested": true,
            "generalization_results": "Zero-shot generalization to 16 held-out navigation tasks: CLUTR substantially improves solve rates over PAIRED (58% vs 43% flexible; 64% vs 44% standard), and converges faster (better sample efficiency). CLUTR generates a broader range of task difficulties during training rather than degenerating to trivial tasks.",
            "sample_efficiency": "Training budgets: flexible-regret experiments 250M timesteps; standard-regret experiments 500M timesteps (5 independent runs). VAE pretraining used 10M random grids for flexible experiments and 1M for standard.",
            "key_findings": "Pretraining a VAE on a large, sorted dataset of generated grids and freezing the decoder yields a latent task manifold that allows the teacher to produce more useful curricula, improving zero-shot generalization and sample efficiency; sorting VAE training data mitigates combinatorial explosion; allowing decoder finetuning reduces performance.",
            "uuid": "e1033.2",
            "source_info": {
                "paper_title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "PAIRED (MiniGrid student)",
            "name_full": "PAIRED — Protagonist/Antagonist regret-maximizing curriculum (baseline; MiniGrid domain)",
            "brief_description": "A baseline adaptive-teacher UED method where a sequential teacher RL agent constructs grid parameters to maximize the regret between antagonist and protagonist; used as a primary baseline in MiniGrid navigation experiments.",
            "citation_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "mention_or_use": "use",
            "agent_name": "PAIRED Student (Protagonist)",
            "agent_description": "MiniGrid navigation agent trained with PPO under PAIRED where the teacher sequentially generates parameter vectors (obstacle placements, goal, start) to maximize antagonist-protagonist regret; students and antagonist are standard RL agents.",
            "agent_type": "simulated agent (virtual grid-world)",
            "environment_name": "MiniGrid partially-observable navigation (15×15 grid; 13×13 active area)",
            "environment_description": "Same as CLUTR MiniGrid description: up to 50 obstacles, sparse reward, partial observability; PAIRED's teacher operates on raw parameter sequences and suffers from permutation-invariance combinatorics unless special handling is applied.",
            "complexity_measure": "Number of obstacles (0–50), sequence length 52 (first 50 obstacles + goal + agent), partial observability, sparse rewards; teacher must generate sequences parameter-by-parameter, leading to long-horizon credit assignment.",
            "complexity_level": "medium–high",
            "variation_measure": "Teacher-generated variation over obstacle placements; compared against CLUTR's VAE-sampled variation (1M/10M datasets). PAIRED's practical exploration suffers from combinatorial explosion in the permutation-invariant parameter space.",
            "variation_level": "high in principle but sample-inefficient in practice",
            "performance_metric": "Solve rate (%) on 16 unseen navigation tasks (zero-shot generalization).",
            "performance_value": "Flexible regret: PAIRED solves 43% of unseen grids vs CLUTR 58%. Standard regret: PAIRED solves 44% vs CLUTR 64%. PAIRED underperforms CLUTR across most test tasks.",
            "complexity_variation_relationship": "PAIRED's simultaneous manifold-and-curriculum learning leads to non-stationarity and instability, making it struggle to find an effective balance across complexity and variation; it often collapses to overly-simple curricula or generates arbitrarily complex grids early that are unsolvable.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Adaptive teacher (PAIRED) generating parameter sequences; students trained with PPO. No separate latent manifold pretraining in baseline.",
            "generalization_tested": true,
            "generalization_results": "PAIRED shows inferior zero-shot generalization compared to CLUTR across the 16 held-out MiniGrid tasks and exhibits less sample-efficient learning and degenerate curricula behavior during training.",
            "sample_efficiency": "Experiments run for 250M (flexible) and 500M (standard) timesteps; original reported PAIRED baselines in prior work sometimes used far larger budgets (e.g., 3B timesteps) highlighting practical sample inefficiency.",
            "key_findings": "PAIRED's approach of jointly learning a task manifold while generating curricula leads to instability and combinatorial difficulties in permutation-invariant parameter spaces; it underperforms CLUTR when evaluated for zero-shot generalization and sample efficiency in MiniGrid.",
            "uuid": "e1033.3",
            "source_info": {
                "paper_title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2,
            "sanitized_title": "emergent_complexity_and_zeroshot_transfer_via_unsupervised_environment_design"
        },
        {
            "paper_title": "Replay-guided adversarial environment design",
            "rating": 2,
            "sanitized_title": "replayguided_adversarial_environment_design"
        },
        {
            "paper_title": "Prioritized Level Replay",
            "rating": 2,
            "sanitized_title": "prioritized_level_replay"
        },
        {
            "paper_title": "Robust PLR",
            "rating": 1,
            "sanitized_title": "robust_plr"
        },
        {
            "paper_title": "Domain Randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        }
    ],
    "cost": 0.018141749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CLUTR: Curriculum Learning via Unsupervised Task Representation Learning
7 Mar 2023</p>
<p>Abdus Salam Azad 
University of California
Berkeley</p>
<p>Izzeddin Gur 
Google Research</p>
<p>Jasper Emhoff 
University of California
Berkeley</p>
<p>Nathaniel Alexis 
University of California
Berkeley</p>
<p>Aleksandra Faust 
Google Research</p>
<p>Pieter Abbeel 
University of California
Berkeley</p>
<p>Ion Stoica 
University of California
Berkeley</p>
<p>CLUTR: Curriculum Learning via Unsupervised Task Representation Learning
7 Mar 202321C8DADF56BC9907F5F88D056E7E0112arXiv:2210.10243v2[cs.LG]{21222324}. This wall
Reinforcement Learning (RL) algorithms are often known for sample inefficiency and difficult generalization.Recently, Unsupervised Environment Design (UED) emerged as a new paradigm for zero-shot generalization by simultaneously learning a task distribution and agent policies on the generated tasks.This is a non-stationary process where the task distribution evolves along with agent policies; creating an instability over time.While past works demonstrated the potential of such approaches, sampling effectively from the task space remains an open challenge, bottlenecking these approaches.To this end, we introduce CLUTR: a novel unsupervised curriculum learning algorithm that decouples task representation and curriculum learning into a two-stage optimization.It first trains a recurrent variational autoencoder on randomly generated tasks to learn a latent task manifold.Next, a teacher agent creates a curriculum by maximizing a minimax REGRETbased objective on a set of latent tasks sampled from this manifold.Using the fixed-pretrained task manifold, we show that CLUTR successfully overcomes the non-stationarity problem and improves stability.Our experimental results show CLUTR outperforms PAIRED, a principled and popular UED method, in the challenging Car-Racing and navigation environments: achieving 10.6X and 45% improvement in zero-shot generalization, respectively.CLUTR also performs comparably to the non-UED state-of-the-art for CarRacing, while requiring 500X fewer environment interactions.</p>
<p>Introduction</p>
<p>Deep Reinforcement Learning (RL) has shown exciting progress in the past decade in many challenging domains including Atari (Mnih et al., 2015), Dota (Berner et al., 2019), Go (Silver et al., 2016).However, deep RL is also known for its sample inefficiency and difficult generalizationperforming poorly on unseen tasks or failing altogether with the slightest change (Cobbe et al., 2019;Azad et al., 2022;Zhang et al., 2018).While, Curriculum Learning (CL) algorithms have shown to improve RL sample efficiency by adapting the training task distribution, i.e., the curriculum (Portelas et al., 2020;Narvekar et al., 2020), recently a class of Unsupervised CL algorithms, called Unsupervised Environment Design (UED) (Dennis et al., 2020;Jiang et al., 2021a) has shown promising zero-shot generalization by automatically generating the training tasks and adapting the curriculum simultaneously.</p>
<p>UED algorithms employ a teacher that generates training tasks by sampling the free parameters of the environment (e.g., the start, goal, and obstacle locations for a navigation task) and can either be adaptive or random.Contemporary adaptive UED teachers, i.e., PAIRED (Dennis et al., 2020) and REPAIRED (Jiang et al., 2021a), are implemented as RL agents with the free task parameters as their action space.The teacher agent aims at generating tasks that maximize the student agent's regret, defined as the performance gap between the student agent and an optimal policy.Inspite of promising zero-shot generalization, adaptive teacher UEDs are still sample inefficient.This sample inefficiency is attributed primarily to the difficulty of training a regret based RL teacher (Parker-Holder et al., 2022).First, the teacher receives a sparse reward only after specifying the full parameterization of a task; leading to a long-horizon credit assignment problem.Additionally, the teacher agent faces a combinatorial explosion problem if the parameter space is permutation invariant-e.g., for a navigation task, a set of obstacles corresponds to factorially different permutations of the parameters 1 .Most importantly, the teacher needs to simultaneously learn a task manifoldfrom scratch-to generate training tasks and navigate this manifold to induce an efficient curriculum.However, the teacher learns this task manifold implicitly based on the student regret and as the student is continuously co-learning with the teacher, the task manifold also keeps evolving over time.Hence, the simultaneous learning of task manifold and curriculum results in an instability over time and makes it a difficult learning problem.</p>
<p>To address the above-mentioned challenges, we present Curriculum Learning via Unsupervised Task Representation Learning (CLUTR).At the core of CLUTR, lies a hierarchical graphical model that decouples task representation learning from curriculum learning.We develop a variational approximation to the UED problem and employ a Recurrent Variational AutoEncoder (VAE) to learn a latent task manifold, which is pretrained unsupervised.Unlike contemporary adaptive-teachers, which builds the tasks from scratch one parameter at a time, the CLUTR teacher generates tasks in a single timestep by sampling points from the latent task manifold and uses the generative model to translate them into complete tasks.The CLUTR teacher learns the curriculum by navigating the pretrained and fixed task manifold via maximizing regret.By utilizing a pretrained latent task-manifold, the CLUTR teacher can train as a contextual bandit -overcoming the long-horizon credit assignment problem -and create a curriculum much more efficiently -improving stability at no cost to its effectiveness.Finally, by carefully introducing bias to the training corpus (such as sorting each parameter vector), CLUTR solves the combinatorial explosion problem of parameter space without using any costly environment interactions.</p>
<p>While CLUTR can be integrated with any adaptive teacher UEDs, we implement CLUTR on top of PAIRED-one of the most principled and popular UEDs.Our experimental results show that CLUTR outperforms PAIRED, both in terms of generalization and sample efficiency, in the challenging pixel-based continuous CarRacing and partially observable discrete navigation tasks.For CarRacing, CLUTR achieves 10.6X higher zero-shot generalization on the F1 benchmark (Jiang et al., 2021a) modeled on 20 real-life F1 racing tracks.Furthermore, CLUTR performs comparably to the non-UED attention-based CarRacing SOTA (Tang et al., 2020), outperforming it in nine of the 20 test tracks while requiring 500X fewer environment interactions.In navigation tasks, CLUTR outperforms PAIRED in 14 out of the 16 unseen tasks, achieving a 45% higher solve rate.</p>
<p>In summary, we make the following contributions: i) we introduce CLUTR, a novel adaptive-teacher UED algorithm derived from a hierarchical graphical model for UEDs, that can be represented using any permutation of this set, e.g., {22, 24, 23, 21}, {23, 21, 24, 22}, resulting in a combinatorial explosion.</p>
<p>augments the teacher with unsupervised task-representation learning ii) CLUTR, by decoupling task representation learning from curriculum learning, solves the long-horizon credit assignment and the combinatorial explosion problems faced by regret-based adaptive-teacher UEDs such as PAIRED.iii) Our experimental results show CLUTR significantly outperforms PAIRED, both in terms of generalization and sample efficiency, in two challenging domains: CarRacing and navigation.</p>
<p>Related Work</p>
<p>Unsupervised Curriculum Design: Dennis et al. (2020) was the first to formalize UED and introduced the minimax regret-based UED teacher algorithm, PAIRED, with a strong theoretical robustness guarantee.However, gradientbased multi-agent RL has no convergence guarantees and often fails to converge in practice (Mazumdar et al., 2019).Pre-existing techniques like Domain Randomization (DR) (Jakobi, 1997;Sadeghi &amp; Levine, 2016;Tobin et al., 2017) and minimax adversarial curriculum learning (Morimoto &amp; Doya, 2005;Pinto et al., 2017) also fall under the category of UEDs.DR teacher follows a uniform random strategy, while the minimax adversarial teachers follow the maximin criteria, i.e., generate tasks that minimize the returns of the agent.POET (Wang et al., 2019) and Enhanced POET (Wang et al., 2020) also approached UED, before PAIRED, using an evolutionary approach of a co-evolving population of tasks and agents.</p>
<p>Recently, Jiang et al. (2021a) proposed Dual Curriculum Design (DCD): a novel class of UEDs that augments UED generation methods (e.g., DR and PAIRED) with replay capabilities.DCD involves two teachers: one that actively generates tasks with PAIRED or DR, while the other curates the curriculum to replay previously generated tasks with Prioritized Level Replay (PLR) (Jiang et al., 2021b).Jiang et al. (2021a) shows that, even with random generation (i.e., DR), updating the students only on the replayed level (but not while they are first generated, i.e., no exploratory student gradient updates as PLR) and with a regret-based scoring function, PLR can also learn minimax-regret agents at Nash Equilibrium and call this variation Robust PLR.It also introduces REPAIRED, combining PAIRED with Robust PLR.Parker-Holder et al. (2022) introduces ACCEL, which improves on Robust PLR by allowing edit/mutation of the tasks with an evolutionary algorithm.Currently, random-teacher UEDs outperform adaptive-teacher UED methods.</p>
<p>While CLUTR and other PAIRED-variants actively adapt task generation to the performance of agents, other algorithms such as PLR generates task from a fixed-random task distribution, resulting in two categories of UED methods, i) adaptive teacher/generator based UEDs and ii) randomgenerator based UEDs.The existing adaptive-teacher UEDs are variants of PAIRED, which try to improve PAIRED from different aspects, but are still susceptible to the instability due to a evolving task-manifold.Unlike other PAIRED variants, CLUTR introduces a novel variational formulation with a VAE-style pretraining for task-manifold learning to solve this instability issue and can be applied, also potentially improve, any adaptive-teacher UEDs.On the other hand, random-generator UEDs focus on identifying or, prioritizing which tasks to present to the student from the randomly generated tasks, and is orthogonal to our proposed approach.</p>
<p>Representation Learning: Variational Auto Encoders (Kingma &amp; Welling, 2013;Rezende et al., 2014;Higgins et al., 2016) have widely been used for their ability to capture high-level semantic information from lowlevel data and generative properties in a wide variety of complex domains such as computer vision (Razavi et al., 2019;Gulrajani et al., 2016;Zhang et al., 2021;2022), natural language (Bowman et al., 2015;Jain et al., 2017), speech (Chorowski et al., 2019), and music (Jiang et al., 2020).VAE has been used in RL as well for representing image observations (Kendall et al., 2019;Yarats et al., 2021) and generating goals (Nair et al., 2018).While CLUTR also utilizes similar VAEs, different from prior work, it combines them in a new curriculum learning algorithm to learn a latent task manifold.Florensa et al. (2018) also proposed a curriculum learning algorithm, however, for latent-space goal generation using a Generative Adversarial Network.</p>
<p>Background</p>
<p>Unsupervised Environment Design (UED)</p>
<p>As formalized by Dennis et al. (2020) UED is the problem of inducing a curriculum by designing a distribution of concrete, fully-specified environments, from an underspecified environment with free parameters.The fully specified environments are represented using a Partially Observable Markov Decision Process (POMDP) represented by (A, O, S, T , I, R, γ), where A, O, and S denote the action, observation, and state spaces, respectively.I → O is the observation function, R : S → R is the reward function, T : S×A → ∆(S) is the transition function and γ is the discount factor.The underspecified environments are defined in terms of an Underspecified Partially Observable Markov Decision Process (UPOMDP) represented by the tuple M = (A, O, Θ, S M , T M , I M , R M , γ).Θ is a set representing the free parameters of the environment and is incorporated in the transition function as T M : S × A × Θ → ∆(S).Assigning a value to θ results in a regular POMDP, i.e., UPOMDP + θ = POMDP.Traditionally (e.g., in Dennis et al. (2020) and Jiang et al. (2021a)) Θ is considered as a trajectory of environment parameters θ or just θ-which we call task in this paper.For example, θ can be a concrete navigation task represented by a sequence of obstacle locations.We denote a concrete environment generated with the parameter θ ∈ Θ as M θ or simply M θ .The value of a policy π in M θ is defined as
V θ (π) = E[ T t=0 r t γ t ],
where r t is the discounted reward obtained by π in M θ .</p>
<p>PAIRED</p>
<p>PAIRED (Dennis et al., 2020) solves UED with an adversarial game involving three players2 : the agent π P and an antagonist π A , are trained on tasks generated by the teacher θ.PAIRED objective is:
max θ,π P min π A U (π P , π A , θ) = E θ∼ θ [REGRET θ (π P , π A )].
Regret is defined by the difference of the discounted rewards obtained by the antagonist and the agent in the generated tasks, i.e., REGRET θ (π
P , π A ) = V θ (π A ) − V θ (π P ).
The PAIRED teacher agent is defined as Λ : Π → ∆(Θ T ), where Π is a set of possible agent policies and Θ T is the set of possible tasks.The teacher is trained with an RL algorithm with U as the reward while, the protagonist and antagonist agents are trained using the usual discounted rewards from the environments.Dennis et al. (2020) also introduced the flexible regret objective, an alternate regret approximation that is less susceptible to local optima.It is defined by the difference between the average score of the agent and antagonist returns and the score of the policy that achieved the highest average return.</p>
<p>Curriculum Learning via Unsupervised Task Representation Learning</p>
<p>In this section, we formally present CLUTR as a latent UED and discuss it in details.We use a variational formulation of UED by using the above graphical model to derive the following ELBO for CLUTR, where V AE(z, E) denotes the VAE objective:</p>
<p>Formulation of CLUTR
ELBO ≈ V AE(z, E) − REGRET(R, E)(1)
We share the details of this derivation in Section A.1 of the Appendix.The above ELBO (Eq.1) defines the optimization objective for CLUTR, which can be seen as optimizing the VAE objective with a regret-based regularization term and vice versa.As previously discussed, it is difficult to train a UED teacher while jointly optimizing for both the curriculum and task representations.Hence we propose a two-level optimization for CLUTR.First, we pretrain a VAE to learn unsupervised task representations, and then in the curriculum learning phase, we optimize for regret to generate the curriculum while keeping the VAE fixed.In Section 5.3, we empirically show that this two-level optimization performs better than the joint optimization of Eq.1, i.e., finetuning the VAE decoder with the regret loss during the curriculum learning phase.</p>
<p>Unsupervised Latent Task Representation Learning</p>
<p>As discussed above, we use a Variational AutoEncoder (VAE) to model our generative latent task-manifold.Aligning with Dennis et al. (2020) and Jiang et al. (2021a), we represent task θ, as a sequence of integers.For example, in a navigation task, these integers denote obstacle, agent, and goal locations.We use an LSTM-based Recurrent VAE (Bowman et al., 2015) to learn task representations from integer sequences.We learn an embedding for each integer and use cross-entropy over the sequences to measure the reconstruction error.This design choice makes CLUTR applicable to task parameterization beyond integer sequences, e.g., to sentences or images.To train our VAEs, we generate random tasks by uniformly sampling from Θ T , the set of possible tasks.Thus, we do not require any interaction with the environment to learn the task manifold.Such unsupervised training of the task manifold is practically very useful as interactions with the environment/simulator are much more costly than sampling.Furthermore, we sort the input sequences, fully or partially, when they are permutation invariant, i.e., essentially represent a set.By sorting the training sequences, we avoid the combinatorial explosion faced by other adaptive UED teachers.</p>
<p>CLUTR</p>
<p>We define CLUTR following the objective given in Eq. 1.</p>
<p>CLUTR uses the same curriculum objective as PAIRED, REGRET(R, E) = REGRET θ (π P , π A ) where, θ denotes a Collect Agent trajectory τ P in M θ .Compute:
U θ (π P ) = T i=0 r t γ t 7: Collect Antagonist trajectory τ A in M θ . Compute: U θ (π A ) = T i=0 r t γ t 8: Compute: REGRET θ (π P , π A ) = U θ (π A ) − U θ (π P ) 9:
Train Protagonist policy π P with RL update and reward R(τ P ) = U θ (π P ) 10:</p>
<p>Train Antagonist policy π A with RL update and reward
R(τ A ) = U θ (π A ) 11:
Train Teacher policy Λ with RL update and reward R(τ Λ) = REGRET 12: until not converged task, i.e., a concrete assignment to the free parameters of the environment E. Unlike PAIRED teacher, which generates θ directly, the CLUTR teacher policy is defined as Λ : Π → ∆(Z), where Π is a set of possible agent policies and Z is as the latent space.Thus, the CLUTR teacher is a latent environment designer, which samples random z and θ is generated by the VAE decoder function G : Z → Θ.We present the outline of the CLUTR in Algorithm 1. CLUTR outline is very similar to PAIRED, differing only in the first two lines of the main loop to incorporate the latent space.Now we discuss a couple of additional properties of CLUTR compared to other adaptive-teacher UEDs, i.e., PAIRED and REPAIRED.First, CLUTR teacher samples from the latent space Z and thus generates a task in a single timestep.Note that this is not possible for other adaptive UED teachers, as they operate on parameter space and generate one task parameter at a time, conditioned on the state of the partiallygenerated task so far.Furthermore, Adaptive-teacher UEDs typically observe the state of their partially generated task to generate the next parameters.Hence they require designing different teacher architectures for environments with different state space.CLUTR teacher architecture, however, is agnostic of the problem domain and does not depend on their state space.Hence the same architecture can be used across different environments.</p>
<p>CLUTR in the context of contemporary UED method landscape</p>
<p>As discussed in Section 2, contemporary UED methods can be characterized by their i) teacher type (random/fixed Hence, these taskmanifolds depend on the quality of the estimates, which in turn depends on the overall health of the multi-agent RL training.Furthermore, they do not take into account the actual task structures.In contrast, CLUTR introduces an explicit task-manifold modeled with VAE, that can represent a local neighborhood structure capturing the similarity of the tasks, subject to the parameter space being used.Hence, similar tasks (in terms of parameterization) would be placed nearby in the latent space.Intuitively this local neighborhood structure should facilitate the teacher to navigate the manifold effectively.The above discussion illustrates that CLUTR along with PAIRED and REPAIRED form a category of UEDs that generates tasks based on a learned task-manifold, orthogonal to the random generation-based methods, while CLUTR being the only one utilizing an unsupervised generative task manifold.Table 1 summarizes the similarity and differences.</p>
<p>Experiments</p>
<p>In this section, we evaluate CLUTR in two challenging domains: i) Pixel-Based Car Racing with continuous control and dense rewards, and ii) partially observable navigation tasks with discrete control and sparse rewards.We compare CLUTR primarily with PAIRED to analyze its impact on improving adaptive-teacher UED algorithms, experimenting with two commonly used regret objectives: standard and flexible.As discussed in Section 2 and 4.4, there are other random-generator and adaptive-teacher UEDs employing techniques complimentary or orthogonal to our approach.</p>
<p>For completeness, we compare CLUTR with such existing UED methods in Section C.1 and D in the Appendix.</p>
<p>We then empirically investigate the following hypotheses: H1: Simultaneous learning of latent task manifold and curriculum degrades performance (Section 5.</p>
<p>CLUTR Performance on Pixel-Based Continuous Control CarRacing Environment</p>
<p>The CarRacing environment (Jiang et al., 2021a;Brockman et al., 2016) requires the agent to drive a full lap around a closed-loop racing track modeled with Bézier Curves (Mortenson, 1999) of up to 12 control points.Both CLUTR and PAIRED were trained for 2M timesteps for flexible regret objective and for 5M timesteps for the standard regret objective experiments.We train the VAE on 1 million randomly generated tracks for 1 million gradient updates.Note that only one VAE was trained and used for all the experiments (10 independent runs, both objectives).We evaluate the agents on the F1 benchmark (Jiang et al., 2021a) containing 20 test tracks modeled on real-life F1 racing tracks.These tracks are significantly out of distribution than any tracks that the UED teachers can generate with just 12 control points.Further details on the environment, network architectures, VAE training, and detailed experimental results with analysis can be found in Section B.1, B.2, B.4, C of the Appendix, respectively.</p>
<p>Figure 2 shows the mean return obtained by CLUTR and PAIRED on the full F1 benchmark, on.We independently experimented with both the standard and flexible regret objectives.We notice that PAIRED performs miserably with standard regret in these tasks.However, implementing CLUTR or changing to the flexible regret objective, improves the performance considerably.Furthermore, CLUTR with flexible regret results in much better performance, comparable to the non-UED attention-based SOTA for CarRacing (Tang et al., 2020), despite not using a self-attention policy and training on 500X fewer environment interactions, while outperforming it on nine of the 20 F1 tracks (See Table 4 in Appendix).We also note, CLUTR improves PAIRED irrespective of the choice of the regret objectives: achieving 10.6X and 82% higher returns with standard and flexible regret objectives, respectively and outperforming PAIRED on each of the 20 F1 tracks (See</p>
<p>CLUTR Performance on Partially Observable Navigation Tasks on MiniGrid</p>
<p>We now compare CLUTR with PAIRED on the popular MiniGrid environment, originally introduced by (Chevalier-Boisvert et al., 2018) and adopted by (Dennis et al., 2020) for UEDs, for both standard and flexible regret objectives.In these navigation tasks, an agent explores a grid world to find the goal while avoiding obstacles and receives a sparse reward upon reaching the goal.For flexible regret experiment, we generated 10 million random grids to train the VAE, with the obstacle locations sorted, and the number of obstacles uniformly varying from zero to 50, aligning with (Dennis et al., 2020).The standard regret experiment uses a similar but smaller dataset of 1 million grids.Note that the results reported in the original PAIRED paper are obtained after 3 billion timesteps of training, while we train PAIRED and CLUTR for 250M and 500M timesteps (5 independent runs), for flexible and standard regret objectives, respectively.We evaluate on a testset of 16 novel navigation tasks from Dennis et al. (2020).</p>
<p>Figure 4: Mean solve rate on the test dataset comprising 16 novel nagivation tasks from 5 independent runs.CLUTR achieves 45% and 35% higher solve rate than PAIRED, with standard and flexible regret objectives, respectively.</p>
<p>Figure 4 shows the mean solve rate obtained by CLUTR and PAIRED on the test dataset.CLUTR improves PAIRED irrespective of the choice of the regret objectives: 45% and 35% higher solve rate than PAIRED outperforming on 14 and 13 individual test grids out of 16 (See Figure 27 and Figure 22 in Section D for details), with standard and flexible regret objectives, respectively.Figure 5 plot solve rate on all the 16 test grids during training for flexible objective and a subset of four grids, namely, Sixteen Rooms, Sixteen Rooms with Fewer Doors, Labyrinth, and Large Corridor, for standard objective.We see CLUTR, though showing an initial dip for flexible objective, shows better sample efficiency by achieving a higher solve rate earlier than PAIRED.</p>
<p>Learning task manifold and curriculum: Joint vs Two-staged Optimization</p>
<p>We hypothesized that learning the task representation and the curriculum simultaneously results in a difficult learning problem due to the non-stationarity of the process.To test this, we conduct an experiment in which we allow finetuning our pretained decoder with the regret loss during the curriculum learning phase.This experiment, namely 'CLUTR with Decoder Finetuning', shows a 29% performance drop in the CarRacing domain with the standard regret objective (Figure 7).Similarly, we see a drop of 10% in case of flexible regret further justifying our hypothesis (See Section C.2.2 for details).As a side note, the smaller drop in the later case indicates that flexible objective mitigates some of the instability problem too.Finally, even with decoder finetuning, CLUTR achieves 7.6X and 65% improvement over PAIRED, for standard and flexible regret respectively-indicating the benefits of pretrained decoupled latent task space.The above experimental results thus empirically validates our hypothesis that keeping the pretrained task manifold fixed during curriculum learning helps solving the instability problem.</p>
<p>Impact of sorting VAE data on solving Combinatorial Explosion</p>
<p>We hypothesized that training a VAE on sorted sequences can solve the combinatorial explosion problem.To test this, we conduct an experiment, 'CLUTR with Shuffled VAE', in which we train CLUTR with an alternate VAE-trained 5X longer on a non-sorted and 10X bigger version of the original dataset.This experiment shows a 31% performance drop in the CarRacing domain as seen in Figure 7, empirically validating our hypothesis.On another note, CLUTR with Shuffled VAE still shows a 7.3X improvement over PAIRED.This indicates that, even when the task manifold is 'suboptimal', a fixed and pretrained task-manifold, i.e., the decoupling of task representation and curriculum learning, helps solving the learning instability and combinatorial explosion problem faced by PAIRED.Further details of this experiment are discussed in Section C.4 of the Appendix.</p>
<p>Analysis of the Curriculum: CLUTR vs PAIRED</p>
<p>In Section 5.1 and 5.2 we discussed how CLUTR outperforms PAIRED, both in terms of sample efficiency and generalization, suggesting CLUTR induces a significantly more effective curriculum than PAIRED.For better understanding of CLUTR curriculum, in Figure 8 we analyze the mean regret-the performance gap between the agent and the adversary-on the teacher-generated curricula for both CarRacing and navigation tasks.</p>
<p>CLUTR and PAIRED show similar regret patterns, which is not surprising as both optimize regret using the same criteria.However, CLUTR converges to a smaller regret value; faster than PAIRED.From a curriculum learning perspective, smoother training is expected with tasks that are 'slightly' harder than the agent can already solve or, can obtain 'slightly' better returns.In practice, both the agent and the antagonist are trained in the same training data and context e.g., the same hyper-parameters, architecture, differing only by their random initial weights.Hence, a lower regret implies that the teacher is generating tasks at the frontier of the agents' capabilities, which are either slightly harder than the agent should be able to solve (because antagonist is solving them) or, the tasks in which antagonist is performing slightly better.On the other hand, higher regret values can result from generating tasks which are biased towards the strength or, idiosyncracy of only one of the agents, which might not be useful for generalization.In   Figure 6 shows snapshots of CLUTR and PAIRED generated curriculums as training progress.We notice, PAIRED generates over-simplified tasks for substantial amount of time, which might hamper its generalization and sample efficiency.On the other hand, CLUTR doesnt seem to start with overly-simplistic tasks, rather generates tasks with a wide range of difficulty throughout.Section D.2.1 shares detailed analysis supporting the above observation and further insights.</p>
<p>Conclusion: Limitations and Future Work</p>
<p>In this work, we introduce CLUTR, an unsupervised latent space adaptive-teacher UED method that augments adaptive UED teachers with a pretrained latent task manifold to decouple task representation learning from curriculum learning.CLUTR first trains a recurrent VAE from random tasks to learn the latent task manifold and then employs a regret-based adaptive-teacher to induce the curriculum.Through this decoupling, CLUTR solves the long-horizon credit assignment and the combinatorial explosion problems faced by regret-based adaptive-teacher UED methods.</p>
<p>Our experimental results show strong empirical evidence supporting the effectiveness of our proposed approach.</p>
<p>Even though CLUTR and other regret-based UEDs empirically show good generalization on human-curated complex transfer tasks, they rarely can generate human-level task structures during training.An interesting direction would be to enable UED algorithms to generate realistic tasks.</p>
<p>Another important direction would be to reduce the gap between the theoretical and practical aspects of regret-based multi-agent UED algorithms, which are subject to the quality of regret estimates and multi-agent RL training.At last, random generator algorithms like Robust PLR or even, DR have been shown to perform better than adaptive-teacher approaches like CLUTR or PAIRED.An interesting direction would be to investigate the conditions/environments under which a random generator performs better than an adaptive generator and vice versa.At last, we are excited about latent-space curriculum design and hope our work will encourage further research in this domain.</p>
<p>Wang, R., Lehman, J., Clune, J., and Stanley, K.</p>
<p>O. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions.arXiv preprint arXiv:1901.01753,2019.</p>
<p>Wang, R., Lehman, J., Rawal, A., Zhi, J., Li, Y., Clune, J., and Stanley, K. Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions.</p>
<p>A. Additional Details of CLUTR</p>
<p>A.1.CLUTR Objective Derivation Let's assume that R is a random variable that denotes a measure of success defined using the protagonist and antagonist agents and z be a latent random variable.We use the graphical model in Figure -9 where z generates an environment E and R is the success defined over E.</p>
<p>Both E and R are observed variables while z is an unobserved variable.R covers a broad range of measures used in different UED methods including PAIRED and DR (Domain Randomization).In PAIRED, R represents the REGRET as the difference of returns between the antagonist and protagonist agents and it depends on the environments that the agents are evaluated on.</p>
<p>We use a variational formulation of UED by using the above graphical model.We first define the variational objective as the KL-divergence between an approximate posterior distribution and true posterior distribution over latent variable z,
D KL (q(z)|p(z|R, E)) = E z∼q(z) [logq(z)] − E z∼q(z) [logp(z|R, E)] = E z∼q(z) [logq(z)] − E z∼q(z) [logp(R, E, z)] + logp(R, E)
where both R and E are given.</p>
<p>Next, we write the ELBO,
ELBO = E z∼q(z) [logq(z)] − E z∼q(z) [logp(R, E, z)] = E z∼q(z) [logq(z)] − E z∼q(z) [logp(R|E)p(E|z)p(z)] = E z∼q(z) [logq(z)] − E z∼q(z) [logp(z)] − E z∼q(z) [logp(E|z)] − E z∼q(z) [logp(R|E)] = E z∼q(z) [log q(z) p(z) ] − E z∼q(z) [logp(E|z)] − logp(R|E) = D KL (q(z)|p(z)) − E z∼q(z) [logp(E|z)] − logp(R|E) = V AE(z, E) − logp(R|E)
We can also induce an objective that includes minimax REGRET.Let R be distributed according to an exponential distribution, p(R|E) ∝ exp(REGRET(π P , π A |E)),</p>
<p>we derive,
ELBO ≈ V AE(z, E) − REGRET(R, E)
where the normalizing factor is ignored.</p>
<p>A.2. Robustness Guarantees</p>
<p>CLUTR essentially proposes including a pretrained latent space within the teacher/generator.From the teacher's perspective, the difference is while the PAIRED teacher starts from randomly initialized weights, CLUTR starts from the pretrained weights.Thus, CLUTR does not impose new assumptions on possible teacher policies.Furthermore, CLUTR does not change any other specifics of the underlying PAIRED algorithm.Hence, CLUTR holds the same theoretical robustness guarantees provided by PAIRED.</p>
<p>In practice, both CLUTR and PAIRED deviate from these theoretical guarantees.For example, both algorithms approximate the regret value, which is the case for other regret-based UEDs such as Robust PLR and REPAIRED ( (Jiang et al., 2021a)).Also, the robustness guarantee depends on reaching the Nash equilibrium of the multiagent adversarial game.However, gradient-based multi-agent RL has no convergence guarantees and often fails to converge in practice( (Mazumdar et al., 2019)).We also note that, by introducing the latent space, CLUTR VAE might not have access to the full task space due to practical limitations on training, e.g., the training dataset not having all possible tasks.However, when the decoder is allowed to be finetuned, CLUTR will have access to the full task space, similar to PAIRED.Our empirical results (discussed in Section 5.3) suggest that keeping the pretrained decoder fixed performs better than finetuning it, so we kept it fixed for our main experiments.We also want to mention, when the flexible objective is used, CLUTR (and PAIRED) does not hold the robustness guarantee as it changes the dynamics of the underlying game between the teacher and the agents, even though flexible regret works better in practice.</p>
<p>B. Training Details</p>
<p>B.1. Environment Details</p>
<p>Car Racing: The CarRacing environment was originally proposed by OpenAI Gym (Brockman et al., 2016), and later has been reparameterized by (Jiang et al., 2021a) with Bézier Curves( (Mortenson, 1999)) for UED algorithms.This environment requires the agents to drive a full lap around a closed-loop track.The track is defined by a Bézier Curve modeled with a sequence of upto 12 arbitrary control points, each spaced within a fixed radius B/2 of the center of the B × B field.This sequence of control points can uniquely identify a track, subject to a set of predefined curvature constraints (Jiang et al., 2021a).The control points are encoded in a 10 × 10 grid-a discrete downsampled version of the racing track field.Each control point hence is a integer denoting a cell of the grid and the cell coordinates are upscaled to match the original scale of the field afterwards.This ensures no two control points are too close together, preventing areas of excessive track overlapping.The track consists of a sequence of L polygons and the agent receives a reward of 1000/L upon visiting each unvisited polygon and a penalty of −0.1 at each time step to incentivize completing the tracks faster.Episodes terminate if the agent drives too far off-track but is not given any additional penalty.The agent controls a 3 dimensional continuous action space corresponding to the car's steer: torque ∈ [−1.0, 1.0], gas: acceleration ∈ [0, 0, 1.0], and brake: deceleration ∈ [0.0, 1.0].Each action is repeated 8 times.The agent receive a 96 × 96 × 3 RGB pixel observation.The top 84 × 96 portion of the frame contains a clipped, egocentric, bird's eye view of the horizontally centered car.The bottom 12 × 96 segment simulates a dashboard visualizing the agent's latest action and return.Snapshots of the test track in the F1 benchmark are shown in Figure 10.</p>
<p>Minigrid:</p>
<p>The environment is partially observable and based on (Chevalier-Boisvert et al., 2018) and adopted for UED by (Dennis et al., 2020).Each navigation task is represented with a sequence of integers denoting the locations of the obstacles, the goal, and the starting position of the agent: on a 15 × 15 grid similar to (Dennis et al., 2020).The grids are surrounded by walls on the sides, making it essentially a 13 × 13 grid.(Dennis et al., 2020) parameterizes the locations using integers.</p>
<p>Each task is a sequence of 52 integers, while the first 50 numbers denote the location of obstacles followed by the goal and the agent's initial location.The sequences may contain duplicates to allow the generation of navigation tasks with fewer than 50 obstacles.Snapshots of the test grids used in our paper are shown in Figure 11.</p>
<p>B.2. Network Architectures</p>
<p>All the student and teacher agents are trained with PPO (Schulman et al., 2017).</p>
<p>Student Architecture</p>
<p>For CarRacing, we use the same student architecture as (Jiang et al., 2021a).The architecture consists an image embedding module composed of 2D Convolutions with square kernels of sizes 2,2,2,2,3,3, stride lengths 2,2,2,2,1,1 and channel outputs of 8, 16, 64, 128, 256 stacked together.The image embedding is of size 256 and is passed through a Fully Connected (FC) layer of 100 hidden units and then passed through ReLU activations.This embedding is then passed through two FC with 100 hidden neurons, and then a softplus layer, and finally added to 1 for the beta distribution used for the continuous action space.Further details can be found in (Jiang et al., 2021a).</p>
<p>For navigation tasks, we use the same student architecture as (Dennis et al., 2020).The observation is a tuple with a 5 × 5 × 3 grid observation and a direction integer in [0 − 3].The grid view is fed to a convolutional layer with kernels of size 3 with 16 filters and the direction integer is passed through a FC with 5 units.This is followed by an LSTM of size 256, and then to two FC layers with 32 units, which connect to the policy outputs.The value network uses the same architecture.</p>
<p>Teacher Architecture For CarRacing, CLUTR teacher takes a random noise and generates a continuous vector, i.e., the latent task vector.We pass the random noise through a feed-forward network with one hidden layer of 8 neurons as the teacher.The output of this layer is fed through two separate fully-connected layers, each with a hidden size of 8 and an output dimension equal to the latent space dimension, followed by soft plus activations.We then add 1 to each component of these two output vectors, which serve as the α and β parameters respectively for the Beta distributions used to sample each latent dimension.In all of our experiments, we used a 64-dimensional latent task space.</p>
<p>For Minigrid experiments with flexible regret objective, we use a similar architecture as CarRacing described above, except the hidden layer consists of 10 neurons, instead of eight.For Minigrid experiments with standard regret objective (which is discussed later in Section D.2), we use the network architecture used in (Dennis et al., 2020) but only take a noise input.As this adversary network generates discrete actions, we scale them to real numbers before feeding into the VAE decoder.</p>
<p>VAE architecture</p>
<p>We use the architecture proposed in (Bowman et al., 2015).We use a word-embedding layer of size 300 with random initialization.The encoder comprises a conditional 'Highway' network followed by an LSTM.The Highway network is a two-staged network stacked on top of each other.Each stage computes σ
(x) f (G(x)) + (1 − σ(x)) Q(x)
, where x is the inputs to each of the highway network stages, G and Q is affine transformation, σ(x) is a sigmoid non-linearization, and is element-wise multiplication.G and Q are feed-forward networks with a single hidden layer with equal input and output dimensions of 300, equal to the word-embedding output dimension.We use ReLU activation as f .The highway network is followed by a bidirectional LSTM with a single layer of 600 units.The LSTM outputs are passed through linear layer of dimension 64 to get the VAE mean and log variance.The mean vectors are passed through a hyperbolic tangent activation.For CarRacing (both Flexible and Standard Objective experiments) and navigation (only Standard Objective) tasks the output of the hyperbolic tangent activation is linearly scaled in [−4, 4].No such scaling is done for the MiniGrid experiments with Flexible Regret Objective.The decoder takes in latent vectors of dimension 64 and passes through a bidirectional LSTM with two hidden layers of size 800 and follows it by a linear layer with size equaling the parameter vector dimension.</p>
<p>B.3. Hyperparameters</p>
<p>All our agents are trained with PPO Schulman et al. (2017).We did not perform any hyperparameter search for our experiments.The CarRacing experiments used the same parameters used in Jiang et al. (2021a) and the Minigrid experiments used the parameters from Dennis et al. (2020).The VAE used for CarRacing and Minigrid standard objective experiments (Section D.2) were trained using the default parameters from Bowman et al. (2015).For the VAE used in the Minigrid flexible objective experiments, which we presented in the main text of the paper, we used a reconstruction weight of 1000 and ran the training for 10M steps to incorporate the larger dataset.The detailed parameters are listed in Table 2 and Table 3.</p>
<p>The flexible objective blurs the distinction between the agent and the antagonist.Hence, we designate the agent achieving the higher average training return during the last 10 steps as the primary student agent and the other one as antagonist.</p>
<p>B.4. VAE Training Data</p>
<p>For CarRacing, we follow the same parameterization as Jiang et al. (2021a): each track is defined with a sequence of up to 12 integers denoting control points of a Bézier Curve. .Each control point is represented with an integer.We generate 1M random sorted integer sequences of fixed length 12 with duplicates-which enables generating tracks defined with less than 12 control points.</p>
<p>For navigation tasks we use the parameterization of Dennis et al. (2020), generating upto 50 obstacles for each task for a 15 × 15 grid, surrounded by walls, effectively an active area of 13 × 13.Hence, each location is numbered in 1 to 169.Every number except the last two of the sequence represent obstacle locations, and the last two for the goal and agent location, respectively.The parameter vector is thus partially permutation invariant.We uniformly generate 1M and 10M sequences of variable length between 2 and 52 (inclusive), for the standard regret objective and flexible regret objective, respectively.The obstacle locations are sorted.</p>
<p>B.5. Details on Compute Resources</p>
<p>We have conducted our experiments in cloud machines from :Amazon EC2 -Secure Cloud Services (https://aws.4, compares CLUTR with contemporary random-generator UED methods, REPAIRED, and the attention based SOTA.It is to be noted that, CLUTR and PAIRED with flexible regret objective was trained for 2M timesteps.All the other UED methods, along with CLUTR and PAIRED with standard regret was trained for 5M timesteps.</p>
<p>We notice that, each of the random-teacher UEDs outperform each of the adaptive-teacher UEDs, except CLUTR with flexible regret objective, indicating that adaptive-teacher UEDs face significant difficulty in this domain.PAIRED performs miserably in its basic form.CLUTR (with standard regret), flexible regret objecctive, or REPAIRED (by introducing replay and stop-gradient capabilities), all can improve PAIRED (with standard objective loss) significantly, yet they stil fall short to any of the random-teacher UEDs.CLUTR with flexible regret emerges as the best adaptive-teacher UED and the only adaptive-teacher UED to show better performance than some of the random-teacher UEDs-despite being trained only for 2M timesteps.CLUTR with flexible regret achieves an impressive 18X higher zero-shot generalization than PAIRED with standard regret and outperforms REPAIRED by 58%.</p>
<p>CLUTR with flexible regret is the only adaptive-teacher UED to outperform other random-teacher UEDs.CLUTR outperforms Domain Randomization and PLR, by 38% and 16%, repectively.It only falls short to Robust PLR by 14%.Nonetheless, CLUTR shows competitive results compared to Robust PLR, showing comparable results in seven out of the 20 test tracks and outperforming in the Netherlands track.CLUTR also outperforms the non-UED SOTA on the full F1 dataset.CLUTR outperforms the Attention Agent on nine out of the 20 tracks and shows comparable performance in another one.</p>
<p>Figure 13 compares how different UEDs perform during training by periodically evaluating them on three tracks from the F1 benchmark: Singapore, Germany, and Italy.CLUTR (with flexible regret) shows better generalization and sample efficiency than all the other UEDs, except Robust PLR.CLUTR showed better performance than Robust PLR till alomost 3M timesteps, after that CLUTR and Robust PLR curves followed each other closely, and near the very end Robust PLR surpasses CLUTR., when the decoder is allowed to finetune with the regret loss, results in a 10% performance drop.This performance drop empricially justify our choice of using a pretrained and fixed VAE to solve learning instability.</p>
<p>In Section 5.3, we empirically justified our hypothesis that learning the task representation and the curriculum simultaneously results in a difficult learning problem due to the non-stationarity of the process-using the standard regret objective.In this section we repeat the experiment with the flexible regret objective.In Figure 15, we see a 10% drop in the performance when the decoder was allowed to finetune with regret loss, further justifying our hypothesis.As a side note, the smaller drop compared to standard regret objective indicates that flexible objective mitigates some of the instability problem too.Finally, even with decoder finetuning, CLUTR achieves a 65% improvement over PAIRED indicating the benefits of pretrained decoupled latent task space.</p>
<p>C.3. CLUTR with standard regret loss</p>
<p>We train CLUTR with the standard regret loss for 5M timesteps.Figure 16 compares the impact of standard/flexible regret loss on the regret and agent returns during training.With standard regret loss, CLUTR shows a lower regret value, but shows similar pattern.The CLUTR agent achieves better returns with flexible loss throughout the training.</p>
<p>Figure 17 compares the mean regret and agent training returns with PAIRED.CLUTR with standard loss shows much lower regret than PAIRED (Figure 17a).Figure 17b shows that the CLUTR agents compete closely, while PAIRED antagonist achieves much higher returns than the PAIRED agent which leads to higher regret returns for the teacher agent but results in a weak student agent.To test the Zero-shot generalization, we evaluate CLUTR with the standard loss on the full F1 benchmark.Figure 18 shows CLUTR with standard regret loss outperforms PAIRED in all the 20 test tracks.This implies that CLUTR outperforms PAIRED irrespective of the choice of the loss function (standard/flexible). Figure 19 compares the sample efficiency of CLUTR with the standard regret loss with PAIRED by evaluating the agents on four selected tracks (Vanilla, Singapore, Germany, Italy) during training.It can be seen that CLUTR, even without the regret loss, outperforms PAIRED significantly.We note that these test environments were not used in any way, neither during training CLUTR (and PAIRED) nor while designing it.</p>
<p>As mentioned in (Jiang et al., 2021a) PAIRED overexploits the relative strengths of the antagonist over the protagonist and generates a curriculum that gradually reduces the task complexity.However, CLUTR overcomes this and generates a curriculum where the agent and the antagonist closely compete (Figure 17b) and shows a robust generalization on the unseen F1 benchmark.The non-sorted dataset was generated by shuffling each track of the original VAE training dataset 10 different times, resulting in a 10X bigger dataset (10M tracks).It was trained for 5X longer for 5M training steps.We planned on training for 10M gradient steps (10X than the original VAE) but stopped at 5M as it converged much sooner.We ran both CLUTR and CLUTR-shuffled, i.e., CLUTR with a VAE trained on non-sorted data up to 5M timesteps.CLUTR-shuffled shows inferior performance and also signs of unlearning compared to CLUTR. Figure 20 shows detailed experiment results.Figure 21: Impact of pretrained decoder weights on performance.The red curve plots the deviation of the decoder from its pretrained weights as it is finetuned.The green curve shows the performance drop from CLUTR with the standard loss.These curves suggest that pretrained weights are crucial for performance.</p>
<p>C.5. Impact of Task Representation Learning</p>
<p>In this section, we discuss the impact of the learned task representation on performance.In Section 5.3, we showed that if we finetune the VAE decoder during curriculum learning, the overall performance drops significantly (Figure 7).To get a better understanding, in Figure 21, we plot how much the performance deviates as the VAE decoder changes during the training process.The curve in red shows the deviation of the decoder from its pretrained weights as it is fine-tuned during the training.We estimate the deviation as the L2 distance between the finetuned and the pretrained decoder weights.The green curve shows the performance drop from CLUTR (with standard loss).</p>
<p>To estimate the performance drop, we periodically evaluate both CLUTR and CLUTR with Finetuned VAE, on the selected test tracks during training.From the figure, we observe that, as the decoder weights are finetuned, they become increasingly different from the initial pretrained weights.At the same time, the overall performance gap from CLUTR also increases.This suggests that the pretrained VAE weights are crucial for better performance.</p>
<p>Furthermore, the quality of the learned representation depends on the quality of the data they are trained on.In section 5.4, we showed that a VAE trained on a non-sorted dataset significantly deteriorates the performance (Figure 7).This further suggests that the learned representation has a significant impact on performance.We also want to note that both of these variations (CLUTR with Finetuned VAE and the CLUTR with Shuffled VAE) perform much better than PAIRED, which suggests that, though CLUTR's performance depends on the representation, with a reasonable representation, it can still perform better than PAIRED.</p>
<p>D. Detailed Experimental results on on MiniGrid</p>
<p>D.1. CLUTR with flexible regret objective</p>
<p>To train the CLUTR VAE, we generated 10 million random grids, with the obstacle locations sorted, and the number of obstacles uniformly varying from zero to 50, aligning with (Dennis et al., 2020).We train both CLUTR and PAIRED using the flexible regret objectives.</p>
<p>Figure 22 shows zero-shot generalization performance of CLUTR and PAIRED on the 16 unseen navigation tasks from Dennis et al. ( 2020), in terms of the percent of environments the agent solved, i.e., solved rate.CLUTR achieves a 1.35X better generalization solving 58% of the unseen grids, than PAIRED which solves 43% of the unseen grids.It can also be seen that CLUTR outperforms PAIRED on 13 out of the 16 test navigation tasks.</p>
<p>Figure 22: Zero-shot generalization of CLUTR and PAIRED, in terms of percent of the environments solved.CLUTR achieves a higher solved rate than PAIRED in 13 out of the 16 tasks.We evaluate the agents with 10 independent episodes on each task.Error bars denote the standard error.</p>
<p>Performance</p>
<p>Figure 27 shows zero-shot generalization performance of CLUTR and PAIRED on 16 unseen navigation tasks from (Dennis et al., 2020) based on the percent of environments the agent solved, i.e., solved rate.CLUTR achieves superior generalization solving 64% of the unseen grids, a 45.45% improvement over PAIRED, which achieves a 44% solve rate.From figure 27 it can be seen CLUTR outperforms PAIRED achieving a higher mean solve rate on 14 out of the 16 unseen navigation tasks.</p>
<p>Figure 26 shows solved rates on four selected grids (Sixteen Rooms, Sixteen Rooms with Fewer Doors, Labyrinth, and Large Corridor) during training.CLUTR shows better sample efficiency, as well as generalization than PAIRED.</p>
<p>Curriculum Snapshot</p>
<p>In this section, we visually inspect the curriculum generated by CLUTR and PAIRED, with snapshots of tasks generated by these methods during different stages of the training (Figure 28).We illustrate one common mode of failure/ineffectiveness shown by PAIRED: The curriculum starts with arbitrarily complex tasks, which none of the agents can solve at the initial stage of training.After a while, PAIRED starts generating rudimentary degenerate tasks.While kept training, PAIRED eventually gets out of the degenerative local minima, and the curriculum complexity starts to emerge.On the other hand, CLUTR does not show such degeneration and generates seemingly interesting tasks throughout.</p>
<p>Curriculum Analysis</p>
<p>CLUTR VS PAIRED</p>
<p>Figure 30 shows 3D Histograms showing the frequency of the generated grids against the total number of obstacles they contain.PAIRED starts with a high number of obstacles and then degenerates quickly into grids with very few numbers of obstacles and stays similar for a significant number of steps.Eventually, the number of obstacles increases sharply, converging into a band of around 20 to 40 obstacles on average.On the other hand, in CLUTR, the number of obstacles starts flat, centers around a peak around the middle but still with a wide interval for some number of steps, and the peak drops slightly while the interval stays almost the same.After the 'convergence', PAIRED rarely generates grids with fewer or more obstacles than the band it converges to.On the contrary, CLUTR still generates grids with few or many blocks, which might help to address unlearning or improve the agents on grids with more obstacles, respectively.The above observations illustrate that we can achieve a more efficient curriculum learning without making the problem too easy early or without focusing on a narrow interval with a flat distribution later.Instead, we can start with a wide interval and gradually focus on a peak around the middle without making the interval very narrow.Both PAIRED and CLUTR converge to a similar band of grids.However, CLUTR converges much faster.</p>
<p>Figure 30a shows the average episode lengths of both CLUTR and PAIRED.The curves show both methods start with long episodes-indicating at the beginning, the agents do not solve the training grids consistently, and many of the episodes end due to timeout.As the agents learn, the episodes become shorter for both methods until they converge to a small value.However, CLUTR converges sooner than PAIRED.</p>
<p>We also compare the average solution length of the solved training grids.Both PAIRED and CLUTR show a similar pattern.However, PAIRED converges to a larger value than CLUTR.This might indicate that CLUTR is solving the environments more efficiently.This might also mean that CLUTR is solving some easier tasks (e.g., fewer obstacles, as we noticed from Figure 30) even after convergence lowering its average solved path length slightly.</p>
<p>ANALYSIS OF THE LATENT TASK MANIFOLD</p>
<p>To grow a sense of the latent task manifold, we linearly interpolate in the latent space between an empty grid and a 15x15 version of the FourRoom grid (shown in Figure 32).Figure 33 visualizes the interpolation results.We first get the latent vectors of the empty grid and the target FourRoom task using the VAE encoder.We then linearly interpolate 23 equidistant points between them.At last, we reconstruct the grids from these vectors using our decoder.From Figure 33 we see that, as we interpolate in the latent space, the reconstructed grid incrementally adds more obstacles and the grids start to look more like the FourRoom target grid.We note that the reconstruction is not perfect.We also note that the increase in the number of obstacles is not uniform, e.g., the first 5 reconstructed grids are all empty grids, and more are added near the target point.Overall, this experiment provides an insight that the latent space holds a useful structure, which CLUTR teacher utilizes to generate the curriculum.</p>
<p>Figure 1: Hierarchical Graphical Model for CLUTR At the core of CLUTR is the latent generative model representing the latent task manifold.Let's assume that R is a random variable that denotes a measure of success over the agent and antagonist agent and z be a latent random variable that generates environments/tasks, denoted by the random variable E. We use the graphical model shown in Figure-1 to formulate CLUTR.Both E and R are observed variables while z is an unobserved latent variable.R can cover a broad range of measures used in different UED methods including PAIRED and DR (Domain</p>
<p>3) H2: Training VAE on sorted data solves the combinatorial explosion problem.(Section 5.4) At last, we analyze CLUTR curriculum in multiple aspects while comparing it with PAIRED to have a closer understanding.Full details of the environments, network architectures, training hyperparameters, VAE training and further details are discussed in the Appendix.</p>
<p>Figure 2 :
2
Figure 2: Comparison on the F1 Benchmark comprising 20 tracks modeled on real-life F1 racing tracks collected from 10 independent runs.CLUTR achieves 10.6X and 82% higher returns than PAIRED with standard and flexible regret objectives, respectively.CLUTR also performs comparably to the attention-based non-UED CarRacing SOTA, while requiring 500X fewer environment interactions.</p>
<p>Figure 3 :
3
Figure 3: Zero-shot generalization over the course of training by periodic evaluation on a subset of three F1 tracks: Singapore, Germany, and Italy.CLUTR indicate significantly better sample efficiency than PAIRED.</p>
<p>Figure 5: Agent solved rate on the 16 unseen grids from Dennis et al. (2020) during training.CLUTR shows better sample efficiency and generalization than PAIRED.The results show an average of 5 independent runs..</p>
<p>Figure 6 :
6
Figure 6: Example tracks(left) and grids(right) generated by CLUTR(top) and PAIRED(bottom) uniformly sampled at different stages of training.The training progresses from left to right.PAIRED seems to generate over simplified tasks for substantial amount of time hampering agent learning.CLUTR generates interesting tasks throughout.</p>
<p>Figure 7 :
7
Figure7: Impact of i) joint vs two-staged optimization of the task manifold and ii) using a 'Shuffled' VAE, trained on a larger shuffled dataset.The leftmost column shows the default CLUTR performance-i.e., using a pretrained decoder (VAE) trained on sorted training data, kept fixed during the curriculum learning phase-with standard regret objective for CarRacing.Allowing the decoder to finetune with the regret loss results in a 29% performance drop and the use of Shuffled VAE shows a drop of 31%.These performance drops empricially justify our hypotheses H1 and H2.Also, CLUTR with decoder finetuning and Shuffled VAE still outperform PAIRED, with 7.6X and 7.3X better returns, respectively.</p>
<p>Figure 8: Mean standard regret during training.CLUTR shows a smaller regret value indicating a smaller performance gap between the agent and the antagonist, compared to PAIRED.</p>
<p>Figure</p>
<p>Figure 9: Hierarchical Graphical Model for CLUTR</p>
<p>Figure 10 :
10
Figure 10: Snapshots of the test tracks in F1 benchmark</p>
<p>Figure</p>
<p>Figure12and Table4, compares CLUTR with contemporary random-generator UED methods, REPAIRED, and the attention based SOTA.It is to be noted that, CLUTR and PAIRED with flexible regret objective was trained for 2M timesteps.All the other UED methods, along with CLUTR and PAIRED with standard regret was trained for 5M timesteps.</p>
<p>Figure 12 :
12
Figure 12: Comparison on the F1 Benchmark comprising 20 tracks modeled on real-life F1 racing tracks.CLUTR (with flexible regret) emerges as the best adaptive-teacher UED for CarRacing and being the only adaptive-teacher UED to outperform some of the random-generator UEDs.Each of the other adaptive-teacher UEDs (REPAIRED, PAIRED with flexible regret, CLUTR with standard regret) are outperformed by all of the random-generator UEDs (DR, PLR, Robust PLR).CLUTR outperforms the adaptive-teacher PAIRED and REPAIRED by 82% and 58%, respectively, while outperforming Domain Randomization and PLR, by 38% and 16%, repectively.It only falls short to Robust PLR by 14%.The results show mean and standard error of 10 independent runs.</p>
<p>Figure 13 :
13
Figure 13: Comparison of mean agent returns on three tracks: Singapore, Germany, and Italy.Based on this subset of tracks, CLUTR (with flexible regret) shows better generalization than all the other UEDs, except Robust PLR.CLUTR was ahead of Robust PLR till around 3M timesteps, followed by both curves following each other closely, and near the very end Robust PLR surpassed CLUTR.</p>
<p>Figure 14 :
14
Figure 14: Mean return on the training tasks for both the student agents.CLUTR student agents show close performance, while PAIRED students show a bigger gap of performance between them.Closely competing agents can indicate the training tasks being slightly harder than the agents can currently solve, resulting in a smoother curriculum</p>
<p>Figure 16: Mean Regret and agent returns during training CLUTR (with flexible regret) vs CLUTR with standard PAIRED regret approximation.</p>
<p>Figure 17: Mean Regret and agent returns during training CLUTR with standard PAIRED regret loss (i.e., without the flexible regret).CLUTR shows a smaller regret value(i.e., closely competing agent and antagonist), indicating a better UED curriculum.</p>
<p>Figure 18 :
18
Figure18: Zero-shot generalization of both PAIRED and CLUTR (with the standard regret loss) agents after 5M timesteps on the full F1 benchmark.CLUTR with the standard regret loss outperforms PAIRED on every track.For each track, we test the agents on 10 different episodes and the error bar denotes the standard error.</p>
<p>Figure 19 :
19
Figure 19: Test Returns on Selected Tracks (Vanilla, Singapore, Germany, and Italy) of CLUTR with standard PAIRED regret loss alongside PAIRED performance.</p>
<p>(a) During training CLUTR agent achieves higher returns while, CLUTR-shuffled agent shows lower returns.CLUTR-Shuffled agent's return is also less stable showing a decrease and increase.(b) CLUTR achieves higher and more stable mean returns on the selected tracks.CLUTR-Shuffle shows signs of unlearning.</p>
<p>Figure 20 :
20
Figure 20: Analysis of sorting training data for VAE.Trained on shuffled data, CLUTR-Shuffled performs inferior compared to CLUTR and shows signs of unlearning.</p>
<p>Figure 23
23
Figure23compared the mean perforamnce of CLUTR, PAIRED, and REPAIRED.REPAIRED outperforms both PAIRED and CLUTR.We note that, REPAIRED and CLUTR are both improvement towards PAIRED.However, REPAIRED involves a dual-curriculum methods, with two different teachers adopting replay capabilities with disabling exploratory gradients.On the other hand CLUTR is a much simpler method, and can also be augmented with REPAIRED too.</p>
<p>Figure 23 :
23
Figure 23: Mean solve rate on Minigrid testset.REPAIRED outperforms both CLUTR and PAIRED.</p>
<p>Figure 24 :
24
Figure 24: Mean return on the training tasks for both the student agents.CLUTR student agents show close performance, while PAIRED students show a bigger gap of performance between them.Closely competing agents can indicate the training tasks being slightly harder than the agents can currently solve, resulting in a smoother curriculum</p>
<p>Figure 26 :
26
Figure 26: Agent solved rate on selected grids during training.CLUTR shows better sample efficiency and generalization than PAIRED.The results show an average of 5 independent runs.</p>
<p>Figure 27 :
27
Figure27: Zero-shot generalization of CLUTR and PAIRED, in terms of percent of the 14 solved.CLUTR achieves a higher solved rate than PAIRED in 14 out of the 16 unseen tasks.We evaluate the agents with 100 independent episodes on each task.Error bars denote the standard error.</p>
<p>Figure 29 :
29
Figure 29: 3D Histograms showing the frequency of the generated grids against the total number of blocks they contain.Both PAIRED and CLUTR converge to a similar band of grids.However, CLUTR converges much faster.</p>
<p>(a) Average length of the training episodes.CLUTR converges sooner than PAIRED to a shorter episode length.(b) Average solution length of the solved training tasks.</p>
<p>Figure 30 :
30
Figure 30: Comparison of CLUTR and PAIRED curriculum based on properties of the generated grids.</p>
<p>Figure 31: 3D Histograms showing the frequency of the CLUTR generated grids against the total number of blocks they contain vs. Domain Randomization on the latent space vs.A random teacher curriculum on the pretrained latent space.The figures clearly show that CLUTR generates a curriculum significantly different from random curriculums.</p>
<p>Figure 33 :
33
Figure 33: A linear interpolation between an empty grid and 15x15 version of the Four-Room grid (Figure 32) in the latent space.The grids are organized from top-left to bottom-right in row-major order.</p>
<p>Table 4
4
Jiang et al. (2021a) the agents' generalization capabilities during training, by periodically evaluating them on a subset of three unseen F1 tracks: Singapore, Germany, and Italy, which are selected aligning withJiang et al. (2021a).Based on these environments, CLUTR shows significantly better trends of sample efficiency, achieving better generalization with significantly fewer environment interactions compared to PAIRED.Furthermore, CLUTR (with flexible regret) emerges as the best adaptive-teacher UED for CarRacing outperforming the other adaptive-teacher UED: REPAIRED and random-generator UEDs: DR, and PLR by 58%, 38% and 16%, repectively.CLUTR is also the only adaptiveteacher UED that outperforms the random-teacher UED methods.CLUTR falls short (by 14%) only to Robust PLRa random generator dual-curriculum UED with replay and stop-gradient capabilities-a method fundamentally different than ours or, PAIRED.Further discussion can be found in Section C.1.</p>
<p>In International Conference on Machine Learning, pp.9940-9951.PMLR, 2020.Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efficiency in modelfree reinforcement learning from images.In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.10674-10681, 2021.Zhang, A., Ballas, N., and Pineau, J.A dissection of overfitting and generalization in continuous reinforcement learning.arXiv preprint arXiv:1806.07937,2018.</p>
<p>Zhang, M.,Zhang, A., and McDonagh, S.On the out-ofdistribution generalization of probabilistic image modelling.Advances in Neural Information Processing Systems, 34:3811-3823, 2021.Zhang, M., Xiao, T. Z., Paige, B., and Barber, D. Improving vae-based representation learning.arXiv preprint arXiv:2205.14539,2022.</p>
<p>Table 2 :
2
Hyperparameters for training the Task VAE
ParameterValueBatch Size32Number of Training Steps1000000Reconstruction Weight79Latent Variable Size64Word Embedding size300Maximum Sequence Length52Encoder ActivationHyperbolic TangentLearning Rate0.00005Dropout0.3</p>
<p>Table 3 :
3
Hyperparameters for PAIRED and CLUTR PPO training.</p>
<p>64GB and 128 GB Memory for CarRacing and Minigrid experiments.A typical 500M Minigrid training of CLUTR ran with a speed of around 800-900 environment interactions per second, taking around 6-8 days, with 32 parallel workers.CarRacing experiments ran on around 90-110 environment interactions per second with 16 parallel processes.
C. Detailed Experimental results on CarRacingC.1. Detailed Comparison on Full F1 dataset
amazon.com/) and Google Cloud Platform (GCP) -Google Cloud (https://cloud.google.com/).We used a single NVIDIA T4 GPUs for our experiments with machines having 8(16) and 16(32) physical(virtual) cores,</p>
<p>Table 4 :
4
Comparison between CLUTR and other UED algorithms on the individual tracks of the F1 benchmark.We report CLUTR and PAIRED for both standard and flexible regret objectives.We note that, CLUTR and PAIRED with flexible regret was trained for 2M timesteps.All the other UEDs were run for 5M timesteps.Boldface denotes SOTA among UED algorithms, while italic in the Attention Agent column means, CLUTR with Flexible Regret, our best performing model, is comparable/outperforms the attention agent on that track.CLUTR outperforms PAIRED, Domain Randomization, PLR, and REPAIRED and only falls short to Robust PLR.Nonetheless, CLUTR shows comparable results cwith respect to Robust PLR in seven out of the 20 test tracks and outperforming it in the Netherlands track.CLUTR also outperforms the non-UED SOTA on 9 out of the 20 tracks and shows comparableperformance in one.
Track DR PLR Robust PLR REPAIRED PAIRED CLUTR PAIRED CLUTR AttentionStandard Regret Flexible Regret (2M) AgentAustralia 484 ± 29 545 ± 23 692 ± 15 414 ± 27 100 ± 22 429 ± 28 342 ± 29 683 ± 20 826Austria 409 ± 21 442 ± 18 615 ± 13 345 ± 19 92 ± 24 309 ± 19 316 ± 23 507 ± 19 511Bahrain 298 ± 27 411 ± 22 590 ± 15 295 ± 23 -35 ± 19 225 ± 24 183 ± 28 414 ± 20 372Belgium 328 ± 16 327 ± 15 474 ± 12 293 ± 19 72 ± 20 315 ± 14 309 ± 17 429 ± 15 668Brazil 309 ± 23 387 ± 17 455 ± 13 256 ± 19 76 ± 18 244 ± 16 237 ± 16 363 ± 18 145China 115 ± 24 84 ± 20 228 ± 24 7 ± 18 -101 ± 9 33 ± 19 23 ± 21 254 ± 28 344France 279 ± 32 290 ± 35 478 ± 22 240 ± 29 -81 ± 13 266 ± 30 158 ± 24 498 ± 31 153Germany 274 ± 23 388 ± 20 499 ± 18 272 ± 22 -33 ± 16 195 ± 26 286 ± 26 404 ± 20 214Hungary 465 ± 32 533 ± 26 708 ± 17 414 ± 29 98 ± 29 325 ± 32 327 ± 31 630 ± 24 769Italy 461 ± 27 588 ± 20 625 ± 12 371 ± 25 132 ± 24 439 ± 31 451 ± 27 639 ± 16 798Malaysia 236 ± 25 283 ± 20 400 ± 18 200 ± 17 -26 ± 17 174 ± 23 192 ± 21 426 ± 22 300Mexico 458 ± 33 561 ± 21 712 ± 12 415 ± 30 67 ± 31 387 ± 31 391 ± 30 627 ± 19 580Monaco 268 ± 28 360 ± 32 486 ± 19 256 ± 26 -28 ± 18 234 ± 30 125 ± 28 460 ± 29 835Netherlands 328 ± 26 418 ± 21 419 ± 25 307 ± 21 70 ± 20 302 ± 27 306 ± 24 488 ± 21 131Portugal 324 ± 27 407 ± 15 483 ± 13 265 ± 21 -49 ± 13 299 ± 24 149 ± 19 462 ± 20 606Russia 382 ± 30 479 ± 24 649 ± 14 419 ± 25 51 ± 21 319 ± 25 337 ± 24 497 ± 23 732Singapore 336 ± 29 386 ± 22 566 ± 15 274 ± 21 -35 ± 14 229 ± 18 192 ± 21 382 ± 19 276Spain 433 ± 24 482 ± 17 622 ± 14 358 ± 24 134 ± 24 373 ± 15 414 ± 19 496 ± 15 759UK 393 ± 28 456 ± 16 538 ± 17 380 ± 22 138 ± 25 396 ± 18 339 ± 18 471 ± 19 729USA 263 ± 31 243 ± 28 381 ± 33 120 ± 25 -119 ± 11 27 ± 29 67 ± 29 238 ± 31 -192Mean 342 ± 27 404 ± 22 531 ± 17 295 ± 23 26 ± 19 276 ± 24 257 ± 16 468 ± 21 478
Consider a 13x13 grid for a navigation task, where the locations are numbered from 1 to 169. Also consider a wall made of
In the original PAIRED paper, the primary student agent was named protagonist. Throughout this paper we refer it simply as the agent.
AcknowledgmentsThis project is a collaboration under the Google-BAIR (Berkeley Artifcial Intelligence Research) Commons program.It was supported in part by NSF CISE Expeditions Award CCF-1730628, in addition to gifts from Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Nexla, Samsung SDS, Uber, and VMware.We gratefully acknowledge Natasha Jaques for their valuable feedback.We also acknowledge Raymond Chong, Adrian Liu, and Sarah Bhaskaran for the valuable discussions.Ethic StatementUnsupervised Environment Design can be applied to many real-world applications and shares similar ethical concerns and considerations with other Artificially Intelligent(AI) systems.For example, AI systems can cause more unemployment or be used for reasons/applications that have a negative societal impact, for which responsible usage of such AI systems must be promoted and established.During our research, all the experiments were done in simulation and no human or living subjects were used.ReproducibilityOur code, saved checkpoints, and training data are available at https://github.com/clutr/clutr
Programmatic modeling and generation of real-time strategic soccer environments for reinforcement learning. A S Azad, E Kim, Q Wu, K Lee, I Stoica, P Abbeel, A Sangiovanni-Vincentelli, S A Seshia, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Debiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>S R Bowman, L Vilnis, O Vinyals, A M Dai, R Jozefowicz, S Bengio, arXiv:1511.06349Generating sentences from a continuous space. 2015arXiv preprint</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>M Chevalier-Boisvert, L Willems, S Pal, Minimalistic gridworld environment for gymnasium. 2018</p>
<p>Unsupervised speech representation learning using wavenet autoencoders. J Chorowski, R J Weiss, S Bengio, A Van Den Oord, 10.1109/TASLP.2019.2938863Speech, and Language Processing. 201927</p>
<p>Quantifying generalization in reinforcement learning. K Cobbe, O Klimov, C Hesse, T Kim, J Schulman, International Conference on Machine Learning. PMLR2019</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. M Dennis, N Jaques, E Vinitsky, A Bayen, S Russell, A Critch, S Levine, Advances in neural information processing systems. 202033</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, International conference on machine learning. PMLR2018</p>
<p>I Gulrajani, K Kumar, F Ahmed, A A Taiga, F Visin, D Vazquez, A Courville, Pixelvae, arXiv:1611.05013A latent variable model for natural images. 2016arXiv preprint</p>
<p>. I Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner, Betavae, 2016Learning basic visual concepts with a constrained variational framework</p>
<p>Creativity: Generating diverse questions using variational autoencoders. U Jain, Z Zhang, A G Schwing, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Evolutionary robotics and the radical envelopeof-noise hypothesis. N Jakobi, Adaptive behavior. 621997</p>
<p>Transformer vae: A hierarchical model for structure-aware and interpretable music representation learning. J Jiang, G G Xia, D B Carlton, C N Anderson, R H Miyakawa, 10.1109/ICASSP40776.2020.9054554ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing. 2020</p>
<p>Replay-guided adversarial environment design. M Jiang, M Dennis, J Parker-Holder, J Foerster, E Grefenstette, T Rocktäschel, Advances in Neural Information Processing Systems. 342021a</p>
<p>Prioritized level replay. M Jiang, E Grefenstette, T Rocktäschel, International Conference on Machine Learning. PMLR2021b</p>
<p>Learning to drive in a day. A Kendall, J Hawke, D Janz, P Mazur, D Reda, J.-M Allen, V.-D Lam, A Bewley, A Shah, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>Policy-gradient algorithms have no guarantees of convergence in linear quadratic games. E Mazumdar, L J Ratliff, M I Jordan, S S Sastry, 2019</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Robust reinforcement learning. J Morimoto, K Doya, Neural computation. 1722005</p>
<p>Mathematics for computer graphics applications. M E Mortenson, 1999Industrial Press Inc</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. A V Nair, V Pong, M Dalal, S Bahl, S Lin, S Levine, S Bengio, H Wallach, H Larochelle, K Grauman, S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, arXiv:2003.04960Advances in Neural Information Processing Systems. N Cesa-Bianchi, R Garnett, Curran Associates, Inc2018. 202031arXiv preprintVisual reinforcement learning with imagined goals</p>
<p>J Parker-Holder, M Jiang, M Dennis, M Samvelyan, J Foerster, E Grefenstette, T Rocktäschel, arXiv:2203.01302Evolving curricula with regret-based environment design. 2022arXiv preprint</p>
<p>Supervision via competition: Robot adversaries for learning tasks. L Pinto, J Davidson, A Gupta, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>
<p>Automatic curriculum learning for deep rl: A short survey. R Portelas, C Colas, L Weng, K Hofmann, P.-Y Oudeyer, arXiv:2003.046642020arXiv preprint</p>
<p>Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems. A Razavi, A Van Den Oord, O Vinyals, 201932</p>
<p>Stochastic backpropagation and approximate inference in deep generative models. D J Rezende, S Mohamed, D Wierstra, International conference on machine learning. PMLR2014</p>
<p>Real single-image flight without a single real image. F Sadeghi, S Levine, Cad2rl, 2016</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, 2017</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Neuroevolution of selfinterpretable agents. Y Tang, D Nguyen, D Ha, Proceedings of the 2020 Genetic and Evolutionary Computation Conference. the 2020 Genetic and Evolutionary Computation Conference2020</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-719 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-719</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-719</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-fbe25e4f069a19dc63daca27b7c98cff338663b9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fbe25e4f069a19dc63daca27b7c98cff338663b9" target="_blank">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task are described.</p>
                <p><strong>Paper Abstract:</strong> Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas. 
To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task. 
We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e719.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e719.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Doc-vs-Query Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between function documentation and real user search queries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Documentation scraped from source (function docstrings/comments) differs in vocabulary, register, and intent from real user search queries, making documentation a poor proxy training signal for code search models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeSearchNet Corpus / CodeSearchNet Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A large corpus of function-documentation pairs scraped from open-source repositories and a challenge of 99 natural-language queries with human relevance annotations to evaluate semantic code search.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>API / function documentation (scraped docstrings/comments)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function implementations from open-source repositories (multiple languages)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different language register / mismatch between documentation and user queries</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Documentation is typically written by the same authors as the code, using the same vocabulary and style as the implementation, whereas real search queries are more generic, vary in wording, and come from different authors; hence models trained on doc->code pairs do not generalize well to user queries.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data and labeling (data-source mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>qualitative analysis in Limitations and empirical evaluation showing different model rankings on the training-proxy task vs. the human query challenge (comparison of baseline performances, model behavior observations)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Indirectly quantified by comparing model performance between Corpus training proxy (MRR on validation/test; Table 3) and Challenge evaluation (NDCG Within/All; Table 4). Example: Self-attention model average MRR on the training proxy = 0.7011 (Table 3) but average NDCG Within on Challenge = 0.493 (Table 4); NBoW average MRR = 0.6167 vs NDCG Within = 0.574, showing relative performance shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Models optimized on documentation pairs showed degraded or different ranking behavior on real queries; simpler keyword-based models (NBoW, ElasticSearch) outperformed or were competitive with more complex neural models on the human query challenge, indicating training-on-docs led to suboptimal real-world performance. Quantitatively: gap in NDCG Within between NBoW (avg 0.574) and SelfAtt (avg 0.493) ~0.081; in NDCG All NBoW avg 0.340 vs SelfAtt avg 0.240 (~0.100).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as systematic for the dataset; no global percentage given. Paper notes the issue is fundamental to the scraping approach (affects many or most scraped doc->code pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Using scraped documentation as proxy for search queries (data-source mismatch) and differing authorship/context of docs vs. users' queries.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Collecting real search queries (they built a 99-query challenge from Bing queries and StaQC rewrites), use human relevance annotations, release preprocessing pipeline to enable better data curation, and suggest future work on pretraining and methods that handle rare terms and semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: using the CodeSearchNet Challenge (human queries + annotations) revealed the mismatch and enabled evaluation; no evaluated end-to-end mitigation that fully resolves mismatch reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / information retrieval for source code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e719.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Outdated/Incorrect Docs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Outdated or inaccurate documentation relative to code implementation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Many scraped documentation comments do not correctly describe their associated function implementation (e.g., comments are outdated), causing incorrect or noisy training pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeSearchNet Corpus</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Corpus constructed by pairing functions with scraped documentation; used to train encoders for semantic code search.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>function documentation / comments</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function implementations (source code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>outdated / incorrect documentation (mismatch between description and implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some documentation entries are out of date or incorrect with respect to the current implementation, so the natural-language description does not faithfully represent the code; this introduces label noise in training and candidate selection.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / labeling (data quality)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual inspection and reported as a limitation; annotators and authors observed examples where comments did not match code behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantified in the paper (authors explicitly state they are 'unable to know the extent' of the inaccuracies).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Introduces noisy training pairs that can mislead models and reduce generalization; degrades quality of learned embeddings and relevance predictions. No numeric effect size provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified — noted as a known limitation / present in the scraped dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mechanical scraping of documentation without semantic verification; code and comments evolve separately leading to drift.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Filtering heuristics (remove very short docs, truncate to first paragraph), duplicate removal, and collecting human annotations for evaluation; authors also released preprocessing code to enable others to improve cleaning.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effectiveness not quantitatively assessed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset curation for code search</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e719.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Documentation language mismatch (non-English docs vs English queries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Some scraped documentation is written in languages other than English while the evaluation queries and challenge focus on English, creating a language mismatch between training data and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeSearchNet Corpus and Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Corpus scraped across projects worldwide; evaluation queries are English-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>function documentation (various natural languages)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>language mismatch between natural-language descriptions and evaluation queries</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Documentation written in other languages (non-English) cannot be used effectively to train or evaluate models for English queries, reducing useful training data and potentially harming model performance on English search.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset composition / preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>dataset inspection reported in Limitations; authors note some docs are in other languages.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantified (no percentage of non-English docs given).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces effective dataset size for English-focused models and may produce irrelevant or low-quality training pairs for the target evaluation language; no quantitative impact reported.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Scraping multilingual open-source repositories and not filtering by language.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors restrict evaluation to English queries; future work could filter docs by language or apply translation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e719.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Truncation Effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Documentation truncation to first paragraph removing important details</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To make documentation resemble queries, the corpus truncates docstrings to the first paragraph, which can remove critical information like argument/return descriptions and create misaligned training signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeSearchNet Corpus preprocessing pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated preprocessing that truncates documentation to the first full paragraph and applies other heuristics to construct doc->code pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>function documentation truncated to first paragraph</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing step / incomplete specification due to truncation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Truncation removes in-depth discussion (e.g., argument and return descriptions) that might be informative for matching code to queries; the truncated doc may no longer fully describe the function's behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>explicitly described preprocessing choice and qualitative rationale in section 2; recognized as a trade-off to make docs shorter like queries.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantified (no ablation comparing truncated vs full documentation).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>May reduce descriptive content necessary for training, contributing to poorer model grounding and mismatches between training and true search behavior; not numerically measured.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Applies to all doc entries in the corpus due to the applied preprocessing rule.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Design decision to truncate documentation to resemble query length; heuristic-based preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Release of preprocessing pipeline so others can alter this choice; authors recommend users may employ the same split but external researchers can experiment with different truncation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e719.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Directionality Errors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic directionality mismatch (inverse-function retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline models sometimes retrieve functions implementing the inverse of the queried operation (e.g., returning string->int when the query asks int->string), indicating limitations in semantic understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Baseline retrieval models (joint embeddings and ElasticSearch)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encoders trained to embed documentation and code into a joint vector space and baselines used for candidate generation and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>user search queries</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>function implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / semantic misunderstanding (directionality)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Models fail to capture the directionality or precise semantic intent of queries and sometimes return functions that perform the inverse operation of what was requested.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model understanding / embedding and retrieval stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>annotator qualitative observations noted in the 'Qualitative Observations' section and examples in annotation UI feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantitatively measured (no count of such cases provided), reported qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to lower relevance scores and incorrect search results; contributes to lower user satisfaction and decreased measured performance on human query evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as a 'common problem' in returned results by annotators; no numerical prevalence reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Embedding models primarily matching lexical/surface cues and failing to model deeper semantics like input/output types or functional direction; training objective encourages proximity of related but semantically different functions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors suggest leveraging code semantics (control/data flow, type information) and richer modeling to capture precise functionality.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / semantic code search</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e719.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-Quality Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Returned code quality mismatch affecting relevance judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Search results may be functionally correct but of low code quality (readability, security, antipatterns), causing annotators to penalize relevance despite functional correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeSearchNet evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Human annotation interface where annotators rate relevance; returned functions drawn from diverse projects of varying code quality.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>user queries / annotation instructions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>open-source function implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation quality vs. description mismatch / subjective filtering</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Annotators rated some semantically-correct results lower because the implementation was poor (bad practices, unreadable, insecure), indicating that relevance is influenced by code quality beyond mere functional match.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation / human relevance assessment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>qualitative observations reported from annotators and notes in the annotation interface.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantified (no systematic code-quality scoring provided); observed in annotator comments and influence on relevance distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Affects relevance scores in the annotated dataset and therefore the evaluation of systems; may bias evaluation toward cleaner code rather than purely functionally correct code.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as a subset of returned results; no frequency statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Heterogeneous source quality in scraped repositories and human preferences/preferences for high-quality code examples.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors propose using code quality as an additional signal to filter or rank results when better results are available.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / information retrieval for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e719.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Indexing/Config Bug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Indexing configuration bug impacting evaluation (Annoy index trees)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bug or suboptimal configuration in the Annoy approximate nearest-neighbor index (too few trees) led to significant underperformance of baseline models until identified and fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Annoy-based nearest-neighbor index for embedding retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At test time, all functions are indexed with Annoy for fast approximate nearest neighbor search; index parameters (number of trees) affect retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>implementation documentation / system configuration (implicit in code)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>indexing and evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation/configuration bug affecting evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Baseline models underperformed when the Annoy index was built with a small number of trees (a suboptimal configuration/bug), significantly impacting evaluation results until identified and corrected.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>retrieval infrastructure / approximate nearest-neighbor indexing (evaluation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>debugging reported in Acknowledgments (external contributor Rok Novosel pinpointed the bug); empirical observation that performance varied with Annoy tree count.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Effect described qualitatively as 'significantly impacting evaluation results'; paper also notes empirical underperformance with a small number of trees but no exact numeric delta provided.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Significant — evaluation scores were materially affected by index configuration; after fixing the bug/index tuning, results improved (exact numeric impact not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Single known occurrence in this project's evaluation pipeline; represents a common class of experimental misconfiguration risk.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Implementation/configuration oversight in building approximate nearest-neighbor index (insufficient number of trees).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Fix the bug and tune Annoy index parameters (use sufficient number of trees); acknowledge and credit bug finder; recommend careful index construction for reproducible evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported as effective (fix corrected significant evaluation impact), though no quantitative before/after numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / evaluation infrastructure</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e719.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training Objective Mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between training objective (doc->code pairing) and target search task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The optimization objective used during training (maximize embedding inner product of doc->code pairs with distractors) does not fully match the downstream retrieval task for human queries, leading to differences in which models perform best.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Joint embedding training pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encoders for code and natural language are trained to place paired doc and code embeddings near each other in vector space using a softmax inner-product loss over distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>documentation-to-code pairing described in paper</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>neural encoder training scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / training objective mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The training loss and proxy task (docs as queries) optimize a different objective than real-world code search with user queries, causing high-capacity models that do well on the training proxy to perform worse on the human query challenge compared to simpler keyword models.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model training objective and evaluation alignment</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical evaluation comparing MRR on corpus-proxy task (Table 3) to NDCG on the human query challenge (Table 4) and observing rank-order changes among models.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparison of performance metrics across tasks: e.g., SelfAtt average MRR on training proxy = 0.7011, but SelfAtt average NDCG Within on Challenge = 0.493; NBoW MRR 0.6167 vs NDCG Within 0.574. These shifts indicate the training objective does not align perfectly with the challenge objective.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Caused model selection paradox where more complex models (higher capacity) that excel on the training proxy do not necessarily yield better search quality for human queries; influenced baseline choice and interpretation of results.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across multiple model variants in the paper; not given as a percent but described as a key finding.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Proxy-training data (documentation) and objective mismatch with real user search behavior and evaluation metric; lexical vs semantic differences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors suggest using more realistic training data (actual queries), improving models to handle rare terms and semantics, and exploring pretraining approaches; they also provide a human-annotated challenge to better align evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in this paper; the challenge itself exposes the mismatch but does not resolve it.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / representation learning for IR</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e719.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Annotation Selection Bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Candidate pre-filtering bias in annotation (ensemble of baselines used to pick candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To limit annotation cost, candidate functions shown to annotators were generated by an ensemble of baseline models and ElasticSearch, which can bias the annotated dataset toward what those baselines retrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Annotation pipeline for the CodeSearchNet Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Annotators judged relevance for top-10 candidate results per query/language returned by an ensemble of baseline models and ElasticSearch.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>human query set (collected from Bing and StaQC)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>candidate selection code combining model and ElasticSearch outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>selection bias in annotation / incomplete specification of candidate space</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Using only ensemble-generated candidate lists constrains annotation to that pool, potentially excluding relevant functions that baselines would not retrieve and biasing evaluation in favor of the candidate-generating systems.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>annotation candidate generation / evaluation dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Method described in Expert Annotations; authors acknowledge using ensembles to generate candidates and note the limitation implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not quantitatively measured (no estimate of how many true positives were omitted); process generated 10 candidates per query per language.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>May limit evaluation coverage and bias leaderboard comparisons toward systems similar to the candidate generators; reduces ability to detect systems that retrieve diverse true positives outside the candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Applies to all annotated query/language pairs in the challenge (design choice).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Practical constraint of annotation cost and scalability leading to reliance on baseline outputs to reduce annotation search space.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Authors prioritize widening the annotation set in future iterations and suggest expanding annotator pool and candidate generation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated; proposed as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>dataset construction / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e719.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e719.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Duplicate Removal Effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of duplicate / near-duplicate removal on dataset representativeness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Removing near-duplicate functions (to avoid auto-generated code and copy/paste) can eliminate legitimate variants and reduce diversity or remove repeated implementations that reflect real code reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeSearchNet preprocessing and deduplication pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Dataset deduplication using techniques from prior work (Allamanis, Lopes et al.) to remove multiple versions of auto-generated or copied code.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>documentation pairing description (dataset documentation)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>corpus preprocessing code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>data curation / potential over-filtering</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Deduplication heuristics remove near-duplicate functions to avoid training artifacts, but this may also remove meaningful variants and reduce the natural distribution of code, which can affect model learning and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / corpus construction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Described in Filtering section and linked prior work; rationale and potential trade-offs discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>No quantitative measure of impact in this paper; deduplication changed dataset counts but effect on model performance not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Could reduce model overfitting to duplicated examples but also remove legitimate examples and alter generalization; specific effect not measured here.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Deduplication applied globally to corpus as part of preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Need to avoid training on duplicated data balanced against preserving real-world diversity; heuristic deduplication choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use established deduplication methods and make preprocessing code public so others can experiment with thresholds or disable deduplication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>dataset curation for machine learning on code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Adverse Effects of Code Duplication in Machine Learning Models of Code <em>(Rating: 2)</em></li>
                <li>When Deep Learning Met Code Search <em>(Rating: 2)</em></li>
                <li>Deep code search <em>(Rating: 2)</em></li>
                <li>Structured Neural Summarization <em>(Rating: 1)</em></li>
                <li>StaQC: A Systematically Mixed Question-Code Dataset from Stack Overflow <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-719",
    "paper_id": "paper-fbe25e4f069a19dc63daca27b7c98cff338663b9",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Doc-vs-Query Mismatch",
            "name_full": "Mismatch between function documentation and real user search queries",
            "brief_description": "Documentation scraped from source (function docstrings/comments) differs in vocabulary, register, and intent from real user search queries, making documentation a poor proxy training signal for code search models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeSearchNet Corpus / CodeSearchNet Challenge",
            "system_description": "A large corpus of function-documentation pairs scraped from open-source repositories and a challenge of 99 natural-language queries with human relevance annotations to evaluate semantic code search.",
            "nl_description_type": "API / function documentation (scraped docstrings/comments)",
            "code_implementation_type": "function implementations from open-source repositories (multiple languages)",
            "gap_type": "different language register / mismatch between documentation and user queries",
            "gap_description": "Documentation is typically written by the same authors as the code, using the same vocabulary and style as the implementation, whereas real search queries are more generic, vary in wording, and come from different authors; hence models trained on doc-&gt;code pairs do not generalize well to user queries.",
            "gap_location": "training data and labeling (data-source mismatch)",
            "detection_method": "qualitative analysis in Limitations and empirical evaluation showing different model rankings on the training-proxy task vs. the human query challenge (comparison of baseline performances, model behavior observations)",
            "measurement_method": "Indirectly quantified by comparing model performance between Corpus training proxy (MRR on validation/test; Table 3) and Challenge evaluation (NDCG Within/All; Table 4). Example: Self-attention model average MRR on the training proxy = 0.7011 (Table 3) but average NDCG Within on Challenge = 0.493 (Table 4); NBoW average MRR = 0.6167 vs NDCG Within = 0.574, showing relative performance shifts.",
            "impact_on_results": "Models optimized on documentation pairs showed degraded or different ranking behavior on real queries; simpler keyword-based models (NBoW, ElasticSearch) outperformed or were competitive with more complex neural models on the human query challenge, indicating training-on-docs led to suboptimal real-world performance. Quantitatively: gap in NDCG Within between NBoW (avg 0.574) and SelfAtt (avg 0.493) ~0.081; in NDCG All NBoW avg 0.340 vs SelfAtt avg 0.240 (~0.100).",
            "frequency_or_prevalence": "Described as systematic for the dataset; no global percentage given. Paper notes the issue is fundamental to the scraping approach (affects many or most scraped doc-&gt;code pairs).",
            "root_cause": "Using scraped documentation as proxy for search queries (data-source mismatch) and differing authorship/context of docs vs. users' queries.",
            "mitigation_approach": "Collecting real search queries (they built a 99-query challenge from Bing queries and StaQC rewrites), use human relevance annotations, release preprocessing pipeline to enable better data curation, and suggest future work on pretraining and methods that handle rare terms and semantics.",
            "mitigation_effectiveness": "Partially effective: using the CodeSearchNet Challenge (human queries + annotations) revealed the mismatch and enabled evaluation; no evaluated end-to-end mitigation that fully resolves mismatch reported.",
            "domain_or_field": "machine learning / information retrieval for source code",
            "reproducibility_impact": true,
            "uuid": "e719.0",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Outdated/Incorrect Docs",
            "name_full": "Outdated or inaccurate documentation relative to code implementation",
            "brief_description": "Many scraped documentation comments do not correctly describe their associated function implementation (e.g., comments are outdated), causing incorrect or noisy training pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeSearchNet Corpus",
            "system_description": "Corpus constructed by pairing functions with scraped documentation; used to train encoders for semantic code search.",
            "nl_description_type": "function documentation / comments",
            "code_implementation_type": "function implementations (source code)",
            "gap_type": "outdated / incorrect documentation (mismatch between description and implementation)",
            "gap_description": "Some documentation entries are out of date or incorrect with respect to the current implementation, so the natural-language description does not faithfully represent the code; this introduces label noise in training and candidate selection.",
            "gap_location": "data preprocessing / labeling (data quality)",
            "detection_method": "manual inspection and reported as a limitation; annotators and authors observed examples where comments did not match code behavior.",
            "measurement_method": "Not quantified in the paper (authors explicitly state they are 'unable to know the extent' of the inaccuracies).",
            "impact_on_results": "Introduces noisy training pairs that can mislead models and reduce generalization; degrades quality of learned embeddings and relevance predictions. No numeric effect size provided.",
            "frequency_or_prevalence": "Not quantified — noted as a known limitation / present in the scraped dataset.",
            "root_cause": "Mechanical scraping of documentation without semantic verification; code and comments evolve separately leading to drift.",
            "mitigation_approach": "Filtering heuristics (remove very short docs, truncate to first paragraph), duplicate removal, and collecting human annotations for evaluation; authors also released preprocessing code to enable others to improve cleaning.",
            "mitigation_effectiveness": "Effectiveness not quantitatively assessed in the paper.",
            "domain_or_field": "machine learning / dataset curation for code search",
            "reproducibility_impact": true,
            "uuid": "e719.1",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Language Mismatch",
            "name_full": "Documentation language mismatch (non-English docs vs English queries)",
            "brief_description": "Some scraped documentation is written in languages other than English while the evaluation queries and challenge focus on English, creating a language mismatch between training data and evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeSearchNet Corpus and Challenge",
            "system_description": "Corpus scraped across projects worldwide; evaluation queries are English-focused.",
            "nl_description_type": "function documentation (various natural languages)",
            "code_implementation_type": "function implementations",
            "gap_type": "language mismatch between natural-language descriptions and evaluation queries",
            "gap_description": "Documentation written in other languages (non-English) cannot be used effectively to train or evaluate models for English queries, reducing useful training data and potentially harming model performance on English search.",
            "gap_location": "dataset composition / preprocessing",
            "detection_method": "dataset inspection reported in Limitations; authors note some docs are in other languages.",
            "measurement_method": "Not quantified (no percentage of non-English docs given).",
            "impact_on_results": "Reduces effective dataset size for English-focused models and may produce irrelevant or low-quality training pairs for the target evaluation language; no quantitative impact reported.",
            "frequency_or_prevalence": "Not specified.",
            "root_cause": "Scraping multilingual open-source repositories and not filtering by language.",
            "mitigation_approach": "Authors restrict evaluation to English queries; future work could filter docs by language or apply translation.",
            "mitigation_effectiveness": "Not evaluated in the paper.",
            "domain_or_field": "machine learning / dataset curation",
            "reproducibility_impact": true,
            "uuid": "e719.2",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Truncation Effects",
            "name_full": "Documentation truncation to first paragraph removing important details",
            "brief_description": "To make documentation resemble queries, the corpus truncates docstrings to the first paragraph, which can remove critical information like argument/return descriptions and create misaligned training signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeSearchNet Corpus preprocessing pipeline",
            "system_description": "Automated preprocessing that truncates documentation to the first full paragraph and applies other heuristics to construct doc-&gt;code pairs.",
            "nl_description_type": "function documentation truncated to first paragraph",
            "code_implementation_type": "function implementations",
            "gap_type": "missing preprocessing step / incomplete specification due to truncation",
            "gap_description": "Truncation removes in-depth discussion (e.g., argument and return descriptions) that might be informative for matching code to queries; the truncated doc may no longer fully describe the function's behavior.",
            "gap_location": "data preprocessing",
            "detection_method": "explicitly described preprocessing choice and qualitative rationale in section 2; recognized as a trade-off to make docs shorter like queries.",
            "measurement_method": "Not quantified (no ablation comparing truncated vs full documentation).",
            "impact_on_results": "May reduce descriptive content necessary for training, contributing to poorer model grounding and mismatches between training and true search behavior; not numerically measured.",
            "frequency_or_prevalence": "Applies to all doc entries in the corpus due to the applied preprocessing rule.",
            "root_cause": "Design decision to truncate documentation to resemble query length; heuristic-based preprocessing.",
            "mitigation_approach": "Release of preprocessing pipeline so others can alter this choice; authors recommend users may employ the same split but external researchers can experiment with different truncation strategies.",
            "mitigation_effectiveness": "Not evaluated in the paper.",
            "domain_or_field": "machine learning / dataset preprocessing",
            "reproducibility_impact": true,
            "uuid": "e719.3",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Directionality Errors",
            "name_full": "Semantic directionality mismatch (inverse-function retrieval)",
            "brief_description": "Baseline models sometimes retrieve functions implementing the inverse of the queried operation (e.g., returning string-&gt;int when the query asks int-&gt;string), indicating limitations in semantic understanding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Baseline retrieval models (joint embeddings and ElasticSearch)",
            "system_description": "Encoders trained to embed documentation and code into a joint vector space and baselines used for candidate generation and evaluation.",
            "nl_description_type": "user search queries",
            "code_implementation_type": "function implementations",
            "gap_type": "ambiguous description / semantic misunderstanding (directionality)",
            "gap_description": "Models fail to capture the directionality or precise semantic intent of queries and sometimes return functions that perform the inverse operation of what was requested.",
            "gap_location": "model understanding / embedding and retrieval stage",
            "detection_method": "annotator qualitative observations noted in the 'Qualitative Observations' section and examples in annotation UI feedback.",
            "measurement_method": "Not quantitatively measured (no count of such cases provided), reported qualitatively.",
            "impact_on_results": "Leads to lower relevance scores and incorrect search results; contributes to lower user satisfaction and decreased measured performance on human query evaluations.",
            "frequency_or_prevalence": "Described as a 'common problem' in returned results by annotators; no numerical prevalence reported.",
            "root_cause": "Embedding models primarily matching lexical/surface cues and failing to model deeper semantics like input/output types or functional direction; training objective encourages proximity of related but semantically different functions.",
            "mitigation_approach": "Authors suggest leveraging code semantics (control/data flow, type information) and richer modeling to capture precise functionality.",
            "mitigation_effectiveness": "Not evaluated in this work.",
            "domain_or_field": "machine learning / semantic code search",
            "reproducibility_impact": false,
            "uuid": "e719.4",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Code-Quality Bias",
            "name_full": "Returned code quality mismatch affecting relevance judgments",
            "brief_description": "Search results may be functionally correct but of low code quality (readability, security, antipatterns), causing annotators to penalize relevance despite functional correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeSearchNet evaluation pipeline",
            "system_description": "Human annotation interface where annotators rate relevance; returned functions drawn from diverse projects of varying code quality.",
            "nl_description_type": "user queries / annotation instructions",
            "code_implementation_type": "open-source function implementations",
            "gap_type": "implementation quality vs. description mismatch / subjective filtering",
            "gap_description": "Annotators rated some semantically-correct results lower because the implementation was poor (bad practices, unreadable, insecure), indicating that relevance is influenced by code quality beyond mere functional match.",
            "gap_location": "evaluation / human relevance assessment",
            "detection_method": "qualitative observations reported from annotators and notes in the annotation interface.",
            "measurement_method": "Not quantified (no systematic code-quality scoring provided); observed in annotator comments and influence on relevance distributions.",
            "impact_on_results": "Affects relevance scores in the annotated dataset and therefore the evaluation of systems; may bias evaluation toward cleaner code rather than purely functionally correct code.",
            "frequency_or_prevalence": "Reported as a subset of returned results; no frequency statistics provided.",
            "root_cause": "Heterogeneous source quality in scraped repositories and human preferences/preferences for high-quality code examples.",
            "mitigation_approach": "Authors propose using code quality as an additional signal to filter or rank results when better results are available.",
            "mitigation_effectiveness": "Not evaluated in the paper.",
            "domain_or_field": "machine learning / information retrieval for code",
            "reproducibility_impact": false,
            "uuid": "e719.5",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Indexing/Config Bug",
            "name_full": "Indexing configuration bug impacting evaluation (Annoy index trees)",
            "brief_description": "A bug or suboptimal configuration in the Annoy approximate nearest-neighbor index (too few trees) led to significant underperformance of baseline models until identified and fixed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Annoy-based nearest-neighbor index for embedding retrieval",
            "system_description": "At test time, all functions are indexed with Annoy for fast approximate nearest neighbor search; index parameters (number of trees) affect retrieval quality.",
            "nl_description_type": "implementation documentation / system configuration (implicit in code)",
            "code_implementation_type": "indexing and evaluation scripts",
            "gap_type": "implementation/configuration bug affecting evaluation",
            "gap_description": "Baseline models underperformed when the Annoy index was built with a small number of trees (a suboptimal configuration/bug), significantly impacting evaluation results until identified and corrected.",
            "gap_location": "retrieval infrastructure / approximate nearest-neighbor indexing (evaluation pipeline)",
            "detection_method": "debugging reported in Acknowledgments (external contributor Rok Novosel pinpointed the bug); empirical observation that performance varied with Annoy tree count.",
            "measurement_method": "Effect described qualitatively as 'significantly impacting evaluation results'; paper also notes empirical underperformance with a small number of trees but no exact numeric delta provided.",
            "impact_on_results": "Significant — evaluation scores were materially affected by index configuration; after fixing the bug/index tuning, results improved (exact numeric impact not provided).",
            "frequency_or_prevalence": "Single known occurrence in this project's evaluation pipeline; represents a common class of experimental misconfiguration risk.",
            "root_cause": "Implementation/configuration oversight in building approximate nearest-neighbor index (insufficient number of trees).",
            "mitigation_approach": "Fix the bug and tune Annoy index parameters (use sufficient number of trees); acknowledge and credit bug finder; recommend careful index construction for reproducible evaluation.",
            "mitigation_effectiveness": "Reported as effective (fix corrected significant evaluation impact), though no quantitative before/after numbers are given.",
            "domain_or_field": "machine learning / evaluation infrastructure",
            "reproducibility_impact": true,
            "uuid": "e719.6",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Training Objective Mismatch",
            "name_full": "Mismatch between training objective (doc-&gt;code pairing) and target search task",
            "brief_description": "The optimization objective used during training (maximize embedding inner product of doc-&gt;code pairs with distractors) does not fully match the downstream retrieval task for human queries, leading to differences in which models perform best.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Joint embedding training pipeline",
            "system_description": "Encoders for code and natural language are trained to place paired doc and code embeddings near each other in vector space using a softmax inner-product loss over distractors.",
            "nl_description_type": "documentation-to-code pairing described in paper",
            "code_implementation_type": "neural encoder training scripts",
            "gap_type": "different algorithm variant / training objective mismatch",
            "gap_description": "The training loss and proxy task (docs as queries) optimize a different objective than real-world code search with user queries, causing high-capacity models that do well on the training proxy to perform worse on the human query challenge compared to simpler keyword models.",
            "gap_location": "model training objective and evaluation alignment",
            "detection_method": "empirical evaluation comparing MRR on corpus-proxy task (Table 3) to NDCG on the human query challenge (Table 4) and observing rank-order changes among models.",
            "measurement_method": "Comparison of performance metrics across tasks: e.g., SelfAtt average MRR on training proxy = 0.7011, but SelfAtt average NDCG Within on Challenge = 0.493; NBoW MRR 0.6167 vs NDCG Within 0.574. These shifts indicate the training objective does not align perfectly with the challenge objective.",
            "impact_on_results": "Caused model selection paradox where more complex models (higher capacity) that excel on the training proxy do not necessarily yield better search quality for human queries; influenced baseline choice and interpretation of results.",
            "frequency_or_prevalence": "Observed across multiple model variants in the paper; not given as a percent but described as a key finding.",
            "root_cause": "Proxy-training data (documentation) and objective mismatch with real user search behavior and evaluation metric; lexical vs semantic differences.",
            "mitigation_approach": "Authors suggest using more realistic training data (actual queries), improving models to handle rare terms and semantics, and exploring pretraining approaches; they also provide a human-annotated challenge to better align evaluation.",
            "mitigation_effectiveness": "Not quantitatively evaluated in this paper; the challenge itself exposes the mismatch but does not resolve it.",
            "domain_or_field": "machine learning / representation learning for IR",
            "reproducibility_impact": true,
            "uuid": "e719.7",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Annotation Selection Bias",
            "name_full": "Candidate pre-filtering bias in annotation (ensemble of baselines used to pick candidates)",
            "brief_description": "To limit annotation cost, candidate functions shown to annotators were generated by an ensemble of baseline models and ElasticSearch, which can bias the annotated dataset toward what those baselines retrieve.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Annotation pipeline for the CodeSearchNet Challenge",
            "system_description": "Annotators judged relevance for top-10 candidate results per query/language returned by an ensemble of baseline models and ElasticSearch.",
            "nl_description_type": "human query set (collected from Bing and StaQC)",
            "code_implementation_type": "candidate selection code combining model and ElasticSearch outputs",
            "gap_type": "selection bias in annotation / incomplete specification of candidate space",
            "gap_description": "Using only ensemble-generated candidate lists constrains annotation to that pool, potentially excluding relevant functions that baselines would not retrieve and biasing evaluation in favor of the candidate-generating systems.",
            "gap_location": "annotation candidate generation / evaluation dataset construction",
            "detection_method": "Method described in Expert Annotations; authors acknowledge using ensembles to generate candidates and note the limitation implicitly.",
            "measurement_method": "Not quantitatively measured (no estimate of how many true positives were omitted); process generated 10 candidates per query per language.",
            "impact_on_results": "May limit evaluation coverage and bias leaderboard comparisons toward systems similar to the candidate generators; reduces ability to detect systems that retrieve diverse true positives outside the candidate pool.",
            "frequency_or_prevalence": "Applies to all annotated query/language pairs in the challenge (design choice).",
            "root_cause": "Practical constraint of annotation cost and scalability leading to reliance on baseline outputs to reduce annotation search space.",
            "mitigation_approach": "Authors prioritize widening the annotation set in future iterations and suggest expanding annotator pool and candidate generation approaches.",
            "mitigation_effectiveness": "Not evaluated; proposed as future work.",
            "domain_or_field": "dataset construction / evaluation methodology",
            "reproducibility_impact": true,
            "uuid": "e719.8",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "Duplicate Removal Effects",
            "name_full": "Effects of duplicate / near-duplicate removal on dataset representativeness",
            "brief_description": "Removing near-duplicate functions (to avoid auto-generated code and copy/paste) can eliminate legitimate variants and reduce diversity or remove repeated implementations that reflect real code reuse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CodeSearchNet preprocessing and deduplication pipeline",
            "system_description": "Dataset deduplication using techniques from prior work (Allamanis, Lopes et al.) to remove multiple versions of auto-generated or copied code.",
            "nl_description_type": "documentation pairing description (dataset documentation)",
            "code_implementation_type": "corpus preprocessing code",
            "gap_type": "data curation / potential over-filtering",
            "gap_description": "Deduplication heuristics remove near-duplicate functions to avoid training artifacts, but this may also remove meaningful variants and reduce the natural distribution of code, which can affect model learning and evaluation.",
            "gap_location": "data preprocessing / corpus construction",
            "detection_method": "Described in Filtering section and linked prior work; rationale and potential trade-offs discussed.",
            "measurement_method": "No quantitative measure of impact in this paper; deduplication changed dataset counts but effect on model performance not reported.",
            "impact_on_results": "Could reduce model overfitting to duplicated examples but also remove legitimate examples and alter generalization; specific effect not measured here.",
            "frequency_or_prevalence": "Deduplication applied globally to corpus as part of preprocessing.",
            "root_cause": "Need to avoid training on duplicated data balanced against preserving real-world diversity; heuristic deduplication choices.",
            "mitigation_approach": "Use established deduplication methods and make preprocessing code public so others can experiment with thresholds or disable deduplication.",
            "mitigation_effectiveness": "Not evaluated in the paper.",
            "domain_or_field": "dataset curation for machine learning on code",
            "reproducibility_impact": true,
            "uuid": "e719.9",
            "source_info": {
                "paper_title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Adverse Effects of Code Duplication in Machine Learning Models of Code",
            "rating": 2
        },
        {
            "paper_title": "When Deep Learning Met Code Search",
            "rating": 2
        },
        {
            "paper_title": "Deep code search",
            "rating": 2
        },
        {
            "paper_title": "Structured Neural Summarization",
            "rating": 1
        },
        {
            "paper_title": "StaQC: A Systematically Mixed Question-Code Dataset from Stack Overflow",
            "rating": 1
        }
    ],
    "cost": 0.016044,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CodeSearchNet Challenge Evaluating the State of Semantic Code Search</h1>
<p>Hamel Husain<br>Ho-Hsiang Wu<br>Tiferet Gazit<br>{hamelsmu, hohsiangwu,tiferet}@github.com<br>GitHub</p>
<h2>ABSTRACT</h2>
<p>Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.</p>
<p>To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4 k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.</p>
<p>We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.</p>
<h2>1 INTRODUCTION</h2>
<p>The deep learning revolution has fundamentally changed how we approach perceptive tasks such as image and speech recognition and has shown substantial successes in working with natural language data. These have been driven by the co-evolution of large (labelled) datasets, substantial computational capacity, and a number of advances in machine learning models.</p>
<p>However, deep learning models still struggle on highly structured data. One example is semantic code search: while search on natural language documents and even images has made great progress, searching code is often still unsatisfying. Standard information retrieval methods do not work well in the code search domain, as there is often little shared vocabulary between search terms and results (e.g. consider a method called deserialize_JSON_obj_from_stream that may be a correct result for the query "read JSON data"). Even more problematic is that evaluating methods for this task is extremely hard, as there are no substantial datasets that were created for this task; instead, the community tries to make do with small datasets from related contexts (e.g. pairing questions on web forums to code chunks found in answers).</p>
<h2>Miltiadis Allamanis <br> Marc Brockschmidt <br> {miallama, mabrocks}@microsoft.com <br> Microsoft Research</h2>
<p>To tackle this problem, we have defined the CodeSearchNet Challenge on top of a new CodeSearchNet Corpus. The CodeSearchNet Corpus was programmatically obtained by scraping open-source repositories and pairing individual functions with their (processed) documentation as natural language annotation. It is large enough ( 2 million datapoints) to enable training of highcapacity deep neural models on the task. We discuss this process in detail in section 2 and also release the data preprocessing pipeline to encourage further research in this area.</p>
<p>The CodeSearchNet Challenge is defined on top of this, providing realistic queries and expert annotations for likely results. Concretely, in version 1.0, it consists of 99 natural languages queries paired with likely results for each of six considered programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). Each query/result pair was labeled by a human expert, indicating the relevance of the result for the query. We discuss the methodology in detail in section 3.</p>
<p>Finally, we create a number of baseline methods using a range of state-of-the-art neural sequence processing techniques (bag of words, RNNs, CNNs, attentional models) and evaluate them on our datasets. We discuss these models in section 4 and present some preliminary results.</p>
<h2>2 THE CODE SEARCH CORPUS</h2>
<p>As it is economically infeasible to create a dataset large enough for training high-capacity models using expert annotations, we instead create a proxy dataset of lower quality. For this, we follow other attempts in the literature [ $5,6,9,11]$ and pair functions in open-source software with the natural language present in their respective documentation. However, to do so requires a number of preprocessing steps and heuristics. In the following, we discuss some general principles and decisions driven by in-depth analysis of common error cases.</p>
<p>CodeSearchNet Corpus Collection. We collect the corpus from publicly available open-source non-fork GitHub repositories, using libraries.io to identify all projects which are used by at least one other project, and sort them by "popularity" as indicated by the number of stars and forks. Then, we remove any projects that do not have a license or whose license does not explicitly permit the re-distribution of parts of the project. We then tokenize all Go, Java, JavaScript, Python, PHP and Ruby functions (or methods) using TreeSitter - GitHub's universal parser - and, where available, their respective documentation text using a heuristic regular expression.</p>
<p>Filtering. To generate training data for the CodeSearchNet Challenge, we first consider only those functions in the corpus</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Number of Functions</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">w/ documentation</td>
<td style="text-align: right;">All</td>
</tr>
<tr>
<td style="text-align: left;">Go</td>
<td style="text-align: center;">347789</td>
<td style="text-align: right;">726768</td>
</tr>
<tr>
<td style="text-align: left;">Java</td>
<td style="text-align: center;">542991</td>
<td style="text-align: right;">1569889</td>
</tr>
<tr>
<td style="text-align: left;">JavaScript</td>
<td style="text-align: center;">157988</td>
<td style="text-align: right;">1857835</td>
</tr>
<tr>
<td style="text-align: left;">PHP</td>
<td style="text-align: center;">717313</td>
<td style="text-align: right;">977821</td>
</tr>
<tr>
<td style="text-align: left;">Python</td>
<td style="text-align: center;">503502</td>
<td style="text-align: right;">1156085</td>
</tr>
<tr>
<td style="text-align: left;">Ruby</td>
<td style="text-align: center;">57393</td>
<td style="text-align: right;">164048</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">2326976</td>
<td style="text-align: right;">6452446</td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset Size Statistics
that have documentation associated with them. This yields a set of pairs $\left(\mathbf{c}<em i="i">{i}, \mathbf{d}</em>}\right)$ where $\mathbf{c<em i="i">{i}$ is some function documented by $\mathbf{d}</em>$. To make the data more realistic proxy for code search tasks, we then implement a number of preprocessing steps:</p>
<ul>
<li>Documentation $\mathbf{d}_{i}$ is truncated to the first full paragraph, to make the length more comparable to search queries and remove in-depth discussion of function arguments and return values.</li>
<li>Pairs in which $\mathbf{d}_{i}$ is shorter than three tokens are removed, since we do not expect such comments to be informative.</li>
<li>Functions $\mathbf{c}_{i}$ whose implementation is shorter than three lines are removed, these often include unimplemented methods, getters, setters, etc.</li>
<li>Functions whose name contains the substring "test" are removed. Similarly, we remove constructors and standard extension methods such as <strong>str</strong> in Python or toString in Java.</li>
<li>We remove duplicates from the dataset by identifying (near) duplicate functions and only keeping one copy of them (we use the methods described in Allamanis [1], Lopes et al. [18]). This removes multiple versions of auto-generated code and cases of copy \&amp; pasting.
The filtered corpus and the data extraction code are released at https://github.com/github/CodeSearchNet.</li>
</ul>
<p>Dataset Statistics. The resulting dataset contains about 2 million pairs of function-documentation pairs and about another 4 million functions without an associated documentation (Table 1). We split the dataset in 80-10-10 train/valid/test proportions. We suggest that users of the dataset employ the same split.</p>
<p>Limitations. Unsurpsingly, the scraped dataset is quite noisy. First, documentation is fundamentally different from queries, and hence uses other forms of language. It is often written at the same time and by the same author as the documented code, and hence tends to use the same vocabulary, unlike search queries. Second, despite our data cleaning efforts we are unable to know the extent to which each documentation $\mathbf{d}<em i="i">{i}$ accurately describes its associated code snippet $\mathbf{c}</em>$. For example, a number of comments are outdated with regard to the code that they describe. Finally, we know that some documentation is written in other languages, whereas our CodeSearchNet Challenge evaluation dataset focuses on English queries.</p>
<h2>3 THE CODE SEARCH CHALLENGE</h2>
<p>To evaluate on the CodeSearchNet Challenge, a method has to return a set of relevant results from CodeSearchNet Corpus for each of 99 pre-defined natural language queries. Note that the task is somewhat simplified from a general code search task by only allowing full functions/methods as results, and not arbitrary chunks of code. ${ }^{1}$ The CodeSearchNet Challenge evaluation dataset consists of the 99 queries with relevance annotations for a small number of functions from our corpus likely to be returned. These annotations were collected from a small set of expert programmers, but we are looking forward to widening the annotation set going forward.</p>
<p>Query Collection. To ensure that our query set is representative, we obtained common search queries from Bing that had high clickthrough rates to code and combined these with intent rewrites in StaQC [24]. We then manually filtered out queries that were clearly technical keywords (e.g. the exact name of a function such as tf.gather_nd) to obtain a set of 99 natural language queries. While most of the collected queries are generic, some of them are language-specific.</p>
<p>Expert Annotations. Obviously, we cannot annotate all query/function pairs. To filter this down to a more realistically-sized set, we used our implementations of baseline methods and ensembled them (see section 4) to generate 10 candidate results per query and programming language. Concretely, we used ensembles of all neural models and ElasticSearch to generate candidate results, merge the suggestions and pick the top 10. We used a simple web interface for the annotation process. The web interface firsts shows instructions (see Figure 1) and then allows the annotator to pick a programming language. Then, one query/function pair is shown at a time, as shown in Figure 2. A link to the origin of the shown function is included, as initial experiments showed that some annotators found inspecting the context of the code snippet helpful to judge relevance. The order of query/code pairs shown to the user is randomized but weakly ordered by the number of expert annotations already collected. Annotators are unlikely to see several results for the same query unless they handle many examples. By randomizing the order, we aim to allow users to score the relevance of each pair individually without encouraging comparisons of different results for the same query.</p>
<p>Annotation Statistics. We collected 4026 annotations across six programming languages and prioritized coverage over multiple annotations per query-snippet pair. Our annotators are volunteers with software engineering, data science and research roles and were asked to only annotate examples for languages they had significant experience with. This led to a skewed distribution of annotations w.r.t. the considered programming languages.</p>
<p>We observed that the obtained relevance scores are distributed differently for each language (Table 2). For example, the relevance scores for Python are evenly distributed across the four categories while for JavaScript the annotations are skewed towards lower relevance scores. There is a number of potential reasons for this, such as the quality of the used corpus, language-specific interactions</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Through the annotations, we want to measure how relevant would these results be to you.</p>
<ul>
<li>You don't have to be absolutely certain about the correctness of the code.</li>
<li>You might be interested in copy-pasting the code, finding a library to use or just getting some understanding about how something is implemented.</li>
<li>You might be searching within your project (e.g. to reuse code within your project), your company or all of GitHub.</li>
<li>We ask annotators to have at least some experience on the programming language they are annotating.</li>
</ul>
<p>Please annotate the results according to the following scheme:</p>
<ul>
<li>3: Exact match. This seems exactly what I was looking for. I would copy-paste the code and make minor adaptations or will use this functionality of the library in my code.</li>
<li>2: Strong match. This does more or less what I was looking for. I would use the code in here as a backbone for my purpose, but I won't necessarily copy-paste it or use this library.</li>
<li>1: Weak match: That's not exactly what I was looking for, but there are some useful elements/pointers to things that I would use (e.g. APIs, code structure) and can form the basis of a new query or exploration towards solving my query.</li>
<li>0: Totally irrelevant. I would never want to see this for this query.</li>
</ul>
<p>Figure 1: Instructions provided to annotators.</p>
<p>Please annotate the following query:
output to html file
\Code
@override
public boolean fileDeleted(file file) throws watchingException (
file output = getOutputFile(file, "html");
fileUtils.deletedpiletly(nutput);
return file(reasted(file);
}
[^ Link to GitHub</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Notes (Optional)</th>
<th style="text-align: center;">Type any notes (optional) before you decide on the relevance score.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0 - <br> Irrelevant</td>
<td style="text-align: center;">1 - Weak <br> Match</td>
<td style="text-align: center;">2 - Strong <br> Match</td>
<td style="text-align: center;">3 - Exact <br> Match</td>
<td style="text-align: center;">$\Delta^{2}$</td>
<td style="text-align: center;">$\rightarrow$ <br> step</td>
</tr>
</tbody>
</table>
<p>Figure 2: Interface used for relevance annotation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Count by Relevance Score</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Annotations</td>
</tr>
<tr>
<td style="text-align: center;">Go</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">166</td>
</tr>
<tr>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">383</td>
<td style="text-align: center;">178</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">137</td>
<td style="text-align: center;">823</td>
</tr>
<tr>
<td style="text-align: center;">JavaScript</td>
<td style="text-align: center;">153</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">319</td>
</tr>
<tr>
<td style="text-align: center;">PHP</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">314</td>
</tr>
<tr>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">498</td>
<td style="text-align: center;">511</td>
<td style="text-align: center;">537</td>
<td style="text-align: center;">543</td>
<td style="text-align: center;">2089</td>
</tr>
<tr>
<td style="text-align: center;">Ruby</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">315</td>
</tr>
</tbody>
</table>
<p>Table 2: Annotation Dataset Statistics
with our pre-filtering strategy, the queries we collected, higher expected relevance standards in the JavaScript community, etc.</p>
<p>For the 891 query-code pairs where we have more than one annotation, we compute the squared Cohen's kappa interannotator
agreement to estimate the quality of the task. The agreement is moderate with Cohen $\kappa=0.47$. This is somewhat expected given that this task was relatively open-ended, as we will discuss next.</p>
<p>Qualitative Observations. During the annotation process we made some observations in discussions with the annotators and through the notes they provided in the web interface (see Fig. 2). These comments point to some general issues in implementing code search:</p>
<p>Code Quality A subset of the results being returned are functionally correct code, but of low quality, even though they originated in reasonably popular projects. In this context, low quality refers to unsatisfactory readability, bad security practices, known antipatterns and potentially slow code. Some annotators felt the need to give lower relevance scores to low-quality code as they would prefer not to see such results.
Query Ambiguity Queries are often ambiguous without additional context. For example, the query "how to determine if a string is a valid word" can have different correct interpretations depending on the domain-specific meaning of "valid word".
Library vs. Project Specific Often a search yields code that is very specific to a given project (e.g. using internal utility functions), whereas other times the code is very general and verbose (e.g. containing code that could be factored out). Which of these is preferable depends on the context of the query, which we did not explicitly specify when asking for annotations.
Context Some results were semantically correct, but not relying on related helper functions and thus not self-contained. Some annotators were uncertain if such results should be considered relevant.
Directionality A common problem in results were functions implementing the inverse functionality of the query, e.g. "convert int to string" would be answered by stringToInt. This suggests that the baseline models used for pre-filtering have trouble with understanding such semantic aspects.</p>
<h3>3.1 Evaluation of Ranking Models</h3>
<p>To track the progress on the CodeSearchNet Challenge we have deployed a Weights \&amp; Biases leaderboard at https://app.wandb.ai/ github/codesearchnet/benchmark. We hope that this leaderboard will allow the community to better compare solutions to the code search task.</p>
<p>Metrics. We used normalized discounted cumulative gain (NDCG) to evaluate each competing method. NDCG is a commonly used metric [19] in information retrieval. We compute two variants of NDCG: (a) NDCG computed over the subset of functions with human annotations ("Within") (b) NDCG over the whole CodeSearchNet Corpus ("All"). We make this distinction as the NDCG score computed over the whole corpus may not necessarily represent the quality of a search tool, as a new tool may yield relevant but not-annotated functions.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: Model Architecture Overview.</p>
<h2>4 BASELINE CODESEARCH MODELS</h2>
<p>We implemented a range of baseline models for the code search task, using standard techniques from neural sequence processing and web search.</p>
<h3>4.1 Joint Vector Representations for Code Search</h3>
<p>Following earlier work [11, 20], we use joint embeddings of code and queries to implement a neural search system. Our architecture employs one encoder per input (natural or programming) language and trains them to map inputs into a single, joint vector space. Our training objective is to map code and the corresponding language onto vectors that are near to each other, as we can then implement a search method by embedding the query and then returning the set of code snippets that are "near" in embedding space. Although more complex models considering more interactions between queries and code can perform better [20], generating a single vector per query/snippet allows for efficient indexing and search.</p>
<p>To learn these embedding functions, we combine standard sequence encoder models in the architecture shown in Figure 3. First, we preprocess the input sequences according to their semantics: identifiers appearing in code tokens are split into subtokens (i.e. a variable camelLase yields two subtokens camel and case), and natural language tokens are split using byte-pair encoding (BPE) $[10,21]$.</p>
<p>Then, the token sequences are processed to obtain (contextualized) token embeddings, using one of the following architectures.
Neural Bag of Words where each (sub)token is embedded to a learnable embedding (vector representation).
Bidirectional RNN models where we employ the GRU cell [7] to summarize the input sequence.
1D Convolutional Neural Network over the input sequence of tokens [15].
Self-Attention where multi-head attention [22] is used to compute representations of each token in the sequence.
The token embeddings are then combined into a sequence embedding using a pooling function, for which we have implemented mean/max-pooling and an attention-like weighted sum mechanism.</p>
<p>For all models, we set the dimensionality of the embedding space to 128 .</p>
<p>During training we are given a set of $N$ pairs $\left(\mathbf{c}<em i="i">{i}, \mathbf{d}</em>$. We train by minimizing the loss}\right)$ of code and natural language descriptions and have instantiated a code encoder $E_{c}$ and a query encoder $E_{q</p>
<p>$$
-\frac{1}{N} \sum_{i} \log \left(\frac{\exp \left(E_{c}\left(\mathbf{c}<em q="q">{i}\right)^{\top} E</em>}\left(\mathbf{d<em j="j">{i}\right)\right)}{\sum</em>} \exp \left(E_{c}\left(\mathbf{c<em q="q">{j}\right)^{\top} E</em>\right)
$$}\left(\mathbf{d}_{i}\right)\right)</p>
<p>i.e. maximize the inner product of the code and query encodings of the pair, while minimizing the inner product between each $\mathbf{c}<em j="j">{i}$ and the distractor snippets $\mathbf{c}</em>(i \neq j)$. Note that we have experimented with other similar objectives (e.g. considering cosine similarity and max-margin approaches) without significant changes in results on our validation dataset. The code for the baselines can be found at https://github.com/github/CodeSearchNet.</p>
<p>At test time, we index all functions in CodeSearchNet Corpus using Annoy. Annoy offers fast, approximate nearest neighbor indexing and search. The index includes all functions in the CodeSearchNet Corpus, including those that do not have an associated documentation comment. We observed that carefully constructing this index is crucial to achieving good performance. Specifically, our baseline models were underperforming when we had a small number of trees in Annoy (trees can be thought as approximate indexes of the multidimensional space).</p>
<h3>4.2 ElasticSearch Baseline</h3>
<p>In our experiments, we additionally included ElasticSearch, a widely used search engine with the default parameters. We configured it with an index using two fields for every function in our dataset: the function name, split into subtokens; and the text of the entire function. We use the default ElasticSearch tokenizer.</p>
<h3>4.3 Evaluation</h3>
<p>Following the training/validation/testing data split, we train our baseline models using our objective from above. While it does not directly correspond to the real target task of code search, it has been widely used as a proxy for training similar models [6, 23].</p>
<p>For testing purposes on CodeSearchNet Corpus, we fix a set of 999 distractor snippets $\mathbf{c}<em i="i">{j}$ for each test pair $\left(\mathbf{c}</em>\right)$ and test all trained models. Table 3 presents the Mean Reciprocal Rank results on this task. Overall, we see that the models achieve relatively good performance on this task, with the self-attention-based model performing best. This is not unexpected, as the self-attention model has the highest capacity of all considered models.}, \mathbf{d}_{i</p>
<p>We have also run our baselines on CodeSearchNet Challenge and show the results in Table 4. Here, the neural bag of words model performs very well, whereas the stronger neural models on the training task do less well. We note that the bag of words model is particularly good at keyword matching, which seems to be a crucial facility in implementing search methods. This hypothesis is further validated by the fact that the non-neural ElasticSearchbased baseline performs competitively among all models we have tested. The NBoW model is the best performing model among the baselines models, despite being the simplest. As noted by Cambronero et al. [6], this can be attributed to the fact that the training</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Encoder</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CodeSearchNet Corpus (MRR)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">Go</td>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">JS</td>
<td style="text-align: center;">PHP</td>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">Ruby</td>
<td style="text-align: center;">Avg</td>
</tr>
<tr>
<td style="text-align: center;">NBoW</td>
<td style="text-align: center;">NBoW</td>
<td style="text-align: center;">0.6409</td>
<td style="text-align: center;">0.5140</td>
<td style="text-align: center;">0.4607</td>
<td style="text-align: center;">0.4835</td>
<td style="text-align: center;">0.5809</td>
<td style="text-align: center;">0.4285</td>
<td style="text-align: center;">0.6167</td>
</tr>
<tr>
<td style="text-align: center;">1D-CNN</td>
<td style="text-align: center;">1D-CNN</td>
<td style="text-align: center;">0.6274</td>
<td style="text-align: center;">0.5270</td>
<td style="text-align: center;">0.3523</td>
<td style="text-align: center;">0.5294</td>
<td style="text-align: center;">0.5708</td>
<td style="text-align: center;">0.2450</td>
<td style="text-align: center;">0.6206</td>
</tr>
<tr>
<td style="text-align: center;">biRNN</td>
<td style="text-align: center;">biRNN</td>
<td style="text-align: center;">0.4524</td>
<td style="text-align: center;">0.2865</td>
<td style="text-align: center;">0.1530</td>
<td style="text-align: center;">0.2512</td>
<td style="text-align: center;">0.3213</td>
<td style="text-align: center;">0.0835</td>
<td style="text-align: center;">0.4262</td>
</tr>
<tr>
<td style="text-align: center;">SelfAtt</td>
<td style="text-align: center;">SelfAtt</td>
<td style="text-align: center;">0.6809</td>
<td style="text-align: center;">0.5866</td>
<td style="text-align: center;">0.4506</td>
<td style="text-align: center;">0.6011</td>
<td style="text-align: center;">0.6922</td>
<td style="text-align: center;">0.3651</td>
<td style="text-align: center;">0.7011</td>
</tr>
<tr>
<td style="text-align: center;">SelfAtt</td>
<td style="text-align: center;">NBoW</td>
<td style="text-align: center;">0.6631</td>
<td style="text-align: center;">0.5618</td>
<td style="text-align: center;">0.4920</td>
<td style="text-align: center;">0.5083</td>
<td style="text-align: center;">0.6113</td>
<td style="text-align: center;">0.4574</td>
<td style="text-align: center;">0.6505</td>
</tr>
</tbody>
</table>
<p>Table 3: Mean Reciprocal Rank (MRR) on Test Set of CodeSearchNet Corpus. This evaluates for our training task, where given the documentation comment as a query, the models try to rank the correct code snippet highly among 999 distractor snippets.
data constructed from code documentation is not a good match for the code search task.</p>
<h2>5 RELATED WORK</h2>
<p>Applying machine learning to code has been widely considered [2]. A few academic works have looked into related tasks. First, semantic parsing has received a lot of attention in the NLP community. Although most approaches are usually aimed towards creating an executable representation of a natural language utterance with a domain-specific language, general-purpose languages have been recently considered by Hashimoto et al. [13], Lin et al. [16], Ling et al. [17], Yin and Neubig [25].</p>
<p>Iyer et al. [14] generate code from natural language within the context of existing methods, whereas Allamanis et al. [3], Alon et al. [4] consider the task of summarizing functions to their names. Finally, Fernandes et al. [9] consider the task of predicting the documentation text from source code.</p>
<p>More related to CodeSearchNet is prior work in code search with deep learning. In the last few years there has been research in this area (Cambronero et al. [6], Gu et al. [11, 12], Yao et al. [23]), and architectures similar to those discussed previously have been shown to work to some extent. Recently, Cambronero et al. [6] looked into the same problem that CodeSearchNet is concerned with and reached conclusions similar to those discussed here. In contrast to the aforementioned works, here we provide a humanannotated dataset of relevance scores and test a few more neural search architectures along with a standard information retrieval baseline.</p>
<h2>6 CONCLUSIONS \&amp; OPEN CHALLENGES</h2>
<p>We hope that CodeSearchNet is a good step towards engaging with the machine learning, IR and NLP communities towards developing new machine learning models that understand source code and natural language. Despite the fact this report gives emphasis on semantic code search we look forward to other uses of the presented datasets. There are still plenty of open challenges in this area.</p>
<ul>
<li>Our ElasticSearch baseline, that performs traditional keywordbased search, performs quite well. It has the advantage of being able to efficiently use rare terms, which often appear in code. Researching neural methods that can efficiently and accurately represent rare terms will improve performance.</li>
<li>Code semantics such as control and data flow are not exploited explicitly by existing methods, and instead search methods seem to be mainly operate on identifiers (such as variable and function) names. How to leverage semantics to improve results remains an open problem.</li>
<li>Recently, in NLP, pretraining methods such as BERT [8] have found great success. Can similar methods be useful for the encoders considered in this work?</li>
<li>Our data covers a wide range of general-purpose code queries. However, anecdotal evidence indicates that queries in specific projects are usually more specialized. Adapting search methods to such use cases could yield substantial performance improvements.</li>
<li>Code quality of the searched snippets was a recurrent issue with our expert annotators. Despite its subjective nature, there seems to be agreement on what constitutes very bad code. Using code quality as an additional signal that allows for filtering of bad results (at least when better results are available) could substantially improve satisfaction of search users.</li>
</ul>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank Rok Novosel for participating in the CodeSearchNet challenge and pinpointing a bug with the indexing that was significantly impacting our evaluation results.</p>
<h2>REFERENCES</h2>
<p>[1] Miltiadis Allamanis. 2018. The Adverse Effects of Code Duplication in Machine Learning Models of Code. arXiv preprint arXiv:1812.06469 (2018).
[2] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 81.
[3] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A Convolutional Attention Network for Extreme Summarization of Source Code. In Proceedings of the International Conference on Machine Learning (ICML).
[4] Uri Alon, Omer Levy, and Eran Yahav. 2018. code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv:1808.01400 (2018).
[5] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of Python functions and documentation strings for automated code documentation and code generation. arXiv preprint arXiv:1707.02275 (2017).
[6] Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra. 2019. When Deep Learning Met Code Search. arXiv preprint arXiv:1905.03813 (2019).
[7] Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the Properties of Neural Machine Translation: Encoder-Decoder Approaches. Syntax, Semantics and Structure in Statistical Translation (2014).
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Encoder</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CodeSearchNet Challenge- NDCG Within</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CodeSearchNet Challenge- NDCG All</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">Go</td>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">JS</td>
<td style="text-align: center;">PHP</td>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">Ruby</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">Go</td>
<td style="text-align: center;">Java</td>
<td style="text-align: center;">JS</td>
<td style="text-align: center;">PHP</td>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">Ruby</td>
</tr>
<tr>
<td style="text-align: center;">ElasticSearch</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;">NBoW</td>
<td style="text-align: center;">NBoW</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.360</td>
</tr>
<tr>
<td style="text-align: center;">1D-CNN</td>
<td style="text-align: center;">1D-CNN</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.162</td>
</tr>
<tr>
<td style="text-align: center;">bilfNN</td>
<td style="text-align: center;">bilfNN</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.030</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">0.017</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.060</td>
</tr>
<tr>
<td style="text-align: center;">SelfAtt</td>
<td style="text-align: center;">SelfAtt</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.219</td>
</tr>
<tr>
<td style="text-align: center;">SelfAtt</td>
<td style="text-align: center;">NBoW</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.566</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.360</td>
</tr>
</tbody>
</table>
<p>Table 4: NDCG Baseline Results on CodeSearchNet Challenge. "Within" computes the NDCG only on the functions within the human-annotated examples. "All" computes NDCG over all functions in the CodeSearchNet Corpus.
preprint arXiv:1810.04805 (2018).
[9] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2018. Structured Neural Summarization. arXiv preprint arXiv:1811.01824 (2018).
[10] Philip Gage. 1994. A new algorithm for data compression. The C Users' Journal 12, 2 (1994), 23-38.
[11] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, $933-944$.
[12] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep API Learning. In Proceedings of the International Symposium on Foundations of Software Engineering (FSE).
[13] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. 2018. A retrieve-and-edit framework for predicting structured outputs. In Advances in Neural Information Processing Systems. 10073-10083.
[14] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018).
[15] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 (2014).
[16] Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D. Ernst. 2018. NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System. In International Conference on Language Resources and Evaluation.
[17] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent Predictor Networks for</p>
<p>Code Generation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).
[18] Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny, Hitesh Sajnani, and Jan Vitek. 2017. DéjàVu: a map of code duplicates on GitHub. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 84.
[19] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press.
[20] Bhaskar Mitra, Nick Craswell, et al. 2018. An introduction to neural information retrieval. Foundations and Trends® in Information Retrieval 13, 1 (2018), 1-126.
[21] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).
[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998-6008.
[23] Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019. CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning. (2019).
[24] Ziyu Yao, Daniel S Weld, Wei-Peng Chen, and Huan Sun. 2018. StaQC: A Systematically Mixed Question-Code Dataset from Stack Overflow. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 1693-1703.
[25] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for GeneralPurpose Code Generation. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Note that on a sufficiently large dataset, this is not a significant restriction: more commonly implemented functionality almost always appears factored out into a function somewhere.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5539 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5539</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5539</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-267301629</p>
                <p><strong>Paper Title:</strong> Scientific Large Language Models: A Survey on Biological & Chemical Domains</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this article, we endeavor to methodically delineate the concept of “scientific language,” whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5539.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5539.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenSLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenSLMs: Genome-scale language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Genome-scale decoder-based language models applied to genomic sequence modeling and evolutionary dynamics, e.g., SARS-CoV-2; pretrained on large prokaryotic and genomic corpora and fine-tuned for viral variant analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GenSLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only genome-scale language models that use attention-based decoders trained on prokaryotic and other genome sequences to capture large-scale evolutionary patterns; pretrained then fine-tuned to target genomes (e.g., SARS-CoV-2).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>25M-25B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Genomics / viral evolution (SARS-CoV-2 evolutionary dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Modeling/evolving genomic sequences to elucidate evolutionary dynamics and identify concerning variants (text-based genome-scale sequence modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Survey states the model 'demonstrates precise and rapid identification of concerning variants' but provides no numerical accuracy values in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Pretraining data domain coverage (prokaryotic vs target viral genomes), scale of model, fine-tuning on target genomes, sequence length handling / architectural choices for long sequences, and prompt/conditioning design for the downstream variant-identification task.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Survey reports GenSLMs were pretrained on prokaryotic sequences and fine-tuned for SARS-CoV-2, and attributes its effectiveness to appropriate pretraining/fine-tuning; no detailed ablation or numeric evidence provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in detail in the survey; described qualitatively ('precise and rapid identification'); likely evaluated by comparing predictions/variant identifications to known variant labels but the survey does not give exact evaluation protocols or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No specific failure cases or quantitative limitations reported in the survey for GenSLMs; general caveats apply (need for adequate domain data, long-sequence modelling issues, possible hallucinations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Survey does not present quantitative comparisons between GenSLMs and other methods within this paper's text; only qualitative claims of capability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Ensure pretraining corpus includes relevant genomic diversity; fine-tune on target organism genomes; address long-sequence modelling; combine with domain-specific evaluation benchmarks for variant detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific Large Language Models: A Survey on Biological & Chemical Domains', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5539.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5539.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tailored language-model approach that predicts protein-protein interactions by simulating signaling pathways via natural-language prompts (i.e., using text-based simulation of pathways to infer interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A protein-focused language model that employs instruction fine-tuning, embedding substitution and chain-of-thought style (textual) simulation of signaling pathways to make PPI predictions; described as based on adapting LLM architectures for protein-language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Molecular & cellular biology — protein-protein interactions / signaling pathway simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of signaling pathways (via natural-language prompts) to predict protein-protein interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>No numerical accuracy values provided in the survey; the model is described as achieving high performance in the context of its design but no numbers are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality and coverage of protein-text aligned data, quality of instruction fine-tuning, prompt engineering (how signaling pathways are described), underlying protein-language pretraining, and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Survey notes ProLLM uses embedding substitution and instruction fine-tuning to enhance protein understanding and PPI prediction; however the survey does not include ablation studies or quantitative evidence for the relative impact of these factors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in detail in the survey; likely evaluated on PPI prediction benchmarks or downstream tasks but exact metrics/protocols are not provided in this survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not list explicit failure cases for ProLLM; general limitations include potential hallucination, dependence on quality of textual pathway descriptions, and absence of 3D structural integration which can limit biological fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>No direct quantitative comparisons presented in the survey between ProLLM and other PPI predictors; only qualitative placement among multimodal protein-text models.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use high-quality paired protein-text datasets, apply careful instruction fine-tuning, leverage prompt engineering for pathway descriptions, and consider integrating structural/graph representations or external knowledge graphs to mitigate hallucination and improve fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific Large Language Models: A Survey on Biological & Chemical Domains', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5539.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5539.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evo (StripedHyena architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A StripedHyena-architecture multimodal model for sequence modeling and design spanning molecular to genome scales, enabling zero-shot function prediction and DNA sequence modeling across scales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sequence modeling and design from molecular to genome scale with Evo</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A StripedHyena-architecture language model (7B parameters in survey) trained on OpenGenome-type datasets to perform sequence modeling from molecular up to genome scale and to enable zero-shot functional predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Genomics / sequence modeling and design across molecular-to-genome scales; function prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based sequence modeling: generation and design of DNA sequences, and zero-shot prediction of functional properties across scales (molecular → genome).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>The survey indicates Evo achieves zero-shot function prediction and sequence modeling capabilities but does not give numeric accuracy measures within the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Scale and diversity of training genomes (OpenGenome), model size, architecture choices for long-range dependencies, tokenization choices (single-base precision vs. k-mers), and fine-tuning/instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Survey cites Evo's training on OpenGenome and its StripedHyena architecture enabling zero-shot tasks; no detailed ablation studies or numeric evidence included in the survey itself.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in the survey; described qualitatively as zero-shot function prediction and modeling—would typically be evaluated against curated benchmarks for function and sequence modeling, but that detail is absent here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not enumerate explicit failures for Evo; general issues include long-sequence modeling challenges, dependency on diverse training genomes, and potential for spurious predictions absent structural/experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>No quantitative head-to-head comparisons presented within the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Train on diverse, large-scale genomic corpora, use architectures that handle very long contexts, adopt single-base tokenization for nucleotide-level precision when required, and validate predictions with orthogonal experimental or computational methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific Large Language Models: A Survey on Biological & Chemical Domains', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5539.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5539.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow / ChatGPT / GPT-4 (chemistry tooling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow (tool-augmented ChatGPT / GPT-4 for chemistry tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments general LLMs (ChatGPT/GPT-4) with domain-specific tools (ChemCrow integrates 13 expert-designed chemistry tools) to perform chemistry tasks such as organic synthesis planning, drug discovery and materials design via text-based interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chem-Crow: Augmenting large-language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-4 (augmented via ChemCrow)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose decoder-only LLMs (ChatGPT / GPT-4) adapted for chemistry by attaching external, expert-designed tools (retrieval, cheminformatics utilities, reaction templates, calculators) to extend capabilities and reduce hallucination in chemistry workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Chemistry — organic synthesis planning, retrosynthesis, drug discovery, materials design (text-guided workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation/planning of organic synthesis steps, retrosynthetic routes, and drug/materials design workflows via chained tool calls and textual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>No quantitative accuracy metrics are provided in the survey for ChemCrow-augmented ChatGPT/GPT-4; the survey reports improved performance qualitatively when expert tools are integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Presence and quality of integrated domain tools, prompt engineering, availability and quality of domain-specific retrieval/fact-bases, grounding against computational chemistry tools (e.g., calculators, reaction filters), and LLM base-model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Survey cites ChemCrow's use of 13 expert-designed tools to augment ChatGPT and states that this improved performance on chemistry tasks; no numerical ablation data are provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not described in detail in the survey; likely involves task-specific evaluations (e.g., retrosynthesis correctness, expert assessment) but the paper does not list exact metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>General LLMs without tool augmentation often struggle with domain-specific scientific languages (molecules, sequences) and can hallucinate; even with tools, rigorous chemical validity and experimental feasibility may remain unverified unless integrated with physics-based checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Survey notes that augmenting LLMs with tools (ChemCrow) improves chemistry-task performance versus using base ChatGPT/GPT-4 alone, but provides no quantitative benchmarks within the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Augment general LLMs with verified domain tools, use retrieval and grounding databases, implement chemistry-specific filters and validators (e.g., reaction rules, molecule validity checks), and perform human/expert review or external computational validation (e.g., MD, quantum calculations) for critical predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific Large Language Models: A Survey on Biological & Chemical Domains', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5539.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5539.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolecularGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolecularGPT: Open large language model for few-shot molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-based LLM fine-tuned for few-shot molecular property prediction, demonstrating LLMs' utility as few-shot learners in molecular property tasks; trained/fine-tuned on molecular datasets and instruction sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolecularGPT: Open large language model (LLM) for few-shot molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolecularGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 7B-parameter LLM (LLaMA2-chat-7B backbone per the survey) fine-tuned on molecular corpora (e.g., ChEMBL, QM9) and hybrid instruction sets to perform few-shot property prediction and text-guided molecular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational chemistry / molecular property prediction (few-shot), molecular generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based few-shot prediction of molecular properties and text-conditioned molecule generation; using natural-language prompts to produce property predictions or candidate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Survey describes MolecularGPT as demonstrating few-shot molecular property prediction capability; no numerical accuracy values or benchmark scores are provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Pretraining and fine-tuning dataset coverage (ChEMBL, QM9, etc.), model size, instruction tuning quality, prompt design, and whether structural / 3D information is incorporated.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Survey indicates MolecularGPT was trained on ChEMBL and QM9 and used instruction sets to achieve few-shot learning; the survey does not include ablation studies or numeric evidence for the impact of each factor.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in the survey; typical evaluation would be property prediction metrics on held-out datasets or few-shot benchmarks, but this survey entry does not provide concrete metrics or protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No specific failure cases enumerated in the survey for MolecularGPT; general limitations include lack of explicit 3D/physics grounding and potential hallucination for out-of-distribution chemistries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Survey does not provide direct quantitative comparisons to other molecular property predictors in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Combine language-model approaches with structural/physics-based validation, expand training/fine-tuning data diversity, use careful instruction tuning and prompt engineering, and evaluate on standardized molecular benchmarks (e.g., MoleculeNet, GuacaMol).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific Large Language Models: A Survey on Biological & Chemical Domains', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics <em>(Rating: 2)</em></li>
                <li>ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction <em>(Rating: 2)</em></li>
                <li>Sequence modeling and design from molecular to genome scale with Evo <em>(Rating: 2)</em></li>
                <li>Chem-Crow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>MolecularGPT: Open large language model (LLM) for few-shot molecular property prediction <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5539",
    "paper_id": "paper-267301629",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GenSLMs",
            "name_full": "GenSLMs: Genome-scale language models",
            "brief_description": "Genome-scale decoder-based language models applied to genomic sequence modeling and evolutionary dynamics, e.g., SARS-CoV-2; pretrained on large prokaryotic and genomic corpora and fine-tuned for viral variant analysis.",
            "citation_title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
            "mention_or_use": "mention",
            "model_name": "GenSLMs",
            "model_description": "Decoder-only genome-scale language models that use attention-based decoders trained on prokaryotic and other genome sequences to capture large-scale evolutionary patterns; pretrained then fine-tuned to target genomes (e.g., SARS-CoV-2).",
            "model_size": "25M-25B",
            "scientific_subdomain": "Genomics / viral evolution (SARS-CoV-2 evolutionary dynamics)",
            "simulation_task": "Modeling/evolving genomic sequences to elucidate evolutionary dynamics and identify concerning variants (text-based genome-scale sequence modeling).",
            "accuracy_metric": null,
            "reported_accuracy": "Survey states the model 'demonstrates precise and rapid identification of concerning variants' but provides no numerical accuracy values in this paper.",
            "factors_affecting_accuracy": "Pretraining data domain coverage (prokaryotic vs target viral genomes), scale of model, fine-tuning on target genomes, sequence length handling / architectural choices for long sequences, and prompt/conditioning design for the downstream variant-identification task.",
            "evidence_for_factors": "Survey reports GenSLMs were pretrained on prokaryotic sequences and fine-tuned for SARS-CoV-2, and attributes its effectiveness to appropriate pretraining/fine-tuning; no detailed ablation or numeric evidence provided in the survey.",
            "evaluation_method": "Not specified in detail in the survey; described qualitatively ('precise and rapid identification'); likely evaluated by comparing predictions/variant identifications to known variant labels but the survey does not give exact evaluation protocols or metrics.",
            "limitations_or_failure_cases": "No specific failure cases or quantitative limitations reported in the survey for GenSLMs; general caveats apply (need for adequate domain data, long-sequence modelling issues, possible hallucinations).",
            "comparisons": "Survey does not present quantitative comparisons between GenSLMs and other methods within this paper's text; only qualitative claims of capability.",
            "recommendations_or_best_practices": "Ensure pretraining corpus includes relevant genomic diversity; fine-tune on target organism genomes; address long-sequence modelling; combine with domain-specific evaluation benchmarks for variant detection.",
            "uuid": "e5539.0",
            "source_info": {
                "paper_title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ProLLM",
            "name_full": "ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction",
            "brief_description": "A tailored language-model approach that predicts protein-protein interactions by simulating signaling pathways via natural-language prompts (i.e., using text-based simulation of pathways to infer interactions).",
            "citation_title": "ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction",
            "mention_or_use": "mention",
            "model_name": "ProLLM",
            "model_description": "A protein-focused language model that employs instruction fine-tuning, embedding substitution and chain-of-thought style (textual) simulation of signaling pathways to make PPI predictions; described as based on adapting LLM architectures for protein-language tasks.",
            "model_size": null,
            "scientific_subdomain": "Molecular & cellular biology — protein-protein interactions / signaling pathway simulation",
            "simulation_task": "Text-based simulation of signaling pathways (via natural-language prompts) to predict protein-protein interactions.",
            "accuracy_metric": null,
            "reported_accuracy": "No numerical accuracy values provided in the survey; the model is described as achieving high performance in the context of its design but no numbers are reported here.",
            "factors_affecting_accuracy": "Quality and coverage of protein-text aligned data, quality of instruction fine-tuning, prompt engineering (how signaling pathways are described), underlying protein-language pretraining, and model capacity.",
            "evidence_for_factors": "Survey notes ProLLM uses embedding substitution and instruction fine-tuning to enhance protein understanding and PPI prediction; however the survey does not include ablation studies or quantitative evidence for the relative impact of these factors.",
            "evaluation_method": "Not specified in detail in the survey; likely evaluated on PPI prediction benchmarks or downstream tasks but exact metrics/protocols are not provided in this survey text.",
            "limitations_or_failure_cases": "Survey does not list explicit failure cases for ProLLM; general limitations include potential hallucination, dependence on quality of textual pathway descriptions, and absence of 3D structural integration which can limit biological fidelity.",
            "comparisons": "No direct quantitative comparisons presented in the survey between ProLLM and other PPI predictors; only qualitative placement among multimodal protein-text models.",
            "recommendations_or_best_practices": "Use high-quality paired protein-text datasets, apply careful instruction fine-tuning, leverage prompt engineering for pathway descriptions, and consider integrating structural/graph representations or external knowledge graphs to mitigate hallucination and improve fidelity.",
            "uuid": "e5539.1",
            "source_info": {
                "paper_title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Evo",
            "name_full": "Evo (StripedHyena architecture)",
            "brief_description": "A StripedHyena-architecture multimodal model for sequence modeling and design spanning molecular to genome scales, enabling zero-shot function prediction and DNA sequence modeling across scales.",
            "citation_title": "Sequence modeling and design from molecular to genome scale with Evo",
            "mention_or_use": "mention",
            "model_name": "Evo",
            "model_description": "A StripedHyena-architecture language model (7B parameters in survey) trained on OpenGenome-type datasets to perform sequence modeling from molecular up to genome scale and to enable zero-shot functional predictions.",
            "model_size": "7B",
            "scientific_subdomain": "Genomics / sequence modeling and design across molecular-to-genome scales; function prediction",
            "simulation_task": "Text-based sequence modeling: generation and design of DNA sequences, and zero-shot prediction of functional properties across scales (molecular → genome).",
            "accuracy_metric": null,
            "reported_accuracy": "The survey indicates Evo achieves zero-shot function prediction and sequence modeling capabilities but does not give numeric accuracy measures within the survey text.",
            "factors_affecting_accuracy": "Scale and diversity of training genomes (OpenGenome), model size, architecture choices for long-range dependencies, tokenization choices (single-base precision vs. k-mers), and fine-tuning/instruction data.",
            "evidence_for_factors": "Survey cites Evo's training on OpenGenome and its StripedHyena architecture enabling zero-shot tasks; no detailed ablation studies or numeric evidence included in the survey itself.",
            "evaluation_method": "Not detailed in the survey; described qualitatively as zero-shot function prediction and modeling—would typically be evaluated against curated benchmarks for function and sequence modeling, but that detail is absent here.",
            "limitations_or_failure_cases": "Survey does not enumerate explicit failures for Evo; general issues include long-sequence modeling challenges, dependency on diverse training genomes, and potential for spurious predictions absent structural/experimental validation.",
            "comparisons": "No quantitative head-to-head comparisons presented within the survey text.",
            "recommendations_or_best_practices": "Train on diverse, large-scale genomic corpora, use architectures that handle very long contexts, adopt single-base tokenization for nucleotide-level precision when required, and validate predictions with orthogonal experimental or computational methods.",
            "uuid": "e5539.2",
            "source_info": {
                "paper_title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ChemCrow / ChatGPT / GPT-4 (chemistry tooling)",
            "name_full": "ChemCrow (tool-augmented ChatGPT / GPT-4 for chemistry tasks)",
            "brief_description": "An approach that augments general LLMs (ChatGPT/GPT-4) with domain-specific tools (ChemCrow integrates 13 expert-designed chemistry tools) to perform chemistry tasks such as organic synthesis planning, drug discovery and materials design via text-based interaction.",
            "citation_title": "Chem-Crow: Augmenting large-language models with chemistry tools",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / GPT-4 (augmented via ChemCrow)",
            "model_description": "General-purpose decoder-only LLMs (ChatGPT / GPT-4) adapted for chemistry by attaching external, expert-designed tools (retrieval, cheminformatics utilities, reaction templates, calculators) to extend capabilities and reduce hallucination in chemistry workflows.",
            "model_size": null,
            "scientific_subdomain": "Chemistry — organic synthesis planning, retrosynthesis, drug discovery, materials design (text-guided workflows).",
            "simulation_task": "Text-based simulation/planning of organic synthesis steps, retrosynthetic routes, and drug/materials design workflows via chained tool calls and textual prompts.",
            "accuracy_metric": null,
            "reported_accuracy": "No quantitative accuracy metrics are provided in the survey for ChemCrow-augmented ChatGPT/GPT-4; the survey reports improved performance qualitatively when expert tools are integrated.",
            "factors_affecting_accuracy": "Presence and quality of integrated domain tools, prompt engineering, availability and quality of domain-specific retrieval/fact-bases, grounding against computational chemistry tools (e.g., calculators, reaction filters), and LLM base-model capabilities.",
            "evidence_for_factors": "Survey cites ChemCrow's use of 13 expert-designed tools to augment ChatGPT and states that this improved performance on chemistry tasks; no numerical ablation data are provided in the survey.",
            "evaluation_method": "Not described in detail in the survey; likely involves task-specific evaluations (e.g., retrosynthesis correctness, expert assessment) but the paper does not list exact metrics here.",
            "limitations_or_failure_cases": "General LLMs without tool augmentation often struggle with domain-specific scientific languages (molecules, sequences) and can hallucinate; even with tools, rigorous chemical validity and experimental feasibility may remain unverified unless integrated with physics-based checks.",
            "comparisons": "Survey notes that augmenting LLMs with tools (ChemCrow) improves chemistry-task performance versus using base ChatGPT/GPT-4 alone, but provides no quantitative benchmarks within the survey.",
            "recommendations_or_best_practices": "Augment general LLMs with verified domain tools, use retrieval and grounding databases, implement chemistry-specific filters and validators (e.g., reaction rules, molecule validity checks), and perform human/expert review or external computational validation (e.g., MD, quantum calculations) for critical predictions.",
            "uuid": "e5539.3",
            "source_info": {
                "paper_title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MolecularGPT",
            "name_full": "MolecularGPT: Open large language model for few-shot molecular property prediction",
            "brief_description": "A decoder-based LLM fine-tuned for few-shot molecular property prediction, demonstrating LLMs' utility as few-shot learners in molecular property tasks; trained/fine-tuned on molecular datasets and instruction sets.",
            "citation_title": "MolecularGPT: Open large language model (LLM) for few-shot molecular property prediction",
            "mention_or_use": "mention",
            "model_name": "MolecularGPT",
            "model_description": "A 7B-parameter LLM (LLaMA2-chat-7B backbone per the survey) fine-tuned on molecular corpora (e.g., ChEMBL, QM9) and hybrid instruction sets to perform few-shot property prediction and text-guided molecular tasks.",
            "model_size": "7B",
            "scientific_subdomain": "Computational chemistry / molecular property prediction (few-shot), molecular generation tasks.",
            "simulation_task": "Text-based few-shot prediction of molecular properties and text-conditioned molecule generation; using natural-language prompts to produce property predictions or candidate molecules.",
            "accuracy_metric": null,
            "reported_accuracy": "Survey describes MolecularGPT as demonstrating few-shot molecular property prediction capability; no numerical accuracy values or benchmark scores are provided in the survey text.",
            "factors_affecting_accuracy": "Pretraining and fine-tuning dataset coverage (ChEMBL, QM9, etc.), model size, instruction tuning quality, prompt design, and whether structural / 3D information is incorporated.",
            "evidence_for_factors": "Survey indicates MolecularGPT was trained on ChEMBL and QM9 and used instruction sets to achieve few-shot learning; the survey does not include ablation studies or numeric evidence for the impact of each factor.",
            "evaluation_method": "Not specified in the survey; typical evaluation would be property prediction metrics on held-out datasets or few-shot benchmarks, but this survey entry does not provide concrete metrics or protocols.",
            "limitations_or_failure_cases": "No specific failure cases enumerated in the survey for MolecularGPT; general limitations include lack of explicit 3D/physics grounding and potential hallucination for out-of-distribution chemistries.",
            "comparisons": "Survey does not provide direct quantitative comparisons to other molecular property predictors in this text.",
            "recommendations_or_best_practices": "Combine language-model approaches with structural/physics-based validation, expand training/fine-tuning data diversity, use careful instruction tuning and prompt engineering, and evaluate on standardized molecular benchmarks (e.g., MoleculeNet, GuacaMol).",
            "uuid": "e5539.4",
            "source_info": {
                "paper_title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
            "rating": 2,
            "sanitized_title": "genslms_genomescale_language_models_reveal_sarscov2_evolutionary_dynamics"
        },
        {
            "paper_title": "ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction",
            "rating": 2,
            "sanitized_title": "prollm_protein_chainofthoughts_enhanced_llm_for_proteinprotein_interaction_prediction"
        },
        {
            "paper_title": "Sequence modeling and design from molecular to genome scale with Evo",
            "rating": 2,
            "sanitized_title": "sequence_modeling_and_design_from_molecular_to_genome_scale_with_evo"
        },
        {
            "paper_title": "Chem-Crow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "MolecularGPT: Open large language model (LLM) for few-shot molecular property prediction",
            "rating": 2,
            "sanitized_title": "moleculargpt_open_large_language_model_llm_for_fewshot_molecular_property_prediction"
        }
    ],
    "cost": 0.021928749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains</p>
<p>Qiang Zhang qiang.zhang.cs@zju.edu.cn 
Hongyang Chen hongyang@zhejianglab.com 
Zhejiang Lab 
Hangzhou </p>
<p>Zhejiang University
HangzhouChina</p>
<p>ZJU-Hangzhou Global Scientific and Tech-nological Innovation Center
HangzhouChina</p>
<p>KEYAN DING
Zhejiang University
HangzhouChina</p>
<p>ZJU-Hangzhou Global Scientific and Techno-logical Innovation Center
HangzhouChina</p>
<p>TIANWEN LV
Zhejiang University
HangzhouChina</p>
<p>XINDA WANG
Zhejiang University
HangzhouChina</p>
<p>QINGYU YIN
Zhejiang University
HangzhouChina</p>
<p>YIWEN ZHANG
Zhejiang University
HangzhouChina</p>
<p>JING YU
Zhejiang University
HangzhouChina</p>
<p>YUHAO WANG
Zhejiang University
HangzhouChina</p>
<p>XIAOTONG LI
Zhejiang University
HangzhouChina</p>
<p>ZHUOYI XIANG
Zhejiang University
HangzhouChina</p>
<p>XIANG ZHUANG
Zhejiang University
HangzhouChina</p>
<p>ZEYUAN WANG
Zhejiang University
HangzhouChina</p>
<p>MING QIN
Zhejiang University
HangzhouChina</p>
<p>MENGYAO ZHANG
Zhejiang University
HangzhouChina</p>
<p>Authors' Contact Information: Qiang Zhang
Zhejiang University
HangzhouChina</p>
<p>ZJU-Hangzhou Global Scientific and Technological Innovation Center
HangzhouZhejiangChina</p>
<p>Zhe-jiang University
Keyan DingHangzhouZhejiangChina</p>
<p>ZJU-Hangzhou Global Scientific and Technological Innovation Center
HangzhouZhejiangChina</p>
<p>Tianwen Lv
Xinda Wang
Jing YuYiwen Zhang, Yuhao WangQingyu Yin</p>
<p>Xiaotong Li
Xiang Zhuang, Zeyuan Wang, Ming QinZhuoyi Xiang, Mengyao Zhang</p>
<p>Jinlu Zhang, Jiyu Cui
Renjun Xu</p>
<p>Zhejiang University
HangzhouZhejiangChina</p>
<p>Zhejiang Lab
Hongyang Chen, HangzhouZhejiangChina</p>
<p>Xiaohui Fan
and Innovation Center of Yangtze River Delta
Zhejiang University
HangzhouZhejiangChina</p>
<p>Zhejiang University
JiashanChina</p>
<p>Huabin Xing
Zhejiang University
HangzhouZhejiangChina</p>
<p>and Engineering Research Center of Functional Materials Intelligent Manufacturing of Zhejiang Province
ZJU-Hangzhou Global Scientific and Technological Innovation Cen-ter
Hangzhou, HangzhouZhejiangChina, China</p>
<p>Zhejiang University
HangzhouZhejiangChina</p>
<p>ZJU-Hangzhou Global Scientific and Technological Innovation Center
HangzhouChina</p>
<p>JINLU ZHANG
Zhejiang University
HangzhouChina</p>
<p>JIYU CUI
Zhejiang University
HangzhouChina</p>
<p>RENJUN XU
Zhejiang University
HangzhouChina</p>
<p>XIAOHUI FAN
Zhejiang University
HangzhouChina, China</p>
<p>Innovation Center of Yangtze River Delta
Zhejiang University
JiashanChina</p>
<p>HUABIN XING
Zhejiang University
HangzhouChina</p>
<p>and Engineering Research Center of Functional Materials Intelligent Manufacturing of Zhejiang Province
ZJU-Hangzhou Global Scientific and Technologi-cal Innovation Center
Hangzhou, HangzhouChina, China</p>
<p>HUAJUN CHEN
Zhejiang University
HangzhouChina</p>
<p>ZJU-Hangzhou Global Scientific and Tech-nological Innovation Center
HangzhouChina</p>
<p>Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains
31C5898242443D42ADCFF403916E53DF10.1145/3715318CCS Concepts:Computing methodologies → Natural language processing• Applied computing → Computational biologyScientific domain, large language models, protein, molecule, genome
Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence.The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines.This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery.As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration.However, a systematic and up-to-date survey introducing them is currently lacking.In this article, we endeavor to methodically delineate the concept of "scientific language, " whilst providing a thorough review of the latest advancements in scientific LLMs.Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains.This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation.Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs.By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.</p>
<p>Introduction</p>
<p>Humanity acquires knowledge of the world through perception and cognition, where natural languages (i.e., human languages) stand as the quintessential medium for articulating this world knowledge.Historically, this plethora of world knowledge has been expressed, chronicled, and disseminated through natural languages.Currently, Large Language Models (LLMs) stand as cutting-edge tools in processing natural language and gathering world knowledge.Typically, LLMs refer to those based on Transformer-based architectures with hundreds of millions (or even billions) of trainable parameters, trained on extensive textual corpus [188].Typical examples include GPT-3 [27], PaLM [39], Galactica [177], LLaMA [212], ChatGLM [260], and Baichuan Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains 161:3 2 [250].They have exhibited strong capacities to understand natural language and address complex tasks (in the manner of text generation), and have incited substantial interest in both academic and industrial domains.The exceptional performance of LLMs sparks hope that they may evolve into Artificial General Intelligence (AGI) in our current era.</p>
<p>Besides natural languages, to encapsulate more specialized science knowledge, an assortment of scientific languages has been developed, as illustrated in Figure 1.This encompasses textual expressions in the scientific research domains, mathematical languages to define mathematical formulas, chemical languages such as SMILES that represent molecular structures, and biological languages that describe proteins or genomes, and detail the complex constitution of living organisms.These scientific languages come with their distinct vocabularies, where each term holds a specific meaning that can be completely different from natural languages.For example, the character "C" in English represents the amino acid Cysteine in protein languages [71], while in the SMILES language system, it denotes a Carbon atom [231].Furthermore, experts in specific domains establish grammatical rules to organize these terms, enabling the construction of sentences with precise semantic functions.For instance, computational chemists create grammatical rules ensuring the accuracy of machine-generated molecules in SELFIES format [108].After decades of evolution, scientific languages have become invaluable tools, significantly accelerating scientific discoveries.Due to the potential semantic and grammatical differences between scientific and natural languages, existing general LLMs (such as ChatGPT or GPT-4 [3]) often fail to properly deal with scientific data like molecules and proteins [6].These limitations hinder their ability to accurately interpret data related to complex scientific domains due to their limited understanding of domain-specific terminology, structure, and rules, particularly in molecular and protein modeling.</p>
<p>To facilitate the understanding of scientific languages, researchers have devised Scientific Large Language Models (Sci-LLMs) customized for various scientific domains and disciplines.For instance, molecular language models have been developed to represent molecule structures as a string of atoms and chemical bonds.These models aid in predicting molecular properties, designing new drugs, and proposing retrosynthesis routes.Similarly, protein language models operate based on sequences of amino acids [25,177].They are used to forecast 3D protein structures and functions [123], enhance existing proteins for improved fitness [163], and create new proteins with specific functionalities [160].As a burgeoning area within the AI-for-Science research, many Sci-LLMs have been proposed with modified architectures, learning methods, training corpus, and evaluation benchmarks and criteria.Despite their notable achievements, these models have mostly been explored within their respective research domains.There is currently a lack of a comprehensive review that unifies these language modeling advancements.</p>
<p>In this survey, we set out to fill this gap by systematically reviewing the technical advancement of Sci-LLMs with a close reference to general LLMs.Considering the expansive scope of scientific languages, we focus our investigation on biological and chemical languages.Specifically, our examination encompassed molecular languages, protein languages, and genomic languages.In addition to these specialized scientific languages, we recognize the immense scientific knowledge embedded in textbooks, patents, and research papers, composed in natural languages.Consequently, we explore the textual LLMs with an emphasis on scientific knowledge, and more importantly, the multimodal LLMs that encompass various types of scientific languages.</p>
<p>When delving into each language system, we first review the LLM architectures and categorize them into three classes: encoder-only, decoder-only and encoder-decoder.Then, we report the model capabilities and summarize the typical downstream tasks that Sci-LLMs can conduct.In terms of model training and evaluation, we collect a bunch of commonly used training corpus and evaluation benchmarks.Finally, we present the proper criteria for discriminative and generative tasks of scientific language modeling.This survey is confined within specific boundaries.First, we focus on scientific languages, specifically chemical and biological languages.We have excluded languages lacking both universally defined vocabularies and grammatical structures, such as mathematical languages.Second, when discussing textual LLMs, our emphasis remains on chemical and biological domain knowledge expressed in natural languages.This choice ensures a consistent and coherent interaction with languages specific to chemistry and biology, such as molecular and protein languages.Third, our technical exploration is primarily confined to Transformer-based language models.We have not included alternative neural architectures like graph neural networks and diffusion models, despite their wide applications in molecule and protein modeling.Figure 2 describes the research scopes of Sci-LLMs in this survey.This survey's distinct boundaries set it apart from other reviews of LLMs and computational modeling for molecules, proteins, and genomes.In contrast to those primarily centered on natural languages [252,279], our emphasis leans more toward scientific languages.Unlike surveys concentrating solely on molecule [53,239], protein [20,87,213,217], or genome data [44], we aim to provide a comprehensive view of language models for chemical and biological research.Furthermore, we delve into multimodal LLMs, exploring the interactions between texts and molecule/protein/genome languages.To the best of our knowledge, this nuanced exploration has not been covered in previous surveys.The contributions of the survey can be summarized as follows:</p>
<p>-We offer a comprehensive review of language modeling within scientific domains, encompassing textual, molecular, protein, and genomic languages, emphasizing domain-specific knowledge.-We provide a detailed summary of existing Sci-LLMs, covering model architectures, capabilities, training data, evaluation benchmarks, and assessment criteria.We also show the evolutionary tree of Sci-LLMs in Figure 3. -We enumerate available resources for Sci-LLMs, open-source and maintain the related materials at https://github.com/HICAI-ZJU/Scientific-LLM-Survey,thereby facilitating accessibility for newcomers to the field.The remainder of this survey is organized as follows: Sections 2-6 present Sci-LLMs, including textual, molecular, protein, genomic, and multimodal LLMs, respectively.Finally, we analyze the limitations of existing models, pinpoint potential research directions, and conclude this survey in Section 7.</p>
<p>Textual Scientific Large Language Models</p>
<p>In this section, we explore and delve into scientific large language models specifically trained using textual corpora (i.e., Text-Sci-LLMs), particularly emphasizing their acquisition of chemical and biological knowledge.We will briefly review the existing Text-Sci-LLMs and examine their capabilities, the employed datasets, as well as the evaluation approach.</p>
<p>Models</p>
<p>To provide a clear overview of Text-Sci-LLMs, we divide them into three distinct categories: biological, chemical, and comprehensive, based on the specific scientific domains they focus on.Table 1 shows the summary of Text-Sci-LLMs.</p>
<p>Biological Domain.Large language models trained on extensive biological corpora, such as Bidirectional Encoder Representations from Transformers (BERT) [104] and its variations with the encoder-only LLM architecture, have demonstrated significant potential in Natural Language Processing (NLP) tasks within biology.BioBert [111], BioMegatron [193], PubMedBERT [73], BioM-BERT [7], and LinkBERT [255] initially trained on broad corpora like Wikipedia and textbooks and then fine-tuned on specific biological NLP tasks, showcase their significant improvements in biological terminology understanding.Since the encoder-only models lack the capability of textual information generation, GPT and its variants [27,171,172] with a decoder-only architecture have emerged as dominant players in the field of biological NLP.BioGPT [142], an extension of GPT-2 [172], has been extensively pretrained on biomedical literature, showcasing remarkable performance in relation extraction and question answering.BioMedGPT-LM [144], which is incrementally pretrained on LLaMA2 [212], enables the comprehensive understanding of various biological modalities and aligning them with natural language.</p>
<p>Chemical Domain.In the realm of chemistry, natural language is commonly used to express various attributes and discoveries related to compounds and chemical reactions, making LLMs wellsuited for comprehending and reasoning about chemical information [24].Built upon the BERT architecture, ChemBERT [75] was pretrained on an extensive dataset comprising 200,000 chemical journal articles.This model is tailored for automated chemical reaction extraction from chemical literatures, employing Masked Language Modeling (MLM) for pretraining and fine-tuning for chemical NLP tasks.MatSciBERT [78] and MaterialsBERT [192] are materials science-specific models, trained on a large corpus of material literature, and show cutting-edge performance in extracting material property information.It is worth noting that many works focus on augmenting</p>
<p>Biology</p>
<p>BioBERT [111] 2019.05117M BERT PubMed, PMC BlueBERT [169] 2019.07117M BERT PubMed BioMegatron [193] 2020.10 345M-1.2BBERT PubMed, PMC PubMedBERT [73] 2020.10 117M BERT PubMed × BioM-BERT [7] 2021.06 235M BERT PubMed, PMC BioLinkBERT [255] 2022.03110M, 340M BERT PubMed BioGPT [142] 2023.03347M GPT PubMed BioMedGPT-LM [144] 2023.08 7B LLaMA PMC, arXiv, WIPO</p>
<p>Chemistry</p>
<p>ChemBERT [75] 2021.06 120M BERT Chemical journals MatSciBERT [78] 2021.09117M BERT Elsevier journals MaterialsBERT [192] 2022.09-BERT Material journals</p>
<p>Comprehensive</p>
<p>SciBERT [16] 2019.09117M BERT Semantic Scholar ScholarBERT [85] 2023.05340M, 770M BERT Wiki, Books, etc. DARWIN-Base [243] 2023.08 7B LLaMA SciQ, Web of Science SciGLM [264] 2024.01 6B, 32B ChatGLM3 SciInstruct Uni-SMART [29] 2024.037B -Patents, news, literature, etc. × SciDFM [204] 2024.0918.2B -BioRxiv, PubChem, books, etc.</p>
<p>ChatGPT or GPT-4 to accomplish chemical-related tasks.For example, ChemCrow [23] augmented the performance of ChatGPT in chemistry by incorporating 13 expert-designed tools, specifically tailored for accomplishing tasks related to organic synthesis, drug discovery, and materials design.</p>
<p>Comprehensive Domain.SciBERT [16] and ScholarBERT [85] are BERT-based models trained on a diverse scientific corpus.DARWIN-Base [243], based on a decoder-only architecture, is trained on a massive corpus from various scientific texts and knowledge bases.Notably, it was fine-tuned with instruction data from the SciQ [232] dataset and additional question-answer pairs, exhibiting proficiency in tasks that require extensive scientific knowledge.We emphasize that, although the general LLMs such as ChatGPT [165], GPT4 [3], and Gemini [208] encompass a wealth of scientific knowledge, they are not tailored for Sci-LLMs, and therefore do not fall under the category of general Text-Sci-LLMs.</p>
<p>Datasets</p>
<p>The pre-training corpora are mainly sourced from numerous scientific articles from various websites, including PubMed, PubMed Central (PMC), bioRxiv, arXiv, Web of Science, and so on.These websites provide rich data resources for training Text-Sci-LLMs in a self-supervised manner.We also curate a collection of benchmarks for Text-Sci-LLMs, as presented in Table 2.These benchmarks include multiple subsets that can be utilized to evaluate LLMs' abilities in grasping scientific knowledge at different levels within the biological, chemical and comprehensive domains.</p>
<p>Evaluation</p>
<p>Capabilities.The evaluation of LLMs is often structured around Bloom's taxonomy [65,107], which outlines six cognitive levels of educational learning objectives: Remember, Understand, Apply, Analyze, Evaluate, and Create.More recently, SciEval [203] has suggested assessing scientific LLMs across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability.Each dimension aligns with one or more cognitive levels in Bloom's taxonomy.Sci-KnowEval [61] extends these frameworks by introducing five progressive dimensions-knowledge memory, comprehension, reasoning, discernment, and application-tailored specifically for evaluating the scientific knowledge capabilities of language models.This multi-dimensional approach Evaluation Metric.To quantitatively measure the performance of Text-Sci-LLMs, one can utilize the standard metrics commonly employed in the field of machine learning.These include Accuracy, Precision, Recall, and F1-score, which are particularly well-suited for discriminative tasks such as multiple choice and judgment questions.For generative tasks such as question-answering, it is evaluated by measuring the similarity of the LLM's output and the ground-truth.These similarity metrics include BLEU (Bilingual Evaluation Understudy) [166], ROUGE (Recall-oriented Understudy for Gisting Evaluation) [122], BERT-Score [271], and LLM-Score [281].When the ground-truth is absent, one could sort to experts or strong LLMs like GPT-4 [3] for assessing LLMs.Human evaluation plays a pivotal role in assessing the quality and accuracy of model-generated results, as it closely aligns with real-world application scenarios and provides comprehensive and precise feedback.However, due to extensive human involvement, this process is labor-intensive.Therefore, there is a growing tendency that employ GPT-4 as a "judge" to evaluate different models [28].Overall, these metrics are often used complementarily in various NLP tasks to provide a comprehensive evaluation of LLMs.</p>
<p>Molecular Large Language Models</p>
<p>Large language models have shown great potential in accelerating chemical molecular discovery.</p>
<p>In this section, we provide a review of LLMs trained in molecular language (Mol-LLMs), including insights into their model architectures, capabilities, utilized datasets, and evaluation criteria.Table 3 summarizes the typical Mol-LLMs.</p>
<p>Models</p>
<p>Encoder-only Models.Encoder-only models focus on understanding and interpreting the input molecules, making them well-suited for tasks that require a deep comprehension of molecular structures and properties.A typical example is SMILES-BERT [222] that leverages the BERT architecture to interpret SMILES representations of molecules.MTL-BERT [273] employs multitask learning to increase the prediction accuracy for multiple molecular properties.The rxnfp-BERT [186] model draws on the principles of BERT to process and learn from chemical reactions.Mol-BERT [115] extracts atomic-level and substructural features centered on the current atom.MoLFormer [179] further enhances this approach by integrating relative and rotational position encoding, thereby facilitating the comprehension of spatial relationships between atoms in a molecule.MolRoPE-BERT [131] augments this method by employing Rotary Position Embedding (RoPE) to efficiently encode positional information in SMILES, aiming to provide a more robust understanding of molecules.RoBERTa [130], introduces dynamic token masking for varied learning and eliminates the next sentence prediction task to better learn long-range dependencies.</p>
<p>A variety of models have been built upon the RoBERTa framework, including chemBERTa [38], chemBERTa-2 [5], MFBERT [1], SELFormer [258], and semi-RoBERTa [214].The above models integrate a vast amount of unlabeled molecular data through a self-supervised learning strategy, but they are based on the sequential format of molecule SMILES or SELFIES, overlooking crucial graph structures and three-dimensional (3D) chemical information.The oversight of graph structures can be effectively addressed by integrating graphs with Transformers, such as GROVER [178], MAT [151], MG-BERT [272], and KPGT [113].</p>
<p>161:10 Q. Zhang et al.</p>
<p>Decoder-only Models.</p>
<p>Mol-LLMs based on the GPT architecture, including MolGPT [12], SMILESGPT [4], iupaccGPT [150], cMolGPT [226], and Taiga [152], primarily use SMILES strings as input, effectively addressing the challenge of navigating the vast chemical space.These models are pivotal in drug discovery and material science, enabling the synthesis of molecules with specific properties.MolGPT [12], a pioneer in utilizing GPT for molecular generation, uses conditional training for property optimization.It stands out for its efficiency and effectiveness in molecular modeling and drug discovery.Moreover, both SMILESGPT and iupacGPT are based on the GPT-2 architecture.The cTransformer [223] and cMolGPT [226] models are specifically developed for the purpose of generating molecules based on given conditions.Taiga [152] exemplifies the combination of GPT and reinforcement learning (RL) methodologies.</p>
<p>Encoder-Decoder Models.In the majority of Transformer-based encoder-decoder models, SMILES or SELFIES serve as inputs for the encoder, and the outputs vary across tasks.BART [112], a Transformer-based architecture renowned for its bidirectional encoder and autoregressive decoder, has been effectively applied in molecular design.Chemformer [93] is a BART language model fine-tuned for sequence-to-sequence and discriminative tasks in cheminformatics.BARTSmiles [37] leverages self-supervised pre-training on over 1.7 billion molecules.MOLGEN [60] combines domain-agnostic pre-training with self-feedback, aligning generative probabilities with real-world chemical preferences to address hallucinations.For chemical reaction prediction, Molecular Transformer [185] is a pioneering Transformer-based model for reaction prediction.Retrosynthesis Transformer [102] stands out for snapshot learning techniques to improve training efficiency.SCROP [282] excels in inferring a set of original candidate reactants and incorporates a syntax corrector with position encoding.ChemReactNet [209] employs advanced data augmentation, notably using SMILES augmentation and beam search algorithms.GO-PRO [149] combines Transformer with a context-free grammar (CFG)-based representation of molecules.Additionally, several models combine Transformers with distinctive architectures.RetroTRAE [215] not only excels in fragment-based tokenization but also learns changes in atomic environments during chemical reactions.GCT [105] integrates a Transformer-based language model with a conditional variational autoencoder, augmented by a conditional Gaussian latent space.RetroSynth-Diversity [211] and Disconnection Aware model [210] both employ prompts to guide their retrosynthesis predictions, enhancing diversity and accuracy in different ways.</p>
<p>Datasets</p>
<p>The pre-training dataset is often used for self-supervised learning, where the model learns to understand and represent molecular structures without explicit guidance on specific tasks.Here is a collection of pre-training datasets for Mol-LLMs, as summarized in Table 4.We also curate a collection of benchmarks for Mol-LLMs, including MoleculeNet [237], MARCEL [289], GuacaMol [26], MOSES [170], and ADMETlab 2.0 [244] (see Table 4), which can be effectively utilized for training and evaluating the performance in various molecular tasks.</p>
<p>Evaluation</p>
<p>Property Prediction.Molecular property prediction stands as a pivotal challenge in computational chemistry, where the goal is to accurately predict the properties of molecules, such as solubility, lipophilicity, affinity, absorption, distribution, metabolism, excretion, toxicity, and biological activity, based on their chemical structure.</p>
<p>Molecule Generation.Molecule generation is an emerging and critical area in computational chemistry and drug design.Molecule generation encompasses two primary approaches: template-based design and de novo design.The former involves modifying known molecules or scaffolds.The de novo design refers to the creation of novel molecular structures from scratch, without relying on pre-existing templates.This method is particularly valuable for discovering unique compounds with potential therapeutic or material applications.</p>
<p>Evaluation Metric.To quantify the performance of Mol-LLMs in prediction tasks, which primarily involve classification and regression, one can employ the standard metrics in machine learning, such as accuracy, F1-score, and correlation coefficient.For molecular generation tasks, their performance is evaluated based on their ability to produce diverse, realistic, and relevant molecules.</p>
<p>There are several popular quantitative metrics, including Validity, Uniqueness, Novelty, Internal and External diversity, Fragment similarity (Frag), Scaffold similarity (Scaff), Fréchet  [25] 2022.0216M BERT UniRef90 Func.pred.LM-GVP [227] 2022.04 -Trans.enc -Func.pred.RSA [146] 2022.05-ESM-1b -Func.pred.OntoProtein [268] 2022.06 -BERT ProteinKG25 Func.pred.ESM-2 [123] 2022.07 8M-15B RoBERTa UniRef50 Func.pred., Struct.pred.PromptProtein [230] 2023.02650M RoBERTa UniRef50, PDB Func.pred.KeAP [286] 2023.02-RoBERTa ProteinKG25 Func.pred.ProtFlash [220] 2023.10 79M/174M Trans.enc UniRef50 Func.pred.ESM-GearNet [277] 2023.10 -ESM-1b, GearNet -Func.pred.SaProt [201] 2023.10 650M BERT -Mutation effect pred.ProteinNPT [163] 2023.ChemNet Distance (FCD), and so on.For detailed information regarding these metrics, we recommend referring to Reference [170].</p>
<p>Protein Large Language Models</p>
<p>In the past years, large language models have become increasingly influential in protein research, offering novel insights and capabilities in understanding and manipulating proteins.In this section, we present a comprehensive review of LLMs for proteins (named Prot-LLMs), encompassing detailed discussions on their model architectures, utilized datasets, various capabilities, and corresponding evaluation criteria.Table 5 provides a summary of Prot-LLMs.</p>
<p>Models</p>
<p>Encoder-only Models.Most of the encoder-only Prot-LLMs are built upon the encoder of Transformer, which enables the encoding of protein sequences or structures into fixed-length vector representations.Prominent pre-trained protein sequence encoders include ESM-1b [177], ESM-1v [154], and ESM-2 [123], ProteinBert [25], ProtTrans [58].The ESM series [123,154,177] mainly employs a Transformer's encoder (i.e., BERT [104] and RoBERTa [130]) to predict protein structure and function, which utilizes extensive sequence information from protein databases without relying on manual annotations of the sequences.ProteinBert [25] enhances the classical BERT architecture by incorporating a novel pretraining task for predicting protein functionality.Prot-Trans [58] trained several auto-encoder models (includes BERT [104], Albert [110], Electra [42]) on a vast of sequence data.PMLM [79] introduced the pairwise masked language model to directly pretrain the encoder with a focus on capturing co-evolutionary information reflected in residue co-variation within the sequence.ProtFlash [220] suggested a mixed-chunk attention mechanism that combines multiple positional encodings and a hybrid block attention mechanism incorporating both local and global attention, effectively reducing model complexity.ProteinNPT [163] 161:13 developed a non-parametric Transformer specifically designed for protein sequences, making it particularly suitable for scenarios involving sparse labels and multi-task learning.Other the other hand, Multiple Sequence Alignment (MSA) is a computational method that can reveal common features and variation patterns among sequences, which has been widely used in protein modeling.For example, ESM-MSA-1b (or MSA Transformer) [176] extends the self-attention mechanism to the MSA setting, which interleaves self-attention across rows and columns to capture dependencies between amino acids and between sequences.However, the utilization of MSA incurs significant computational overheads, and cannot handle orphan proteins.An alternative method, Retrieved Sequence Augmentation (RSA) [146], eliminates the need for additional alignment or pre-processing steps.Given that the function of a protein is intricately linked to its structure, another reasonable approach for protein representation would involve the integration of its 3D conformation.ESM-GearNet [277] combines ESM-1b and GearNet [242] to enhance model performance.SaProt [201] introduces a structure-aware vocabulary that integrates residue tokens with structure tokens.LM-GVP [227] compose Transformer blocks and a graph network derived from the protein 3D structure.PromptProtein [230] utilizes prompt-guided multi-task pretraining to fuse different levels of protein structure.Additionally, some approaches propose to introduce extra knowledge such as Gene Ontology (GO) to enhance the protein representation, such as OntoProtein [268] and KeAP [286].</p>
<p>Decoder-only Models.Decoder-based Prot-LLMs play a predominant role in the generation of novel proteins, serving as a crucial tool in protein engineering and drug design.A representative model is ProGen [147], which utilizes GPT for controllable protein generation.It has been trained on a dataset comprising 280M protein sequences, accompanied by conditioning tags that encode diverse annotations encompassing taxonomic, functional, and locational information.Pro-Gen2 [160] extends the model to 6.4B parameters and is trained on diverse sequence datasets extracted from over one billion proteins from genomic, metagenomic, and immune repertoire databases.ProtGPT2 [62] is another GPT-based autoregressive model that effectively generates protein sequences exhibiting amino acid compositions and disorder propensities comparable to those observed in natural proteins.RITA [82] presents a suite of autoregressive generative models, which is the first systematic investigation of how capabilities evolve with model size for these protein models.The decoder-based models have been widely applied in specific protein designs.Timothy et al. introduced the protein evolutionary Transformer (PoET), an autoregressive generative model over protein families, which can generate sets of related proteins.Zheng et al.</p>
<p>proposed the LM-Design [283] that uses language models for structure-based protein design (i.e., inverse folding).ZymCTRL [156] trained a conditional language model on the BRENDA enzyme database [30], aimed at designing customized artificial enzymes by generating specific enzyme classes based on user prompts.IgLM [195] employed autoregressive sequence generation for antibody design.</p>
<p>Encoder-decoder Models.The encoder-decoder architecture is commonly used for sequenceto-sequence tasks.ProstT5 [80], a protein language model based on the T5 architecture [173], is an exemplary encoder-decoder model that facilitates translation between protein sequences and structures.The conversion of protein structure from 3D to 1D is achieved through the utilization of 3D tokens introduced by Foldseek [218].pAbT5 [41] is another T5-based model that takes into consideration the constraints imposed by protein-protein interactions on generation and design.It can generate complementary heavy or light chains from their pairing partners.xTrimoPGLM [31], based on General Language Model (GLM) architecture [54], is a unified protein language model for understanding and generating protein sequences.Building upon the Transformer, several Comprehensive and non-redundant protein sequence database Pfam [63] 2023.0947M Protein family database BFD [99,196,197] 2021.07 2.5B Protein sequences from multiple databases and resources PDB [238] 2023.12 214K Experimentally determined accurate protein structures AlphaFoldDB [99,219] 2021.11200M Protein structures predicted by AlphaFold Benchmark EC [153] 2023.112.6M Enzymes classification database GO [9] 2023.111.5M Gene Ontology knowledgebase HIPPIE [182] 2022.04 39K Protein-protein interaction networks ProteinGym [162] 2022.12 ∼ 300K Predict the effects of protein mutations FLIP [49] 2022.01 ∼ 320K Fitness landscape prediction (AAV, Thermostability, GB1)</p>
<p>PEER [248] 2022.11∼ 390K Protein function, Localization, Structure prediction, Protein-protein interaction, Protein-ligand interaction TAPE [175] 2021.09∼ 120K Remote homology detection, Secondary structure, Contact, Fluorescence, Stability prediction models have incorporated MSA modules.MSA2Prot [174] is an MSA-to-protein Transformer, which developed axial and cross attentions for the encoder and decoder to model sequence probabilities autoregressively.MSA-Augmenter [267] utilizes protein-specific attention mechanisms and large-scale MSAs to generate novel protein sequences.</p>
<p>Datasets</p>
<p>Protein datasets can be classified into two categories based on the availability of annotations: pretraining datasets and benchmarks.The former, which often lacks labels, is commonly employed for self-supervised pre-training, such as UniProtKB (Swiss-Prot and TrEMBL) [22,155], Pfam [63], and BFD [99,196,197].The latter, with labeled data, is utilized for supervised fine-tuning or model evaluation, including EC [153], GO [9], ProteinGym [161], FLIP [49], PEER [248], and TAPE [175].</p>
<p>We summarize these pre-training datasets and benchmarks in Table 6.</p>
<p>Evaluation</p>
<p>Protein Function Prediction.This task aims to determine the biological function of protein sequences, including how they operate within an organism and their interactions with other biomolecules.Function prediction encompasses a multitude of subtasks, such as protein stability prediction, binary localization prediction, subcellular localization prediction, remote homology detection, fluorescence landscape prediction, β-lactamase activity prediction, solubility prediction, mutation effect prediction, protein-protein interaction prediction, and protein-ligand interaction prediction.</p>
<p>Protein Sequence Generation.It refers to the computational process of creating novel amino acid sequences that could potentially form functional proteins, which can be used for various applications such as drug design, enzyme engineering, and fundamental biological research.Protein sequence generation can be roughly classified into two categories: de novo protein design and protein sequence optimization.The former involves creating entirely new protein sequences that are not based on existing proteins.The autoregressive generative models (e.g., ProGen series) are often employed for the tasks of protein sequence generation.Protein sequence optimization focuses on modifying existing protein sequences to enhance or alter their functions, employing techniques such as directed evolution.</p>
<p>Evaluation metric.The evaluation of protein function predictor typically includes the standard metrics in machine learning (e.g., accuracy for classification tasks and correlation coefficient for Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains 161:15 regression tasks) can be employed to quantitatively assess the performance of these Prot-LLMs.</p>
<p>To evaluate the performance of protein sequence generation, one can use the following metrics: model perplexity (PPL), sequence novelty, similarity (e.g., Fréchet Protein Distance [95]), and diversity of generated protein sequences, as well as the condition consistency if the generation is conditioned on certain inputs (e.g., protein backbone).Additionally, for structure-based generation (i.e., inverse folding), recovery rate, foldability are often employed [229].</p>
<p>Genomic Large Language Models</p>
<p>In the field of computational biology, genomic data exhibits resemblances to sequence-based information observed in natural language, enabling the application of large language models for analyzing genomic sequences.In this section, we provide a review of LLMs tailored for genomic language (Gene-LLMs) for DNA and RNA sequences, including insights into their model architectures, datasets, and evaluation.Table 7 provides a summary of Gene-LLMs.</p>
<p>Models</p>
<p>Encoder-only Models.In the realm of encoder-only architecture, significant models such as SpliceBERT [282], DNABERT [94], DNABERT-2 [288], iEnhancer-BERT [140], scBERT [251],</p>
<p>BioSeq-BLM [114], PLPMpro [120], GPN [18], SA DNA-LM [68], RNA-MSM [275], miProBERT [224], TFBert [141], MRM-BERT [274], GPN-MSA [17], iDNA-ABF [96], and GET [67] are noteworthy attempts employing the Transformer encoder within genomics.These models utilize a mask training mechanism where partial gene sequences are masked, prompting the model to predict and complete them, progressively learning inherent patterns within gene sequences.MoDNA [8] employs a Bert-like encoder-only architecture, introducing a unique training paradigm with a stacked Generator-Discriminator structure.D GENA-LM [64] introduces a suite of encoder-based foundational DNA language models designed for genomics research.Nucleotide-Transformer [48] is a foundation encoder-only model pre-trained on DNA sequences.It integrates data from 3,202 diverse human genomes and 850 genomes from various species.EpiGePT [70] is a transformer-based language pretrained model designed for epigenomics.It predicts genome-wide epigenomic signals by considering transcriptional regulation mechanisms.Uni-RNA [225] is an encoder-only model that excels in predicting RNA structures and functions, outperforming previous methods in various tasks.As gene sequences often significantly exceed the length of natural language text, many models aim to overcome the quadratic time complexity of attention mechanisms.Enformer [10] and LOGO [254] employ a blend of convolutional downsampling to handle longer DNA sequences efficiently.BioSeq-BLM [114] integrate traditional analysis methods with language models.PLPMpro [120] marks the advent of pre-training and fine-tuning in this domain, comparing language models' effects with traditional methods.GET [67] is a model for transcriptional regulation that relies on chromatin accessibility data and sequence information to learn transcriptional regulation syntax in different cell types through unique architectural designs.</p>
<p>Decoder-only Models.Decoder-only models have gained significant attention due to their potent generative capacity.GenSLMs [292] leverage genome-scale language models comprising multiple layers of attention-based decoders to elucidate the evolutionary dynamics of SARS-CoV-2.GenSLMs thus adeptly capture the evolutionary landscape of the SARS-CoV-2 genome.Pre-trained on prokaryotic gene sequences and further fine-tuned for SARS-CoV-2 genomes, GenSLM demonstrates precise and rapid identification of concerning variants.Besides commonly used architectures like transformers, niche architectures also find applications in genomic research.For instance, Models like DNAGPT [266] have introduced decoder-only architectures, akin to GPT, into the genomics domain by integrating specific symbols to guide different tasks, enabling zero-shot capabilities in species identification and regulatory factor prediction. HyenaDNA [159] incorporates a fast Fourier transform-accelerated recursive long-range convolution, a quadratic time complexity solution, enabling the handling of ultra-long DNA sequences.Employing convolution on the entire DNA sequence preserves single-nucleotide resolution, facilitating research into single nucleotide variability.Bio-xLSTM [184] stands out with its recurrent architecture, featuring reversecomplement equivariant blocks and linear time complexity, enabling efficient modeling of long DNA sequences.</p>
<p>Encoder-Decoder Models.Encoder-decoder models in genomics, such as the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) [148], represent a significant advancement in bioinformatics.These models combine the strengths of both encoder and decoder to effectively analyze and interpret complex genomic sequences.The encoder part of the model is responsible for compressing and encoding the input genomic data into a meaningful representation, capturing the essential features and patterns within the DNA sequences.This encoded representation is then passed to the decoder, which is tasked with generating or reconstructing sequences or performing other relevant bioinformatics tasks.MegaDNA [189] focuses on the phage genome, using 100,000 high-quality unlabeled genomes from databases like NCBI GenBank.It treats each Mammalian-Gene [199] 1999.10 -Mammalian Genomic Data GRCh38 [77] 2013.12 3.2G nucleotides Genome Reference, Human, Annotations 690 ChIP-seq [261] 2016.06 -DNA-binding Protein Data DeepSEA [287] 2017.04 -Chromatin Effects 1000 Genomes Project [45] 2017.10 20.5T nucleotides Human Genetic Variation EPDnew [52] 2019.11187K promoters Promoter Prediction Panglao [66] 2020.034M cells scRNA-seq Data ExPecto [181] 2020.12 -Gene Expression UCSC-Genome [101] 2022.11-Various Bio Data BV-BRC [164] 2023.01 -Bacterial and Viral Pathogens Ensembl [109] 2023.02-Various Bio Data RNAcmap [181] 2023.07 -RNA Evolutionary analysis ENCODE [47] 2023.09-Functional Genomics NCBI-Genome 2023.10 -Various Bio Data TAIR [109] 2023.12 33,602 genes Arabidopsis Information VGDB [83] 2023.12 2,847 genes Viral Genome Sequence CAGI5 [103] 2019.07-Assessing Genetic and Genomic Predictions Benchmark Protein-RNA [86] 2023.08 -Evaluating RBP-RNA Interaction Prediction Methods NT-Bench [48] 2023.093.2G+20.5T+174Gnucleotides Evaluating Genomics Foundational Models gene-benchmark [100] 2024.12 -Evaluating Gene Attribute Prediction DNA base as an independent token, avoiding segmentation bias and enabling single-base precision in learning DNA regulation.CpGPT [50] is an innovative foundation model designed specifically for DNA methylation analysis.Based on an improved Transformer++ architecture, it integrates sequence, location and epigenetic information to learn a rich representation of methylation patterns through pre-training on a massive CpGCorpus dataset containing over 100,000 samples.</p>
<p>Datasets</p>
<p>Currently, the majority of Gene-LLMs are trained on human DNA data.To increase the amount of training data, genomes of organisms closely related to humans, such as chimpanzees or pigs, or even the genomes of all mammals are utilized for training.We list prevalent training datasets and benchmarks in Table 8.</p>
<p>Evaluation</p>
<p>Function Prediction.Function prediction can be specified as subtasks: (1) promoter prediction, which involves predicting the locations of promoters, specific DNA segments recognized and initiated for transcription by RNA polymerase; (2) enhancer prediction, which plays a pivotal role in enhancing the likelihood of gene transcription by providing binding sites for various molecules, facilitating the assembly and stabilization of transcription machinery; (3) binding site prediction, which aims to pinpoint regions on DNA, RNA, or proteins where specific molecules, such as transcription factors or peptides, can bind.</p>
<p>Sequence Generation.This task focuses on the models' capacity to create artificial sequences that closely mimic real biological sequences.This ability becomes particularly crucial when considering the generation of artificial human genomes, serving as tools to safeguard genetic privacy and reduce costs linked with genetic sample collection.</p>
<p>Evaluation Metric.The evaluation metrics for function prediction mainly include accuracy, F1-score, sensitivity, specificity, and precision.In sequence generation, one can use metrics like Matthews correlation coefficient (MCC), perpendicular distance, true/false positive rate to evaluate the generated labels.As for the sequence variation and evolution analysis, AUROC can be used for classifying pathogenic variants.[200] 2022.09110M SciBERT, GIN S2ORC [135], PubChem, OGB [89] MolT5 [55] 2022.1160M T5 ChEBI-20 Text+Chem T5 [40] 2023.0540M/220M T5 ChEBI-20 DrugChat [121] 2023.0513B GNN, Vicuna PubChem, ChEMBL CLAMP [187] 2023.06 350M GPT-2 PubChem, PubMed GIMLET [278] 2023.06 -T5 Chembl ChatMol [263] 2023.06 -T5 ChEBL-dia [263] MolXPT [134] 2023.05350M GPT-2 PubChem, PubMed × MolFM [143] 2023.07 138M GIN, Trans.enc., TransE [21] PubChem, S2ORC GIT-Mol [124] 2023.08 700M SciBERT, GIN, Swin.Trans [132] PubChem, ChEBI-20 × GPT-MolBERTa [14] 2023.10 -RoBERTa MoleculeNet MoleculeSTM [126] 2023.12 252M GraphMVP [127], SciBERT• • • PubChemSTM MolecularGPT [129] 2024.06 7B LLaMA2-chat-7B ChEMBL, QM9 ChemLML [51] 2024.10
- SciBERT, Galactica• • • ZINC-15, NPASS• • • Prot.&amp;Text
ProTranslator [245] 2022.04 -PubMedBert -ProteinDT [125] 2023.02-SciBERT, ProtBERT SwissProtCLAP × ProtST-ProtBert [247] 2023.07 420M ProtBert, PubMedBERT ProtDescribe ProtST-ESM-1b [247] 2023.07 650M ESM-1b, PubMedBERT ProtDescribe ProtST-ESM-2 [247] 2023.07 650M ESM-2, PubMedBERT ProtDescribe Prot2Text [2] 2023.07 283M ESM2, RGCN [183], Trans.dec.Prot2Text × InstructProtein [229] 2023.10 1.3B OPT [270] InstructProtein × ProtLLM [291] 2024.02-LLaMA-7B InterPT [291] ProLLaMA [145] 2024.02-LLaMA2 UniRef50 ProtT3 [133] 2024.05-ESM2, Galactica Swiss-Prot [13], ProteinKG25 [269] ProteinCLIP [236] 2024.05-ESM2, ProtT5 UniRef50 ProLLM [97] 2024.07 -Flan-T5-large Mol-Instructions TourSynbio [191] 2024.10 7B InternLM2-7B ProteinLMDataset [190] ProtDAT [76] 2024.</p>
<p>ChemBERTaLM [216] 2022.09-RoBERTa, ChemBERTa BindingDB, MOSES [170] DeepTarget [35] 2023.03-Transformer ChEMBL DrugGPT [119] 2023.06 1.5B GPT-2 ZINC20 DrugCLIP [69] 2023.10 -SE(3) 3D Transformer PDBBind [221], BioLip, ChEMBL × scChat [139] 2024.10 ---</p>
<p>Comprehensive</p>
<p>Galactica [207] 2022.11120B Trans.dec.</p>
<p>-BioTranslator [246] 2023.02-PubMedBERT -ChatDrug [128] 2023.05-ChatGPT -BioMedGPT-10B [144] 2023.08 10B LLaMA2, GIN, ESM-2 PubChemQA, UniProtQA DARWIN-MDP [243] 2023.08 7B LLaMA SciQ, FAIR [233] BioT5 [168] 2023.10 252M T5 ZINC20 Mol-Instructions [59] 2023.11-LLaMA Mol-Instructions BioBridge [228] 2024.01 -KGs PrimeKG ChemDFM [280] 2024.01 13B LLaMa-13B Chemical literature, textbooks ChemLLM [265] 2024.027B InternLM2 ChemData and Multi-Corpus Evo [158] 2024.027B StripedHyena OpenGenome BioT5+ [167] 2024.02252M T5 PubChem, Uniref50, C4 MolBind [241] 2024.04 -SciBERT,GIN,UniMol MolBind-M4 MAMMAL [194] 2024.10 458M Transformer UniRef90,OAS,ZINC22• • •</p>
<p>Multi-modal Scientific Large Language Models</p>
<p>Multimodal Large Language Models have emerged as a prominent research area, with LLMs as the core to handle multimodal data.They hold promising prospects, particularly in the realms of biological and chemical sciences encompassing protein, molecular, and genomic studies.In this section, we explore recent advancements in multimodal models within these scientific fields (i.e., MM-Sci-LLMs), highlighting their capabilities and the exploited datasets.Note that, this survey focuses on cross-lingual multimodal models, involving at least two languages from different domains, such as text and molecule.Hence, we exclude the monolingual multimodal approaches from MM-Sci-LLMs, like the joint modeling of protein sequences and structures [201,230,276].Table 9 shows the summary of MM-Sci-LLMs.</p>
<p>Models</p>
<p>Molecule-Text Models.Molecule-text models are designed to understand both the chemical structure and text descriptions of molecules by creating associations between text-molecular structure 161:19 data pairs.This approach enables the model to effectively discern distinctions between various molecules, enhancing its ability for generalization.Text2Mol [57], MoMu [200], MolFM [143], GPT-MolBERTa [14], and MoleculeSTM [126] represent a suite of encoder-based molecule-text multimodal models.Text2Mol [57] is a multimodal embedding approach to establish an aligned semantic space between text and molecules, facilitating cross-modal retrieval.MoMu [200] is a molecular multimodal model pretrained on both molecular graphs with Graph Isomorphism Network (GIN) and their semantically related textual data with SciBERT [16] via contrastive learning, establishing a crucial link between molecular graphs and natural language.MolFM [143] serves as a multimodal molecular foundation model, facilitating joint representation learning from molecular structures, descriptive texts, and knowledge graphs.GPT-MolBERTa [14] utilizes ChatGPT to generate rich textual descriptions of molecules, which is then used to pre-train the RoBERTa model in a self-supervised manner.MoleculeSTM [126] employs a molecule encoder and a text encoder with a contrastive learning strategy to jointly learn the chemical structures and their textual descriptions.DrugChat [121], CLAMP [187], MolXPT [134], and MolecularGPT [129] are decoder-based molecule-text multimodal models.DrugChat [121] combines a graph neural network (GNN) and Vicuna [36] (a classical LLM) to enable ChatGPT-like capabilities on drug molecule graphs.CLAMP [187] is an advanced drug discovery model that utilizes contrastive pre-training and language understanding to excel in adapting to new prediction tasks, particularly in few-shot and zero-shot learning scenarios.MolXPT [134] is a generative pre-trained model that jointly models text and molecules.It aims to enhance the interaction between molecules and text, facilitating tasks such as molecular property prediction and text-guided molecule generation.MolecularGPT [129] is an advanced LLM fine-tuned for few-shot molecular property prediction, using a hybrid instruction set to demonstrate LLMs' potential as universal few-shot learners in the molecular domain.</p>
<p>MolT5 [55], Text+Chem T5 [40], GIMLET [278], ChatMol [263], GIT-Mol [124], and ChemLML [51] use encoder-decoder architectures.MolT5 [55] is a T5-based self-supervised learning framework that allows for the translation between molecules and natural language.Text+Chem T5 [40] leverages multi-task learning to unify molecular and textual representations, which can solve language tasks, chemical tasks, and cross-domain tasks, without the need for task-specific fine-tuning or retraining.GIMLET [278] is a novel model specifically designed for zero-shot learning in molecular property prediction.It distinctively integrates graph and text data using a transformer-based approach, with an emphasis on generalized position embedding.ChatMol [263] is initialized with T5 and conducts multi-task pre-training, facilitating conversational molecular design through natural language interaction.Based on Q-Former design from BLIP2 [116], GIT-Mol [124] proposes a novel modality mixer with a cross-attention mechanism, enabling seamless fusion of multiple molecular modalities, including SMILES, texts, images, and structure graphs.ChemLML [51] introduces an adapter-based lightweight strategy that operates within a dedicated molecular embedding space by combining pretrained text models with molecular models.This approach enables conditional molecular generation based on textual descriptions.</p>
<p>Protein-Text Models.The utilization of textual data from literature and databases has become increasingly important in protein research, leading to the emergence of protein-text multimodal models.ProTranslator [245] innovatively approaches protein function prediction by redefining it as a machine translation problem.By utilizing textual descriptions, ProTranslator can annotate proteins to functions without the need for pre-known associated proteins.ProteinDT [125] adopts contrastive learning to align the representation space of text and protein sequences.The ProteinDT model addresses the problem of aligning text and protein sequences to enable text-guided protein design and editing tasks.ProtST series [247], including ProtST-ESM-1B, ProtST-ESM-2, and ProtST-ProtBert, are models enhancing protein sequence pre-training by integrating biomedical texts.</p>
<p>Leveraging tasks like unimodal mask prediction and multimodal representation alignment, these models demonstrate superior performance in representation learning benchmarks.Prot2Text [2] predicts a protein's function in free text format by integrating GNNs and LLMs in an encoderdecoder framework.InstructProtein [229] stands out as a decode-only LLM bridging the gap between human and protein languages through bidirectional generative capabilities.ProtLLM [291] is a versatile crossmodal LLM that uses an interleaved protein-text pre-training approach to excel in both protein-centric and protein-language tasks through dynamic protein mounting and proteinas-word language modeling.ProLLaMA [145] transforms a pre-trained general LLM, LLaMA2, for protein language processing using a two-stage framework and Low-Rank Adaptation (LoRA) to efficiently retain natural language capabilities while learning protein language.ProtT3 [133] is a novel framework for protein-to-text generation that integrates a protein language model with a cross-modal projector, enhancing language models' ability to understand and generate textual descriptions of proteins.ProteinCLIP [236] utilizes contrastive learning to associate protein amino acid sequences with texts describing their functions.This approach optimizes the sequence embeddings of pre-trained protein language models, generating function-centric embeddings.ProLLM [97] employs a tailored language model to predict protein-protein interactions by simulating signaling pathways with natural language prompts and enhances protein understanding via embedding substitution and instruction fine-tuning.TourSynbio [191], a multimodal large model built on InternLM2-7B, excels in protein engineering tasks without an external encoder by posttraining on ProteinLMDataset [190] and surpasses GPT-4 in benchmark tests.ProtDAT [76] is a fine-grained framework that designs proteins from descriptive text using a multimodal crossattention mechanism to unify sequences and text, achieving high-performance protein sequence generation.</p>
<p>Protein-Molecule Models.Protein-molecule models focus on converting protein information into molecular information and exploring potential associations between the two.Proteinmolecular models typically employ an encoder-decoder architecture, where the encoder converts a protein sequence into fixed-length vectors, and the decoder generates molecular information from those vectors.ChemBERTaLM [216] uses a pre-trained biochemical language model to initialize a target molecule generation model, aiming to address the challenge of limited data on new protein labeling in drug discovery.DeepTarget [35] is an innovative deep generative model that designs new drug molecules directly from protein amino acid sequences, integrating adversarial networks and contrastive learning to minimize reliance on existing structural information.DrugGPT [119] is built upon the GPT framework for ligand design, enabling precise capture of both structural information and chemical principles governing drug molecules.DrugCLIP [69] is a novel approach to virtual screening for potential drugs that uses contrastive learning to align representations of binding protein pockets and molecules.scChat [139] is an AI tool for contextual single-cell RNA sequencing analysis, using LLM to offer insights and design suggestions while integrating techniques to enhance accuracy and reduce errors.These methods aim to improve the efficiency and accuracy of drug discovery through AI-assisted methods.</p>
<p>Comprehensive Models.In this survey, the comprehensive multimodal models include text, proteins, molecules, genomes and other scientific languages.Galactica [207], BioTranslator [246], ChatDrug [128], BioMedGPT0-10B [144], DARWIN-MDP [243], BioT5 [168], Mol-Instructions [59], BioBridge [228], ChemDFM [280], ChemLLM [265], Evo [158], BioT5+ [167], MolBind [241], and MAMMAL [194] are representative comprehensive multimodal models.Galactica [207] is a scientific language model based on a decoder-only architecture, trained on a massive corpus of 106 billion tokens from various scientific texts and knowledge bases.ChatDrug [128] utilizes conversational language models to perform drug editing tasks.BioMedGPT-10B [144] is an open multimodal 161:21</p>
<p>generative pre-trained transformer, to bridge the modality gap in biomedicine by aligning different biological modalities with natural language.DARWIN-MDP [243] is a variant of the DARWIN series that involves 16 FAIR datasets to generate instructions for material and device prediction tasks, such as classification, regression, and design.BioT5 [168] is a pre-training framework that leverages structured and unstructured data sources to capture the underlying relations and properties of bio-entities.It aims to address the challenges in cross-modal generation and prediction tasks in the field of computational biology.Mol-Instructions [59] aims to address the challenge of enhancing LLMs' performance in tasks related to molecules, proteins, and biomolecular text by providing task-specific instructions.BioBridge [228] is an innovative model that uses Knowledge Graphs to connect unimodal Foundation Models, facilitating multimodal integration in the biomedical field without requiring broad-scale retraining.ChemDFM [280], built on the LLaMa-13B model, is trained on chemical literature to assist with molecular recognition, property prediction, and reaction analysis, and to support research through free-form dialogues.ChemLLM [265] is a chemistryfocused LLM using InternLM2 and ChemBench for evaluation, while ChemDFM [280] leverages LLaMa-13B and chemical literature for molecular tasks and research dialogues.Evo [158] employs the StripedHyena architecture with 7 billion parameters, trained on OpenGenome datasets for zero-shot function prediction and DNA sequence modeling from molecular to genome-wide scales.BioT5+ [167] extends BioT5 by integrating IUPAC names and utilizing biotext and molecule data, employing multitask instruction tuning and a novel numerical tokenization technique for improved molecular understanding.MolBind [241] uses contrastive learning to align natural language, 2D and 3D molecular data, and proteins in a shared feature space, excelling in zero-shot cross-modal retrieval and classification.MAMMAL [194] is a foundational model for multitasking and multi-alignment, pre-trained on diverse biological datasets and using a prompt syntax for classification, regression, and generation across various modalities.</p>
<p>Datasets</p>
<p>Molecule-Text Datasets.The molecule-text datasets are mainly sourced from chemical literature, patents, drug instructions, and so on, which contain descriptions of molecules.ChEBI-20 [57], PCdes [262], MoMu [200], PubChemSTM [126], PubChemQA [144], MoleculeQA [138], MolCap-Arena [56], and TOMG-Bench [117] are widely used molecule-text datasets used to train models to achieve communication between molecular language and natural language.</p>
<p>Protein-Text Datasets.The text-protein dataset focuses on converting text information into protein-related information, including predictions about protein structure, function, and interactions.These datasets typically contain protein sequences, descriptive text, and corresponding annotation information.SwissProtCLAP [125], ProtDescribe [247], UniProtQA [144], InstructProtein [229], ProteinLMDataset [190], ProteinLMBench [190], and OPI [240] are eight representative datasets, providing a rich resource for the research community to explore new strategies for converting text information into protein information, or vice versa.Table 10 summarizes the datasets.</p>
<p>Molecule-Protein Datasets.Protein-molecule interaction datasets are an important resource in the field of bioinformatics and drug discovery, providing detailed information on interactions between proteins and small molecule ligands (e.g., DUD-E [157], BioLiP [253], and BindingDB [72]).These datasets are critical for understanding complex interactions between proteins and small molecules, developing new drugs, and predicting molecular behavior.</p>
<p>Comprehensive Datasets.The comprehensive datasets amalgamate various data modalities, encompassing protein, molecular, genomic compositions, and associated textual data, thereby enabling a holistic perspective in comprehending biological entities.Galactica [207], Scientific  [200] 2022.0915K Bidirectional Molecular Graph-to-text Retrieval PubChemSTM [126] 2022.12 281K+250K Structure-Text Pairing ChEBL-dia [263] 2023.06 10K Molecule Generation Evaluation PubChemQA [144] 2023.08 325K+365K Molecule Description MoleculeQA [138] 2024.0362K+23K Comprehensive Evaluation of Molecular MolCap-Arena [56] 2024.11-Molecular Property Prediction TOMG-Bench [117] 2024.12 5000 Molecule Generation</p>
<p>Prot.&amp;Text SwissProtCLAP [125] 2023.02441K Protein Sequence Analysis ProtDescribe [247] 2023.07 553K Protein Sequence Representation Prot2Text [2] 2023.07 256K Protein Function Prediction UniProtQA [144] 2023.08 569K+1.8MProtein Function and Property InstructProtein [229] 2023.10 2.8M Structured KG, Bidirectional Protein-Text Generation ProteinLMDataset [190] 2024.06 89.3KProtein Sequence-Text ProteinLMBench [190] 2024.06 944 A benchmark for protein understanding OPI [240] 2024.11Knowledge Dataset [243], Mol-Instructions [59], SMolInstruct [257], and MolBind-M4 [241] are available comprehensive datasets, which plays a crucial role in the training of LLMs for tasks such as prediction of protein-molecule interactions, drug discovery, and automated analysis of biomedical texts, effectively bridging the disciplinary divide between molecular biology and computational linguistics.</p>
<p>Evaluation</p>
<p>Evaluation Tasks.When evaluating MM-Sci-LLMs in the realm of multimodal models, their capabilities are primarily assessed based on three critical areas: cross-modal prediction, retrieval, and generation.Cross-modal prediction is a task that involves using multimodal models to predict the functionality of molecules, proteins, and genomes based on textual instructions.Cross-modal retrieval refers to the process of retrieving information from one modality based on a query from another modality, and vice versa.Cross-modal generation tasks aim to generate data in one modality based on data from another modality, which involves establishing interconnections between the different modalities.</p>
<p>Evaluation Metric.The evaluation criteria for MM-Sci-LLMs are similar to those used by unimodal LLMs introduced in previous sections (Sections 2.3, 3.3, 4.3, and 5.3), such as the classification metrics.In particular, in multimodal tasks, we emphasize the crucial alignment between the generated content and the input instruction, which necessitates evaluation using specific crossmodal alignment metrics.However, defining these metrics is challenging due to inherent difficulties in quantifying alignment across different modalities.Therefore, human evaluation often plays a crucial role as expert evaluators can assess biological relevance, novelty, and practical applicability of the generated content.</p>
<p>Conclusion and Perspective</p>
<p>In this survey, we have explored the latest advancements in Sci-LLMs, particularly in the biological and chemical fields.We delved into each scientific language type, examining their recent developments.This examination included an in-depth look at their model architectures, capabilities, datasets, and evaluation, covered in Sections 2-5, respectively.A notable highlight of the survey is Section 6, where we presented the emerging multimodal Sci-LLMs, focusing on the integration of multiple scientific language systems.In what follows, we discuss the research challenges of Sci-LLMs and potential avenues for future exploration.</p>
<p>Critical Challenges</p>
<p>-Scale of Pre-training Datasets.In the realm of language models, scaling laws delineate the relationship between an LLM's size (quantified by parameters or computational resources) and its performance, indicating that a model's capabilities and effectiveness tend to enhance, often in a nonlinear or exponential manner, as its size increases.Therefore amassing a substantial volume of pre-training data is critical for the development of larger and potentially more proficient LLMs.-Quality of Finetuning Datasets.The effectiveness of fine-tuning is heavily dependent on the quality of the labeled datasets, which can be assessed in terms of scale, diversity, and balance.A larger scale of fine-tuning datasets generally leads to more proficient models, as they are exposed to a wider array of scenarios.However, compiling extensive annotated data is resource-intensive.Besides, data diversity is crucial, as exposing LLMs to a broader spectrum of instructions enhances their ability to generalize across various downstream tasks.-Handling Longer Sequences.Scientific languages often comprise sequences that are significantly longer than typical natural language sentences.For instance, a protein sequence might contain hundreds or thousands of amino acids, while a natural language sentence usually has fewer than 30 words.This disparity in sequence length poses computational efficiency challenges for traditional Transformer-based architectures.-Autoregressive Learning Objective Limitations.Natural language is generally processed in an autoregressive manner.However, this is not the case with biological and chemical languages, which are not inherently created or interpreted autoregressively.In these domains, sequences function as a whole.Therefore, an ideal learning objective for a generative Sci-LLM should allow the model to capture semantic information from the entire sequence, considering bidirectional context, rather than relying solely on one-way information flow.-Ethical Considerations.The ethical considerations in the development and application of Sci-LLMs are multifaceted and critical.First, data privacy and consent are paramount, especially when dealing with sensitive biological data.Ensuring that genomic data, for instance, is anonymized and used with proper consent is vital to maintaining individual trust.Second, there is the risk of misuse of information, such as the potential for creating harmful biological substances or contributing to bioterrorism, necessitating stringent control measures.Bias in algorithmic decision-making also poses a significant ethical challenge.Sci-LLMs, like general LLM systems, can perpetuate and amplify biases present in their training data, leading to skewed outcomes in scientific research and applications.Last, there is an ethical imperative to ensure equitable access to the benefits of Sci-LLMs, preventing the exacerbation of existing inequalities in scientific research and healthcare.improving the understanding of single-modal language systems by Sci-LLMs but also in enabling them to tackle biological and chemical tasks from a more integrated, cross-modal perspective.This dual focus on expanding and refining training datasets is crucial for realizing the potential of larger and more effective Sci-LLMs in practical scientific research scenarios.To facilitate this, it is essential to establish automatic protocols for mining large-scale datasets from scientific repositories.-Incorporating 3D Stereo-Temporal Information into Scientific Language Systems.</p>
<p>The inherent nature of biological and chemical data is rooted in the three-dimensional physical world, where the functionalities are primarily determined by their constantly changing 3D structures.This highlights the urgent need to integrate 3D structural and temporal information into language-based modeling approaches.In addition to the recent structural tokens, it may be equally crucial to explore structural motifs.These motifs, akin to Lego building blocks in their ability to interlock and compose protein conformations, represent specific 3D structures that hold significant potential for modeling.By incorporating these structural motifs into the scientific lexicon, they can be learned and processed by Sci-LLMs in a manner similar to other tokens.To support this integration, research should focus on encoding 3D structures as learnable tokens within Sci-LLMs, potentially training models to predict conformations from textual descriptions.-Sci-LLM's Marriage with External Knowledge Sources.Scientists have accumulated a wealth of domain knowledge.For instance, biologists have developed the Gene Ontology, a comprehensive knowledge graph that systematically delineates the relationships between protein sequences and their functions.The potential synergy between Sci-LLMs and these external knowledge sources represents a substantial opportunity for enhancement.By integrating Sci-LLMs with such rich, structured external resources, their capabilities can be significantly expanded, overcoming the potential issues such as hallucination.To realize this potential, developing methods to incorporate knowledge graphs like Gene Ontology into Sci-LLMs is essential, augmenting training with embeddings for complex reasoning tasks.-Sci-LLM Interacting with Physical Simulation.Current Sci-LLMs are primarily adept at capturing the semantics inherent in sequence data.However, the realm of scientific understanding extends far beyond sequence data alone, encompassing a wealth of additional resources that can significantly contribute to a deeper comprehension.For instance, the field of Molecular Dynamics (MD) offers simulations that mirror the physical world, providing insights into molecular behavior and interactions.The interplay between Sci-LLMs and MD would not only complement the existing data-processing strengths of Sci-LLMs but also enrich their analytical power.To enhance this interaction, exploring hybrid models that combine data-driven learning with physical simulations will be beneficial.Moreover, using physical simulations to create augmented training data for rare molecular events can help evaluate models' generalization beyond real-world scenarios.-Augmenting Sci-LLMs with Specialized Tools and Agents.General LLMs have demonstrated remarkable proficiency in leveraging software and tools, such as calculators, to accomplish complex tasks.This capability is equally pertinent to Sci-LLMs, which can be significantly enhanced through integration with science-specific computational tools.A prime example of this is the recent success of GPT-4, which has shown proficiency in web searching, academic paper analysis, and designing experiments, including replicating intricate crosscoupling reactions like the Suzuki and Sonogashira experiments.-Super-alignment with Human Ethics.Alignment with human ethics is pivotal in the development of Sci-LLMs.Super-alignment, in this context, extends beyond basic adherence to ethical norms and encompasses a deep integration of ethical reasoning capabilities within</p>
<p>What is the solubility of this protein?</p>
<p>Fig. 1 .
1
Fig. 1.Illustrations that general LLMs struggle to effectively handle scientific languages, such as molecules, RNA, and amino acid sequences in this example.</p>
<p>Fig. 2 .
2
Fig.2.Research scopes of Scientific Large Language Models in this survey.We focus on scientific languages (i.e., textual, molecular, protein, and genomic languages), and their combination (i.e., multimodal language), within the biological and chemical domains.</p>
<p>Fig. 3 .
3
Fig. 3.An evolutionary tree of Sci-LLMs, which consists of five main branches corresponding to the research scopes in this survey.Due to the extensive number of Sci-LLMs, it is not feasible to include all of them in this figure, despite their exceptional quality.For detailed information on the featured models, please refer to Tables 1, 3, 5, 7, and 9. Considering the rapid development of Sci-LLMs, we will share the source file of this figure and encourage readers to make incremental updates at https://kdocs.cn/l/cbRA94QwMhmn.</p>
<ol>
<li>2
2
Future Directions -Construction of Larger-scale, High-quality, and Cross-modal Training Datasets.Expanding the pre-training dataset is contingent upon advancements in biological sequencing technologies and the open-sourcing of chemical molecular data.Concurrently, the responsibility of maintaining high-quality fine-tuning datasets, particularly in the cross-modal domain, falls to AI research teams.These enhanced datasets are instrumental in not only</li>
</ol>
<p>Table 1 .
1
Summary of Text-Sci-LLMs
161:7</p>
<p>Table 2 .
2
Summary of the Benchmarks for Text-Sci-LLMs
DatasetTimeSubset#Item DomainTypeCapabilityLanguageHigh-school-biology344BiologyPre-collegeMMLU [81]2020.09High-school-chemistry College-biology227 162Chemistry BiologyMultiple choicePre-college CollegeEnglishCollege-chemistry110ChemistryCollegeMid-school-biology218BiologyPre-collegeMid-school-chemistry210ChemistryPre-collegeC-Eval [90]2023.05High-school-biology199BiologyMultiple choicePre-collegeChineseHigh-school-chemistry196ChemistryPre-collegeCollege-chemistry253ChemistryCollegeAGIEval [284]2023.04Gaokao-biology Gaokao-chemistry210 207Biology ChemistryMultiple choicePre-college ChineseScienceQA [137]2022.09Natural-science-biology Natural-science-chemistry 1194 4098Biology ChemistryMultiple choice/QA Pre-college EnglishXieZhi [74]2023.06Science-biology Science-chemistry2831 399Biology ChemistryMultiple choiceMixedBothBasic-biology2142BiologyKnowledge-biology1369BiologyCalculation-biology299BiologySciEval [203]2023.08Research-biology995BiologyMultiple choice/QA MixedEnglishBasic-hemistry2909ChemistryKnowledge-chemistry1700ChemistryCalculation-chemistry3396ChemistryBioinfo-Bench-QA [34] 2023.10-150BiologyMultiple choicePost-collegeBLURB [73]2020.07-648kBiologyMultiple NLP tasks MixedPubMedQA [98]2019.09-273.2k BiologyTrue or falseCollegeEnglishARC [43]2018.03-7.78k Natural Science Multiple choicePre-collegeSciQ [232]2017.07-13.7k Natural Science Multiple choiceMixedallows for a more nuanced assessment of how well models can process, understand, and applyscientific knowledge across various domains.</p>
<p>Table 3 .
3
Summary of Mol-LLMs
ModelTime #ParametersBase modelPre-training DatasetCapabilityOpen-sourceSMILES-BERT [222]2019.09-BERTZINCProp. pred.ChemBERTa [38]2020.10-RoBERTaPubChemProp. pred.GROVER [178]2020.1048/100MTrans. enc.+GNNZINC-15, ChEMBLProp. pred.rxnfp-BERT [186]2021.01-BERTPistachio, USPTOReact. pred.MG-BERT [272]2021.05-BERT+GNNChEMBLProp. pred.AGBT [32]2021.06-BERTChEMBLProp. pred.Mol-BERT [115]2021.09-BERTZINC-15, ChEMBL 27Prop. pred.KPGT [113]2022.08100MTrans. enc.ChEMBL 29Prop. pred.ChemBERTa-2 [5]2022.095/46MRoBERTaPubChemProp. pred.Encoder-onlyMFBERT [1]2022.1088MRoBERTaGDB-13, ZINC-15, PubChem, ChEMBL, USPTOProp. pred.MolFormer [179]2022.12-Trans. enc.PubChem, ZINCProp. pred.MTL-BERT [273]2022.12-BERTChEMBLProp. pred.MolRoPE-BERT [131] 2023.01-BERTZINC-15, ChEMBL 27Prop. pred.×Molformer [235]2023.01-Trans. enc.ZINC, PubChemProp. pred.Uni-Mol [285]2023.0247.61MTrans. enc.ZINC, ChemBL PDBbindProp. pred. Conform. gen.SELFormer [258]2023.0558/87MRoBERTaChEMBLProp. pred.Semi-RoBERTa [214] 2023.07-RoBERTaPubChemProp. pred.×MolGPT [12]2021.056MGPTMOSES, GuacaMolMol. gen.SMILES GPT [4]2021.0913.4MGPTPubChemMol. gen., Prop. pred.Decoder-onlycTransformer [223] iupacGPT [150]2022.10 2023.05-1,500MTrans. dec. GPTMOSES PubChemMol. gen. Mol. gen., Prop. pred.×cMolGPT [226]2023.05-GPTMOSESMol. gen.Taiga [152]2023.05-GPTMOSES, ZINC, GDB 13Mol. gen., Prop. pred.Molecular-2019.0812MTransformerUSPTO, PistachioReact. pred.Transformer [185]Retrosynthesis-2019.09-TransformerUSPTOReact. pred.Transformer [102]SMILES-2019.11-TransformerChEMBL 24Prop. pred.Transformer [84]ChemReactNet [209] 2020.11-TransformerUSPTOReact. pred.X-MOL [249]2020.12-TransformerZINC-15Prop. pred., React. pred. Mol. optim., Mol. gen.GO-PRO [149]2021.035MTransformerUSPTOReact. pred.×Transmol [290]2021.07-TransformerMOSESMol. gen.RetroTRAE [215]2021.08-TransformerUSPTO, PubChem, ChEMBLReact. pred.Encoder-DecoderGCT [105] Chemformer [93]2021.12 2022.01-45/230MTransformer BARTMOSES ZINC-15Mol. gen. Prop. pred., React. pred.BARTSmiles [37]2022.11-BARTZINC 20Prop. pred.RetroSynth-2023.0212MTransformerUSPTO, PistachioReact. pred.Diversity [211]MOLGEN [60]2023.10-BARTZINC-15Mol. gen.</p>
<p>Table 4 .
4
Summary of the Datasets for Mol-LLMs
DatasetLast updatedSubset/VersionScaleKeywords/Tasks2015.10ZINC-15 [198]120MZINC2020.12ZINC-20 [92]140MLigand discovery2012.07ZINC-250k[91]250kCompound111.9MUnique chemical structuresSubstance296.9MChemical entitiesPubChem [106]2023.01BioAssay Bioactivity1.5M 296.8MBiological experiments Biological activity data pointsTaxonomy112.6KOrganisms of proteins/genesPatent42.4MPatents with links in PubChemUSPTO MIT480kUSPTO [136]2012.10USPTO-15K15kU.S. Patents, unique reactionsUSPTO-full950kPre-trainingPCQM4M2021.10 2021.06PCQM4Mv2 [88] PCQM4M-LSC [256]3.7M 3.8MQuantum property, molecular graphs pred. Large-ScaleGEOM [11]2022.04QM9 AICures drug dataset 317.9K 134KConformers Organic molecules, high-quality conformersToyMix154K Quantum ML, drug discovery, GNN expressivityMolTLU [15]2023.10LargeMix5.4MBiological propertiesUltraLarge83MQuantum propertiesChEMBL [259]2023.05-22.7MDrug-like bioactive compoundsDrugBank 5.0 [234]2017.11-500KDrug's mechanisms, interactions, targetsGDB-17 [180]2012.10-166.4BOrganic small moleculesExCAPE-DB [202]2017.03-70MBioactivity, ChemogenomicsESOL1kFreeSoly1KPhysical chemistry predictionLipophilicity4KPCBA437KMUV92KHIV41KBiophysics predictionMoleculeNet [237]2017.10PDBbind11KBACE1KBBBP2KBenchmarksTox217KToxCast8KPhysiology predictionSIDER1KClinTox1KDrugs-75K75kHighly flexible moleculesMARCEL [289]2023.09Kraken EE1.5k 872Organophosphorus ligand sterics Chiral catalysts and enantiomersBDE5.9kOrganometallic catalyst conformationsGuacaMol [26]2019.03--Molecule generationMOSES [170]2020.12ZINC Clean Leads1.9MMolecule generationADMETlab 2.0 [244]2021.04-250KProperty prediction</p>
<p>Table 5 .
5
Summary of Prot-LLMs
ModelTime #ParametersBase modelPretraining DatasetCapabilityOpen-sourceESM-1b [177]2020.02650MRoBERTaUniRef50Secondary struct. pred., Contact pred., etc.ESM-MSA-1b [176]2021.02100MESM-1bUniRef50Secondary struct. pred., Contact pred., etc.ESM-1v [154]2021.02650MESM-1bUniRef90Mutation effect pred.ProtTrans [58]2021.07-BERT, Albert, ElectraUniRef, BFDSecondary struct. pred., Func. pred., etcPMLM [79]2021.07 87M -731MTrans. enc.Uniref50/PfamContact pred.×ProteinBERTEncoder-only</p>
<p>Table 6 .
6
Summary of Datasets for Prot-LLMs
DatasetLast updatedScaleKeywords/TasksUniRef100/90/50 [205, 206]2023.11314M/150M/53MCollection of protein sequences from UniProtKBUniProtKB/Swiss-Prot [22] UniProtKB/TrEMBL [155]2023.11570K 251MHigh-quality, manually curated protein sequence database Computationally annotated protein sequence databaseUniParc [46]2023.11632MPretraining</p>
<p>Table 7 .
7
Summary of Gene-LLMs
ModelTime #ParametersBase ModelPretraining DatasetCapabilityData TypeOpen-SourceDNABERT [94]2021.02110MBERTGRCh38Func. pred.DNAEnformer [10]2021.10240MBERTGRCh38Func. pred.DNAMoDNA [8]2022.08-BERTGRCh38Func. pred.DNA×GPN [19]2022.08-ConvNCBI-GenomeVariant pred.DNAiEnhancer-BERT [140] 2022.08110MBERT-Enhancer. pred.DNA×iDNA-ABF [96]2022.10110MBERT-Func. pred.DNAiEnhancer-ELM [118] 2022.12110MBERT-Enhancer. pred.DNASA DNA-LM [68]2023.01-BERTEnsemblExpression. pred.DNANucleotide-Transformer [48]2023.01 50M-2.5BBERT1000 Genomes Project, etc.Func. pred.DNASpliceBERT [33]2023.0219.4MBERTUCSCSplicing pred.RNAEncoder-onlymiProBERT [224] RNA-MSM [275]2023.03 2023.03110M -BERT BERTEPDnew RNAcmapPromoter pred. Structure pred.RNA RNA×TFBert [141]2023.05-BERT690 ChIP-seqBinding pred.DNAMRM-BERT [274]2023.06110MBERT-Modif. pred.RNAGENA-LM [64]2023.06 110M-360MBERTGRCh38Func. pred. and profilingDNADNABERT-2 [288]2023.06117MBERTGRCh38Func. pred.DNAPLPMpro [120]2023.07-BERTEPDnewPromoter pred.DNA×EpiGePT [70]2023.07-BERTENCODEInterpret genomeDNAUni-RNA [225]2023.07-BERTNCBI-GenomeFunc. pred.DNA×GPN-MSA [17]2023.1086MBERT-Variant class. and pred.DNA×GET [67]2025.01-Trans.enc.scATAC-seqFunc. pred.DNAGenSLMs [292]2022.1025M-25BDiffusion+ GPTBV-BRCEvolution analysisRNADecoder-HyenaDNA [159]2023.06 0.44M-6.6MLongConvGRCh38Func. pred. and class.DNAonlyDNAGPT [266]2023.08100M-3BGPTGRCh38Func. pred. and generationDNABio-xLSTM [184]2024.12 0.5M-102MLSTMGRCh38Variant pred.DNA×ENBED [148]2023.11 580M-1.2BTransformerNCBI-GenomeFunc. pred. and analysisDNA×Encoder-DecodermegaDNA [189]2024.10145MTransformerNCBI-Genome GPDVariant pred. and generationDNACpGPT [50]2024.10 2.5M-65M Transformer++CpGCorpusFunc. pred.DNA×</p>
<p>Table 8 .
8
Summary of Datasets for Gene-LLMs
DatasetLast updated ScaleKeywordsPre-training</p>
<p>Table 9 .
9
Summary of MM-Sci-LLMs
ModelTime #ParametersBase ModelPretraining DatasetOpen-sourceText2Mol [57]2021.11117MGCN, SciBERTPubChem, ChEBIMoMuMol.&amp;Text</p>
<p>Table 10 .
10
Summary of Datasets for MM-Sci-LLMs
DatasetLast updatedScaleKeywordsChEBI-20 [57]2021.1133KCross-Lingual RetrievalMoMuMol.&amp;Text
ACM Comput. Surv., Vol. 57, No. 6, Article 161. Publication date: February 2025.
ACM Comput. Surv., Vol. 57, No. 6, Article 161. Publication date: February 2025. Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains
This work is supported by National Science and Technology Major Project (Grant No. 2023ZD0120802), National Key Research and Development Program of China (Grant No. 2022YFB4500300), National Natural Science Foundation of China (Grants No. 62302433, No. U23A20496, No. 62301480, No. U19B2027, and No. 91846204), Hangzhou West Lake Pearl Project Leading Innovative Youth Team Project (Grant No. TD2023017), Zhejiang Provincial "Jianbing" "Lingyan" Research and Development Program of China (Grant No. 2024C01135), Zhejiang Provincial Natural Science Foundation of China (Grant No. LQ24F020007), and CCF-Tencent Rhino-Bird Fund (Grant No. RAGR20230122).
Large-scale distributed training of transformers for chemical fingerprinting. Hisham Abdel, -Aty , Ian R Gould, J. Chem. Inf. Model. 622022. 2022</p>
<p>Prot2text: Multimodal protein's function generation with gnns and transformers. Michail Hadi Abdine, Costas Chatzianastasis, Michalis Bouyioukos, Vazirgiannis, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat et al. 2023. Gpt-4 technical report. </p>
<p>Generative pre-training from molecules. Sanjar Adilov, 2021</p>
<p>Chemberta-2: Towards chemical foundation models. Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar, 2022</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The impact of large language models on scientific discovery: A preliminary study using gpt-4. </p>
<p>BioM-transformers: Building large biomedical language models with BERT, ALBERT, and ELECTRA. Sultan Alrowili, K Vijay-Shanker, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language Processing2021</p>
<p>MoDNA: Motiforiented pre-training for DNA language model. Yuzhi Weizhi An, Yatao Guo, Hehuan Bian, Jinyu Ma, Chunyuan Yang, Junzhou Li, Huang, Proceedings of the 13th ACM International Conference on Bioinformatics. the 13th ACM International Conference on Bioinformatics2022</p>
<p>Gene ontology: Tool for the unification of biology. Michael Ashburner, Catherine A Ball, Judith A Blake, David Botstein, Heather Butler, Michael Cherry, Allan P Davis, Kara Dolinski, Selina S Dwight, Janan T Eppig, Nature Genet. 252000. 2000</p>
<p>Effective gene expression prediction from sequence by integrating long-range interactions. Žiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R Kelley, Nature Methods. 182021. 2021</p>
<p>GEOM, energy-annotated molecular conformations for property prediction and molecular generation. Simon Axelrod, Rafael Gomez-Bombarelli, Sci. Data. 91852022. 2022</p>
<p>MolGPT: Molecular generation using a transformer-decoder model. Viraj Bagal, P K Aggarwal, U Deva Vinod, Priyakumar, J. Chem. Inf. Model. 622021. 2021</p>
<p>The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000. Amos Bairoch, Rolf Apweiler, Nucl. Acids Res. 282000. 2000</p>
<p>GPT-MolBERTa: GPT molecular features language model for molecular property prediction. Suryanarayanan Balaji, Rishikesh Magar, Yayati Jadhav, 2023</p>
<p>Publication date: February 2025. foundational models for molecular learning on large-scale multi-task datasets. Dominique Beaini, Shenyang Huang, Joao Alex Cunha, Gabriela Moisescu-Pareja, Oleksandr Dymov, Samuel Maddrell-Mander, Callum Mclean, Frederik Wenkel, Luis Müller, Jama Hussein Mohamud, Towards ACM Comput. Surv. 5762023</p>
<p>SciBERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 2019</p>
<p>GPN-MSA: An alignment-based DNA language model for genome-wide variant effect prediction. Gonzalo Benegas, Carlos Albors, Alan J Aw, Chengzhong Ye, Yun S Song, 10.1101/2023.10.10.561776v12023</p>
<p>DNA language models are powerful zero-shot predictors of genome-wide variant effects. Gonzalo Benegas, Sanjit Singh Batra, Yun S Song, 10.1101/2022.08.22.504706v12022</p>
<p>DNA language models are powerful predictors of genome-wide variant effects. Gonzalo Benegas, Sanjit Singh Batra, Yun S Song, Proc. Natl. Acad. Sci. U.S.A. 120e23112191202023. 2023</p>
<p>Learning the protein language: Evolution, structure, and function. Tristan Bepler, Bonnie Berger, Cell Syst. 122021. 2021</p>
<p>Translating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, Adv. Neural Inf. Process. Syst. 262013. 2013</p>
<p>UniProtKB/Swiss-Prot, the manually annotated section of the UniProt KnowledgeBase: How to use the entry view. Emmanuel Boutet, Damien Lieberherr, Michael Tognolli, Michel Schneider, Parit Bansal, Alan J Bridge, Sylvain Poux, Lydie Bougueleret, Ioannis Xenarios, Methods Protocols. 2016. 2016</p>
<p>Chem-Crow: Augmenting large-language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, 2023</p>
<p>Transformers and large language models for chemistry and drug discovery. Andres M Bran, Philippe Schwaller, Drug Development Supported by Informatics. Springer2024</p>
<p>ProteinBERT: A universal deeplearning model of protein sequence and function. Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial, Bioinformatics. 382022. 2022</p>
<p>GuacaMol: Benchmarking models for de novo molecular design. Nathan Brown, Marco Fiscato, Marwin H S Segler, Alain C Vaucher, J. Chem. Inf. Model. 592019. 2019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Adv. Neural Inf. Process. Syst. 332020. 2020</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Uni-SMART: Universal science multimodal analysis and research transformer. Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao, Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, https://arXiv:2403.103012024</p>
<p>BRENDA, the ELIXIR core data resource in 2021: New developments and updates. Antje Chang, Lisa Jeske, Sandra Ulbrich, Julia Hofmann, Julia Koblitz, Ida Schomburg, Meina Neumann-Schaal, Dieter Jahn, Dietmar Schomburg, Nucl. Acids Res. 492021. 2021</p>
<p>xTrimoPGLM: Unified 100B-scale pre-trained transformer for deciphering the language of protein. Bo Chen, Xingyi Cheng, Pan Li, Yangli-Ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu Tan, Boyan Wang, Xin Zeng, 2024</p>
<p>Algebraic graphassisted bidirectional transformers for molecular property prediction. Dong Chen, Kaifu Gao, Duc Duy Nguyen, Xin Chen, Yi Jiang, Guo-Wei Wei, Feng Pan, Nature Commun. 1235212021. 2021</p>
<p>Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction. Ken Chen, Yue Zhou, Maolin Ding, Yu Wang, Zhixiang Ren, Yuedong Yang, 10.1101/2023.01.31.526427v12023</p>
<p>Bioinfo-Bench: A simple benchmark framework for LLM bioinformatics skills evaluation. Qiyuan Chen, Cheng Deng, 10.1101/2023.10.18.563023v12023</p>
<p>Deep generative model for drug design from protein target sequence. Yangyang Chen, Zixu Wang, Lei Wang, Jianmin Wang, Pengyong Li, Dongsheng Cao, Xiangxiang Zeng, Xiucai Ye, Tetsuya Sakurai, J. Cheminform. 15382023. 2023</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. 14 April 2023</p>
<p>Bartsmiles: Generative masked language models for molecular representations. Hovhannes Gayane Chilingaryan, Ani Tamoyan, Nelly Tevosyan, Lusine Babayan, Karen Khondkaryan, Zaven Hambardzumyan, Hrant Navoyan, Armen Khachatrian, Aghajanyan, 2022</p>
<p>ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction. Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar, 2020</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, J. Mach. Learn. Res. 242023. 2023</p>
<p>Unifying molecular and textual representations via multi-task language modelling. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, Matteo Manica, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPMLR2023</p>
<p>Generative antibody design for complementary chain pairing sequences through encoder-decoder language model. K S Simon, Kathy Y Chu, Wei, 2023</p>
<p>Electra: Pre-training text encoders as discriminators rather than generators. Kevin Clark, Minh-Thang Luong, Quoc V Le, Christopher D Manning, 2020</p>
<p>Think you have solved question answering? Try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, 2018</p>
<p>To transformers and beyond: Large language models for the genome. Micaela E Consens, Cameron Dufault, Michael Wainberg, Duncan Forster, Mehran Karimzadeh, Hani Goodarzi, Fabian J Theis, Alan Moses, Bo Wang, 2023</p>
<p>A global reference for human genetic variation. Genomes Project, Consortium , Nature. 526682015. 2015</p>
<p>UniProt: The universal protein knowledgebase in 2023. Nucl. Acids Res. 512023. 2023</p>
<p>ENCODE Project Consortium Overall coordination (data analysis coordination) Dunham Ian 2 Kundaje Anshul 3 81 82 82, Writing group Bernstein Bradley E. 7 34 Birney Ewan Dunham Ian Green Eric D. 35 Gunter Chris 15 Snyder Michael 13 et al. 2012. An integrated encyclopedia of DNA elements in the human genome. Nature. 4892012</p>
<p>The nucleotide transformer: Building and evaluating robust foundation models for human genomics. Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, P Bernardo, Hassan De Almeida, Sirelkhatim, 2023</p>
<p>FLIP: Benchmark tasks in fitness landscape inference for proteins. Christian Dallago, Jody Mou, Kadina E Johnston, Bruce J Wittmann, Nicholas Bhattacharya, Samuel Goldman, Ali Madani, Kevin K Yang, 2021</p>
<p>CpGPT: A foundation model for DNA methylation. Lucas Paulo De Lima Camillo, Raghav Sehgal, Jenel Armstrong, Albert Tzongyang Higgins-Chen, Steve Horvath, Bo Wang, 10.1101/2024.10.24.619766v12024</p>
<p>Chemical language model linker: Blending text and molecules with modular adapters. Yifan Deng, Spencer S Ericksen, Anthony Gitter, https://arXiv:2410.201822024</p>
<p>EPD and EPDnew, high-quality promoter resources in the next-generation sequencing era. René Dreos, Giovanna Ambrosini, Rouayda Cavin Périer, Philipp Bucher, Nucl. Acids Res. 412013. 2013</p>
<p>Molgensurvey: A systematic survey in machine learning models for molecule design. Yuanqi Du, Tianfan Fu, Jimeng Sun, Shengchao Liu, 2022</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, 2021</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, 2022</p>
<p>MolCap-Arena: A comprehensive captioning benchmark on language-enhanced molecular property prediction. Carl Edwards, Ziqing Lu, Ehsan Hajiramezanali, Tommaso Biancalani, Ji Heng, Gabriele Scalia, https://arXiv:2411.007372024</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2021</p>
<p>ProtTrans: Toward understanding the language of life through self-supervised learning. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, IEEE Trans. Pattern Anal. Mach. Intell. 442022. 2022</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen, 2023</p>
<p>Domain-agnostic molecular generation with self-feedback. Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, Huajun Chen, 2023</p>
<p>SciKnowEval: Evaluating multi-level scientific knowledge of large language models. Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen, 2024</p>
<p>A deep unsupervised language model for protein design. Noelia Ferruz, Steffen Schmidt, Birte Höcker, 2022</p>
<p>Pfam: Clans, web tools and services. Robert D Finn, Jaina Mistry, Benjamin Schuster-Böckler, Sam Griffiths-Jones, Volker Hollich, Timo Lassmann, Simon Moxon, Mhairi Marshall, Ajay Khanna, Richard Durbin, Nucl. Acids Res. 342006. 2006</p>
<p>GENA-LM: A family of open-source foundational models for long DNA sequences. Veniamin Fishman, Yuri Kuratov, Maxim Petrov, Aleksei Shmelev, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, Mikhail Burtsev, 2023</p>
<p>Bloom's taxonomy. Mary Forehand, Emerg. Perspect. Learn. Teach. Technol. 412010. 2010</p>
<p>PanglaoDB: A web server for exploration of mouse and human single-cell RNA sequencing data. Oscar Franzén, Li-Ming Gan, Johan L M Björkegren, Database. 2019462019. 2019</p>
<p>A foundation model of transcription across human cell types. Xi Fu, Shentong Mo, Alejandro Buendia, Anouchka P Laurent, Anqi Shao, Maria Del, Mar Alvarez-Torres, Tianji Yu, Jimin Tan, Jiayu Su, Romella Sagatelian, Nature. 2025. 2025</p>
<p>Species-aware DNA language modeling. Dennis Gankin, Alexander Karollus, Martin Grosshauser, Kristian Klemon, Johannes Hingerl, Julien Gagneur, 2023</p>
<p>DrugCLIP: Contrasive protein-molecule representation learning for virtual screening. Bowen Gao, Bo Qiang, Haichuan Tan, Yinjun Jia, Minsi Ren, Minsi Lu, Jingjing Liu, Wei-Ying Ma, Yanyan Lan, Adv. Neural Inf. Process. Syst. 362024. 2024</p>
<p>EpiGePT: A pretrained transformer model for epigenomics. Zijing Gao, Qiao Liu, Wanwen Zeng, H Wing, Rui Wong, Jiang, https://bioRxiv:2023.07.15.5491342023</p>
<p>Nomenclature and symbolism for amino acids and peptides. H Bielka, N Sharon, E W Australia, Pure Appl. Chem. 561984. 1984</p>
<p>BindingDB in 2015: A public database for medicinal chemistry, computational chemistry and systems pharmacology. K Michael, Tiqing Gilson, Michael Liu, George Baitaluk, Linda Nicola, Jenny Hwang, Chong, Nucl. Acids Res. 442016. 2016</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, ACM Trans. Comput. Health. 32021. 2021</p>
<p>Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Automated chemical reaction extraction from scientific literature. A Jiang Guo, Hanyu Santiago Ibanez-Lopez, Victor Gao, Connor W Quach, Klavs F Coley, Regina Jensen, Barzilay, J. Chem. Inf. Model. 622021. 2021</p>
<p>ProtDAT: A unified framework for protein sequence design from any protein text description. Xiao-Yu Guo, Yi-Fan Li, Yuan Liu, Xiaoyong Pan, Hong-Bin Shen, https://arXiv:2412.040692024</p>
<p>Improvements and impacts of GRCh38 human reference on high throughput sequencing data analysis. Yan Guo, Yulin Dai, Hui Yu, Shilin Zhao, David C Samuels, Yu Shyr, Genomics. 1092017. 2017</p>
<p>MatSciBERT: A materials domain language model for text mining and information extraction. Tanishq Gupta, Mohd Zaki, N M Anoop Krishnan, Mausam , npj Comput. Mater. 81022022. 2022</p>
<p>Pan Deng et al. 2021. Pre-training co-evolutionary protein representation via a pairwise masked language model. Liang He, Shizhuo Zhang, Lijun Wu, Huanhuan Xia, Fusong Ju, He Zhang, Siyuan Liu, Yingce Xia, Jianwei Zhu, </p>
<p>ProstT5: Bilingual language model for protein sequence and structure. Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Martin Steinegger, Burkhard Rost, 2023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2020</p>
<p>Rita: A study on scaling up generative protein sequence models. Daniel Hesslow, Niccoló Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks, 2022</p>
<p>Viral genome DataBase: Storing and analyzing genes and proteins from complete viral genomes. David Hiscock, Chris Upton, Bioinformatics. 162000. 2000</p>
<p>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. Shion Honda, Shoi Shi, Hiroki R Ueda, 2019</p>
<p>The diminishing returns of masked language models to science. Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Kyle Chard, Ian Foster, 2022</p>
<p>A systematic benchmark of machine learning methods for protein-RNA interaction prediction. Marc Horlacher, Giulia Cantini, Julian Hesse, Patrick Schinke, Nicolas Goedert, Shubhankar Londhe, Lambert Moyon, Annalisa Marsico, Brief. Bioinform. 243072023. 2023</p>
<p>Protein language models and structure prediction: Connection and progression. Bozhen Hu, Jun Xia, Jiangbin Zheng, Cheng Tan, Yufei Huang, Yongjie Xu, Stan Z Li, 2022</p>
<p>Ogb-lsc: A large-scale challenge for machine learning on graphs. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec, 2021</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Adv. Neural Inf. Process. Syst. 332020. 2020</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Adv. Neural Inf. Process. Syst. 362024. 2024</p>
<p>ZINC: A free tool to discover chemistry for biology. John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, Ryan G Coleman, J. Chem. Inf. Model. 522012. 2012</p>
<p>ZINC20-a free ultralarge-scale chemical database for ligand discovery. John J Irwin, G Khanh, Jennifer Tang, Chinzorig Young, Benjamin R Dandarchuluun, Munkhzul Wong, Yurii S Khurelbaatar, John Moroz, Roger A Mayfield, Sayle, J. Chem. Inf. Model. 602020. 2020</p>
<p>Chemformer: A pre-trained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum, Mach. Learn.: Sci. Technol. 3150222022. 2022</p>
<p>DNABERT: Pre-trained bidirectional encoder representations from transformers model for DNA-language in genome. Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri, Bioinformatics. 372021. 2021</p>
<p>Protein structure-structure alignment with discrete Fréchet distance. Minghui Jiang, Ying Xu, Binhai Zhu, J. Bioinform. Comput. Biol. 62008. 2008</p>
<p>iDNA-ABF: Multi-scale deep biological language learning model for the interpretable prediction of DNA methylations. Junru Jin, Yingying Yu, Ruheng Wang, Xin Zeng, Chao Pang, Yi Jiang, Zhongshen Li, Yutong Dai, Ran Su, Quan Zou, Genome Biol. 232022. 2022</p>
<p>ProLLM: Protein chain-of-thoughts enhanced LLM for protein-protein interaction prediction. Mingyu Jin, Haochen Xue, Zhenting Wang, Boming Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du, Yongfeng Zhang, 2024</p>
<p>Pubmedqa: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu, 2019</p>
<p>Highly accurate protein structure prediction with AlphaFold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Nature. 5962021. 2021</p>
<p>Does your model understand genes? A benchmark of gene properties for biological and text models. Yoav Kan-Tor, Michael Morris Danziger, Eden Zohar, Matan Ninio, Yishai Shimoni, 2024</p>
<p>The UCSC genome browser database. Donna Karolchik, Robert Baertsch, Mark Diekhans, Terrence S Furey, Angie Hinrichs, Krishna M Lu, Matt Roskin, Charles W Schwartz, Daryl J Sugnet, Thomas, Nucl. Acids Res. 312003. 2003</p>
<p>A transformer model for retrosynthesis. Pavel Karpov, Guillaume Godin, Igor V Tetko, Proceedings of the International Conference on Artificial Neural Networks. the International Conference on Artificial Neural NetworksSpringer2019</p>
<p>CAGI5: Objective performance assessments of predictions based on the Evolutionary Action equation. Panagiotis Katsonis, Olivier Lichtarge, Hum. Mutat. 402019. 2019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT'19). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT'19)Minneapolis2019. Minnesota, 21</p>
<p>Generative chemical transformer: Neural machine learning of molecular geometric structures from chemical language via attention. Hyunseung Kim, Jonggeol Na, Won Bo, Lee , J. Chem. Inf. Model. 612021. 2021</p>
<p>PubChem 2023 update. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucl. Acids Res. 512023. 2023</p>
<p>A revision of Bloom's taxonomy: An overview. David R Krathwohl, Theory Pract. 412002. 2002</p>
<p>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Pascal Friederich, Alan Aspuru-Guzik, Mach. Learn.: Sci. Technol. 1450242020. 2020</p>
<p>The arabidopsis information resource (TAIR): Improved gene annotation and new tools. Philippe Lamesch, Tanya Z Berardini, Donghui Li, David Swarbreck, Christopher Wilks, Rajkumar Sasidharan, Robert Muller, Kate Dreher, Debbie L Alexander, Margarita Garcia-Hernandez, Nucl. Acids Res. 402012. 2012</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, 2019</p>
<p>BioBERT: A pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 362020. 2020</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, 2019Bart</p>
<p>KPGT: Knowledge-guided pre-training of graph transformer for molecular property prediction. Han Li, Dan Zhao, Jianyang Zeng, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>BioSeq-BLM: A platform for analyzing DNA, RNA and protein sequences based on biological language models. Hong-Liang Li, Yi-He Pang, Bin Liu, Nucl. Acids Res. 492021. 2021</p>
<p>Mol-BERT: An effective molecular representation with BERT for molecular property prediction. Juncai Li, Xiaofei Jiang, Wireless Commun. Mobile Comput. 20212021. 2021</p>
<p>BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, 2023</p>
<p>TOMG-Bench: Evaluating LLMs on textbased open molecule generation. Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li, https://arXiv:2412.146422024</p>
<p>iEnhancer-ELM: Improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models. Jiahao Li, Zhourun Wu, Wenhao Lin, Jiawei Luo, Jun Zhang, Qingcai Chen, Junjie Chen, Bioinform. Adv. 3432023. 2023</p>
<p>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. Yuesen Li, Chengyi Gao, Xin Song, Xiangyu Wang, Yungang Xu, Suxia Han, 2023</p>
<p>PLPMpro: Enhancing promoter sequence prediction with prompt-learning based pre-trained language model. Zhongshen Li, Junru Jin, Wentao Long, Leyi Wei, Comput. Biol. Med. 1641072602023. 2023</p>
<p>DrugChat: Towards enabling ChatGPT-like capabilities on drug molecule graphs. Youwei Liang, Ruiyi Zhang, Li Zhang, Pengtao Xie, 2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. BarcelonaAssociation for Computational Linguistics2004</p>
<p>Language models of protein sequences at the scale of evolution enable accurate structure prediction. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan Dos, Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, 10.1101/2022.07.20.500902v12022</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Pengfei Liu, Yiming Ren, Comput. Biol. Med. 171108073Jun Tao, and Zhixiang Ren. 2024. 2024</p>
<p>Arvind Ramanathan, Chaowei Xiao et al. 2023. A text-guided protein design framework. Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, </p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Animashree Anandkumar, Nature Mach. Intell. 52023. 2023</p>
<p>Pre-training molecular graph representation with 3D geometry. Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang, 2021</p>
<p>Chatgpt-powered conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, 2023</p>
<p>MolecularGPT: Open large language model (LLM) for few-shot molecular property prediction. Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan, https://arXiv:2406.129502024</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019</p>
<p>MolRoPE-BERT: An enhanced molecular representation with rotary position embedding for molecular property prediction. Yunwu Liu, Ruisheng Zhang, Tongfeng Li, Jing Jiang, Jun Ma, Ping Wang, J. Mol. Graph. Model. 1181083442023. 2023</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>ProtT3: Protein-to-text generation for text-based protein understanding. Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua, https://arXiv:2405.125642024</p>
<p>MolXPT: Wrapping molecules with text for generative pre-training. Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, Tie-Yan Liu, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Dan S Weld, 2019</p>
<p>Extraction of chemical structures and reactions from the literature. Ph.D. Dissertation, University of Cambridge. Daniel Mark, Lowe , 2012</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Adv. Neural Inf. Process. Syst. 352022. 2022</p>
<p>MoleculeQA: A dataset to evaluate factual accuracy in molecular comprehension. Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, Yu Li, https://arXiv:2403.081922024</p>
<p>Xiaoping Bao, and Can Li. 2024. scChat: A large language model-powered co-pilot for contextualized single-cell RNA sequencing analysis. Yen-Chun Lu, Ashley Varghese, Rahul Nahar, Hao Chen, Kunming Shao, 10.1101/2024.10.01.616063v1</p>
<p>iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their strength. Hanyu Luo, Cheng Chen, Wenyu Shan, Pingjian Ding, Lingyun Luo, Proceedings of the International Conference on Intelligent Computing. the International Conference on Intelligent ComputingSpringer2022</p>
<p>Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training. Hanyu Luo, Wenyu Shan, Cheng Chen, Pingjian Ding, Lingyun Luo, Interdisc. Sci.: Comput. Life Sci. 152023. 2023</p>
<p>BioGPT: Generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Brief. Bioinform. 234092022. 2022</p>
<p>Molfm: A multimodal molecular foundation model. Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zaiqing Nie, 2023</p>
<p>Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, Zaiqing Nie, 2023</p>
<p>ProLLaMA: A protein large language model for multi-task protein language processing. Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian, https://arXiv:2402.164452024</p>
<p>Retrieved sequence augmentation for protein representation learning. Chang Ma, Haiteng Zhao, Lin Zheng, Jiayi Xin, Qintong Li, Lijun Wu, Zhihong Deng, Yang Lu, Qi Liu, Lingpeng Kong, 2023</p>
<p>Progen: Language modeling for protein generation. Ali Madani, Bryan Mccann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, Richard Socher, 2020</p>
<p>Understanding the natural language of DNA using encoder-decoder foundation models with byte-level precision. Aditya Malusare, Harish Kothandaraman, Dipesh Tamboli, Nadia A Lanman, Vaneet Aggarwal, 2023</p>
<p>Predicting chemical reaction outcomes: A grammar ontologybased transformer framework. Vipul Mann, Venkat Venkatasubramanian, AIChE J. 67e171902021. 2021</p>
<p>iupacGPT: IUPAC-based large-scale molecular pre-trained model for property prediction and molecule generation. Jiashun Mao, Jianmin Wang, Kwang-Hwi Cho, Kyoung Tai, No , 2023</p>
<p>Molecule attention transformer. Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, Stanisław Jastrzębski, 2020</p>
<p>Molecule generation using transformers and policy gradient reinforcement learning. Eyal Mazuz, Guy Shtar, Bracha Shapira, Lior Rokach, Sci. Rep. 1387992023. 2023</p>
<p>ExplorEnz: The primary source of the IUBMB enzyme list. Andrew G Mcdonald, Sinéad Boyce, Keith F Tipton, Nucl. Acids Res. 372009. 2009</p>
<p>Language models enable zeroshot prediction of the effects of mutations on protein function. Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, Alex Rives, Adv. Neural Inf. Process. Syst. 342021. 2021</p>
<p>EDITtoTrEMBL: A distributed approach to high-quality automated protein sequence annotation. S Moller, Ulf Leser, Wolfgang Fleischmann, Rolf Apweiler, Bioinformatics. 151999. 1999</p>
<p>ZymCTRL: A conditional language model for the controllable generation of artificial enzymes. Geraldene Munsamy, Sebastian Lindner, Philipp Lorenz, Noelia Ferruz, Proceedings of the Machine Learning for Structural Biology Workshop. the Machine Learning for Structural Biology WorkshopNeurIPS2022. 2022</p>
<p>Directory of useful decoys, enhanced (DUD-E): Better ligands and decoys for better benchmarking. M Michael, Michael Mysinger, John J Carchia, Brian K Irwin, Shoichet, J. Med. Chem. 552012. 2012</p>
<p>Sequence modeling and design from molecular to genome scale with Evo. Eric Nguyen, Michael Poli, Matthew G Durrant, Armin W Thomas, Brian Kang, Jeremy Sullivan, Madelena Y Ng, Ashley Lewis, Aman Patel, Aaron Lou, 10.1126/science.ado93362024</p>
<p>Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, Adv. Neural Inf. Process. Syst. 362024. 2024</p>
<p>ProGen2: Exploring the boundaries of protein language models. Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, Ali Madani, Cell Syst. 142023. 2023</p>
<p>Tranception: Protein fitness prediction with autoregressive transformers and inference-time retrieval. Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan N Gomez, Debora Marks, Yarin Gal, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPMLR2022</p>
<p>ProteinGym: Large-scale benchmarks for protein design and fitness prediction. Pascal Notin, Aaron W Kollasch, Daniel Ritter, Lood Van Niekerk, Steffanie Paul, Hansen Spinner, Nathan Rollins, Ada Shaw, Ruben Weitzman, Jonathan Frazer, 2023</p>
<p>ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Pascal Notin, Ruben Weitzman, Debora S Marks, Yarin Gal, 2023</p>
<p>Introducing the bacterial and viral bioinformatics resource center (BV-BRC): A resource combining PATRIC, IRD and ViPR. Robert D Olson, Rida Assaf, Thomas Brettin, Neal Conrad, Clark Cucinell, James J Davis, Donald M Dempsey, Allan Dickerman, Emily M Dietrich, Ronald W Kenyon, Nucl. Acids Res. 512023. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Adv. Neural Inf. Process. Syst. 352022. 2022</p>
<p>Bleu: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational Linguistics2002</p>
<p>Biot5+: Towards generalized biological understanding with iupac integration and multi-task tuning. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan, https://arXiv:2402.178102024</p>
<p>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, 2023</p>
<p>An empirical study of multi-task learning on BERT for biomedical text mining. Yifan Peng, Qingyu Chen, Zhiyong Lu, 2020</p>
<p>Molecular sets (MOSES): A benchmarking platform for molecular generation models. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Front. Pharmacol. 115656442020. 2020</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 192019. 2019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 212020. 2020</p>
<p>Few shot protein generation. Soumya Ram, Tristan Bepler, 2022</p>
<p>Evaluating protein transfer learning with TAPE. Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, Yun Song, Adv. Neural Inf. Process. Syst. 322019. 2019</p>
<p>MSA transformer. M Roshan, Jason Rao, Robert Liu, Joshua Verkuil, John Meier, Pieter Canny, Tom Abbeel, Alexander Sercu, Rives, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPMLR2021</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, Proc. Natl. Acad. Sci. U.S.A. 118e20162391182021. 2021</p>
<p>Selfsupervised graph transformer on large-scale molecular data. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, Junzhou Huang, Adv. Neural Inf. Process. Syst. 332020. 2020</p>
<p>Largescale chemical language representations capture molecular structure and properties. Jerret Ross, Brian Belgodere, Inkit Vijil Chenthamarakshan, Youssef Padhi, Payel Mroueh, Das, Nature Mach. Intell. 42022. 2022</p>
<p>Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17. Lars Ruddigkeit, Ruud Van Deursen, C Lorenz, Jean-Louis Blum, Reymond, J. Chem. Inf. Model. 522012. 2012</p>
<p>Sequence-based prediction of variants' effects. Nicole Rusk, Nature Methods. 152018. 2018</p>
<p>HIPPIE: Integrating protein interaction networks with experiment based quality scores. Martin H Schaefer, Jean-Fred Fontaine, Arunachalam Vinayagam, Pablo Porras, Erich E Wanker, Miguel A Andrade-Navarro, PloS One. 72e318262012. 2012</p>
<p>Modeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, Proceedings of the 15th International Conference on the Semantic Web (ESWC'18). the 15th International Conference on the Semantic Web (ESWC'18)Springer2018</p>
<p>Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences. Niklas Schmidinger, Lisa Schneckenreiter, Philipp Seidl, Johannes Schimunek, Pieter-Jan Hoedt, Johannes Brandstetter, Andreas Mayr, Sohvi Luukkonen, Sepp Hochreiter, Günter Klambauer, 2024</p>
<p>Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, Alpha A Lee, ACS Central Sci. 52019. 2019</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. Philippe Schwaller, Daniel Probst, Alain C Vaucher, H Vishnu, David Nair, Teodoro Kreutter, Jean-Louis Laino, Reymond, Nature Mach. Intell. 32021. 2021</p>
<p>Enhancing activity prediction models in drug discovery with the ability to understand human language. Philipp Seidl, Andreu Vall, Sepp Hochreiter, Günter Klambauer, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPMLR2023</p>
<p>Talking about large language models. Murray Shanahan, Commun. ACM. 672024. 2024</p>
<p>A long-context language model for deciphering and generating bacteriophage genomes. Bin Shao, Jiawei Yan, Nature Commun. 1593922024. 2024</p>
<p>A fine-tuning dataset and benchmark for large language models for protein understanding. Yiqing Shen, Zan Chen, Michail Mamalakis, Luhan He, Haiyang Xia, Tianbin Li, Yanzhou Su, Junjun He, Yu Guang, Wang , https://arXiv:2406.055402024</p>
<p>Toursynbio: A multi-modal large model and agent framework to bridge text and protein sequences for protein engineering. Yiqing Shen, Zan Chen, Michail Mamalakis, Yungeng Liu, Tianbin Li, Yanzhou Su, Junjun He, Pietro Liò, Yu Guang, Wang , https://arXiv:2408.152992024</p>
<p>A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. Pranav Shetty, Arunkumar Chitteth Rajan, Chris Kuenneth, Sonakshi Gupta, Lakshmi Prerana Panchumarti, Lauren Holm, Chao Zhang, Rampi Ramprasad, Comput. Mater. 9522023. 2023</p>
<p>BioMegatron: Larger biomedical domain language model. Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Ido Amos et al. 2024. MAMMAL-molecular aligned multi-modal architecture and language. Yoel Shoshan, Moshiko Raboh, Michal Ozery-Flato, Vadim Ratner, Alex Golts, Jeffrey K Weber, Ella Barkan, Simona Rabinovici-Cohen, Sagi Polaczek, https://arXiv:2410.22367</p>
<p>Generative language modeling for antibody design. Richard W Shuai, Jeffrey A Ruffolo, Jeffrey J Gray, 10.1101/2021.12.13.472419v22021</p>
<p>Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold. Martin Steinegger, Milot Mirdita, Johannes Söding, Nature Methods. 162019. 2019</p>
<p>Clustering huge protein sequence sets in linear time. Martin Steinegger, Johannes Söding, Nature Commun. 925422018. 2018</p>
<p>ZINC 15-ligand discovery for everyone. Teague Sterling, John J Irwin, J. Chem. Inf. Model. 2015. 2015</p>
<p>The mammalian gene collection. Robert L Strausberg, Elise A Feingold, Richard D Klausner, Francis S Collins, Science. 2861999. 1999</p>
<p>A molecular multimodal foundation model associating molecule graphs with natural language. Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen, 2022</p>
<p>Saprot: Protein language modeling with structure-aware vocabulary. Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan, 2023</p>
<p>ExCAPE-DB: An integrated large scale dataset facilitating Big Data analysis in chemogenomics. Jiangming Sun, Nina Jeliazkova, Vladimir Chupakhin, Jose-Felipe Golib-Dzib, Ola Engkvist, Lars Carlsson, Jörg Wegner, Hugo Ceulemans, Ivan Georgiev, Vedrin Jeliazkov, J. Cheminform. 92017. 2017</p>
<p>Scieval: A multilevel large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, Kai Yu, https://arXiv:2409.18412SciDFM: A large language model with mixture-of-experts for science. 2024</p>
<p>UniRef: Comprehensive and non-redundant UniProt reference clusters. E Baris, Hongzhan Suzek, Peter Huang, Raja Mcgarvey, Cathy H Mazumder, Wu, Bioinformatics. 232007. 2007</p>
<p>UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches. E Baris, UniProt ConsortiumYuqi Suzek, UniProt ConsortiumHongzhan Wang, UniProt ConsortiumPeter B Huang, UniProt ConsortiumCathy H Mcgarvey, UniProt ConsortiumWu, UniProt ConsortiumBioinformatics. 312015. 2015</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, Galactica: A large language model for science. 2022</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, 2023</p>
<p>State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis. Igor V Tetko, Pavel Karpov, Ruud Van Deursen, Guillaume Godin, Nature Commun. 1155752020. 2020</p>
<p>Unbiasing retrosynthesis language models with disconnection prompts. Amol Thakkar, Alain C Vaucher, Andrea Byekwaso, Philippe Schwaller, Alessandra Toniato, Teodoro Laino, ACS Central Sci. 92023. 2023</p>
<p>Enhancing diversity in language based models for single-step retrosynthesis. Alessandra Toniato, Alain C Vaucher, Philippe Schwaller, Teodoro Laino, Dig. Discov. 22023. 2023</p>
<p>Faisal Azhar et al. 2023. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, </p>
<p>Survey of protein sequence embedding models. Chau Tran, Siddharth Khadkikar, Aleksey Porollo, Int. J. Mol. Sci. 2437752023. 2023</p>
<p>Molecular descriptors property prediction using transformer-based approach. Tuan Tran, Chinwe Ekenna, Int. J. Mol. Sci. 24119482023. 2023</p>
<p>Retrosynthetic reaction pathway prediction through neural machine translation of atomic environments. V Umit, Islambek Ucak, Junsu Ashyrmamatov, Juyong Ko, Lee, Nature Commun. 1311862022. 2022</p>
<p>Exploiting pretrained biochemical language models for targeted drug design. Gökçe Uludoğan, Elif Ozkirimli, O Kutlu, Nilgün Ulgen, Arzucan Karalı, Özgür, Bioinformatics. 382022. 2022</p>
<p>Learning functional properties of proteins with language models. Serbulent Unsal, Heval Atas, Muammer Albayrak, Kemal Turhan, C Aybar, Tunca Acar, Doğan, Nature Mach. Intell. 42022. 2022</p>
<p>Foldseek: Fast and accurate protein structure search. Stephanie S Michel Van Kempen, Charlotte Kim, Milot Tumescheit, Mirdita, L M Cameron, Johannes Gilchrist, Martin Söding, Steinegger, 10.1101/2022.02.07.479398v12022</p>
<p>AlphaFold protein structure database: Massively expanding the structural coverage of protein-sequence space with high-accuracy models. Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, Nucl. Acids Res. 502021. 11 2021</p>
<p>Deciphering the protein landscape with ProtFlash: A lightweight language model. Lei Wang, Hui Zhang, Wei Xu, Zhidong Xue, Yan Wang, Cell Rep. Phys. Sci. 41016002023. 2023</p>
<p>The PDBbind database: Methodologies and updates. Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, Shaomeng Wang, J. Med. Chem. 482005. 2005</p>
<p>Smiles-bert: Large scale unsupervised pre-training for molecular property prediction. Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang, Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics. the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics2019</p>
<p>A pre-trained conditional transformer for Targetspecific De Novo Molecular Generation. Wenlu Wang, Ye Wang, Honggang Zhao, Simone Sciabola, 2022</p>
<p>miProBERT: Identification of microRNA promoters based on the pre-trained model BERT. Xin Wang, Xin Gao, Guohua Wang, Dan Li, Brief. Bioinform. 24932023. 2023</p>
<p>UNI-RNA: Universal pre-trained models revolutionize RNA research. Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen, 2023</p>
<p>cMolGPT: A conditional generative pre-trained transformer for target-specific De Novo Molecular generation. Ye Wang, Honggang Zhao, Simone Sciabola, Wenlu Wang, Molecules. 2844302023. 2023</p>
<p>LM-GVP: An extensible sequence and structure informed deep learning framework for protein property prediction. Zichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Priya Sri, Ponnapalli, Sci. Rep. 1268322022. 2022</p>
<p>BioBridge: Bridging biomedical foundation models via knowledge graph. Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, https://arXiv:2310.033202023Vassilis N. Ioannidis, Huzefa Rangwala, and Rishita Anubhai</p>
<p>Instruct-Protein: Aligning human and protein language via knowledge instruction. Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, Huajun Chen, 2023</p>
<p>Multilevel protein structure pre-training via prompt learning. Zeyuan Wang, Qiang Zhang, Shuang-Wei Hu, Haoran Yu, Xurui Jin, Zhichen Gong, Huajun Chen, Proceedings of the 11th International Conference on Learning Representations. the 11th International Conference on Learning Representations2023</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. David Weininger, J. Chem. Inf. Comput. Sci. 281988. 1988</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, 2017</p>
<p>The FAIR guiding principles for scientific data management and stewardship. D Mark, Michel Wilkinson, Dumontier, Jan Ijsbrand, Gabrielle Aalbersberg, Myles Appleton, ; -Willem Axton, Luiz Boiten, Silva Bonino Da, Philip E Santos, Bourne, Sci. Data. 3Jan. 2016. 2016</p>
<p>DrugBank 5.0: A major update to the DrugBank database for. David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, Nucl. Acids Res. 462018. 2018. 2018</p>
<p>Molformer: Motif-based transformer on 3D heterogeneous molecular graphs. Fang Wu, Dragomir Radev, Stan Z Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Proteinclip: Enhancing Protein Language Models with Natural Language. Kevin E Wu, Howard Chang, James Zou, 10.1101/2024.05.14.594226v12024</p>
<p>MoleculeNet: A benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, Vijay Pande, Chem. Sci. 92018. 2018</p>
<p>Protein data bank: The single global archive for 3D macromolecular structure data. Nucl. Acids Res. 472018. 10 2018</p>
<p>A systematic survey of chemical pre-trained models. Jun Xia, Yanqiao Zhu, Yuanqi Du, Stan Z Li, 2022</p>
<p>OPI: An open instruction dataset for adapting large language models to protein-related tasks. Hongwang Xiao, Wenjun Lin, Hui Wang, Zheng Liu, Qiwei Ye, Proceedings of the NeurIPS Workshop Foundation Models for Science: Progress, Opportunities, and Challenges. the NeurIPS Workshop Foundation Models for Science: Progress, Opportunities, and Challengesn.d.</p>
<p>Molbind: Multimodal alignment of language, molecules, and proteins. Teng Xiao, Chao Cui, Huaisheng Zhu, Vasant G Honavar, https://arXiv:2403.081672024</p>
<p>Gearnet: Stepwise dual learning for weakly supervised domain adaptation. Renchunzi Xie, Hongxin Wei, Lei Feng, Bo An, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Darwin series: Domain specific large language models for natural science. Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, 2023</p>
<p>ADMETlab 2.0: An integrated online platform for accurate and comprehensive predictions of ADMET properties. Guoli Xiong, Zhenxing Wu, Jiacai Yi, Li Fu, Zhijiang Yang, Changyu Hsieh, Mingzhu Yin, Xiangxiang Zeng, Chengkun Wu, Aiping Lu, Nucl. Acids Res. 492021. 2021</p>
<p>ProTranslator: Zero-shot protein function prediction using textual description. Hanwen Xu, Sheng Wang, Proceedings of the International Conference on Research in Computational Molecular Biology. the International Conference on Research in Computational Molecular BiologySpringer2022</p>
<p>Multilingual translation for zeroshot biomedical classification using BioTranslator. Hanwen Xu, Addie Woicik, Hoifung Poon, Russ B Altman, Sheng Wang, Nature Commun. 147382023. 2023</p>
<p>Protst: Multi-modality learning of protein sequences and biomedical texts. Minghao Xu, Xinyu Yuan, Santiago Miret, Jian Tang, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPMLR2023</p>
<p>Peer: A comprehensive and multi-task benchmark for protein sequence understanding. Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng Liu, Jian Tang, Adv. Neural Inf. Process. Syst. 352022. 2022</p>
<p>X-MOL: Large-scale pre-training for molecular understanding and diverse molecular analysis. Dongyu Xue, Han Zhang, Dongling Xiao, Yukang Gong, Guohui Chuai, Yu Sun, Hua Hao Tian, Yukun Wu, Qi Li, Liu, 2020</p>
<p>Baichuan 2: Open large-scale language models. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, 2023</p>
<p>scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data. Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, Jianhua Yao, Nature Mach. Intell. 42022. 2022</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu, 2023</p>
<p>BioLiP: A semi-manually curated database for biologically relevant ligand-protein interactions. Jianyi Yang, Ambrish Roy, Yang Zhang, Nucl. Acids Res. 412012. 2012</p>
<p>LOGO, a contextualized pre-trained language model of human genome flexibly adapts to various downstream tasks by fine-tuning. Meng Yang, Haiping Huang, Lichao Huang, Nan Zhang, Jihong Wu, Huanming Yang, Feng Mu, 2021</p>
<p>Linkbert: Pretraining language models with document links. Michihiro Yasunaga, Jure Leskovec, Percy Liang, 2022</p>
<p>First place solution of KDD Cup 2021 &amp; OGB large-scale challenge graph prediction track. Chengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin Wu, Yuxin Wang, Yanming Shen, Di He, 2021</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, https://arXiv:2402.093912024</p>
<p>Selformer: Molecular representation learning via selfies language models. Atakan Yüksel, Erva Ulusoy, Atabey Ünlü, Tunca Doğan, Mach. Learn.: Sci. Technol. 2023. 2023</p>
<p>The ChEMBL database in 2023: A drug discovery platform spanning multiple bioactivity data types and time periods. Barbara Zdrazil, Eloy Felix, Fiona Hunter, Emma J Manners, James Blackshaw, Sybilla Corbett, Marleen De Veij, Harris Ioannidis, David Mendez Lopez, Juan F Mosquera, Nucl. Acids Res. 10042023. 2023</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, 2022</p>
<p>Convolutional neural network architectures for predicting DNA-protein binding. Haoyang Zeng, Matthew D Edwards, Ge Liu, David K Gifford, Bioinformatics. 322016. 2016</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun, Nature Commun. 138622022. 2022</p>
<p>Interactive molecular discovery with natural language. Zheni Zeng, Bangchen Yin, Shipeng Wang, Jiarui Liu, Cheng Yang, Haishen Yao, Xingzhi Sun, Maosong Sun, Guotong Xie, Zhiyuan Liu, 2023</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, https://arXiv:2401.079502024</p>
<p>Chemllm: A chemical large language model. Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, https://arXiv:2402.068522024</p>
<p>DNAGPT: A generalized pretrained tool for multiple DNA sequence analysis tasks. Daoan Zhang, Weitong Zhang, Bing He, Jianguo Zhang, Chenchen Qin, Jianhua Yao, 2023</p>
<p>Enhancing the protein tertiary structure prediction by multiple sequence alignment generation. Le Zhang, Jiayang Chen, Tao Shen, Yu Li, Siqi Sun, 2023</p>
<p>Ontoprotein: Protein pretraining with gene ontology embedding. Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, Huajun Chen, 2022</p>
<p>Ontoprotein: Protein pretraining with gene ontology embedding. Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, Huajun Chen, https://arXiv:2201.111472022</p>
<p>OPT: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, 2022</p>
<p>BERTScore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 2019</p>
<p>MG-BERT: Leveraging unsupervised atomic representation learning for molecular property prediction. Xiao-Chen Zhang, Cheng-Kun Wu, Zhi-Jiang Yang, Zhen-Xing Wu, Jia-Cai Yi, Chang-Yu Hsieh, Ting-Jun Hou, Dong-Sheng Cao, Brief. Bioinform. 221522021. 2021</p>
<p>Pushing the boundaries of molecular property prediction for drug discovery with multitask learning BERT enhanced by SMILES enumeration. Xiao-Chen Zhang, Cheng-Kun Wu, Jia-Cai Yi, Xiang-Xiang Zeng, Can-Qun Yang, Ai-Ping Lu, Ting-Jun Hou, Dong-Sheng Cao, Research. 202242022. 2022</p>
<p>Prediction of multiple types of RNA modifications via biological language model. Ying Zhang, Fang Ge, Fuyi Li, Xibei Yang, Jiangning Song, Dong-Jun Yu, IEEE/ACM Trans. Comput. Biol. Bioinform. 202023. 2023</p>
<p>Multiple sequence-alignment-based RNA language model and its application to structural inference. Yikun Zhang, Mei Lang, Jiuhong Jiang, Zhiqiang Gao, Fan Xu, Thomas Litfin, Ke Chen, Jaswinder Singh, Xiansong Huang, Guoli Song, 2023</p>
<p>A systematic study of joint representation learning on protein sequences and structures. Zuobai Zhang, Chuanrui Wang, Minghao Xu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang, 2023</p>
<p>Enhancing protein language models with structure-based encoder and pre-training. Zuobai Zhang, Minghao Xu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang, 2023</p>
<p>GIMLET: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, Qi Liu, 2023</p>
<p>Zican Dong et al. 2023. A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zhang, </p>
<p>Chemdfm: Dialogue foundation model for chemistry. Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, https://arXiv:2401.148182024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Adv. Neural Inf. Process. Syst. 362023. 2023</p>
<p>Predicting retrosynthetic reactions using self-corrected transformer neural networks. Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, J. Chem. Inf. Model. 60Jun Xu, and Yuedong Yang. 2019. 2019</p>
<p>Structure-informed language models are protein designers. Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, Quanquan Gu, 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>
<p>Uni-Mol: A universal 3D molecular representation learning framework. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke, 2023</p>
<p>Protein representation learning via knowledge enhanced primary structure reasoning. Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Bian Cheng, Yizhou Yu, Proceedings of the 11th International Conference on Learning Representations. the 11th International Conference on Learning Representations2023</p>
<p>Predicting effects of noncoding variants with deep learning-based sequence model. Jian Zhou, Olga G Troyanskaya, Nature Methods. 122015. 2015</p>
<p>Dnabert-2: Efficient foundation model and benchmark for multi-species genome. Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, Han Liu, 2023</p>
<p>Learning over molecular conformer ensembles: Datasets and benchmarks. Yanqiao Zhu, Jeehyun Hwang, Keir Adams, Zhen Liu, Bozhao Nan, Brock Stenfors, Yuanqi Du, Jatin Chauhan, Olaf Wiest, Olexandr Isayev, 2023</p>
<p>Transmol: Repurposing a language model for molecular generation. Rustam Zhumagambetov, Ferdinand Molnár, A Vsevolod, Siamac Peshkov, Fazli, RSC Adv. 112021. 2021</p>
<p>ProtLLM: An interleaved protein-language LLM with protein-as-word pre-training. Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang, https://arXiv:2403.079202024</p>
<p>GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics. Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Int. J. High Perform. Comput. Appl. 372023. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1697 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1697</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1697</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-265043627</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.50.pdf" target="_blank">Multitask Multimodal Prompted Training for Interactive Embodied Task Completion</a></p>
                <p><strong>Paper Abstract:</strong> Interactive and embodied tasks pose at least two fundamental challenges to existing Vision&Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks. Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion. EMMA performs on par with similar models on several VL benchmarks and sets a new state-of-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1697.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1697.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied MultiModal Agent (EMMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified encoder-decoder vision-and-language model that casts action prediction as multimodal text generation, trained with multitask VLP objectives and fine-tuned to complete dialog-guided 3D embodied tasks in the Alexa Arena (DTC).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>EMMA (Embodied MultiModal Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Single multitask encoder-decoder (BART-base architecture) that encodes language and object-centric region/frame features (with sentinel tokens) into a single-stream encoder and uses a shared autoregressive decoder to generate text tokens representing actions, routing decisions, object references (visual sentinel ids), and task outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-and-language (image-text pairs, VQA, scene graphs) — language-conditioned image data (not pure text-only worlds)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained on publicly available V&L datasets: COCO Captions, Conceptual Captions (3M), GQA, VQA-v2, Visual Genome; total ~10.4M training examples across ~3.3M images. Pretraining tasks framed as text-to-text: MLM/denoising, ITM, captioning, dense captioning, visual grounding, relationship prediction. Trained for 100k steps, batch size 2048; mixed-task sampling with upsampling ratio R=3.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Dialog-guided Task Completion (DTC) in the Alexa Arena</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D simulated multi-room household missions where an agent must follow natural language instructions (optionally with clarification QA), perform navigation (primitive actions and GoTo) and object manipulations (PickUp, Place, Open, Close, Toggle, Fill, Clean, Pour, Break, Scan, etc.) using object masks and room/object targets. The benchmark evaluates mission success rate (MSR) and number of robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Pretraining: natural language tokens and generation objectives (captions, QA answers, grounding tokens). The model's action vocabulary is textual (task prefixes and action tokens) rather than low-level motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Alexa Arena: discrete navigation primitives (MoveForward, MoveBackward, RotateLeft/Right), high-level GoTo (room or object mask), and manipulation actions that require specifying an affordance + object mask (object sentinel token and frame id).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Actions are represented as generated text tokens: the decoder emits an action-type token followed by object name and, for manipulation, the frame sentinel and visual sentinel id referencing a region/mask; an external search routine and object detector provide visual candidates. Thus high-level action semantics are mapped to embodied API calls by interpreting generated text tokens (action type + visual token → environment primitive/invocation).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Frozen VinVL vision encoder for region proposals (object-centric features), object detector providing region masks/ids, RGB images (no depth), spatial embeddings (normalized bbox), temporal/frame sentinel embeddings; object affordances metadata from simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>EMMA-unified achieved 36.81% Mission Success Rate (MSR) on the DTC benchmark (state-of-the-art reported in paper). Pretraining: 10.4M image-text examples; fine-tuned on Alexa Arena with 10K steps (batch size 256).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Base model trained from scratch on same data performed poorly: 5.2% MSR. (Also reported: EMMA-small variant with pretraining achieved 32.11% MSR; baseline model trained with augmentations reached ~34.42% MSR in a controlled comparison.)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Pretraining used ~10.4M multimodal examples; fine-tuning on embodied downstream used 10K gradient steps (batch size 256). Additional in-domain augmentations: visual augmentations ~180k synthetic (train) / 88k (val) (image-instruction-action triplets); CDF augmentations: 38k sampled missions (train) and 15k (val). Together these fine-tuning/augmentation datasets were used to reach the reported MSR.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>No explicit per-episode sample-to-performance curves reported for training-from-scratch; reported baseline 'trained from scratch' run reached only 5.2% MSR despite training on the same data (implying much higher sample requirements and/or poor convergence). The paper does not report an exact number of episodes needed without pretraining to reach comparable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not reported as a single numeric factor. Empirically, pretraining + fine-tuning produced MSR 36.81% vs 5.2% when training the same (base) model from scratch — indicating large effective benefit of VLP pretraining; exact sample-efficiency gain (e.g., x-fold fewer episodes) is not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale vision-and-language pretraining that aligns visual features with text; casting actions as text (shared action-language vocabulary); object-centric region features with sentinel tokens enabling direct visual reference; multitask pretraining (captioning, grounding, relationship predicates) that provides transferable semantics; targeted augmentations (visual synthetic data and CDF trajectory augmentations) to cover missing downstream phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretraining did not include trajectory/frame-association objectives so the model initially struggled to associate visual tokens across timesteps; perception-domain gaps (object detector mismatch) impacted sim→real transfer; errors in contextual routing (CR) cascade to irreversible environment states; limited temporal understanding and commonsense reasoning; reliance on an external search routine.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vision-and-language pretraining (image-text datasets and VQA/scene-graph tasks) combined with casting actions as textual tokens enables effective transfer to a 3D dialog-guided embodied benchmark (DTC), yielding state-of-the-art MSR when coupled with in-domain augmentations (visual and CDF trajectory data). However, pretraining must be complemented by trajectory-style fine-tuning or augmentations to learn frame–visual-token associations; major failure modes arise from contextual routing errors, temporal token reference mistakes, and perception-domain mismatches during Sim2Real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Multimodal Prompted Training for Interactive Embodied Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1697.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1697.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Palm-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Palm-e: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a recent work that connects vision and language experts into models used for embodied tasks; cited as an example of leveraging unimodal pretraining for generative and multimodal embodied models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Palm-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced as a multimodal embodied language model that composes pretrained experts (vision and language) for embodied tasks (paper does not use it experimentally).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Mentioned as leveraging unimodal pretraining (large vision and language corpora) — not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Not specified in this paper (cited in related work as an embodied multimodal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites Palm-E as part of a trend that composes pretrained unimodal experts for embodied and generative multimodal models; no experimental details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Multimodal Prompted Training for Interactive Embodied Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1697.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1697.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language-only planners (Huang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models as zero-shot planners / Inner Monologue (Huang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced family of works that use large language models for planning in embodied settings by generating plans from textual descriptions of the environment; they are contrasted with VLP approaches which ground language directly to images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Language-only planners (LM planners)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Large language models used to produce plans from textual environment descriptions or prompts; in the paper they are discussed as prior work that cannot by themselves ground fine-grained visual nuances unless the prompt encodes them explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained on large text corpora / language data (general LMs) — mentioned as language-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used in prior literature for planning in embodied settings (not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural language plans and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Typically mapped to high-level discrete actions via planners or symbolic interfaces (paper does not provide specifics).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Require textual descriptions of visual state (i.e., need image→text conversion) to be effective; lack direct visual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong language planning ability, commonsense knowledge from text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Cannot capture fine-grained visual nuances unless image state is converted to detailed text; therefore limited grounding for low-level visually-conditioned actions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper contrasts LM-only planners with VLP-based agents, arguing VLPs can better ground instructions into visual observations while LM planners need text encodings of visual nuances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Multimodal Prompted Training for Interactive Embodied Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1697.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1697.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vima: General robot manipulation with multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a recent work using multimodal prompting for robot manipulation that leverages pretrained multimodal models; cited in related work as part of the trend of leveraging pretrained models for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vima: General robot manipulation with multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced as a model that uses multimodal prompts to perform general robot manipulation (no experimental usage in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multimodal pretraining (vision + language), implied but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of using pretrained multimodal models with prompting for manipulation; no details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Multimodal Prompted Training for Interactive Embodied Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1697.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1697.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Socratic Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Socratic models: Composing zero-shot multimodal reasoning with language</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a method that composes pretrained unimodal experts to enable zero-shot multimodal reasoning; referenced in context of connecting vision-only and language-only experts to build embodied-capable models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Socratic models: Composing zero-shot multimodal reasoning with language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Socratic Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Referenced approach that composes separate pretrained models (e.g., vision and language) to perform multimodal reasoning without joint training; not experimentally used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Compositions of pretrained unimodal models (vision and language) — not specified in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited among works showing that connecting unimodal pretrained experts can enable multimodal, embodied-like capabilities; no further experimental detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Multimodal Prompted Training for Interactive Embodied Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1697.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1697.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied BERT: A transformer model for embodied, language-guided visual task completion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work on transformer-based models for embodied instruction following and visual task completion; included in related work to situate EMMA among prior embodied transformer efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodied bert: A transformer model for embodied, language-guided visual task completion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Embodied BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An earlier transformer model proposed for embodied language-guided visual task completion; cited for context but not used or analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Not detailed in this paper (original work likely involves multimodal or language-grounded pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed in related work as an example of transformer approaches for embodied tasks; the current paper differs by unifying tasks as text generation and emphasizing VLP pretraining plus trajectory augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multitask Multimodal Prompted Training for Interactive Embodied Task Completion', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Vima: General robot manipulation with multimodal prompts <em>(Rating: 2)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language <em>(Rating: 2)</em></li>
                <li>Embodied bert: A transformer model for embodied, language-guided visual task completion <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1697",
    "paper_id": "paper-265043627",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "EMMA",
            "name_full": "Embodied MultiModal Agent (EMMA)",
            "brief_description": "A unified encoder-decoder vision-and-language model that casts action prediction as multimodal text generation, trained with multitask VLP objectives and fine-tuned to complete dialog-guided 3D embodied tasks in the Alexa Arena (DTC).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "EMMA (Embodied MultiModal Agent)",
            "model_agent_description": "Single multitask encoder-decoder (BART-base architecture) that encodes language and object-centric region/frame features (with sentinel tokens) into a single-stream encoder and uses a shared autoregressive decoder to generate text tokens representing actions, routing decisions, object references (visual sentinel ids), and task outputs.",
            "pretraining_data_type": "Vision-and-language (image-text pairs, VQA, scene graphs) — language-conditioned image data (not pure text-only worlds)",
            "pretraining_data_details": "Pretrained on publicly available V&L datasets: COCO Captions, Conceptual Captions (3M), GQA, VQA-v2, Visual Genome; total ~10.4M training examples across ~3.3M images. Pretraining tasks framed as text-to-text: MLM/denoising, ITM, captioning, dense captioning, visual grounding, relationship prediction. Trained for 100k steps, batch size 2048; mixed-task sampling with upsampling ratio R=3.",
            "embodied_task_name": "Dialog-guided Task Completion (DTC) in the Alexa Arena",
            "embodied_task_description": "3D simulated multi-room household missions where an agent must follow natural language instructions (optionally with clarification QA), perform navigation (primitive actions and GoTo) and object manipulations (PickUp, Place, Open, Close, Toggle, Fill, Clean, Pour, Break, Scan, etc.) using object masks and room/object targets. The benchmark evaluates mission success rate (MSR) and number of robot actions.",
            "action_space_text": "Pretraining: natural language tokens and generation objectives (captions, QA answers, grounding tokens). The model's action vocabulary is textual (task prefixes and action tokens) rather than low-level motor commands.",
            "action_space_embodied": "Alexa Arena: discrete navigation primitives (MoveForward, MoveBackward, RotateLeft/Right), high-level GoTo (room or object mask), and manipulation actions that require specifying an affordance + object mask (object sentinel token and frame id).",
            "action_mapping_method": "Actions are represented as generated text tokens: the decoder emits an action-type token followed by object name and, for manipulation, the frame sentinel and visual sentinel id referencing a region/mask; an external search routine and object detector provide visual candidates. Thus high-level action semantics are mapped to embodied API calls by interpreting generated text tokens (action type + visual token → environment primitive/invocation).",
            "perception_requirements": "Frozen VinVL vision encoder for region proposals (object-centric features), object detector providing region masks/ids, RGB images (no depth), spatial embeddings (normalized bbox), temporal/frame sentinel embeddings; object affordances metadata from simulator.",
            "transfer_successful": true,
            "performance_with_pretraining": "EMMA-unified achieved 36.81% Mission Success Rate (MSR) on the DTC benchmark (state-of-the-art reported in paper). Pretraining: 10.4M image-text examples; fine-tuned on Alexa Arena with 10K steps (batch size 256).",
            "performance_without_pretraining": "Base model trained from scratch on same data performed poorly: 5.2% MSR. (Also reported: EMMA-small variant with pretraining achieved 32.11% MSR; baseline model trained with augmentations reached ~34.42% MSR in a controlled comparison.)",
            "sample_complexity_with_pretraining": "Pretraining used ~10.4M multimodal examples; fine-tuning on embodied downstream used 10K gradient steps (batch size 256). Additional in-domain augmentations: visual augmentations ~180k synthetic (train) / 88k (val) (image-instruction-action triplets); CDF augmentations: 38k sampled missions (train) and 15k (val). Together these fine-tuning/augmentation datasets were used to reach the reported MSR.",
            "sample_complexity_without_pretraining": "No explicit per-episode sample-to-performance curves reported for training-from-scratch; reported baseline 'trained from scratch' run reached only 5.2% MSR despite training on the same data (implying much higher sample requirements and/or poor convergence). The paper does not report an exact number of episodes needed without pretraining to reach comparable performance.",
            "sample_complexity_gain": "Not reported as a single numeric factor. Empirically, pretraining + fine-tuning produced MSR 36.81% vs 5.2% when training the same (base) model from scratch — indicating large effective benefit of VLP pretraining; exact sample-efficiency gain (e.g., x-fold fewer episodes) is not quantified in the paper.",
            "transfer_success_factors": "Large-scale vision-and-language pretraining that aligns visual features with text; casting actions as text (shared action-language vocabulary); object-centric region features with sentinel tokens enabling direct visual reference; multitask pretraining (captioning, grounding, relationship predicates) that provides transferable semantics; targeted augmentations (visual synthetic data and CDF trajectory augmentations) to cover missing downstream phenomena.",
            "transfer_failure_factors": "Pretraining did not include trajectory/frame-association objectives so the model initially struggled to associate visual tokens across timesteps; perception-domain gaps (object detector mismatch) impacted sim→real transfer; errors in contextual routing (CR) cascade to irreversible environment states; limited temporal understanding and commonsense reasoning; reliance on an external search routine.",
            "key_findings": "Vision-and-language pretraining (image-text datasets and VQA/scene-graph tasks) combined with casting actions as textual tokens enables effective transfer to a 3D dialog-guided embodied benchmark (DTC), yielding state-of-the-art MSR when coupled with in-domain augmentations (visual and CDF trajectory data). However, pretraining must be complemented by trajectory-style fine-tuning or augmentations to learn frame–visual-token associations; major failure modes arise from contextual routing errors, temporal token reference mistakes, and perception-domain mismatches during Sim2Real transfer.",
            "uuid": "e1697.0",
            "source_info": {
                "paper_title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Palm-E",
            "name_full": "Palm-e: An embodied multimodal language model",
            "brief_description": "Mentioned as a recent work that connects vision and language experts into models used for embodied tasks; cited as an example of leveraging unimodal pretraining for generative and multimodal embodied models.",
            "citation_title": "Palm-e: An embodied multimodal language model",
            "mention_or_use": "mention",
            "model_agent_name": "Palm-E",
            "model_agent_description": "Referenced as a multimodal embodied language model that composes pretrained experts (vision and language) for embodied tasks (paper does not use it experimentally).",
            "pretraining_data_type": "Mentioned as leveraging unimodal pretraining (large vision and language corpora) — not detailed in this paper.",
            "pretraining_data_details": null,
            "embodied_task_name": "Not specified in this paper (cited in related work as an embodied multimodal LM)",
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Paper cites Palm-E as part of a trend that composes pretrained unimodal experts for embodied and generative multimodal models; no experimental details provided here.",
            "uuid": "e1697.1",
            "source_info": {
                "paper_title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Language-only planners (Huang et al.)",
            "name_full": "Language models as zero-shot planners / Inner Monologue (Huang et al.)",
            "brief_description": "Referenced family of works that use large language models for planning in embodied settings by generating plans from textual descriptions of the environment; they are contrasted with VLP approaches which ground language directly to images.",
            "citation_title": "Language models as zeroshot planners: Extracting actionable knowledge for embodied agents",
            "mention_or_use": "mention",
            "model_agent_name": "Language-only planners (LM planners)",
            "model_agent_description": "Large language models used to produce plans from textual environment descriptions or prompts; in the paper they are discussed as prior work that cannot by themselves ground fine-grained visual nuances unless the prompt encodes them explicitly.",
            "pretraining_data_type": "Pretrained on large text corpora / language data (general LMs) — mentioned as language-only pretraining.",
            "pretraining_data_details": null,
            "embodied_task_name": "Used in prior literature for planning in embodied settings (not used here).",
            "embodied_task_description": null,
            "action_space_text": "High-level natural language plans and instructions.",
            "action_space_embodied": "Typically mapped to high-level discrete actions via planners or symbolic interfaces (paper does not provide specifics).",
            "action_mapping_method": null,
            "perception_requirements": "Require textual descriptions of visual state (i.e., need image→text conversion) to be effective; lack direct visual grounding.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong language planning ability, commonsense knowledge from text.",
            "transfer_failure_factors": "Cannot capture fine-grained visual nuances unless image state is converted to detailed text; therefore limited grounding for low-level visually-conditioned actions.",
            "key_findings": "Paper contrasts LM-only planners with VLP-based agents, arguing VLPs can better ground instructions into visual observations while LM planners need text encodings of visual nuances.",
            "uuid": "e1697.2",
            "source_info": {
                "paper_title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "VIMA",
            "name_full": "Vima: General robot manipulation with multimodal prompts",
            "brief_description": "Mentioned as a recent work using multimodal prompting for robot manipulation that leverages pretrained multimodal models; cited in related work as part of the trend of leveraging pretrained models for embodied tasks.",
            "citation_title": "Vima: General robot manipulation with multimodal prompts",
            "mention_or_use": "mention",
            "model_agent_name": "VIMA",
            "model_agent_description": "Referenced as a model that uses multimodal prompts to perform general robot manipulation (no experimental usage in this paper).",
            "pretraining_data_type": "Multimodal pretraining (vision + language), implied but not detailed here.",
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Cited as an example of using pretrained multimodal models with prompting for manipulation; no details provided in this paper.",
            "uuid": "e1697.3",
            "source_info": {
                "paper_title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Socratic Models",
            "name_full": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "brief_description": "Mentioned as a method that composes pretrained unimodal experts to enable zero-shot multimodal reasoning; referenced in context of connecting vision-only and language-only experts to build embodied-capable models.",
            "citation_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "mention_or_use": "mention",
            "model_agent_name": "Socratic Models",
            "model_agent_description": "Referenced approach that composes separate pretrained models (e.g., vision and language) to perform multimodal reasoning without joint training; not experimentally used in this paper.",
            "pretraining_data_type": "Compositions of pretrained unimodal models (vision and language) — not specified in detail here.",
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Cited among works showing that connecting unimodal pretrained experts can enable multimodal, embodied-like capabilities; no further experimental detail in this paper.",
            "uuid": "e1697.4",
            "source_info": {
                "paper_title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Embodied BERT",
            "name_full": "Embodied BERT: A transformer model for embodied, language-guided visual task completion",
            "brief_description": "Referenced prior work on transformer-based models for embodied instruction following and visual task completion; included in related work to situate EMMA among prior embodied transformer efforts.",
            "citation_title": "Embodied bert: A transformer model for embodied, language-guided visual task completion",
            "mention_or_use": "mention",
            "model_agent_name": "Embodied BERT",
            "model_agent_description": "An earlier transformer model proposed for embodied language-guided visual task completion; cited for context but not used or analyzed in this paper.",
            "pretraining_data_type": "Not detailed in this paper (original work likely involves multimodal or language-grounded pretraining).",
            "pretraining_data_details": null,
            "embodied_task_name": null,
            "embodied_task_description": null,
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": "Listed in related work as an example of transformer approaches for embodied tasks; the current paper differs by unifying tasks as text generation and emphasizing VLP pretraining plus trajectory augmentations.",
            "uuid": "e1697.5",
            "source_info": {
                "paper_title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 2,
            "sanitized_title": "palme_an_embodied_multimodal_language_model"
        },
        {
            "paper_title": "Language models as zeroshot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "Vima: General robot manipulation with multimodal prompts",
            "rating": 2,
            "sanitized_title": "vima_general_robot_manipulation_with_multimodal_prompts"
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "rating": 2,
            "sanitized_title": "socratic_models_composing_zeroshot_multimodal_reasoning_with_language"
        },
        {
            "paper_title": "Embodied bert: A transformer model for embodied, language-guided visual task completion",
            "rating": 1,
            "sanitized_title": "embodied_bert_a_transformer_model_for_embodied_languageguided_visual_task_completion"
        }
    ],
    "cost": 0.016179,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multitask Multimodal Prompted Training for Interactive Embodied Task Completion</p>
<p>Georgios Pantazopoulos 
Heriot-Watt University</p>
<p>Malvina Nikandrou 
Heriot-Watt University</p>
<p>Amit Parekh amit.parekh@hw.ac.uk 
Heriot-Watt University</p>
<p>Bhathiya Hemanthage 
Heriot-Watt University</p>
<p>Arash Eshghi a.eshghi@hw.ac.uk 
Heriot-Watt University</p>
<p>AlanaAI</p>
<p>Ioannis Konstas i.konstas@hw.ac.uk 
Heriot-Watt University</p>
<p>AlanaAI</p>
<p>Verena Rieser v.t.rieser@hw.ac.uk 
Heriot-Watt University</p>
<p>AlanaAI</p>
<p>Oliver Lemon o.lemon@hw.ac.uk 
Heriot-Watt University</p>
<p>AlanaAI</p>
<p>Alessandro Suglia a.suglia@hw.ac.uk 
Heriot-Watt University</p>
<p>Multitask Multimodal Prompted Training for Interactive Embodied Task Completion
1AFF07A5D3B3772215AECB6B13D538DC
Interactive and embodied tasks pose at least two fundamental challenges to existing Vision &amp; Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation.To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation.By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks.Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion.EMMA performs on par with similar models on several VL benchmarks and sets a new stateof-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena(Gao et al., 2023). 1</p>
<p>Introduction</p>
<p>Embodied AI aims to develop agents that interact with the environment, reason over natural language instructions and visual observations, and plan future actions.Recently, Vision &amp; Language pretraining (VLP) has established new standards across image-based tasks (Bao et al., 2022;Wang et al., 2022a,b;Yang et al., 2021;Zhang et al., 2021) by aligning visual and textual input to generate image captions, answer visual questions, and reason over images.As a result, VLP promotes learning of important skills transferable to embodied tasks.</p>
<p>Interactive and embodied tasks present significant challenges for VLP models including 1) grounding language in environments where the interpretation of instructions depends not just on static images, but on trajectories of actions and * Now at Google DeepMind 1 Code available on this link observations, and 2) referential disambiguation, where agents must interact with users -often as clarifications -to successfully disambiguate referents.In contrast to language-only planners (Huang et al., 2022a;Min et al., 2021), which cannot generate feasible plans without prompts that encode all the visual nuances -colors, attributes, relational information, inter alia -as text; instead VLP models can ground language directly to images.However, to enable grounded decision-making, VLP models must generalize from static images to trajectories that express how a situation evolves over time (McClelland et al., 2020).Additionally, in language-guided collaborative tasks, referents for target objects may be ambiguous, depending on the complexity of the visual scene.Therefore, clarifications must be adopted to resolve uncertainty between referents and act upon the correct target object (Madureira and Schlangen, 2023).</p>
<p>To address both of these challenges, we develop EMMA: a unified approach which casts several VL and embodied tasks, such as image captioning and action execution, as text generation inspired by Cho et al. (2021).By doing so, EMMA is able to learn a language of actions which facilitates transfer across tasks.Concretely, EMMA is a multitask encoder-decoder model, which encodes language and pairs of object-centric representations with sentinel tokens.As a result, EMMA can encode both trajectories and clarifications to reference individual frames and objects across an entire trajectory.</p>
<p>We focus on dialog-enabled task completion, which requires an agent to follow natural language instructions, perform navigation and object manipulation actions, and utilize dialog interactions to disambiguate visual observations.Similar to previous modular approaches (Min et al., 2021), we break down the embodied task into separate processes for search and action execution.The search process combines exploration with visual grounding.Differently to prior work that only uses object labels from object detectors directly (Min et al., 2021;Pashevich et al., 2021), EMMA uses both language and visual representations to discriminate between objects of the same class (e.g., discerning between a 'red ' and a 'green' desk).Separately, the action execution process predicts the navigation and object interaction actions as textual tokens.Notably, a key distinction in our approach is that we perform all tasks using a single VL model.</p>
<p>We evaluate our approach on the Dialog-guided Task Completion (DTC) benchmark (Gao et al., 2023).Our experiments show the benefits of the multitask VLP, and the importance of data augmentations for completing multi-step instructions.By exploiting clarifications, EMMA improves the success rate by 3.55%.Finally, when fine-tuned on DTC, EMMA can transfer the downstream action execution task back to real-world images, showcasing how using text tokens to represent actions in the environment enables cross-domain generalization.</p>
<p>Related Work</p>
<p>Vision &amp; Language Pretraining Early methods for developing VLP models rely on multimodal encoders with object-centric or patch representations (Chen et al., 2020;Kim et al., 2021;Li et al., 2021Li et al., , 2020;;Lu et al., 2019;Singh et al., 2022a;Tan and Bansal, 2019).While effective, these methods introduce task-specific heads for each downstream task.Generative and unified architectures represent an alternative approach for tackling VL tasks with a single model.Previous work on unified models explores region proposals (Cho et al., 2021;Gupta et al., 2022), or patch features (Wang et al., 2022a,b;Yang et al., 2022).More recently, the paradigm of connecting vision-only and languageonly experts allows leveraging unimodal pretraining for generative and multimodal models (Alayrac et al., 2022;Driess et al., 2023;Li et al., 2023;Liu et al., 2023;Tsimpoukelli et al., 2021;Zhu et al., 2023).By casting every task as a text generation task, unified VLP models are transitioning from specialized to generalist models.Therefore, adopting a similar multitask pretraining approach enables EMMA to learn a language that describes actions conditioned on visual observations.</p>
<p>Embodied AI Approaches Previous approaches on interactive task completion (Shridhar et al., 2020) involve both end-to-end (Pashevich et al., 2021;Suglia et al., 2021) and modular architectures (Blukis et al., 2022;Kim and Lee, 2023;Liu et al., 2022;Min et al., 2021;Singh et al., 2021).State-of-the-art works employ a modular architecture that incrementally builds a semantic map of the environment used by symbolic planners.However, these methods often make strong domain-specific assumptions, or they are typically trained from scratch, underutilizing recent progress in VLP.</p>
<p>An exciting direction is leveraging large-scale pretrained models.Recent works (Ahn et al., 2022;Driess et al., 2023;Huang et al., 2022a;Singh et al., 2022b;Zeng et al., 2022) use a large language model or a mixture of pretrained models for object manipulation.Additionally, Huang et al. (2022b); Shridhar et al. (2021) use text to describe the environment state and employ a language model for planning.Different to previous work, we use a VLP model to ground instructions on visual scenes.For example, given the instruction 'Get the milk from the fridge', EMMA executes the correct action sequence by inferring whether the fridge door is open or not from its visual observations.Dialog-enabled interactive task completion introduces an additional layer of complexity as the agent often needs to incorporate clarification context before acting on the environment (Gao et al., 2023(Gao et al., , 2022;;Padmakumar et al., 2022).Both endto-end and modular architectures underperform in these tasks, which Min et al. (2022) argues is due this is due imitation learning using few and suboptimal demonstrations.EMMA is trained using expert demonstrations and interactions in the form of question-answer pairs about the visual scene.These clarifications do not involve complex dialog phenomena (e.g., repair), yet are sufficiently informative to disambiguate target objects.</p>
<p>Task Description</p>
<p>The DTC benchmark evaluates dialog-enabled agents to complete missions in the Alexa Arena, a simulated environment with multi-room layouts (Gao et al., 2023).Each example corresponds to a mission completed by an expert planner and annotated by three humans.Each instruction is optionally accompanied by a clarification questionanswer pair.Navigation is performed with primitive actions, but also with the GoTo action, which allows moving to different rooms, or objects.Along with the GoTo action, the agent needs to predict the name of the room or an object mask within the current observation.Each room also contains a set of viewpoints that differ between layouts and can  be visited by the agent to explore the room.To perform an action on an object, the agent needs to provide the action type and the mask of the object.The set of supported actions for an object is determined by its affordances such as openable, toggleable, inter alia (see Appendix C for details).</p>
<p>EMMA</p>
<p>EMMA is an encoder-decoder model following the architecture of BART-base (Lewis et al., 2020).As shown in Figure 1, both vision and language inputs are embedded through modality-specific projection layers, concatenated into a sequence of embeddings, and fed into a single-stream encoder.EMMA uses a shared decoder across all pretraining tasks with sentinel tokens in the vocabulary to allow referencing specific image frames and regions.</p>
<p>Text Embedding For the language input, we apply sub-word byte-pair encoding (Sennrich et al., 2016) with a vocabulary of 10K tokens extracted from our pretraining data.Each token is represented by the sum of its word and absolute positional embedding (Lewis et al., 2020).Similar to Sanh et al. (2022), we use natural language prompts as task prefixes to prompt the model for each task.</p>
<p>Scene and Object Embeddings EMMA is capable of encoding sequences of images.For each frame of the visual input, we extract global scene features representing the entire frame and a maximum of n = 36 region features.To reference a frame or an object within it, the language vocabulary is extended with sentinel tokens in the form of <frame_token_i> and <visual_token_j>.After projecting the visual features to the encoder's dimensionality, we add spatial, temporal, and visual sentinel token embeddings.Spatial embeddings encode the 2D position of the region within the frame by its normalized bounding box coordinates, while temporal embeddings encode the temporal order within the sequence using a frame sentinel token.</p>
<p>The choice between object-centric and patch representations for embodied tasks remains an open question.Although patch representations contain explicit information about the semantic segmentation of an image (Caron et al., 2021), their static grid structure has not been proven beneficial in previous embodied tasks (Driess et al., 2023;Jiang et al., 2023).Furthermore, patch representations lead to an increase in both input length, as more visual tokens are required to represent a frame, and output length, as the model must generate coordinate tokens instead of a single sentinel token to reference an object.For these reasons, we opted for object-centric representations.</p>
<p>Pretraining Setup</p>
<p>We formulate seven VL tasks in a text-to-text framework motivated by established pretraining tasks (Cho et al., 2021;Wang et al., 2022b)   (Dense) Captioning: Produce a textual description of the overall image or a specified region denoted by a visual sentinel token.</p>
<p>Visual Grounding: Given a description of an image region, the model must predict the visual token for the region that matches this description.</p>
<p>Relationship Prediction: Describe the relationship between two regions of an image.The output follows the format: Subject Attributes, Subject, Relationship Predicate, Object Attributes, Object.</p>
<p>Interactive Task Completion</p>
<p>Our agent uses three distinct yet complementary tasks to complete the DTC missions: Contextual Routing (CR), Action Execution (AE), and Visual Grounding (VG).As shown by Figure 2, we use a single model to tackle all three tasks.Since CR &amp; AE are downstream-specific tasks, we introduce new prompts when fine-tuning.</p>
<p>Contextual Routing (CR) Given the current instruction and observation, the CR task determines whether the agent should act or search for an object, disentangling the decision of what to do next from how to achieve it.The output of the model follows a structured hierarchical scheme (Figure 3).The first token determines whether the instruction refers to an action or to search for an object.The second token indicates if there are no objects present, one object present, or multiple matches of the object mentioned in the instruction.Finally, for nonnavigation actions, the model generates the name of the target object.</p>
<p>Action Execution (AE) The agent acts on the environment if the output of the CR task is <act><one match> or <act><multiple matches>.The input to the model consists of the current frame, the instruction, and the clarification pair.We use the <follower> and <commander> tokens as prefixes to each dialog turn.The model predicts the action type, the name of the object, and, for manipulation actions, the frame and visual token id that correspond to the object (Figure 1).Each action yields a new observation concatenated with the previous visual context to make a new prediction.The period character ('.') delimits actions, and the <stop> token marks the end of the trajectory for that instruction.</p>
<p>Visual Grounding (VG) For <search> and <act><no match> predictions, the agent tries to find the object outputted by the CR task.We iterate over viewpoints in a room to collect panoramic views and use the pretraining visual grounding prefix with the instruction as input to the model (Appendix D).If the instruction matches with a visual token, the model outputs that token; else, the model outputs 'no OBJECT'.For <act><no match>, once the object has been found, we invoke the AE task.</p>
<p>Data Augmentations</p>
<p>Since the trajectories have been generated by an expert planner, the predictions <search> and <act><no match> used by the CR task are underrepresented in the training data.Training for VG is unsupported as DTC instances only contain the ground-truth object mask for each action but not for all other objects in a scene.Furthermore, preliminary experiments revealed that, when trained solely on instances from the DTC benchmark, our agent struggled to learn associations between frame and visual tokens.For example, when interacting with a fridge across multiple frames, the agent sometimes predicted visual tokens that matched the fridge in a previous rather than the current frame.Finally, there is a mismatch between how the trajectories were annotated and the data the agent is being exposed to.While annotators observed interpolated trajectories, the agent only perceives the state before and after an action.This discrepancy adds significant language variability and complexity, posing challenges to the agent's comprehension.</p>
<p>To address these challenges, we use the vision dataset provided by Gao et al. (2023) to create synthetic instruction data.Additionally, we deploy an early version of our agent to collect mini-episodes of successful trajectories within the Alexa Arena.Supplementary material regarding the data generation process is provided in Appendix E.</p>
<p>Visual Augmentations Starting from the images within the vision dataset, we create an additional 180k for training and 88k instances for validation, where each instance is an (image, instruction, action) triplet.When generating target actions, we used the ground truth positions of the agent and the object to ensure the agent was in proximity.For synthetic instructions, we used ChatGPT (OpenAI, 2022) as a starting point to create templates.To select diverse visual scenes, we used CLIP (Radford et al., 2021) to embed the images.We then applied k-means clustering, where the number of clusters is determined by the maximum number of action-object instances.Finally, we selected the image that is closest to each centroid.</p>
<p>CDF Augmentations While the visual augmentations are useful for single-step interactions, they cannot help the model on longer trajectories requiring consecutive interaction with the environment.To address this challenge, we use one of the early variants of EMMA to collect more training trajectories.For each mission, we provide the agent with step-by-step instructions and only include successful trajectories in our training data.We refer to these trajectories as CDF augmentations following the challenge definition format of the Alexa Arena (Gao et al., 2023).Overall, we sampled 38k missions for training and 15k for validation.These missions include: 1) picking up and placing objects from receptacles or containers; 2) picking up objects under the presence of objects acting as distractors (i.e., the same object but with a different color, or the same object placed in a different position); 3) cleaning and filling objects in the sink; and 4) interacting with domain-specific objects (e.g., the color changer and the time machine).</p>
<p>Experiments &amp; Results</p>
<p>VLP Evaluation</p>
<p>We evaluate EMMA on four image-based downstream tasks including image captioning, visual question answering, referring expression comprehension, and natural language for visual reasoning.We treat all tasks as language generation and finetune EMMA separately on each downstream task without introducing any task-specific parameters.</p>
<p>To evaluate the ability of our model to generate image captions, we use the MS-COCO dataset (Lin et al., 2014) and report evaluation results on the Karpathy test split (Karpathy and Fei-Fei, 2015) for BLEU-4 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016).For visual question answering, we report the VQA accuracy (Antol et al., 2015) on the test-std set of the VQA-v2 dataset (Goyal et al., 2017).</p>
<p>We also evaluate referring expressions comprehension on RefCOCOg (Mao et al., 2016) -a visual grounding task that requires selecting the region described by a given phrase.We use the visual grounding prompts from our pretraining and ask the model to generate the visual token id that matches the given phrase in the image.A predicted region is correct if the intersection over union with Table 1: Performance of the pretrained model on downstream image-based tasks.We report the number of pretraining samples as the number of image-text pairs.OFA uses additional vision-only and language-only data.</p>
<p>the ground truth region is larger than 0.5.</p>
<p>In the NLVR 2 dataset (Suhr et al., 2019), given a caption and two images, the model must predict whether the caption matches both images.We provide the two images directly to the model and prefix the caption with the image-text matching prompt.The model then predicts either true or false.</p>
<p>Table 1 illustrates the results of our model.We compare EMMA against similar VL modelsi.e., single-stream encoder-decoder models like VL-T5 and VL-BART (Cho et al., 2021), UniTab (Yang et al., 2022), and OFA (Wang et al., 2022b).Among these models OFA-base achieves state-ofthe-art performance, however, it has 1.37× more parameters than EMMA and is pretrained with nearly double the amount of data.On RefCOCOg, our model achieves competitive performance with UniTAB, even though it is almost half the size.Cho et al. (2021) hypothesize that the reason for the poor performance of VL-BART on RefCOCOg is that the use of absolute positional embeddings leads to memorization during training.However, EMMA achieves competitive performance despite using absolute positional embeddings.We believe this is due to our sampling procedure that takes into account the dataset imbalance in pretraining (see Appendix A for details).</p>
<p>Interactive Task Completion</p>
<p>We compare two variants of EMMA: a modular agent with two experts that are each fine-tuned on the downstream-specific tasks (i.e., CR &amp; AE), and a unified model fine-tuned with all the tasks.As shown in Table 2, both agent variants outperform the baseline models.Additionally, the modular agent performs on par with the best-performing models on the leaderboard, while the unified agent slightly outperforms the state-of-the-art.We believe that the synergy between tasks provides an advantage to the unified model.We also compare the success rate under three conditions for the inclusion of clarifications: 1) no clarifications, 2) clarifications whenever they are available for an instruction, 3) clarifications whenever they are available, and the CR predicts an ambiguous referent, that is <act><multiple matches> is predicted.We observe that both versions of EMMA improve when clarification exchanges are added to the input.Performance gains are marginal in the CR case, however, the agents benefit substantially when clarifications are always provided.Note that the role of the CR task is not to decide when to clarify but to drive the agent to either act or search the environment.On top of that, during training, the agent learns to rely on all available clarification types.</p>
<p>Performance per Mission Type Table 3 shows the performance of the modular and the unified agents in every mission category along with the average number of ground-truth actions required   by the expert planner to solve the mission.Both agents perform well on small trajectories, whereas for longer trajectories the unified approach yields better performance.Clarifications play a key role across all mission types with the exception of scanObject missions, a behavior which is also shown in baseline models (Gao et al., 2023).Interestingly, both agents are completely unable to solve color&amp;deliver missions.Based on the predictions of the model, the main challenge is finding the correct receptacle at the end of the trajectory, which can be easily confused with other similar objects.).The modular agent benefits more from reference and location clarifications, while the unified agent benefits from reference and direction clarifications.We hypothesize that the transfer between tasks enables the unified agent to perform well on instructions even without the location clarifications, thereby the clarifications provide only small improvements.</p>
<p>Impact of Clarification</p>
<p>Data Ablations</p>
<p>We also provide ablations in the dataset that showcase the effectiveness of our augmentations by ablating the visual as well as the CDF augmentations. Figure 4 depicts the performance of the unified model for both ablations.We observe that the model benefits from both augmentations.Vision augmentations provide performance gains from very early stages since they enable certain properties of the agent (e.g., <act><no match>).On the other hand, CDF augmentations provide a steady boost as they enable the agent to solve missions that require longer trajectories.Note that our pretraining consists of image-caption objectives that enable the model to learn to ground text in singular frames.Learning frame-visual token associations is obtained during fine-tuning only.Future work could explore agents pretrained on tasks that favor learning this ability as well.7 Performance Analysis DTC Error Analysis We inspected the trajectories of our unified model for 120 unsuccessful trajectories (ten per mission type).Overall, the main source of errors stems from the output of the contextual routing component.This task plays a key role in our model since an incorrect <act><one match> prediction triggers the action execution task.In the follow-up timestep, the agent is positioned differently from what is expected and it is forced to act on the environment, which likely leads to an unrecoverable situation.For instance, the agent may be facing an empty desk while being asked to pick up a mug.Consequently, the agent acts in unpredictable ways, resulting in a state from which the agent cannot recover.The incorrect predictions affect mostly <x>&amp;deliver missions, where the agent completed the <x> subgoal, however, failed on the deliver subgoal as it was unable to disambiguate between multiple receptacles.Furthermore, the output of the action execution task is also susceptible to errors.In long trajectories, we observed a few cases where the model seems to lack temporal understanding.For example, when interacting with objects on consecutive timesteps the model used a visual token to reference the object that matched one of the previous frames in the trajectory.We also noticed a few error cases due to a lack of commonsense reasoning.Some instructions describe properties of an object, like its color and shape without explicit reference to the object but our agent could not make the associations between an object and its properties.</p>
<p>Visual Domain Transfer</p>
<p>We inspect the model's ability to transfer the action prediction task to real images.We observe qualitatively that after finetuning the object detector struggles to detect objects from classes that are not present in the Alexa Arena.However, after reverting back to the base object detector, the EMMA-unified model is able to make reasonable predictions as shown in Figure 5.To quantify the improvement of using the base object detector, we use scene graph annotations of GQA (Hudson and Manning, 2019) by creating synthetic instructions, as described in the Appendix H. Table 5 shows the accuracy for single-step instructions using real images where the target object can be from any class or the subset of classes that appear in the Alexa Arena.Although EMMAunified has been fine-tuned on the action prediction task with image inputs from the fine-tuned object detector, we see 26.48% absolute improvement when switching to the base object detector.We leave further exploration of the potential for Sim2Real transfer as future work.</p>
<p>Conclusion &amp; Future Work</p>
<p>We described EMMA, a unified and multitask model for embodied task completion.EMMA performs on par with VLP models of comparable size in VL benchmarks, and also achieves a new state-of-the-art performance on the DTC benchmark (Gao et al., 2023).We decouple the downstream embodied task into three complementary tasks all solved by the same unified model.Finally, EMMA benefits from interaction with users in the form of clarifications that are crucial for the agent's success, particularly for referential disambiguation.</p>
<p>In terms of future work, our experiments show that trajectory augmentations are required in order to learn frame and visual token associations.However, acquiring additional data might be prohibitive for target applications where an environment simulator is not available.This requirement could be alleviated by introducing the concept of trajectories in the pretraining.Furthermore, given our visual domain transfer analysis, it would be interesting to explore if visual encoders specialized to various target domains can be plugged into the same multimodal model to allow cross-domain transfer.</p>
<p>Limitations</p>
<p>We focused on the development of a dialog-enabled agent within an embodied environment.Our design aimed at creating an agent that is as autonomous as possible without relying on external components to support it.We took one step away from modular agents with individual components to a single model that facilitates all tasks.Nevertheless, the search routine is an external component that our agent depends upon.In theory, an agent could learn low-level actions in order to search for an object.However, this is a challenging problem that would require an excessive amount of demonstrations even for simple instructions.Consider the case where the model has to search within multiple containers (e.g., cabinets) for the required object (e.g., a spoon).This would require the agent to learn to summarize the state from the context of history and recognize which cabinets it has already visited before opening the next cabinet.Finally, as shown by our results, our agent does not fully model dialog actions.We used the output of the contextual routing task to request clarifications, however, we did not emphasize on the type of clarifications when making a request.An interesting follow-up work would be to explore how an agent can further incorporate dialog actions similar to the ones supported in the environment and pose clarification requests when necessary.</p>
<p>Ethics Statement</p>
<p>Embodied AI and physical robots have some wellknown ethical trade-offs.On the one hand, they can generate a positive impact, e.g. as part of care settings or helping in disaster zones.On the other hand, they can also lead to negative sociotechnical consequences, such as job displacement or dis-empowerment of individuals (Müller, 2021).</p>
<p>Embodied agents based on large-scale VLP inherit additional challenges -some of which they share with large language models, including hallucinations, discrimination and exclusion, malicious uses, and environmental harms (Bender et al., 2021;Dinan et al., 2022;Weidinger et al., 2022).</p>
<p>Furthermore, their multimodal nature creates additional privacy concerns, especially when developing systems for assistive home settings, as is the case in EMMA.In order to address the need for large amounts of in-domain data, EMMA is developed using simulated environments.However, transferring the learned behavior to real-world environments is still an open challenge.For example, VLP models can be prone to distribution shifts, e.g. through naturally occurring noise (Yu et al., 2023), or targeted attacks (Yu and Rieser, 2023).Consequently, embodied agents based on VLP may exhibit unintended behaviors when operating in complex real-world environments.These failures are especially grave when interacting with vulnerable groups, as is the case for care assistants.</p>
<p>It is therefore paramount that these models are released in a responsible way (Bergman et al., 2022), developed, and tested in ecologically valid setups in order to anticipate real-world impact (De Vries et al., 2020).Addressing these issues responsibly and proactively is essential to maximize the benefits while minimizing the potential negative impacts on individuals and society as a whole.</p>
<p>A Pretraining Details</p>
<p>Vision Encoder For encoding images, we use VinVL (Zhang et al., 2021), as it has showed strong performance on established VL benchmarks, keeping it frozen for pretraining and only fine-tuning it on the Alexa Arena.</p>
<p>Pretraining Dataset We pretrain our model on publicly available VL datasets including COCO captions (Lin et al., 2014), Conceptual Captions 3M (Sharma et al., 2018), GQA (Hudson and Manning, 2019), VQA (Goyal et al., 2017), and Visual Genome (Krishna et al., 2017).Table 6 shows the statistics for our compiled pretraining corpus.Overall we used 10.4M examples on 3.3M different images.We used custom task-specific prefixes for each task.Table 7 illustrates example input-output pairs for each pretraining task, while Table 8 illustrates the prompts used for each task.</p>
<p>Model Pretraining Strategy During pretraining, we apply teacher forcing and compute the crossentropy loss between the predicted and target token.As the typical length of the target prediction varies per task, we ensure the losses across tasks are comparable in scale by averaging the loss by the target sequence length first and then by the number of samples in the batch.</p>
<p>We employ a 'mixed batches' pretraining scheme where each batch contains examples sampled from any task.Assuming that task i has n i examples, the probability of sampling an example from task i from all j tasks is p i = n i / j n j .However, as shown in Table 6, the pretraining tasks have a large variance in the number of available examples, which can lead to poor performance on low-resource tasks.Therefore, similar to Raffel et al. ( 2020), we re-adjust the probability p i by limiting the maximum number of examples allowed per task.The limit is controlled by a ratio R, which defines how many more samples are included in training task i versus the task with the smallest quantity of examples n min .Therefore, ∀j tasks, the final probability of sampling an example from task i becomes pi = min(n i , R × n min )/ j min(n j , R × n min ).</p>
<p>In our experiments we set R = 3.</p>
<p>B Implementation Details</p>
<p>Pretraining Setup We pretrain our model for 100k steps using a batch size of 2048 and the AdamW optimizer with weight decay of 0.01.We apply a linear learning rate schedule with warm-up for 10K steps and a maximum learning rate of 1e-4.The pretrained model was trained on 8 NVIDIA Tesla V100 GPUs.</p>
<p>Vision Encoder For the Alexa Arena, we finetune the VinVL pretrained checkpoint for 300K steps with a batch size of 4. We set the base learning rate to 10 −4 and weight decay to 10 −5 with an SGD optimizer, decaying the learning rate by 0.1 after steps 55K and 75K steps.During training, we use the default preprocessing and image transformations (Han et al., 2021).The model is trained on 4 RTX 2080 Ti GPUs.</p>
<p>Fine-tuning EMMA on VL downstream tasks For all tasks, we fine-tune the pretrained model using LM loss for up to 20 epochs.</p>
<p>Fine-tuning EMMA on Alexa Arena A key feature of EMMA is the ability to predict a 'negative' output for the contextual routing and the visual grounding task.For example, the agent can output <act><no match> apple or no apple when trying to pick up or find an apple.Since the DTC data do not include these types of outputs, we use visual and CDF augmentations to simulate these instances.In particular, with a probability 50% we convert a 'positive' instance (an instance that the synthetic instruction matches with the visual scene), into a 'negative' one by selecting an image from the train set where the target object is missing.</p>
<p>For the modular and unified models, we fine-tune our pretrained model for 10K steps, using crossentropy loss and teacher-forcing.We use a batch size of 256 and the AdamW optimizer with learning rate 10 −4 , weight decay 0.01, a linear learning rate schedule with 1K warmup steps, and 0.1 label smoothing.We shuffle the identities of the visual tokens for each frame.The model is trained on a single RTX 2080 Ti GPUs.</p>
<p>C DTC Benchmark</p>
<p>The DTC benchmark contains 2661 missions in training and 383 in validation.Each mission is annotated by three separate annotators.Each human annotation corresponds to a single episode, meaning that there are 7983    action.The agent also performs manipulation actions on each object which include PickUp, Place, Open, Close, Toggle, Fill, Clean, Pour, Break, and Scan.The set of supported actions for each object are determined based on its affordances.In total, there are 14 affordances: pickupable, openable, breakable, receptacle, toggleable, powerable, dirtyable, heatable, eatable, chillable, fillable, cookable, decor, and infectable.</p>
<p>D Search Routine</p>
<p>We focused on the development of a dialogueenabled agent within an embodied environment.</p>
<p>Our design aimed at creating an agent that is as autonomous as possible without relying on external components to support it.Nevertheless, the search routine is an external component that our agent depends upon.Learning how to search for an object is a challenging problem that would require an excessive amount of demonstrations.The search pipeline is triggered when the output of the contextual routing task requires the agent to search for an object or to interact with an object that is not in the agent's view.The agent searches the current room by iterating through selected viewpoints -including the agent's original position.At each viewpoint, we collect a panoramic view by rotating left by 90 degrees three times.</p>
<p>Since each room has a maximum of eight viewpoints, the search routine could amount to a large number of redundant steps.To mitigate this, the agent selects a subset of viewpoints.We assume that the original agent position and each viewpoint can cover an area of a fixed radius.This way, we create a graph where each candidate position is a node and add edges between nodes whose areas overlap.As a result, we turn the viewpoint selection into the Maximum Vertex Coverage problem and apply a greedy algorithm.We empirically set the radius to four, which leads to selecting up to two viewpoints depending on the room size.After preparing the search plan, the agent starts executing it step-by-step.We use the visual grounding task to localize a referenced object within each new frame.The agent continues to execute the search plan until the object is found or the plan is exhausted.</p>
<p>To minimize the number of search steps, we also keep track of the objects observed at each position.If any of the object labels from the vision model is not present in the memory, we create a new entry that stores the object label, its bounding box area, and the closest viewpoint.Since we are not using depth estimation, we used the area as an approximation of distance.The memory is queried at the beginning of the search routine.If the target object is in memory for the current room, the agent will consider the retrieved viewpoint as the starting position for the search.</p>
<p>A limitation of our setup is that our routine does not keep track of the spatial positions of an object.Some objects may be inside containers like a fridge, Task Prompts MLM "Denoise: {caption}" "Denoise the statement: {caption}" "Denoise the description: {caption}" "Reconstruct: {caption}" "Reconstruct the description: {caption}" "Reconstruct the statement: {caption}" ITM "Assess the statement: {statement}" "Assess the description: {statement}" "Evaluate the statement: {statement}" "Evaluate the description: {statement}" Captioning "Caption this" "Caption the image" "Caption this image" "Describe this" "Describe the image" "Describe this image" Dense Captioning "Caption {region}" "Caption object {region}" "Describe {region}" "Describe object {region}" Visual Grounding "Find the object: {caption}" "Locate the object: {caption}" "Pick the object: {caption}" "Select the object: {caption}" VQA "Answer: {question}" "Answer the question: {question}" "What is the answer to: {question}" "What is the answer to the question: {question}" Relation Detection "Explain the relationship between: {subject} and {object}" "Explain how {subject} relates to {object}" "Describe the relationship between {subject} and {object}" "Describe how {subject} relates to {object}" Action Execution "Act according to the instruction: {instruction}" "Execute the instruction: {instruction}" "Follow the instruction: {instruction}" Countextual Routing "Predict the system act: instruction" Table 8: Task prompts used for each pretraining and downstream task.</p>
<p>or a cabinet.If the search routine is triggered after the object has been observed inside a container then the agent would need first to interact with the container before retrieving the object.A more sophisticated semantic search policy (Blukis et al., 2018a(Blukis et al., ,b, 2022;;Chaplot et al., 2020;Min et al., 2021) would likely translate to better performance.</p>
<p>E Data Augmentations</p>
<p>Visual Augmentations We leveraged the images used to fine-tune the object detector to create synthetic instances.Figure 6 shows a comparison between the distribution of objects from the DTC and our synthetic dataset.Our synthetic dataset includes objects that are not used within the DTC benchmark.To prevent overpopulation of the synthetic dataset with objects that occur frequently (tables and desks), we set a maximum number of objects for each action.Table 9 shows the maxi- mum number of each object used for every action used for train and validation.Each image contains ground truth information regarding the position of the agent, the position of the objects as well as their current states.We used the ground truth metadata to ensure the credibility of the synthetic instances.First, we used the distance between the agent and the object to be less than the minimum interaction distance defined by the Alexa Arena.For search instances, we omit this requirement since we want the agent to be able to find any object that is visible from the current view but arbitrarily far away from its position.Second, we used the states of the objects to determine if an action can be executed on an object.For example, the close action on a fridge cannot be performed if the fridge is already closed.The goto and scan actions are executable regardless of the state of the object.In these cases, we ignore the preconditions regarding the state of the object.</p>
<p>CDF Augmentations</p>
<p>The objective of the CDF augmentations is to help the agent learn frame and visual token associations.The DTC benchmark includes long trajectories but from our preliminary experiments, we found that the model was not learning these associations, particularly as the number of steps for a single instruction increased.</p>
<p>In order to create CDF augmentations we deployed one of the earliest versions of EMMA in the Alexa Arena.We created missions similar to those in the DTC benchmark and manually wrote low-level instructions so that the model could complete the missions with one or few-shot interactions without having to perform long trajectories.When creating a mission we randomly sampled objects equipped with the affordances that we are interested in for the specific mission.To ensure variety within the missions, we randomized the layout of the room, the position and the color of objects in the layout, as well as the position of the agent at the start of the mission.</p>
<p>We collected successful mini-episodes and used them to train the model.Note that these miniepisodes were hard for the initial version of EMMA and were only feasible due to external guidance, such as the search routine.</p>
<p>Paraphrazable Instructions To create instructions for both visual and CDF augmentations we used ChatGPT as a starting point.Examples of interactions with the ChatGPT are shown in Figure 7.To create the high-level instruction for each CDF trajectory, we merged the low-level instructions and asked ChatGPT to provide a high-level one.We did not use any sophisticated prompt engineering techniques.We only provided introductory sentences along with a few examples of semantically similar instructions and asked the model to provide paraphrases.Some of the generated paraphrases do not match the target domain.For example, to clean the bowl in the sink, the agent does not require soap.We manually filtered these out before compiling our set of instructions.</p>
<p>F Experiments</p>
<p>Object category</p>
<p>Baseline Ours Small (0-   Task-specific Prefixes Previous approaches have shown that variations in the text prompt can affect the results on downstream performance (Cho et al., 2021;Gao et al., 2021;Radford et al., 2021).We initially experimented with two types of prefixes: task-specific special tokens and natural language prompts.Task-specific special tokens are singleword descriptors (Cho et al., 2021), while natural language prompts are longer, varied descriptions similar to the approach proposed by Sanh et al. (2022).For example, the image captioning task is denoted either by the single token [Cap] or by prompts such as 'Describe this' or 'Caption the image'.Before pretraining our EMMA-base we explored this design choice using an EMMA-small variant following BART architecture (Lewis et al., 2020).In particular, we observed the validation loss of the model with tag and text prefixes after pretraining for 75K steps.The model using textual prefixes was slightly outperforming the model using tags (0.818 vs 0.824) which aligns with previous findings (Cho et al., 2021).For this reason, we chose to use natural language prompts but also they allow for a more flexible interface with the model which is especially useful if the downstream task involves language variety and complexity.</p>
<p>Data Ablations When training solely on the DTC data, the baseline model outperforms EMMA.</p>
<p>To verify that our approach scales better in terms of the data, we also experiment with applying our augmentations to the existing baseline.We train the baseline model using the same set of hyperparameters as in (Gao et al., 2023) on the DTC data and vision augmentations.Then, we compare it against our model trained on the same data (see red curve in Figure 4).The baseline model achieves a 34.42% success rate, a small performance boost as opposed to training with only the DTC data, while our model benefits substantially from these augmentations achieving a success rate of 34.72%.</p>
<p>Model Scaling</p>
<p>We also experiment with a small variant of our model (3 encoder layers, 3 decoder layers, 8 attention heads, 368 hidden size, 512 feedforward size) with a total of 20M parameters and the base model trained from scratch on the entire dataset.The performance of the small variant is 32.11%, while the base model trained from scratch performed poorly with a success rate of 5.2%.Our results indicate that scaling the model's size up provides a substantial performance increase and highlights the necessity of pretraining.</p>
<p>Object Detector Comparison We validate the performance of our vision encoder by comparing it against the baseline (Gao et al., 2023).We evaluate our object detection model using the standard COCO evaluation metric, the Mean Average Precision (mAP), calculated by averaging the precision at IoU thresholds ranging from 0.5 to 0.95 in steps of 0.05.Similar to Gao et al. (2023), we set the maximum detection proposals to 100 for evaluation.Table 10 compares our vision encoder with the baseline across different object sizes as defined in (Gao et al., 2023).Our model achieves strong performance with approximately 40% relative improvement for all object sizes.We note, however, that our vision encoder is not directly comparable to the baseline since the baseline model is trained on fewer classes (86 vs 133) and also performs segmentation instead of object detection.</p>
<p>Offline Contextual Routing Performance Section 7 concludes that CR is one of the main sources of errors because as the first step in completing an instruction, an error will have cascading effects.</p>
<p>We evaluate offline the model performance on the CR task using the validation set of DTC.Table 11 shows the accuracy and macro-average F1 score for the model predictions of the CR-specific tokens.Both the modular and unified models achieve high scores.However, assuming that action execution and search are performed without errors, the probability of mission success for a mission with 5 instructions is (1 − 0.03) 5 = 0.85.</p>
<p>G Examples of trajectories</p>
<p>In Section 7 we highlighted the main source of errors; the output of the contextual routing task and often the lack of temporal understanding when our model acts on the environment.We observed that when performing the contextual routing task the model can be confused when there are multiple object candidates in the scene that could be suitable candidates for an instruction.When performing multiple actions over the same object, the model may use a token to reference the object in a previous timestep as opposed to the current one.We show two examples of such errors in Figure 9.</p>
<p>H Visual Domain Transfer</p>
<p>We create a synthetic dataset based on scene graphs from the validation set of GQA (Hudson and Manning, 2019), which provides a cleaned version of Visual Genome scene graphs (Krishna et al., 2017).This toy dataset is created to estimate the model's ability to transfer the downstream task of action execution to the pretraining visual domain, and quantify the impact of using the base or fine-tune object decoder.We generate synthetic instructions by populating templates for the action types: go to, pick up, place, open, close.The templates are in the form of '{action verb synonym} the {optional attribute} {target object}', e.g.'head towards the wooden table'.We generate go to instructions for all objects, and use the affordances from Alexa Arena to determine if the object class is compatible with the remaining action types.We want to avoid cluttered images and ambiguous instructions.Therefore, we keep images with up to 10 objects and skip objects whenever multiple objects of that class appear in the image.We create a total of 57740 instructions.We also investigate the ability of the fine-tuned model to perform the pretraining tasks in the down-stream visual domain through qualitative examples.This requires the model not to forget the pretraining tasks and to demonstrate generalization to the visual domain of the downstream tasks.To improve model outputs, we prohibit the generation of added special tokens (frame, visual, stop and CR tokens).The examples in Figure 10 show that the fine-tuned model retrains VQA, dense captioning and relationship detection capabilities and transfers them to the simulated environment domain.Figure 11 shows some additional examples of the action prediction task on real images.</p>
<p>Figure 1 :
1
Figure1: Overview of the EMMA architecture.EMMA encodes vision and language inputs with modality-specific layers before providing the input to the bidirectional multimodal encoder.Scene embeddings act as global features for a frame.Object embeddings correspond to a combination of region features, object coordinates, and their identities within the frame sequence.Language input is concatenated with task-specific prompts.The shared autoregressive language decoder treats every task as a text generation task.</p>
<p>Subgoal 1 :
1
Grab the cereal from the fridge Subgoal 2: Find the bowl Subgoal 3: Pour cereal</p>
<p>Figure 2 :
2
Figure 2: Example trajectory where the objective is to pour cereal into the bowl.The user instructions and our agent response are shown on the left and right of each frame respectively.At each timestep the agent is provided with the current view, a user instruction (Input), and optionally a clarification question (Q) and answer (A).The Contextual Routing task (CR) determines whether the agent is going to use the Action Execution task (AE) to interact with the environment, or the Visual Grounding task (VG) to search for an object in the scene.</p>
<p>Figure 3 :
3
Figure 3: Example outputs for the Contextual Routing (CR) task.The first token determines whether the agent will search or act on the environment.The second token determines if the instruction matches an entity in the scene.Finally, for instructions that require manipulation of objects, the model generates the name of the object.</p>
<p>Figure 4 :
4
Figure 4: MSR against data ablations.The DTC line is the performance of the model after training exclusively on the DTC data, without any augmentations.Since the visual grounding task is not modeled by the DTC benchmark, we keep the grounding instances and only downsample the action execution instances.</p>
<p>Figure 5 :
5
Figure 5: Example transfer of the action prediction task to the visual domain of real images.</p>
<p>Figure 6 :
6
Figure 6: Distribution of objects used in our visual augmentations and the DTC benchmark.</p>
<p>Table 2 :
2MSR (↑)NRA (↓)QALeaderboard:GauchoAI36.47--SEAGULL30.98--Kingfisher22.37--Baseline:NS (Gao et al., 2023)19.3211.73✗NS (Gao et al., 2023)22.8012.73✓VL (Gao et al., 2023)18.1911.82✗VL (Gao et al., 2023)34.208.82✓EMMA:EMMA-modular33.768.91✗EMMA-modular33.959.05CREMMA-modular35.168.92✓EMMA-unified33.268.79✗EMMA-unified33.598.89CREMMA-unified36.818.69✓
Mission Success Rate (MSR) and the Number of Robot Actions (NRA) of EMMA against top-3 leaderboard and baseline models.CR denotes QA based on the CR task (<act><multiple matches>).</p>
<p>Table 3 :
3
MSR of EMMA for each mission category.</p>
<p>Table 4 :
4
Gao et al. (2023)on accuracy per question type for the modular and unified model.QA ✗ columns refer to the performance of the model on the same instructions but without clarification.andwithoutclarification.Table4illustrates the localization accuracy for the modular and the unified model across question types.We use the same taxonomy asGao et al. (2023)with the addition of 'other', which refers to out-of-scope clarifications (e.g, 'What should I deliver', while the agent already holds the mug and is instructed to place it on the desk).Both models benefit the most from description clarifications (e.g., 'What does the cartridge look like?', 'It is black with a white label'
EMMA-modularEMMA-unifiedQA ✗QA ✓GainQA ✗QA ✓GainDescription59.0566.40+7.3562.8467.98+5.14Direction73.4376.56+3.1379.4183.82+4.41Location62.2766.45+4.1864.2766.66+2.39Reference59.0564.09+5.0462.0266.56+4.54Other75.0075.00+0.0079.4179.41+0.00
Type Next, we are interested in identifying which clarifications help the model disambiguate object referents.Similarly to Chiyah-Garcia et al. (2022), we compare the object localization performance of the model with</p>
<p>Table 5 :
5
(Krishna et al., 2017)racy for real images from Visual Genome(Krishna et al., 2017).
Object DetectorArena ClassesNon-Arena ClassesOverallFine-tuned42.6532.1333.88Base67.8558.8560.35</p>
<p>To collect panoramic views from its position, the agent can perform the LookAround
training episodes and 1149 navigation validation episodes. Primitive actions include MoveForward, MoveBackward, RotateLeft, and # Samples Tasks 443K VQA 987K VQA 592K MLM, ITM, Captioning 3M MLM, ITM, Captioning 5.4M MLM, Dense Captioning, Visual Grounding, Relationship Detection RotateRight. Dataset # Images VQA-v2 (Goyal et al., 2017) 83K GQA (Hudson and Manning, 2019) 86K COCO Captioning (Lin et al., 2014) 118K Conceptual Captions (Sharma et al., 2018) 3M Visual Genome (Krishna et al., 2017) 108K Total 3.3M 10.4M</p>
<p>Table 6 :
6
Dataset statistics for pretraining.
TaskExample inputExample target outputMLMDenoise: Fridge <MASK> is openFridge door is openITMAssess the statement: Fridge door is openTrueCaptioningDescribe the imageFood inside a refrigerator with its door openDense CaptioningDescribe object <visual_token_5>Silver fridgeVisual GroundingLocate the milk carton<visual_token_3>VQAWhat color are the cabinets?WhiteRelationship DetectionExplain how <visual_token_3> relates to <visual_token_5>Milk inside of fridge</p>
<p>Table 7 :
7
Example input and output formats used for the pretrained tasks.</p>
<p>Table 9 :
9
Maximum number per object for each action.Search instances are used for multimodal grounding which is not supported by the DTC benchmark.
Train ValidationBreak750400Clean400200Close750400Fill750400Goto750400Open750400Pickup750400Place750400Pour750400Scan400200Search750500Toggle750400</p>
<p>Table 10 :
10
Object detection results for small, medium and large objects.The allowed area of an object in each category is shown in parentheses.
pickupplace37.6%20.7%pickup&amp;ambiguousclean5.6% 4.4% fill 7.2%24.4%interactFigure 8: Distribution of CDF augmentations during training.ModelAccuracyF1EMMA-modular0.9683.33EMMA-unified0.9784.67
Figure 7: Examples of using ChatGPT as a starting point for paraphrasing goals and subgoals within the Alexa Arena.</p>
<p>Table 11 :
11
Offline evaluation of the CR task.</p>
<p>AcknowledgementsWe would like to thank the Alexa Prize team and Amazon Science for their crucial technical and administrative support, along with the resources that greatly contributed towards model development.This work also used the Cirrus UK National Tier-2 HPC Service at EPCC funded by the University of Edinburgh and EPSRC (EP/P020267/1).Instruction: fill the pot with water from the sink Paraphrase: fill the pot on the kitchen sink Instruction: fill the pot with water from the sink Paraphrase: fill the pot with water Write 5 more paraphrases of the instruction fill the pot with water from the sink Sure, here are 5 paraphrases of the instruction fill the pot with water from the sink:1. Add water to the pot from the sink 2. Use the sink to fill the pot with water 3. Take water from the sink and put it in the pot 4. Pour water from the sink into the pot 5. Put water in the pot using the kitchen sink You are responsible for instructing a robot to clean a plate in the sink.You need to come up with a few instructions that are creative, simple, and straightforward.Here are a few examples:1. clean the plate on the sink 2. rinse the plate on the sink 3. wash the plate on the sink 4. use the sink to clean up the plate Write 5 instructions that have the same meaning.The instructions must be different from each other Sure, here are 5 different instructions to clean a plate in the sink:1. Rinse the plate thoroughly in the sink to remove any food debris.2. Clean the plate using the sink's water and cleaning solution.3. Crush the mug with the hammer.4. Use the sink to remove any dirt or food particles from the plate.5. Use soap and water to clean the plate in the sink.Imagine a special machine that can be used to turn an object into another object.You need to come up with creative ways of instructions, where somebody must follow to repair a broken bowl.Here
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Spice: Semantic propositional image caption evaluation. Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould, 10.1007/978-3-319-46454-1_24Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringer2016. October 11-14, 2016Proceedings, Part V 14</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu ; Dhruv, Lawrence Batra, Devi Zitnick, Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMargaret Mitchell,. 2015</p>
<p>Vlmo: Unified vision-language pre-training with mixture-ofmodality-experts. Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, Furu Wei, Advances in Neural Information Processing Systems. 202235</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21Association for Computing Machinery2021</p>
<p>Guiding the release of safer e2e conversational ai through value sensitive design. Stevie Bergman, Gavin Abercrombie, Shannon Spruit, Dirk Hovy, Emily Dinan, Y-Lan Boureau, Verena Rieser, Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics. the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics2022</p>
<p>Following highlevel navigation instructions on a simulated quadcopter with imitation learning. Valts Blukis, Nataly Brukhim, Andrew Bennett, Ross A Knepper, Yoav Artzi, arXiv:1806.000472018aarXiv preprint</p>
<p>Mapping navigation instructions to continuous control actions with position-visitation prediction. Valts Blukis, Dipendra Misra, Ross A Knepper, Yoav Artzi, Conference on Robot Learning. PMLR2018b</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, Conference on Robot Learning. PMLR2022</p>
<p>Emerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Object goal navigation using goal-oriented semantic exploration. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 202033</p>
<p>Uniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, 10.1007/978-3-030-58577-8_7Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer2020. August 23-28, 2020Proceedings, Part XXX</p>
<p>Exploring multi-modal representations for ambiguity detection &amp; coreference resolution in the simmc 2.0 challenge. Francisco Javier, Chiyah- Garcia, Alessandro Suglia, José Lopes, Arash Eshghi, Helen Hastie, arXiv:2202.126452022arXiv preprint</p>
<p>Unifying vision-and-language tasks via text generation. Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal, International Conference on Machine Learning. PMLR2021</p>
<p>De Harm, Dzmitry Vries, Christopher Bahdanau, Manning, arXiv:2007.14435Towards ecologically valid research on language user interfaces. 2020arXiv preprint</p>
<p>Safetykit: First aid for measuring safety in open-domain conversational systems. Emily Dinan, Gavin Abercrombie, Stevie A Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, Verena Rieser, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221Association for Computational Linguistics</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Qiaozi Gao, Govind Thattai, Xiaofeng Gao, Suhaila Shakiah, Shreyas Pansare, Vasu Sharma, Gaurav Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zheng, arXiv:2303.01586Alexa arena: A user-centric interactive platform for embodied ai. 2023arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021. Association for Computational Linguistics2021</p>
<p>Dialfred: Dialogue-enabled agents for embodied instruction following. Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S Sukhatme, IEEE Robotics and Automation Letters. 742022</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Towards general purpose vision systems: An end-to-end task-agnostic vision-language architecture. Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Image scene graph generation (sgg) benchmark. Xiaotian Han, Jianwei Yang, Houdong Hu, Lei Zhang, Jianfeng Gao, Pengchuan Zhang, 2021</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019</p>
<p>Vima: General robot manipulation with multimodal prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Fortieth International Conference on Machine Learning. 2023</p>
<p>Deep visualsemantic alignments for generating image descriptions. Andrej Karpathy, Li Fei-Fei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Improving embodied instruction following with deterministic methods. Dahyun Kim, Yong-Ju Lee, 2023 25th International Conference on Advanced Communication Technology (ICACT). IEEE2023</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, International Conference on Machine Learning. PMLR2021</p>
<p>Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, 10.1007/S11263-016-0981-7Proceedings of the second workshop on statistical machine translation. the second workshop on statistical machine translation2017. 2007123Visual genome: Connecting language and vision using crowdsourced dense image annotations</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBart2020</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Align before fuse: Vision and language representation learning with momentum distillation. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven Chu, Hong Hoi, Advances in neural information processing systems. 202134</p>
<p>Oscar: Object-semantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, 10.1007/978-3-030-58577-8_8European Conference on Computer Vision (ECCV). 2020</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, 10.1007/978-3-319-10602-1_48European conference on computer vision. Springer2014</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Lebp-language expectation &amp; binding policy: A two-stream framework for embodied visionand-language interaction task learning agents. Haoyu Liu, Yang Liu, Hongkai He, Hangfang Yang, arXiv:2203.046372022arXiv preprint</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in Neural Information Processing Systems. 2019</p>
<p>Instruction Clarification Requests in Multimodal Collaborative Dialogue Games: Tasks, and an Analysis of the CoDraw Dataset. Brielen Madureira, David Schlangen, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Generation and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. James L Mcclelland, Felix Hill, Maja Rudolph, Jason Baldridge, Hinrich Schütze, 10.1073/pnas.1910416117Proceedings of the National Academy of Sciences. 117422020</p>
<p>Film: Following instructions in language with modular methods. So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov, International Conference on Learning Representations. 2021</p>
<p>Don't copy the teacher: Data and model challenges in embodied dialogue. So Yeon, Min , Hao Zhu, Ruslan Salakhutdinov, Yonatan Bisk, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Ethics of Artificial Intelligence and Robotics. C Vincent, Müller, The Stanford Encyclopedia of Philosophy. Edward N Zalta, 2021Metaphysics Research Lab, Stanford UniversitySummer 2021 edition</p>
<p>Introducing ChatGPT. 2022OpenAI</p>
<p>Teach: Task-driven embodied agents that chat. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational Linguistics2002</p>
<p>Episodic transformer for vision-and-language navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. Noam Shazeer, Adam Roberts, Katherine Lee,2021. 202021PMLR. Colin Raffel</p>
<p>Multitask prompted training enables zeroshot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, The Tenth International Conference on Learning Representations. 2022</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics20161</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, International Conference on Learning Representations. 2021</p>
<p>Flava: A foundational language and vision alignment model. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022a</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, Workshop on Language and Robotics at CoRL. 2022b. 2022</p>
<p>Factorizing perception and policy for interactive instruction following. Pratap Kunal, Suvaansh Singh, Byeonghwi Bhambri, Roozbeh Kim, Jonghyun Mottaghi, Choi, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme, arXiv:2108.049272021arXiv preprint</p>
<p>A corpus for reasoning about natural language grounded in photographs. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Lxmert: Learning cross-modality encoder representations from transformers. Hao Tan, Mohit Bansal, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Multimodal few-shot learning with frozen language models. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, Oriol Eslami, Felix Vinyals, Hill, Advances in Neural Information Processing Systems. 202134</p>
<p>CIDEr: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Git: A generative image-to-text transformer for vision and language. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, Transactions of Machine Learning Research. 2022a</p>
<p>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, International Conference on Machine Learning. PMLR2022b</p>
<p>Taxonomy of Risks posed by Language Models. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, 10.1145/3531146.35330882022 ACM Conference on Fairness, Accountability, and Transparency. ACM2022</p>
<p>Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, Lijuan Wang, arXiv:2111.12085Crossing the format boundary of text and boxes: Towards unified vision-language modeling. 2021arXiv preprint</p>
<p>Unitab: Unifying text and box outputs for grounded vision-language modeling. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, Lijuan Wang, 10.1007/978-3-031-20059-5_30Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringer2022. October 23-27, 2022Proceedings, Part XXXVI</p>
<p>Quality-agnostic image captioning to safely assist people with vision impairment. Lu Yu, Malvina Nikandrou, Jiali Jin, Verena Rieser, International Joint Conference on Artificial Intelligence. 2023</p>
<p>Adversarial robustness of visual dialog. Lu Yu, Verena Rieser, Findings of the Association for Computational Linguistics (ACL). 2023</p>
<p>Vincent Vanhoucke, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, arXiv:2204.00598arXiv preprint</p>
<p>Vinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>
<p>Pound the mug into pieces with the hammer. </p>
<p>Strike the mug with the hammer to break it. </p>
<p>Crush the mug with the hammer. </p>
<p>Break the mug by hitting it with the hammer. </p>
<p>Take the hammer and smash the mug. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1822 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1822</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1822</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0" target="_blank">Qwen Technical Report</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts, and includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1822.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1822.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QWEN Reward Model (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QWEN Reward Model (preference / human-feedback reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned reward model based on the QWEN architecture trained to predict human pairwise preferences over model responses; used for RLHF and evaluated against human annotation datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>learned reward model / preference model (automated classifier of pairwise human preference)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>QWEN-based reward model (same-sized QWEN architecture with pooling layer)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>pairwise-comparison scoring (models score which of two responses is preferred)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>model responses / chat outputs (includes code, math, general conversational responses)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general (mixed; includes code and math prompts among other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>human preference (helpfulness/overall preference) as expressed in pairwise comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators scored multiple model responses following a standard annotation guideline; comparison pairs built from annotator scores</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators (not otherwise specified as expert developers)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>pairwise accuracy (reward model prediction vs human annotated pairwise preference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Reported test pairwise accuracies (PMP and RM) across multiple human-preference datasets. Example values: QWEN Helpful-base PMP=62.68%, RM=74.78%; QWEN Helpful-online PMP=61.62%, RM=69.71%; Anthropic Helpful-base PMP=76.52%, RM=73.98%; Anthropic Helpful-online PMP=65.43%, RM=64.57%; OpenAI Summ. PMP=69.60%, RM=69.99%; Stanford SHP PMP=60.05%, RM=60.10%; OpenAI PRM800K PMP=70.59%, RM=70.52%.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When reward model is fine-tuned on high-quality, in-distribution human preference datasets (e.g., QWEN-specific reward datasets), accuracy improves relative to pretraining (PMP).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Out-of-distribution settings and some external datasets show smaller gains or mixed results (e.g., Anthropic and some online datasets where RM improvements are smaller or slightly lower), indicating dataset shifts reduce alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not explicitly quantified for code-only artifacts; evaluation spans mixed response types — RM shows gains on the QWEN-specific datasets which include diverse prompts, but no direct breakdown showing complexity-vs-agreement for software artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Implicit: higher-quality, well-annotated pairwise labels (clear annotation guidelines and curated prompt sampling) improved RM performance; authors emphasize balanced sampling and annotation guidelines improved training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Measured via pairwise accuracy: RM predictions compared against human pairwise labels (values reported in agreement_score); no explicit comparison reported between RM-human agreement and human-human inter-rater agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Yes — the reward model was pretrained via preference model pretraining (PMP) on large comparison datasets and then fine-tuned on higher-quality human-annotated preference data; balanced sampling across ~6600 prompt tags and diverse model-response generation strategies were used.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A QWEN-based reward model substantially improves pairwise accuracy on the authors' QWEN preference datasets compared to PMP (e.g., +~12 percentage points on QWEN Helpful-base), showing the benefit of fine-tuning reward models on high-quality annotated preference data; however, improvements are dataset dependent and smaller or mixed on some external benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Authors note pairwise accuracy is an important but not exclusive metric for reward model quality; improvements vary across datasets (dataset shift), and high accuracy on some benchmarks does not guarantee capturing all aspects of human preference or downstream helpfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen Technical Report', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1822.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1822.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human pairwise ranking (chat eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human evaluation via 3-annotator pairwise ranking on 300 instructions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human evaluation protocol where three annotators ranked model responses (helpfulness, informativeness, validity) over 300 Chinese instructions to compare SFT, RLHF, and proprietary baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>human pairwise ranking (not an automated proxy) used as gold-standard; authors contrast this with traditional automatic benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>chat responses (including code examples and multi-domain outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>general (knowledge, language, creative writing, coding, mathematics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>overall helpfulness, informativeness, validity and related factors (annotators produced ranked preferences per instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>300 Chinese instructions; for each instruction, three annotators ranked model responses; rankings used to compute win/tie/loss rates against GPT-3.5 baseline</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3 (per instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators following a standard guideline (expertise level not specified as senior developers)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not explicitly quantified; authors used diverse prompts (including coding and math) and noted RLHF improved human-preferred responses across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Authors used explicit guidelines for annotators (format and scoring guidance), implying clearer criteria supported consistent ranking, though inter-annotator agreement numbers are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>300 instructions</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RLHF-trained QWEN-CHAT significantly outperforms SFT-only models in human pairwise rankings; RLHF model still trails GPT-4 on the curated human-eval set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Authors caution that traditional automatic benchmarks may not capture aligned chat model quality and that more extensive and rigorous assessments are required; they also note difficulty in precisely quantifying gaps to proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen Technical Report', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1822.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1822.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QWEN-VL used as automated judge for visualizations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QWEN-VL multimodal model used to verify visualization outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal QWEN-VL model was used as an automated evaluator to confirm whether images (plots) generated by executing model-produced code satisfy the user's request, enabling automated correctness scoring of visualization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (multimodal LLM evaluating image outputs against textual requests)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>QWEN-VL (multimodal QWEN variant)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>multimodal question-answering to verify whether generated image fulfills textual requirement (used as an automatic correctness oracle for visualization)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>visual artifacts produced by code (plots/images) and the code that generated them</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Python data-visualization code / plot images</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>visualization correctness (does the generated plot satisfy the textual specification?)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not reported; visualization evaluation distinguishes 'easy' vs 'hard' tasks but automated judge use is described without human-comparison statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Not reported; correctness judged automatically by QWEN-VL but no analysis of rubric clarity effect vs human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors used QWEN-VL to automatically validate visualization outputs from code execution, enabling an automated correctness signal for the visualization subset of their Code Interpreter benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>No human-vs-LLM-judge agreement statistics reported in the paper; therefore the reliability of QWEN-VL as an evaluation oracle relative to human judgments is not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen Technical Report', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1822.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1822.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated code metrics (executability / pass@1 / HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated code correctness and executability metrics (including HumanEval pass@1 and executability checks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automated evaluations of generated code using execution (executability) and test-suite based correctness (pass@1 on HumanEval/MBPP/HumanEvalPack) to measure code generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics: test-suite based correctness (pass@1), executability checks, and dataset-specific zero-shot pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated solutions to programming problems)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>primarily Python (HumanEval, MBPP) and multi-language in HumanEvalPACK (Python, JavaScript, Java, Go, C++, Rust)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>code executability (can the code run) and correctness measured by test cases / pass@1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Automated execution and test-based metrics are effective for objective, unit-testable problems where numerical or functional outputs can be validated deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Authors note automated benchmarks are insufficient to fully capture strengths/weaknesses of models (e.g., models may excel on automated metrics but still miss aspects important to humans), and some tasks require more rigorous/human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not directly quantified versus human judges; authors observe that broader capabilities (planning, language constraints, ReAct format) impact performance beyond pure code synthesis, suggesting complexity of tasks (multi-step visualization, planning) affects automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Objective, well-specified test suites yield reliable automated scores; however, for tasks requiring judgment or multi-step planning, automated metrics are less adequate.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CODE-QWEN and CODE-QWEN-CHAT achieve substantially higher pass@1 and executability rates than many open-source baselines on HumanEval/MBPP/HumanEvalPACK; nevertheless, the authors caution that such automatic benchmarks do not capture all dimensions of model capability and call for more rigorous tests and human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>The paper explicitly cautions that automatic benchmarks (e.g., HumanEval pass@1) are insufficient to fully characterize model behavior and that human evaluation remains crucial for aligned chat models; no quantitative agreement between automated code metrics and human expert judgments is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Qwen Technical Report', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Learning to summarize with human feedback <em>(Rating: 2)</em></li>
                <li>Preference model pretraining (PMP) <em>(Rating: 2)</em></li>
                <li>RLHF: Deep reinforcement learning from human preferences <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1822",
    "paper_id": "paper-5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "QWEN Reward Model (RM)",
            "name_full": "QWEN Reward Model (preference / human-feedback reward model)",
            "brief_description": "A learned reward model based on the QWEN architecture trained to predict human pairwise preferences over model responses; used for RLHF and evaluated against human annotation datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "learned reward model / preference model (automated classifier of pairwise human preference)",
            "llm_judge_model": "QWEN-based reward model (same-sized QWEN architecture with pooling layer)",
            "llm_judge_prompt_approach": "pairwise-comparison scoring (models score which of two responses is preferred)",
            "artifact_type": "model responses / chat outputs (includes code, math, general conversational responses)",
            "artifact_domain": "general (mixed; includes code and math prompts among other domains)",
            "evaluation_criteria": "human preference (helpfulness/overall preference) as expressed in pairwise comparisons",
            "human_evaluation_setup": "Human annotators scored multiple model responses following a standard annotation guideline; comparison pairs built from annotator scores",
            "human_expert_count": null,
            "human_expert_expertise": "annotators (not otherwise specified as expert developers)",
            "agreement_metric": "pairwise accuracy (reward model prediction vs human annotated pairwise preference)",
            "agreement_score": "Reported test pairwise accuracies (PMP and RM) across multiple human-preference datasets. Example values: QWEN Helpful-base PMP=62.68%, RM=74.78%; QWEN Helpful-online PMP=61.62%, RM=69.71%; Anthropic Helpful-base PMP=76.52%, RM=73.98%; Anthropic Helpful-online PMP=65.43%, RM=64.57%; OpenAI Summ. PMP=69.60%, RM=69.99%; Stanford SHP PMP=60.05%, RM=60.10%; OpenAI PRM800K PMP=70.59%, RM=70.52%.",
            "high_agreement_conditions": "When reward model is fine-tuned on high-quality, in-distribution human preference datasets (e.g., QWEN-specific reward datasets), accuracy improves relative to pretraining (PMP).",
            "low_agreement_conditions": "Out-of-distribution settings and some external datasets show smaller gains or mixed results (e.g., Anthropic and some online datasets where RM improvements are smaller or slightly lower), indicating dataset shifts reduce alignment.",
            "artifact_complexity_effect": "Not explicitly quantified for code-only artifacts; evaluation spans mixed response types — RM shows gains on the QWEN-specific datasets which include diverse prompts, but no direct breakdown showing complexity-vs-agreement for software artifacts.",
            "criteria_clarity_effect": "Implicit: higher-quality, well-annotated pairwise labels (clear annotation guidelines and curated prompt sampling) improved RM performance; authors emphasize balanced sampling and annotation guidelines improved training signal.",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Measured via pairwise accuracy: RM predictions compared against human pairwise labels (values reported in agreement_score); no explicit comparison reported between RM-human agreement and human-human inter-rater agreement.",
            "calibration_or_training": "Yes — the reward model was pretrained via preference model pretraining (PMP) on large comparison datasets and then fine-tuned on higher-quality human-annotated preference data; balanced sampling across ~6600 prompt tags and diverse model-response generation strategies were used.",
            "key_findings": "A QWEN-based reward model substantially improves pairwise accuracy on the authors' QWEN preference datasets compared to PMP (e.g., +~12 percentage points on QWEN Helpful-base), showing the benefit of fine-tuning reward models on high-quality annotated preference data; however, improvements are dataset dependent and smaller or mixed on some external benchmarks.",
            "limitations_noted": "Authors note pairwise accuracy is an important but not exclusive metric for reward model quality; improvements vary across datasets (dataset shift), and high accuracy on some benchmarks does not guarantee capturing all aspects of human preference or downstream helpfulness.",
            "uuid": "e1822.0",
            "source_info": {
                "paper_title": "Qwen Technical Report",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Human pairwise ranking (chat eval)",
            "name_full": "Human evaluation via 3-annotator pairwise ranking on 300 instructions",
            "brief_description": "A human evaluation protocol where three annotators ranked model responses (helpfulness, informativeness, validity) over 300 Chinese instructions to compare SFT, RLHF, and proprietary baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "human pairwise ranking (not an automated proxy) used as gold-standard; authors contrast this with traditional automatic benchmarks",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "chat responses (including code examples and multi-domain outputs)",
            "artifact_domain": "general (knowledge, language, creative writing, coding, mathematics)",
            "evaluation_criteria": "overall helpfulness, informativeness, validity and related factors (annotators produced ranked preferences per instruction)",
            "human_evaluation_setup": "300 Chinese instructions; for each instruction, three annotators ranked model responses; rankings used to compute win/tie/loss rates against GPT-3.5 baseline",
            "human_expert_count": "3 (per instruction)",
            "human_expert_expertise": "annotators following a standard guideline (expertise level not specified as senior developers)",
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": null,
            "low_agreement_conditions": null,
            "artifact_complexity_effect": "Not explicitly quantified; authors used diverse prompts (including coding and math) and noted RLHF improved human-preferred responses across tasks.",
            "criteria_clarity_effect": "Authors used explicit guidelines for annotators (format and scoring guidance), implying clearer criteria supported consistent ranking, though inter-annotator agreement numbers are not reported.",
            "sample_size": "300 instructions",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": null,
            "calibration_or_training": null,
            "key_findings": "RLHF-trained QWEN-CHAT significantly outperforms SFT-only models in human pairwise rankings; RLHF model still trails GPT-4 on the curated human-eval set.",
            "limitations_noted": "Authors caution that traditional automatic benchmarks may not capture aligned chat model quality and that more extensive and rigorous assessments are required; they also note difficulty in precisely quantifying gaps to proprietary models.",
            "uuid": "e1822.1",
            "source_info": {
                "paper_title": "Qwen Technical Report",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "QWEN-VL used as automated judge for visualizations",
            "name_full": "QWEN-VL multimodal model used to verify visualization outputs",
            "brief_description": "The multimodal QWEN-VL model was used as an automated evaluator to confirm whether images (plots) generated by executing model-produced code satisfy the user's request, enabling automated correctness scoring of visualization tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (multimodal LLM evaluating image outputs against textual requests)",
            "llm_judge_model": "QWEN-VL (multimodal QWEN variant)",
            "llm_judge_prompt_approach": "multimodal question-answering to verify whether generated image fulfills textual requirement (used as an automatic correctness oracle for visualization)",
            "artifact_type": "visual artifacts produced by code (plots/images) and the code that generated them",
            "artifact_domain": "Python data-visualization code / plot images",
            "evaluation_criteria": "visualization correctness (does the generated plot satisfy the textual specification?)",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": null,
            "low_agreement_conditions": null,
            "artifact_complexity_effect": "Not reported; visualization evaluation distinguishes 'easy' vs 'hard' tasks but automated judge use is described without human-comparison statistics.",
            "criteria_clarity_effect": "Not reported; correctness judged automatically by QWEN-VL but no analysis of rubric clarity effect vs human judgments.",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": null,
            "calibration_or_training": null,
            "key_findings": "The authors used QWEN-VL to automatically validate visualization outputs from code execution, enabling an automated correctness signal for the visualization subset of their Code Interpreter benchmark.",
            "limitations_noted": "No human-vs-LLM-judge agreement statistics reported in the paper; therefore the reliability of QWEN-VL as an evaluation oracle relative to human judgments is not quantified here.",
            "uuid": "e1822.2",
            "source_info": {
                "paper_title": "Qwen Technical Report",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Automated code metrics (executability / pass@1 / HumanEval)",
            "name_full": "Automated code correctness and executability metrics (including HumanEval pass@1 and executability checks)",
            "brief_description": "Standard automated evaluations of generated code using execution (executability) and test-suite based correctness (pass@1 on HumanEval/MBPP/HumanEvalPack) to measure code generation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics: test-suite based correctness (pass@1), executability checks, and dataset-specific zero-shot pass@1",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code (generated solutions to programming problems)",
            "artifact_domain": "primarily Python (HumanEval, MBPP) and multi-language in HumanEvalPACK (Python, JavaScript, Java, Go, C++, Rust)",
            "evaluation_criteria": "code executability (can the code run) and correctness measured by test cases / pass@1",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": null,
            "agreement_score": null,
            "high_agreement_conditions": "Automated execution and test-based metrics are effective for objective, unit-testable problems where numerical or functional outputs can be validated deterministically.",
            "low_agreement_conditions": "Authors note automated benchmarks are insufficient to fully capture strengths/weaknesses of models (e.g., models may excel on automated metrics but still miss aspects important to humans), and some tasks require more rigorous/human evaluation.",
            "artifact_complexity_effect": "Not directly quantified versus human judges; authors observe that broader capabilities (planning, language constraints, ReAct format) impact performance beyond pure code synthesis, suggesting complexity of tasks (multi-step visualization, planning) affects automated metrics.",
            "criteria_clarity_effect": "Objective, well-specified test suites yield reliable automated scores; however, for tasks requiring judgment or multi-step planning, automated metrics are less adequate.",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": null,
            "calibration_or_training": null,
            "key_findings": "CODE-QWEN and CODE-QWEN-CHAT achieve substantially higher pass@1 and executability rates than many open-source baselines on HumanEval/MBPP/HumanEvalPACK; nevertheless, the authors caution that such automatic benchmarks do not capture all dimensions of model capability and call for more rigorous tests and human evaluation.",
            "limitations_noted": "The paper explicitly cautions that automatic benchmarks (e.g., HumanEval pass@1) are insufficient to fully characterize model behavior and that human evaluation remains crucial for aligned chat models; no quantitative agreement between automated code metrics and human expert judgments is reported.",
            "uuid": "e1822.3",
            "source_info": {
                "paper_title": "Qwen Technical Report",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Preference model pretraining (PMP)",
            "rating": 2
        },
        {
            "paper_title": "RLHF: Deep reinforcement learning from human preferences",
            "rating": 1
        }
    ],
    "cost": 0.01689125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QWEN TECHNICAL REPORT</h1>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu.</p>
<p>Qwen Team, Alibaba Group*</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce QWEN ${ }^{1}$, the first installment of our large language model series. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes QWEN, the base pretrained language models, and QWEN-CHAT, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, CODE-QWEN and CODE-QWEN-CHAT, as well as mathematics-focused models, MATH-QWEN-CHAT, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Contents</h1>
<p>1 Introduction ..... 3
2 Pretraining ..... 4
2.1 Data ..... 4
2.2 Tokenization ..... 6
2.3 Architecture ..... 6
2.4 Training ..... 7
2.5 Context Length Extension ..... 7
2.6 Experimental Results ..... 8
3 Alignment ..... 9
3.1 Supervised Finetuning ..... 9
3.1.1 Data ..... 10
3.1.2 Training ..... 10
3.2 Reinforcement Learning from Human Feedback ..... 10
3.2.1 Reward Model ..... 10
3.2.2 Reinforcement Learning ..... 11
3.3 Automatic and Human Evaluation of Aligned Models ..... 11
3.4 Tool Use, Code Interpreter, and Agent ..... 13
4 Code-Qwen: Specialized Model for Coding ..... 16
4.1 Code Pretraining ..... 16
4.2 Code Supervised Fine-Tuning ..... 17
4.3 Evaluation ..... 17
5 Math-Qwen: Specialized Model for Mathematics Reasoning ..... 17
5.1 Training ..... 17
5.2 Evaluation ..... 20
6 Related Work ..... 20
6.1 Large Language Models ..... 20
6.2 Alignment ..... 20
6.3 Tool Use and Agents ..... 21
6.4 LLM for Coding ..... 21
6.5 LLM for Mathematics ..... 22
7 Conclusion ..... 22
A Appendix ..... 36
A. 1 More Training Details ..... 36
A.1.1 Data Format for QWEN-CHAT ..... 36
A. 2 Evaluation ..... 36
A.2.1 Automatic Evaluation ..... 36
A.2.2 Human Evaluation ..... 40
A. 3 Analysis of Code Interpreter ..... 58</p>
<h1>1 INTRODUCTION</h1>
<p>Large language models (LLMs) (Radford et al., 2018; Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Anil et al., 2023; Thoppilan et al., 2022; Touvron et al., 2023a;b) have revolutionized the field of artificial intelligence (AI) by providing a powerful foundation for complex reasoning and problem-solving tasks. These models have the ability to compress vast knowledge into neural networks, making them incredibly versatile agents. With a chat interface, LLMs can perform tasks that were previously thought to be the exclusive domain of humans, especially those involving creativity and expertise (OpenAI, 2022; Ouyang et al., 2022; Anil et al., 2023; Google, 2023; Anthropic, 2023a;b). They can engage in natural language conversations with humans, answering questions, providing information, and even generating creative content such as stories, poems, and music. This has led to the development of a wide range of applications, from chatbots and virtual assistants to language translation and summarization tools.</p>
<p>LLMs are not just limited to language tasks. They can also function as a generalist agent (Reed et al., 2022; Bai et al., 2022a; Wang et al., 2023a; AutoGPT, 2023; Hong et al., 2023), collaborating with external systems, tools, and models to achieve the objectives set by humans. For example, LLMs can understand multimodal instructions (OpenAI, 2023; Bai et al., 2023; Liu et al., 2023a; Ye et al., 2023; Dai et al., 2023; Peng et al., 2023b), execute code (Chen et al., 2021; Zheng et al., 2023; Li et al., 2023d), use tools (Schick et al., 2023; LangChain, Inc., 2023; AutoGPT, 2023), and more. This opens up a whole new world of possibilities for AI applications, from autonomous vehicles and robotics to healthcare and finance. As these models continue to evolve and improve, we can expect to see even more innovative and exciting applications in the years to come. Whether it's helping us solve complex problems, creating new forms of entertainment, or transforming the way we live and work, LLMs are poised to play a central role in shaping the future of AI.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model Lineage of the Qwen Series. We have pretrained the language models, namely QWEN, on massive datasets containing trillions of tokens. We then use SFT and RLHF to align QWEN to human preference and thus we have QWEN-CHAT and specifically its improved version QWEN-CHAT-RLHF. Additionally, we also develop specialized models for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT based on QWEN with similar techniques. Note that we previously released the multimodal LLM, QWEN-VL and QWEN-VLChat (Bai et al., 2023), which are also based on our QWEN base models.</p>
<p>Despite their impressive capabilities, LLMs are often criticized for their lack of reproducibility, steerability, and accessibility to service providers. In this work, we are pleased to present and release the initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase Qianwen, which translates to "thousands of prompts" and conveys the notion of embracing a wide range of inquiries. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. The model series include the base pretrained language models, chat models finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement learning with human feedback (RLHF), etc., as well as specialized models in coding and math. The details are outlined below:</p>
<ol>
<li>The base language models, namely QWEN, have undergone extensive training using up to 3 trillion tokens of diverse texts and codes, encompassing a wide range of areas. These models have consistently demonstrated superior performance across a multitude of downstream tasks, even when compared to their more significantly larger counterparts.</li>
<li>The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task performing, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that the SFT models can achieve superior performance. Furthermore, we have trained reward models to mimic human preference and applied them in RLHF for chat models that can produce responses preferred by humans. Through the human evaluation of a challenging test, we find that QWEN-CHAT models trained with RLHF are highly competitive, still falling behind GPT-4 on our benchmark.</li>
<li>In addition, we present specialized models called CODE-QWEN, which includes CODE-QWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14BCHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained on extensive datasets of code and further fine-tuned to handle conversations related to code generation, debugging, and interpretation. The results of experiments conducted on benchmark datasets, such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and HumanEvalPack (Muennighoff et al., 2023), demonstrate the high level of proficiency of CODE-QWEN in code understanding and generation.</li>
<li>This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle mathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATH-QWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins and are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021).</li>
<li>Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile ability to comprehend visual and language instructions. These models outperform the current open-source vision-language models across various evaluation benchmarks and support text recognition and visual grounding in both Chinese and English languages. Moreover, these models enable multi-image conversations and storytelling. Further details can be found in Bai et al. (2023).</li>
</ol>
<p>Now, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN and aligned chat models QWEN-CHAT ${ }^{2}$. This release aims at providing more comprehensive and powerful LLMs at developer- or application-friendly scales.
The structure of this report is as follows: Section 2 describes our approach to pretraining and results of QWEN. Section 3 covers our methodology for alignment and reports the results of both automatic evaluation and human evaluation. Additionally, this section describes details about our efforts in building chat models capable of tool use, code interpreter, and agent. In Sections 4 and 5, we delve into specialized models of coding and math and their performance. Section 6 provides an overview of relevant related work, and Section 7 concludes this paper and points out our future work.</p>
<h1>2 Pretraining</h1>
<p>The pretraining stage involves learning vast amount of data to acquire a comprehensive understanding of the world and its various complexities. This includes not only basic language capabilities but also advanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the data, the model design and scaling, as well as the comprehensive evaluation results on benchmark datasets.</p>
<h3>2.1 DATA</h3>
<p>The size of data has proven to be a crucial factor in developing a robust large language model, as highlighted in previous research (Hoffmann et al., 2022; Touvron et al., 2023b). To create an effective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of GPT-4, GPT-3.5, the previous 13B SOTA, as well as QWEN-14B. We demonstrate the results on 12 datasets covering multiple domains, including language understanding, knowledge, reasoning, etc. QWEN significantly outperforms the previous SOTA of similar model sizes, but still lag behind both GPT-3.5 and GPT-4.
of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.</p>
<p>To ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing procedure. For public web data, we extract text from HTML and use language identification tools to determine the language. To increase the diversity of our data, we employ deduplication techniques, including exact-match deduplication after normalization and fuzzy deduplication using MinHash and LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and machine-learning-based methods. Specifically, we use multiple models to score the content, including language models, text-quality scoring models, and models for identifying potentially offensive or inappropriate content. We also manually sample texts from various sources and review them to ensure their quality. To further enhance the quality of our data, we selectively up-sample data from certain sources, to ensure that our models are trained on a diverse range of high-quality content. In recent studies (Zeng et al., 2022; Aribandi et al., 2021; Raffel et al., 2020), it has been demonstrated that pretraining language models with multi-task instructions can enhance their zero-shot and few-shot performance. To further enhance the performance of our model, we have incorporated high-quality instruction data into our pretraining process. To safeguard the integrity of our benchmark assessment, we have adopted a similar approach as Brown et al. (2020) and meticulously eliminated any instruction</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Encoding compression rates of different models. We randomly selected 1 million document corpora of each language to test and compare the encoding compression rates of different models (with XLM-R (Conneau et al., 2019), which supports 100 languages, as the base value 1, not shown in the figure). As can be seen, while ensuring the efficient decoding of Chinese, English, and code, QWEN also achieves a high compression rate for many other languages (such as th, he, ar, ko, vi, ja, tr, id, pl, ru, nl, pt, it, de, es, fr, etc.), equipping the model with strong scalability as well as high training and inference efficiency in these languages.
samples that exhibit a 13-gram overlap with any data present in the test sets utilized in our evaluation. Given the large number of downstream tasks, it is not feasible to repeat this filtering process for all tasks. Instead, we have made sure that the instruction data for the reported tasks have undergone our filtering process to ensure their accuracy and reliability. Finally, we have built a dataset of up to 3 trillion tokens.</p>
<h1>2.2 Tokenization</h1>
<p>The design of vocabulary significantly impacts the training efficiency and the downstream task performance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following GPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken (Jain, 2022), and select the vocabulary cl100k base as our starting point. To enhance the performance of our model on multilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly used Chinese characters and words, as well as those in other languages. Also, following Touvron et al. (2023a;b), we have split numbers into single digits. The final vocabulary size is approximately 152 K .</p>
<p>The performance of the QWEN tokenizer in terms of compression is depicted in Figure 3. In this comparison, we have evaluated QWEN against several other tokenizers, including XLM-R (Conneau et al., 2019), LLaMA (Touvron et al., 2023a), Baichuan (Inc., 2023a), and InternLM (InternLM Team, 2023). Our findings reveal that QWEN achieves higher compression efficiency than its competitors in most languages. This implies that the cost of serving can be significantly reduced since a smaller number of tokens from QWEN can convey more information than its competitors. Furthermore, we have conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does not negatively impact the downstream performance of the pretrained model. Despite the increase in vocabulary size, our experiments have shown that QWEN maintains its performance levels in downstream evaluation.</p>
<h3>2.3 ARCHITECTURE</h3>
<p>QWEN is designed using a modified version of the Transformer architecture. Specifically, we have adopted the recent open-source approach of training large language models, LLaMA (Touvron et al., 2023a), which is widely regarded as the top open-source LLM. Our modifications to the architecture include:</p>
<p>Table 1: Model sizes, architectures, and optimization hyper-parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># of Params</th>
<th style="text-align: center;">Hidden size</th>
<th style="text-align: center;">Heads</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">Learning rate</th>
<th style="text-align: center;">Batch size</th>
<th style="text-align: center;">Training tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
<td style="text-align: center;">4 M</td>
<td style="text-align: center;">2.2 T</td>
</tr>
<tr>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
<td style="text-align: center;">4 M</td>
<td style="text-align: center;">2.4 T</td>
</tr>
<tr>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">5120</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$3.0 \times 10^{-4}$</td>
<td style="text-align: center;">4 M</td>
<td style="text-align: center;">3.0 T</td>
</tr>
</tbody>
</table>
<ul>
<li>Embedding and output projection. Based on preliminary experimental findings, we have opted for the untied embedding approach instead of tying the weights of input embedding and output projection. This decision was made in order to achieve better performance with the price of memory costs.</li>
<li>Positional embedding. We have chosen RoPE (Rotary Positional Embedding) (Su et al., 2021) as our preferred option for incorporating positional information into our model. RoPE has been widely adopted and has demonstrated success in contemporary large language models, notably PaLM (Chowdhery et al., 2022; Anil et al., 2023) and LLaMA (Touvron et al., 2023a;b). In particular, we have opted to use FP32 precision for the inverse frequency matrix, rather than BF16 or FP16, in order to prioritize model performance and achieve higher accuracy.</li>
<li>Bias. For most layers, we remove biases following Chowdhery et al. (2022), but we add biases in the QKV layer of attention to enhance the extrapolation ability of the model (Su, 2023b).</li>
<li>Pre-Norm \&amp; RMSNorm. In modern Transformer models, pre-normalization is the most widely used approach, which has been shown to improve training stability compared to post-normalization. Recent research has suggested alternative methods for better training stability, which we plan to explore in future versions of our model. Additionally, we have replaced the traditional layer normalization technique described in (Ba et al., 2016) with RMSNorm (Jiang et al., 2023). This change has resulted in equivalent performance while also improving efficiency.</li>
<li>Activation function. We have selected SwiGLU (Shazeer, 2020) as our activation function, a combination of Swish (Ramachandran et al., 2017) and Gated Linear Unit (Dauphin et al., 2017). Our initial experiments have shown that activation functions based on GLU generally outperform other baseline options, such as GeLU (Hendrycks \&amp; Gimpel, 2016). As is common practice in previous research, we have reduced the dimension of the feed-forward network (FFN) from 4 times the hidden size to $\frac{8}{3}$ of the hidden size.</li>
</ul>
<h1>2.4 Training</h1>
<p>To train QWEN, we follow the standard approach of autoregressive language modeling, as described in Radford et al. (2018). This involves training the model to predict the next token based on the context provided by the previous tokens. We train models with context lengths of 2048. To create batches of data, we shuffle and merge the documents, and then truncate them to the specified context lengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention in the attention modules (Dao et al., 2022). We adopt the standard optimizer AdamW (Kingma \&amp; Ba, 2014; Loshchilov \&amp; Hutter, 2017) for pretraining optimization. We set the hyperparameters $\beta_{1}=0.9$, $\beta_{2}=0.95$, and $\epsilon=10^{-8}$. We use a cosine learning rate schedule with a specified peak learning rate for each model size. The learning rate is decayed to a minimum learning rate of $10 \%$ of the peak learning rate. All the models are trained with BFloat16 mixed precision for training stability.</p>
<h3>2.5 CONTEXT LENGTH EXTENSION</h3>
<p>Transformer models have a significant limitation in terms of the context length for their attention mechanism. As the context length increases, the quadratic-complexity computation leads to a drastic increase in both computation and memory costs. In this work, we have implemented simple training-free techniques that are solely applied during inference to extend the context length of the model. One of the key techniques we have used is NTK-aware interpolation (bloc97, 2023).</p>
<p>Table 2: Overall performance on widely-used benchmarks compared to open-source base models. Our largest QWEN model with 14 billion parameters outperforms previous 13B SoTA models on all datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">MMLU <br> 5-shot</th>
<th style="text-align: center;">C-Eval <br> 5-shot</th>
<th style="text-align: center;">GSM8K <br> 8-shot</th>
<th style="text-align: center;">MATH <br> 4-shot</th>
<th style="text-align: center;">HumanEval <br> 0-shot</th>
<th style="text-align: center;">MBPP <br> 3-shot</th>
<th style="text-align: center;">BBH <br> 3-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: left;">Falcon</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">40B</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">37.1</td>
</tr>
<tr>
<td style="text-align: left;">ChatGLM2</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.7</td>
</tr>
<tr>
<td style="text-align: left;">InternLM</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: left;">Baichuan2</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">41.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">37.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">33B</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">65B</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">58.4</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA 2</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">45.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">44.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">64.9</td>
</tr>
<tr>
<td style="text-align: left;">StableBeluga2</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: left;">QWEN</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">28.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">53.4</td>
</tr>
</tbody>
</table>
<p>Unlike position interpolation (PI) (Chen et al., 2023a) which scales each dimension of RoPE equally, NTK-aware interpolation adjusts the base of RoPE to prevent the loss of high-frequency information in a training-free manner. To further improve performance, we have also implemented a trivial extension called dynamic NTK-aware interpolation, which is later formally discussed in (Peng et al., 2023a). It dynamically changes the scale by chunks, avoiding severe performance degradation. These techniques allow us to effectively extend the context length of Transformer models without compromising their computational efficiency or accuracy.</p>
<p>QWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang \&amp; Cholak, 2022; Su, 2023a) and window attention (Beltagy et al., 2020). LogN-Scaling rescales the dot product of the query and value by a factor that depends on the ratio of the context length to the training length, ensuring that the entropy of the attention value remains stable as the context length grows. Window attention restricts the attention to a limited context window, preventing the model from attending to tokens that are too far away.</p>
<p>We also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.</p>
<h1>2.6 EXPERIMENTAL RESULTS</h1>
<p>To evaluate the zero-shot and few-shot learning capabilities of our models, we conduct a thorough benchmark assessment using a series of datasets. We compare QWEN with the most recent open-source base models, including LLaMA (Touvron et al., 2023a), Llama 2 (Touvron et al., 2023b), MPT (Mosaic ML, 2023), Falcon (Almazrouei et al., 2023), Baichuan2 (Yang et al., 2023), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), XVERSE (Inc., 2023b), and StableBeluga2 (Stability AI, 2023). Our evaluation covers a total of 7 popular benchmarks,</p>
<p>Table 3: Results of QWEN on long-context inference using various techniques. Our experimental findings reveal that the application of our crucial techniques enables the model to consistently achieve low perplexity as the context length increases. This suggests that these techniques play a significant role in enhancing the model's ability to comprehend and generate lengthy texts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Sequence Length</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">16384</td>
</tr>
<tr>
<td style="text-align: left;">QWEN-7B</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">39.35</td>
<td style="text-align: center;">469.81</td>
<td style="text-align: center;">2645.09</td>
</tr>
<tr>
<td style="text-align: left;">+ dynamic_ntk</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">3.59</td>
<td style="text-align: center;">3.66</td>
<td style="text-align: center;">5.71</td>
</tr>
<tr>
<td style="text-align: left;">+ dynamic_ntk + logn</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">3.58</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">4.62</td>
</tr>
<tr>
<td style="text-align: left;">+ dynamic_ntk + logn + window_attn</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">3.58</td>
<td style="text-align: center;">3.49</td>
<td style="text-align: center;">4.32</td>
</tr>
<tr>
<td style="text-align: left;">QWEN-14B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">22.79</td>
<td style="text-align: center;">334.65</td>
<td style="text-align: center;">3168.35</td>
</tr>
<tr>
<td style="text-align: left;">+ dynamic_ntk + logn + window_attn</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">3.18</td>
<td style="text-align: center;">3.42</td>
</tr>
</tbody>
</table>
<p>which are MMLU (5-shot) (Hendrycks et al., 2020), C-Eval (5-shot) (Huang et al., 2023), GSM8K (8-shot) (Cobbe et al., 2021), MATH (4-shot) (Hendrycks et al., 2021), HumanEval (0-shot) (Chen et al., 2021), MBPP (0-shot) (Austin et al., 2021), and BBH (Big Bench Hard) (3 shot) (Suzgun et al., 2022). We aim to provide a comprehensive summary of the overall performance of our models across these benchmarks.</p>
<p>In this evaluation, we focus on the base language models without alignment and collect the baselines' best scores from their official results and OpenCompass (OpenCompass Team, 2023). The results are presented in Table 2.</p>
<p>Our experimental results demonstrate that the three QWEN models exhibit exceptional performance across all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are outperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA213B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small number of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even outperforms larger models in some instances. The findings highlight the impressive capabilities of the QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B, can still achieve strong performance in certain applications.</p>
<p>To evaluate the effectiveness of context length extension, Table 3 presents the test results on $\operatorname{arXiv}^{3}$ in terms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation, LogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of our models in the context of over 8192 tokens.</p>
<h1>3 ALIGNMENT</h1>
<p>Pretrained large language models have been found to be not aligned with human behavior, making them unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of alignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), can significantly improve the ability of language models to engage in natural conversation. In this section, we will delve into the details of how QWEN models have been trained using SFT and RLHF, and evaluate their performance in the context of chat-based assistance.</p>
<h3>3.1 SUPERVISED Finetuning</h3>
<p>To gain an understanding of human behavior, the initial step is to carry out SFT, which finetunes a pretrained LLM on chat-style data, including both queries and responses. In the following sections, we will delve into the details of data construction and training methods.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.1.1 DATA</h1>
<p>To enhance the capabilities of our supervised finetuning datasets, we have annotated conversations in multiple styles. While conventional datasets (Wei et al., 2022a) contain a vast amount of data prompted with questions, instructions, and answers in natural language, our approach takes it a step further by annotating human-style conversations. This practice, inspired by Ouyang et al. (2022), aims at improving the model's helpfulness by focusing on natural language generation for diverse tasks. To ensure the model's ability to generalize to a wide range of scenarios, we specifically excluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore, we have prioritized the safety of the language model by annotating data related to safety concerns such as violence, bias, and pornography.
In addition to data quality, we have observed that the training method can significantly impact the final performance of the model. To achieve this, we utilized the ChatML-style format (OpenAI, 2022), which is a versatile meta language capable of describing both the metadata (such as roles) and the content of a turn. This format enables the model to effectively distinguish between various types of information, including system setup, user inputs, and assistant outputs, among others. By leveraging this approach, we can enhance the model's ability to accurately process and analyze complex conversational data.</p>
<h3>3.1.2 Training</h3>
<p>Consistent with pretraining, we also apply next-token prediction as the training task for SFT. We apply the loss masks for the system and user inputs. More details are demonstrated in Section A.1.1.
The model's training process utilizes the AdamW optimizer, with the following hyperparameters: $\beta_{1}$ set to $0.9, \beta_{2}$ set to 0.95 , and $\epsilon$ set to $10^{-8}$. The sequence length is limited to 2048 , and the batch size is 128 . The model undergoes a total of 4000 steps, with the learning rate gradually increased over the first 1430 steps, reaching a peak of $2 \times 10^{-6}$. To prevent overfitting, weight decay is applied with a value of 0.1 , dropout is set to 0.1 , and gradient clipping is enforced with a limit of 1.0 .</p>
<h3>3.2 Reinforcement Learning from Human Feedback</h3>
<p>While SFT has proven to be effective, we acknowledge that its generalization and creativity capabilities may be limited, and it is prone to overfitting. To address this issue, we have implemented Reinforcement Learning from Human Feedback (RLHF) to further align SFT models with human preferences, following the approaches of Ouyang et al. (2022); Christiano et al. (2017). This process involves training a reward model and using Proximal Policy Optimization (PPO) (Schulman et al., 2017) to conduct policy training.</p>
<h3>3.2.1 ReWard Model</h3>
<p>To create a successful reward model, like building a large language model (LLM), it is crucial to first undergo pretraining and then finetuning. This pretraining process, also known as preference model pretraining (PMP) (Bai et al., 2022b), necessitates a vast dataset of comparison data. This dataset consists of sample pairs, each containing two distinct responses for a single query and their corresponding preferences. Similarly, finetuning is also conducted on this type of comparison data, but with a higher quality due to the presence of quality annotations.</p>
<p>During the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on human feedback for responses from the QWEN models. To ensure the diversity and complexity of user prompts are properly taken into account, we have created a classification system with around 6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and complexity when selecting prompts for annotation by the reward model (Lu et al., 2023). To generate a wide range of responses, we have utilized QWEN models of different sizes and sampling strategies, as diverse responses can help reduce annotation difficulties and enhance the performance of the reward model. These responses are then evaluated by annotators following a standard annotation guideline, and comparison pairs are formed based on their scores.
In creating the reward model, we utilize the same-sized pre-trained language model QWEN to initiate the process. It is important to mention that we have incorporated a pooling layer into the original</p>
<p>Table 4: Test Accuracy of QWEN preference model pretraining (PMP) and reward model (RM) on diverse human preference benchmark datasets.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>QWEN</th>
<th>QWEN</th>
<th>Anthropic</th>
<th>Anthropic</th>
<th>OpenAI</th>
<th>Stanford</th>
<th>OpenAI</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Helpful-base</td>
<td>Helpful-online</td>
<td>Helpful-base</td>
<td>Helpful-online</td>
<td>Summ.</td>
<td>SHP</td>
<td>PRM800K</td>
</tr>
<tr>
<td>PMP</td>
<td>62.68</td>
<td>61.62</td>
<td>76.52</td>
<td>65.43</td>
<td>69.60</td>
<td>60.05</td>
<td>70.59</td>
</tr>
<tr>
<td>RM</td>
<td>74.78</td>
<td>69.71</td>
<td>73.98</td>
<td>64.57</td>
<td>69.99</td>
<td>60.10</td>
<td>70.52</td>
</tr>
</tbody>
</table>
<p>QWEN model to extract the reward for a sentence based on a specific end token. The learning rate for this process has been set to a constant value of $3 \times 10^{-6}$, and the batch size is 64 . Additionally, the sequence length is set to 2048 , and the training process lasts for a single epoch.</p>
<p>We adopted the accuracy on the test dataset as an important but not exclusive evaluation metric for the reward model. In Table 4, we report the test pairwise accuracy of PMP and reward models on diverse human preference benchmark datasets (Bai et al., 2022b; Stiennon et al., 2020; Ethayarajh et al., 2022; Lightman et al., 2023). Specifically, QWEN Helpful-base and QWEN Helpful-online are our proprietary datasets. The responses in QWEN Helpful-base are generated from QWEN without RLHF, whereas QWEN Helpful-online includes responses from QWEN with RLHF. The results show that the PMP model demonstrates high generalization capabilities on out-of-distribution data, and the reward model demonstrates significant improvement on our QWEN reward datasets.</p>
<h1>3.2.2 REINFORCEMENT LEARNING</h1>
<p>Our Proximal Policy Optimization (PPO) process involves four models: the policy model, value model, reference model, and reward model. Before starting the PPO procedure, we pause the policy model's updates and focus solely on updating the value model for 50 steps. This approach ensures that the value model can adapt to different reward models effectively.</p>
<p>During the PPO operation, we use a strategy of sampling two responses for each query simultaneously. This strategy has proven to be more effective based on our internal benchmarking evaluations. We set the KL divergence coefficient to 0.04 and normalize the reward based on the running mean.</p>
<p>The policy and value models have learning rates of $1 \times 10^{-6}$ and $5 \times 10^{-6}$, respectively. To enhance training stability, we utilize value loss clipping with a clip value of 0.15 . For inference, the policy top-p is set to 0.9 . Our findings indicate that although the entropy is slightly lower than when top-p is set to 1.0 , there is a faster increase in reward, ultimately resulting in consistently higher evaluation rewards under similar conditions.</p>
<p>Additionally, we have implemented a pretrained gradient to mitigate the alignment tax. Empirical findings indicate that, with this specific reward model, the KL penalty is adequately robust to counteract the alignment tax in benchmarks that are not strictly code or math in nature, such as those that test common sense knowledge and reading comprehension. It is imperative to utilize a significantly larger volume of the pretrained data in comparison to the PPO data to ensure the effectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly large value for this coefficient can considerably impede the alignment to the reward model, eventually compromising the ultimate alignment, while an overly small value would only have a marginal effect on alignment tax reduction.</p>
<h3>3.3 Automatic and Human Evaluation of Aligned Models</h3>
<p>To showcase the effectiveness of our aligned models, we conduct a comparison with other aligned models on well-established benchmarks, including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022). Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting to demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists of an instruction and a question without any previous examples in the context. The results of the baselines are collected from their official reports and OpenCompass (OpenCompass Team, 2023).</p>
<p>The results in Table 5 demonstrate the effectiveness of our aligned models in understanding human instructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models</p>
<p>Table 5: Performance of aligned models on widely-used benchmarks. We report both zero-shot and few-shot performance of the models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">C-Eval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;">BBH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0 -shot / 5-shot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0-shot / 5-shot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0-shot / 8-shot</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0-shot</td>
<td style="text-align: center;">0-shot / 3-shot</td>
</tr>
<tr>
<td style="text-align: center;">Proprietary models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">- / 69.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 52.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 78.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">- / 70.1</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">- / 83.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 69.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 91.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">- / 86.7</td>
</tr>
<tr>
<td style="text-align: center;">Open-source models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGLM2</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">45.5 / 46.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.1 / 52.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 28.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">- / 32.7</td>
</tr>
<tr>
<td style="text-align: center;">InternLM-Chat</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">- / 51.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 53.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 33.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">- / 32.5</td>
</tr>
<tr>
<td style="text-align: center;">Baichuan2-Chat</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">- / 52.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 55.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 32.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">- / 35.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">- / 57.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 56.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 55.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">- / 49.9</td>
</tr>
<tr>
<td style="text-align: center;">LLAMA 2-CHAT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">- / 46.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 31.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 26.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">- / 35.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">- / 54.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 36.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 37.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">- / 40.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">- / 63.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 44.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">- / 59.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">- / 60.8</td>
</tr>
<tr>
<td style="text-align: center;">QWEN-CHAT</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">42.4 / 43.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.7 / 50.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.8 / 19.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">27.1 / 25.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">55.8 / 57.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">59.7 / 59.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.3 / 54.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">39.6 / 46.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">64.6 / 66.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">69.8 / 71.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60.1 / 59.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">46.9 / 58.7</td>
</tr>
</tbody>
</table>
<p>except ChatGPT (OpenAI, 2022) and Llama 2-Chat-70B (Touvron et al., 2023b) in all datasets, including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022). In particular, QWEN's performance in HumanEval, which measures the quality of generated codes, is significantly higher than that of other open-source models.</p>
<p>Moreover, QWEN's performance is consistently better than that of open-source models of similar size, such as LLaMA2 (Touvron et al., 2023b), ChatGLM2 (ChatGLM2 Team, 2023), InternLM (InternLM Team, 2023), and Baichuan2 (Yang et al., 2023). This suggests that our alignment approach, which involves fine-tuning the model on a large dataset of human conversations, has been effective in improving the model's ability to understand and generate human-like language.</p>
<p>Despite this, we have reservations about the ability of traditional benchmark evaluation to accurately measure the performance and potential of chat models trained with alignment techniques in today's landscape. The results mentioned earlier provide some evidence of our competitive standing, but we believe that it is crucial to develop new evaluation methods specifically tailored to aligned models.</p>
<p>We believe that human evaluation is crucial, which is why we have created a carefully curated dataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered a wide range of topics, including knowledge, language understanding, creative writing, coding, and mathematics. To evaluate the performance of different models, we chose the SFT version of QWEN-CHAT-7B and the SFT and RLHF versions of QWEN-CHAT-14B, and added two strong baselines, GPT-3.5 and GPT-4 ${ }^{4}$, for comparison. For each instruction, we asked three annotators to rank the model responses by the overall score of helpfulness, informativeness, validity, and other relevant factors. Our dataset and evaluation methodology provides a comprehensive and rigorous assessment of the capabilities of different language models in various domains.</p>
<p>Figure 4 illustrates the win rates of the various models. For each model, we report the percentage of wins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing these statistics. The experimental results clearly demonstrate that the RLHF model outperforms the SFT models by significant margins, indicating that RLHF can encourage the model to generate responses that are more preferred by humans. In terms of overall performance, we find that the RLHF model significantly outperforms the SFT models, falling behind GPT-4. This indicates the effectiveness of RLHF for aligning to human preference. To provide a more comprehensive understanding of the models' performance, we include a case study with examples from different models in Appendix A.2.2. Nonetheless, it remains difficult to accurately capture the gap between our</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of the human evaluation for chat models. We compare Qwen-7B (SFT), Qwen14B (SFT), Qwen-14B (RLHF), as well as GPT-4 against GPT-3.5. Each bar segment represents the percentage of wins, ties, and losses, from bottom to top. On average, the RLHF model outperforms the SFT model. The dataset consists of 300 Chinese instructions.
models and the proprietary models. As such, a more extensive and rigorous assessment is required for the chat models.</p>
<h1>3.4 Tool Use, Code Interpreter, and Agent</h1>
<p>Table 6: Performance of QWEN on the in-house Chinese benchmark that evaluates its ability to use unseen tools via ReAct prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Tool Selection (Acc. $\uparrow$ )</th>
<th style="text-align: center;">Tool Input (Rouge-L $\uparrow$ )</th>
<th style="text-align: center;">False Positive Error (\%) $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">1.8 B</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">19.3</td>
</tr>
<tr>
<td style="text-align: left;">QWEN-CHAT</td>
<td style="text-align: center;">7 B</td>
<td style="text-align: center;">$\mathbf{9 8}$</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">14 B</td>
<td style="text-align: center;">$\mathbf{9 8}$</td>
<td style="text-align: center;">$\mathbf{9 3}$</td>
<td style="text-align: center;">$\mathbf{2 . 4}$</td>
</tr>
</tbody>
</table>
<p>The QWEN models, which are designed to be versatile, have the remarkable ability to assist with (semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can serve as agents or copilots to help streamline various tasks. We explore QWEN's proficiency in the following areas:</p>
<ul>
<li>Utilizing unseen tools through ReAct prompting (Yao et al., 2022) (see Table 6).</li>
<li>Using a Python code interpreter to enhance math reasoning, data analysis, and more (see Table 7 and Table 8).</li>
<li>Functioning as an agent that accesses Hugging Face's extensive collection of multimodal models while engaging with humans (see Table 9).</li>
</ul>
<p>Table 7: The proportion of code generated by QWEN that is executable on the in-house evaluation benchmark for Code Interpreter. This benchmark examines QWEN's coding proficiency in math problem solving, data visualization, and general purposes. CODE LLAMA underperforms on visualization tasks because it hallucinates non-existent columns solely based on CSV file names (see Figure 5).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Math (\%)</td>
<td style="text-align: center;">Visualization (\%)</td>
<td style="text-align: center;">General (\%)</td>
<td style="text-align: center;">All (\%)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">86.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">72.9</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2-Chat</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">24.1</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: center;">CODE LLAMA-INSTRUCT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">65.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: center;">InternLM-Chat</td>
<td style="text-align: center;">7B v1.1</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">56.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">54.9</td>
</tr>
<tr>
<td style="text-align: center;">QWEN-CHAT</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">26.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">81.7</td>
</tr>
</tbody>
</table>
<p>Table 8: Correctness of the final response on the in-house evaluation benchmark for Code Interpreter. Visualization-Hard tasks involve planning multiple steps, while Visualization-Easy tasks do not. Visualization-All measures both types of tasks. CODE LLAMA excels in performing VisualizationEasy tasks but tends to underperform in Visualization-Hard tasks, due to its inclination to hallucinate non-existent columns based on the name of a CSV file (see Figure 5).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Math (\%)</td>
<td style="text-align: center;">Vis.-Hard (\%)</td>
<td style="text-align: center;">Vis.-Easy (\%)</td>
<td style="text-align: center;">Vis.-All (\%)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2-Chat</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">23.9</td>
</tr>
<tr>
<td style="text-align: center;">CODE LLAMA-INSTRUCT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">42.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">44.2</td>
</tr>
<tr>
<td style="text-align: center;">InternLM-Chat</td>
<td style="text-align: center;">7B v1.1</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">22.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: center;">QWEN-CHAT</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">56.4</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of QWEN-Chat on the Hugging Face Agent benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Params</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tool Selection $\uparrow$</td>
<td style="text-align: center;">Tool Used $\uparrow$</td>
<td style="text-align: center;">Code Correctness $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">Run Mode</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starcoder-Base</td>
<td style="text-align: center;">15B</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starcoder</td>
<td style="text-align: center;">15B</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QWEN-CHAT</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">61.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">71.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">87.0</td>
</tr>
<tr>
<td style="text-align: center;">Chat Mode</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">98.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">89.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starcoder-Base</td>
<td style="text-align: center;">15B</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">91.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starcoder</td>
<td style="text-align: center;">15B</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">89.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QWEN-CHAT</td>
<td style="text-align: center;">1.8B</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14B</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">95.5</td>
</tr>
</tbody>
</table>
<p>To enhance QWEN's capabilities as an agent or copilot, we employ the self-instruct (Wang et al., 2023c) strategy for SFT. Specifically, we utilize the in-context learning capability of QWEN for self-instruction. By providing a few examples, we can prompt QWEN to generate more relevant queries and generate outputs that follow a specific format, such as ReAct (Yao et al., 2022). We then apply rules and involve human annotators to filter out any noisy samples. Afterwards, the samples are incorporated into QWEN's training data, resulting in an updated version of QWEN that is more dependable for self-instruction. We iterate through this process multiple times until we gather an ample number of samples that possess both exceptional quality and a wide range of diversity. As a result, our final collection consists of around 2000 high-quality samples.</p>
<p>During the finetuning process, we mix these high-quality samples with all the other general-purpose SFT samples, rather than introducing an additional training stage. By doing so, we are able to retain essential general-purpose capabilities that are also pertinent for constructing agent applications.</p>
<p>Using Tools via ReAct Prompting We have created and made publicly available a benchmark for evaluating QWEN's ability to call plugins, tools, functions, or APIs using ReAct Prompting (see Qwen Team, Alibaba Group, 2023b). To ensure fair evaluation, we have excluded any plugins that were included in QWEN's training set from the evaluation set. The benchmark assesses the model's accuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility of the parameters passed into the plugin and the frequency of false positives. In this evaluation, a false positive occurs when the model incorrectly invokes a plugin in response to a query, despite not being required to do so.</p>
<p>The results presented in Table 6 demonstrate that QWEN consistently achieves higher accuracy in identifying the relevance of a query to the available tools as the model size increases. However, the table also highlights that beyond a certain point, there is little improvement in performance when it comes to selecting the appropriate tool and providing relevant arguments. This suggests that the current preliminary benchmark may be relatively easy and may require further enhancement in future iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal performance on this particular benchmark. This could potentially be attributed to the fact that the benchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5's capabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if the query cannot be effectively addressed by the provided tools.</p>
<p>Using Code Interpreter for Math Reasoning and Data Analysis The Python code interpreter is widely regarded as a powerful tool for augmenting the capabilities of an LLM agent. It is</p>
<p>worth investigating whether QWEN can harness the full potential of this interpreter to enhance its performance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this exploration, we have developed and made publicly available a benchmark that is specifically tailored for this purpose (see Qwen Team, Alibaba Group, 2023a).
The benchmark encompasses three primary categories of tasks: math problem-solving, data visualization, and other general-purpose tasks like file post-processing and web crawling. Within the visualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved by simply writing and executing a single code snippet without the need for advanced planning skills. However, the more challenging level requires strategic planning and executing multiple code snippets in a sequential manner. This is because the subsequent code must be written based on the output of the previous code. For example, an agent may need to examine the structure of a CSV file using one code snippet before proceeding to write and execute additional code to create a plot.</p>
<p>Regarding evaluation metrics, we consider both the executability and correctness of the generated code. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying if the ground truth numerical answer is present in both the code execution result and the final response. When it comes to data visualization, we assess accuracy by utilizing QWEN-VL (Bai et al., 2023), a powerful multimodal language model. QWEN-VL is capable of answering text questions paired with images, and we rely on it to confirm whether the image generated by the code fulfills the user's request.</p>
<p>The results regarding executability and correctness are presented in Table 7 and Table 8, respectively. It is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which is not surprising since this benchmark specifically requires coding skills. However, it is worth noting that specialist models that are optimized for code synthesis do not necessarily outperform generalist models. This is due to the fact that this benchmark encompasses various skills beyond coding, such as abstracting math problems into equations, understanding language-specified constraints, and responding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT surpass all other open-source alternatives of similar scale significantly, despite being generalist models.</p>
<p>Serving as a Hugging Face Agent Hugging Face provides a framework called the Hugging Face Agent or Transformers Agent (Hugging Face, 2023), which empowers LLM agents with a curated set of multimodal tools, including speech recognition and image synthesis. This framework allows an LLM agent to interact with humans, interpret natural language commands, and employ the provided tools as needed.</p>
<p>To evaluate QWEN's effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks offered by Hugging Face. The results are presented in Table 9. The evaluation results reveal that QWEN performs quite well in comparison to other open-source alternatives, only slightly behind the proprietary GPT-4, demonstrating QWEN's competitive capabilities.</p>
<h1>4 Code-QWEN: Specialized Model for Coding</h1>
<p>Training on domain-specific data has been shown to be highly effective, particularly in the case of code pretraining and finetuning. A language model that has been reinforced with training on code data can serve as a valuable tool for coding, debugging, and interpretation, among other tasks. In this work, we have developed a series of generalist models using pretraining and alignment techniques. Building on this foundation, we have created domain-specific models for coding by leveraging the base language models of QWEN, including continued pretrained model, CODE-QWEN and supervised finetuned model, CODE-QWEN-CHAT. Both models have 14 billion and 7 billion parameters versions.</p>
<h3>4.1 Code Pretraining</h3>
<p>We believe that relying solely on code data for pretraining can result in a significant loss of the ability to function as a versatile assistant. Unlike previous approaches that focused solely on pretraining on code data (Li et al., 2022; 2023d), we take a different approach (Rozière et al., 2023) by starting with our base models QWEN trained on a combination of text and code data, and then continuing to</p>
<p>pretrain on the code data. We continue to pretrain the models on a total of around 90 billion tokens. During the pre-training phase, we initialize the model using the base language models QWEN. Many applications that rely on specialized models for coding may encounter lengthy contextual scenarios, such as tool usage and code interpretation, as mentioned in Section 3.4. To address this issue, we train our models with context lengths of up to 8192. Similar to base model training in Section 2.4, we employ Flash Attention (Dao et al., 2022) in the attention modules, and adopt the standard optimizer AdamW (Kingma \&amp; Ba, 2014; Loshchilov \&amp; Hutter, 2017), setting $\beta_{1}=0.9, \beta_{2}=0.95$, and $\epsilon=10^{-8}$. We set the learning rate as $6.0 \times 10^{-5}$ for CODE-QWEN-14B and $3.0 \times 10^{-5}$ for CODE-QWEN-7B, with $3 \%$ warm up iterations and no learning rate decays.</p>
<h1>4.2 CODE Supervised Fine-Tuning</h1>
<p>After conducting a series of empirical experiments, we have determined that the multi-stage SFT strategy yields the best performance compared to other methods. In the supervised fine-tuning stage, the model CODE-QWEN-CHAT initialized by the code foundation model CODE-QWEN are optimized by the AdamW (Kingma \&amp; Ba, 2014; Loshchilov \&amp; Hutter, 2017) optimizer ( $\beta_{1}=0.9, \beta_{2}=0.95$, $\epsilon=10^{-8}$ ) with a learning rate of $2.0 \times 10^{-6}$ and $1.0 \times 10^{-5}$ for the 14 B and 7 B model respectively. The learning rate increases to the peaking value with the cosine learning rate schedule ( $3 \%$ warm-up steps) and then remains constant.</p>
<h3>4.3 Evaluation</h3>
<p>Our CODE-QWEN models have been compared with both proprietary and open-source language models, as shown in Tables 10 and 11. These tables present the results of our evaluation on the test sets of Humaneval (Chen et al., 2021), MBPP (Austin et al., 2021), and the multi-lingual code generation benchmark HumanEvalPACK (Muennighoff et al., 2023). The comparison is based on the pass@1 performance of the models on these benchmark datasets. The results of this comparison are clearly demonstrated in Tables 10 and 11.</p>
<p>Our analysis reveals that specialized models, specifically CODE-QWEN and CODE-QWEN-CHAT, significantly outperform previous baselines with similar parameter counts, such as OctoGeEX (Muennighoff et al., 2023), InstructCodeT5+ (Wang et al., 2023d), and CodeGeeX2 (Zheng et al., 2023). In fact, these models even rival the performance of larger models like Starcoder (Li et al., 2023d).</p>
<p>When compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODEQWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that these models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with the continued scaling of both model size and data size, we believe that this gap can be narrowed in the near future.</p>
<p>It is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping the full extent of the strengths and weaknesses of the models. In our opinion, it is necessary to develop more rigorous tests to enable us to accurately assess our relative performance in comparison to GPT-4.</p>
<h2>5 Math-QWEn: Specialized Model for Mathematics ReASONing</h2>
<p>We have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is built on top of the QWEN pretrained language models. Specifically, we have developed assistant models that are specifically designed to excel in arithmetic and mathematics and are aligned with human behavior. We are releasing two versions of this model series, MATH-QWEN-14B-CHAT and MATH-QWEN-7B-CHAT, which have 14 billion and 7 billion parameters, respectively.</p>
<h3>5.1 Training</h3>
<p>We carry out math SFT on our augmented math instructional dataset for mathematics reasoning, and therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average lengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs in the math SFT dataset are examination questions, and it is easy for the model to predict the input</p>
<p>Table 10: Results of pass@1 (\%) on HumanEval and MBPP. Most scores are retrieved from the papers of StarCoder (Li et al., 2023d), CodeT5+ (Wang et al., 2023d), WizardCoder (Luo et al., 2023b) and CODE LLAMA (Rozière et al., 2023).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>HumanEval</th>
<th>MBPP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proprietary models</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>PaLM</td>
<td>540B</td>
<td>26.2</td>
<td>36.8</td>
</tr>
<tr>
<td>PaLM-Coder</td>
<td>540B</td>
<td>36.0</td>
<td>47.0</td>
</tr>
<tr>
<td>PaLM 2-S</td>
<td>-</td>
<td>37.6</td>
<td>50.0</td>
</tr>
<tr>
<td>Code-Cushman-001</td>
<td>-</td>
<td>33.5</td>
<td>45.9</td>
</tr>
<tr>
<td>Code-Davinci-002</td>
<td>-</td>
<td>47.0</td>
<td>58.1</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>-</td>
<td>73.2</td>
<td>-</td>
</tr>
<tr>
<td>GPT-4</td>
<td>-</td>
<td>86.6</td>
<td>-</td>
</tr>
<tr>
<td>Open-source models</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLAMA 2</td>
<td>7B</td>
<td>12.2</td>
<td>20.8</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>20.1</td>
<td>27.6</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>22.6</td>
<td>33.8</td>
</tr>
<tr>
<td></td>
<td>70B</td>
<td>30.5</td>
<td>45.4</td>
</tr>
<tr>
<td>CodeGen-Multi</td>
<td>16B</td>
<td>18.3</td>
<td>20.9</td>
</tr>
<tr>
<td>CodeGen-Mono</td>
<td>16B</td>
<td>29.3</td>
<td>35.3</td>
</tr>
<tr>
<td>CodeGeeX2</td>
<td>6B</td>
<td>35.9</td>
<td>-</td>
</tr>
<tr>
<td>StarCoder-Prompted</td>
<td>15B</td>
<td>40.8</td>
<td>49.5</td>
</tr>
<tr>
<td>CodeT5+</td>
<td>16B</td>
<td>30.9</td>
<td>-</td>
</tr>
<tr>
<td>InstructCodeT5+</td>
<td>16B</td>
<td>35.0</td>
<td>-</td>
</tr>
<tr>
<td>CODE LLAMA</td>
<td>7B</td>
<td>33.5</td>
<td>41.4</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>36.0</td>
<td>47.0</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>48.8</td>
<td>55.0</td>
</tr>
<tr>
<td>CODE LLAMA-INSTRUCT</td>
<td>7B</td>
<td>34.8</td>
<td>44.4</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>42.7</td>
<td>49.4</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>41.5</td>
<td>57.0</td>
</tr>
<tr>
<td>CODE LLAMA-PYTHON</td>
<td>7B</td>
<td>38.4</td>
<td>47.6</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>43.3</td>
<td>49.0</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>53.7</td>
<td>56.2</td>
</tr>
<tr>
<td>Unnatural Code Llama</td>
<td>34B</td>
<td>62.2</td>
<td>61.2</td>
</tr>
<tr>
<td>WizardCoder-Python</td>
<td>13B</td>
<td>64.0</td>
<td>55.6</td>
</tr>
<tr>
<td></td>
<td>34B</td>
<td>73.2</td>
<td>61.2</td>
</tr>
<tr>
<td>QWEN-CHAT</td>
<td>7B</td>
<td>37.2</td>
<td>35.8</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>43.9</td>
<td>46.4</td>
</tr>
<tr>
<td>CODE-QWEN</td>
<td>7B</td>
<td>40.2</td>
<td>41.8</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>45.1</td>
<td>51.4</td>
</tr>
<tr>
<td>CODE-QWEN-CHAT</td>
<td>7B</td>
<td>43.3</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>66.4</td>
<td>52.4</td>
</tr>
</tbody>
</table>
<p>Table 11: Zero-shot pass@1 (\%) performance on the HumanEvalPACK (synthesize) benchmark. The baseline results are partly from OctoPack (Muennighoff et al., 2023).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>Programming Language</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Python</td>
<td>JavaScript</td>
<td>Java</td>
<td>Go</td>
<td>C++</td>
<td>Rust</td>
<td>Avg.</td>
</tr>
<tr>
<td>Proprietary models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>-</td>
<td>86.6</td>
<td>82.9</td>
<td>81.7</td>
<td>72.6</td>
<td>78.7</td>
<td>67.1</td>
<td>78.3</td>
</tr>
<tr>
<td>Open-source models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>InstructCodeT5+</td>
<td>16B</td>
<td>37.0</td>
<td>18.9</td>
<td>17.4</td>
<td>9.5</td>
<td>19.8</td>
<td>0.3</td>
<td>17.1</td>
</tr>
<tr>
<td>StarChat- $\beta$</td>
<td>15B</td>
<td>33.5</td>
<td>31.4</td>
<td>26.7</td>
<td>25.5</td>
<td>26.6</td>
<td>14.0</td>
<td>26.3</td>
</tr>
<tr>
<td>StarCoder</td>
<td>15B</td>
<td>33.6</td>
<td>30.8</td>
<td>30.2</td>
<td>17.6</td>
<td>31.6</td>
<td>21.8</td>
<td>27.6</td>
</tr>
<tr>
<td>CodeGeeX2</td>
<td>6B</td>
<td>35.9</td>
<td>32.2</td>
<td>30.8</td>
<td>22.5</td>
<td>29.3</td>
<td>18.1</td>
<td>28.1</td>
</tr>
<tr>
<td>OctoGeEX</td>
<td>6B</td>
<td>44.7</td>
<td>33.8</td>
<td>36.9</td>
<td>21.9</td>
<td>32.3</td>
<td>15.7</td>
<td>30.9</td>
</tr>
<tr>
<td>OctoCoder</td>
<td>15B</td>
<td>46.2</td>
<td>39.2</td>
<td>38.2</td>
<td>30.4</td>
<td>35.6</td>
<td>23.4</td>
<td>35.5</td>
</tr>
<tr>
<td>WizardCoder</td>
<td>15B</td>
<td>59.8</td>
<td>49.5</td>
<td>36.1</td>
<td>36.4</td>
<td>40.9</td>
<td>20.2</td>
<td>40.5</td>
</tr>
<tr>
<td>QWEN-CHAT</td>
<td>7B</td>
<td>37.2</td>
<td>23.2</td>
<td>32.9</td>
<td>20.7</td>
<td>22.0</td>
<td>9.1</td>
<td>24.2</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>43.9</td>
<td>38.4</td>
<td>42.7</td>
<td>34.1</td>
<td>24.4</td>
<td>18.9</td>
<td>33.7</td>
</tr>
<tr>
<td>CODE-QWEN</td>
<td>7B</td>
<td>40.2</td>
<td>40.4</td>
<td>40.2</td>
<td>26.2</td>
<td>20.7</td>
<td>15.8</td>
<td>30.6</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>45.1</td>
<td>51.8</td>
<td>57.3</td>
<td>39.6</td>
<td>18.2</td>
<td>20.7</td>
<td>38.8</td>
</tr>
<tr>
<td>CODE-QWEN-CHAT</td>
<td>7B</td>
<td>43.3</td>
<td>41.5</td>
<td>49.4</td>
<td>29.3</td>
<td>32.9</td>
<td>20.1</td>
<td>36.1</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>66.4</td>
<td>58.5</td>
<td>56.1</td>
<td>47.6</td>
<td>54.2</td>
<td>28.7</td>
<td>51.9</td>
</tr>
</tbody>
</table>
<p>Table 12: Results of models on mathematical reasoning. We report the accuracy of QWEN for all benchmarks using greedy decoding. For MATH, we are reporting QWEN's performances on the test set from Lightman et al. (2023).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>GSM8K</th>
<th>MATH</th>
<th>Math401</th>
<th>Math23K</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proprietary models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>-</td>
<td>92.0</td>
<td>42.5</td>
<td>83.5</td>
<td>74.0</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>-</td>
<td>80.8</td>
<td>34.1</td>
<td>75.1</td>
<td>60.0</td>
</tr>
<tr>
<td>Minerva</td>
<td>8B</td>
<td>16.2</td>
<td>14.1</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>62B</td>
<td>52.4</td>
<td>27.6</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>540B</td>
<td>58.8</td>
<td>33.6</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Open-source models</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-1 RFT</td>
<td>7B</td>
<td>46.5</td>
<td>5.2</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>52.1</td>
<td>5.1</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>WizardMath</td>
<td>7B</td>
<td>54.9</td>
<td>10.7</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>63.9</td>
<td>14.0</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>70B</td>
<td>81.6</td>
<td>22.7</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GAIRMath-Abel</td>
<td>7B</td>
<td>59.7</td>
<td>13.0</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>13B</td>
<td>66.4</td>
<td>17.3</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>70B</td>
<td>83.6</td>
<td>28.3</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>QWEN-CHAT</td>
<td>7B</td>
<td>50.3</td>
<td>6.8</td>
<td>57.4</td>
<td>51.2</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>60.1</td>
<td>18.4</td>
<td>70.1</td>
<td>67.0</td>
</tr>
<tr>
<td>Math-QWEN-CHAT</td>
<td>7B</td>
<td>62.5</td>
<td>17.2</td>
<td>80.8</td>
<td>75.4</td>
</tr>
<tr>
<td></td>
<td>14B</td>
<td>69.8</td>
<td>24.2</td>
<td>85.0</td>
<td>78.4</td>
</tr>
</tbody>
</table>
<p>format and it is meaningless for the model to predict the input condition and numbers which could be random. Thus, we mask the inputs of the system and user to avoid loss computation on them and find masking them accelerates the convergence during our preliminary experiments. For optimization, we use the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning rate of $2 \times 10^{-5}$ and a training step of 50000 .</p>
<h1>5.2 Evaluation</h1>
<p>We evaluate models on the test sets of GSM8K (Grade school math) (Cobbe et al., 2021), MATH (Challenging competition math problems) (Hendrycks et al., 2021), Math401 (Arithmetic ability) (Yuan et al., 2023b), and Math23K (Chinese grade school math) (Wang et al., 2017). We compare MATH-QWEN-CHAT with proprietary models ChatGPT and Minerva (Lewkowycz et al., 2022) and open-sourced math-specialized model RFT (Yuan et al., 2023a), WizardMath (Luo et al., 2023a), and GAIRMath-Abel (Chern et al., 2023a) in Table 12. MATH-QWEN-CHAT models show better math reasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of similar sizes. Compared to proprietary models, MATH-QWEN-7B-CHAT outperforms Minerva-8B in MATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and delivers better performance on arithmetic ability and Chinese math problems.</p>
<h2>6 Related Work</h2>
<h3>6.1 Large Language Models</h3>
<p>The excitement of LLM began with the introduction of the Transformer architecture (Vaswani et al., 2017), which was then applied to pretraining large-scale data by researchers such as Radford et al. (2018); Devlin et al. (2018); Liu et al. (2019). These efforts led to significant success in transfer learning, with model sizes growing from 100 million to over 10 billion parameters (Raffel et al., 2020; Shoeybi et al., 2019).</p>
<p>In 2020, the release of GPT-3, a massive language model that is 10 times larger than T5, demonstrated the incredible potential of few-shot and zero-shot learning through prompt engineering and in-context learning, and later chain-of-thought prompting (Wei et al., 2022c). This success has led to a number of studies exploring the possibilities of further scaling these models (Scao et al., 2022; Zhang et al., 2022; Du et al., 2021; Zeng et al., 2022; Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022; Black et al., 2022; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Thoppilan et al., 2022). As a result, the community has come to view these large language models as essential foundations for downstream models (Bommasani et al., 2021).</p>
<p>The birth of ChatGPT (OpenAI, 2022) and the subsequent launch of GPT-4 (OpenAI, 2023) marked two historic moments in the field of artificial intelligence, demonstrating that large language models (LLMs) can serve as effective AI assistants capable of communicating with humans. These events have sparked interests among researchers and developers in building language models that are aligned with human values and potentially even capable of achieving artificial general intelligence (AGI) (Anil et al., 2023; Anthropic, 2023a;b).</p>
<p>One notable development in this area is the emergence of open-source LLMs, specifically LLaMA (Touvron et al., 2023a) and LLAMA 2 (Touvron et al., 2023b), which have been recognized as the most powerful open-source language models ever created. This has led to a surge of activity in the open-source community (Wolf et al., 2019), with a series of large language models being developed collaboratively to build upon this progress (Mosaic ML, 2023; Almazrouei et al., 2023; ChatGLM2 Team, 2023; Yang et al., 2023; InternLM Team, 2023).</p>
<h3>6.2 Alignment</h3>
<p>The community was impressed by the surprising effectiveness of alignment on LLMs. Previously, LLMs without alignment often struggle with issues such as repetitive generation, hallucination, and deviation from human preferences. Since 2021, researchers have been diligently working on developing methods to enhance the performance of LLMs in downstream tasks (Wei et al., 2022a; Sanh et al., 2021; Longpre et al., 2023; Chung et al., 2022; Muennighoff et al., 2022). Furthermore,</p>
<p>researchers have been actively exploring ways to align LLMs with human instructions (Ouyang et al., 2022; Askell et al., 2021; Bai et al., 2022b;c). One major challenge in alignment research is the difficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or instructions, it is not feasible for others to collect such data.</p>
<p>However, there has been some progress in this area, such as the self-instruct approach proposed in Wang et al. (2023c). This innovative work offers a potential solution to the data collection problem in alignment research. As a result, there has been a surge in open-source chat data, including Alpaca (Taori et al., 2023), MOSS (Sun et al., 2023a), Dolly (Conover et al., 2023), Evol-Instruct (Xu et al., 2023b), and others (Sun et al., 2023b; Xu et al., 2023a;c; Chen et al., 2023c; Ding et al., 2023; Ji et al., 2023; Yang, 2023). Similarly, there has been an increase in open-source chat models, such as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Guanaco (Dettmers et al., 2023), MOSS (Sun et al., 2023a), WizardLM (Xu et al., 2023b), and others (Xu et al., 2023c; Chen et al., 2023c; Ding et al., 2023; Wang et al., 2023b).</p>
<p>To train an effective chat model, available solutions are mostly based on SFT and RLHF (Ouyang et al., 2022). While SFT is similar to pretraining, it focuses on instruction following using the aforementioned data. However, for many developers, the limited memory capacity is a major obstacle to further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA (Hu et al., 2021) and Q-LoRA (Dettmers et al., 2023), have gained popularity in the community. LoRA tunes only low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and paged attention (Dettmers et al., 2022; Frantar et al., 2022; Kwon et al., 2023). In terms of RLHF, recent methods such as PPO (Schulman et al., 2017; Touvron et al., 2023b) have been adopted, but there are also alternative techniques aimed at addressing the complexity of optimization, such as RRHF (Yuan et al., 2023c), DPO (Rafailov et al., 2023), and PRO (Song et al., 2023). Despite the ongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it enhances the intelligence of LLMs and what potential drawbacks it may have.</p>
<h1>6.3 TOOL Use AND AGENTS</h1>
<p>LLM's planning function allows for the invocation of tools, such as APIs or agent capabilities, through in-context learning, as demonstrated by Schick et al. (2023). Yao et al. (2022) introduced ReAct, a generation format that enables the model to generate thoughts on which tool to use, accept input from API observations, and generate a response. GPT-3.5 and GPT-4, when prompted with few shots, have shown consistent and impressive performance. In addition to tool usage, LLMs can utilize external memory sources like knowledge bases (Hu et al., 2023; Zhong et al., 2023b) or search engines (Nakano et al., 2021; Liu et al., 2023b) to generate more accurate and informative answers. This has led to the popularity of frameworks like LangChain (LangChain, Inc., 2023). The research on LLMs for tool use has also sparked interest in building agents with LLM capabilities, such as agents that can call different AI models (Shen et al., 2023; Li et al., 2023a), embodied lifelong learning or multimodal agents (Wang et al., 2023a; Driess et al., 2023), and multiple agents interacting with each other and even building a micro-society (Chen et al., 2023b; Li et al., 2023b; Xu et al., 2023d; Hong et al., 2023).</p>
<h3>6.4 LLM FOR CODING</h3>
<p>Previous research has demonstrated that LLMs possess remarkable capabilities in code understanding and generation, particularly those with massive numbers of parameters (Chowdhery et al., 2022; Anil et al., 2023; Rae et al., 2021; Hoffmann et al., 2022). Moreover, several LLMs have been pretrained, continued pre-trained, or fine-tuned on coding-related data, which has resulted in significantly improved performance compared to general-purpose LLMs. These models include Codex Chen et al. (2021), AlphaCode (Li et al., 2022), SantaCoder (Allal et al., 2023), Starcoder-Base (Li et al., 2023d), InCoder (Fried et al., 2022), CodeT5 (Wang et al., 2021), CodeGeeX (Zheng et al., 2023), and CODE LLAMA (Rozière et al., 2023). In addition to these models, recent studies have focused on developing specialized alignment techniques for coding, such as Code Llama-Instruct (Rozière et al., 2023) and StarCoder (Li et al., 2023d). These models can assist developers in various code-related tasks, including code generation (Chen et al., 2021; Austin et al., 2021), code completion (Zhang et al., 2023a), code translation (Szafraniec et al., 2023), bug fixing (Muennighoff et al., 2023), code refinement (Liu et al., 2023c), and code question answering (Liu \&amp; Wan, 2021). In a word, LLMs</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ To obtain the results from the models, we use the OpenAI APIs of GPT-3.5-turbo-0613 and GPT-4-0613.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
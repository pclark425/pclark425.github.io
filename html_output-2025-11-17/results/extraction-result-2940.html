<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2940 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2940</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2940</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-277350072</p>
                <p><strong>Paper Title:</strong> Large Language Model Agent: A Survey on Methodology, Applications and Challenges</p>
                <p><strong>Paper Abstract:</strong> The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2940.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2940.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt/agent pattern that interleaves reasoning (chain-of-thought style) and actions (tool calls or environment actions) so an LLM can both deliberate and act in interactive/text environments; highlighted in the survey as an example short-term memory usage for text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interleaves natural-language reasoning traces with explicit actions; keeps recent internal reasoning steps and observations in the agent's working context so the model can decide next actions conditioned on recent thought and observations.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>text environments (generic text games / interactive textual environments)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Interactive text-based environments where agents must parse textual observations, maintain context, plan, and issue discrete text actions (e.g., navigation, object manipulation in text form); the survey cites ReAct as used for reasoning+acting in such environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term memory / working memory (dialogue and recent reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Retains recent internal dialog histories and intermediate reasoning traces in the prompt/context window (transient working memory); relies on summarization or selective retention to manage context window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Recency / in-context conditioning (context window retrieval); selective summarization to compress prior turns into prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Recent observations, recent reasoning traces (chain-of-thought steps), recent actions and short dialogue history used to inform the next action.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Short-term memory (keeping recent reasoning and observations in context) supports decision-making in text environments by enabling stepwise planning and action selection, but is transient and limited by context window size.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Transient storage limited by LLM context window; requires compression/summarization and is not persistent across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2940.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2940.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven lifelong learning agent that autonomously explores Minecraft, discovers and synthesizes reusable skills (skill library) via long-term memory mechanisms to improve performance over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An open-ended embodied agent that discovers, stores, and reuses procedural skills discovered while exploring Minecraft; organizes discovered behaviors into a skill library which can be invoked as higher-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Minecraft (open-world / interactive game environment)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A large open-world sandbox environment (here used in text/command form by the agent) requiring multi-step procedural tasks, tool crafting, exploration and skill composition; presents long-horizon planning and continual learning challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term memory (skill library / procedural memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Archives intermediate reasoning trajectories and synthesized procedural skills into a persistent skill library (structured long-term storage of discovered action sequences and their conditions/effects) that can be invoked in future episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Invocation of stored skills / retrieval by matching current subgoal to stored skill signatures (survey describes reuse/invocation of learned skills), effectively a skill-call mechanism rather than pure vector search (details not specified in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Discovered procedural skills (action sequences), distilled behaviors, intermediate reasoning/experience examples that produced successful outcomes (skill primitives and their metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Long-term memory that codifies procedural knowledge (skill libraries) enables lifelong learning and skill reuse in open-ended environments like Minecraft, improving long-horizon performance and sample efficiency relative to purely myopic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Requires methods to organize/curate skill libraries; potential retrieval/selection mismatch and scalability challenges as the library grows; specifics not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2940.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2940.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ghost-in-the-Minecraft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A work that equips LLM agents for Minecraft with a text-based knowledge base and memory, demonstrating how textual knowledge and memory structures can support general capability in open-world game environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ghost-in-the-Minecraft (GITM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven agent that augments model reasoning with a text-based knowledge base and memory module to handle open-world Minecraft tasks, synthesizing past experiences and world knowledge for planning and action execution.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Minecraft (open-world environment)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Open-world sandbox environment requiring persistent world knowledge, multi-step planning, and skill composition; here treated with text-based knowledge and memory augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term memory / text-based knowledge base</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Maintains a text-based knowledge base capturing world knowledge and agent experiences; archives reasoning trajectories and uses text retrieval to ground planning and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval from text-based knowledge base (likely via similarity/relevance to current context) to ground reasoning and access prior experiences or world facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Textual world knowledge, past experiences and intermediate reasoning traces relevant to open-world tasks and strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Text-based persistent memory / knowledge bases help LLM agents perform better in complex open-world game environments by providing grounded facts and reusable experiential knowledge over long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Needs efficient retrieval and curation; subject to external knowledge poisoning and contamination; specifics not reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2940.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2940.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph of Thoughts: Solving elaborate problems with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning/short-term memory paradigm that uses graph-structured intermediate reasoning states (a 'graph of thoughts') to explore multiple reasoning paths and enable backtracking and consolidation of successful partial solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph of thoughts: Solving elaborate problems with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Graph of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Represents intermediate reasoning states as nodes in a graph, allowing the agent to expand multiple reasoning paths, backtrack, and recombine promising partial solutions; treated in survey as a short-term memory/planning mechanism for elaborate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>not specified (applied to elaborate multi-step problems; potentially applicable to text games requiring multi-path planning)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>General multi-step problem domains where multiple alternative reasoning paths should be explored and pruned; enables trial-error-correct processes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term graph-based memory / planning memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Graph-structured working memory with nodes representing intermediate reasoning states and edges representing transitions; stores candidate states, partial solutions and their evaluations to allow backtracking and recombination.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph traversal and node selection heuristics (expansion/pruning) to retrieve promising intermediate states; enables backtracking to prior nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Intermediate reasoning states (thought nodes), partial solution paths, evaluations/scores of nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Graph-structured short-term memory enables exploring multiple reasoning branches and correcting mistakes via backtracking, improving robustness on elaborate multi-step tasks compared to single-chain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Increased computational cost and search complexity; managing graph size and pruning heuristics is necessary; not quantified in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2940.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2940.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL: LLM agents are experiential learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-term memory approach that distills agent experiences into a reusable experience pool capturing success/failure patterns to improve future decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Expel: Llm agents are experiential learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Distills episodic interaction data (successes and failures) into an experience repository that the agent consults to inform future planning and avoid past mistakes; presented as a long-term memory paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term experience repository / distilled experience pool</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Persistent repository of distilled experiences (successful and failed attempts) summarized into reusable patterns or exemplars that can be retrieved during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieval of relevant experiences by context matching/relevance to current task (survey-level description; mechanism details not given).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Summarized past trajectories, success/failure examples, distilled heuristics derived from previous interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Distilling and reusing past experiences helps agents generalize better and avoid repeating known failures; long-term experiential memory transforms ephemeral trials into reusable knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Requires careful distillation and indexing; potential for storing spurious correlations; retrieval relevance critical but not detailed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2940.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2940.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent paradigm leveraging verbal self-reflection (trial-optimized memory) to iteratively refine agent behavior using feedback from past trials; placed in survey under long-term memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses verbal self-reflection and trial feedback to create a memory of trial-optimized behaviors; the agent reflects on failures and generates corrective instructions or heuristics to guide future actions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term reflection-derived memory / trial-optimized memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Stores reflections, corrective strategies and distilled improvements derived from past trials; these reflections form a persistent memory that can be consulted for future decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Retrieve reflection summaries or corrective heuristics relevant to the current situation (survey-level description; exact retrieval method not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Verbalized reflections, corrective actions, trial outcomes, and distilled heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Self-reflection and storing corrective heuristics enable agents to iteratively improve across trials, turning failures into actionable memory that guides future planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Depends on quality of reflections and ability to index/retrieve relevant reflections; potential for overfitting to past failures; specifics not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2940.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2940.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT: Towards LLMs as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tiered memory architecture that structures memory in layers to support persistent storage and efficient retrieval for agentic behavior; cited as a demonstration of structured long-term memory enhancing reasoning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Proposes a tiered memory architecture (multiple memory tiers) for LLM agents to store and retrieve information at different granularities and time scales, supporting long-term reasoning and tool synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tiered long-term memory (multi-tier architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Tiered memory layers that separate short-term transient context from longer-term structured knowledge and experiential stores; organizes stored items by recency, importance, and abstraction level for efficient reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Layer-aware retrieval (survey-level description): short-term items accessed via context, longer-term items retrieved by relevance/importance scoring (details not fully specified in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>At different tiers: recent context and dialog traces (short-term), distilled experiences and skills (mid-term), and structured knowledge resources (long-term).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>A tiered memory architecture can strategically reuse experiences and knowledge at appropriate time scales, improving reasoning efficiency and enabling persistent capability accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Complexity of managing tiers, retrieval policies and scalability; specifics not evaluated in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Agent: A Survey on Methodology, Applications and Challenges', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>Chain of Agents: Large language models collaborating on long-context tasks <em>(Rating: 1)</em></li>
                <li>IRCoT: Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2940",
    "paper_id": "paper-277350072",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompt/agent pattern that interleaves reasoning (chain-of-thought style) and actions (tool calls or environment actions) so an LLM can both deliberate and act in interactive/text environments; highlighted in the survey as an example short-term memory usage for text environments.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "Interleaves natural-language reasoning traces with explicit actions; keeps recent internal reasoning steps and observations in the agent's working context so the model can decide next actions conditioned on recent thought and observations.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "text environments (generic text games / interactive textual environments)",
            "text_game_description": "Interactive text-based environments where agents must parse textual observations, maintain context, plan, and issue discrete text actions (e.g., navigation, object manipulation in text form); the survey cites ReAct as used for reasoning+acting in such environments.",
            "uses_memory": true,
            "memory_type": "short-term memory / working memory (dialogue and recent reasoning traces)",
            "memory_architecture": "Retains recent internal dialog histories and intermediate reasoning traces in the prompt/context window (transient working memory); relies on summarization or selective retention to manage context window limits.",
            "memory_retrieval_mechanism": "Recency / in-context conditioning (context window retrieval); selective summarization to compress prior turns into prompt context.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Recent observations, recent reasoning traces (chain-of-thought steps), recent actions and short dialogue history used to inform the next action.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Short-term memory (keeping recent reasoning and observations in context) supports decision-making in text environments by enabling stepwise planning and action selection, but is transient and limited by context window size.",
            "memory_limitations": "Transient storage limited by LLM context window; requires compression/summarization and is not persistent across episodes.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.0",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "An LLM-driven lifelong learning agent that autonomously explores Minecraft, discovers and synthesizes reusable skills (skill library) via long-term memory mechanisms to improve performance over time.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "An open-ended embodied agent that discovers, stores, and reuses procedural skills discovered while exploring Minecraft; organizes discovered behaviors into a skill library which can be invoked as higher-level actions.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Minecraft (open-world / interactive game environment)",
            "text_game_description": "A large open-world sandbox environment (here used in text/command form by the agent) requiring multi-step procedural tasks, tool crafting, exploration and skill composition; presents long-horizon planning and continual learning challenges.",
            "uses_memory": true,
            "memory_type": "long-term memory (skill library / procedural memory)",
            "memory_architecture": "Archives intermediate reasoning trajectories and synthesized procedural skills into a persistent skill library (structured long-term storage of discovered action sequences and their conditions/effects) that can be invoked in future episodes.",
            "memory_retrieval_mechanism": "Invocation of stored skills / retrieval by matching current subgoal to stored skill signatures (survey describes reuse/invocation of learned skills), effectively a skill-call mechanism rather than pure vector search (details not specified in survey).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Discovered procedural skills (action sequences), distilled behaviors, intermediate reasoning/experience examples that produced successful outcomes (skill primitives and their metadata).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Long-term memory that codifies procedural knowledge (skill libraries) enables lifelong learning and skill reuse in open-ended environments like Minecraft, improving long-horizon performance and sample efficiency relative to purely myopic approaches.",
            "memory_limitations": "Requires methods to organize/curate skill libraries; potential retrieval/selection mismatch and scalability challenges as the library grows; specifics not quantified in survey.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.1",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Ghost-in-the-Minecraft",
            "name_full": "Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "brief_description": "A work that equips LLM agents for Minecraft with a text-based knowledge base and memory, demonstrating how textual knowledge and memory structures can support general capability in open-world game environments.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "agent_name": "Ghost-in-the-Minecraft (GITM)",
            "agent_description": "LLM-driven agent that augments model reasoning with a text-based knowledge base and memory module to handle open-world Minecraft tasks, synthesizing past experiences and world knowledge for planning and action execution.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Minecraft (open-world environment)",
            "text_game_description": "Open-world sandbox environment requiring persistent world knowledge, multi-step planning, and skill composition; here treated with text-based knowledge and memory augmentation.",
            "uses_memory": true,
            "memory_type": "long-term memory / text-based knowledge base",
            "memory_architecture": "Maintains a text-based knowledge base capturing world knowledge and agent experiences; archives reasoning trajectories and uses text retrieval to ground planning and action selection.",
            "memory_retrieval_mechanism": "Retrieval from text-based knowledge base (likely via similarity/relevance to current context) to ground reasoning and access prior experiences or world facts.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Textual world knowledge, past experiences and intermediate reasoning traces relevant to open-world tasks and strategies.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Text-based persistent memory / knowledge bases help LLM agents perform better in complex open-world game environments by providing grounded facts and reusable experiential knowledge over long horizons.",
            "memory_limitations": "Needs efficient retrieval and curation; subject to external knowledge poisoning and contamination; specifics not reported in survey.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.2",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Graph-of-Thoughts",
            "name_full": "Graph of Thoughts: Solving elaborate problems with large language models",
            "brief_description": "A planning/short-term memory paradigm that uses graph-structured intermediate reasoning states (a 'graph of thoughts') to explore multiple reasoning paths and enable backtracking and consolidation of successful partial solutions.",
            "citation_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "mention_or_use": "mention",
            "agent_name": "Graph of Thoughts",
            "agent_description": "Represents intermediate reasoning states as nodes in a graph, allowing the agent to expand multiple reasoning paths, backtrack, and recombine promising partial solutions; treated in survey as a short-term memory/planning mechanism for elaborate tasks.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "not specified (applied to elaborate multi-step problems; potentially applicable to text games requiring multi-path planning)",
            "text_game_description": "General multi-step problem domains where multiple alternative reasoning paths should be explored and pruned; enables trial-error-correct processes.",
            "uses_memory": true,
            "memory_type": "short-term graph-based memory / planning memory",
            "memory_architecture": "Graph-structured working memory with nodes representing intermediate reasoning states and edges representing transitions; stores candidate states, partial solutions and their evaluations to allow backtracking and recombination.",
            "memory_retrieval_mechanism": "Graph traversal and node selection heuristics (expansion/pruning) to retrieve promising intermediate states; enables backtracking to prior nodes.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Intermediate reasoning states (thought nodes), partial solution paths, evaluations/scores of nodes.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Graph-structured short-term memory enables exploring multiple reasoning branches and correcting mistakes via backtracking, improving robustness on elaborate multi-step tasks compared to single-chain reasoning.",
            "memory_limitations": "Increased computational cost and search complexity; managing graph size and pruning heuristics is necessary; not quantified in survey.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.3",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ExpeL",
            "name_full": "ExpeL: LLM agents are experiential learners",
            "brief_description": "A long-term memory approach that distills agent experiences into a reusable experience pool capturing success/failure patterns to improve future decision-making.",
            "citation_title": "Expel: Llm agents are experiential learners",
            "mention_or_use": "mention",
            "agent_name": "ExpeL",
            "agent_description": "Distills episodic interaction data (successes and failures) into an experience repository that the agent consults to inform future planning and avoid past mistakes; presented as a long-term memory paradigm.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "long-term experience repository / distilled experience pool",
            "memory_architecture": "Persistent repository of distilled experiences (successful and failed attempts) summarized into reusable patterns or exemplars that can be retrieved during planning.",
            "memory_retrieval_mechanism": "Retrieval of relevant experiences by context matching/relevance to current task (survey-level description; mechanism details not given).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Summarized past trajectories, success/failure examples, distilled heuristics derived from previous interactions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Distilling and reusing past experiences helps agents generalize better and avoid repeating known failures; long-term experiential memory transforms ephemeral trials into reusable knowledge.",
            "memory_limitations": "Requires careful distillation and indexing; potential for storing spurious correlations; retrieval relevance critical but not detailed in survey.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.4",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "An agent paradigm leveraging verbal self-reflection (trial-optimized memory) to iteratively refine agent behavior using feedback from past trials; placed in survey under long-term memory approaches.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Uses verbal self-reflection and trial feedback to create a memory of trial-optimized behaviors; the agent reflects on failures and generates corrective instructions or heuristics to guide future actions.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "long-term reflection-derived memory / trial-optimized memory",
            "memory_architecture": "Stores reflections, corrective strategies and distilled improvements derived from past trials; these reflections form a persistent memory that can be consulted for future decision-making.",
            "memory_retrieval_mechanism": "Retrieve reflection summaries or corrective heuristics relevant to the current situation (survey-level description; exact retrieval method not specified).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Verbalized reflections, corrective actions, trial outcomes, and distilled heuristics.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Self-reflection and storing corrective heuristics enable agents to iteratively improve across trials, turning failures into actionable memory that guides future planning.",
            "memory_limitations": "Depends on quality of reflections and ability to index/retrieve relevant reflections; potential for overfitting to past failures; specifics not provided in survey.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.5",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT: Towards LLMs as operating systems",
            "brief_description": "A tiered memory architecture that structures memory in layers to support persistent storage and efficient retrieval for agentic behavior; cited as a demonstration of structured long-term memory enhancing reasoning efficiency.",
            "citation_title": "Memgpt: Towards llms as operating systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Proposes a tiered memory architecture (multiple memory tiers) for LLM agents to store and retrieve information at different granularities and time scales, supporting long-term reasoning and tool synthesis.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": null,
            "text_game_description": null,
            "uses_memory": true,
            "memory_type": "tiered long-term memory (multi-tier architecture)",
            "memory_architecture": "Tiered memory layers that separate short-term transient context from longer-term structured knowledge and experiential stores; organizes stored items by recency, importance, and abstraction level for efficient reuse.",
            "memory_retrieval_mechanism": "Layer-aware retrieval (survey-level description): short-term items accessed via context, longer-term items retrieved by relevance/importance scoring (details not fully specified in survey).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "At different tiers: recent context and dialog traces (short-term), distilled experiences and skills (mid-term), and structured knowledge resources (long-term).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "A tiered memory architecture can strategically reuse experiences and knowledge at appropriate time scales, improving reasoning efficiency and enabling persistent capability accumulation.",
            "memory_limitations": "Complexity of managing tiers, retrieval policies and scalability; specifics not evaluated in survey.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2940.6",
            "source_info": {
                "paper_title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_environments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Chain of Agents: Large language models collaborating on long-context tasks",
            "rating": 1,
            "sanitized_title": "chain_of_agents_large_language_models_collaborating_on_longcontext_tasks"
        },
        {
            "paper_title": "IRCoT: Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "rating": 1,
            "sanitized_title": "ircot_interleaving_retrieval_with_chainofthought_reasoning_for_knowledgeintensive_multistep_questions"
        }
    ],
    "cost": 0.022386249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Model Agent: A Survey on Methodology, Applications and Challenges
27 Mar 2025</p>
<p>Junyu Luo 
Weizhi Zhang 
Ye Yuan 
Yusheng Zhao 
Junwei Yang 
Yiyang Gu 
Bohan Wu 
Binqi Chen 
Ziyue Qiao 
Qingqing Long 
Rongcheng Tu 
Xiao Luo 
Wei Ju 
Zhiping Xiao 
Yifan Wang 
Meng Xiao 
Chenwu Liu 
Jingyang Yuan 
Shichang Zhang 
Yiqiao Jin 
Fan Zhang 
Xian Wu 
Hanqing Zhao 
Fellow, IEEEDacheng Tao 
Fellow, IEEEPhilip S Yu 
Ming Zhang 
Large Language Model Agent: A Survey on Methodology, Applications and Challenges
27 Mar 20256AC8945A1A80AC364E855E211B89E0C8arXiv:2503.21460v1[cs.CL]Large language modelLLM agentAI agentintelligent agentmulti-agent systemLLMliterature survey Construction Evolution Collaboration Self-Learning Multi-agent Co-Evolution Benchmark and Datasets Tools LLM Use Tools LLM Create Tools Tools Develop LLM General Assessment Domain-specific Evaluation Collaboration Evaluation Security Social Impact Privacy Agent-centric Security Data-centric Security Memorization Vulnerability Intellectual Property Exploitation Large Language Model Agent Profile Definition 2.1.1 Human-Curated Static Profiles Camel [25]AutoGen [26]MetaGPT [27]ChatDev [28]AFlow [29] Betch-Generated Dynamic Profiles Generative Agents [30]RecAgent [31]DSPy [32] Memory Mechanism 2.1.2 Short-Term Memory ReAct [33]ChatDev [28]Graph of Thoughts [34]AFlow [29] Long-Term Memory Voyager [35]GITM [36]ExpeL [37]Reflexion [38]TPTU [39]OpenAgents [40]Lego-Prover [41]MemGPT [42] Knowledge Retrieval as Memory RAG [43]GraphRAG [44]Chain of Agnets [45]IRCoT [46]
The era of intelligent agents is upon us, driven by revolutionary advancements in large language models.Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence.This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways.We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments.Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains.By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research.The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.</p>
<p>INTRODUCTION</p>
<p>A rtificial Intelligence is entering a pivotal era with the emergence of LLM agents-intelligent entities powered by large language models (LLMs) capable of perceiving environments, reasoning about goals, and executing actions [1].Unlike traditional AI systems that merely respond to user inputs, modern LLM agents actively engage with their environments through continuous learning, reasoning, and adaptation.This shift represents a technological advancement and a fundamental reimagining of humanmachine relationships.Commercial LLM agent systems (e.g., DeepResearch, DeepSearch, and Manus) exemplify this paradigm shift-autonomously executing complex tasks that once required human expertise, from in-depth research to computer operation, while adapting to specific user needs.Compared to traditional agent systems [2], LLM-based agents have achieved generational across multiple dimensions, including knowledge sources [3], generalization capabilities [4], and interaction modalities [5].Today's agents represent a qualitative leap driven by the convergence of three key developments:  unprecedented reasoning capabilities of LLMs [6],  advancements in tool manipulation and environmental interaction [7], and  sophisticated memory architectures that support longitudinal experience accumulation [8], [9].This convergence has transformed theoretical constructs into practical systems, increasingly blurring the boundary between assistants and collaborators.This shift fundamentally arises from LLMs' role as general-purpose task processors, unifying perception, decision-making, and action within semantic space through generative architectures, thereby forming human-like cognitive loops [10].</p>
<p>Our study presents a novel examination of agent systems through a unified taxonomy that connects agent construction, collaboration mechanisms, and evolutionary pathways.We offer a comprehensive perspective tracing on how agents are defined, how they function individually or collectively, and how they evolve over time.Beyond clarifying the current landscape, our work not only clarifies the current landscape but identifies emerging patterns that signal future developments.The rapid advancement of agent technologies necessitates timely surveys to provide researchers with an up-to-date taxonomy for understanding this dynamic field.</p>
<p>Figure 1 presents our organizational framework for understanding the LLM agent ecosystem.At its core, our methodology-centered approach examines the technical foundations of agent systems through three interconnected dimensions: construction (how agents are defined and built), Fig. 1: An overview of the LLM agent ecosystem organized into four interconnected dimensions:  Agent Methodology, covering the foundational aspects of construction, collaboration, and evolution;  Evaluation and Tools, presenting benchmarks, assessment frameworks, and development tools;  Real-World Issues, addressing critical concerns around security, privacy, and social impact; and  Applications, highlighting diverse domains where LLM agents are being deployed.We provide a structured framework for understanding the complete lifecycle of modern LLM-based agent systems.collaboration (how they interact and work together), and evolution (how they learn and improve over time).This tripartite foundation is complemented by practical considerations, including evaluation methodologies, development tools, real-world challenges related to security and ethics, and diverse application domains.This framework shapes the structure of our survey, enabling a systematic exploration of each dimension while highlighting their interconnections.Distinction from Previous Surveys.Despite several surveys exploring various aspects of AI agents in recent years, our study makes a distinctive contribution through its methodological focus and comprehensive analysis of LLM agent architectures.Previous surveys have primarily focused on specific applications (e.g., gaming [11], [12]), deployment environments [13], [14], multi-modality [15] or security [16], while others have provided broad overviews without a detailed methodological taxonomy [1], [17].Recent works also have examined LLM-based agents compared to traditional AI agents [9], multi-agent interaction [18], workflows [19], and cooperative decision-making mechanisms [20].In contrast to these works, our survey stands out through:</p>
<p>1) Methodology-centered taxonomy: We propose a systematic taxonomy that deconstructs LLM agent systems into their fundamental methodological components, including role definition, memory mechanisms, planning capabilities, and action execution [21].2) Build-Collaborate-Evolve framework: We analyze three interconnected dimensions of LLM agents -construction, collaboration, and evolution -offering a more holistic understanding than previous approaches [22], [23].This integrated architectural perspective highlights the continuity between individual LLM agent design and collaborative systems, whereas prior studies have often examined these aspects separately [22], [24].3) Frontier applications and real-world focus: Beyond addressing theoretical concepts, our work examines cutting-edge tools, communication protocols, and diverse applications on LLM agents.We provide comprehensive analysis of pressing real-world challenges including security, privacy, and ethics.This forwardlooking perspective is particularly valuable as agent technologies transition from research to widespread implementation.</p>
<p>Our survey provides researchers and practitioners with a more structured taxonomy for understanding, comparing, and advancing research of LLM agents from different perspectives.As LLM agent systems increasingly integrate into various critical domains, understanding their architectural foundations becomes essential not only for researchers but also for policy scholars, industry practitioners, and society at large.This survey aims to provide this foundation while charting a path forward for this rapidly evolving field.</p>
<p>AGENT METHODOLOGY</p>
<p>This section presents a comprehensive framework for understanding LLM-based agent systems through three interconnected dimensions: construction, collaboration, and evolution.As illustrated in Figure 2, we first examine agent construction (Section 2.1), which establishes the foundational components including profile definition, memory mechanisms, planning capabilities, and action execution.We then explore collaboration paradigms (Section 2.2) that enable multiple agents to work together through centralized Fig. 2: A taxonomy of large language model agent methodologies.control, decentralized cooperation, or hybrid architectures.Finally, we investigate evolution mechanisms (Section 2.3) that allow agents to improve over time through autonomous optimization, multi-agent co-evolution, and external resource integration.This three-dimensional framework provides a systematic approach to analyzing the full lifecycle of LLM agent systems.</p>
<p>Agent Construction</p>
<p>Agent construction serves as the foundational phase in developing LLM-based autonomous systems, encompassing the systematic design of core components that enable goaldirected behaviors.This process prioritizes four interdependent pillars: profile definition (2.1.1),memory mechanism (2.1.2),planning capability (2.1.3),and action execution (2.1.4).These components collectively form a recursive optimization loop, where memory informs planning, execution outcomes update memory, and contextual feedback refines agent profiles.The construction paradigm emphasizes modular interoperability while preserving system-wide coherence, enabling subsequent collaboration and evolutionary adaptation mechanisms, which will be discussed in later sections.</p>
<p>Profile Definition</p>
<p>Profile definition establishes an agent's operational identity by configuring its intrinsic attributes and behavioral patterns [25], [26].Current methodologies encompass two approaches: human-curated static profiles ensure domainspecific consistency through manual specification, while batch-generated dynamic profiles adaptively modulate operational parameters to stochastically yield a batch of agent initializations.These mechanisms collectively govern an agent's decision boundaries and interaction protocols while maintaining alignment with predefined objectives.Human-Curated Static Profiles.This approach establishes fixed agent profiles through manual specification by domain experts, embedding explicit rules and domain-specific knowledge.It ensures strict adherence to predefined behavioral guidelines and task requirements enabling standardized communication protocols among agents.This is particularly effective in scenarios demanding high interpretability and regulatory compliance.Such frameworks typically employ coordinated interactions between predefined agent components to achieve complex functionalities through structured communication patterns.Representative implementations demonstrate two key paradigms: systems like Camel [25], AutoGen [26], and OpenAgents [40] orchestrate humanagent collaboration through predefined conversational roles (e.g., user proxy and assistant), enabling task execution through structured dialogues.Meanwhile, frameworks such as MetaGPT [27], ChatDev [28], and AFlow [29] showcase role-based coordination patterns.ChatDev specializes in code development by coordinating static technical roles (e.g., product managers and programmers) with deterministic interaction protocols, while MetaGPT and AFlow extend this paradigm to general task solving through structured role orchestration.</p>
<p>Batch-Generated Dynamic Profiles.This paradigm employs parameterized initialization to systematically generate diverse agent profiles that emulate human societal behaviors.By injecting controlled variations into personality traits, knowledge backgrounds, or value systems during agent creation (e.g., through template-based prompting or latent space sampling), the framework produces heterogeneous populations capable of exhibiting complex social dynamics.Such parameter-driven diversity is essential for simulating realistic human-agent interactions in applications ranging from social behavior studies to emergent group intelligence simulations.This is demonstrated in systems for human behavior simulation [30] and simulated user data collection [31] where different profile configurations directly shape collective interaction patterns.Moreover, DSPy [32] can further optimize the parameters of the agent profile initialization.</p>
<p>Memory Mechanism</p>
<p>Memory mechanisms equip agents with the ability to store, organize, and retrieve information across temporal dimensions.Short-term memory maintains transient contextual data for immediate task execution, while long-term memory preserves structured experiential knowledge for persistent reference.Integrating knowledge retrieval mechanisms further optimizes information accessibility with Retrieval-Augmented Generation (RAG) techniques [43].</p>
<p>Short-Term Memory.Short-term memory retains agentinternal dialog histories and environmental feedback to support context-sensitive task execution.This mechanism is widely implemented in frameworks such as ReAct [33] for thinking with reflection, ChatDev [28] for software development, Graph of Thoughts [34] for solving elaborate problems, and AFlow [29] for workflow automation, demonstrating its versatility across domains.While this mechanism enables detailed reasoning through interactive exchanges, its transient nature limits knowledge retention beyond immediate contexts-intermediate reasoning traces often dissipate after task completion and cannot be directly transferred to new scenarios.Furthermore, due to LLMs' context window limitations, practical implementations require active information compression (e.g., summarization or selective retention) and impose many constraints on multiturn interaction depth to prevent performance degradation.Long-Term Memory.Long-term memory systematically archives agents' intermediate reasoning trajectories and synthesizes them into reusable tools for future invocation.This process transforms ephemeral cognitive efforts into persistent operational assets through three dominant paradigms:  skill libraries that codify procedural knowledge (e.g., Voyager's automated skill discovery in Minecraft [35] and GITM's text-based knowledge base [36]),  experience repositories that store success/failure patterns (e.g., ExpeL's distilled experience pool [37] and Reflexion's trialoptimized memory [38]), and  tool synthesis frameworks that evolve capabilities through combinatorial adaptation (e.g., TPTU's adaptive tool composition [39] and OpenAgents' self-expanding toolkit [40]).Cross-domain implementations, such as Lego-Prover's theorem bank [41] and MemGPT's tiered memory architecture [42], further demonstrate how structured long-term storage enhances reasoning efficiency through strategic knowledge reuse.Knowledge Retrieval as Memory.This paradigm diverges from agent-internal memory generation by integrating external knowledge repositories into generation processes, effectively expanding agents' accessible information boundaries.Current implementations exhibit three dominant approaches:  Static knowledge grounding through text corpora (RAG [43]) or structured knowledge graphs (GraphRAG [44]),  Interactive retrieval that integrates agent dialogues with external queries, as demonstrated in Chain of Agents [45] where short-term inter-agent communications trigger contextualized knowledge fetching, and  Reasoning-integrated retrieval, exemplified by IRCoT [46] and Llatrieval [47], which interleave step-by-step reasoning with dynamic knowledge acquisition.Advanced variants like KG-RAR [48] further construct task-specific subgraphs during reasoning, while DeepRAG [49] introduces fine-tuned retrieval decision modules to balance parametric knowledge and external evidence.These hybrid architectures enable agents to transcend training data limitations while maintaining contextual relevance, establishing knowledge retrieval as critical infrastructure for scalable memory systems.</p>
<p>Planning Capability</p>
<p>Planning capabilities are a critical aspect of LLM agents' abilities, enabling them to navigate through complex tasks and problem-solving scenarios with high accuracy [103].Effective planning is essential for deploying LLM agents in real-world applications, where they must handle a diverse range of complex tasks and scenarios.The planning capability of an LLM agent can be viewed from two perspectives: task decomposition and feedback-driven iteration.Task Decomposition Strategies.Task decomposition represents a basic approach to enhancing LLM planning capabilities by breaking down complex problems into more manageable subtasks.Although solving an entire problem may be challenging for LLM agents, they can more easily handle subtasks and then integrate the results to address the full problem.Task decomposition strategies fall into two main categories: single-path chaining and multi-path tree expansion.</p>
<p>Single-path chaining is a simple method with the simplist version as zero-shot chain-of-thought [104], [105].It first asks the agent to devise a plan, which consists of a sequence of subtasks that are built upon one another.Subsequently, the agent is asked to solve the subtasks in the order they are presented [50], [105].This plan-and-solve paradigm [51] is straightforward and easy to implement.However, it may suffer from a lack of flexibility and error accumulation during chaining, as the agent is required to follow the predefined plan without any deviation during the problemsolving procedure.Therefore, one line of work proposes to adopt dynamic planning that only generates the next subtask based on the current situation of the agent [33], [105].This enables the agent to receive environmental feedback and adjust its plan accordingly, enhancing its robustness and adaptability.Moreover, another line of work proposes to use multiple chain-of-thoughts to improve the robustness of the planning process.This is similar to ensemble methods, involving self-consistency [62], [106], majority voting [107], and agent discussion [52] to combine multiple chains.By combining the wisdom of multiple chains, the agent can make more accurate decisions and reduce the risk of error accumulation.</p>
<p>A more complicated method is to use trees instead of chains as the planning data structure, where multiple possible reasoning paths exist when the agent is planning, and the agent is allowed to backtrack with information from feedback [53], [54].Long et al. [55] propose a treeof-thought (ToT) method that explores the solution space through a tree-like thought process.This allows the LLMs to backtrack to previous states, which makes it possible for the model to correct its previous mistakes, enabling applications to various complicated tasks that involve the "trial-error-correct" process.In more realistic scenarios, the agent can gather feedback from the environment or humans and dynamically adjust its reasoning path, potentially incorporating reinforcement learning [56], [108].This enables the agent to make more informed decisions in real-world applications using advanced algorithms such as Monte Carlo Tree Search [109], facilitating use cases in robotics [57]- [59] and game-playing [110], [111].</p>
<p>Feedback-Driven Iteration.Feedback-driven iteration is a crucial aspect of LLM planning capabilities, enabling the agent to learn from the feedback and enhance its performance over time.Feedback can originate from various sources, such as environmental input, human guidance, model introspection, and multi-agent collaboration.</p>
<p>Environmental feedback is one of the most common types of feedback in robotics [60], generated by the environment in which the embodied agent operates.Human feedback, another crucial type, comes from user interactions or manually labeled data prepared in advance [61], [112].Model introspection provides an additional source of feedback, which is generated by the agent itself [62].Multi-agent collaboration also serves as a feedback mechanism, where multiple agents work together to solve a problem and exchange insights [63], [112].These sources of feedback</p>
<p>Centralized Control</p>
<p>Coscientist [73] Human-centralized experimental control LLM-Blender [74] Cross-attention response fusion MetaGPT [27] Role-specialized workflow management AutoAct [75] Triple-agent task differentiation Meta-Prompting [76] Meta-prompt task decomposition WJudge [77] Weak-discriminator validation</p>
<p>Decentralized Collaboration</p>
<p>MedAgents [78] Expert voting consensus ReConcile [79] Multi-agent answer refinement METAL [115] Domain-specific revision agents DS-Agent [116] Database-driven revision MAD [80] Structured anti-degeneration protocols MADR [81] Verifiable fact-checking critiques MDebate [82] Stubborn-collaborative consensus AutoGen [26] Group-chat iterative debates</p>
<p>Hybrid Architecture</p>
<p>CAMEL [25] Grouped role-play coordination AFlow [29] Three-tier hybrid planning EoT [117] Multi-topology collaboration patterns DiscoGraph [118] Pose-aware distillation DyLAN [119] Importance-aware topology MDAgents [120] Complexity-aware routing help evaluate the agent's performance and thus guide its planning.For instance, the agent can use feedback to update (regenerate) its plan, adjust its reasoning path, or even modify its goal.This iterative process continues until a satisfactory plan is achieved [64], [65].</p>
<p>Action Execution</p>
<p>With the planning capability, it is important for the LLMs to have the ability to execute the planned actions in the real world.Action execution is a critical aspect of LLM agents' abilities, as good plans are useless if the agent cannot execute them effectively.Action execution involves two aspects: tool utilization [113], and physical interaction [114].</p>
<p>Tool utilization [113] is an important aspect of LLM action execution, enabling a wide range of abilities such as precise calculation of numbers, up-to-date information understanding, and proficient code generation.The tool use ability involves two aspects: tool use decision and tool selection.The tool-use decision is the process of deciding whether to use a tool to solve a problem.When the agent is generating content with less confidence or facing problems related to specific tool functions, the agent should decide to use specific tools [66], [67].Tool selection is another important aspect of tool utilization, involving the understanding of tools and the agent's current situation [68], [69].For example, Yuan et al. [68] propose simplifying the tool documentation to better understand the available tools, enabling a more accurate selection of tools.</p>
<p>Physical interaction [114] is a fundamental aspect of embodied LLM agents.Their ability to perform specific actions in the real world and interpret environmental feedback is crucial.When deployed in real-world settings, LLM agents must comprehend various factors to execute actions accurately.These factors include robotic hardware [114], social knowledge [70], and interactions with other LLM agents [71], [72].</p>
<p>Agent Collaboration</p>
<p>Collaboration among LLM agents plays a crucial role in extending their problem-solving capabilities beyond individual reasoning.Effective collaboration enables agents to leverage distributed intelligence, coordinate actions, and refine decisions through multi-agent interactions [26], [121].We categorize existing collaboration paradigms into three fundamental architectures: centralized control, decentralized cooperation, and hybrid architectures.These paradigms differ in their decision hierarchies, communication topologies, and task allocation mechanisms, each offering distinct advantages for specific application scenarios.</p>
<p>Centralized Control</p>
<p>Centralized control architectures employ a hierarchical coordination mechanism where a central controller organizes agent activities through task allocation and decision integration, while other sub-agents can only communicate with the controller.This paradigm features two implementation strategies: explicit controller systems utilize dedicated coordination modules (often implemented as separate LLM agents) to decompose tasks and assign subgoals, while differentiation-based systems achieve centralized control by using prompts to guide the meta agent in assuming distinct sub-roles.The centralized approach excels in mission-critical scenarios requiring strict coordination, such as industrial automation [122] and scientific research [73].Explicit Controller Systems.Multiple related works have been developed to explicitly implenment centralized architectures.The Coscientist [73] exemplifies the explicit controller paradigm, where a human operator serves as the central controller.It establishes standardized scientific experimental workflows, allocates specialized agents and tools to distinct experimental phases, and maintains direct control over the final execution plan.LLM-Blender [74] explicitly creates a controller that employs a cross-attention encoder for pairwise comparison to identify the best responses, and then fuses the top-ranked responses, enhancing their strengths while mitigating weaknesses.MetaGPT [27] simulates realworld software development workflows, direclty assigning specialized managers to control distinct functional roles and phases.Differentiation-based Systems.AutoAct [75] exemplifies the differentiation-based paradigm, which implicitly differentiates the meta-agent into three sub-agents-plan-agent, tool-agent, and reflect-agent-to break down the complex ScienceQA task.Meta-Prompting [76] decomposes complex tasks into domain-specific subtasks through carefully crafted meta-prompts.A single model acts as a coordinator, dynamically assigning subtasks to specialized sub-agents guided by task-oriented prompts.The centrol manager then integrates all intermediate outputs to produce the final solution.These works predominantly employ highly capable agents as central controllers to optimize task allocation and decision aggregation.However, WJudge [77] demonstrates that even controllers with limited discriminative power can also significantly enhance the overall performance of agent systems.</p>
<p>Decentralized Collaboration</p>
<p>In contrast to centralized architectures where a single control node often becomes a bottleneck due to handling all inter-agent communication, task scheduling, and contention resolution, decentralized collaboration enables direct nodeto-node interaction through self-organizing protocols.This paradigm can be further categorized into two distinct approaches: revision-based systems and communication-based systems.</p>
<p>Revision-based Systems.In this paradigm, agents only observe finalized decisions generated by peers and iteratively refine a shared output through structured editing protocols.This approach typically produces more standardized and deterministic outcomes.For instance, MedAgents [78] employs predefined domain-specific expert agents that sequentially propose and modify decisions independently, with consensus achieved through final voting.ReConcile [79] coordinates agents to iteratively refine answers through mutual response analysis, confidence evaluation, and human-curated exemplars.METAL [115] introduces specialized text and visual revision agents for chart generation tasks, demonstrating how domain-specific refinement improves output quality.Notably, revision signals may originate not only from agent interactions but also from external knowledge bases [116], [123], enabling hybrid refinement strategies.Communication-based Systems.Compared to revision-based approaches, communication-based methods feature more flexible organizational structures, allowing agents to directly engage in dialogues and observe peers' reasoning processes.This makes them particularly suitable for modeling dynamic scenarios such as human social interactions [30].Key implementations include: MAD [80] employs structured communication protocols to address the "degeneration-ofthought" problem, where agents overly fixate on initial solutions.MADR [81] enhances this by enabling agents to critique implausible claims, refine arguments, and generate verifiable explanations for fact-checking.MDebate [82] optimizes consensus-building through strategic alternation between stubborn adherence to valid points and collaborative refinement.AutoGen [26] implements a group-chat framework that supports multi-agent participation in iterative debates for decision refinement.</p>
<p>Hybrid Architecture</p>
<p>Hybrid architectures strategically combine centralized coordination and decentralized collaboration to balance controllability with flexibility, optimize resource utilization, and adapt to heterogeneous task requirements.This approach introduces two implementation patterns: static systems with predefined coordination rules and dynamic systems featuring self-optimizing topologies.Static Systems.Static systems predefine fixed patterns for combining different collaboration modalities.Representative implementations include: CAMEL [25] partitions agents into intra-group decentralized teams for role-playing simulations, while maintaining inter-group coordination through centralized governance.AFlow [29] employs a three-tier hierarchy consisting of centralized strategic planning, decentralized tactical negotiation, and market-driven operational resource allocation.EoT [117] formalizes four collaboration patterns (BUS, STAR, TREE, RING) to align network topologies with specific task characteristics.Dynamic Systems.Recent innovations introduce neural topology optimizers that dynamically reconfigure collaboration structures based on real-time performance feedback, enabling automatic adaptation to changing conditions.Key implementations demonstrate this paradigm: DiscoGraph [118] introduces trainable pose-aware collaboration through a teacherstudent framework.The teacher model with holistic-view TABLE 2: A summary of agent evolution methods.</p>
<p>Category</p>
<p>Method Key Contribution</p>
<p>Self-Supervised Learning SE [86] Adaptive token masking for pretraining Evolutionary Optimization [87] Efficient model merging and adaptation DiverseEvol [88] Improved instruction tuning via diverse data</p>
<p>Self-Reflection &amp; Self-Correction</p>
<p>SELF-REFINE [89]</p>
<p>Iterative self-feedback for refinement STaR [90] Bootstrapping reasoning with few rationales V-STaR [91] Training a verifier using DPO Self-Verification [92] Backward verification for correction</p>
<p>Self-Rewarding &amp; RL</p>
<p>Self-Rewarding [93] LLM-as-a-Judge for self-rewarding RLCD [94] Contrastive distillation for alignment RLC [95] Evaluation-generation gap for optimization</p>
<p>Cooperative Co-Evolution</p>
<p>ProAgent [96] Intent inference for teamwork CORY [97] Multi-agent RL fine-tuning CAMEL [25] Role-playing framework for cooperation</p>
<p>Competitive Co-Evolution</p>
<p>Red-Team LLMs [98] Adversarial robustness training Multi-Agent Debate [82] Iterative critique for refinement MAD [99] Debate-driven divergent thinking</p>
<p>Knowledge-Enhanced Evolution</p>
<p>KnowAgent [83] Action knowledge for planning WKM [84] Synthesizing prior and dynamic knowledge</p>
<p>Feedback-Driven Evolution</p>
<p>CRITIC [100] Tool-assisted self-correction STE [101] Simulated trial-and-error for tool learning SelfEvolve [102] Automated debugging and refinement inputs guides the student model via feature map distillation, while matrix-valued edge weights enable adaptive spatial attention across agents.DyLAN [119] first utilizes the Agent Importance Score to identify the most contributory agents and then dynamically adjusts the collaboration structure to optimize task completion.MDAgents [120] dynamically assigns collaboration structures based on the task at hand.It first performs a complexity check to classify tasks as low, moderate, or high complexity.Simple tasks are handled by a single agent, while more complex tasks are addressed through hierarchical collaboration.</p>
<p>Agent Evolution</p>
<p>LLM Agents are evolving through various mechanisms that enable autonomous improvement, multi-agent interaction, and external resource integration.This section explores three key dimensions of agent evolution: autonomous optimization and self-learning, multi-agent co-evolution, and evolution via external resources.These mechanisms collectively enhance model adaptability, reasoning, and performance in complex environments.We summarize the methods in Table 2.</p>
<p>Autonomous Optimization and Self-Learning</p>
<p>Autonomous optimization and self-learning allow LLMs to improve their capabilities without extensive supervision.This includes self-supervised learning, self-reflection, selfcorrection, and self-rewarding mechanisms that enable models to explore, adapt, and refine their outputs dynamically.</p>
<p>Self-Supervised Learning and Adaptive Adjustment.Selfsupervised learning enables LLMs to improve using unlabeled or internally generated data, reducing reliance on human annotations.For example, self-evolution learning (SE) [86] enhances pretraining by dynamically adjusting token masking and learning strategies.Evolutionary optimization techniques facilitate efficient model merging and adaptation, improving performance without extensive additional resources [87].DiverseEvol [88] refines instruction tuning by improving data diversity and selection efficiency.These advancements contribute to the autonomous adaptability of LLMs, enabling more efficient learning and generalization across tasks.</p>
<p>Self-Reflection and Self-Correction.Self-reflection and selfcorrection enable LLMs to iteratively refine their outputs by identifying and addressing errors.For instance, SELF-REFINE [89] applies iterative self-feedback to improve generated responses without external supervision.In reasoning tasks, STaR [90] and V-STaR [91] train models to verify and refine their own problem-solving processes, reducing reliance on labeled data.Additionally, self-verification techniques enable models to retrospectively assess and correct their outputs, leading to more reliable decision-making [92].These approaches collectively enhance LLM agents' ability to self-reflect and self-correct, reducing hallucinations and improving reasoning quality.</p>
<p>Self-Rewarding and Reinforcement Learning.Self-rewarding and reinforcement learning approaches enable LLMs to enhance performance by generating internal reward signals.</p>
<p>Self-generated rewards help models refine decision-making, with techniques ensuring stable and consistent learning improvements [93].Contrastive distillation further enables models to align themselves through self-rewarding mechanisms [94].Additionally, RLC [95] leverages the evaluationgeneration gap via reinforcement learning strategies, facilitating self-improvement.These methods enhance LLM adaptability by integrating self-rewarding strategies and reinforcement learning paradigms.</p>
<p>Multi-Agent Co-Evolution</p>
<p>Multi-agent co-evolution enables LLMs to improve through interactions with other agents.This involves cooperative learning, where agents share information and coordinate actions, as well as competitive co-evolution, where agents engage in adversarial interactions to refine strategies and enhance performance.</p>
<p>Cooperative and Collaborative Learning.Multi-agent collaboration enhances LLMs by enabling knowledge sharing, joint decision-making, and coordinated problem-solving.For instance, ProAgent [96] enables LLM-based agents to adapt dynamically in cooperative tasks by inferring teammates' intentions and updating beliefs, enhancing zeroshot coordination.CORY [97] extends RL fine-tuning into a cooperative multi-agent framework, where LLMs iteratively improve through role-exchange mechanisms, enhancing policy optimality and stability.CAMEL [25] develops a roleplaying framework where communicative agents collaborate autonomously using inception prompting, improving coordination and task-solving efficiency in multi-agent settings.These approaches contribute to more efficient, adaptable, and intelligent multi-agent LLM systems.</p>
<p>Competitive and Adversarial Co-Evolution.Competitive coevolution strengthens LLMs through adversarial interactions, debate, and strategic competition.For example, Red-team LLMs [98] dynamically evolve in adversarial interactions, continuously challenging LLMs to uncover vulnerabilities and mitigate mode collapse, leading to more robust safety alignment.Du et al. propose a multi-agent debate framework [82] to enhance reasoning by having multiple LLMs critique and refine each other's arguments over multiple rounds, improving factuality and reducing hallucinations.Furthermore, the MAD framework [99] structures debates among agents in a tit-for-tat manner, encouraging divergent thinking and refining logical reasoning in complex tasks.These competitive co-evolution strategies drive LLMs to</p>
<p>Benchmark and Datasets Tools</p>
<p>Evaluation and Tools develop stronger reasoning, resilience, and strategic adaptability in a multi-agent adversarial manner.</p>
<p>Evolution via External Resources</p>
<p>External resources enhance the evolution of agents by providing structured information and feedback.Knowledgeenhanced evolution integrates structured knowledge to improve reasoning and decision-making, while external feedback-driven evolution leverages real-time feedback from tools and environments to refine model performance.Knowledge-Enhanced Evolution.LLMs can evolve by integrating structured external knowledge, improving reasoning, decision-making, and task execution.For example, KnowAgent [83] improves LLM-based planning by integrating action knowledge, constraining decision paths, and mitigating hallucinations, leading to more reliable task execution.The world knowledge model (WKM) [84] enhances agent planning by synthesizing expert and empirical knowledge, providing global priors and dynamic local knowledge to guide decisionmaking.These approaches collectively improve the evolution of LLM by incorporating diverse and structured external information.</p>
<p>External Feedback-Driven Evolution.LLMs can refine their behavior by leveraging external feedback from tools, evaluators, and humans to improve performance iteratively.For example, CRITIC [100] allows LLMs to validate and revise their outputs through tool-based feedback, improving accuracy and reducing inconsistencies.STE [101] enhances tool learning by simulating trial-and-error, imagination, and memory, enabling more effective tool use and long-term adaptation.SelfEvolve [102] adopts a two-step framework where LLMs generate and debug code using feedback from execution results, enhancing performance without human intervention.These approaches enable LLMs to evolve iteratively by integrating structured feedback, improving adaptability and robustness.</p>
<p>EVALUATION AND TOOLS</p>
<p>As LLM agents continue to evolve in complexity and capability, robust evaluation frameworks and specialized tools have become essential components of the agent ecosystem.This section explores the comprehensive landscape of benchmarks, datasets, and tools that enable the development, assessment, and deployment of LLM agents.We first examine evaluation methodologies in Section 3.1, covering general assessment frameworks, domain-specific evaluation systems, and collaborative evaluation approaches.We then discuss the tools ecosystem in Section 3.2, including tools used by LLM agents, tools created by agents themselves, and infrastructure for deploying agent systems.</p>
<p>Evaluation Benchmarks and Datasets</p>
<p>The evolution of LLM agents has driven the creation of specialized benchmarks that systematically evaluate agent capabilities across technical dimensions and application domains.These frameworks address three key requirements: general assessment frameworks, domain-specific scenario simulation, and collaborative evaluation of complex systems.</p>
<p>General Assessment Frameworks</p>
<p>The evolution of intelligent agents requires evaluation frameworks to move beyond simple success-rate metrics to comprehensive cognitive analysis.Recent advances focus on building adaptive and interpretable assessment systems capable of capturing the subtle interplay between reasoning depth, environmental adaptability, and task complexity.</p>
<p>Multi-Dimensional Capability Assessment.Modern benchmarks are increasingly adopting a hierarchical paradigm that dissects agent intelligence across various dimensions of reasoning, planning, and problem solving.AgentBench [124] builds a unified test field across eight interactive environments, revealing the advantages of a commercial LLM in complex reasoning.Mind2Web [125] extends this paradigm to web interaction scenarios, proposing the first generalist agent for evaluating 137 real-world websites with different tasks spanning 31 domains.This open environment benchmark enables multi-dimensional capability assessment through real web-based challenges.This is in line with MMAU [126], which enhances explainability through granular capability mapping and breaks down agent intelligence into five core competencies by more than 3,000 cross-domain tasks.BLADE [127] extends evaluation to scientific discovery by tracking the analytical decision patterns of expert validation workflows.VisualAgentBench [128] further extends this approach to multimodal foundation agents, establishing a unified benchmark across materialized interactions, GUI operations, and visual design tasks, and rigorously testing the LLM's ability to handle the dynamics of the complex visual world.Embodied Agent Interface [129] introduces modular inference components (object interpretation, subobject decomposition, etc.) to provide fine-grained error classification for embedded systems.CRAB [130] offers cross-platform testing with graphics-based assessment and a unified Python interface.These frameworks emphasize the shift from a single measure of success to multifaceted cognitive analysis.Dynamic and Self-Evolving Evaluation Paradigms.Nextgeneration framework addresses baseline obsolescence through adaptive generation and human-AI collaboration.BENCHAGENTS [131] automatically creates benchmarks through LLM agents for planning, validating, and measuring designs, enabling rapid capacity expansion.Benchmark self-evolving [132] introduces six refactoring operations to dynamically generate test instances for short-cut biases.</p>
<p>Revisiting Benchmark [133] proposed TestAgent with reinforcement learning for domain adaptive assessment.Other methods such as Seal-Tools [134] (1,024 nested instances of tool calls) and CToolEval [135] (398 Chinese APIs across 14 domains), complement static datasets and standardize tool usage evaluation.</p>
<p>Domain-Specific Evaluation System</p>
<p>The increasing specialization of agent applications requires evaluation systems tailored to domain-specific knowledge and environmental constraints.Researchers are developing dual-axis frameworks that combine vertical competency testing for professional scenarios with horizontal validation in real-world simulated environments.</p>
<p>Domain-Specific Competency Tests.</p>
<p>Several key application areas are specifically benchmarked with scenario-driven assessments.For example, healthcare applications are rigorously tested by MedAgentBench [136] and AI Hospital [137].Specifically, MedAgentBench contains tasks designed by 300 clinicians in an FHIR-compliant environment, while the AI hospital simulates clinical workflows through multi-agent collaboration.The autonomous driving system benefits from LaMPilot [138], which connects the LLM to the autonomous driving architecture through code generation benchmarks.Data science capabilities are evaluated by DSEval [139] and DA-Code [140], covering lifecycle management from data debate to model deployment, while DCA-Bench [141] evaluates dataset curation agents based on real-world quality issues.TravelPlanner [142] provides a sandbox environment for travel planning scenarios.It contains 1225 planning tasks that require multi-step reasoning, tool integration, and constraint balancing under realistic conditions (e.g., budget and time).Machine learning engineering capabilities, measured by MLAgant-Bench [143] and MLE-Bench [144], simulate kaggle-like challenges that require optimization of an end-to-end pipeline.Security-focused AgentHarm [145] curated 440 malicious agent tasks in 11 hazard categories, and systematically assessed LLM abuse risk for the first time in a multi-step tool usage scenario.These domain-specific benchmarks reveal significant performance gaps compared to general testing in practical applications.</p>
<p>Real-World Environment Simulation.Several benchmarks bridge the simulation to reality gap with real interactive environments.OSWorld [146] builds the first scalable realcomputer ecosystem that supports 369 multi-application tasks across Ubuntu/Windows/macOS. TurkingBench [147] evaluates 158 micro-tasks using a crowdsourcing-derived HTML interface, and LaMPilot [138] introduces an executable code generation benchmark for autonomous driving scenarios.OmniACT [148] provides 32K web/desktop automation instances with basic requirements for visualization.EgoLife [149] advances real-world simulation through a 300-hour multimodal egocentric dataset capturing daily human activities (e.g., shopping, cooking, socializing), paired with Ego-LifeQA tasks that test agents' long-term memory retrieval, health habit monitoring, and personalized recommendation capabilities in dynamic environments.GTA [150] further integrates real-world deployed tools and multi-modal inputs (images, web pages) to evaluate real-world problem-solving capabilities.</p>
<p>Collaborative Evaluation of Complex Systems</p>
<p>As agency systems evolve toward organizational complexity, evaluation frameworks must quantify emergent coordination patterns and collective intelligence.Recent approaches shift evaluation from isolated agent proficiency to system-level cognitive collaboration, revealing scalability challenges in multi-agent workflows.</p>
<p>Multi-Agent System Benchmarking.TheAgentCompany [151] pioneered enterprise-level assessments using simulated software company environments to test web interaction and code collaboration capabilities.Comparative analysis like AutoGen and CrewAI [152] establishes methodological standards through ML code generation challenges.Large Visual Language Model Survey [153] systematizes over 200 multimodal benchmarks.For multi-agent collaboration, MLRB [154] designs 7 competition-level ML research tasks, and MLE-Bench [144] evaluates Kaggle-style model engineering through 71 real-world competitions.These efforts collectively establish rigorous evaluation protocols for emergent agent coordination capabilities.</p>
<p>Tools</p>
<p>Tools are an important part of LLM agents.When dealing with complex tasks, LLM agents can call on external tools to generate more precise answers.Depending on their creativity, they can also create tools to solve tasks.In addition, LLM agents need corresponding tools for deployment, maintenance, and data acquisition.</p>
<p>Tools used by LLM agents</p>
<p>Since LLM agents do not perform well in handling some specific tasks, such aas those requiring real-time information and accurate calculations, external tools are introduced to help the LLM agents perform these tasks more effectively.These external tools can be categorized into three main groups.Knowledge Retrieval.For those real-time information that LLM agents are not aware of, knowledge retrieval tools, such as search engines, can help LLM agents to quickly access up-to-date knowledge so that they are no longer limited to the knowledge base they had during training.WebGPT [155] successfully combines online search engines and LLMs with the incorporation of the commercial API 1 .WebCPM [156], inspired by WebGPT, develops a web search interface and uses it to construct the first Chinese long-form question answer (LFQA) dataset.ToolCoder [157] uses DuckDuckgo 2 as the search engine for those frequently used public libraries and employs the BM25 [158] score for those less-known or private libraries.</p>
<p>Computation.LLM agents may suffer hallucinations when dealing with tasks requiring precise computation.Computational tools like Python interpreters and math calculators 1. https://www.microsoft.com/en-us/bing/apis/bing-web-searchapi 2. https://duckduckgo.comcan help LLM agents with complex code execution or computational tasks.AutoCoder [159] designs a dataset with the interaction with coding execution results to facilitate LLMbased code generation.RLEF [160] improves code generation performance through an end-to-end reinforcement learning framework that enables LLMs to learn feedback from code executors.CodeActAgent [161] is an automatic agentic system which can update the actions based on the interaction with the code interpreter.Toolformer [162] integrates a range of tools, including calculators, to significantly improve the performance of models in tasks such as mathematical calculations without compromising the model's generality.ART [163] enables LLM to invoke external tools, such as calculators, when solving complex tasks and excels in mathematical reasoning and complex computational tasks.API Interactions.Building on external APIs, such as REST APT, can enable LLM agents to call external services and extend their functionality, such as manipulating databases and implementing end-to-end automated processes.Rest-GPT [164] explores more realistic scenarios by combining LLM with RESTful APIs and presents RestBench to evaluate the performance of RestGPT.GraphQLRestBench [165] builds a dataset consisting of sequences of natural language statements, and function calls to review existing open-source LLMs, exploring the capabilities of LLMs for API calls.</p>
<p>Tools created by LLM agents</p>
<p>Since the users of traditional tools tend to be humans, LLM agents often have limitations when making calls.In addition, the limitations of existing tools make it difficult to effectively handle new problems.In recent years, many studies have explored how LLM agents can create their tools.CRAFRT [166] provides a flexible framework for tool creation and retrieval by collecting GPT-4 code solutions for specific tasks and abstracting them into code snippets to create specialized tool sets for the tasks.Toolink [167] performs task resolution by creating a toolset and then integrating the planning and invocation of tools through a Chain of Solutions (CoS) approach.CREATOR [168] proposes a four-phase framework-Creation, Decision, Execution, and Reflection-to enable LLM agents to create tools and improve the robustness of the output.LATM [169] proposes a twostage framework that allows LLMs to act as tool makers and tool users, respectively and proposes a tool caching mechanism that improves the efficiency of task solving and reduces the cost while maintaining performance by assigning different models to different tasks with different levels of difficulty.</p>
<p>Tools for deploying LLM agents</p>
<p>LLM tools are essential for the deployment, development, operation, and maintenance of LLM agents and for the secure transmission of data.According to their role, these tools can be categorized into three types.</p>
<p>Productionization.The main purpose of the productionization tools is to make it easy for users to deploy LLM agents in production environments.AutoGen [26] is an open-source framework that enables developers to build LLM applications with customizable, conversational multiple agents.LangChain [170] is an open-source framework for  building LLM applications that is highly extensible and allows users to create custom modules and workflows to meet their specific needs.LlamaIndex [171] is a data framework serving large model applications, allowing users to build LLM applications based on local data.It also provides a rich toolbox for accessing and indexing data, retrieving and reordering, and building custom query engines.Dify [172] is an open-source LLM application development platform that differs from other platforms in that it allows users to build and test powerful AI workflows on canvas.Operation and Maintenance.After deploying LLM agents, the O&amp;M tool ensures that the model performs well during training and remains reliable during production.Ollama [173] is a platform for building LLM agents that also offers observability and monitoring support, allowing teams to track their models' performance in real-time.Dify [172] enables users to monitor and analyze application logs and performance over time, allowing for continuous improvements in prompts, datasets, and models based on production data and annotations.Model Context Protocol.MCP 3 is an open protocol that standardizes how applications provide context to LLMs.It is used to create secure links between LLMs and data sources as well as to build LLM agents and workflows.MCP-Agent [174] is a simple framework to build agents using MCP.As more services become MCP-aware, users will be able to take full advantage of them.</p>
<p>REAL-WORLD ISSUES</p>
<p>As LLM agents become increasingly integrated into various aspects of society, they bring forth significant real-world challenges that must be addressed for responsible deployment.Figure 4 provides an overview of these challenges, categorized into three primary domains: security, privacy, and social impact.Security concerns encompass both agentcentric threats (Section 4.1) that target model components and data-centric threats (Section 4.2) that contaminate input data.Privacy issues (Section 4.3) include memorization vulnerabilities and intellectual property exploitation.Beyond technical 3. https://modelcontextprotocol.io/introductionconcerns, LLM agents raise important ethical considerations and have broad societal implications (Section 4.4), including both potential benefits and risks to society.Understanding these challenges is crucial for developing robust, trustworthy agent systems.</p>
<p>Agent-centric Security</p>
<p>Agent-centric security targets defending different types of attacks on the agent models, where attacks aim to manipulate, tamper, and steal critical components of the weights, architecture, and inference process of the agent models.These agent-centric attacks may lead to performance degradation, maliciously manipulated outputs, and privacy leaks within agent systems.Li et al. [175] analyze the security vulnerabilities of LLM agents under attacks categorized by threat actors, objectives, entry points, and so on.They also conduct experiments on certain popular agents to demonstrate their security vulnerabilities.Agent security bench [176] introduces a comprehensive framework to evaluate attacks and defenses for LLM-based agents across 10 scenarios, 10 agents, 400+ tools, 23 attack/defense methods, and 8 metrics, revealing significant vulnerabilities and limited defense effectiveness of current LLM agents.We summarize the agent-centric security issues in the blow categories.</p>
<p>Adversarial Attacks and Defense</p>
<p>Adversarial attacks aim to compromise the reliability of the agents, rendering them ineffective in specific tasks.Mo et al. [177] categorize adversarial attacks into three components, i.e., Perception, Brain, and Action.AgentDojo [178] provides an evaluation framework designed to measure the adversarial robustness of AI agents by testing them on 97 realistic tasks and 629 security test cases.ARE [179] evaluates multimodal agent robustness under adversarial attacks.For adversarial attack methods, CheatAgent [180] uses an LLM-based agent to attack black-box LLM-empowered recommender systems by identifying optimal insertion positions, generating adversarial perturbations, and refining attacks through iterative prompt tuning and feedback.GIGA [181] introduces generalizable infectious gradient attacks to propagate adversarial inputs across multi-agent, multi-round LLM-powered systems by finding self-propagating inputs that generalize well across contexts.For adversarial attacks defense methods, LLAMOS [182] introduces a defense technique for adversarial attacks by purifying adversarial inputs using agent instruction and defense guidance before they are input into the LLM.Chern et al. [183] introduce a multi-agent debate method to reduce the susceptibility of agents to adversarial attacks.</p>
<p>Jailbreaking Attacks and Defense</p>
<p>Jailbreaking attacks attempt to break through the protection of the model and obtain unauthorized functionality or information.For jailbreaking attack methods, RLTA [184] uses reinforcement learning to automatically generate attacks that produce malicious prompts, triggering LLM agents' jailbreaking to produce specific output.These can be adapted to both white box and black box scenarios.Atlas [185] jailbreaks text-to-image models with safety filters using a mutation agent and a selection agent, enhanced by in-context learning and chain-of-thought techniques.RLbreaker [186] is a black-box jailbreaking attack using deep reinforcement learning to model jailbreaking as a search problem, featuring a customized reward function and PPO algorithm.Path-Seeker [187] also uses multi-agent reinforcement learning to guide smaller models in modifying inputs based on the target LLM's feedback, with a reward mechanism leveraging vocabulary richness to weaken security constraints.For jailbreaking defense methods, AutoDefense [188] proposes a multi-agent defense framework that uses LLM agents with specialized roles to collaboratively filter harmful responses, effectively resisting jailbreak attacks.Guardians [189] uses three examination methods-reverse Turing Tests, multiagent simulations, and tool-mediated adversarial scenarios-to detect rogue agents and counter jailbreaking attacks.ShieldLearner [190] proposes a novel defense paradigm for jailbreak attacks by autonomously learning attack patterns and synthesizing defense heuristics through trial and error.</p>
<p>Backdoor Attacks and Defense</p>
<p>Backdoor attacks implant specific triggers to cause the model to produce preset errors when encountering these triggers while performing normally under normal inputs.For backdoor attack methods, DemonAgent [191] proposes a dynamically encrypted muti-backdoor implantation attack method by using dynamic encryption to map and decompose backdoors into multiple fragments to avoid safety audits.Yang et al. [192] investigate and implement diverse forms of backdoor attacks on LLM-based agents, demonstrating their vulnerability through experiments on tasks like web shopping and tool utilization.BadAgent [193] attacks LLM-based intelligent agents to trigger harmful operations through specific inputs or environment cues as backdoors.BadJudge [194] introduces a backdoor threat specific to the LLM-as-a-judge agent system, where adversaries manipulate evaluator models to inflate scores for malicious candidates, demonstrating significant score inflation across various data access levels.DarkMind [195] is a latent backdoor attack that exploits the reasoning processes of customized LLM agents by covertly altering outcomes during the reasoning chain without requiring trigger injection in user inputs.</p>
<p>Model Collaboration Attacks and Defense</p>
<p>Model collaboration attack is an emerging type of attack that mainly targets scenarios where multiple models work together.In this type of attack, attackers manipulate the interaction or collaboration mechanisms between multiple models to disrupt the overall functionality of the system.For model collaboration attack methods, CORBA [196] introduces a novel yet simple attack method for the LLM multi-agent system.It exploits contagion and recursion, which are hard to mitigate via alignment, disrupting agent interactions.AiTM [197] introduces an attack method to the LLM multiagent system by intercepting and manipulating inter-agent messages using an adversarial agent with a reflection mechanism.For the defense methods, Netsafe [198] identifies critical safety phenomena and topological properties that influence the safety of multi-agent networks against adversarial attacks.G-Safeguard [199] is also based on topology guidance and leverages graph neural networks to detect anomalies TABLE 3: Summary of agent-centric attacks and defense in LLM agents.</p>
<p>Reference Description Adversarial Attacks and Defense</p>
<p>Mo et al. [177]</p>
<p>Attack: Adversarial attack benchmark AgentDojo [178] Attack: Adversarial attack framework ARE [179] Attack: Adversarial attack evaluation for multimodal agents GIGA [181] Attack: Generalizable infectious gradient attacks CheatAgent [180] Attack: Adversarial attack agent for recommender systems LLAMOS [182] Defense: Purifying adversarial attack input Chern et al. [183] Defense: Defense via multi-agent debate</p>
<p>Jailbreaking Attacks and Defense</p>
<p>RLTA [184] Attack: Produce jailbreaking prompts via reinforcement learning Atlas [185] Attack: Jailbreaks text-to-image models with safety filters RLbreaker [186] Attack: Model jailbreaking as a search problem PathSeeker [187] Attack: Use multi-agent reinforcement learning to jailbreak AutoDefense [188] Defense: Multi-agent defense to filter harmful responses Guardians [189] Defense: Detect rogue agents to counter jailbreaking attacks.ShieldLearner [190] Defense: Learn attack jailbreaking patterns.</p>
<p>Backdoor Attacks and Defense</p>
<p>DemonAgent [191] Attack: Encrypted muti-backdoor implantation attack Yang et al. [192] Attack: Backdoor attacks evaluations on LLM-based agents BadAgent [193] Attack: Inputs or environment cues as backdoors BadJudge [194] Attack: Backdoor to the LLM-as-a-judge agent system DarkMind [195] Attack: latent backdoor attack to customized LLM agents</p>
<p>Agent Collaboration Attacks and Defense</p>
<p>CORBA [196] Attack: Multi-agent attack via multi-agent AiTM [197] Attack: Intercepte and manipulate inter-agent messages Netsafe [198] Defense: Identify critical safety phenomena in multi-agent networks G-Safeguard [199] Defense: leverages graph neural networks to detect anomalies Trustagent [200] Defense: Agent constitution in task planning.PsySafe [201] Defense: Mitigate safety risks via agent psychology in the LLM multi-agent system.Trustagent [200] aims to enhance the planning safety of LLM agentic framework in three different planning stages.PsySafe [201] is grounded in agent psychology to identify, evaluate, and mitigate safety risks in multi-agent systems by analyzing dark personality traits, assessing psychological and behavioral safety, and devising risk mitigation strategies.</p>
<p>Data-centric Security</p>
<p>The goal of data-centric attacks is to contaminate the input data of LLM agents, ultimately leading to unreasonable tool calling, aggressive outputs and resource depletion, etc [202].</p>
<p>In data-centric attacks, any components in LLM agent systems or default parameters are not allowed to be modified.Based on the data type, we categorize attacks into external data attacks and execution data attacks.Corresponding defense strategies are summarized to counter these agent attacks.</p>
<p>External Data Attack and Defense</p>
<p>User Input Falsifying.Modifying the user input is the most straightforward and widely used data-centric attacks.These injections [176] can lead to uncontrolled and dangerous outputs.Though it is simple, it always achieves the highest Attack Success Rate (ASR) [176], [203].Li et al. [204] propose malicious prefix prompts, such as "ignore the document".InjectAgent [205] and Agentdojo [203] are two prompt injection benchmarks, which test the single and multi-turn attacks in LLM agents.As the widespread effect of injections on user inputs increases, various defense models have been designed.Mantis [206] defenses through hacking back to attackers' own systems.[207] offers a defense module called the Input Firewall, which extracts key points from users' natural language and converts them into a structured JSON format.RTBAS [208] and TaskShield [209] check the every step of information flow and agent process, including function calls and tool execution, to make sure the execution aligns with the original instructions and intentions.In the ASB [176] benchmark, a sandwich defend strategy adds additional guarding instructions to help LLM agents ignore malicious injections.Dark Psychological Guidance.Attackers can carry out dark psychological guidance in the prompts, e.g., use "cheating" instead of "care", "betrayal" instead of "fairness", "subversion" instead of "authority".Then LLM agents are guided to be aggressive and antisocial, which may cause serious social impacts.[210] proposes the "Evil Geniuses" to generate prompts to put agents into specific role-playing states.Its prompts are optimized through the red-blue exercises.[201] injects the dark psychological traits into the user inputs.</p>
<p>To defense dark psychological injections, doctor and police agents [201] are incorporated into the agents systems.The doctor agents conduct the psychological assessment, while the police agents supervise the safety of agent systems.They work together to guard the healthy psychology at any time.External Source Poisoning.Many attackers pay their attention to the RAG-based LLM agents, as they have been proven to be more reliable than general memory-based LLM agents [211].The attackers inject poisoning samples into the knowledge databases [175], [212].Based on this, the Indirect Prompt Injection (IPI) attack embeds malicious instructions into other external knowledge sources [213], such as the websites, support literature, emails, online BBS, which can manipulate agents and cause them to deviate from the original intentions.WIPI [214] controls the agents through a public web page to indirectly poison instructions.</p>
<p>[215] describes a Foot-in-the-Door (FITD) attack, which begins with inconspicuous, unrelated requests and gradually incorporates harmless ones.This approach increases the likelihood of the agent executing subsequent actions, leading to resource consumption that could have been avoided.</p>
<p>AgentPoison [216] is a typical red teaming work, which achieves a high success rate in knowledge-intensive QA agent.[183] employs a multi-agent debate for defense, where each agent acts as a domain expert to verify the facticity of external knowledge.</p>
<p>Interaction Attack and Defense</p>
<p>Interaction between user and agent interface.Some LLM agents store the private user-agent interactions in users' computer memory to enhance dialogue performance.During these interactions, LLM agents are usually black-box to attackers.[217] is a private memory extraction attack that aggregates multiple levels of knowledge from the stored memory.[218] presents an attack that occurs at the interface between users and LLM agents, where it solicits information from users.Interaction among LLM agents.In multi-agent LLM systems, the interactions among agents are frequent and essential [12].Attackers poison a single agent, which then infects other agents [219].This recursive attack can ultimately deplete the computational resources.AgentSmith [220] concludes that the infectious spread occurs exponentially fast.The Contagious Recursive Blocking Attack (CORBA) [196] is designed to disrupt the communications among agents, TABLE 4: Summary of data-centric attack and defense in LLM agents.</p>
<p>Reference Description External Data Attacks and Security</p>
<p>Li et al. [204] Attack: Malicious prefix injection Psysafe [201] Attack: A dark psychological injection benchmark Tian et al. [210] Attack: Guide agents into specific role-playing states InjectAgent [205] Attack: A prompting injection benchmark Agentdojo [203] Attack: A user injection benchmark AgentPoison [216] Attack: Poisoning samples in knowledge databases Nakash et al. [215] Attack: Indirect prompt injection through FITD attack WIPI [214] Attack: control agents through a public web page ASB [176] Attack: A multi-type attack benchmark AgentHarm [223] Attack: A multi-type attack benchmark Mantis [206] Defense: Hacking back to attackers Chern et al. [183] Defense: Employ multi-agent debate to verify external knowledge RTBAS [208] Defense: Check every step of agent information flow TaskShield [209] Defense: Check every step of agent process Zhang et al. [201] Defense: Doctor and police agents guard the healthy psychology</p>
<p>Interaction Attacks and Security</p>
<p>Wang et al. [217] Attack: Private memory extraction attack CORBA [196] Attack: Disrupt the communications among agents AgentSmith [220] Attack: Poison one agent to infectious other agents Lee et al. [221] Attack: Conduct injections to self-replicate among agents He et al. [197] Attack: Inject semantic disruptions to agent communications BlockAgents [222] Defense: Incorporate blockchain and PoT against byzantine attacks Abdelnabi et al. [207] Defense: A multi-layer agent firewall allowing the infection to propagate across the entire communication network.[197] incorporates a reflection mechanism to finish disruptions based on the semantic understanding of communications.[221] injects malicious instructions into one agent, enabling them to self-replicate across the agent network, resembling the spread of a computer virus.Additionally, [221] develops a tagging strategy to control the infection spread.To defend against Byzantine attacks during the agent interactions, BlockAgents [222] introduces a consensus mechanism based on blockchain and proofof-thought (PoT) techniques.The agent that contributes the most to the planning process is granted the accounting rights.</p>
<p>Interaction between agents and tools.</p>
<p>To call appropriate tools, the agents first make a plan, and then finish the action.The interaction between agents and tools is vulnerable.Some attackers maliciously modify planning thoughts, and thus alter the agent actions.The agent may call unconvincing or harmful tools to complete the task, and further cause unexpected consequences.AgentHarm [223] adds harmful distractions during multi-step execution tasks.InjectAgent [205] conducts attacks during the agent planning process.The multi-layer agent firewall [207] incorporates a self-correction mechanism, known as the trajectory firewall layer, to correct the deviated trajectory of agents.This firewall layer verifies the generated responses to ensure compliance with security rules.</p>
<p>Privacy</p>
<p>The widespread use of LLMs in multi-agent systems has also raised several privacy concerns.These issues are mainly caused by the memory capacity of LLMs, which may lead to the leakage of private information during conversations or when completing tasks.In addition, LLM agents are vulnerable to attacks involving model and prompt theft, along with other forms of intellectual property theft.This section explores the privacy threats posed by LLM Memorization Vulnerabilities and LLM Intellectual Property Exploitation emphasizing the importance of ensuring the safe and secure deployment of LLMs in collaborative environments.Addi-tionally, it discusses potential countermeasures to mitigate these risks.</p>
<p>LLM Memorization Vulnerabilities</p>
<p>It has been shown that LLMs are able to generate text similar to humans.However, such generated text may be retained training data, which poses serious privacy protection issues.These risks are particularly severe in multi-agent systems, where LLMs may leak sensitive information when collaborating to solve complex tasks.This section explores the privacy threats posed by LLM memory and discusses protection measures against these threats.Data Extraction Attacks.They exploit the memory capacity of LLMs to extract sensitive information from training data.Carlini et al. [224] show that an attacker can extract personally identifiable information (PII) such as name, email, and phone number from a GPT-2 model through specific queries.The risk of data extraction increases with model size, frequency of repeated data, and context length [225].Huang et al. [226] further study data extraction attacks against pretrained LLMs such as GPT-neo, highlighting the feasibility of such attacks in practical applications.</p>
<p>Member Inference Attacks.</p>
<p>Their purpose is to determine whether a particular data sample has been part of the LLM training data.Mireshghallah et al. [227] empirically analyze the vulnerability of fine-tuned LLMs to membership inference attacks and find that fine-tuning the model head makes it more vulnerable to such attacks.Fu et al. [228] propose a self-calibrated membership inference attack method based on probability changes, which provides a more reliable membership signal through these variations.This type of attack is particularly dangerous in multi-agent systems, as the training data may originate from multiple sources of sensitive information.In response to these risks, protection strategies such as differential privacy (DP) and knowledge distillation have been developed [229], [230].</p>
<p>Attribute Inference Attacks.The goal of attribute inference attacks is to infer a certain feature or characteristic of a data sample using training data.To confirm the existence of sensitive attribute inference in LLMs, Pan et al. [231] conduct an in-depth study of privacy issues related to attribute inference attacks in LLMs.Wang et al. [232] study attribute existence inference attacks on generative models and find that most generative models are vulnerable to such attacks.</p>
<p>Protective Measures.Several protective strategies have been proposed to reduce the chance of LLM memorization.Data cleaning strategies can successfully reduce the risk of memorization by locating and eliminating sensitive information in training data [233].Another effective way to minimize privacy leakage is to introduce differential privacy noise into model gradients and training data [229] during pretraining and fine-tuning.Knowledge distillation techniques have become an intuitive means of privacy protection by transferring knowledge from private teacher models to public student models [230].In addition, privacy leakage detection tools such as ProPILE can help service providers assess the extent of their PII leakage before deploying LLM agents [234].TABLE 5: Summary of privacy threats and countermeasures in LLM agents.</p>
<p>Reference Description</p>
<p>LM Memorization Vulnerabilities</p>
<p>Carlini et al. [224] Attack: Data Extraction Huang et al. [226] Attack: Data Extraction on Pretrained LLMs Mireshghallah et al. [227] Attack: Membership Inference on Fine-Tuned LLMs Fu et al. [228] Attack: Self-Calibrated Membership Inference Pan et al. [231] Attack: Attribute Inference in General-Purpose LLMs Wang et al. [232] Attack: Property Existence Inference in Generative Models Kandpal et al. [233] Defense: Data Sanitization to Mitigate Memorization Hoory et al. [229] Defense: Differential Privacy for Pre-Trained LLMs Kang et al. [230] Defense: Knowledge Distillation for Privacy Preservation Kim et al. [234] Defense: Privacy Leakage Assessment Tool</p>
<p>LM Intellectual Property Exploitation</p>
<p>Krishna et al. [235] Attack: Model Stealing via Query APIs Naseh et al. [236] Attack: Stealing Decoding Algorithms of LLMs Li et al. [237] Attack: Extracting Specialized Code Abilities from LLMs Shen et al. [240] Attack: Prompt Stealing in Text-to-Image Models Sha et al. [241] Attack: Prompt Stealing in LLMs Hui et al. [242] Attack: Closed-Box Prompt Extraction Kirchenbauer et al. [238] Defense: Model Watermarking for IP Protection Lin et al. [239] Defense: Blockchain for IP Verification</p>
<p>LM Intellectual Property Exploitation</p>
<p>LLM agents are subject to memory concerns as well as privacy risks associated with intellectual property (IP), such as model theft and prompt theft.These attacks put both individuals and organizations at serious danger by taking advantage of the LLMs's economic value and signaling.</p>
<p>Model Stealing Attacks.Model theft attacks attempt to extract model information (such as parameters or hyperparameters) by querying the model and observing its responses.Krishna et al. [235] show that an attacker can steal information from language models such as BERT through multiple queries without accessing the original training data.Naseh et al. [236] demonstrate that attackers can steal the types and hyperparameters of LLM decoding algorithms at a low cost.Li et al. [237] investigate the feasibility of extracting specialized code from LLMs, highlighting the risk of model theft in multi-agent systems.In response to these attacks, protective measures such as model watermarking [238] and blockchain-based IP authentication [239] have been proposed.</p>
<p>Prompt Stealing Attacks.Prompt theft attacks involve inferring original hints from generated content that may have significant business value.Shen et al. [240] conduct the first study of prompt stealer attacks against text-toimage generation models and propose an effective attack method called PromptStealer.Sha et al. [241] extend this study to LLMs, using a parameter extractor to determine the properties of the original prompt.Hui et al. [242] propose PLEAK, a closed-box prompt extraction framework that extracts system prompts for LLM applications by optimizing adversarial queries.To prevent prompt theft, adversarial samples have been proposed as an effective method to obstruct attackers from inferring the original prompt by introducing disturbance to the generated content [240].</p>
<p>The privacy challenges for LLM agents are multifaceted, ranging from memory threats to risks related to intellectual property.As LLMs continue to evolve, robust privacy protection technologies must be developed to mitigate these privacy risks while ensuring that LLMs play an effective role in multi-agent systems.</p>
<p>Social Impact and Ethical Concerns</p>
<p>LLM agents profoundly impact society, driving automation, industrial innovation, and productivity gains.However, ethical concerns remain.The following section explores both the benefits and challenges associated with their use.We summarize the content in Table 6.</p>
<p>Benefits to Sociaty</p>
<p>LLM agents have significantly impacted human society, offering numerous benefits across various domains.Automation Enhancement.LLM agents have found applications across diverse fields, including healthcare, biomedicine, law, and education [243].By automating labor-intensive tasks, they reduce time costs and enhance efficacy.In healthcare, for example, they assist in interpreting clinical symptoms, explaining lab results, and even drafting medical documentation.In legal and educational settings, they streamline administrative work, generate summaries, and provide instant, context-aware responses [243]- [245].Their ability to alleviate repetitive workloads allows professionals to focus on more complex, high-stake tasks, ultimately improving productivity and accessibility across industries.</p>
<p>Job Creation and Workforce Transformation.While researchers acknowledge the potential for AI agents to replace human jobs and disrupt the job market [243], others argue that their advancements will reshape workforce demands [246].The rise of LLM agents is transforming the job market, not only expanding technical roles such as machine learning engineers and data scientists but also driving demand for managerial positions like AI project managers and business strategists.Given their growing economic impact, governments are encouraged to support AI-focused training programs to equip individuals for this evolving landscape.Unlike LLMs, which often require specialized expertise to use effectively, LLM agents are designed for accessibility, attracting a broader user base and enabling wider applications across various industries.As a result, their societal impact is expected to surpass that of LLMs or other AI models alone, bringing both challenges and unprecedented opportunities.</p>
<p>Enhance Information Distribution.Businesses reliant on large-scale text generation, such as online advertising, benefit significantly from LLM agents.However, their misuse is a growing concern, particularly regarding the proliferation of fake news and misinformation [244], [245].Beyond accelerating advertisement distribution, enhanced information dissemination offers broader societal benefits.For instance, the global shortage of patient, experienced, and knowledgeable teachers has long been a challenge.LLM agents introduce transformative solutions, such as intelligent online tutoring systems, revolutionizing education accessibility [247].</p>
<p>Ethical Concerns</p>
<p>Although LLM agents bring numerous benefits to society, they also pose potential risks that cannot be overlooked.These challenges raise significant ethical concerns, including bias in decision-making, misinformation propagation, and privacy issues, highlighting the need for responsible development and regulation.</p>
<p>Bias and Discrimination.LLM agents inherently inherit biases present in their training datasets and may even amplify them during the learning process, leading to skewed outputs and reinforcing existing stereotypes [248].Recognizing this issue, many existing works have implemented strategies to mitigate harmful content generation.These methods include filtering sensitive topics, applying reinforcement learning with human feedback, and refining model training processes to promote fairness and reduce bias [243]- [245].The pursuit of fairness has become a critical focus in studies on LLM agents, as researchers strive to develop models that minimize bias, promote inclusivity, and ensure ethical AI deployment in real-world applications [249], [250].Accountability.Despite efforts to mitigate toxic content in LLM agents, the risk of harmful outputs persists [244], [245], [251].Accountability remains a key challenge, as documented datasets provide limited oversight, while vast amounts of undocumented data can be easily integrated into training.Rigorous dataset documentation is essential, despite its costs [252].Additionally, proper governance frameworks are necessary to ensure accountability in LLM agents [253], [254].</p>
<p>Copyright.Copyright concerns are closely linked to privacy and accountability.Some argue that AI should adhere to the same legal and ethical standards as humans, ensuring fair use and intellectual property protection [250].Many creators oppose their work being used to train models that could replace them, yet the absence of clear regulations and the growing demand for data lead to widespread misuse [255].This issue is often underestimated and requires urgent attention, as it threatens human creators, increases the prevalence of AI-generated content over human-produced work in certain domains, and risks content degradation, particularly when large AI models are increasingly trained on AI-generated data [256].Addressing these issues is particularly crucial in the use of LLM agents, where users often lack direct awareness of the training data sources.This opacity increases the risk of unintended consequences, as individuals may unknowingly rely on models trained on controversial datasets, potentially resulting in reputational harm or even legal repercussions.Others.Some ethical concerns in the use of LLM agents, such as privacy [243], [257], [258], data manipulation [259], and misinformation [244], [260], are so critical that we provide a thorough discussion in Sections 4.1, 4.2 and 4.3.Beyond these, additional ethical concerns remain.One major issue is that LLM agents lack true semantic and contextual understanding, relying purely on statistical word associations.This limitation is often misinterpreted and overestimated, leading to undue reliance on these models [244], especially when their behavior may not align well with human intentions [261].Moreover, concerns have been raised about the significant carbon footprint of LLM agents, posing environmental challenges [262], alongside the high computational costs associated with training large models [263].</p>
<p>APPLICATIONS</p>
<p>The versatility of LLM agents has led to their adoption across diverse domains, transforming how complex tasks are approached in both research and industry settings.This section</p>
<p>Impact Reference</p>
<p>Benefits to Society</p>
<p>Automation Enhancement Foundation Models [243], GPT-3 [244], LLaMA [245] Workforce Transformation</p>
<p>Foundation Models [243], Redefining Work [246] Enhance Information Distribution GPT-3 [244], LLaMa [245], Empower Online Education [247]</p>
<p>Ethical Concerns</p>
<p>Bias and Discrimination</p>
<p>Fair Use [249], Fair Learning [250]</p>
<p>Accountability</p>
<p>Stochastic Parrots [252], Governance [253], [254]</p>
<p>Copyright</p>
<p>Fair Learning [250], Ethics of LLMs [255], AI collapse [256]</p>
<p>Data Privacy</p>
<p>Foundation Models [243], Ethical and Social Risks [257] Manipulation &amp; Misinformation Data-Poisoning Attacks [259] Others Overreliance [244], Alignment [261], Carbon Footprint [262], Expenses [263] surveys the broad spectrum of LLM agent applications, from accelerating scientific discovery (Section 5.1) to enhancing interactive gaming experiences (Section 5.2), modeling complex social phenomena (Section 5.3), and boosting productivity (Section 5.4).These applications demonstrate how the integration of LLM-based agent systems enables enhanced problem-solving capabilities through specialized knowledge application, multi-agent collaboration, and human-AI interaction paradigms.</p>
<p>Scientific Discovery</p>
<p>By leveraging multiple specialized LLM agents that communicate and coordinate, LLM-based multi-agent AI systems can combine diverse expertise, access external tools, and decompose tasks, thereby extending the capabilities of single LLMs [264], [265].In this part, we survey advances in applying LLM-driven multi-agent systems to scientific research over the past three years.</p>
<p>Agentic AI Across Scientific Disciplines</p>
<p>LLM-based multi-agent systems are increasingly applied across scientific disciplines to emulate human collaborative workflows and tackle complex, interdisciplinary problems that require diverse knowledge and skills.For example, the SciAgents [266] framework uses distinct LLM agents such as "Ontologist," "Scientist," and "Critic" to collectively generate and refine scientific hypotheses.Centered on an ontological knowledge graph that encodes relationships between scientific concepts, SciAgents orchestrates ChatGPT-4-based agents to generate novel research ideas and experimental plans.In a case study on bio-inspired materials, one agent generated a proposal to integrate silk with novel pigments; another agent suggested simulation experiments to test the idea, and a critical agent identified weaknesses and prompted improvements.Beyond hypothesis generation, LLM-based agents are being used to plan and execute experimental research.For instance, Curie [267] developed an AI agent framework for rigorous automated experimentation.In Curie, an Architect agent first designs high-level experimental plans to answer a scientific question, then multiple Technician agents carry out specific experimental steps.In tests on questions derived from computer science research papers, Curie's structured multi-agent approach improved the correctness of experimental results, outperforming more straightforward prompt-based automation by a notable margin.This indicates that multi-agent systems can bring not just creativity but also discipline and reliability.Aside from scientific findings, LLMs are also used to improve the generation pipeline of academic works.AgentReview [268] proposes an LLM-agent-based framework for simulating academic peer review processes, offering valuable insights to improve the design of evaluation protocols for academic papers.</p>
<p>Agentic AI in Chemistry, Materials Science and Astronomy</p>
<p>Due to the abundance of digital tools and data in these fields, chemistry, materials science, and Astronomy have been early adopters of LLM-based agentic AI.In the chemistry domain, ChemCrow [269] exemplifies an LLM-driven chemistry agent designed to foster scientific advancement by bridging the gap between experimental and computational chemistry.</p>
<p>ChemCrow integrates an LLM with a suite of 18 expertdesigned chemistry tools, such as molecule property predictors, reaction planners and databases, enabling it to plan and execute chemical syntheses autonomously.Materials science problems, which often span multiple scales and modalities (from atomic simulations to empirical data), also benefit from multi-agent AI.AtomAgents [270] framework is a physics-aware multi-agent system for automating alloy design.In this system, a Planner agent (GPT-4) decomposes a complex materials design challenge into a sequence of tasks, which are then verified by a Critic agent and delegated to specialist modules.Similar principles are being applied in physics and astronomy.For example, an AI copilot agent has been developed for the Cherenkov Telescope Array in astronomy [271], using an instruction-tuned LLM to autonomously manage telescope configuration databases and even generate code for data analysis workflows.Although still experimental, these efforts indicate that LLM-based agents could soon be used in physics labs and astronomical observatories.They could handle routine decision-making and free human scientists to focus on high-level insights.</p>
<p>Agentic AI in Biology</p>
<p>The life sciences are likewise beginning to embrace LLMbased multi-agent systems for hypothesis generation and data analysis [272].One notable direction is using LLM agents to propose biological experiments or interpret multiomics data.BioDiscoveryAgent [273] proposed an AI agent to design genetic perturbation experiments in molecular biology.By parsing literature and gene databases, an LLM agent can suggest which gene knockouts or edits might elucidate a certain biological pathway.Another system, GeneAgent [274], uses a self-refinement loop to discover gene associations from biomedical databases, improving the reliability of findings by cross-checking against known gene sets.RiGPS [275] developed a multi-agent system with an experiment-based self-verified reinforcement learning framework, enhancing the biomarker identification task in the single-cell dataset.BioRAG [211] developed a multi-agent-based RAG system to handle biology-related QA, where several agents are designed to retrieve information using multiple tools, and one agent is specifically used to self-evaluate the retrieval results.These examples illustrate the methodology of selfquestioning or self-verification in multi-agent AI: one or more agents propose a scientific insight, and another evaluates its plausibility with known knowledge, thereby reducing errors.</p>
<p>Agentic AI in Scientific Dataset Construction</p>
<p>Multi-agent systems also accelerate the construction of scientific datasets.For instance, PathGen-1.6M[276]
Method Domain Core Idea Scientific Discovery
SciAgents [266] General Sciences Collaborative hypothesis generation Curie [267] General Sciences Automated experimentation ChemCrow [269] Chemistry Tool-augmented synthesis planning AtomAgents [270] Materials Science Physics-aware alloy design D. Kostunin el al [271] Astronomy Telescope configuration management BioDiscoveryAgent [273] Biology Genetic perturbation design GeneAgent [274] Biology Self-verifying gene association discovery RiGPS [275] Biology Biomarker identification BioRAG [211] Biology Biology-focused retrieval augmentation PathGen-1.6M[276] Medical Dataset Pathology image dataset generation KALIN [277] Biology Dataset Scientific question corpus generation GeneSUM [278] Biology Dataset Gene function knowledge maintenance AgentHospital [281] Medical Virtual hospital simulation ClinicalLab [282] Medical Multi-department diagnostics AIPatient [283] Medical Patient simulation CXR-Agent [284] Medical Chest X-ray interpretation MedRAX [285] Medical Multimodal medical reasoning</p>
<p>Gaming</p>
<p>ReAct [33] Game Playing Reasoning and acting in text environments Voyager [35] Game Playing Lifelong learning in Minecraft ChessGPT [287] Game Playing Chess gameplay evaluation GLAM [288] Game Playing Reinforcement learning in text environments CALYPSO [289] Game Generation Narrative generation for D&amp;D GameGPT [290] Game Generation Automated game development Sun et al. [291] Game Generation Interactive storytelling experience</p>
<p>Social Science</p>
<p>Econagent [292] Economy Economic decision simulation TradingGPT [293] Economy Financial trading simulation CompeteAI [294] Economy Market competition modeling Ma et al. [295] Psychology Mental health support analysis Zhang et al. [296] Psychology Social behavior simulation TE [297] Psychology Psychological experiment simulation Generative agents [30] Social Simulation Human behavior emulation Liu et al. [298] Social Simulation Learning from social interactions S 3 [299] Social Simulation Social network behavior modeling</p>
<p>Productivity Tools</p>
<p>SDM [300] Software Development Self-collaboration for code generation ChatDev [301] Software Development Chat-powered development framework MetaGPT [27] Software Development Meta-programming for collaboration Agent4Rec [302] Recommender Systems User behavior modeling AgentCF [303] Recommender Systems User-item interaction modeling MACRec [304] Recommender Systems Multi-agent recommendation RecMind [305] Recommender Systems Knowledge-enhanced recommendation X-ray cases that require referring to patient history and imaging simultaneously.Evaluations of these approaches on standard chest X-ray benchmarks [286] showed that it could achieve diagnostic accuracy on par with state-of-the-art standalone models while also providing an uncertainty score that correlates with its correctness.In summary, the multiagent paradigm in medicine holds promise for improving AI reliability by introducing redundancy, specialization, and oversight.However, it also complicates the system, requiring rigorous validation.</p>
<p>Gaming</p>
<p>The development of LLM agents offers an unprecedented opportunity in gaming, enabling agents to take on diverse roles and exhibit human-like decision-making skills in intricate game environments.Based on the different characteristics of the games and roles of the agent, the applications can be categorized into game playing and game generation.Game Playing.In role-playing games, LLM agents can assume various character roles, both as player-controlled characters and non-player characters (NPCs).ReAct [33] prompts LLMs to integrate reasoning and reflection into action generation, enhancing decision-making in the embodied environment.Voyager [35] introduces an LLM-powered lifelong learning agent in Minecraft that persistently explores the game world.ChessGPT [287] presents an autonomous agent on mixed game-language data to facilitate board state evaluation and chess gameplay.GLAM [288] builds an agent in the BabyAI-text environment, where a policy is used to select the next action, with training conducted through online reinforcement learning.</p>
<p>Game Generation.In game generation, LLMs are used to create dynamic and interactive game content.CALYPSO [289] creates LLM agents as the assistants to help build a compelling narrative to present in the context of playing Dungeons &amp; Dragons.GameGPT [290] leverages dual-agent collaboration and a hierarchical approach, using multiple internal dictionaries to automate and enhance the game development process.Sun et al. [291] create an interactive storytelling game experience in 1001 Nights, where instructive language models and image generation are combined to shape the narrative and world.</p>
<p>Social Science</p>
<p>The application of LLM agents in social science has seen significant advancements, providing new opportunities for understanding and simulating complex human behaviors and interactions.These models facilitate insights into various domains, including economics, psychology and social simulation.Below, we explore how LLM agents are being applied across these three critical areas.</p>
<p>Economy.In economics, LLM agents are utilized to analyze financial data and simulate financial activities.Econagent [292] employs prompt engineering to create agents that mimic human-like decisions or macroeconomic simulations.</p>
<p>TradingGPT [293] presents a multi-agent framework for financial trading, which simulates human decision processes by incorporating hierarchical memory structures and debate mechanisms with individualized trading profiles.CompeteAI [294] leverages LLM agents to model a virtual town where restaurants and customers interact, providing insights consistent with sociological and economic theories.</p>
<p>Psychology.In psychological research, LLM agents are utilized to model human behavior with diverse traits and cognitive processes.Ma et al. [295] investigate the psychological effects and potential benefits of using LLM-based conversational agents for mental health support.Zhang et al. [296] examine how LLM agents with unique traits and thought processes replicate human-like social behaviors, including conformity and majority influence.TE [297] uses LLM agents to simulate psychological experiments, potentially revealing consistent distortions in how language models replicate specific human behaviors.</p>
<p>Social Simulation.In societal simulation, LLM agents are employed to model complex societal behaviors.These simulations help in understanding real-world phenomena, such as social influence, information diffusion, and collective decision-making.Generative agents [30] introduce a multiagent interaction model within an interactive sandbox environment, leveraging LLM agents to simulate realistic human behavior in a variety of contexts.Building on this, Liu et al. [298] introduce a training paradigm that enables LLMs to learn from these simulated social interactions involving multiple LLM agents.S 3 [299] develops an LLM-based multiagent system to ensure the agents' behaviors closely mimic those of real humans within social networks.</p>
<p>Productivity Tools</p>
<p>LLM agents are increasingly leveraged to boost productivity by automating diverse tasks, facilitating collaboration in solving complex problems, and optimizing efficiency across multiple domains.Below, we highlight their applications in software development and recommender systems.</p>
<p>Software Development.Since software development involves multiple roles, such as product managers, developers, and testers, all working together to deliver high-quality products, LLM agents are increasingly being used to streamline various aspects of the process.SDM [300] introduces a self-collaboration framework that guides multiple LLM agents to work together on code generation tasks, enhancing their ability to tackle complex software development challenges collaboratively.ChatDev [301] proposes a chatpowered software development framework, where agents are guided on both what to communicate and how to communicate effectively.MetaGPT [27] further incorporates human workflows (i.e., Standardized Operating Procedures) into LLM-powered multi-agent collaboration through a meta-programming approach to enhance coordination and streamline the collaborative process.</p>
<p>Recommender Systems.In the realm of recommender systems, LLM agents are increasingly utilized to simulate user behaviors.Agent4Rec [302] employs LLM agents with integrated user profiling, memory, and action modules to model user behavior in recommender systems.AgentCF [303] treats both users and items as LLM agents, introducing a collaborative learning framework to model user-item interactions in recommender systems.MACRec [304] directly develops multiple agents to tackle the recommendation task.</p>
<p>RecMind [305] employs LLM agents to incorporate external knowledge and carefully plans the utilization of tools for zero-shot personalized recommendations.</p>
<p>CHALLENGES AND FUTURE TRENDS</p>
<p>Advancements in LLM-based multi-agent systems bring significant opportunities but also present pressing challenges in scalability, memory, reliability, and evaluation.This section outlines key obstacles and emerging trends shaping the future of agentic AI.</p>
<p>Scalability and Coordination</p>
<p>Scaling LLM-based multi-agent systems remains challenging due to high computational demands, inefficiencies in coordination, and resource utilization [306], [307].Existing multi-agent frameworks, designed for lightweight agents like function calls and rule-based systems [308], [309], lack system-level optimization for LLM agents with billionscale parameters [26].Future directions include hierarchical structuring, where high-level LLM agents delegate subtasks to specialized lower-level agents, and decentralized planning, which enables agents to plan concurrently and synchronize periodically to mitigate bottlenecks.Advancements in robust communication protocols and efficient scheduling mechanisms are needed to enhance coordination, real-time decisionmaking, and system robustness [306], [307].</p>
<p>Memory Constraints and Long-Term Adaptation.</p>
<p>Maintaining coherence across multi-turn dialogues and the longitudinal accumulation of knowledge requires effective memory mechanisms [310].However, as LLMs possess very limited effective context [74], [311], integrating sufficient historical information into prompts becomes challenging.This hinders the models' contextual awareness over extended interactions.Ensuring interaction continuity requires efficient memory scalability and relevance management [312] beyond current practice such as vector databases, memory caches, context window management, and retrieval-augmented generation (RAG) [43].Future directions include hierarchical memory architectures that combine episodic memory for shortterm planning with semantic memory for long-term retention, as well as autonomous knowledge compression [313] to refine memory dynamically and enhance reasoning over extended interactions.</p>
<p>Reliability and Scientific Rigor</p>
<p>LLMs, while knowledge-rich, are neither comprehensive nor up-to-date, thus potentially unsuitable as standalone replacements for structured databases.Their stochastic nature makes outputs highly sensitive to minor variations in prompts [314], causing hallucinations [315] and compounding uncertainty in multi-agent systems, such as agentic frameworks for medical applications and autonomous scientific discovery [316], where unreliable outputs can mislead high-stake decision-making.Addressing these challenges necessitates the development of rigorous validation mechanisms and structured verification pipelines, including knowledge-graphbased verification, where outputs are cross-checked against structured databases [317], and cross-referencing via retrieval, which grounds responses in cited source like web pages as in WebGPT [318].Along this direction, future work can explore LLMs capable of direct citation generation, as well as up-to-date and comprehensive knowledge sources readily available for LLM applications.Meanwhile, in high-stakes domains like healthcare, law, or scientific research, pure automation remains risky.AI-human verification loops are becoming standard for ensuring safety, reliability, and accountability [315].Future works can enhance cross-referencing mechanisms [319], self-consistency [320], and standardized AI auditing frameworks, such as fact-checking logs, to improve accountability.For example, one critical challenge is determining optimal intervention points amid the vast scale of LLM-generated content.</p>
<p>Multi-turn, Multi-agent Dynamic Evaluation</p>
<p>Traditional AI evaluation frameworks, designed for static datasets and single-turn tasks, fail to capture the complexities of LLM agents in dynamic, multi-turn, and multi-agent environments [310].Current benchmarks primarily assess task execution such as code completion [321], [322] and dialogue generation [57] in isolated settings, overlooking emergent agent behaviors, long-term adaptation, and collaborative reasoning that unfold across multi-turn interactions.Additionally, static benchmarks struggle to keep pace with evolving LLM capabilities [323].Concerns persist regarding potential data contamination, where model performance may stem from memorization rather than genuine reasoning.Future research should focus on dynamic evaluation methodologies, integrating multi-agent interaction scenarios, structured performance metrics, and adaptive sample generation algorithms [324] to create more robust and reliable assessment frameworks.</p>
<p>Regulatory Measures for Safe Deployment</p>
<p>As agentic AI systems gain autonomy, regulatory frameworks must evolve to ensure accountability, transparency, and safety.</p>
<p>A key challenge is mitigating algorithmic bias-agents may inadvertently discriminate based on gender, age, ethnicity, or other sensitive attributes, often in ways imperceptible to developers [248], [325].Addressing this requires standardized auditing protocols to systematically identify and correct biases, alongside traceability mechanisms that log decision-making pathways and model confidence for posthoc accountability.Future work can explore multidisciplinary approaches combining fairness-aware training pipelines with legal and ethical safeguards.Collaboration between policymakers, researchers, and industry stakeholders will be critical to ensuring AI-driven systems operate safely and equitably in alignment societal values [326].</p>
<p>Role-playing Scenarios</p>
<p>LLM agents can simulate roles such as researchers, debators, and instructors [307], [327], but their effectiveness is constrained by training data limitations and an incomplete understanding of human cognition [326], [328].Since LLMs are predominantly trained on web-based corpora, they struggle to emulate roles with insufficient representation online [329] and often produce conversations lacking diversity [268].Future research should focus on enhancing role-play fidelity by improving multi-agent coordination, incorporating realworld reasoning frameworks, and refining dialogue diversity to better support complex human-AI interactions.</p>
<p>CONCLUSION</p>
<p>This survey has presented a systematic taxonomy of LLM agents, deconstructing their methodological components across construction, collaboration, and evolution dimensions.</p>
<p>We have advanced a unified architectural perspective that bridges individual agent design principles with multi-agent collaborative systems-an approach that distinguishes our work from previous surveys.Despite remarkable progress, significant challenges remain, including scalability limitations, memory constraints, reliability concerns, and inadequate evaluation frameworks.Looking forward, we anticipate transformative developments in coordination protocols, hybrid architectures, self-supervised learning, and safety mechanisms that will enhance agent capabilities across diverse domains.By providing this foundational understanding and identifying promising research directions, we hope to contribute to the responsible advancement of LLM agent technologies that may fundamentally reshape humanmachine collaboration.</p>
<p>Fig. 3 :
3
Fig. 3: An overview of evaluation benchmarks and tools for LLM agents.The left side shows various evaluation frameworks categorized by general assessment, domainspecific evaluation, and collaboration evaluation.The right side illustrates tools used by LLM agents, tools created by agents, and tools for deploying agents.</p>
<p>Fig. 4 :
4
Fig.4: An overview of real-world issues in LLM agent systems, organized into three domains: security challenges (including agent-centric and data-centric threats), privacy concerns (covering memorization vulnerabilities and intellectual property exploitation), and social impact considerations (highlighting both benefits and ethical challenges).</p>
<p></p>
<p>Junyu Luo, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Wei Ju, Chenwu Liu, Jingyang Yuan, and Ming Zhang are with the School of Computer Science and PKU-Anker LLM Lab, Peking University, Beijing, China.(e-mail: luojunyu@stu.pku.edu.cn,mzhang cs@pku.edu.cn) Weizhi Zhang and P.S. Yu are with the Department of Computer Science, University of Illinois at Chicago, Chicago, USA. Ziyue Qiao is with the School of Computing and Information Technology, Great Bay University, Guangdong, China. Qingqing Long and Meng Xiao are with the Computer Network Information Center, Chinese Academy of Sciences, Beijing, China. Rongcheng Tu, Hanqing Zhao, and Dacheng Tao are with Nanyang Technological University, Singapore. Xiao Luo is with the Department of Computer Science, University of California, Los Angeles, USA.</p>
<p> Zhiping Xiao is with Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA. Yifan Wang is with the School of Information Technology &amp; Management, University of International Business and Economics, Beijing, China. Shichang Zhang is with Harvard University, Cambridge, USA. Yiqiao Jin is with Georgia Institute of Technology, Atlanta, USA. Fan Zhang and Xian Wu are with Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China.</p>
<p>TABLE 1 :
1
A summary of agent collaboration methods.
CategoryMethodKey Contribution</p>
<p>TABLE 6 :
6
Overview of Social Impacts and Ethical Considerations in LLM Agents.</p>
<p>TABLE 7 :
7
Overview of Applications in LLM Agents.
gen-
[285]ration.For instance, CXR-Agent[284]uses a visionlanguage model together with an LLM to interpret chest X-rays and generate radiology reports with uncertainty estimates.MedRAX[285]integrates several specialized tools, such as an optical character reader for reading prior reports, a segmentation model for highlighting image regions, and an LLM for clinical reasoning, to solve complex chest</p>
<p>The rise and potential of large language model based agents: A survey. Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, Science China Information Sciences. 6821211012025</p>
<p>Intelligent agents: Theory and practice. M Wooldridge, N R Jennings, The knowledge engineering review. 1021995</p>
<p>Large language models as reliable knowledge bases. D Zheng, M Lapata, J Z Pan, arXiv:2407.135782024arXiv preprint</p>
<p>Non-vacuous generalization bounds for large language models. S Lotfi, M Finzi, Y Kuang, T G Rudner, M Goldblum, A G Wilson, arXiv:2312.171732023arXiv preprint</p>
<p>From multimodal llm to human-level ai: Modality, instruction, reasoning, efficiency and beyond. H Fei, Y Yao, Z Zhang, F Liu, A Zhang, T.-S Chua, COLING. 2024</p>
<p>Towards reasoning in large language models: A survey. J Huang, K C , -C Chang, arXiv:2212.104032022arXiv preprint</p>
<p>Tool-lmm: A large multi-modal model for tool agent learning. C Wang, W Luo, Q Chen, H Mai, J Guo, S Dong, Z Li, L Ma, S Gao, 20242401arXiv e-prints</p>
<p>A survey on the memory mechanism of large language model based agents. Z Zhang, X Bo, C Ma, R Li, X Chen, Q Dai, J Zhu, Z Dong, J.-R Wen, arXiv:2404.135012024arXiv preprint</p>
<p>An in-depth survey of large language model-based artificial intelligence agents. P Zhao, Z Jin, N Cheng, arXiv:2309.143652023arXiv preprint</p>
<p>Cognitive architectures for language agents. T Sumers, S Yao, K Narasimhan, T Griffiths, 2023TMLR</p>
<p>S Hu, T Huang, F Ilhan, S Tekin, G Liu, R Kompella, L Liu, arXiv:2404.02039A survey on large language model-based game agents. 2024arXiv preprint</p>
<p>X Xu, Y Wang, C Xu, Z Ding, J Jiang, Z Ding, B F Karlsson, arXiv:2403.10249A survey on game playing agents and large models: Methods, applications, and challenges. 2024arXiv preprint</p>
<p>Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services. M Xu, H Du, D Niyato, J Kang, Z Xiong, S Mao, Z Han, A Jamalipour, D I Kim, X Shen, IEEE Communications Surveys &amp; Tutorials. 2622024</p>
<p>Mobile edge intelligence for large language models: A contemporary survey. G Qu, Q Chen, W Wei, Z Lin, X Chen, K Huang, 2025IEEE Communications Surveys &amp; Tutorials</p>
<p>Agent ai: Surveying the horizons of multimodal interaction. Z Durante, Q Huang, N Wake, R Gong, J S Park, B Sarkar, R Taori, Y Noda, D Terzopoulos, Y Choi, arXiv:2401.035682024arXiv preprint</p>
<p>Large model agents: State-of-the-art, cooperation paradigms, security and privacy, and future trends. Y Wang, Y Pan, Q Zhao, Y Deng, Z Su, L Du, T H Luan, arXiv:2409.144572024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, Frontiers of Computer Science. 1861863452024</p>
<p>A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. X Li, S Wang, S Zeng, Y Wu, Y Yang, Vicinagearth. 1192024</p>
<p>A review of prominent paradigms for llm-based agents: Tool use (including rag), planning, and feedback learning. X Li, arXiv:2406.058042024arXiv preprint</p>
<p>A comprehensive survey on multi-agent cooperative decision-making: Scenarios, approaches, challenges and perspectives. W Jin, H Du, B Zhao, X Tian, B Shi, G Yang, arXiv:2503.134152025arXiv preprint</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024arXiv preprint</p>
<p>Large language model based multiagents: A survey of progress and challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. T Masterman, S Besen, M Sawtell, A Chao, arXiv:2404.115842024arXiv preprint</p>
<p>Y Cheng, C Zhang, Z Zhang, X Meng, S Hong, W Li, Z Wang, Z Wang, F Yin, J Zhao, arXiv:2401.03428Exploring large language model based intelligent agents: Definitions, methods, and prospects. 2024arXiv preprint</p>
<p>Camel: Communicative agents for "mind" exploration of large language model society. G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, NeurIPS2023</p>
<p>Autogen: Enabling next-gen llm applications via multiagent conversation. Q Wu, G Bansal, J Zhang, Y Wu, B Li, E Zhu, L Jiang, X Zhang, S Zhang, J Liu, A H Awadallah, R W White, D Burger, C Wang, 2023</p>
<p>Metagpt: Meta programming for a multi-agent collaborative framework. S Hong, X Zheng, J Chen, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, ICLR. 2024</p>
<p>Chatdev: Communicative agents for software development. C Qian, W Liu, H Liu, N Chen, Y Dang, J Li, C Yang, W Chen, Y Su, X Cong, ACL. 2024</p>
<p>AFlow: Automating agentic workflow generation. J Zhang, J Xiang, Z Yu, F Teng, X.-H Chen, J Chen, M Zhuge, X Cheng, S Hong, J Wang, B Liu, Y Luo, C Wu, ICLR. 2025</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, UIST. 2023</p>
<p>User behavior simulation with large language model-based agents. L Wang, J Zhang, H Yang, Z.-Y Chen, J Tang, Z Zhang, X Chen, Y Lin, H Sun, R Song, ACM Transactions on Information Systems. 4322025</p>
<p>Dspy: Compiling declarative language model calls into self-improving pipelines. O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, S Vardhamanan, S Haq, A Sharma, T T Joshi, H Moazam, H Miller, M Zaharia, C Potts, ICLR. 2024</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, ICLR2023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, AAAI. 202438690</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, 2023TMLR</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. X Zhu, Y Chen, H Tian, C Tao, W Su, C Yang, G Huang, B Li, L Lu, X Wang, arXiv:2305.171442023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, AAAI. 202419642</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, NeurIPS. 362023</p>
<p>Tptu: Task planning and tool usage of large language model-based ai agents. J Ruan, Y Chen, B Zhang, Z Xu, T Bao, H Mao, Z Li, X Zeng, R Zhao, NeurIPS2023</p>
<p>Openagents: An open platform for language agents in the wild. T Xie, F Zhou, Z Cheng, P Shi, L Weng, Y Liu, T J Hua, J Zhao, Q Liu, C Liu, arXiv:2310.106342023arXiv preprint</p>
<p>Lego-prover: Neural theorem proving with growing libraries. H Wang, H Xin, C Zheng, Z Liu, Q Cao, Y Huang, J Xiong, H Shi, E Xie, J Yin, ICLR. 2024</p>
<p>Memgpt: Towards llms as operating systems. C Packer, V Fang, S G Patil, K Lin, S Wooders, J E Gonzalez, 2023CoRR</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H ttler, M Lewis, W -T. Yih, T Rocktschel, NeurIPS. 332020</p>
<p>From local to global: A graph rag approach to query-focused summarization. D Edge, H Trinh, N Cheng, J Bradley, A Chao, A Mody, S Truitt, D Metropolitansky, R O Ness, J Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Chain of agents: Large language models collaborating on long-context tasks. Y Zhang, R Sun, Y Chen, T Pfister, R Zhang, S Arik, Advances in Neural Information Processing Systems. 202437</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions. H Trivedi, N Balasubramanian, T Khot, A Sabharwal, arXiv:2212.105092022arXiv preprint</p>
<p>Llatrieval: Llmverified retrieval for verifiable generation. X Li, C Zhu, L Li, Z Yin, T Sun, X Qiu, NAACL. 2024</p>
<p>Graph-augmented reasoning: Evolving step-by-step knowledge graph retrieval for llm reasoning. W Wu, Y Jing, Y Wang, W Hu, D Tao, 2025</p>
<p>Deeprag: Thinking to retrieval step by step for large language models. X Guan, J Zeng, F Meng, C Xin, Y Lu, H Lin, X Han, L Sun, J Zhou, arXiv:2502.011422025arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R K W Lee, E.-P Lim, arXiv:2305.040912023arXiv preprint</p>
<p>Distributed problem solving and planning. E H Durfee, ECCAI Advanced Course on Artificial Intelligence. Springer2001</p>
<p>Chain-of-discussion: A multimodel framework for complex evidence-based question answering. M Tao, D Zhao, Y Feng, arXiv:2402.163132024arXiv preprint</p>
<p>Tree-planner: Efficient closeloop task planning with large language models. M Hu, Y Mu, X Yu, M Ding, S Wu, W Shao, Q Chen, B Wang, Y Qiao, P Luo, arXiv:2310.085822023arXiv preprint</p>
<p>Reactree: Hierarchical task planning with dynamic tree expansion using llm agent nodes. J.-W Choi, H Kim, H Ong, Y Yoon, M Jang, J Kim, 2025</p>
<p>Large language model guided tree-of-thought. J Long, arXiv:2305.082912023arXiv preprint</p>
<p>Restmcts*: Llm self-training via process reward guided tree search. D Zhang, S Zhoubian, Z Hu, Y Yue, Y Dong, J Tang, NeurIPS. 372024</p>
<p>Llmmars: Large language model for behavior tree generation and nlpenhanced dialogue in multi-agent robot systems. A Lykov, M Dronova, N Naglov, M Litvinov, S Satsevich, A Bazhenov, V Berman, A Shcherbak, D Tsetserukou, arXiv:2312.093482023arXiv preprint</p>
<p>Llm as btplanner: Leveraging llms for behavior tree generation in robot task planning. J Ao, F Wu, Y Wu, A Swikir, S Haddadin, arXiv:2409.104442024arXiv preprint</p>
<p>Conceptagent: Llm-driven precondition grounding and tree search for robust task planning and execution. C Rivera, G Byrd, W Paul, T Feldman, M Booker, E Holmes, D Handelman, B Kemp, A Badger, A Schmidt, arXiv:2410.061082024arXiv preprint</p>
<p>Grounding llms for robot task planning using closed-loop state feedback. V Bhat, A U Kaypak, P Krishnamurthy, R Karri, F Khorrami, arXiv:2402.085462024arXiv preprint</p>
<p>Traineragent: Customizable and efficient model training through llm-powered multi-agent system. H Li, H Jiang, T Zhang, Z Yu, A Yin, H Cheng, S Fu, Y Zhang, W He, arXiv:2311.066222023arXiv preprint</p>
<p>Dynamic self-consistency: Leveraging reasoning paths for efficient llm sampling. G Wan, Y Wu, J Chen, S Li, arXiv:2408.170172024arXiv preprint</p>
<p>Llm-based cooperative agents using information relevance and plan validation. S Seo, J Lee, S Noh, H Kang, arXiv:2405.167512024arXiv preprint</p>
<p>Adaplanner: Adaptive planning from feedback with language models. H Sun, Y Zhuang, L Kong, B Dai, C Zhang, NeurIPS. 362452023</p>
<p>Adaptive iterative feedback prompting for obstacle-aware path planning via llms. M Jafaripour, S Golestan, S Miwa, Y Mitsuka, O Zaiane, AAAI Workshop. 2025</p>
<p>Making language models better tool learners with execution feedback. S Qiao, H Gui, C Lv, Q Jia, H Chen, N Zhang, arXiv:2305.130682023arXiv preprint</p>
<p>Gpt4tools: Teaching large language model to use tools via selfinstruction. R Yang, L Song, Y Li, S Zhao, Y Ge, X Li, Y Shan, NeurIPS. 3672023</p>
<p>Easytool: Enhancing llm-based agents with concise tool instruction. S Yuan, K Song, J Chen, X Tan, Y Shen, R Kan, D Li, D Yang, arXiv:2401.062012024arXiv preprint</p>
<p>Avatar: Optimizing llm agents for tool usage via contrastive reasoning. S Wu, S Zhao, Q Huang, K Huang, M Yasunaga, K Cao, V Ioannidis, K Subbian, J Leskovec, J Y Zou, NeurIPS. 37102025</p>
<p>Drivlme: Enhancing llm-based autonomous driving agents with embodied and social experiences. Y Huang, J Sansom, Z Ma, F Gervits, J Chai, IROS. 2024IEEE</p>
<p>Towards efficient llm grounding for embodied multi-agent collaboration. Y Zhang, S Yang, C Bai, F Wu, X Li, Z Wang, X Li, arXiv:2405.143142024arXiv preprint</p>
<p>Improving embodied llm agents capabilities through collaboration. B Colle, 2024</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Llmlingua: Compressing prompts for accelerated inference of large language models. H Jiang, Q Wu, C.-Y Lin, Y Yang, L Qiu, EMNLP. 202313</p>
<p>Autoact: Automatic agent learning from scratch for qa via self-planning. S Qiao, N Zhang, R Fang, Y Luo, W Zhou, Y E Jiang, C Lv, H Chen, arXiv:2401.052682024arXiv preprint</p>
<p>Meta-prompting: Enhancing language models with task-agnostic scaffolding. M Suzgun, A T Kalai, arXiv:2401.129542024arXiv preprint</p>
<p>Debating with more persuasive llms leads to more truthful answers. A Khan, J Hughes, D Valentine, L Ruis, K Sachan, A Radhakrishnan, E Grefenstette, S R Bowman, T Rocktschel, E Perez, arXiv:2402.067822024arXiv preprint</p>
<p>Medagents: Large language models as collaborators for zero-shot medical reasoning. X Tang, A Zou, Z Zhang, Z Li, Y Zhao, X Zhang, A Cohan, M Gerstein, arXiv:2311.105372023arXiv preprint</p>
<p>Reconcile: Round-table conference improves reasoning via consensus among diverse llms. J C -Y. Chen, S Saha, M Bansal, arXiv:2309.130072023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, S Shi, Z Tu, arXiv:2305.191182023arXiv preprint</p>
<p>Can llms produce faithful explanations for fact-checking? towards faithful explainable fact-checking via multi-agent debate. K Kim, S Lee, K.-H Huang, H P Chan, M Li, H Ji, arXiv:2402.074012024arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, ICML. 2023</p>
<p>Knowagent: Knowledge-augmented planning for llm-based agents. Y Zhu, S Qiao, Y Ou, S Deng, N Zhang, S Lyu, Y Shen, L Liang, J Gu, H Chen, arXiv:2403.031012024arXiv preprint</p>
<p>Agent planning with world knowledge model. S Qiao, R Fang, N Zhang, Y Zhu, X Chen, S Deng, Y Jiang, P Xie, F Huang, H Chen, NeurIPS. 372024</p>
<p>Refining guideline knowledge for agent planning using textgrad. R Fang, S Qiao, Z Xi, ICKG. IEEE2024</p>
<p>Self-evolution learning for discriminative language model pretraining. Q Zhong, L Ding, J Liu, B Du, D Tao, ACL Findings. 2023</p>
<p>Evolutionary optimization of model merging recipes. T Akiba, M Shing, Y Tang, Q Sun, D Ha, Nature Machine Intelligence. 2025</p>
<p>Self-evolved diverse data sampling for efficient instruction tuning. S Wu, K Lu, B Xu, J Lin, Q Su, C Zhou, arXiv:2311.081822023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, NeurIPS. 362023</p>
<p>Star: Self-taught reasoner bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N D Goodman, NeurIPS. 11262024</p>
<p>V-star: Training verifiers for self-taught reasoners. A Hosseini, X Yuan, N Malkin, A Courville, A Sordoni, R Agarwal, COLM. 2024</p>
<p>Large language models are better reasoners with self-verification. Y Weng, M Zhu, F Xia, B Li, S He, S Liu, B Sun, K Liu, J Zhao, EMNLP Findings. 2023</p>
<p>Self-rewarding language models. W Yuan, R Y Pang, K Cho, X Li, S Sukhbaatar, J Xu, J Weston, 2024</p>
<p>Rlcd: Reinforcement learning from contrastive distillation for lm alignment. K Yang, D Klein, A Celikyilmaz, N Peng, Y Tian, ICLR2024</p>
<p>Language model self-improvement by reinforcement learning contemplation. J.-C Pang, P Wang, K Li, X.-H Chen, J Xu, Z Zhang, Y Yu, ICLR2024</p>
<p>Proagent: building proactive cooperative agents with large language models. C Zhang, K Yang, S Hu, Z Wang, G Li, Y Sun, C Zhang, Z Zhang, A Liu, S.-C Zhu, AAAI. 202438599</p>
<p>Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning. H Ma, T Hu, Z Pu, L Boyin, X Ai, Y Liang, M Chen, NeurIPS. 375252024</p>
<p>Evolving diverse red-team language models in multi-round multiagent games. C Ma, Z Yang, H Ci, J Gao, M Gao, X Pan, Y Yang, arXiv:2310.003222023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, S Shi, Z Tu, EMNLP. 202417904</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, Y Yang, N Duan, W Chen, ICLR2024</p>
<p>Trial and error: Exploration-based trajectory optimization of llm agents. Y Song, D Yin, X Yue, J Huang, S Li, B Y Lin, ACL. 2024</p>
<p>Selfevolve: A code evolution framework via large language models. S Jiang, Y Wang, Y Wang, arXiv:2306.029072023arXiv preprint</p>
<p>Understanding the planning of llm agents: A survey. X Huang, W Liu, X Chen, X Wang, H Wang, D Lian, Y Wang, R Tang, E Chen, arXiv:2402.027162024arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, NeurIPS. 352022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, NeurIPS. 352022</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Enhancing chain-of-thought reasoning in large language models through text style diversity and prompt fusion. W Li, W Pan, EIBDCT. SPIE202413181</p>
<p>J Jiang, Z Chen, Y Min, J Chen, X Cheng, J Wang, Y Tang, H Sun, J Deng, W X Zhao, arXiv:2411.11694Technical report: Enhancing llm reasoning with reward-guided tree search. 2024arXiv preprint</p>
<p>A survey of monte carlo tree search methods. C B Browne, E Powley, D Whitehouse, S M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Can large language models play games? a case study of a self-play approach. H Guo, Z Liu, Y Zhang, Z Wang, arXiv:2403.056322024arXiv preprint</p>
<p>Large language models as agents in two-player games. Y Liu, P Sun, H Li, arXiv:2402.080782024arXiv preprint</p>
<p>A survey on enhancing reinforcement learning in complex environments: Insights from human and llm feedback. A R Laleh, M N Ahmadabadi, arXiv:2411.134102024arXiv preprint</p>
<p>Llm with tools: A survey. Z Shen, arXiv:2409.188072024arXiv preprint</p>
<p>Understanding large-language model (llm)-powered human-robot interaction. C Y Kim, C P Lee, B Mutlu, HRI. 2024</p>
<p>Metal: A multiagent framework for chart generation with test-time scaling. B Li, Y Wang, J Gu, K.-W Chang, N Peng, arXiv:2502.176512025arXiv preprint</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. S Guo, C Deng, Y Wen, H Chen, Y Chang, J Wang, arXiv:2402.174532024arXiv preprint</p>
<p>Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. Z Yin, Q Sun, C Chang, Q Guo, J Dai, X Huang, X Qiu, arXiv:2312.018232023arXiv preprint</p>
<p>Learning distilled collaboration graph for multi-agent perception. Y Li, S Ren, P Wu, S Chen, C Feng, W Zhang, NeurIPS. 342021</p>
<p>A dynamic llmpowered agent network for task-oriented agent collaboration. Z Liu, Y Zhang, P Li, Y Liu, D Yang, COLM. 2024</p>
<p>Mdagents: An adaptive collaboration of llms for medical decision-making. Y Kim, C Park, H Jeong, Y S Chan, X Xu, D Mcduff, H Lee, M Ghassemi, C Breazeal, H Park, NeurIPS. 374522024</p>
<p>Inferring the goals of communicating agents from actions and instructions. L Ying, T Zhi-Xuan, V Mansinghka, J B Tenenbaum, Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20232</p>
<p>Autonomous industrial control using an agentic framework with large language models. J Vyas, M Mercang z, arXiv:2411.059042024arXiv preprint</p>
<p>Data-driven revision of conditional norms in multi-agent systems. D Dell'anna, N Alechina, F Dalpiaz, M Dastani, B Logan, Journal of Artificial Intelligence Research. 752022</p>
<p>X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023arXiv preprint</p>
<p>Mind2web: Towards a generalist agent for the web. X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, NeurIPS. 362023</p>
<p>Mmau: A holistic benchmark of agent capabilities across diverse domains. G Yin, H Bai, S Ma, F Nan, Y Sun, Z Xu, S Ma, J Lu, X Kong, A Zhang, arXiv:2407.189612024arXiv preprint</p>
<p>Blade: Benchmarking language model agents for data-driven science. K Gu, R Shang, R Jiang, K Kuang, R.-J Lin, D Lyu, Y Mao, Y Pan, T Wu, J Yu, arXiv:2408.096672024arXiv preprint</p>
<p>X Liu, T Zhang, Y Gu, I L Iong, Y Xu, X Song, S Zhang, H Lai, X Liu, H Zhao, arXiv:2408.06327Visualagentbench: Towards large multimodal models as visual foundation agents. 2024arXiv preprint</p>
<p>Embodied agent interface: Benchmarking llms for embodied decision making. M Li, S Zhao, Q Wang, K Wang, Y Zhou, S Srivastava, C Gokmen, T Lee, E L Li, R Zhang, NeurIPS. 372025</p>
<p>Crab: Cross-platfrom agent benchmark for multi-modal embodied language model agents. T Xu, L Chen, D.-J Wu, Y Chen, Z Zhang, X Yao, Z Xie, Y Chen, S Liu, B Qian, NeurIPS Workshop. 2024</p>
<p>Benchagents: Automated benchmark creation with agent interaction. N Butt, V Chandrasekaran, N Joshi, B Nushi, V Balachandran, arXiv:2410.225842024arXiv preprint</p>
<p>Benchmark selfevolving: A multi-agent framework for dynamic llm evaluation. S Wang, Z Long, Z Fan, Z Wei, X Huang, arXiv:2402.114432024arXiv preprint</p>
<p>Revisiting benchmark and assessment: An agent-based exploratory dynamic evaluation framework for llms. W Wang, Z Ma, P Liu, M Chen, arXiv:2410.115072024arXiv preprint</p>
<p>Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. M Wu, T Zhu, H Han, C Tan, X Zhang, W Chen, NLPCC. Springer2024</p>
<p>Ctooleval: a chinese benchmark for llm-powered agent evaluation in real-world api interactions. Z Guo, Y Huang, D Xiong, ACL Findings. 2024</p>
<p>Medagentbench: Dataset for benchmarking llms as agents in medical applications. Y Jiang, K C Black, G Geng, D Park, A Y Ng, J H Chen, arXiv:2501.146542025arXiv preprint</p>
<p>Ai hospital: Benchmarking large language models in a multi-agent medical interaction simulator. Z Fan, J Tang, W Chen, S Wang, Z Wei, J Xi, F Huang, J Zhou, arXiv:2402.097422024arXiv preprint</p>
<p>Lampilot: An open benchmark dataset for autonomous driving with language model programs. Y Ma, C Cui, X Cao, W Ye, P Liu, J Lu, A Abdelraouf, R Gupta, K Han, A Bera, CVPR. 2024151</p>
<p>Benchmarking data science agents. Y Zhang, Q Jiang, X Han, N Chen, Y Yang, K Ren, arXiv:2402.171682024arXiv preprint</p>
<p>Da-code: Agent data science code generation benchmark for large language models. Y Huang, J Luo, Y Yu, Y Zhang, F Lei, Y Wei, S He, L Huang, X Liu, J Zhao, arXiv:2410.073312024arXiv preprint</p>
<p>Dcabench: A benchmark for dataset curation agents. B Huang, Y Yu, J Huang, X Zhang, J Ma, arXiv:2406.072752024arXiv preprint</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. J Xie, K Zhang, J Chen, T Zhu, R Lou, Y Tian, Y Xiao, Y Su, arXiv:2402.016222024arXiv preprint</p>
<p>Benchmarking large language models as ai research agents. Q Huang, J Vora, P Liang, J Leskovec, NeurIPS Workshop. 2023</p>
<p>Mlebench: Evaluating machine learning agents on machine learning engineering. J S Chan, N Chowdhury, O Jaffe, J Aung, D Sherburn, E Mays, G Starace, K Liu, L Maksin, T Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>Agentharm: Benchmarking robustness of llm agents on harmful tasks. M Andriushchenko, A Souly, M Dziemian, D Duenas, M Lin, J Wang, D Hendrycks, A Zou, J Z Kolter, M Fredrikson, ICLR. 2024</p>
<p>Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. T Xie, D Zhang, J Chen, X Li, S Zhao, R Cao, J H Toh, Z Cheng, D Shin, F Lei, NeurIPS. 372025</p>
<p>Tur [k] ingbench: A challenge benchmark for web agents. K Xu, Y Kordi, T Nayak, A Asija, Y Wang, K Sanders, A Byerly, J Zhang, B Van Durme, D Khashabi, arXiv:2403.119052024arXiv preprint</p>
<p>Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. R Kapoor, Y P Butala, M Russak, J Y Koh, K Kamble, W Al-Shikh, R Salakhutdinov, ECCV. Springer2024</p>
<p>Egolife: Towards egocentric life assistant. J Yang, S Liu, H Guo, Y Dong, X Zhang, S Zhang, P Wang, Z Zhou, B Xie, Z Wang, arXiv:2503.038032025arXiv preprint</p>
<p>Gta: a benchmark for general tool agents. J Wang, M Zerun, Y Li, S Zhang, C Chen, K Chen, X Le, NeurIPS2024</p>
<p>F F Xu, Y Song, B Li, Y Tang, K Jain, M Bao, Z Z Wang, X Zhou, Z Guo, M Cao, arXiv:2412.14161Theagentcompany: benchmarking llm agents on consequential real world tasks. 2024arXiv preprint</p>
<p>Benchmarking large language models for multi-agent systems: A comparative analysis of autogen, crewai, and taskweaver. R Barbarroxa, L Gomes, Z Vale, International Conference on Practical Applications of Agents and Multi-Agent Systems. Springer2024</p>
<p>Benchmark evaluations, applications, and challenges of large vision language models: A survey. Z Li, X Wu, H Du, H Nghiem, G Shi, arXiv:2501.021892025arXiv preprint</p>
<p>Ml research benchmark. M Kenney, arXiv:2410.225532024arXiv preprint</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, X Jiang, K Cobbe, T Eloundou, G Krueger, K Button, M Knight, B Chess, J Schulman, 2022</p>
<p>WebCPM: Interactive web search for Chinese long-form question answering. Y Qin, Z Cai, D Jin, L Yan, S Liang, K Zhu, Y Lin, X Han, N Ding, H Wang, R Xie, F Qi, Z Liu, M Sun, J Zhou, ACL, A. Rogers, J. Boyd-Graber, and N. OkazakiJul. 2023Association for Computational LinguisticsToronto, Canada</p>
<p>Toolcoder: Teach code generation models to use api search tools. K Zhang, H Zhang, G Li, J Li, Z Li, Z Jin, 2023</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, Foundations and Trends in Information Retrieval. 342009</p>
<p>Autocoder: Enhancing code large language model with AIEV-INSTRUCT. B Lei, Y Li, Q Chen, 2024</p>
<p>Rlef: Grounding code llms in execution feedback with reinforcement learning. J Gehring, K Zheng, J Copet, V Mella, Q Carbonneaux, T Cohen, G Synnaeve, 2025</p>
<p>Executable code actions elicit better llm agents. X Wang, Y Chen, L Yuan, Y Zhang, Y Li, H Peng, H Ji, abs/2402.01030ArXiv. 2024</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dess, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Advances in Neural Information Processing Systems. 202336551</p>
<p>Art: Automatic multi-step reasoning and tooluse for large language models. B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, arXiv:2303.090142023arXiv preprint</p>
<p>Restgpt: Connecting large language models with real-world restful apis. Y Song, W Xiong, D Zhu, W Wu, H Qian, M Song, H Huang, C Li, K Wang, R Yao, Y Tian, S Li, 2023</p>
<p>Sequential API function calling using GraphQL schema. A Saha, L Mandal, B Ganesan, S Ghosh, R Sindhgatta, C Eberhardt, D Debrunner, S Mehta, EMNLP, Y. Al-Onaizan, M. Bansal, and Y.-N. ChenNov. 202419458Miami, Florida, USA</p>
<p>Craft: Customizing llms by creating and retrieving from specialized toolsets. L Yuan, Y Chen, X Wang, Y R Fung, H Peng, H Ji, arXiv:2309.174282023arXiv preprint</p>
<p>Toolink: Linking toolkit creation and using through chain-of-solving on open-source model. C Qian, C Xiong, Z Liu, Z Liu, NAACL. 2024</p>
<p>CREATOR: Tool creation for disentangling abstract and concrete reasoning of large language models. C Qian, C Han, Y Fung, Y Qin, Z Liu, H Ji, EMNLP Findings, H. Bouamor, J. Pino, and K. BaliDec. 2023Singapore</p>
<p>Large language models as tool makers. T Cai, X Wang, T Ma, X Chen, D Zhou, 2024</p>
<p>LangChain. 2023</p>
<p>LlamaIndex. 2022</p>
<p>Dify. 2023</p>
<p>Ollama. 2023</p>
<p>. MCP Agent. 22025</p>
<p>Commercial llm agents are already vulnerable to simple yet dangerous attacks. A Li, Y Zhou, V C Raghuram, T Goldstein, M Goldblum, arXiv:2502.085862025arXiv preprint</p>
<p>Agent-pro: Learning to evolve via policy-level reflection and optimization. W Zhang, K Tang, H Wu, M Wang, Y Shen, G Hou, Z Tan, P Li, Y Zhuang, W Lu, ACL. 2024</p>
<p>A trembling house of cards? mapping adversarial attacks against language agents. L Mo, Z Liao, B Zheng, Y Su, C Xiao, H Sun, arXiv:2402.101962024arXiv preprint</p>
<p>Agentdojo: A dynamic environment to evaluate prompt injection attacks and defenses for llm agents. E Debenedetti, J Zhang, M Balunovic, L Beurer-Kellner, M Fischer, F Tramer, NeurIPS. 379202024</p>
<p>C H Wu, J Y Koh, R Salakhutdinov, D Fried, A Raghunathan, arXiv:2406.12814Adversarial attacks on multimodal agents. 2024arXiv preprint</p>
<p>Cheatagent: Attacking llm-empowered recommender systems via llm agent. L.-B Ning, S Wang, W Fan, Q Li, X Xu, H Chen, F Huang, KDD. 2024</p>
<p>Infecting llm agents via generalizable adversarial attack. W Yu, K Hu, T Pang, C Du, M Lin, M Fredrikson, NeurIPS Workshop2024</p>
<p>Large language model sentinel: Llm agent for adversarial purification. G Lin, Q Zhao, arXiv:2405.207702024arXiv preprint</p>
<p>Combating adversarial attacks with multi-agent debate. S Chern, Z Fan, A Liu, arXiv:2401.059982024arXiv preprint</p>
<p>Reinforcement learning-driven llm agent for automated attacks on llms. X Wang, J Peng, K Xu, H Yao, T Chen, ACL Findings. 2024</p>
<p>Jailbreaking text-to-image models with llm-based agents. Y Dong, Z Li, X Meng, N Yu, S Guo, arXiv:2408.005232024arXiv preprint</p>
<p>When llm meets drl: Advancing jailbreaking efficiency via drl-guided search. X Chen, Y Nie, W Guo, X Zhang, NeurIPS2024</p>
<p>Pathseeker: Exploring llm security vulnerabilities with a reinforcement learning-based jailbreak approach. Z Lin, W Ma, M Zhou, Y Zhao, H Wang, Y Liu, J Wang, L Li, arXiv:2409.141772024arXiv preprint</p>
<p>Autodefense: Multi-agent llm defense against jailbreak attacks. Y Zeng, Y Wu, X Zhang, H Wang, Q Wu, arXiv:2403.047832024arXiv preprint</p>
<p>Guardians of the agentic system: Preventing many shots jailbreak with agentic system. S Barua, M Rahman, M J Sadek, R Islam, S Khaled, A Kabir, arXiv:2502.167502025arXiv preprint</p>
<p>Shieldlearner: A new paradigm for jailbreak attack defense in llms. Z Ni, H Wang, H Wang, arXiv:2502.131622025arXiv preprint</p>
<p>Demonagent: Dynamically encrypted multi-backdoor implantation attack on llm-based agent. P Zhu, Z Zhou, Y Zhang, S Yan, K Wang, S Su, arXiv:2502.125752025arXiv preprint</p>
<p>Watch out for your agents! investigating backdoor threats to llm-based agents. W Yang, X Bi, Y Lin, S Chen, J Zhou, X Sun, NeurIPS. 372025</p>
<p>Badagent: Inserting and activating backdoor attacks in llm agents. Y Wang, D Xue, S Zhang, S Qian, ACL. 2024</p>
<p>Badjudge: Backdoor vulnerabilities of llm-as-a-judge. T Tong, F Wang, Z Zhao, M Chen, ICLR2025</p>
<p>Darkmind: Latent chain-of-thought backdoor in customized llms. Z Guo, R Tourani, arXiv:2501.186172025arXiv preprint</p>
<p>Corba: Contagious recursive blocking attacks on multiagent systems based on large language models. Z Zhou, Z Li, J Zhang, Y Zhang, K Wang, Y Liu, Q Guo, arXiv:2502.145292025arXiv preprint</p>
<p>Red-teaming llm multi-agent systems via communication attacks. P He, Y Lin, S Dong, H Xu, Y Xing, H Liu, arXiv:2502.148472025arXiv preprint</p>
<p>Netsafe: Exploring the topological safety of multiagent networks. M Yu, S Wang, G Zhang, J Mao, C Yin, Q Liu, Q Wen, K Wang, Y Wang, arXiv:2410.156862024arXiv preprint</p>
<p>G-safeguard: A topology-guided security lens and treatment on llm-based multi-agent systems. S Wang, G Zhang, M Yu, G Wan, F Meng, C Guo, K Wang, Y Wang, arXiv:2502.111272025arXiv preprint</p>
<p>Trustagent: Towards safe and trustworthy llm-based agents through agent constitution. W Hua, X Yang, M Jin, Z Li, W Cheng, R Tang, Y Zhang, EMNLP Findings. 2024</p>
<p>Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety. Z Zhang, Y Zhang, L Li, H Gao, L Wang, H Lu, F Zhao, Y Qiao, J Shao, arXiv:2401.118802024arXiv preprint</p>
<p>Ai agents under threat: A survey of key security challenges and future pathways. Z Deng, Y Guo, C Han, W Ma, J Xiong, S Wen, Y Xiang, ACM Computing Surveys. 2024</p>
<p>Agentdojo: A dynamic environment to evaluate prompt injection attacks and defenses for llm agents. E Debenedetti, J Zhang, M Balunovic, L Beurer-Kellner, M Fischer, F Tramr, NeurIPS. 379202025</p>
<p>Targeting the core: A simple and effective method to attack rag-based agents via direct llm manipulation. X Li, Z Li, Y Kosuga, Y Yoshida, V Bian, arXiv:2412.044152024arXiv preprint</p>
<p>Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents. Q Zhan, Z Liang, Z Ying, D Kang, arXiv:2403.026912024arXiv preprint</p>
<p>Hacking back the ai-hacker: Prompt injection as a defense against llm-driven cyberattacks. D Pasquini, E M Kornaropoulos, G Ateniese, arXiv:2410.209112024arXiv preprint</p>
<p>Firewalls to secure dynamic llm agentic networks. S Abdelnabi, A Gomaa, E Bagdasarian, P O Kristensson, R Shokri, arXiv:2502.018222025arXiv preprint</p>
<p>Rtbas: Defending llm agents against prompt injection and privacy leakage. P Y Zhong, S Chen, R Wang, M Mccall, B L Titzer, H Miller, arXiv:2502.089662025arXiv preprint</p>
<p>F Jia, T Wu, X Qin, A Squicciarini, arXiv:2412.16682The task shield: Enforcing task alignment to defend against indirect prompt injection in llm agents. 2024arXiv preprint</p>
<p>Evil geniuses: Delving into the safety of llm-based agents. Y Tian, X Yang, J Zhang, Y Dong, H Su, arXiv:2311.118552023arXiv preprint</p>
<p>Biorag: A rag-llm framework for biological question reasoning. C Wang, Q Long, X Meng, X Cai, C Wu, Z Meng, X Wang, Y Zhou, arXiv:2408.011072024arXiv preprint</p>
<p>Navigating the risks: A survey of security, privacy, and ethics threats in llm-based agents. Y Gan, Y Yang, Z Ma, P He, R Zeng, Y Wang, Q Li, C Zhou, S Li, T Wang, arXiv:2411.095232024arXiv preprint</p>
<p>Clas 2024: The competition for llm and agent safety. Z Xiang, Y Zeng, M Kang, C Xu, J Zhang, Z Yuan, Z Chen, C Xie, F Jiang, M Pan, NeurIPS Workshop. 2024</p>
<p>F Wu, S Wu, Y Cao, C Xiao, arXiv:2402.16965Wipi: A new web threat for llm-driven web agents. 2024arXiv preprint</p>
<p>Breaking react agents: Foot-in-the-door attack will get you in. I Nakash, G Kour, G Uziel, A Anaby-Tavor, arXiv:2410.169502024arXiv preprint</p>
<p>Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. Z Chen, Z Xiang, C Xiao, D Song, B Li, NeurIPS. 372025</p>
<p>Unveiling privacy risks in llm agent memory. B Wang, W He, P He, S Zeng, Z Xiang, Y Xing, J Tang, arXiv:2502.131722025arXiv preprint</p>
<p>Malicious chatgpt agents: How gpts can quietly grab your data (demo). E T Red, Embrace The Red. 2023</p>
<p>Y Li, H Wen, W Wang, X Li, Y Yuan, G Liu, J Liu, W Xu, X Wang, Y Sun, arXiv:2401.05459Personal llm agents: Insights and survey about the capability, efficiency and security. 2024arXiv preprint</p>
<p>Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast. X Gu, X Zheng, T Pang, C Du, Q Liu, Y Wang, J Jiang, M Lin, arXiv:2402.085672024arXiv preprint</p>
<p>Prompt infection: Llm-to-llm prompt injection within multi-agent systems. D Lee, M Tiwari, arXiv:2410.072832024arXiv preprint</p>
<p>Blockagents: Towards byzantine-robust llm-based multi-agent coordination via blockchain. B Chen, G Li, X Lin, Z Wang, J Li, ACM Turing Award Celebration Conference. 2024</p>
<p>Agentharm: A benchmark for measuring harmfulness of llm agents. M Andriushchenko, A Souly, M Dziemian, D Duenas, M Lin, J Wang, D Hendrycks, A Zou, Z Kolter, M Fredrikson, arXiv:2410.090242024arXiv preprint</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, USENIX. 2021</p>
<p>Quantifying memorization across neural language models. N Carlini, D Ippolito, M Jagielski, K Lee, F Tramer, C Zhang, ICLR2022</p>
<p>Are large pre-trained language models leaking your personal information. J Huang, H Shao, K C , -C Chang, arXiv:2205.126282022arXiv preprint</p>
<p>Quantifying privacy risks of masked language models using membership inference attacks. F Mireshghallah, K Goyal, A Uniyal, T Berg-Kirkpatrick, R Shokri, arXiv:2203.039292022arXiv preprint</p>
<p>Practical membership inference attacks against fine-tuned large language models via self-prompt calibration. W Fu, H Wang, C Gao, G Liu, Y Li, T Jiang, arXiv:2311.060622023arXiv preprint</p>
<p>Learning and evaluating a differentially private pre-trained language model. S Hoory, A Feder, A Tendler, S Erell, A Peled-Cohen, I Laish, H Nakhost, U Stemmer, A Benjamini, A Hassidim, EMNLP Findings. 2021</p>
<p>Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks. M Kang, S Lee, J Baek, K Kawaguchi, S J Hwang, NeurIPS. 366022023</p>
<p>Privacy risks of generalpurpose language models. X Pan, M Zhang, S Ji, M Yang, IEEE Symposium on Security and Privacy (SP). IEEE2020</p>
<p>Property existence inference against generative models. L Wang, J Wang, J Wan, L Long, Z Yang, Z Qin, USENIX. 2024</p>
<p>Deduplicating training data mitigates privacy risks in language models. N Kandpal, E Wallace, C Raffel, ICML. PMLR202210707</p>
<p>Propile: Probing privacy leakage in large language models. S Kim, S Yun, H Lee, M Gubri, S Yoon, S J Oh, NeurIPS. 367622023</p>
<p>Thieves on sesame street! model extraction of bert-based apis. K Krishna, G S Tomar, A P Parikh, N Papernot, M Iyyer, arXiv:1910.123662019arXiv preprint</p>
<p>Stealing the decoding algorithms of language models. A Naseh, K Krishna, M Iyyer, A Houmansadr, ACM SIGSAC, 2023. </p>
<p>On extracting specialized code abilities from large language models: A feasibility study. Z Li, C Wang, P Ma, C Liu, S Wang, D Wu, C Gao, Y Liu, ICSE. 2024</p>
<p>A watermark for large language models. J Kirchenbauer, J Geiping, Y Wen, J Katz, I Miers, T Goldstein, ICML. PMLR20231784</p>
<p>Blockchain-based efficient and trustworthy aigc services in metaverse. Y Lin, Z Gao, H Du, D Niyato, J Kang, Z Xiong, Z Zheng, IEEE Transactions on Services Computing. 2024</p>
<p>Prompt stealing attacks against {Text-to-Image} generation models. X Shen, Y Qu, M Backes, Y Zhang, USENIX. 2024</p>
<p>Prompt stealing attacks against large language models. Z Sha, Y Zhang, arXiv:2402.129592024arXiv preprint</p>
<p>Pleak: Prompt leaking attacks against large language model applications. B Hui, H Yuan, N Gong, P Burlina, Y Cao, ACM SIGSAC. 2024</p>
<p>On the Opportunities and Risks of Foundation Models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>GPT-3: Its Nature, Scope, Limits, and Consequences. L Floridi, M Chiriatti, 202030Minds and Machines</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozire, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Redefining Work in the Age of AI: Challenges and Pathways to Opportunities. P Tadas, S Agarmore, in SPICES. IEEE, 2024</p>
<p>Empowering Education with LLMs -The Next-Gen Interface and Content Generation. S Moore, R Tong, A Singh, Z Liu, X Hu, Y Lu, J Liang, C Cao, H Khosravi, P Denny, International Conference on Artificial Intelligence in Education. Springer2023</p>
<p>Culturevlm: Characterizing and improving cultural understanding of vision-language models for over 100 countries. S Liu, Y Jin, C Li, D F Wong, Q Wen, L Sun, H Chen, X Xie, J Wang, arXiv:2501.012822025</p>
<p>Foundation Models and Fair Use. P Henderson, X Li, D Jurafsky, T Hashimoto, M A Lemley, P Liang, JMLR. 244002023</p>
<p>Fair Learning. M A Lemley, B Casey, Tex. L. Rev. 997432020</p>
<p>Uniguard: Towards universal safety guardrails for jailbreak attacks on multimodal large language models. S Oh, Y Jin, M Sharma, D Kim, E Ma, G Verma, S Kumar, arXiv:2411.017032024</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, in FAccT, 2021</p>
<p>M Brundage, S Avin, J Wang, H Belfield, G Krueger, G Hadfield, H Khlaaf, J Yang, H Toner, R Fong, arXiv:2004.07213Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. 2020arXiv preprint</p>
<p>Predictability and Surprise in Large Generative Models. D Ganguli, D Hernandez, L Lovitt, A Askell, Y Bai, A Chen, T Conerly, N Dassarma, D Drain, N Elhage, FAccT2022</p>
<p>C Deng, Y Duan, X Jin, H Chang, Y Tian, H Liu, H P Zou, Y Jin, Y Xiao, Y Wang, Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas: A Survey. 20242406arXiv e-prints</p>
<p>AI models collapse when trained on recursively generated data. I Shumailov, Z Shumaylov, Y Zhao, N Papernot, R Anderson, Y Gal, Nature. 63180222024</p>
<p>Ethical and social risks of harm from Language Models. L Weidinger, J Mellor, M Rauh, C Griffin, J Uesato, P.-S Huang, M Cheng, M Glaese, B Balle, A Kasirzadeh, arXiv:2112.043592021arXiv preprint</p>
<p>Large language models can be contextual privacy protection learners. Y Xiao, Y Jin, Y Bai, Y Wu, X Yang, X Luo, W Yu, X Zhao, Y Liu, Q Gu, EMNLP. 202414</p>
<p>Medical large language models are vulnerable to datapoisoning attacks. D A Alber, Z Yang, A Alyakin, E Yang, S Rai, A A Valliani, J Zhang, G R Rosenbaum, A K Amend-Thomas, D B Kurland, Nature Medicine. 2025</p>
<p>Towards fine-grained reasoning for fake news detection. Y Jin, X Wang, R Yang, Y Sun, W Wang, H Liao, X Xie, AAAI. 202236</p>
<p>Large Language Model Alignment: A Survey. T Shen, R Jin, Y Huang, C Liu, W Dong, Z Guo, X Wu, Y Liu, D Xiong, arXiv:2309.150252023arXiv preprint</p>
<p>Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. A S Luccioni, S Viguier, A.-L Ligozat, JMLR. 242532023</p>
<p>Energy and Policy Considerations for Deep Learning in NLP. E Strubell, A Ganesh, A Mccallum, AAAI. 202034696</p>
<p>Awesome ai agents for scientific discovery. J Zhou, Awesome-LLM-Agents-Scientific-Discovery. 2024</p>
<p>Aaai 2025 presidential panel: Future of ai research. AAAI. 2025</p>
<p>Sciagents: Automating scientific discovery through bioinspired multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, 10.1002/adma.202413523Advanced Materials. 2413523</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. P T J Kon, J Liu, Q Ding, Y Qiu, Z Yang, Y Huang, J Srinivasa, M Lee, M Chowdhury, A Chen, 2025</p>
<p>Agentreview: Exploring peer review dynamics with llm agents. Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, EMNLP. 2024</p>
<p>Chemcrow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 2023</p>
<p>Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. A Ghafarollahi, M J Buehler, 2024</p>
<p>Ai agents for ground-based gamma astronomy. D Kostunin, V Sotnikov, S Golovachev, A Strube, 2025</p>
<p>Large language models as biomedical hypothesis generators: A comprehensive evaluation. B Qi, K Zhang, K Tian, H Li, Z -R. Chen, S Zeng, E Hua, H Jinfang, B Zhou, 2024</p>
<p>Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. Y Roohani, A Lee, Q Huang, J Vora, Z Steinhart, K Huang, A Marson, P Liang, J Leskovec, arXiv:2405.176312024arXiv preprint</p>
<p>Geneagent: Self-verification language agent for gene set knowledge discovery using domain databases. Z Wang, Q Jin, C.-H Wei, S Tian, P.-T Lai, Q Zhu, C.-P Day, C Ross, Z Lu, 2024</p>
<p>Knowledge-guided biomarker identification for label-free singlecell rna-seq data: A reinforcement learning perspective. M Xiao, W Zhang, X Huang, H Zhu, M Wu, X Li, Y Zhou, arXiv:2501.047182025arXiv preprint</p>
<p>Pathgen-1.6m: 1.6 million pathology image-text pairs generation through multi-agent collaboration. Y Sun, Y Zhang, Y Si, C Zhu, Z Shui, K Zhang, J Li, X Lyu, T Lin, L Yang, 2024</p>
<p>Knowledge hierarchy guided biological-medical dataset distillation for domain llm training. X Cai, C Wang, Q Long, Y Zhou, M Xiao, arXiv:2501.151082025arXiv preprint</p>
<p>Genesum: Large language model-based gene summary extraction. Z Chen, C Hu, M Wu, Q Long, X Wang, Y Zhou, M Xiao, 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE2024</p>
<p>Best practices in emr implementation: a systematic review. K Keshavjee, J Bosomworth, J Copen, J Lai, B Kucukyazici, R Lilani, A M Holbrook, AMIA Annual Symposium Proceedings. 2006. 2006982</p>
<p>Needed: Introducing hierarchical transformer to eye diseases diagnosis. X Ye, M Xiao, Z Ning, W Dai, W Cui, Y Du, Y Zhou, Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). the 2023 SIAM International Conference on Data Mining (SDM)SIAM2023</p>
<p>Agent hospital: A simulacrum of hospital with evolvable medical agents. J Li, Y Lai, W Li, J Ren, M Zhang, X Kang, S Wang, P Li, Y.-Q Zhang, W Ma, arXiv:2405.029572024arXiv preprint</p>
<p>Clinicallab: Aligning agents for multi-departmental clinical diagnostics in the real world. W Yan, H Liu, T Wu, Q Chen, W Wang, H Chai, J Wang, W Zhao, Y Zhang, R Zhang, arXiv:2406.138902024arXiv preprint</p>
<p>Aipatient: Simulating patients with ehrs and llm powered agentic workflow. H Yu, J Zhou, L Li, S Chen, J Gallifant, A Shi, X Li, W Hua, M Jin, G Chen, Y Zhou, Z Li, T Gupte, M.-L Chen, Z Azizi, Y Zhang, T L Assimes, X Ma, D S Bitterman, L Lu, L Fan, 2024</p>
<p>Cxr-agent: Vision-language models for chest x-ray interpretation with uncertainty aware radiology reporting. N Sharma, arXiv:2407.088112024arXiv preprint</p>
<p>Medrax: Medical reasoning agent for chest x-ray. A Fallahpour, J Ma, A Munim, H Lyu, B Wang, 2025</p>
<p>Comparative analysis of m4cxr, an llm-based chest x-ray report generation model, and chatgpt in radiological interpretation. R W Lee, K H Lee, J S Yun, M S Kim, H S Choi, Journal of Clinical Medicine. 132370572024</p>
<p>Chessgpt: Bridging policy learning and language modeling. X Feng, Y Luo, Z Wang, H Tang, M Yang, K Shao, D Mguni, Y Du, J Wang, NeurIPS. 2023</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, C Romac, T Wolf, S Lamprier, O Sigaud, P.-Y Oudeyer, ICML. 2023</p>
<p>Calypso: Llms as dungeon master's assistants. A Zhu, L Martin, A Head, C Callison-Burch, AAAI. 2023</p>
<p>Gamegpt: Multiagent collaborative framework for game development. D Chen, H Wang, Y Huo, Y Li, H Zhang, arXiv:2310.080672023arXiv preprint</p>
<p>Language as reality: a co-creative storytelling game experience in 1001 nights using generative ai. Y Sun, Z Li, K Fang, C H Lee, A Asadipour, AAAI. 2023</p>
<p>Econagent: large language model-empowered agents for simulating macroeconomic activities. N Li, C Gao, M Li, Y Li, Q Liao, ACL. 2024</p>
<p>Tradinggpt: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance. Y Li, Y Yu, H Li, Z Chen, K Khashanah, arXiv:2309.037362023arXiv preprint</p>
<p>Competeai: Understanding the competition dynamics in large language model-based agents. Q Zhao, J Wang, Y Zhang, Y Jin, K Zhu, H Chen, X Xie, ICML. 2024107</p>
<p>Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. Z Ma, Y Mei, Z Su, AMIA Annual Symposium Proceedings. 202420231105</p>
<p>Exploring collaboration mechanisms for llm agents: A social psychology view. J Zhang, X Xu, N Zhang, R Liu, B Hooi, S Deng, ACL. 2024607</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. G V Aher, R I Arriaga, A T Kalai, ICML. 2023</p>
<p>Training socially aligned language models on simulated social interactions. R Liu, R Yang, C Jia, G Zhang, D Zhou, A M Dai, D Yang, S Vosoughi, ICLR2024</p>
<p>C Gao, X Lan, Z Lu, J Mao, J Piao, H Wang, D Jin, Y Li, arXiv:2307.14984S3: Social-network simulation system with large language modelempowered agents. 2023arXiv preprint</p>
<p>Self-collaboration code generation via chatgpt. Y Dong, X Jiang, Z Jin, G Li, ACM Transactions on Software Engineering and Methodology. 3372024</p>
<p>Chatdev: Communicative agents for software development. C Qian, X Cong, C Yang, W Chen, Y Su, J Xu, Z Liu, M Sun, ACL. 2024</p>
<p>On generative agents in recommendation. A Zhang, Y Chen, L Sheng, X Wang, T.-S Chua, SIGIR. 2024</p>
<p>Agentcf: Collaborative learning with autonomous language agents for recommender systems. J Zhang, Y Hou, R Xie, W Sun, J Mcauley, W X Zhao, L Lin, J.-R Wen, 2024WWW</p>
<p>Macrec: A multi-agent collaboration framework for recommendation. Z Wang, Y Yu, W Zheng, W Ma, M Zhang, SIGIR. 2024</p>
<p>Recmind: Large language model powered agent for recommendation. Y Wang, Z Jiang, Z Chen, F Yang, Y Zhou, E Cho, X Fan, X Huang, Y Lu, Y Yang, arXiv:2308.142962023arXiv preprint</p>
<p>Scaling large-language-model-based multiagent collaboration. C Qian, Z Xie, Y Wang, W Liu, Y Dang, Z Du, W Chen, C Yang, Z Liu, M Sun, arXiv:2406.071552024</p>
<p>Chateval: Towards better llm-based evaluators through multi-agent debate. C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.072012023arXiv preprint</p>
<p>What is scalability in multi-agent systems. O F Rana, K Stout, Proceedings of the fourth international conference on Autonomous agents. the fourth international conference on Autonomous agents2000</p>
<p>Scalable multi-agent systems. R Deters, Proceedings of the 2001 joint ACM-ISCOPE conference on Java Grande. the 2001 joint ACM-ISCOPE conference on Java Grande2001182</p>
<p>Adaptagent: Adapting multimodal web agents with few-shot learning from human demonstrations. G Verma, R Kaur, N Srishankar, Z Zeng, T Balch, M Veloso, arXiv:2411.134512024arXiv preprint</p>
<p>Mm-soc: Benchmarking multimodal large language models in social media platforms. Y Jin, M Choi, G Verma, J Wang, S Kumar, ACL Findings. 2024</p>
<p>Velo: A vector database-assisted cloud-edge collaborative llm qos optimization framework. Z Yao, Z Tang, J Lou, P Shen, W Jia, ICWS. 2024IEEE</p>
<p>xrag: Extreme context compression for retrievalaugmented generation with one token. X Cheng, X Wang, X Zhang, T Ge, S.-Q Chen, F Wei, H Zhang, D Zhao, NeurIPS2024</p>
<p>Better to ask in english: Cross-lingual evaluation of large language models for healthcare queries. Y Jin, M Chandra, G Verma, Y Hu, M De Choudhury, S Kumar, 2024WWW</p>
<p>Medhalu: Hallucinations in responses to healthcare queries by large language models. V Agarwal, Y Jin, M Chandra, M De Choudhury, S Kumar, N Sastry, arXiv:2409.194922024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Can knowledge graphs reduce hallucinations in llms?: A survey. G Agrawal, T Kumarage, Z Alghamdi, H Liu, NAACL. 2024</p>
<p>Webgpt: Browserassisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.093322021arXiv preprint</p>
<p>Enabling large language models to generate text with citations. T Gao, H Yen, J Yu, D Chen, EMNLP. 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, ICLR. 2023</p>
<p>Codebertscore: Evaluating code generation with pretrained models of code. S Zhou, U Alon, S Agarwal, G Neubig, EMNLP. 202313937</p>
<p>Execution-based evaluation for open-domain code generation. Z Wang, S Zhou, D Fried, G Neubig, EMNLP. 2023</p>
<p>Dyval: Dynamic evaluation of large language models for reasoning tasks. K Zhu, J Chen, J Wang, N Z Gong, D Yang, X Xie, ICLR2024</p>
<p>Dynamic evaluation of large language models by meta probing agents. K Zhu, J Wang, Q Zhao, R Xu, X Xie, ICML. PMLR202462617</p>
<p>Unpacking the ethical value alignment in big models. X Yi, J Yao, X Wang, X Xie, arXiv:2310.175512023arXiv preprint</p>
<p>Evaluating general-purpose ai with psychometrics. X Wang, L Jiang, J Hernandez-Orallo, D Stillwell, L Sun, F Luo, X Xie, arXiv:2310.163792023arXiv preprint</p>
<p>Chatarena: Multi-agent language game environments for large language models. Y Wu, Z Jiang, A Khan, Y Fu, L Ruis, E Grefenstette, T Rocktschel, 2023</p>
<p>Value fulcra: Mapping large language models to the multidimensional spectrum of basic human value. J Yao, X Yi, Y Gong, X Wang, X Xie, NAACL. 2024</p>
<p>Do large language models align with core mental health counseling competencies. V C Nguyen, M Taher, D Hong, V K Possobom, V T Gopalakrishnan, E Raj, Z Li, H J Soled, M L Birnbaum, S Kumar, NAACL. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
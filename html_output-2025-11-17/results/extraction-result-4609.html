<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4609 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4609</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4609</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-5e5bca8f0b203785d371d265371ac88a2390c014</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5e5bca8f0b203785d371d265371ac88a2390c014" target="_blank">PaSa: An LLM Agent for Comprehensive Academic Paper Search</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper Abstract:</strong> We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4609.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4609.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaSa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaSa: An LLM Agent for Comprehensive Academic Paper Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic paper-search system built from two cooperating LLM agents (Crawler and Selector) that autonomously issues searches, reads papers, expands citations, and returns comprehensive sets of relevant papers for fine-grained scholarly queries; trained with imitation learning followed by session-level PPO within the AGILE framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaSa</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PaSa is composed of two LLM agents: (1) the Crawler, a token-level MDP policy LLM that generates search queries, invokes a search tool (Google with site:arxiv and before:query_date), and issues [Expand] actions to extract cited papers from target subsections; it maintains a growing paper queue and explores citation networks (exploration depth limited to 3 in experiments). (2) the Selector, an LLM classifier that takes a user query plus a paper (title+abstract+outline) and outputs a single decision token (True/False) followed by a textual rationale; the Selector's single-token decision is used as an auxiliary reward signal for Crawler RL. Training: imitation learning to initialize behavior, then a session-level PPO variant (session partitioning, in-session and across-session discounts, per-token KL penalty) to cope with sparse rewards and long trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen2.5-7b (PaSa-7b implementation); GPT-4o used for a prompting-based variant (PaSa-GPT-4o) and for parts of data synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-driven query generation + retrieval via Google (site:arxiv & time filter) + citation expansion from paper subsections parsed via ar5iv; token-level actions [Search], [Expand], [Stop] drive extraction and paper-queue management.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregates papers by crawling citation networks to build a comprehensive paper queue, then uses the Selector to filter/score papers (decision token probability used for ranking); ensemble sampling at inference aggregates multiple crawler runs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Per-query trajectories can yield hundreds to thousands of papers (paper queue sizes may grow very large); experiments restricted crawler depth to 3; AutoScholarQuery dataset used for training contains 33,551 / 1,000 / 1,000 query-paper instances (train/dev/test).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science / Machine learning literature (ICLR, ICML, NeurIPS, ACL, CVPR papers used for dataset and evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Sets/ranked lists of relevant papers (paper queue), boolean relevance decisions with textual rationales, and aggregated recall/precision metrics for discovered literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Recall@20 / Recall@50 / Recall@100 for ranked baselines; precision, recall, F1 for unranked retrieval; Crawler recall and Crawler action counts; Selector precision/recall/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>On AutoScholarQuery test: PaSa-7b achieved Recall@100=0.6947, Recall@50=0.6334, Recall@20=0.5301 and Crawler recall=0.7931. On RealScholarQuery: Recall@100=0.6929, Recall@50=0.6563, Recall@20=0.5798 and Crawler recall=0.7071. Selector: F1=0.85 (precision 0.95, recall 0.78).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Google, Google Scholar, Google + GPT-4o paraphrase, ChatGPT (search-enabled GPT-4o), GPT-o1 (no web), and PaSa-GPT-4o (prompted GPT-4o implementing PaSa behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PaSa-7b outperformed Google+GPT-4o by 33.80%/38.83%/42.64% (Recall@20/50/100) on AutoScholarQuery and outperformed Google+GPT-4o on RealScholarQuery by 37.78% (Recall@20) and 39.90% (Recall@50). PaSa-7b exceeded PaSa-GPT-4o by 11.12% recall on AutoScholarQuery and by 30.36% recall and 4.25% precision on RealScholarQuery.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agentic LLMs that actively search and expand citation networks (Crawler) combined with a trained relevance classifier/reward model (Selector) significantly increase recall for fine-grained scholarly queries; RL fine-tuning (session-level PPO) and using the Selector as an auxiliary reward improve performance; explicit citation expansion ([Expand]) is crucial (ablation shows large recall drop without it).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Experiments limited to ML literature (dataset bias), used 7B LLMs (compute/scale constraints), training data synthesized from Related Work may omit ground-truth papers (sparse reward), long RL trajectories and computational expense, potential for missing or biased coverage outside curated conference corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors report improved recall with increased reward coefficient α (e.g., crawler recall 0.7227 at α=0.5 → 0.8063 at α=2.0, with more crawler actions). They expect larger LLMs to further improve performance; session-level PPO converged after ~200 RL steps in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4609.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGILE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AGILE: A novel reinforcement learning framework of LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning framework for training LLM-driven agents end-to-end, enabling joint optimization of tool use, planning, memory and multi-step behaviors; used by the authors to train PaSa with a tailored session-level PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agile: A novel reinforcement learning framework of llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AGILE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AGILE is an RL training framework for LLM agents that supports joint optimization of agent skills (tool use, planning, memory) in an end-to-end fashion; the PaSa work adopts AGILE and extends it with a session-level PPO algorithm to cope with sparse rewards and long multi-session trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Framework-agnostic; in PaSa experiments AGILE was applied to Qwen2.5-7b policies.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not an information-extraction system per se — provides RL machinery to train agents that perform extraction via tool invocations.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Optimizes agent policies that synthesize multi-step information-gathering behaviors (e.g., repeated search and citation expansion) across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM agent training (applied here to academic paper search).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Trained LLM agent policies (e.g., PaSa Crawler/Selector).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RL return curves, downstream recall/precision improvements when AGILE-trained agents are applied (e.g., PaSa gains).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Applying AGILE-style RL (session-level PPO) to PaSa produced measurable gains: RL training improved recall by ~6.24% on AutoScholarQuery and ~19.96% on RealScholarQuery compared to imitation-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Imitation-only initialization / prompting-only agents (e.g., PaSa-GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AGILE-based RL fine-tuning yields superior retrieval recall compared to imitation-only or prompting-only baselines in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Session partitioning for PPO (sessions ending with [Stop]) plus estimating across-session value-to-go and using a Selector as auxiliary reward mitigates sparse-reward and long-trajectory issues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RL sampling of full trajectories is expensive; requires careful session design, value estimation across sessions, and per-token KL penalties to stabilize policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper describes training hyperparameters and that returns converged after ~200 PPO steps for their configuration; no broader empirical scaling curves beyond application-specific results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4609.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaSa-GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaSa (prompted GPT-4o variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based baseline that implements the PaSa architecture by prompt-engineering GPT-4o to perform multi-step searches, read papers, and crawl citations instead of training a policy via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaSa-GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Re-creation of PaSa's workflow via prompting: a prompted GPT-4o is used to generate search queries, read paper metadata/content, propose sections to expand citations, and decide relevance — all driven by carefully designed prompts rather than policy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-driven query generation + web search (Google with site:arxiv/time filter) + instructed citation expansion via section selection prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregates papers by repeatedly prompting GPT-4o to expand and select; uses prompted decisions to form a paper queue and output results.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Variable; experiments constrained crawler exploration depth to 3 for fair comparison with PaSa-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning literature (same evaluation corpora as PaSa).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieved candidate paper sets and relevance decisions (from GPT-4o prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same retrieval metrics as PaSa (Recall@20/50/100; precision/recall for final sets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PaSa-GPT-4o achieved lower recall than PaSa-7b (example: PaSa-GPT-4o recall 0.3873 vs PaSa-7b 0.4834 on AutoScholarQuery per paper results), and was outperformed notably on RealScholarQuery (PaSa-7b surpassed PaSa-GPT-4o by ~30.36% recall).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline vs RL-trained PaSa-7b and Google-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Prompting-only GPT-4o implementation of PaSa underperforms RL-trained PaSa in recall despite having access to the same search tools.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt engineering of a strong LLM can approximate agent behaviors but RL fine-tuning provides measurable additional gains for thorough literature crawling and selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No learned long-term planning; limited by prompt-context management; less capable at multi-session planning compared to RL-trained PaSa.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly reported beyond observed performance gap vs RL-trained PaSa-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4609.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoScholarQuery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoScholarQuery (synthetic academic search dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large synthetic dataset of fine-grained scholarly queries paired with corresponding relevant papers, created by prompting GPT-4o on the Related Work sections of recent top-tier ML conference papers to produce query–answer (paper) pairs for RL training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoScholarQuery</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructed by extracting Related Work sections from papers (ICLR/ICML/NeurIPS/ACL/CVPR), prompting GPT-4o to generate scholarly queries whose answers correspond to the cited references, retaining only answers with arXiv IDs; split into 33,551 / 1,000 / 1,000 instances for train/dev/test.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o used to generate query formulations and in parts of data/label creation (e.g., Selector rationales).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Transforming Related Work citation lists into explicit query–paper pairs via LLM prompting plus arXiv lookups for canonical identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Dataset synthesizes natural scholar queries and maps them to cited papers; does not itself synthesize cross-paper theories but provides targets for agent training.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset instances: 33,551 (train), 1,000 (dev), 1,000 (test); papers come from the cited corpora (tables show per-conference paper/query counts).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning / AI conference literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Query ↔ answer-paper sets for training and evaluating paper search agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human quality check: percent qualified queries and relevant papers; used downstream for RL reward and retrieval evaluations (recall metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Human evaluation: 94.0% of sampled queries were qualified; among qualified queries, 93.7% of corresponding papers were judged relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality synthetic query-paper pairs derived from Related Work can bootstrap RL training of LLM agents and generalize to real-world queries (RealScholarQuery) despite being synthetic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>AutoScholarQuery may only capture a subset of ground-truth relevant papers because Related Work sections cite only a subset of all relevant literature; potential domain bias to ML conferences.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Dataset size provides large-scale synthetic supervision; no explicit scaling law reported in paper beyond dataset statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4609.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaSa Selector (relevance classifier / reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LLM module that classifies whether a given paper satisfies a scholar query by emitting a single decision token (True/False) followed by a textual rationale; used both as the system's final decision-maker and as an auxiliary single-token reward model during Crawler RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Selector</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implemented by supervised fine-tuning (Qwen2.5-7b backbone) on data synthesized from AutoScholarQuery and GPT-4o-generated rationales; the model is trained to emit a decision token before the rationale so the decision can be read as a single-token reward; its token probability is used to rank search results or provide reward signal to the Crawler.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen2.5-7b (fine-tuned) in PaSa-7b experiments; GPT-4o used to generate initial rationale+decision training data.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompted classification + rationale generation (single-token decision then textual explanation); trained with supervised fine-tuning and evaluated on human-annotated pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Does not synthesize multiple papers itself; provides judgments and rationales that enable the Crawler to aggregate higher-quality paper collections.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Training dataset comprised 19,812 <query,paper> pairs; test benchmark of 200 pairs used for Selector evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning literature (same domains as PaSa).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Decision (True/False) and textual rationale; probability of decision token used for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Precision, recall, F1 measured on a held-out selector test set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Selector achieved precision 0.95, recall 0.78, F1 0.85 on the selector test set; outperformed GPT-4o (F1=0.80) and Qwen-2.5-7b baseline (F1=0.55).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-4o and base Qwen-2.5-7b classification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PaSa-7b Selector improved F1 by ~5% over GPT-4o and by ~30% over the Qwen baseline on the selector test set.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generating the decision token before the rationale enables the Selector to be used as a cheap auxiliary reward signal for RL; high precision makes it suitable as an RL reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Training relies on synthetic labels and GPT-4o generated rationales; possible biases and domain-limited training data.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly studied beyond showing Selector is effective at 7B scale in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4609.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Crawler</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaSa Crawler (search & citation expansion agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven policy that iteratively generates search queries, issues retrievals, inspects papers, and expands citation subsections to discover relevant literature; modeled as a token-level MDP and trained with imitation learning followed by session-level PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Crawler</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Crawler is framed as a token-level MDP where each generated token is an action. It supports three registered functions: [Search] to generate a search query and invoke Google (site:arxiv & before:date), [Expand] to propose a subsection name then parse and append referenced papers from that subsection to the paper queue, and [Stop] to reset context to the next paper. Training uses imitation to seed behavior and session-level PPO (return estimator includes across-session estimated values for appended papers and per-token KL penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Policy implemented with Qwen2.5-7b (PaSa-7b); imitation data generated with GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-prompted search query generation + web retrieval API + subsection-based citation extraction via ar5iv parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Constructs a paper tree (paper queue) by exploring citation networks and accumulating candidate papers for later filtering by the Selector; across-session value estimates incorporate future sessions spawned by added papers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Single-query trajectories can add hundreds to thousands of papers; in ablations and controlled experiments crawler actions vary (e.g., 382 actions at α=1.5) and depth limited to 3 in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Paper queue (discovered candidate papers), search queries, expansion decisions; Crawler recall metric measuring fraction of target papers appended to queue.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Crawler recall (fraction of target papers collected), number of crawler actions, and downstream recall/precision after Selector filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Crawler recall for PaSa-7b: 0.7931 on AutoScholarQuery test and 0.7071 on RealScholarQuery; removal of [Expand] reduced recall by ~22.98% (Auto) and ~32.21% (Real).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>PaSa-GPT-4o Crawler (prompted), Google-based retrievals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PaSa-7b Crawler recall exceeded PaSa-GPT-4o by a few percentage points in experiments, contributing to higher final recall after selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Citation expansion ([Expand]) is essential to find deeper relevant works not retrieved by surface-level search queries; session-level RL that includes value estimates for appended papers helps optimize long-horizon discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Long trajectories (many papers) create sampling and context-size challenges; reward sparsity if ground-truth datasets only include a subset of actual relevant papers.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Increasing reward coefficient α increases both crawler recall and the number of actions (e.g., recall 0.7227→0.8063 as α increases from 0.5→2.0); action cost parameter trades off recall vs. action volume.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4609.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o system card / GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (GPT-4o) used in this work for data synthesis (AutoScholarQuery prompt generation), for generating imitation trajectories, for paraphrasing queries in baselines, and as the backbone of the prompting-based PaSa-GPT-4o baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4o system card</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPT-4o is used to generate scholarly queries from Related Work sections (AutoScholarQuery), create imitation-learning trajectories and Selector rationales, paraphrase search queries for Google+GPT-4o baseline, and as the LLM in the PaSa-GPT-4o prompting baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (OpenAI system referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-driven generation of queries and rationales; paraphrasing of user queries for retrieval; prompted classification in some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Used to synthesize query–paper pairs and rationales from paper Related Work text; can be prompted to implement multi-step paper search behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General-purpose LLM used for ML literature tasks here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Paraphrased queries, candidate paper lists, decision+rationale texts used as training labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>When evaluated as a Selector baseline, achieved precision 0.96, recall 0.69, F1 0.80 on the selector test set reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GPT-4o used as strong baseline in many components; PaSa-7b Selector marginally outperformed GPT-4o as a relevance classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against PaSa-7b Selector and Qwen-2.5-7b baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Selector (PaSa-7b) obtained F1 0.85 vs GPT-4o F1 0.80 on the paper relevance test set.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o is effective for synthetic data creation and as a prompting baseline, but task-specific fine-tuning and RL can outperform prompting-only usage in this paper's retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prompted GPT-4o (PaSa-GPT-4o) is less effective than RL-trained PaSa for deep citation-crawling tasks in the evaluated setting.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4609.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Litllm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced toolkit (Agarwal et al., 2024) that applies LLM techniques to assist scientific literature review; cited in the paper's related work as an example of LLMs applied to literature tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Litllm</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in Related Work as an example of tools that use LLMs to assist in literature review; the PaSa paper references this work but does not describe its architecture or experiments in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review tools (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper (likely toolkits / literature-review outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as related work demonstrating the use of LLMs for literature review assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4609.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autosurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work (Wang et al., 2024b) claiming LLMs can auto-generate surveys/surveys of literature; cited in Related Work as part of the emerging literature on LLMs for survey and synthesis tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autosurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related work that explores automatic survey-generation using LLMs; the PaSa paper does not provide implementation details for Autosurvey.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Survey generation over scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automatically-generated surveys (per the referenced title).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to show LLMs have been used to generate survey-level outputs from research literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4609.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced system (Baek et al., 2024) that iteratively uses LLMs over literature to generate research ideas; cited in Related Work as another LLM application across stages of scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in Related Work as an example of iterative LLM-based workflows for mining scientific literature to generate research ideas; PaSa does not detail ResearchAgent's internal methods.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research idea proposals/summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to motivate broader use of LLMs for research assistance beyond retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4609.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work (Wang et al., 2024a) about systems optimizing LLM outputs for novel scientific inspiration; included in related literature about LLMs in the research process.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciMON: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related work exploring LLM-driven systems for scientific inspiration and novelty-focused generation; PaSa references it in the broader context of LLMs assisting scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific idea generation and inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Novel research suggestions / inspiration artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an instance of LLMs used to synthesize novel scientific outputs across literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4609.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4609.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced conceptual/system work (Lu et al., 2024) that aims at fully automated scientific discovery using AI components; cited to contextualize LLMs' potential role in discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as high-level related work on automated scientific discovery; details are not provided in PaSa beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Automated scientific discovery systems.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Open-ended discoveries / experimental proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced to motivate the broader research agenda of LLM agents in science.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaSa: An LLM Agent for Comprehensive Academic Paper Search', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>SciMON: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Agile: A novel reinforcement learning framework of llm agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4609",
    "paper_id": "paper-5e5bca8f0b203785d371d265371ac88a2390c014",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "PaSa",
            "name_full": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
            "brief_description": "An agentic paper-search system built from two cooperating LLM agents (Crawler and Selector) that autonomously issues searches, reads papers, expands citations, and returns comprehensive sets of relevant papers for fine-grained scholarly queries; trained with imitation learning followed by session-level PPO within the AGILE framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PaSa",
            "system_description": "PaSa is composed of two LLM agents: (1) the Crawler, a token-level MDP policy LLM that generates search queries, invokes a search tool (Google with site:arxiv and before:query_date), and issues [Expand] actions to extract cited papers from target subsections; it maintains a growing paper queue and explores citation networks (exploration depth limited to 3 in experiments). (2) the Selector, an LLM classifier that takes a user query plus a paper (title+abstract+outline) and outputs a single decision token (True/False) followed by a textual rationale; the Selector's single-token decision is used as an auxiliary reward signal for Crawler RL. Training: imitation learning to initialize behavior, then a session-level PPO variant (session partitioning, in-session and across-session discounts, per-token KL penalty) to cope with sparse rewards and long trajectories.",
            "llm_model_used": "Qwen2.5-7b (PaSa-7b implementation); GPT-4o used for a prompting-based variant (PaSa-GPT-4o) and for parts of data synthesis.",
            "extraction_technique": "LLM-driven query generation + retrieval via Google (site:arxiv & time filter) + citation expansion from paper subsections parsed via ar5iv; token-level actions [Search], [Expand], [Stop] drive extraction and paper-queue management.",
            "synthesis_technique": "Aggregates papers by crawling citation networks to build a comprehensive paper queue, then uses the Selector to filter/score papers (decision token probability used for ranking); ensemble sampling at inference aggregates multiple crawler runs.",
            "number_of_papers": "Per-query trajectories can yield hundreds to thousands of papers (paper queue sizes may grow very large); experiments restricted crawler depth to 3; AutoScholarQuery dataset used for training contains 33,551 / 1,000 / 1,000 query-paper instances (train/dev/test).",
            "domain_or_topic": "Computer science / Machine learning literature (ICLR, ICML, NeurIPS, ACL, CVPR papers used for dataset and evaluation).",
            "output_type": "Sets/ranked lists of relevant papers (paper queue), boolean relevance decisions with textual rationales, and aggregated recall/precision metrics for discovered literature.",
            "evaluation_metrics": "Recall@20 / Recall@50 / Recall@100 for ranked baselines; precision, recall, F1 for unranked retrieval; Crawler recall and Crawler action counts; Selector precision/recall/F1.",
            "performance_results": "On AutoScholarQuery test: PaSa-7b achieved Recall@100=0.6947, Recall@50=0.6334, Recall@20=0.5301 and Crawler recall=0.7931. On RealScholarQuery: Recall@100=0.6929, Recall@50=0.6563, Recall@20=0.5798 and Crawler recall=0.7071. Selector: F1=0.85 (precision 0.95, recall 0.78).",
            "comparison_baseline": "Google, Google Scholar, Google + GPT-4o paraphrase, ChatGPT (search-enabled GPT-4o), GPT-o1 (no web), and PaSa-GPT-4o (prompted GPT-4o implementing PaSa behavior).",
            "performance_vs_baseline": "PaSa-7b outperformed Google+GPT-4o by 33.80%/38.83%/42.64% (Recall@20/50/100) on AutoScholarQuery and outperformed Google+GPT-4o on RealScholarQuery by 37.78% (Recall@20) and 39.90% (Recall@50). PaSa-7b exceeded PaSa-GPT-4o by 11.12% recall on AutoScholarQuery and by 30.36% recall and 4.25% precision on RealScholarQuery.",
            "key_findings": "Agentic LLMs that actively search and expand citation networks (Crawler) combined with a trained relevance classifier/reward model (Selector) significantly increase recall for fine-grained scholarly queries; RL fine-tuning (session-level PPO) and using the Selector as an auxiliary reward improve performance; explicit citation expansion ([Expand]) is crucial (ablation shows large recall drop without it).",
            "limitations_challenges": "Experiments limited to ML literature (dataset bias), used 7B LLMs (compute/scale constraints), training data synthesized from Related Work may omit ground-truth papers (sparse reward), long RL trajectories and computational expense, potential for missing or biased coverage outside curated conference corpora.",
            "scaling_behavior": "Authors report improved recall with increased reward coefficient α (e.g., crawler recall 0.7227 at α=0.5 → 0.8063 at α=2.0, with more crawler actions). They expect larger LLMs to further improve performance; session-level PPO converged after ~200 RL steps in their setup.",
            "uuid": "e4609.0",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "AGILE",
            "name_full": "AGILE: A novel reinforcement learning framework of LLM agents",
            "brief_description": "A reinforcement learning framework for training LLM-driven agents end-to-end, enabling joint optimization of tool use, planning, memory and multi-step behaviors; used by the authors to train PaSa with a tailored session-level PPO.",
            "citation_title": "Agile: A novel reinforcement learning framework of llm agents",
            "mention_or_use": "use",
            "system_name": "AGILE",
            "system_description": "AGILE is an RL training framework for LLM agents that supports joint optimization of agent skills (tool use, planning, memory) in an end-to-end fashion; the PaSa work adopts AGILE and extends it with a session-level PPO algorithm to cope with sparse rewards and long multi-session trajectories.",
            "llm_model_used": "Framework-agnostic; in PaSa experiments AGILE was applied to Qwen2.5-7b policies.",
            "extraction_technique": "Not an information-extraction system per se — provides RL machinery to train agents that perform extraction via tool invocations.",
            "synthesis_technique": "Optimizes agent policies that synthesize multi-step information-gathering behaviors (e.g., repeated search and citation expansion) across sessions.",
            "number_of_papers": "",
            "domain_or_topic": "General LLM agent training (applied here to academic paper search).",
            "output_type": "Trained LLM agent policies (e.g., PaSa Crawler/Selector).",
            "evaluation_metrics": "RL return curves, downstream recall/precision improvements when AGILE-trained agents are applied (e.g., PaSa gains).",
            "performance_results": "Applying AGILE-style RL (session-level PPO) to PaSa produced measurable gains: RL training improved recall by ~6.24% on AutoScholarQuery and ~19.96% on RealScholarQuery compared to imitation-only training.",
            "comparison_baseline": "Imitation-only initialization / prompting-only agents (e.g., PaSa-GPT-4o).",
            "performance_vs_baseline": "AGILE-based RL fine-tuning yields superior retrieval recall compared to imitation-only or prompting-only baselines in this study.",
            "key_findings": "Session partitioning for PPO (sessions ending with [Stop]) plus estimating across-session value-to-go and using a Selector as auxiliary reward mitigates sparse-reward and long-trajectory issues.",
            "limitations_challenges": "RL sampling of full trajectories is expensive; requires careful session design, value estimation across sessions, and per-token KL penalties to stabilize policy updates.",
            "scaling_behavior": "Paper describes training hyperparameters and that returns converged after ~200 PPO steps for their configuration; no broader empirical scaling curves beyond application-specific results.",
            "uuid": "e4609.1",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "PaSa-GPT-4o",
            "name_full": "PaSa (prompted GPT-4o variant)",
            "brief_description": "A prompting-based baseline that implements the PaSa architecture by prompt-engineering GPT-4o to perform multi-step searches, read papers, and crawl citations instead of training a policy via RL.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "PaSa-GPT-4o",
            "system_description": "Re-creation of PaSa's workflow via prompting: a prompted GPT-4o is used to generate search queries, read paper metadata/content, propose sections to expand citations, and decide relevance — all driven by carefully designed prompts rather than policy fine-tuning.",
            "llm_model_used": "GPT-4o (prompting)",
            "extraction_technique": "Prompt-driven query generation + web search (Google with site:arxiv/time filter) + instructed citation expansion via section selection prompts.",
            "synthesis_technique": "Aggregates papers by repeatedly prompting GPT-4o to expand and select; uses prompted decisions to form a paper queue and output results.",
            "number_of_papers": "Variable; experiments constrained crawler exploration depth to 3 for fair comparison with PaSa-7b.",
            "domain_or_topic": "Machine learning literature (same evaluation corpora as PaSa).",
            "output_type": "Retrieved candidate paper sets and relevance decisions (from GPT-4o prompts).",
            "evaluation_metrics": "Same retrieval metrics as PaSa (Recall@20/50/100; precision/recall for final sets).",
            "performance_results": "PaSa-GPT-4o achieved lower recall than PaSa-7b (example: PaSa-GPT-4o recall 0.3873 vs PaSa-7b 0.4834 on AutoScholarQuery per paper results), and was outperformed notably on RealScholarQuery (PaSa-7b surpassed PaSa-GPT-4o by ~30.36% recall).",
            "comparison_baseline": "Used as a baseline vs RL-trained PaSa-7b and Google-based methods.",
            "performance_vs_baseline": "Prompting-only GPT-4o implementation of PaSa underperforms RL-trained PaSa in recall despite having access to the same search tools.",
            "key_findings": "Prompt engineering of a strong LLM can approximate agent behaviors but RL fine-tuning provides measurable additional gains for thorough literature crawling and selection.",
            "limitations_challenges": "No learned long-term planning; limited by prompt-context management; less capable at multi-session planning compared to RL-trained PaSa.",
            "scaling_behavior": "Not explicitly reported beyond observed performance gap vs RL-trained PaSa-7b.",
            "uuid": "e4609.2",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "AutoScholarQuery",
            "name_full": "AutoScholarQuery (synthetic academic search dataset)",
            "brief_description": "A large synthetic dataset of fine-grained scholarly queries paired with corresponding relevant papers, created by prompting GPT-4o on the Related Work sections of recent top-tier ML conference papers to produce query–answer (paper) pairs for RL training and evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AutoScholarQuery",
            "system_description": "Constructed by extracting Related Work sections from papers (ICLR/ICML/NeurIPS/ACL/CVPR), prompting GPT-4o to generate scholarly queries whose answers correspond to the cited references, retaining only answers with arXiv IDs; split into 33,551 / 1,000 / 1,000 instances for train/dev/test.",
            "llm_model_used": "GPT-4o used to generate query formulations and in parts of data/label creation (e.g., Selector rationales).",
            "extraction_technique": "Transforming Related Work citation lists into explicit query–paper pairs via LLM prompting plus arXiv lookups for canonical identifiers.",
            "synthesis_technique": "Dataset synthesizes natural scholar queries and maps them to cited papers; does not itself synthesize cross-paper theories but provides targets for agent training.",
            "number_of_papers": "Dataset instances: 33,551 (train), 1,000 (dev), 1,000 (test); papers come from the cited corpora (tables show per-conference paper/query counts).",
            "domain_or_topic": "Machine learning / AI conference literature.",
            "output_type": "Query ↔ answer-paper sets for training and evaluating paper search agents.",
            "evaluation_metrics": "Human quality check: percent qualified queries and relevant papers; used downstream for RL reward and retrieval evaluations (recall metrics).",
            "performance_results": "Human evaluation: 94.0% of sampled queries were qualified; among qualified queries, 93.7% of corresponding papers were judged relevant.",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "High-quality synthetic query-paper pairs derived from Related Work can bootstrap RL training of LLM agents and generalize to real-world queries (RealScholarQuery) despite being synthetic.",
            "limitations_challenges": "AutoScholarQuery may only capture a subset of ground-truth relevant papers because Related Work sections cite only a subset of all relevant literature; potential domain bias to ML conferences.",
            "scaling_behavior": "Dataset size provides large-scale synthetic supervision; no explicit scaling law reported in paper beyond dataset statistics.",
            "uuid": "e4609.3",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Selector",
            "name_full": "PaSa Selector (relevance classifier / reward model)",
            "brief_description": "A fine-tuned LLM module that classifies whether a given paper satisfies a scholar query by emitting a single decision token (True/False) followed by a textual rationale; used both as the system's final decision-maker and as an auxiliary single-token reward model during Crawler RL training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Selector",
            "system_description": "Implemented by supervised fine-tuning (Qwen2.5-7b backbone) on data synthesized from AutoScholarQuery and GPT-4o-generated rationales; the model is trained to emit a decision token before the rationale so the decision can be read as a single-token reward; its token probability is used to rank search results or provide reward signal to the Crawler.",
            "llm_model_used": "Qwen2.5-7b (fine-tuned) in PaSa-7b experiments; GPT-4o used to generate initial rationale+decision training data.",
            "extraction_technique": "Prompted classification + rationale generation (single-token decision then textual explanation); trained with supervised fine-tuning and evaluated on human-annotated pairs.",
            "synthesis_technique": "Does not synthesize multiple papers itself; provides judgments and rationales that enable the Crawler to aggregate higher-quality paper collections.",
            "number_of_papers": "Training dataset comprised 19,812 &lt;query,paper&gt; pairs; test benchmark of 200 pairs used for Selector evaluation.",
            "domain_or_topic": "Machine learning literature (same domains as PaSa).",
            "output_type": "Decision (True/False) and textual rationale; probability of decision token used for ranking.",
            "evaluation_metrics": "Precision, recall, F1 measured on a held-out selector test set.",
            "performance_results": "Selector achieved precision 0.95, recall 0.78, F1 0.85 on the selector test set; outperformed GPT-4o (F1=0.80) and Qwen-2.5-7b baseline (F1=0.55).",
            "comparison_baseline": "GPT-4o and base Qwen-2.5-7b classification.",
            "performance_vs_baseline": "PaSa-7b Selector improved F1 by ~5% over GPT-4o and by ~30% over the Qwen baseline on the selector test set.",
            "key_findings": "Generating the decision token before the rationale enables the Selector to be used as a cheap auxiliary reward signal for RL; high precision makes it suitable as an RL reward model.",
            "limitations_challenges": "Training relies on synthetic labels and GPT-4o generated rationales; possible biases and domain-limited training data.",
            "scaling_behavior": "Not explicitly studied beyond showing Selector is effective at 7B scale in this work.",
            "uuid": "e4609.4",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Crawler",
            "name_full": "PaSa Crawler (search & citation expansion agent)",
            "brief_description": "An LLM-driven policy that iteratively generates search queries, issues retrievals, inspects papers, and expands citation subsections to discover relevant literature; modeled as a token-level MDP and trained with imitation learning followed by session-level PPO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Crawler",
            "system_description": "Crawler is framed as a token-level MDP where each generated token is an action. It supports three registered functions: [Search] to generate a search query and invoke Google (site:arxiv & before:date), [Expand] to propose a subsection name then parse and append referenced papers from that subsection to the paper queue, and [Stop] to reset context to the next paper. Training uses imitation to seed behavior and session-level PPO (return estimator includes across-session estimated values for appended papers and per-token KL penalty).",
            "llm_model_used": "Policy implemented with Qwen2.5-7b (PaSa-7b); imitation data generated with GPT-4o.",
            "extraction_technique": "LLM-prompted search query generation + web retrieval API + subsection-based citation extraction via ar5iv parsing.",
            "synthesis_technique": "Constructs a paper tree (paper queue) by exploring citation networks and accumulating candidate papers for later filtering by the Selector; across-session value estimates incorporate future sessions spawned by added papers.",
            "number_of_papers": "Single-query trajectories can add hundreds to thousands of papers; in ablations and controlled experiments crawler actions vary (e.g., 382 actions at α=1.5) and depth limited to 3 in evaluations.",
            "domain_or_topic": "Machine learning literature.",
            "output_type": "Paper queue (discovered candidate papers), search queries, expansion decisions; Crawler recall metric measuring fraction of target papers appended to queue.",
            "evaluation_metrics": "Crawler recall (fraction of target papers collected), number of crawler actions, and downstream recall/precision after Selector filtering.",
            "performance_results": "Crawler recall for PaSa-7b: 0.7931 on AutoScholarQuery test and 0.7071 on RealScholarQuery; removal of [Expand] reduced recall by ~22.98% (Auto) and ~32.21% (Real).",
            "comparison_baseline": "PaSa-GPT-4o Crawler (prompted), Google-based retrievals.",
            "performance_vs_baseline": "PaSa-7b Crawler recall exceeded PaSa-GPT-4o by a few percentage points in experiments, contributing to higher final recall after selection.",
            "key_findings": "Citation expansion ([Expand]) is essential to find deeper relevant works not retrieved by surface-level search queries; session-level RL that includes value estimates for appended papers helps optimize long-horizon discovery.",
            "limitations_challenges": "Long trajectories (many papers) create sampling and context-size challenges; reward sparsity if ground-truth datasets only include a subset of actual relevant papers.",
            "scaling_behavior": "Increasing reward coefficient α increases both crawler recall and the number of actions (e.g., recall 0.7227→0.8063 as α increases from 0.5→2.0); action cost parameter trades off recall vs. action volume.",
            "uuid": "e4609.5",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4o (used)",
            "name_full": "GPT-4o system card / GPT-4o",
            "brief_description": "A large language model (GPT-4o) used in this work for data synthesis (AutoScholarQuery prompt generation), for generating imitation trajectories, for paraphrasing queries in baselines, and as the backbone of the prompting-based PaSa-GPT-4o baseline.",
            "citation_title": "Gpt-4o system card",
            "mention_or_use": "use",
            "system_name": "GPT-4o (as used in experiments)",
            "system_description": "GPT-4o is used to generate scholarly queries from Related Work sections (AutoScholarQuery), create imitation-learning trajectories and Selector rationales, paraphrase search queries for Google+GPT-4o baseline, and as the LLM in the PaSa-GPT-4o prompting baseline.",
            "llm_model_used": "GPT-4o (OpenAI system referenced).",
            "extraction_technique": "Prompt-driven generation of queries and rationales; paraphrasing of user queries for retrieval; prompted classification in some baselines.",
            "synthesis_technique": "Used to synthesize query–paper pairs and rationales from paper Related Work text; can be prompted to implement multi-step paper search behaviors.",
            "number_of_papers": "",
            "domain_or_topic": "General-purpose LLM used for ML literature tasks here.",
            "output_type": "Paraphrased queries, candidate paper lists, decision+rationale texts used as training labels.",
            "evaluation_metrics": "When evaluated as a Selector baseline, achieved precision 0.96, recall 0.69, F1 0.80 on the selector test set reported in this paper.",
            "performance_results": "GPT-4o used as strong baseline in many components; PaSa-7b Selector marginally outperformed GPT-4o as a relevance classifier.",
            "comparison_baseline": "Compared against PaSa-7b Selector and Qwen-2.5-7b baselines.",
            "performance_vs_baseline": "Selector (PaSa-7b) obtained F1 0.85 vs GPT-4o F1 0.80 on the paper relevance test set.",
            "key_findings": "GPT-4o is effective for synthetic data creation and as a prompting baseline, but task-specific fine-tuning and RL can outperform prompting-only usage in this paper's retrieval tasks.",
            "limitations_challenges": "Prompted GPT-4o (PaSa-GPT-4o) is less effective than RL-trained PaSa for deep citation-crawling tasks in the evaluated setting.",
            "scaling_behavior": "",
            "uuid": "e4609.6",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Litllm",
            "name_full": "Litllm: A toolkit for scientific literature review",
            "brief_description": "Referenced toolkit (Agarwal et al., 2024) that applies LLM techniques to assist scientific literature review; cited in the paper's related work as an example of LLMs applied to literature tasks.",
            "citation_title": "Litllm: A toolkit for scientific literature review",
            "mention_or_use": "mention",
            "system_name": "Litllm",
            "system_description": "Mentioned in Related Work as an example of tools that use LLMs to assist in literature review; the PaSa paper references this work but does not describe its architecture or experiments in detail.",
            "llm_model_used": "",
            "extraction_technique": "",
            "synthesis_technique": "",
            "number_of_papers": "",
            "domain_or_topic": "Scientific literature review tools (general).",
            "output_type": "Not specified in this paper (likely toolkits / literature-review outputs).",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited as related work demonstrating the use of LLMs for literature review assistance.",
            "limitations_challenges": "Not discussed in this paper.",
            "scaling_behavior": "",
            "uuid": "e4609.7",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Autosurvey",
            "name_full": "Autosurvey: Large language models can automatically write surveys",
            "brief_description": "Referenced work (Wang et al., 2024b) claiming LLMs can auto-generate surveys/surveys of literature; cited in Related Work as part of the emerging literature on LLMs for survey and synthesis tasks.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys",
            "mention_or_use": "mention",
            "system_name": "Autosurvey",
            "system_description": "Mentioned as related work that explores automatic survey-generation using LLMs; the PaSa paper does not provide implementation details for Autosurvey.",
            "llm_model_used": "",
            "extraction_technique": "",
            "synthesis_technique": "",
            "number_of_papers": "",
            "domain_or_topic": "Survey generation over scientific literature.",
            "output_type": "Automatically-generated surveys (per the referenced title).",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited to show LLMs have been used to generate survey-level outputs from research literature.",
            "limitations_challenges": "",
            "scaling_behavior": "",
            "uuid": "e4609.8",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "Referenced system (Baek et al., 2024) that iteratively uses LLMs over literature to generate research ideas; cited in Related Work as another LLM application across stages of scientific discovery.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "ResearchAgent",
            "system_description": "Mentioned in Related Work as an example of iterative LLM-based workflows for mining scientific literature to generate research ideas; PaSa does not detail ResearchAgent's internal methods.",
            "llm_model_used": "",
            "extraction_technique": "",
            "synthesis_technique": "",
            "number_of_papers": "",
            "domain_or_topic": "Scientific literature idea generation.",
            "output_type": "Research idea proposals/summaries.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited to motivate broader use of LLMs for research assistance beyond retrieval.",
            "limitations_challenges": "",
            "scaling_behavior": "",
            "uuid": "e4609.9",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "SciMON: Scientific inspiration machines optimized for novelty",
            "brief_description": "Referenced work (Wang et al., 2024a) about systems optimizing LLM outputs for novel scientific inspiration; included in related literature about LLMs in the research process.",
            "citation_title": "SciMON: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "system_name": "SciMON",
            "system_description": "Mentioned as related work exploring LLM-driven systems for scientific inspiration and novelty-focused generation; PaSa references it in the broader context of LLMs assisting scientific discovery.",
            "llm_model_used": "",
            "extraction_technique": "",
            "synthesis_technique": "",
            "number_of_papers": "",
            "domain_or_topic": "Scientific idea generation and inspiration.",
            "output_type": "Novel research suggestions / inspiration artifacts.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Cited as an instance of LLMs used to synthesize novel scientific outputs across literature.",
            "limitations_challenges": "",
            "scaling_behavior": "",
            "uuid": "e4609.10",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "The AI Scientist",
            "name_full": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "Referenced conceptual/system work (Lu et al., 2024) that aims at fully automated scientific discovery using AI components; cited to contextualize LLMs' potential role in discovery workflows.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "The AI Scientist",
            "system_description": "Cited as high-level related work on automated scientific discovery; details are not provided in PaSa beyond the citation.",
            "llm_model_used": "",
            "extraction_technique": "",
            "synthesis_technique": "",
            "number_of_papers": "",
            "domain_or_topic": "Automated scientific discovery systems.",
            "output_type": "Open-ended discoveries / experimental proposals.",
            "evaluation_metrics": "",
            "performance_results": "",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "key_findings": "Referenced to motivate the broader research agenda of LLM agents in science.",
            "limitations_challenges": "",
            "scaling_behavior": "",
            "uuid": "e4609.11",
            "source_info": {
                "paper_title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 2
        },
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "SciMON: Scientific inspiration machines optimized for novelty",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Agile: A novel reinforcement learning framework of llm agents",
            "rating": 2
        }
    ],
    "cost": 0.029117,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PaSa: An LLM Agent for Comprehensive Academic Paper Search</h1>
<p>Yichen $\mathrm{He}^{+1}$ Guanhua Huang ${ }^{+1} \quad$ Peiyuan Feng ${ }^{1} \quad$ Yuan Lin ${ }^{11}$<br>Yuchen Zhang ${ }^{1}$ Hang $\mathbf{L i}^{1}$ Weinan $\mathbf{E}^{2}$<br>${ }^{1}$ ByteDance Seed ${ }^{2}$ Peking University<br>{hyc, huangguanhua, fpy, linyuan.0}@bytedance.com, {zhangyuchen.zyc, lihang. lh}@bytedance.com, weinan@math. pku.edu.cn<br>Demo: https://pasa-agent.ai</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Paper Search</p>
<h4>Abstract</h4>
<p>We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35 k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-01, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by $37.78 \%$ in recall@20 and $39.90 \%$ in recall@50, and exceeds PaSa-GPT40 by $30.36 \%$ in recall and $4.25 \%$ in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.</p>
<h2>1 Introduction</h2>
<p>Academic paper search lies at the core of research yet represents a particularly challenging informa-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion retrieval task. It requires long-tail specialized knowledge, comprehensive survey-level coverage, and the ability to address fine-grained queries. For instance, consider the query: "Which studies have focused on non-stationary reinforcement learning using value-based methods, specifically UCB-based algorithms?" While widely used academic search systems like Google Scholar are effective for general queries, they often fall short when addressing these complex queries (Gusenbauer and Haddaway, 2020). Consequently, researchers frequently spend substantial time conducting literature surveys (Kingsley et al., 2011; Gusenbauer and Haddaway, 2021).</p>
<p>The advancements in large language models (LLMs) (OpenAI, 2023; Anthropic, 2024; Gemini, 2023; Yang et al., 2024) have inspired numerous studies leveraging LLMs to enhance information retrieval, particularly by refining or reformulating search queries to improve retrieval quality (Alaofi et al., 2023; Li et al., 2023; Ma et al., 2023; Peng et al., 2024). In academic search, however, the process goes beyond simple retrieval. Human researchers not only use search tools, but also engage in deeper activities, such as reading relevant papers and checking citations, to perform comprehensive and accurate literature surveys.</p>
<p>In this paper, we introduce PaSa, a novel paper search agent designed to mimic human behavior for comprehensive and accurate academic paper searches. As illustrated in Figure 1, PaSa consists of two LLM agents: the Crawler and the Se-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: Architecture of PaSa. The system consists of two LLM agents, Crawler and Selector. The Crawler processes the user query and can access papers from the paper queue. It can autonomously invoke the search tool, expand citations, or stop processing of the current paper. All papers collected by the Crawler are appended to the paper queue. The Selector reads each paper in the paper queue to determine whether it meets the criteria specified in the user query.</p>
<p>lector. For a given user query, the Crawler can autonomously collect relevant papers by utilizing search tools or extracting citations from the current paper, which are then added to a growing <em>paper queue</em>. The Crawler iteratively processes each paper in the paper queue, navigating citation networks to discover increasingly relevant papers. The Selector carefully reads each paper in the paper queue to determine whether it meets the requirements of the user query. We optimize PaSa within the AGILE, a reinforcement learning (RL) framework for LLM agents (Feng et al., 2024).</p>
<p>Effective training requires high-quality academic search data. Fortunately, human scientists have already created a vast amount of high-quality academic papers, which contain extensive surveys on a wide range of research topics. We build a synthetic but high-quality academic search dataset, AutoScholarQuery, which collects fine-grained scholar queries and their corresponding relevant papers from the related work sections of papers published at ICLR 2023 <sup>1</sup>, ICML 2023 <sup>2</sup>, NeurIPS 2023 <sup>3</sup>, ACL 2024 <sup>4</sup>, and CVPR 2024 <sup>5</sup>. AutoScholarQuery includes 33,511 / 1,000 / 1,000 query-paper pairs in the training / development / test split.</p>
<p>Although AutoScholarQuery only provides query and paper answers, without demonstrating the path by which scientists collect the papers, we can utilize it to perform RL training to improve PaSa. In addition, we design a new session-level</p>
<p>PPO (Proximal Policy Optimization (Schulman et al., 2017)) training method to address the unique challenges of the paper search task: 1) sparse reward: The papers in AutoScholarQuery are collected via citations, making it a smaller subset of the actual qualified paper set. 2) long trajectories: The complete trajectory of the Crawler may involve hundreds of papers, which is too long to directly input into the LLM context.</p>
<p>To evaluate PaSa, besides the test set of AutoScholarQuery, we also develop a benchmark, RealScholarQuery. It contains 50 real-world academic queries with annotated relevant papers, to assess PaSa in real-world scenarios. We compare PaSa with several baselines including Google, Google Scholar, Google paired with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1 and PaSa-GPT-4o (PaSa agent realized by prompting GPT-4o). Our experiments show that PaSa-7b significantly outperforms all baselines. Specifically, for AutoScholarQuery test set, PaSa-7b achieves a 34.05% improvement in Recall@20 and a 39.36% improvement in Recall@50 compared to Google with GPT-4o, the strongest Google-based baseline. PaSa-7b surpasses PaSa-GPT-4o by 11.12% in recall, with similar precision. For RealScholarQuery, PaSa-7b outperforms Google with GPT-4o by 37.78% in Recall@20 and 39.90% in Recall@50. PaSa-7b surpasses PaSa-GPT-4o by 30.36% in recall and 4.25% in precision.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<p>• We introduce PaSa, a comprehensive and accurate paper search agent that can autonomously use online search tools, read entire papers, and navigate citation networks.</p>
<p><sup>1</sup>https://iclr.cc/Conferences/2023</p>
<p><sup>2</sup>https://icml.cc/Conferences/2023</p>
<p><sup>3</sup>https://neurips.cc/Conferences/2023</p>
<p><sup>4</sup>https://2024.aclweb.org/</p>
<p><sup>5</sup>https://cvpr.thecvf.com/Conferences/2024</p>
<ul>
<li>We develop two high-quality datasets for complex academic search, AutoScholarQuery and RealScholarQuery.</li>
<li>Although PaSa is trained solely on synthetic data, it achieves remarkable real-world performance. Experiments demonstrate that PaSa, built on 7B LLM, significantly outperforms all baselines, including GPT-4 agent, Googlebased search, and ChatGPT.</li>
</ul>
<h2>2 Related Work</h2>
<p>LLMs in Scientific Discovery LLMs have been applied across various stages of scientific discovery (Van Noorden and Perkel, 2023; Lu et al., 2024; Messeri and Crockett, 2024; Liao et al., 2024), such as brainstorming ideas (Girotra et al., 2023; Wang et al., 2024a; Baek et al., 2024), designing experiments (M. Bran et al., 2024), writing code (Xu et al., 2022), and generating research papers (Shao et al., 2024; Agarwal et al., 2024; Wang et al., 2024b). One of the most fundamental yet critical stages in research is conducting academic surveys. Despite its importance, current tools like Google Scholar are often insufficient, leading researchers to spend considerable time on literature review tasks (Kingsley et al., 2011; Gusenbauer and Haddaway, 2021, 2020). This challenge motivates us to develop PaSa, an LLM agent designed to autonomously and comprehensively assist researchers in collecting relevant research papers for complex scholarly queries.</p>
<p>LLM Agents LLM Agents combine LLMs with memory, tool use, and planning, enabling them to perform more complex tasks such as personal copilots (Stratton, 2024), travel planning (Gundawar et al., 2024), web operations (Deng et al., 2024), software development (Qian et al., 2023), and scientific experimentation (Bran et al., 2023). In addition to realizing LLM Agents through prompt engineering (Park et al., 2023; Yao et al., 2023; Shinn et al., 2024; Chen et al., 2023), recent research has focused on optimizing and training these agents (Feng et al., 2024; Putta et al., 2024; Liu et al., 2023). Among these efforts, AGILE (Feng et al., 2024), a reinforcement learning framework for LLM agents, allows the joint optimization of all agent skills in an end-to-end manner. In our work, we adopt the AGILE framework to implement PaSa. Specifically, we design a novel session-level PPO algorithm to address the unique challenges of the
paper search task, including sparse rewards and long trajectories.</p>
<h2>3 Datasets</h2>
<h3>3.1 AutoScholarQuery</h3>
<p>AutoScholarQuery is a synthetic but high-quality dataset of academic queries and related papers, specifically curated for the AI field.</p>
<p>To construct AutoScholarQuery, we began by collecting all papers published at ICLR 2023, ICML 2023, NeurIPS 2023, ACL 2024, and CVPR 2024. For the Related Work section of each paper, we prompted GPT-4o (Hurst et al., 2024) to generate scholarly queries, where the answers to these queries correspond to the references cited in the Related Work section. The prompt used is shown in Appendix H.1. For each query, we retained only the papers that could be retrieved on $\operatorname{arXiv}{ }^{6}$, using their arxiv_id as the unique article identifier in the dataset. We adopt the publication date of the source paper as the query date. During both training and testing, we only considered papers published prior to the query date.</p>
<p>The final AutoScholarQuery dataset comprises 33,551, 1,000, and 1,000 instances in the training, development, and testing splits, respectively. Each instance consists of a query, the associated paper set, and the query date, with queries in each split derived from distinct source papers. Table 1 provides illustrative examples from AutoScholarQuery, while additional dataset statistics are summarized in Table 2.</p>
<p>To evaluate the quality of AutoScholarQuery, we sampled 100 query-paper pairs and assessed the rationality and relevance of each query and the corresponding paper. A qualified query should be meaningful and unambiguous. A qualified paper should match the requirements of the scholarly query. Detailed evaluation criteria are provided in Appendix. A. Three authors manually reviewed each pair, determining that $94.0 \%$ of the queries were qualified. Among these qualified queries, $93.7 \%$ had corresponding papers that were deemed relevant and appropriate.</p>
<h3>3.2 RealScholarQuery</h3>
<p>To evaluate PaSa in more realistic scenarios, we constructed RealScholarQuery, a test dataset consisting of 50 real-world research queries. After launching the demo of PaSa, we invited several AI</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Query: Could you provide me some studies that proposed hierarchical neural models to capture spatiotemporal features in sign videos?</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Query Date: 2023-05-02</td>
<td></td>
</tr>
<tr>
<td>Answer Papers:</td>
<td></td>
</tr>
<tr>
<td>[1] TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation (2010.05468)</td>
<td></td>
</tr>
<tr>
<td>[2] Sign Language Translation with Hierarchical Spatio-Temporal Graph Neural Network (2111.07258)</td>
<td></td>
</tr>
<tr>
<td>Source: SLTUnet: A Simple Unified Model for Sign Language Translation, ICLR 2023</td>
<td></td>
</tr>
<tr>
<td>Query: Which studies have focused on nonstationary RL using value-based methods, specifically Upper Confidence Bound (UCB) based algorithms?</td>
<td></td>
</tr>
<tr>
<td>Query Date: 2023-08-10</td>
<td></td>
</tr>
<tr>
<td>Answer Papers:</td>
<td></td>
</tr>
<tr>
<td>[1] Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism (2006.14389)</td>
<td></td>
</tr>
<tr>
<td>[2] Efficient Learning in Non-Stationary Linear Markov Decision Processes (2010.12870)</td>
<td></td>
</tr>
<tr>
<td>[3] Nonstationary Reinforcement Learning with Linear Function Approximation (2010.04244)</td>
<td></td>
</tr>
<tr>
<td>Source: Provably Efficient Algorithm for Nonstationary Low-Rank MDPs, NeurIPS 2023</td>
<td></td>
</tr>
<tr>
<td>Query: Which studies have been conducted in long-form text generation, specifically in story generation?</td>
<td></td>
</tr>
<tr>
<td>Query Date: 2024-01-26</td>
<td></td>
</tr>
<tr>
<td>Answer Papers:</td>
<td></td>
</tr>
<tr>
<td>[1] Strategies for Structuring Story Generation (1902.01109)</td>
<td></td>
</tr>
<tr>
<td>[2] MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models (2010.00840)</td>
<td></td>
</tr>
<tr>
<td>Source: ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models, ACL 2024</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of queries and corresponding papers in AutoScholarQuery.</p>
<table>
<thead>
<tr>
<th>Conference</th>
<th>$</th>
<th>P</th>
<th>$</th>
<th>$</th>
<th>Q</th>
<th>$</th>
<th>$Ans(/Q)$</th>
<th>$Ans$-50</th>
<th>$Ans$-90</th>
</tr>
</thead>
<tbody>
<tr>
<td>ICLR 2023</td>
<td>888</td>
<td>5204</td>
<td>2.46</td>
<td>2.0</td>
<td>5.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ICML 2023</td>
<td>981</td>
<td>5743</td>
<td>2.37</td>
<td>2.0</td>
<td>5.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>NeurIPS 2023</td>
<td>1948</td>
<td>11761</td>
<td>2.59</td>
<td>2.0</td>
<td>5.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CVPR 2024</td>
<td>1336</td>
<td>9528</td>
<td>2.94</td>
<td>2.0</td>
<td>6.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ACL 2024</td>
<td>485</td>
<td>3315</td>
<td>2.16</td>
<td>2.0</td>
<td>4.0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of AutoScholarQuery. $|P|$ and $|Q|$ represent the total number of papers and queries collected for each conference. $A n s(/Q)$ denotes the average number of answer papers per query. $A n s$-50 and $A n s$-90 refers to the 50th and 90th percentiles of answer paper counts per query.
researchers to use the system. From the queries they provided, we randomly sampled a subset of queries and manually filtered out overly broad topics (e.g., "multimodal large language models," "video generation"). Ultimately, we collected 50 fine-grained and realistic queries.</p>
<p>For each query, we first manually gathered relevant papers to the best of our ability. To ensure comprehensive coverage, we then applied multiple methods to retrieve additional papers, including PaSa, Google, Google Scholar, ChatGPT (search-enabled GPT-4o), and Google paired with GPT-4o for paraphrased queries. As these methods also serve as baselines for comparison with PaSa, implementation details are deferred to Section 5.2. The results from all methods were aggregated into a pool of candidate papers. Finally, professional annotators reviewed all candidate papers for each query, selecting those that met the specific require-
ments of the query to create the final set of relevant papers. Annotation guidelines and quality control procedures are detailed in Appendix. B. The query date of all instances in RealScholarQuery is 2024-10-01. Table 9 in Appendix C provides an example from RealScholarQuery.</p>
<p>The annotators included professors from the Department of Computer Science at a top-tier university in China. On average, each query required the annotators to review 76 candidate papers. We paid $\$ 4$ per data entry (a query-paper pair), resulting in an average of $\$ 304$ per query. Given the high annotation cost, we completed this process for only 50 instances. On average, each query is associated with 15.82 answer papers. The 50th percentile of answer counts per query is 9 , while the 90th percentile reaches 37.</p>
<h2>4 Methodology</h2>
<h3>4.1 Overview</h3>
<p>As illustrated in Figure 1, the PaSa system consists of two LLM agents: Crawler and Selector. The crawler reads the user’s query, generates multiple search queries, and retrieves relevant papers. The retrieved papers are added to a paper queue. The Crawler further processes each paper in the paper queue to identify key citations worth exploring further, appending any newly relevant papers to the paper queue. The selector conducts a thorough review of each paper in the paper queue to assess whether it fulfills the user’s query requirements.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: An example of the PaSa workflow. The Crawler runs multiple [Search] using diverse and complementary queries. In addition, the Crawler can evaluate the long-term value of its actions. Notably, it discovers many relevant papers as it explores deeper in the citation network, even when intermediate papers along the path do not align with the user query.</p>
<p>In summary, the Crawler is designed to maximize the recall of relevant papers, whereas the Selector emphasizes precision in identifying papers that meet the user's needs.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td>[Search]</td>
<td>Generate a search query and invoke the search tool. Append all resulting papers to the paper queue.</td>
</tr>
<tr>
<td>[Expand]</td>
<td>Generate a subsection name, then add all referenced papers in the subsection to the paper queue.</td>
</tr>
<tr>
<td>[Stop]</td>
<td>Reset the context to the user query and the next paper in the paper queue.</td>
</tr>
</tbody>
</table>
<p>Table 3: Functions of the Crawler.</p>
<h3>4.2 Crawler</h3>
<p>In RL terminology, the Crawler performs a token-level Markov Decision Process (MDP). The action space $\mathcal{A}$ corresponds to the LLM's vocabulary, where each token represents an action. The LLM functions as the policy model. The agent's state is defined by the current LLM context and the paper queue. The Crawler operates with three registered functions, as outlined in Table 3. When an action matches a function name, the corresponding function is executed, further modifying the agent's state.</p>
<p>For example, as Figure 2 shows, the agent begins by receiving a user query, incorporating it into its context, and initiating actions. If the token generated is [Search], the LLM continues to generate a search query, and the agent invokes a search tool to retrieve papers, which are then added to the paper queue. If the token is [Expand], the LLM continues to extract a subsection name from the current paper in its context. The agent then extracts all referenced papers within that subsection, adding them to the paper queue. If the token is [Stop], the agent resets its context to the user query and information of the next paper in the paper queue. This information includes the title, abstract, and an outline of all sections and subsections.</p>
<p>The training process for the Crawler comprises two stages. In the first stage, we generate trajectories for a small subset of the training data and then perform imitation learning (see Appendix D.1 for details). In the second stage, reinforcement learning is applied. The details of the RL training implementation are described below.</p>
<p><strong>Reward Design</strong> We conduct RL training on the AutoScholarQuery training set, where each instance consists of a query $q$ and a corresponding paper set $\mathcal{P}$. Starting with a query $q$, the Crawler generates a trajectory $\tau = (s_{1}, a_{1}, \cdots, s_{T}, a_{T})$. At each time step $t$, we denote the current paper queue as $\mathcal{Q}<em t="t">{t}$. Upon taking action $a</em> = [Stop]$, the set is empty and no papers are added.}$, the Crawler appends a set of new papers $(p_{1}, p_{2}, \cdots, p_{n_{t}})$ to the paper queue. If $a_{t</p>
<p>The reward of executing action $a_{t}$ in state $s_{t}$ is defined as</p>
<p>$$r(s_t, a_t) = \alpha \times \sum_{i=1}^{n_t} \mathbb{I}(q, p_i, t) - c(a_t), \tag{1}$$</p>
<p>where $\mathbb{I}\left(q, p_{i}, t\right)=1$ if $p_{i}$ matches the query $q$ and is not already in $\mathcal{Q}<em i="i">{t}$, and $\mathbb{I}\left(q, p</em>$.}, t\right)=0$ otherwise. Here, $\alpha$ is a reward coefficient, and $c\left(a_{t}\right)$ is the cost of action $a_{t</p>
<p>The indicator function $\mathbb{I}\left(q, p_{i}, t\right)$ can be determined by checking if $p_{i}$ belongs to $\mathcal{P}-\mathcal{Q}<em i="i">{t}$. However, it is important to note that the AutoScholarQuery may only include a subset of the groundtruth papers, as citations often emphasize a limited number of key references. If the Crawler receives rewards solely based on matching papers in AutoScholarQuery, this could lead to sparse rewards during training. To mitigate this, we use the Selector as an auxiliary reward model for the Crawler. The revised definition of $\mathbb{I}\left(q, p</em>, t\right)$ is:</p>
<p>$$
\mathbb{I}\left(q, p_{i}, t\right)= \begin{cases}1, &amp; \text { if }\left(\operatorname{Selector}\left(q, p_{i}\right)=1 \text { or } p_{i} \in \mathcal{P}\right) \ &amp; \text { and } p_{i} \notin \mathcal{Q}_{t} \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Here $\operatorname{Selector}\left(q, p_{i}\right)=1$ if paper $p_{i}$ is identified as correct to meet the query $q$ by the Selector, and $\operatorname{Selector}\left(q, p_{i}\right)=0$ otherwise.</p>
<p>RL Training A key challenge in training the Crawler with RL is the significant time required to sample a complete trajectory for a given query. This is due to each [Search] or [Expand] action adding multiple papers to the paper queue, resulting in hundreds or even thousands of papers in the final paper queue.</p>
<p>To address this issue, we define a session as a sub-trajectory that ends with the [Stop] action, after which a new session begins. We identify two types of initial states for such sub-trajectories: $S_{q}$, containing only the user query, and $S_{q+p}$, containing both the query and a paper. $S_{q}$ represents the task's starting point, where the LLM context includes only the query. In contrast, $S_{q+p}$ arises after a [Stop] action, where the LLM context is reset to the query and the next paper in the queue.</p>
<p>Formally, we model the Crawler as a policy $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$. We partition the entire trajectory $\tau=\left(s_{1}, a_{1}, \cdots, s_{T}, a_{T}\right)$ into a sequence of sessions: $\left(\tau_{t_{1}: t_{2}-1}, \tau_{t_{2}: t_{3}-1}, \cdots\right)$. Each session is $\tau_{t_{i}: t_{i+1}-1}=\left(s_{t_{i}}, a_{t_{i}}, \cdots, s_{t_{i+1}-1}, a_{t_{i+1}-1}\right)$, where the initial state $s_{t_{i}}$ is either belonging to type $S_{q}$ or $S_{q+p}$, and the final action $a_{t_{i+1}-1}$ is [STOP].</p>
<p>Sampling such a sub-trajectory from these session initial states is computationally efficient. During the PPO training, at time step $t \in\left[t_{i}, t_{i+1}\right)$, we estimate the return in the session using Monte Carlo sampling:</p>
<p>$$
\begin{aligned}
\hat{R}<em k="t">{t}= &amp; \sum</em>\right)\right. \
&amp; \left.+\gamma_{1} \sum_{j=1}^{n_{k}} \hat{V}}^{t_{i+1}-1} \gamma_{0}^{k-t}\left[r\left(s_{k}, a_{k<em q_p__j="q+p_{j">{\phi}\left(S</em>
\end{aligned}
$$}}\right)\right]-\beta \cdot \log \frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\mathrm{aft}}\left(a_{t} \mid s_{t}\right)</p>
<p>Here, $\gamma_{0}$ is the in-session discount factor, and $\gamma_{1}$ is the across-session discount factor. $\hat{V}<em k="k">{\phi}(\cdot)$ is the value function model to approximate the state value. After executing $a</em>$ obtained through imitation learning at each token to mitigate overoptimization. This term is scaled by the coefficient $\beta$.}$, the paper queue is updated to include the newly found papers $\left(p_{1}, p_{2}, \cdots, p_{n_{k}}\right)$. Since the Crawler will subsequently initiate new sessions to process these additional papers, their associated reward-to-go should be incorporated into the return estimate. In addition, we include a per-token KL penalty term from the learned policy $\pi_{\theta}$ to the initial policy $\pi_{\mathrm{aft}</p>
<p>Then the advantage function can be approximated by</p>
<p>$$
\hat{A}\left(s_{t}, a_{t}\right)=\hat{R}<em _phi="\phi">{t}-\hat{V}</em>\right)
$$}\left(s_{t</p>
<p>Finally, the policy and value objectives can be given by</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _tau_prime="\tau^{\prime">{\text {policy }}(\theta)= &amp; \mathbb{E}</em>\right)\right.\right. \
&amp; \left.\left.\operatorname{clip}\left(\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta}^{\text {old }}\left(a_{t} \mid s_{t}\right)}, 1-\epsilon, 1+\epsilon\right) \hat{A}\left(s_{t}, a_{t}\right)\right)\right]
\end{aligned}
$$} \sim \pi_{\theta}^{\text {old }}}\left[\min \left(\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta}^{\text {old }}\left(a_{t} \mid s_{t}\right)} \hat{A}\left(s_{t}, a_{t</p>
<p>and</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}<em _tau_prime="\tau^{\prime">{\text {value }}(\phi)=\mathbb{E}</em>} \sim \pi_{\theta}^{\text {old }}}\left[\max \left(\left(\hat{\mathrm{R}<em _phi="\phi">{t}-\hat{\mathrm{V}}</em>}\left(\mathrm{s<em t="t">{t}\right)\right)^{2}\right.\right. \
&amp; \left.\left.\left(\hat{R}</em>}-\hat{V<em t="t">{\phi}^{\text {clip }}\left(s</em>\right)\right]
\end{aligned}
$$}\right)\right)^{2</p>
<p>respectively, where</p>
<p>$$
\hat{V}<em t="t">{\phi}^{\text {clip }}\left(s</em>}\right)=\operatorname{clip}\left(\hat{V<em t="t">{\phi}\left(s</em>\right)+\epsilon\right)
$$}\right), V_{\phi}^{\text {old }}\left(s_{t}\right)-\epsilon, V_{\phi}^{\text {old }}\left(s_{t</p>
<p>Here, $\pi_{\theta}^{\text {old }}$ and $V_{\phi}^{\text {old }}$ is used for sampling and $\tau^{\prime}$ is session trajectory. We then combine these into the unified RL loss:</p>
<p>$$
\mathcal{L}<em _policy="{policy" _text="\text">{\mathrm{RL}}(\theta, \phi)=\mathcal{L}</em>(\phi)
$$}}(\theta)+\eta \cdot \mathcal{L}_{\text {value }</p>
<p>where $\eta$ is the coefficient of the value objective.</p>
<p>4.3 Selector</p>
<p>The Selector is an LLM agent that takes two inputs: a scholar query and a research paper (including its title and abstract). It generates two outputs: (1) a single decision token $d$, either "True" or "False", indicating whether the paper satisfies the query, and (2) a rationale $r=\left(r_{1}, r_{2}, \ldots, r_{m}\right)$ containing $m$ tokens that support this decision. The rationale serves two purposes: enhancing decision accuracy by jointly training the model to generate decisions and explanations, and improving user trust by providing the reasoning in PaSa application.</p>
<p>To optimize training efficiency for the Crawler, the decision token is presented before the rationale, allowing the Selector to act as a single-token reward model during the Crawler training. Additionally, the token probability of the decision token can be used to rank search results. At last, as shown in Table 6, the order of the decision and rationale does not affect the Selector's performance.</p>
<p>We perform imitation learning to optimize the Selector. See Appendix E for training data collection and training details.</p>
<h2>5 Experiments</h2>
<h3>5.1 Experimental Setting</h3>
<p>We sequentially trained the Selector and Crawler, both based on the Qwen2.5-7b (Yang et al., 2024), to develop the final agent, referred to as PaSa-7b.</p>
<p>Selector The Selector was fine-tuned using the training dataset described in Appendix E. We conducted supervised fine-tuning for one epoch with a learning rate of $1 \mathrm{e}-5$ and a batch size of 4 . The training runs on 8 NVIDIA-H100 GPUs.</p>
<p>Crawler The training process involves two stages. First, we perform imitation learning for 1 epoch on 12,989 training data with a learning rate of $1 \mathrm{e}-5$ and batch size of 4 per device, using 8 NVIDIA H100 GPUs. In the second stage, we apply PPO training. To ensure stability, we first freeze the policy model and train the value model, followed by co-training both the policy and value models. The hyperparameters used during the training process are listed in Table 12 in Appendix D.2.</p>
<p>During imitation learning, the model encounters 5,000 queries, while during the RL training phase, the model processes a total of 16,000 queries. For more details please refer to Appendix D. 1 for the imitation learning data construction and Appendix D. 2 for the PPO training data sampling.</p>
<p>Implementation of [Search] The LLM predicts a query based on the context. Then the agent calls Google ${ }^{7}$ with the parameters site:arxiv.org and before:query_date, restricting search results by source and publication time.</p>
<p>Paper Management We developed a database to manage and restore research papers. PaSa retrieves paper information from the database. If no matching record is found, we use ar5iv ${ }^{8}$ to obtain the full paper content, including citations, and then parse this data and store it in the database.</p>
<h3>5.2 Baselines and Evaluation</h3>
<p>We evaluate our paper search agent on both the test set of AutoScholarQuery and RealScholarQuery. We compare PaSa-7b against the following baselines:</p>
<ul>
<li>Google. We use Google to search the query directly, with the same parameter settings in Section 5.1.</li>
<li>Google Scholar. Queries are submitted directly to Google Scholar ${ }^{7}$, with the same parameter settings in Section 5.1.</li>
<li>Google with GPT-4o. We first employ GPT40 to paraphrase the scholar query. The paraphrased query is then searched on Google.</li>
<li>ChatGPT. We submit scholar query to ChatGPT $^{9}$, powered by search-enabled GPT-4o.</li>
<li>GPT-o1. Prompt GPT-o1 to process the scholar query. Note that it does not have access to external search tools.</li>
<li>PaSa-GPT-4o. Implement PaSa as illustrated in Figure 1 by prompting GPT-4o. It can perform multiple searches, paper reading, and citation network crawling.</li>
</ul>
<p>We carefully designed prompts for all baselines and they are shown in Appendix H.2. All baselines, except PaSa-GPT-4o, represent the best-known scholar search methods. These comparisons highlight the effectiveness of our agentic approach. The comparison with PaSa-GPT-4o isolates the impact of RL training.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Crawler Recall</th>
<th>Precision</th>
<th>Recall</th>
<th>Recall@100</th>
<th>Recall@50</th>
<th>Recall@20</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.2015</td>
<td>0.1891</td>
<td>0.1568</td>
</tr>
<tr>
<td>Google Scholar</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.1130</td>
<td>0.0970</td>
<td>0.0609</td>
</tr>
<tr>
<td>Google with GPT-4o</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.2683</td>
<td>0.2450</td>
<td>0.1921</td>
</tr>
<tr>
<td>ChatGPT*</td>
<td>-</td>
<td>0.0507</td>
<td>0.3046</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-o1</td>
<td>-</td>
<td>0.0413</td>
<td>0.1925</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaSa-GPT-4o</td>
<td>0.7565</td>
<td>$\mathbf{0 . 1 4 5 7}$</td>
<td>0.3873</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaSa-7b</td>
<td>$\mathbf{0 . 7 9 3 1}$</td>
<td>$\mathbf{0 . 1 4 4 8}$</td>
<td>$\mathbf{0 . 4 8 3 4}$</td>
<td>$\mathbf{0 . 6 9 4 7}$</td>
<td>$\mathbf{0 . 6 3 3 4}$</td>
<td>$\mathbf{0 . 5 3 0 1}$</td>
</tr>
<tr>
<td>PaSa-7b-ensemble</td>
<td>$\mathbf{0 . 8 2 6 5}$</td>
<td>0.1410</td>
<td>$\mathbf{0 . 4 9 8 5}$</td>
<td>$\mathbf{0 . 7 0 9 9}$</td>
<td>$\mathbf{0 . 6 3 8 6}$</td>
<td>$\mathbf{0 . 5 3 2 6}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on AutoScholarQuery test set. *: Due to the need for manual query submission, the ChatGPT baseline is evaluated on 100 randomly sampled instances. Results for all methods on this subset are reported in Table 14.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Crawler Recall</th>
<th>Precision</th>
<th>Recall</th>
<th>Recall@100</th>
<th>Recall@50</th>
<th>Recall@20</th>
</tr>
</thead>
<tbody>
<tr>
<td>Google</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.2535</td>
<td>0.2342</td>
<td>0.1834</td>
</tr>
<tr>
<td>Google Scholar</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.2809</td>
<td>0.2155</td>
<td>0.1514</td>
</tr>
<tr>
<td>Google with GPT-4o</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.2946</td>
<td>0.2573</td>
<td>0.2020</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>-</td>
<td>0.2280</td>
<td>0.2007</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-o1</td>
<td>-</td>
<td>0.058</td>
<td>0.0134</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaSa-GPT-4o</td>
<td>0.5494</td>
<td>0.4721</td>
<td>0.3075</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaSa-7b</td>
<td>$\mathbf{0 . 7 0 7 1}$</td>
<td>$\mathbf{0 . 5 1 4 6}$</td>
<td>$\mathbf{0 . 6 1 1 1}$</td>
<td>$\mathbf{0 . 6 9 2 9}$</td>
<td>$\mathbf{0 . 6 5 6 3}$</td>
<td>$\mathbf{0 . 5 7 9 8}$</td>
</tr>
<tr>
<td>PaSa-7b-ensemble</td>
<td>$\mathbf{0 . 7 5 0 3}$</td>
<td>$\mathbf{0 . 4 9 3 8}$</td>
<td>$\mathbf{0 . 6 4 8 8}$</td>
<td>$\mathbf{0 . 7 2 8 1}$</td>
<td>$\mathbf{0 . 6 8 7 7}$</td>
<td>$\mathbf{0 . 5 9 8 6}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results on RealScholarQuery.</p>
<p>As shown in Figure 2, the crawling process of PaSa can be visualized as a paper tree. In practice, considering the computational expense, we limit the Crawler's exploration depth to three for both PaSa-7b and PaSa-GPT-4o.</p>
<p>For Google-based baselines, we evaluate recall using Recall@20, Recall@50, and Recall@100 metrics for the top-20, top-50, and top-100 search results, respectively. For other baselines that do not produce rankings, we assess precision and recall for the final retrieved papers. Additionally, we compare Crawler recall between PaSa-GPT-4o and PaSa-7b, defined as the proportion of target papers collected by the Crawler. This measures how many target papers are successfully included in the paper queue generated by the Crawler.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o</td>
<td>0.96</td>
<td>0.69</td>
<td>0.80</td>
</tr>
<tr>
<td>Qwen-2.5-7b</td>
<td>1.0</td>
<td>0.38</td>
<td>0.55</td>
</tr>
<tr>
<td>PaSa-7b-Selector</td>
<td>0.95</td>
<td>0.78</td>
<td>$\mathbf{0 . 8 5}$</td>
</tr>
<tr>
<td>PaSa-7b-Selector (Reason First)</td>
<td>0.94</td>
<td>0.76</td>
<td>$\mathbf{0 . 8 4}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Selector Evaluation.</p>
<h3>5.3 Main results</h3>
<p>As shown in Table 4, PaSa-7b outperforms all baselines on AutoScholarQuery test set. Specifically, compared to the strongest baseline, PaSa-GPT-4o,</p>
<p>PaSa-7b demonstrates a 9.64\% improvement in recall with comparable precision. Moreover, the recall of the Crawler in PaSa-7b is 3.66\% higher than that in PaSa-GPT-4o. When compared to the best Google-based baseline, Google with GPT-4o, PaSa7b achieves an improvement of $33.80 \%, 38.83 \%$ and $42.64 \%$ in Recall@20, Recall@50 and Recall@100, respectively.</p>
<p>We observe that using multiple ensembles of Crawler during inference can improve performance. Specifically, we use sampling decoding to run Crawler twice in the PaSa-7b-ensemble setting, which boosts Crawler recall by $3.34 \%$ on AutoScholarQuery and increases the final recall by $1.51 \%$, with no significant change in precision.</p>
<p>To evaluate PaSa in a more realistic setting, we assess its effectiveness on RealScholarQuery. As illustrated in Table 5, PaSa-7b exhibits a greater advantage in real-world academic search scenarios. Compared to PaSa-GPT-4o, PaSa-7b achieves improvements of $30.36 \%$ in recall and $4.25 \%$ in precision. Against the best Google-based baseline on RealScholarQuery, Google with GPT-4o, PaSa7b outperforms Google by $37.78 \%, 39.90 \%$, and $39.83 \%$ in recall@20, recall@50 and recall@100, respectively. Additionally, the PaSa-7b-ensemble further enhances crawler recall by $4.32 \%$, contribut-</p>
<p>| Method | AutoScholarQuery | | | RealScholarQuery | | |
| | Crawler Recall | Precision | Recall | Crawler Recall | Precision | Recall |
| --- | --- | --- | --- | --- | --- | --- |
| w/o [Expand] | 0.3355 | 0.1445 | 0.2536 | 0.3359 | $\mathbf{0 . 6 7 3 8}$ | 0.2890 |
| w/o RL training | 0.6556 | 0.1476 | 0.4210 | 0.4847 | 0.5155 | 0.4115 |
| w/o Selector as RM | 0.7041 | $\mathbf{0 . 1 5 3 5}$ | 0.4458 | 0.5994 | 0.5489 | 0.5148 |
| PaSa-7b | $\mathbf{0 . 7 9 3 1}$ | 0.1448 | $\mathbf{0 . 4 8 3 4}$ | $\mathbf{0 . 7 0 7 1}$ | 0.5146 | $\mathbf{0 . 6 1 1 1}$ |</p>
<p>Table 7: Ablation study results on AutoScholarQuery test set and RealScholarQuery.
ing to an overall $3.52 \%$ improvement in the recall of the entire agent system.</p>
<p>As both the final decision-maker and auxiliary reward model in RL training for the Crawler, the performance of the Selector is crucial. To evaluate its effectiveness, we collected a dataset of 200 query-paper pairs, annotating whether each paper meets the query's requirements. This dataset serves as the benchmark for evaluating the Selector (see Appendix F for details). We then compared our Selector against GPT-4o (Hurst et al., 2024) and Qwen-2.5-7b (Yang et al., 2024), as shown in Table 6. The results show that our Selector achieves an F1 score of $85 \%$, outperforming GPT-4o by $5 \%$ and Qwen-2.5-7b by $30 \%$. Additionally, when compared to a setting where reasoning precedes decision token generation, the performance is comparable. Lastly, the Selector's precision reaches $95 \%$, confirming its effectiveness as an auxiliary reward model for the Crawler RL training.</p>
<h3>5.4 Ablation study</h3>
<p>We perform ablation studies in Table 7 to evaluate the individual contributions of exploring citation networks, RL training, and using the Selector as the reward model. The results indicate that removing the [Expand] action from the Crawler leads to a significant drop in the recall: a decrease of $22.98 \%$ on AutoScholarQuery and $32.21 \%$ on RealScholarQuery. Furthermore, RL training enhances recall by $6.24 \%$ on AutoScholarQuery and $19.96 \%$ on RealScholarQuery. The RL training curves are depicted in Figure 3 in Appendix D.2, where the training curves show a steady increase in return with the training steps, eventually converging after 200 steps. Finally, removing the Selector as an auxiliary reward model results in a $3.76 \%$ recall drop on AutoScholarQuery and a $9.63 \%$ drop on RealScholarQuery.</p>
<p>We investigate how to control agent behavior by adjusting the rewards in RL training. Experiments are conducted with varying reward coefficients $\alpha$ in Equation 1, and results are presented in Table 8. We
report two metrics: crawler recall and crawler action. The crawler action refers to the total number of [Search] and [Expand] actions throughout the Crawler's entire trajectory. As the reward increases, both crawler recall and crawler action increase, suggesting that adjusting rewards in RL training can effectively influence PaSa's behavior.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\alpha$</th>
<th style="text-align: center;">Crawler <br> Recall</th>
<th style="text-align: center;">Crawler <br> Actions</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0.5</td>
<td style="text-align: center;">0.7227</td>
<td style="text-align: center;">175.9</td>
<td style="text-align: center;">0.1458</td>
<td style="text-align: center;">0.4602</td>
</tr>
<tr>
<td style="text-align: left;">1.0</td>
<td style="text-align: center;">0.7708</td>
<td style="text-align: center;">319.8</td>
<td style="text-align: center;">0.1451</td>
<td style="text-align: center;">0.4792</td>
</tr>
<tr>
<td style="text-align: left;">1.5</td>
<td style="text-align: center;">0.7931</td>
<td style="text-align: center;">382.4</td>
<td style="text-align: center;">0.1448</td>
<td style="text-align: center;">0.4834</td>
</tr>
<tr>
<td style="text-align: left;">2.0</td>
<td style="text-align: center;">0.8063</td>
<td style="text-align: center;">785.5</td>
<td style="text-align: center;">0.1409</td>
<td style="text-align: center;">0.4869</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance of the Crawler trained on different reward coefficient $\alpha$ on AutoScholarQuery test set.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we introduce PaSa, a novel paper search agent designed to provide comprehensive and accurate results for complex academic queries. PaSa is implemented within the AGILE, a reinforcement learning framework for LLM agents. To train PaSa, we developed AutoScholarQuery, a dataset of fine-grained academic queries and corresponding papers drawn from top-tier AI conference publications. To evaluate PaSa in real-world scenarios, we also constructed RealScholarQuery, a dataset of actual academic queries paired with annotated papers. Our experimental results demonstrate that PaSa outperforms all baselines, including Google, Google Scholar, and Google with GPT-4o, ChatGPT, GPT-o1, and PaSa-GPT-4o. In particular, PaSa-7B surpasses Google with GPT-4o by $37.78 \%$ in recall@20 and $39.90 \%$ in recall@50, while also exceeding PaSa-GPT-4o by $30.36 \%$ in recall and $4.25 \%$ in precision. These findings underscore that PaSa significantly improves the efficiency and accuracy of academic search.</p>
<h2>Limitations</h2>
<p>(1) Our dataset collection and experiments were primarily focused on the field of machine learning. Although our proposed method is general, we did not explore its performance in other scientific fields. We leave to investigate its applicability to other domains in future work.
(2) Due to resource constraints, our experiments primarily use LLMs with 7b parameters. We expect that scaling up to larger models will lead to more powerful agents. Expanding PaSa to leverage larger LLMs is our future work.</p>
<h2>Acknowledgments</h2>
<p>The authors thank Yaohua Fang, Zheng Li, Qiang Lu, Yelong Shi, Xuguang Wei, and Tingshuai Yan from ByteDance for their support in developing the PaSa demo. We also thank Jianghui Xie at ByteDance for her assistance with the release of the PaSa demo. Finally, we thank the anonymous reviewers for their valuable suggestions that helped improve this work.</p>
<h2>References</h2>
<p>Shubham Agarwal, Issam H Laradji, Laurent Charlin, and Christopher Pal. 2024. Litllm: A toolkit for scientific literature review. arXiv preprint arXiv:2402.01788.</p>
<p>Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul Thomas. 2023. Can generative llms create query variants for test collections? an exploratory study. In Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval, pages 18691873.</p>
<p>A Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku; 2024. URL https://wwwcdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7 bbc618857627/Model_Card_Claude_3.pdf.</p>
<p>Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738.</p>
<p>Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. 2023. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376.</p>
<p>Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin Shi. 2023. Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288.</p>
<p>Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36.</p>
<p>Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, and Hang Li. 2024. Agile: A novel reinforcement learning framework of llm agents. Advances in Neural Information Processing Systems, 37:5244-5284.</p>
<p>Team Gemini. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.</p>
<p>Karan Girotra, Lennart Meincke, Christian Terwiesch, and Karl T Ulrich. 2023. Ideas are dimes a dozen: Large language models for idea generation in innovation. Available at SSRN 4526071.</p>
<p>Atharva Gundawar, Mudit Verma, Lin Guan, Karthik Valmeekam, Siddhant Bhambri, and Subbarao Kambhampati. 2024. Robust planning with llm-modulo framework: Case study in travel planning. arXiv preprint arXiv:2405.20625.</p>
<p>Michael Gusenbauer and Neal R Haddaway. 2020. Which academic search systems are suitable for systematic reviews or meta-analyses? evaluating retrieval qualities of google scholar, pubmed, and 26 other resources. Research synthesis methods, 11(2):181-217.</p>
<p>Michael Gusenbauer and Neal R Haddaway. 2021. What every researcher should know about searchingclarified concepts, search advice, and an agenda to improve finding in academia. Research synthesis methods, 12(2):136-147.</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276.</p>
<p>Karl Kingsley, Gillian M Galbraith, Matthew Herring, Eva Stowers, Tanis Stewart, and Karla V Kingsley. 2011. Why not just google it? an assessment of information literacy skills in a biomedical science curriculum. BMC medical education, 11:1-8.</p>
<p>Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, and Michael Bendersky. 2023. Generate, filter, and fuse: Query expansion via multi-step keyword generation for zero-shot neural rankers. arXiv preprint arXiv:2311.09175.</p>
<p>Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen Cheng, Ai-Heng Lee, Kyle Lo, Joseph Chee Chang, and Amy X Zhang. 2024. Llms as research tools: A large scale survey of researchers' usage and perceptions. arXiv preprint arXiv:2411.05025.</p>
<p>Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang. 2023. Reason for future, act for now: A principled framework for autonomous llm agents with provable sample efficiency. arXiv preprint arXiv:2309.17382.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292.</p>
<p>Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. 2024. Augmenting large language models with chemistry tools. Nature Machine Intelligence, pages 1-11.</p>
<p>Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting for retrievalaugmented large language models. arXiv preprint arXiv:2305.14283.</p>
<p>Lisa Messeri and MJ Crockett. 2024. Artificial intelligence and illusions of understanding in scientific research. Nature, 627(8002):49-58.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.</p>
<p>Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. 2024. Large language model based long-tail query rewriting in taobao search. In Companion Proceedings of the ACM on Web Conference 2024, pages 20-28.</p>
<p>Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. 2024. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. arXiv preprint arXiv:2307.07924.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in writing Wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6252-6278, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36.</p>
<p>Jess Stratton. 2024. An introduction to microsoft copilot. In Copilot for Microsoft 365: Harness the Power of Generative AI in the Microsoft Apps You Use Every Day, pages 19-35. Springer.</p>
<p>Richard Van Noorden and Jeffrey M Perkel. 2023. Ai and science: what 1,600 researchers think. Nature, 621(7980):672-675.</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2024a. SciMON: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279-299, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, et al. 2024b. Autosurvey: Large language models can automatically write surveys. arXiv preprint arXiv:2406.10252.</p>
<p>Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1-10.</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: synergizing reasoning and acting in language models (2022). arXiv preprint arXiv:2210.03629.</p>
<h2>A Quality Evaluation of AutoScholarQuery</h2>
<p>To assess the quality of AutoScholarQuery, we sampled 100 query-paper pairs and evaluated the rationality and relevance of each query and its corresponding paper. The detailed evaluation criteria are as follows:</p>
<ul>
<li>A qualified query should be a complete and understandable sentence. For example, incomplete or fragmented sentences are not acceptable.</li>
<li>A query that misrepresents the meaning of the source paper, leading to irrelevant citations, is not qualified. This includes queries that</li>
</ul>
<p>Query: Give me papers about how to rank search results by the use of LLM
Query Date: 2024-10-01</p>
<h1>Answer Papers:</h1>
<p>[0] Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers (2311.01555)
[1] Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels (2310.14122)
[2] Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting (2306.17563)
[3] A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models (2310.09497)
[4] RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models (2309.15088)
[5] PaRaDe: Passage Ranking using Demonstrations with Large Language Models (2310.14408)
[6] Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents (2304.09542)
[7] Large Language Models are Zero-Shot Rankers for Recommender Systems (2305.08845)
[8] TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy (2406.11678)
[9] ExaRanker: Explanation-Augmented Neural Ranker (2301.10521)
[10] RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs (2407.02485)
[11] Make Large Language Model a Better Ranker (2403.19181)
[12] LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking (2406.00231)
[13] Improving Zero-shot LLM Re-Ranker with Risk Minimization (2406.13331)
[14] Zero-Shot Listwise Document Reranking with a Large Language Model (2305.02156)
[15] Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing (2404.11791)
[16] Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models (2406.18740)
[17] Large Language Models for Relevance Judgment in Product Search (2406.00247)
[18] PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval (2404.18424)
[19] Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models (2405.20654)
[20] When Search Engine Services meet Large Language Models: Visions and Challenges (2407.00128)
[21] RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze! (2312.02724)
[22] Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models (2312.02969)
[23] MuGI: Enhancing Information Retrieval through Multi-Text Generation Integration with Large Language Models (2401.06311)
[24] Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker (2305.13729)
[25] REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering (2402.17497)
[26] Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM (2312.15450)
[27] FIRST: Faster Improved Listwise Reranking with Single Token Decoding (2406.15657)
[28] Leveraging LLMs for Unsupervised Dense Retriever Ranking (2402.04853)
[29] Unsupervised Contrast-Consistent Ranking with Language Models (2309.06991)
[30] Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models (2403.18093)
[31] Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models (2310.07712)
[32] Fine-Tuning LLaMA for Multi-Stage Text Retrieval (2310.08319)
[33] Zero-shot Audio Topic Reranking using Large Language Models (2309.07606)
[34] Uncovering ChatGPT's Capabilities in Recommender Systems (2305.02182)
[35] Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism (2402.10548)
[36] Towards More Relevant Product Search Ranking Via Large Language Models: An Empirical Study (2409.17460)
[37] Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction (2306.01599)
[38] Open-source large language models are strong zero-shot query likelihood models for document ranking (2310.13243)
Table 9: Examples of queries and corresponding papers in RealScholarQuery.
exaggerate the scope or introduce incorrect conditions.</p>
<ul>
<li>A query is ambiguous if there is insufficient context and additional information is needed. For instance, abbreviations with multiple meanings can create ambiguity, leading to the corresponding citations being incomplete answer lists.</li>
<li>An answer paper is considered qualified if it aligns with the requirements of the query. The paper should address all or most of the essential factors that make it a suitable response.</li>
</ul>
<p>Our quality check found that $94.0 \%$ of the
queries were qualified. Among them, $93.7 \%$ of the corresponding answer papers were also qualified. The primary reason for unqualified papers was inaccurate citations in the source paper.</p>
<h2>B Annotation details</h2>
<p>The annotators of RealScholarQuery include professors from the Department of Computer Science at a top-tier university in China. They are paid $\$ 4$ per data entry, which consists of a user query and a research paper. Their task is to determine whether the paper satisfies the query.</p>
<table>
<thead>
<tr>
<th>The prompt for search query generation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>You are an elite researcher in the field of AI, please generate some mutually exclusive queries in a list to search the relevant</td>
<td></td>
</tr>
<tr>
<td>papers according to the User Query. Searching for a survey paper would be better.</td>
<td></td>
</tr>
<tr>
<td user_query="user_query">User Query:</td>
<td></td>
</tr>
<tr>
<td>The semantics between generated queries are not mutually inclusive. The format of the list is: ["query1", "query2", ...]</td>
<td></td>
</tr>
<tr>
<td>Queries:</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 10: The prompt for GPT-4o to generate search queries from the user query.</p>
<table>
<thead>
<tr>
<th></th>
<th>Search Session starting from $S_{q}$</th>
<th>Expand Session starting from $S_{q+p}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>prompt</td>
<td>Please, generate some mutually exclusive queries</td>
<td>You are conducting research on ‘{user_query}’. You need to predict</td>
</tr>
<tr>
<td></td>
<td>in a list to search the relevant papers according</td>
<td>which sections to look at to get more relevant papers.</td>
</tr>
<tr>
<td></td>
<td>to the User Query. Searching for survey papers</td>
<td title="title">Title:</td>
</tr>
<tr>
<td></td>
<td>would be better.</td>
<td abstract="abstract">Abstract:</td>
</tr>
<tr>
<td></td>
<td user_query="user_query">User Query:</td>
<td sections="sections">Sections:</td>
</tr>
<tr>
<td>response</td>
<td 1="1" query="query">[Search]</td>
<td 1="1" section="section">[Expand]</td>
</tr>
<tr>
<td></td>
<td 2="2" query="query">[Search]</td>
<td 2="2" section="section">[Expand]</td>
</tr>
<tr>
<td></td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<td></td>
<td>[Stop]</td>
<td>[Stop]</td>
</tr>
</tbody>
</table>
<p>Table 11: The session trajectory templates of the Crawler.</p>
<h3>B.1 Annotation Instructions</h3>
<p>For each <user query, paper> pair, carefully assess whether the paper address the user query. Write your decision and provide a brief explanation (1-2 sentences). Specific guidelines are as follows:</p>
<ul>
<li>You may read the entire paper to determine whether it satisfies the user query.</li>
<li>Exclude survey papers unless the user query specifically requests them.</li>
<li>All conditions in the user query must be met for the paper to be considered qualified.</li>
</ul>
<h3>B.2 Quality control</h3>
<p>The annotation process follows the following quality control measures:</p>
<ul>
<li>Stage 1: Annotators work in batches of 20. Authors review 100% of the annotations. Once the consistency rate on the first pass reaches 90%, the process moves to Stage 2.</li>
<li>Stage 2: Annotators work in batches of 50. Authors randomly check 40% of the annotations. If the consistency rate is below 90%, the entire batch is re-annotated and re-checked. Once the batch meets the 90% consistency rate on the first pass, the process moves to Stage 3.</li>
<li>Stage 3: Annotators work in batches of 100. Authors randomly check 20% of the annotations. If the consistency rate is below 90%, the entire batch is re-annotated and re-checked.</li>
</ul>
<p>Two authors conducted the quality control.</p>
<h2>C Example in RealScholarQuery</h2>
<p>Table 9 presents an example query and corresponding papers from RealScholarQuery.</p>
<h2>D Implementation Details of the Crawler</h2>
<h3>D.1 Imitation learning data generation</h3>
<p>We generate training data for imitation learning on a session-by-session basis. There are two types of sessions: search session (starting from state $S_{q}$ ) and expand session (starting from state $S_{q+p}$ ).</p>
<p>For search sessions starting from $S_{q}$, we sample user queries from the AutoScholarQuery training set and prompt GPT-4o to generate corresponding search queries. The prompt template is shown in Table 10. The session trajectory is constructed by adding a [Search] token before each query, concatenating the queries, and appending a [Stop] token at the end, as shown in Table 11. A total of 3,011 search session trajectories are generated.</p>
<p>For expanded sessions starting from $S_{q+p}$, we continue by searching for the generated queries using Google. We then sample papers from the search results and obtain the initial state, which includes</p>
<p>both the query and a paper. To build the session trajectory, we examine each sub-section of the paper. If the sub-section references at least one paper in the AutoScholarQuery training set corresponding to the query, the sub-section is selected. Otherwise, the sub-section is selected with a 10% probability to enhance trajectory diversity. The selected sections are filled into the template in Table 11, completing the session trajectory. In total, 9,978 expanded session trajectories are constructed.</p>
<h3>D.2 PPO training</h3>
<p>During PPO training, each device processes 4 user queries in each step, generating a search session for each user query. Then, 6 expansion sessions are created by randomly sampling 6 papers from the search results. This process is repeated with the expanded citation results, yielding 6 additional expanded sessions. In total, 16 session trajectories are generated per step.</p>
<table>
<thead>
<tr>
<th></th>
<th>Name</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(Equation 1)</td>
<td>1.5</td>
</tr>
<tr>
<td>$c$([Search])</td>
<td>(Equation 1)</td>
<td>0.1</td>
</tr>
<tr>
<td>$c$([Expand])</td>
<td>(Equation 1)</td>
<td>0.1</td>
</tr>
<tr>
<td>$c$([Stop])</td>
<td>(Equation 1)</td>
<td>0.0</td>
</tr>
<tr>
<td>$\gamma_{0}$</td>
<td>(Equation 3)</td>
<td>1.0</td>
</tr>
<tr>
<td>$\gamma_{1}$</td>
<td>(Equation 3)</td>
<td>0.1</td>
</tr>
<tr>
<td>$\beta$</td>
<td>(Equation 3)</td>
<td>0.1</td>
</tr>
<tr>
<td>$\epsilon$</td>
<td>(Equation 5, Equation 6)</td>
<td>0.2</td>
</tr>
<tr>
<td>$\eta$</td>
<td>(Equation 8)</td>
<td>10</td>
</tr>
<tr>
<td>learning rate</td>
<td></td>
<td>1e-6</td>
</tr>
<tr>
<td>epoch per step</td>
<td></td>
<td>2</td>
</tr>
<tr>
<td>forward batch size</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>accumulate batch size</td>
<td></td>
<td>16</td>
</tr>
<tr>
<td>NVIDIA H100 GPU</td>
<td></td>
<td>16</td>
</tr>
<tr>
<td>policy freezing step</td>
<td></td>
<td>50</td>
</tr>
<tr>
<td>total step</td>
<td></td>
<td>250</td>
</tr>
</tbody>
</table>
<p>Table 12: The hyperparameters used in PPO training.</p>
<p>Table 12 lists the hyperparameters used during the training process. Figure 3 depicts the RL training curves, which show a steady increase in return with the training steps, eventually converging after 200 steps.</p>
<h2>Appendix E Implementation Details of the Selector</h2>
<p>We begin by sampling user queries from the AutoScholarQuery training set. For each user query and one of its corresponding papers in the AutoScholarQuery training set, we prompt GPT-4o</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Return and value function loss curves during the PPO training process. The smoothing method of the curve in the figures is the exponential moving average(EMA) formula that aligns with the one used in TensorBoard, and the smoothing weight is set to 0.95.</p>
<p>to generate a decision token and rationale (see Table 13 for the prompt). We reject any data where the decision token is "False", as this contradicts the AutoScholarQuery label. The remaining data are retained as positive <user query, paper> pairs.</p>
<p>Next, we simulate a partial paper search using PaSa-GPT-4o. In this simulation, each paper has a 50% probability of being added to the paper queue. Pairs where the paper is not selected by GPT-4o and is not in the AutoScholarQuery training set are labeled as negative examples.</p>
<p>The final training dataset consists of 19,812 <user query, paper> pairs, each with a decision token and rationale generated by GPT-4o, drawn from 9,000 instances in the AutoScholarQuery training set.</p>
<h2>Appendix F Selector Test Dataset</h2>
<p>We select 200 queries from the AutoScholarQuery development set. For each query, we perform a Google search and randomly choose one paper from the union of the search results and the relevant paper set in AutoScholarQuery. This yields a set of <user query, paper> pairs. Annotators then evaluate whether each paper aligns with the requirements of the user query. The final test dataset consists of 98 positive samples and 102 negative samples.</p>
<p>The prompt for paper selection
You are an elite researcher in the field of AI, conducting research on {user_query}. Evaluate whether the following paper fully satisfies the detailed requirements of the user query and provide your reasoning. Ensure that your decision and reasoning are consistent.
Searched Paper:
Title: {title}
Abstract: {abstract}
User Query: {user_query}
Output format: Decision: True/False
Reason:...
Decision:</p>
<p>Table 13: Prompt used by PaSa Selector or GPT-4o to evaluate paper relevance.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Crawler Recall</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">Recall@100</th>
<th style="text-align: center;">Recall@50</th>
<th style="text-align: center;">Recall@20</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Google</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.2101</td>
<td style="text-align: center;">0.2010</td>
<td style="text-align: center;">0.1788</td>
</tr>
<tr>
<td style="text-align: left;">Google Scholar</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0801</td>
<td style="text-align: center;">0.0739</td>
<td style="text-align: center;">0.0612</td>
</tr>
<tr>
<td style="text-align: left;">Google with GPT-4o</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.2101</td>
<td style="text-align: center;">0.2010</td>
<td style="text-align: center;">0.1788</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0507</td>
<td style="text-align: center;">0.3046</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-o1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0374</td>
<td style="text-align: center;">0.2006</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PaSa-GPT-4o</td>
<td style="text-align: center;">0.7595</td>
<td style="text-align: center;">0.1817</td>
<td style="text-align: center;">0.4488</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PaSa-7b</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 5 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 8 8 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 2 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 5 4 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 9 4}$</td>
</tr>
<tr>
<td style="text-align: left;">PaSa-7b-ensemble</td>
<td style="text-align: center;">$\mathbf{0 . 8 2 4 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 8 2 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 6 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 4 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 7 9 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 3 5}$</td>
</tr>
</tbody>
</table>
<p>Table 14: Results on 100-sample subset of AutoScholarQuery test.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$c\left(a_{t}\right)$</th>
<th style="text-align: center;">Crawler <br> Recall</th>
<th style="text-align: center;">Crawler <br> Actions</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.8239</td>
<td style="text-align: center;">1296.3</td>
<td style="text-align: center;">0.1388</td>
<td style="text-align: center;">0.4852</td>
</tr>
<tr>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.7931</td>
<td style="text-align: center;">382.4</td>
<td style="text-align: center;">0.1448</td>
<td style="text-align: center;">0.4834</td>
</tr>
<tr>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.7478</td>
<td style="text-align: center;">230.1</td>
<td style="text-align: center;">0.1489</td>
<td style="text-align: center;">0.4764</td>
</tr>
</tbody>
</table>
<p>Table 15: Performance of the Crawler trained on different action cost $c\left(a_{t}\right)$ on AutoScholarQuery test set.</p>
<h2>G Additional Experimental Results</h2>
<h2>G. 1 Results on 100-sample subset of AutoScholarQuery</h2>
<p>To ensure a fair comparison with the ChatGPT baseline, which is evaluated on only 100 samples from AutoScholarQuery test, we report the performance of all methods on the same subset in Table 14. The results align with those in Table 4, confirming that PaSa-7b consistently outperforms all baselines.</p>
<h2>G. 2 Action cost</h2>
<p>We incorporate action costs to prevent the agent from taking excessive, unproductive actions. Without such costs, the total number of actions would increase significantly without yielding meaningful outcomes.</p>
<p>The key consideration is the reward coefficient $\alpha$ and the action cost $c\left(a_{t}\right)$ in Equation 1. In Table 8 ,
we fix $c\left(a_{t}\right)$ and analyze how varying $\alpha$ affects performance.</p>
<p>Additionally, Table 15 shows how different values of $c\left(a_{t}\right)$ affect the final performance.</p>
<h2>H Prompt Templates</h2>
<h2>H. 1 Prompts used for data synthesis in AutoScholarQuery</h2>
<p>Table 16 presents the prompt template used with GPT-4o to automatically generate AutoScholarQuery. For each paper, we extract its Related Work section, input it into GPT-4o, and use the prompt to generate scholarly queries along with their corresponding paper answers.</p>
<h2>H. 2 Prompts for baselines</h2>
<p>Table 17 presents the search query paraphrasing prompt used for the baseline Google with GPT-4o.</p>
<p>Table 18, 19 and 20 show the prompts used for the ChatGPT baseline (search-enabled GPT-4o), the GPT-o1 baseline and PaSa-GPT-4o, respectively.</p>
<h1>The prompt for AutoScholarQuery generation</h1>
<p>You are provided a 'Related Work' section of a research paper. The researcher reviewed the relevant work, conducted a literature survey, and cited corresponding references in this text (enclosed by "cite" tags with IDs). Can you guess what research questions the researcher might have posed when preparing this text? The answers to these questions should be the references cited in this passage. Please list questions and provide the corresponding answers.
[Requirements:]</p>
<ol>
<li>Craft questions similar to those a researcher would pose when reviewing related works, such as "Which paper studied ...?", "Any works about...?", "Could you provide me some works...?"</li>
<li>Construct the question-answer pairs based on [Section from A Research Paper]. The answer should be the cited papers in [Section from A Research Paper].</li>
<li>Do not ask questions including "or" or "and" that may involve more than one condition.</li>
<li>Clarity: Formulate questions clearly and unambiguously to prevent confusion.</li>
<li>Contextual Definitions: Include explanations or definitions for specialized terms and concepts used in the questions.</li>
<li>Format the output as a JSON array containing five objects corresponding to the three question-answer pairs.</li>
</ol>
<p>Here are some examples:
[Begin of examples]
[Section from A Research Paper-1}
[OUTPUT-1]
[Section from A Research Paper-2}
[OUTPUT-2]
[Section from A Research Paper-3}
[OUTPUT-3]
[End of examples]
[Section from A Research Paper]
[OUTPUT]:
Table 16: The prompt used with GPT-4o to automatically synthesize AutoScholarQuery.</p>
<h2>The prompt for search query paraphrase</h2>
<p>Generate a search query suitable for Google based on the given academic paper-related query. Here's the structure and requirements for generating the search query:
Understand the Query: Read and understand the given specific academic query.
Identify Key Elements: Extract the main research field and the specific approaches or topics mentioned in the query.
Formulate the Search Query: Combine these elements into a concise query that includes terms indicating academic sources.
Do not add any site limitations to your query.</p>
<p>[Generated Search Query]:
Table 17: The search query paraphrasing prompt used for the Google with GPT-4o baseline.</p>
<h2>The prompt for ChatGPT (search-enabled GPT-4o)</h2>
<p><a href="\{user_query\}">User's Query</a>
You should return the Arxiv papers. You should provide more than 10 papers you searched in JSON format:
['paper_1': {"title": , 'authors': , 'link': }, "paper_2": {"title": , 'authors': , 'link': }}</p>
<p>Table 18: The prompt for ChatGPT baseline (search-enabled GPT-4o).</p>
<h2>The prompt for GPT-o1</h2>
<p>{user_query $}$
You should return arxiv papers. You should provide more than 10 paper you searched in JSON format: {"paper_1": {"title": , "authors": , "link": }, "paper_2": {"title": , "authors": , "link": }}. Do not return any other description.</p>
<p>Table 19: The prompt for GPT-o1 baseline.</p>
<h1>The prompt for search session of Crawler</h1>
<p>You are an elite researcher in the field of AI, please generate some mutually exclusive queries in a list to search the relevant papers according to the User Query. Searching for a survey paper would be better.
User Query: {user_query}
The semantics between generated queries are not mutually inclusive. The format of the list is: ["query1", "query2", ...] Queries:</p>
<h2>The prompt for the expand session of Crawler</h2>
<p>You are an elite researcher in the field of AI, currently conducting research on the [topic] specified below. Your task is to determine if the paper specified below likely contains highly relevant citations for the [topic] and, if so, to identify specific sections where these citations might appear.
Task Instructions:</p>
<ol>
<li>Relevance Assessment: Decide if the paper is likely to include citations highly relevant to the given [topic]. Output "Yes" or "No" on the first line.</li>
<li>Section Selection: If you answered "Yes" in step 1, identify which sections of the paper are likely to contain these relevant citations. From the list of provided sections, select only those you think may contain relevant citations. If no sections seem relevant even if your answer to step 1 was "Yes," leave this empty. Output the selected sections in JSON format on the second line.
[topic]: {user_query }
Output Format: Output exactly two lines:</li>
<li>The first line: Either "Yes" or "No" based on the relevance assessment.</li>
<li>The second line: A JSON string with selected sections, e.g., {"selected_section_1": section_name_1, "selected_section_2": section_name_2}}. If no sections are selected, output ${}}$.</li>
</ol>
<h2>The prompt for Selector</h2>
<p>You are an elite researcher in the field of AI, conducting research on {user_query}. Evaluate whether the following paper fully satisfies the detailed requirements of the user query and provide your reasoning. Ensure that your decision and reasoning are consistent.
Searched Paper:
Title: {title}
Abstract: {abstract}
User Query: {user_query}
Output format: Decision: True/False
Reason:...
Decision:</p>
<p>Table 20: The prompts for PaSa-GPT-4o.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Accessed via the Google Search API provided by https: //serper.dev.
${ }^{8}$ https://ar5iv.org/
${ }^{9}$ https://chatgpt.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
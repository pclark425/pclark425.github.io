<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1147 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1147</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1147</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-233714799</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2105.01606v1.pdf" target="_blank">Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments</a></p>
                <p><strong>Paper Abstract:</strong> Performing autonomous exploration is essential for unmanned aerial vehicles (UAVs) operating in unknown environments. Often, these missions start with building a map for the environment via pure exploration and subsequently using (i.e. exploiting) the generated map for downstream navigation tasks. Accomplishing these navigation tasks in two separate steps is not always possible or even disadvantageous for UAVs deployed in outdoor and dynamically changing environments. Current exploration approaches either use a priori human-generated maps or use heuristics such as frontier-based exploration. Other approaches use learning but focus only on learning policies for specific tasks by either using sample inefficient random exploration or by making impractical assumptions about full map availability. In this paper, we develop an adaptive exploration approach to trade off between exploration and exploitation in one single step for UAVs searching for areas of interest (AoIs) in unknown environments using Deep Reinforcement Learning (DRL). The proposed approach uses a map segmentation technique to decompose the environment map into smaller, tractable maps. Then, a simple information gain function is repeatedly computed to determine the best target region to search during each iteration of the process. DDQN and A2C algorithms are extended with a stack of LSTM layers and trained to generate optimal policies for the exploration and exploitation, respectively. We tested our approach in 3 different tasks against 4 baselines. The results demonstrate that our proposed approach is capable of navigating through randomly generated environments and covering more AoI in less time steps compared to the baselines.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1147.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1147.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>πAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Exploration Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-branched DRL system that adaptively trades off exploration and exploitation for UAV search in unknown, partially observable environments by selecting regions with maximal information gain and switching between long-range navigation and local AoI coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>πAE (Adaptive Exploration Policy)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical agent composed of two learned sub-policies: Target-Directed Exploration (πn, Recurrent-DDQN) for region-to-region navigation and AoI Exploitation (πr, Recurrent-A2C) for dense local coverage; both networks include stacked LSTM layers, use egocentric (mt) and allocentric (M) maps plus a visited-vector Vt, and are coordinated by an information-gain objective to pick target regions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information-gain maximization combined with DRL (active/adaptive exploration); uses map segmentation and curiosity-like local exploitation (A2C) for adaptive sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision epoch the agent computes an information-gain score T = argmax_i (ω1·α_i + ω2·β_i + ω3·ζ_i) over map regions (α: coverage, β: AoI probability, ζ: distance) and adaptively selects the next target region; while navigating it biases trajectories to increase AoI coverage, and once inside a region it switches to a local exploitation policy after κ1/κ2 thresholds. It updates allocentric map M and egocentric mt continuously from on-board observations and the visited-vector Vt to influence future region selection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Simulated AoI maps (180×180 grid), Planet satellite regions for illegal mining (5 regions, 8×8 km^2), and a real 50×50 m^2 field test</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (POMDP) due to limited onboard camera FOV; unknown prior map; discrete grid-like spatial decomposition; sparse AoI rewards; non-episodic/long episodes (no fixed terminal state); stochastic/noisy observations (simulator supports image noise)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Large discrete spatial map (180×180 cells for simulations); action space size = 5 (forward, back, left, right, hover); episode/time budget up to 18,000 timesteps in simulations; region decomposition hyperparameter α yields smaller region maps (eg. mt of size 25×25 or similar flattened inputs); real-world field 50×50 m with FOV ≈10×10 m.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Simulated AoI detection: AoI coverage = 0.81385 ± 0.0674 (fraction of AoI area covered) with total map coverage = 0.3051 ± 0.0396 after 18,000 steps. Illegal mining detection (Planet data): AoI coverage = 0.79779 ± 0.06462, total coverage = 0.2038 ± 0.1394 (averaged over 100 episodes). Real-world field: time to reach TL average 1m15s (vs straight-line 1m8s; sweeping full coverage 21m16s).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines at 18,000 steps (averages over 100 episodes): Sweeping AoI coverage = 0.55502 ± 0.07147 (total coverage 0.5883 ± 0.1349); Random AoI = 0.3558 ± 0.0653 (total 0.1708 ± 0.04045); Curiosity AoI = 0.08192 ± 0.0525 (total 0.03809 ± 0.01587); Robustness-driven exploration (RDE/P-MTL) AoI = 0.83807 ± 0.07354 but total coverage = 0.65466 ± 0.03254 (i.e., less efficient).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained on 35 randomly generated maps and tested without retraining on 20 unseen simulated maps and 5 Planet maps; networks pre-trained per-task then fine-tuned together for 100 full episodes before testing. Exact episodes-to-convergence not numerically specified, but training graphs (Fig.3) show stable learning with LSTM stacks and Recurrent-DDQN/A2C hyperparameters: DDQN lr=1e-3 batch=32, experience buffer=2000, A2C lr=1e-5 batch=1.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit: an information-gain function over allocentric regions balances coverage cost and AoI probability via weights (ω1, ω2, ω3). The system uses πn to explore/relocate to high-potential regions and πr to exploit within-region AoI coverage; switching is triggered by reaching TL or time limits (κ1, κ2). Internal memory (LSTM) and visited-vector Vt bias action selection away from repeatedly visited cells.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: Sweeping (zigzag) policy, Random policy (same framework but random actions), Curiosity-driven exploration (Burda et al.), Robustness-driven exploration (RDE, probabilistic MTL + Monte Carlo from [3]).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>The adaptive information-gain + dual-policy DRL approach yields substantially higher AoI coverage per exploration effort (≈81% AoI coverage while imaging ~30% of area) versus baselines; LSTM memory, egocentric map mt, and visited-vector Vt are each critical (ablation: removing Vt or mt drops AoI coverage to ≈32–34%; removing LSTM drops AoI to ≈27%). The approach generalizes to unseen simulated and satellite maps without retraining and demonstrates real-world feasibility on a quadcopter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations include: experiments omit obstacles and adversarial forces (planned future work), performance depends on parameterized region size α and thresholds κ1/κ2; RDE baseline sometimes attains comparable/higher AoI coverage but with much larger total coverage (inefficient); RL training instability in POMDPs requires LSTM stacks and still may be sample intensive; exact sample-efficiency (episodes-to-target performance) not fully quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1147.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1147.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>πn / Recurrent-DDQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Target-Directed Exploration Policy (Recurrent Double DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent DDQN policy (stacked LSTMs) trained to navigate from the agent's current location to an assigned target region while maximizing AoI coverage along the path, operating on egocentric map mt, current observation ot, and visited-vector Vt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>πn (Target-Directed Exploration, Recurrent-DDQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent-DDQN network with LSTM stack: inputs include a time-history window (shape [5,31]) and flattened egocentric map ([5,625]); processed through FC layers (64,10) and (100,100) then concatenated and passed to LSTM (110 units) and a final 5-way action output. Trained with Adam (lr=1e-3), batch=32, replay buffer=2000, epsilon-greedy annealing from 0.95→<0.01, discount γ=0.95.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information-gain guided target selection + Recurrent-DDQN (active adaptive exploration while navigating)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects next region via the information-gain objective T over allocentric map M and then uses Recurrent-DDQN to produce short-term navigation actions that prioritize flying over cells with higher AoI probability and unvisited cells; it updates maps and Vt online and re-centers mt when crossing mt bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same as πAE: 180×180 simulated AoI maps, Planet satellite regions, real 50×50 m field</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable due to camera-limited FOV; discrete movement actions; long episodes; unknown prior map; noisy observations possible.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State vector includes flattened ot probabilities (w×h entries), 4 adjacent-visited indicators, and x/y distance to target TL; action space 5; episodes up to 18,000 steps; egocentric map size used in inputs leads to large flattened vectors (e.g., 625 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Contributes to overall system results: leads to high AoI coverage in combined model (see πAE totals). Training curves (Fig.3) show Recurrent-DDQN with stacked LSTMs reduces instability vs vanilla DDQN and converges to an effective navigation policy (no isolated numeric for πn alone provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Ablation removing LSTM or Vt/mt significantly degrades performance: No LSTM leads to final system AoI coverage ≈0.2723 ± 0.0861 (vs full model 0.8135); no mt leads to ≈0.3449 ± 0.0648; no V leads to ≈0.3214 ± 0.0543, indicating πn's reliance on these components.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained on the same 35-map training set using experience replay; exact episode counts to convergence not reported, but DDQN training hyperparams and replay buffer size given; inclusion of LSTM reduced observed unstable learning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Operates on exploration objective (reach region R) while maximizing AoI coverage along trajectory by weighting AoI detection rewards against revisit/coverage/distance costs in reward Rn; therefore it performs opportunistic exploitation en-route to an exploration target.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared as part of full model against same baselines (sweeping, random, curiosity, RDE).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Recurrent-DDQN with LSTM enables stable long-horizon navigation under partial observability and importantly biases paths to cover AoIs on-route; without it the agent gets stuck or revisits heavily, harming AoI coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>DDQN instability without LSTM; requires careful hyperparameter tuning (ε-annealing, replay buffer size); no explicit obstacle avoidance tested; per-component performance numbers (e.g., success to reach TL under varied starts) not fully quantified beyond aggregate metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1147.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1147.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>πr / Recurrent-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AoI Exploitation Policy (Recurrent Advantage Actor-Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent A2C policy (actor and value networks with stacked LSTMs) trained to maximize within-region AoI coverage by freely exploring the current region and prioritizing high-probability AoI cells while minimizing wasted steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>πr (AoI Exploitation, Recurrent-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent A2C actor-critic architecture with identical actor and value networks; inputs include current observation ot and adjacent-cell visited information Vt; networks include stacked LSTM layers; trained with Adam (lr=1e-5), batch size 1; outputs action probabilities for 5 discrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>local active exploitation using policy-gradient A2C guided by AoI-detection rewards (a form of adaptive sampling/active learning focused on local coverage)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Within a selected region, πr adaptively explores based on ot and Vt, prioritizing cells with higher AoI probabilities and unvisited locations; it may leave and re-center mt when necessary and returns control when information-gain indicates the current region is no longer best (after κ2 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same regional environments as πAE (simulated AoI grids, Planet satellite regions, field test)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable local region with noisy visual detection; continuous/ongoing local exploration (not strictly episodic within region); sparse positive rewards for AoI detection.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Operates on local egocentric maps (mt); action space 5; switching criteria based on κ1/κ2 and information gain; episodes can be long (≤18,000 steps) when combined with πn.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>As part of the combined system, πr enables high in-region AoI coverage contributing to overall AoI coverage ≈0.81385 ± 0.0674 in simulations and ≈0.79779 ± 0.06462 on Planet maps; ablation removing components (mt, Vt, LSTM) reduces performance dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>When replaced by random actions (Random baseline) within the same two-policy framework, AoI coverage drops to 0.3558 ± 0.0653 (simulated), showing the learned πr's strong contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pre-trained separately using Recurrent-A2C on the training set; A2C training used low lr (1e-5) and batch size 1. Exact step counts to achieve stable local policies not numerically provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>πr is the exploitation arm — it focuses on local exploitation of detected AoI while the high-level information-gain function and πn ensure sufficient exploration; switching thresholds κ1/κ2 mediate time spent exploiting vs re-targeting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Functionally compared inside the combined system against Random policy (random actions in place of πr/πn), Curiosity-driven exploration (global exploration), Sweeping, and RDE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Recurrent-A2C with memory and visited information strongly improves local AoI coverage and avoids wasted revisits; including LSTM and Vt is critical for avoiding local minima and improving per-region coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>A2C's sample efficiency can be low (batch size 1, lr 1e-5) and requires LSTM to deal with POMDP; exact failure modes (e.g., when AoIs are extremely sparse or deceptive) not exhaustively characterized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep recurrent q-learning for partially observable mdps <em>(Rating: 2)</em></li>
                <li>Nonmyopic active learning of gaussian processes: An exploration-exploitation approach <em>(Rating: 2)</em></li>
                <li>Bayesian optimisation for informative continuous path planning <em>(Rating: 2)</em></li>
                <li>Robustness-driven exploration with probabilistic metric temporal logic <em>(Rating: 2)</em></li>
                <li>Large-scale study of curiosity-driven learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1147",
    "paper_id": "paper-233714799",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "πAE",
            "name_full": "Adaptive Exploration Policy",
            "brief_description": "A two-branched DRL system that adaptively trades off exploration and exploitation for UAV search in unknown, partially observable environments by selecting regions with maximal information gain and switching between long-range navigation and local AoI coverage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "πAE (Adaptive Exploration Policy)",
            "agent_description": "Hierarchical agent composed of two learned sub-policies: Target-Directed Exploration (πn, Recurrent-DDQN) for region-to-region navigation and AoI Exploitation (πr, Recurrent-A2C) for dense local coverage; both networks include stacked LSTM layers, use egocentric (mt) and allocentric (M) maps plus a visited-vector Vt, and are coordinated by an information-gain objective to pick target regions.",
            "adaptive_design_method": "information-gain maximization combined with DRL (active/adaptive exploration); uses map segmentation and curiosity-like local exploitation (A2C) for adaptive sampling",
            "adaptation_strategy_description": "At each decision epoch the agent computes an information-gain score T = argmax_i (ω1·α_i + ω2·β_i + ω3·ζ_i) over map regions (α: coverage, β: AoI probability, ζ: distance) and adaptively selects the next target region; while navigating it biases trajectories to increase AoI coverage, and once inside a region it switches to a local exploitation policy after κ1/κ2 thresholds. It updates allocentric map M and egocentric mt continuously from on-board observations and the visited-vector Vt to influence future region selection.",
            "environment_name": "Simulated AoI maps (180×180 grid), Planet satellite regions for illegal mining (5 regions, 8×8 km^2), and a real 50×50 m^2 field test",
            "environment_characteristics": "Partially observable (POMDP) due to limited onboard camera FOV; unknown prior map; discrete grid-like spatial decomposition; sparse AoI rewards; non-episodic/long episodes (no fixed terminal state); stochastic/noisy observations (simulator supports image noise)",
            "environment_complexity": "Large discrete spatial map (180×180 cells for simulations); action space size = 5 (forward, back, left, right, hover); episode/time budget up to 18,000 timesteps in simulations; region decomposition hyperparameter α yields smaller region maps (eg. mt of size 25×25 or similar flattened inputs); real-world field 50×50 m with FOV ≈10×10 m.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Simulated AoI detection: AoI coverage = 0.81385 ± 0.0674 (fraction of AoI area covered) with total map coverage = 0.3051 ± 0.0396 after 18,000 steps. Illegal mining detection (Planet data): AoI coverage = 0.79779 ± 0.06462, total coverage = 0.2038 ± 0.1394 (averaged over 100 episodes). Real-world field: time to reach TL average 1m15s (vs straight-line 1m8s; sweeping full coverage 21m16s).",
            "performance_without_adaptation": "Baselines at 18,000 steps (averages over 100 episodes): Sweeping AoI coverage = 0.55502 ± 0.07147 (total coverage 0.5883 ± 0.1349); Random AoI = 0.3558 ± 0.0653 (total 0.1708 ± 0.04045); Curiosity AoI = 0.08192 ± 0.0525 (total 0.03809 ± 0.01587); Robustness-driven exploration (RDE/P-MTL) AoI = 0.83807 ± 0.07354 but total coverage = 0.65466 ± 0.03254 (i.e., less efficient).",
            "sample_efficiency": "Trained on 35 randomly generated maps and tested without retraining on 20 unseen simulated maps and 5 Planet maps; networks pre-trained per-task then fine-tuned together for 100 full episodes before testing. Exact episodes-to-convergence not numerically specified, but training graphs (Fig.3) show stable learning with LSTM stacks and Recurrent-DDQN/A2C hyperparameters: DDQN lr=1e-3 batch=32, experience buffer=2000, A2C lr=1e-5 batch=1.",
            "exploration_exploitation_tradeoff": "Explicit: an information-gain function over allocentric regions balances coverage cost and AoI probability via weights (ω1, ω2, ω3). The system uses πn to explore/relocate to high-potential regions and πr to exploit within-region AoI coverage; switching is triggered by reaching TL or time limits (κ1, κ2). Internal memory (LSTM) and visited-vector Vt bias action selection away from repeatedly visited cells.",
            "comparison_methods": "Compared against: Sweeping (zigzag) policy, Random policy (same framework but random actions), Curiosity-driven exploration (Burda et al.), Robustness-driven exploration (RDE, probabilistic MTL + Monte Carlo from [3]).",
            "key_results": "The adaptive information-gain + dual-policy DRL approach yields substantially higher AoI coverage per exploration effort (≈81% AoI coverage while imaging ~30% of area) versus baselines; LSTM memory, egocentric map mt, and visited-vector Vt are each critical (ablation: removing Vt or mt drops AoI coverage to ≈32–34%; removing LSTM drops AoI to ≈27%). The approach generalizes to unseen simulated and satellite maps without retraining and demonstrates real-world feasibility on a quadcopter.",
            "limitations_or_failures": "Reported limitations include: experiments omit obstacles and adversarial forces (planned future work), performance depends on parameterized region size α and thresholds κ1/κ2; RDE baseline sometimes attains comparable/higher AoI coverage but with much larger total coverage (inefficient); RL training instability in POMDPs requires LSTM stacks and still may be sample intensive; exact sample-efficiency (episodes-to-target performance) not fully quantified.",
            "uuid": "e1147.0",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "πn / Recurrent-DDQN",
            "name_full": "Target-Directed Exploration Policy (Recurrent Double DQN)",
            "brief_description": "A recurrent DDQN policy (stacked LSTMs) trained to navigate from the agent's current location to an assigned target region while maximizing AoI coverage along the path, operating on egocentric map mt, current observation ot, and visited-vector Vt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "πn (Target-Directed Exploration, Recurrent-DDQN)",
            "agent_description": "Recurrent-DDQN network with LSTM stack: inputs include a time-history window (shape [5,31]) and flattened egocentric map ([5,625]); processed through FC layers (64,10) and (100,100) then concatenated and passed to LSTM (110 units) and a final 5-way action output. Trained with Adam (lr=1e-3), batch=32, replay buffer=2000, epsilon-greedy annealing from 0.95→&lt;0.01, discount γ=0.95.",
            "adaptive_design_method": "information-gain guided target selection + Recurrent-DDQN (active adaptive exploration while navigating)",
            "adaptation_strategy_description": "Selects next region via the information-gain objective T over allocentric map M and then uses Recurrent-DDQN to produce short-term navigation actions that prioritize flying over cells with higher AoI probability and unvisited cells; it updates maps and Vt online and re-centers mt when crossing mt bounds.",
            "environment_name": "Same as πAE: 180×180 simulated AoI maps, Planet satellite regions, real 50×50 m field",
            "environment_characteristics": "Partially observable due to camera-limited FOV; discrete movement actions; long episodes; unknown prior map; noisy observations possible.",
            "environment_complexity": "State vector includes flattened ot probabilities (w×h entries), 4 adjacent-visited indicators, and x/y distance to target TL; action space 5; episodes up to 18,000 steps; egocentric map size used in inputs leads to large flattened vectors (e.g., 625 entries).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Contributes to overall system results: leads to high AoI coverage in combined model (see πAE totals). Training curves (Fig.3) show Recurrent-DDQN with stacked LSTMs reduces instability vs vanilla DDQN and converges to an effective navigation policy (no isolated numeric for πn alone provided).",
            "performance_without_adaptation": "Ablation removing LSTM or Vt/mt significantly degrades performance: No LSTM leads to final system AoI coverage ≈0.2723 ± 0.0861 (vs full model 0.8135); no mt leads to ≈0.3449 ± 0.0648; no V leads to ≈0.3214 ± 0.0543, indicating πn's reliance on these components.",
            "sample_efficiency": "Trained on the same 35-map training set using experience replay; exact episode counts to convergence not reported, but DDQN training hyperparams and replay buffer size given; inclusion of LSTM reduced observed unstable learning.",
            "exploration_exploitation_tradeoff": "Operates on exploration objective (reach region R) while maximizing AoI coverage along trajectory by weighting AoI detection rewards against revisit/coverage/distance costs in reward Rn; therefore it performs opportunistic exploitation en-route to an exploration target.",
            "comparison_methods": "Compared as part of full model against same baselines (sweeping, random, curiosity, RDE).",
            "key_results": "Recurrent-DDQN with LSTM enables stable long-horizon navigation under partial observability and importantly biases paths to cover AoIs on-route; without it the agent gets stuck or revisits heavily, harming AoI coverage.",
            "limitations_or_failures": "DDQN instability without LSTM; requires careful hyperparameter tuning (ε-annealing, replay buffer size); no explicit obstacle avoidance tested; per-component performance numbers (e.g., success to reach TL under varied starts) not fully quantified beyond aggregate metrics.",
            "uuid": "e1147.1",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "πr / Recurrent-A2C",
            "name_full": "AoI Exploitation Policy (Recurrent Advantage Actor-Critic)",
            "brief_description": "A recurrent A2C policy (actor and value networks with stacked LSTMs) trained to maximize within-region AoI coverage by freely exploring the current region and prioritizing high-probability AoI cells while minimizing wasted steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "πr (AoI Exploitation, Recurrent-A2C)",
            "agent_description": "Recurrent A2C actor-critic architecture with identical actor and value networks; inputs include current observation ot and adjacent-cell visited information Vt; networks include stacked LSTM layers; trained with Adam (lr=1e-5), batch size 1; outputs action probabilities for 5 discrete actions.",
            "adaptive_design_method": "local active exploitation using policy-gradient A2C guided by AoI-detection rewards (a form of adaptive sampling/active learning focused on local coverage)",
            "adaptation_strategy_description": "Within a selected region, πr adaptively explores based on ot and Vt, prioritizing cells with higher AoI probabilities and unvisited locations; it may leave and re-center mt when necessary and returns control when information-gain indicates the current region is no longer best (after κ2 steps).",
            "environment_name": "Same regional environments as πAE (simulated AoI grids, Planet satellite regions, field test)",
            "environment_characteristics": "Partially observable local region with noisy visual detection; continuous/ongoing local exploration (not strictly episodic within region); sparse positive rewards for AoI detection.",
            "environment_complexity": "Operates on local egocentric maps (mt); action space 5; switching criteria based on κ1/κ2 and information gain; episodes can be long (≤18,000 steps) when combined with πn.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "As part of the combined system, πr enables high in-region AoI coverage contributing to overall AoI coverage ≈0.81385 ± 0.0674 in simulations and ≈0.79779 ± 0.06462 on Planet maps; ablation removing components (mt, Vt, LSTM) reduces performance dramatically.",
            "performance_without_adaptation": "When replaced by random actions (Random baseline) within the same two-policy framework, AoI coverage drops to 0.3558 ± 0.0653 (simulated), showing the learned πr's strong contribution.",
            "sample_efficiency": "Pre-trained separately using Recurrent-A2C on the training set; A2C training used low lr (1e-5) and batch size 1. Exact step counts to achieve stable local policies not numerically provided.",
            "exploration_exploitation_tradeoff": "πr is the exploitation arm — it focuses on local exploitation of detected AoI while the high-level information-gain function and πn ensure sufficient exploration; switching thresholds κ1/κ2 mediate time spent exploiting vs re-targeting.",
            "comparison_methods": "Functionally compared inside the combined system against Random policy (random actions in place of πr/πn), Curiosity-driven exploration (global exploration), Sweeping, and RDE.",
            "key_results": "Recurrent-A2C with memory and visited information strongly improves local AoI coverage and avoids wasted revisits; including LSTM and Vt is critical for avoiding local minima and improving per-region coverage.",
            "limitations_or_failures": "A2C's sample efficiency can be low (batch size 1, lr 1e-5) and requires LSTM to deal with POMDP; exact failure modes (e.g., when AoIs are extremely sparse or deceptive) not exhaustively characterized.",
            "uuid": "e1147.2",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep recurrent q-learning for partially observable mdps",
            "rating": 2,
            "sanitized_title": "deep_recurrent_qlearning_for_partially_observable_mdps"
        },
        {
            "paper_title": "Nonmyopic active learning of gaussian processes: An exploration-exploitation approach",
            "rating": 2,
            "sanitized_title": "nonmyopic_active_learning_of_gaussian_processes_an_explorationexploitation_approach"
        },
        {
            "paper_title": "Bayesian optimisation for informative continuous path planning",
            "rating": 2,
            "sanitized_title": "bayesian_optimisation_for_informative_continuous_path_planning"
        },
        {
            "paper_title": "Robustness-driven exploration with probabilistic metric temporal logic",
            "rating": 2,
            "sanitized_title": "robustnessdriven_exploration_with_probabilistic_metric_temporal_logic"
        },
        {
            "paper_title": "Large-scale study of curiosity-driven learning",
            "rating": 1,
            "sanitized_title": "largescale_study_of_curiositydriven_learning"
        }
    ],
    "cost": 0.01232225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments
4 May 2021</p>
<p>Ashley Peake 
Computer Science Department
Wake Forest University Winston-Salem
North CarolinaUSA</p>
<p>Joe Mccalmon 
Computer Science Department
Wake Forest University Winston-Salem
North CarolinaUSA</p>
<p>Yixin Zhang 
Computer Science Department
Wake Forest University Winston-Salem
North CarolinaUSA</p>
<p>Daniel Myers 
Computer Science Department
Wake Forest University Winston-Salem
North CarolinaUSA</p>
<p>Sarra Alqahtani sarra-alqahtani@wfu.edu 
Computer Science Department
Wake Forest University Winston-Salem
North CarolinaUSA</p>
<p>Paul Pauca paucavp@wfu.edu 
Computer Science Department
Wake Forest University Winston-Salem
North CarolinaUSA</p>
<p>Deep Reinforcement Learning for Adaptive Exploration of Unknown Environments
4 May 20217ACF98F0A3F498DD3AC7DA318961CA19arXiv:2105.01606v1[cs.LG]
Performing autonomous exploration is essential for unmanned aerial vehicles (UAVs) operating in unknown environments.Often, these missions start with building a map for the environment via pure exploration and subsequently using (i.e.exploiting) the generated map for downstream navigation tasks.Accomplishing these navigation tasks in two separate steps is not always possible or even disadvantageous for UAVs deployed in outdoor and dynamically changing environments.Current exploration approaches either use a priori human-generated maps or use heuristics such as frontier-based exploration.Other approaches use learning but focus only on learning policies for specific tasks by either using sample inefficient random exploration or by making impractical assumptions about full map availability.In this paper, we develop an adaptive exploration approach to trade off between exploration and exploitation in one single step for UAVs searching for areas of interest (AoIs) in unknown environments using Deep Reinforcement Learning (DRL).The proposed approach uses a map segmentation technique to decompose the environment map into smaller, tractable maps.Then, a simple information gain function is repeatedly computed to determine the best target region to search during each iteration of the process.DDQN and A2C algorithms are extended with a stack of LSTM layers and trained to generate optimal policies for the exploration and exploitation, respectively.We tested our approach in 3 different tasks against 4 baselines.The results demonstrate that our proposed approach is capable of navigating through randomly generated environments and covering more AoI in less time steps compared to the baselines.</p>
<p>I. INTRODUCTION</p>
<p>Exploration of an unknown environment is an important task in many applications of mobile robotics.In large, outdoor environments, Unmanned Aerial Vehicles (UAVs) are often employed for exploration due to their ease of use and maneuverability.Typically, a map of the target environment is built (via Simultaneous Localization and Mapping, i.e.SLAM) and is subsequently exploited in downstream navigation tasks [1].Accomplishing these navigation tasks in two separate steps is not always possible in dynamically changing environments.To overcome this, adaptive exploration can be used to combine and trade off between exploration and exploitation in one single step, allowing the UAV to efficiently collect relevant information from the environment.Developing adaptive exploration algorithms is particularly important in time-critical tasks in outdoor environments such as Search and Rescue (SaR) [2], illegal activity detection (e.g.human-trafficking, gold mining [3]), or environmental applications (localizing wildlife, pollution, or mobile machinery).</p>
<p>Reinforcement Learning (RL) has been utilized to learn exploration policies in training environments [1,2,[4][5][6].RL algorithms do not require the agents to have an explicit model of the environment to properly navigate within it.Instead, the RL agent learns how to act in an environment by repetitively interacting with it and receiving positive or negative feedback from a predefined reward function [7].Moreover, RL algorithms generalize well to novel environments when combined with deep learning networks and can greatly adapt to new scenes in testing environments.In this paper, we propose a novel approach for adaptive exploration that simultaneously prioritizes coverage sampling and area of interest (AoI) sampling, without depending on user-defined weighted objectives as in [8][9][10][11].We use deep reinforcement learning (DRL) to learn an adaptive exploration policy πAE that allows the agent to autonomously explore an unknown environment while prioritizing navigation of AoIs.There are, however, significant challenges to overcome.First, outdoor environments can be theoretically extended infinitely in each direction.A large region to explore introduces a large set of possible observations that the RL agent (i.e.UAV) can encounter.While deep learning networks can greatly adapt to the unseen observations, performance will still be hindered, and the training process will be expensive.Additionally, in such environments, to take advantage of previously discovered information, each individual observation may include thousands of features and can quickly make the problem intractable.Second, RL algorithms are known to have poor convergence in partially observable environments.Since the agent can only observe a limited area accessible by its on-board camera, the search environment becomes partially observable for the agent at any time step.Lastly, the RL agent in this problem has no predefined, stable condition to end a training episode, i.e. reaching a predefined target location, which can make it very hard for the RL agent to train on any reward function.To the best of our knowledge, this paper is the first to address the exploration problem with those constraints.</p>
<p>To tackle these challenges, we design an adaptive exploration policy architecture.First, we employ a map segmentation technique to decompose the environment into disjoint regions, where each region contains the probability that the region contains an AoI, provided through exploration.This allows the UAV to only navigate within a small region at a time, while still preserving information from across the entire mission for use in decision making (i.e.egocentric exploration).We then train a Deep Reinforcement Learning (DRL) model to direct UAV flight to optimize efficient information gain.We incorporate two neural networks, each selecting an action according to one of two competing navigation tasks.On one hand, the UAV must quickly explore the entire environment in search of AoIs.On the other, it must fully cover or "exploit" AoIs that have already been located (i.e.image or deploy sensors for detailed information collection).We call these the exploration and exploitation tasks, respectively.To control the trade-off between these tasks, we design an efficient information gain function that utilizes current knowledge the UAV has collected about the entire map of the environment (i.e.allocentric exploration).</p>
<p>To train the exploration task, we use the double deep Q-Learning (DDQN) algorithm [12] to generate an optimal flight trajectory to a target region, while still prioritizing flying over AoIs on the path to that target.For AoI exploitation, we train another DRL model using the Actor-Critic (A2C) algorithm [7] that allows the UAV to freely explore its current region in search for AoIs.To deal with the poor convergence of RL algorithms in partially observed environments, we embed a stack of recurrent layers, in particular long-short term memory (LSTM), to the DDQN and A2C models.The LSTM layers combat partial observability by letting the UAV maintain an internal representation of the areas it has visited, despite not having direct access to those state representations.</p>
<p>We have tested our approach in simulation on two different settings: 1) 35 randomly simulated maps of AoIs of different shapes and distributions and 2) 19 maps from Planet satellite imagery for detecting illegal gold mining activity in Amazonian forests in Peru [3].We built a random scenario generator to create the training and testing maps for the first task.The maps are structured as 180 × 180 cell grids.The scenario generator randomly places the AoI using a normal distribution with values of the standard deviation and mean based on map width and height.Additionally, we tested our approach in hardware using a Pixhawk quadcopter connected to a Raspberry Pi 3 Model B to search for specific targets in a grassy field.We compared the results of our approach against several baselines and conducted ablation experiments to identify which parts of our proposed solution contribute the most to the performance.Overall, the results reveal that our model is able to more efficiently cover AoIs in an unknown environment compared to the baselines.For reproducibility, our code can be accessed through https://github.com/RL-WFU/Dronefield.git</p>
<p>The remainder of this paper is organized as follows.We discuss previous and related work on autonomous controllers for UAVs in Section II and an overview of RL and other algorithms used in Section III.Section IV formulates the problem and introduces the proposed approach.Section V presents our experiments and discusses results.Final remarks and conclusions are offered in Section VI.</p>
<p>II. RELATED WORK</p>
<p>As we study the problem of trading off between exploration and exploitation in navigation tasks, we draw upon recent efforts that use heuristic-based and learning-based approaches for this problem.We survey related efforts in these two directions.</p>
<p>A. Heuristic-Based Exploration</p>
<p>Early autonomous exploration methods explored simple environments, for example, by following walls or similar obstacles.Frontier exploration [13] was one of the the first exploration methods capable of exploring a generic 2D environment.Frontier regions are defined as the borders between free and unexplored areas.Exploration is done by sequentially navigating close frontiers.Advanced variants [14,15] of this algorithm improve the coverage of unknown space along the path to the frontier.</p>
<p>Next-best-view (NBV) exploration methods are a common alternative to frontier-based exploration.Recently, a Receding Horizon NBV planner [16] was proposed for online autonomous exploration of unknown 3D spaces.This planner employed rapidly exploring random trees (RRT) with a cost function that considered the information gain at each node of the tree.A path to the best node was extracted and the algorithm was repeated each time the vehicle moved along the first edge of the best path.An extension of this work [17] resolves the problem of sticking to local minima by extending it with a frontier-based planner for global exploration.In our work, we choose to use RL over RRT and its extensions for two reasons.First, RRT typically employs a uniform proposal distribution for sampling which does not make use of the structures of the environment and thus may require many samples to obtain an initial feasible solution path in a new environment.Second, the RRT does not have any systematic way to take advantage of information from previous experiences.It requires computing samples from scratch to build trees whenever a new start or goal configuration is specified, even when a similar solution has been computed on a previous query in that environment, which is what RL excels in.</p>
<p>Temporal logic has also been used in the context of robotic motion and path planning in unknown environments.For instance, deterministic µ-calculus was used to define specifications for sampling-based algorithms [18].Robustness of Metric Temporal Logic (MTL) has been embedded in A* to increase the safety of UAVs navigating adversarial environments [19].In [3], the NBV is sampled according to the current field of view of the UAV.Here, the views are randomly sampled as potential targets via Markov Chain Monte Carlo methods and evaluated by their robustness values of the probabilistic metric temporal logic (P-MTL) constraints.In Section V, we compare our approach against the P-MTL method [3] for the illegal mining search task.</p>
<p>B. Learning-Based Exploration</p>
<p>Recently, machine learning has been used to develop autonomous navigation and exploration algorithms for robotics [1,2,[4][5][6].A number of design choices have been investigated including different policy architectures for representing the exploration space.For example, [20] uses feed-forward networks, [21] uses vanilla neural network memory, [22] uses spatial memory and planning modules, and [23] uses semi-parametric topological memory.In [5], a model-based RL algorithm is used for autonomous navigation that learns faster than traditional table-based Q-learning methods due to its parallel architecture.In [6], a function approximationbased RL approach is exploited for a large number of states.The use of function approximation reduces the convergence time of the algorithm needed for search and rescue operations.A deep deterministic policy gradient is developed in [22] to provide results in continuous time and action.The use of neural networks and deep learning was briefly performed in [24] to guarantee safe take-off, navigation, and landing of the UAV on a fixed target.</p>
<p>Instead of focusing only on navigation for a specific task or on pure exploration of an environment, however, here we combine both to efficiently train our adaptive exploration policy.We achieve this by encouraging the RL agent to actively exploit the sensed information in its egocentric map via the exploitation task, while escaping local optima by prioritizing exploring unseen areas in the allocentric map via the exploration task.In doing so, unlike previous work, we do not assume access to human demonstrations in the given novel training and test environments [1,23], nor do we assume availability of prior knowledge about the environment [20,21].Our RL agent starts with no knowledge about the environment and derives it using the on-board camera during runtime.This makes our proposed approach amenable to real world deployment as shown in Section V.</p>
<p>III. REINFORCEMENT LEARNING</p>
<p>Reinforcement learning is suitable for solving problems that can be formulated as a Markov decision process (MDP).An MDP is a method for framing an environment such that it can be written as a tuple containing the state space S, the action space A, the reward space R, and the probability of transitioning from a state st at time t to another state st+1 at time t + 1.An MDP is defined formally as p(rt+1, st+1|at, st), for all time steps t, and additionally must satisfy the Markov property.This property necessitates that this transition only depends on the state-action pair at the current time step t, and not on any state-action pairs from prior time steps.</p>
<p>Actions are chosen by the RL agent according to its policy, π(at|st) = p(at|st).The optimal policy is one which maximizes the state-action value function:
Qπ(st, at) = E ∞ γ R t+ +1 |St = st, At = at (1)
where γ is a discount factor between 0 and 1. Rt is the expected reward from a state-action pair,(st, at).For , we have the expected reward at time instant t + 1:
rt+1(st, at) = E[Rt+1|St = st, At = at] = r t+1 ∈R rt+1 s t+1 ∈S p(rt+1, St+1|St, at) (2)
The goal of the RL agent is to maximize expected future reward.If the transitions from Eq. 1 are known, then an MDP of this nature can be solved through policy evaluation and policy iteration.However in most practical cases, the exact transition probabilities are unknown, so the agent must collect experience by interacting with the environment and internally computing the state-action value function itself.This process can be done with tabular methods in small environments, such as in Q-learning, or with a neural network such as in deep Q-learning (DQN).In the case that the Markov property is not satisfied, and the RL agent does not have full access to the environment at any given time step, the environment can be framed as a partially observable Markov decision process (POMDP).By using LSTM layers in the agent's policy and value neural networks, the agent can maintain an internal representation of the entire environment, despite only observing part of it, in order to more closely resemble an MDP.In this paper, we use the double DQN (DDQN) and advantage-actor critic (A2C) algorithms to train the exploration and exploitation tasks respectively while enhancing their networks with a stack of LSTM layers to deal with limited observability.We use DDQN for training in exploration due to its ability to learn to reach target locations and its strength at reducing the overestimation of the values of successor states [25].A2C is used in exploitation training because of its ability to learn a successful policy in continuous, rather than episodic, environments [7].</p>
<p>IV. ADAPTIVE EXPLORATION POLICY FOR UNKNOWN ENVIRONMENTS</p>
<p>In this paper, the agent's objective is to learn an adaptive exploration policy πAE enabling it to efficiently explore the given environment while prioritizing navigating detected AoIs.We formulate the estimation of πAE as a learning problem.We design a reward function that is estimated using the features extracted from the environment.πAE is learned on a set of training environments and tested on a different set.</p>
<p>Adaptive exploration in outdoor, large environments requires long-term coherent behavior over multiple time steps, such as avoiding obstacles, exploring new directions, finding AoIs and following them.Such long-term behavior is hard to learn using purely RL, given sparsity in reward which leads to excessively large sample complexity for learning.To overcome this large sample complexity, we design a simple map segmentation technique to make learning more tractable for the exploration and exploitation policies.</p>
<p>A. Map Segmentation for Tractable Learning</p>
<p>Given an unknown, outdoor environment E, we first structurally decompose the continuous space into a regular grid of smaller regions.We divide the map of M of size W × H into N regions of size W ×H α where α is a hyperparameter to control the learning performance of the agent.Thus, each region R gives a large enough map for the agent to navigate, without being so large that learning is intractable.We then dynamically create two maps with different levels of egocentrism around the agent.One is a detailed map ot representing the agent's current observation of dimension w × h, determined by the camera vision range.The second is a course map mt of size w 2 × h 2 that contains information collected about the area around the agent.This design choice is inspired by [1,26,27], though our work improves upon the levels of egocentrism, increasing the success of πAE as we will show in Section V.</p>
<p>The agent updates mt based on its current observation while navigating through the area.When the agent moves outside the bounds of mt, mt+1 will be centered around the agent's new position.This egocentric map mt is then used to update the allocentric map M , which stores the probability distribution of AoI in each region.We similarly record encoded information for each cell based on whether it has been visited by the agent.This value is scaled if the agent has visited the cell more than once.We use this to construct a vector V of the visited information for each adjacent cell to the UAVs current position in order to enhance the learning of πAE (next section).</p>
<p>B. Adaptive Exploration Policy Architecture</p>
<p>We design the learning architecture of the adaptive exploration policy πAE in two sub-policies: one policy for Target-Directed Exploration πn and the other for AoI Exploitation πr.Decomposing the policy into 2 sub-policies improves the efficiency of learning process of πAE and enables the generated policy to successfully complete the exploration task.The policy architecture is shown in Fig. 2 and described in more detail in the following sections.1) Target-Directed Exploration Policy πn : Under policy πn the agent prioritizes exploration across the entire search space by navigating to the next best region R.The agent determines the best region to navigate by using the allocentric map of the environment M to compute the information gain from each region.A simple objective function (Eq.3) is then used to identify the region i ∈ N with the greatest potential for having unexplored AoI based on: the percentage of the region which the agent has previously visited (αi), the concentration of AoI previously seen in the region (βi), and the Euclidean distance from the region's centroid to the current location of the agent (ζi).
T = argmax ∀i∈N (ω1αi + ω2βi + ω3ζi)(3)
where ω1, ω2, ω3 represent the weights for coverage, AoI probability, and the distance from the agent to the region i, respectively.It is worth mentioning that the coverage and distance are weighted with negative ω to represent the cost, while the weight of the potential AoI is positive to represent the information gain.</p>
<p>To navigate to the assigned region R, the agent learns to maximize the AoI coverage over its flight trajectory, while efficiently reaching R. The agent learns its πn by fusing information from its current observation ot, its current egocentric map of mt, and the recently updated visited vector Vt:</p>
<p>1) Observation ot: The images taken by the agent's camera are processed through any classification or detection algorithm such as CNN to extract and detect the features of AoI.In our simulated experiment, we use a simple color thresholding technique to detect AoI.For our gold mining experiment, we use a CNN to recognize the mining machines and traces as AoI based on a dataset of remote sensing images taken from Planet Satellite and UAVs as in [3].2) Egocentric Map mt: We use previous observations up to time step t to derive the egocentric map mt.We use mt instead of using the allocentric map M to simplify the learning process by using only information local to the current position of the agent xt.This map allows the navigation algorithm to not only utilize previously detected AoI but also to locate it with respect to xt. 3) Visited Vector Vt: We use information about visited and unvisited cells in mt.This information is scaled based on the number of times a cell has been visited, and is stored across the entire map, effectively creating a heat map.Local visited information helps the agent explore areas it has not been to before, which eventually increases the potential of covering more AoI.</p>
<p>The Target-Directed Exploration policy πn is trained by the Recurrent-DDQN algorithm.The agent navigates within its current mt using πn until it reaches the cell closest to the target region, denoted TL.Once the agent reaches TL, a new mt+1 is created with the agent's current position at the center and a new TL is selected accordingly.The state st is the first input into the Recurrent-DDQN network at each time step.s is a vector of flattened information from ot, Vt and the distance between the agent and the assigned destination.The first h × w entries are the probabilities of detecting an AoI in the ot.The next 4 entries represent information about how frequently the agent has visited the adjacent cells (left, right, top, bottom), retrieved from Vt in the allocentric map M .V (x, y) ∈ (0, 1) if the location (x, y) has been visited by the agent, with a value closer to 1 corresponding to a higher number of visits to that cell.Otherwise, if the cell has not been previously visited, it has value of 0. The last two entries in the state s's vector are the Euclidean distance in the x and y directions to TL. (See Fig. 2).</p>
<p>The second input to the Target-Directed Navigation network is the agent's current egocentric map mt.The state st and egocentric map mt are then processed in the network to select the optimal action for the agent.As the agent observes new information in its ot, the probabilities of detecting AoI are added to the allocentric map, M , which contains all the information that has been discovered throughout the entire training episode.</p>
<p>2) AoI Exploitation πr: Target-Directed Exploration is prioritized until the agent reaches its assigned target region R, or when the step limit of κ1 is reached.Then, the agent transitions to the AoI Exploitation policy πr.Given an Area of Interest (AoI) of unknown size and distribution within a region of the environment, πr aims to maximize AoI coverage while minimizing the number of steps taken without a specific target to reach.This policy uses the same mt architecture as πn.The dynamics of mt, however, differ slightly between the two policies.Since the goal of AoI exploitation is to maximize the probability of detecting AoIs, the agent is allowed to leave its mt if necessary to cover a promising area or venture towards an unknown area.When it leaves its current mt, a new mt+1 is formed.</p>
<p>AoI exploitation utilizes another neural network, with a different state representation, s, and is trained with the Recurrent-A2C algorithm as showed in 2. s is a vector of the ot, and the visited information for each adjacent cell in Vt.The objective function in Eq.3 is triggered after κ2 time steps have elapsed, which is a parameter configurable based on the given environment.The AoI exploitation task is considered complete once the function computes that the best region to explore is no longer the agent's current region.</p>
<p>C. Reward Design</p>
<p>Our reward design is focused on improving adaptive exploration by rewarding the agent for detecting new AoI and visiting new areas of the map.We define the reward for the Target-Directed Exploration, Rn, to minimize the time spent to reach the assigned target region R while maximizing the exploration of new AoIs along the way.Given x as the position that the RL algorithm is evaluating:
Rn =          r AoI if p(AoI|x) &gt; −r visited * V (x) if V (x) &gt; 1 r reach T L if x ∈ TL r reach R if x ∈ R(4)
Where r AOI is the weighted reward value for locating AoI, −r visited for revisiting location x, r reach T L for reaching the cell TL, and r reach R for reaching the target region R. We define the reward for the AoI Exploitation agent, Re, so that it maximizes coverage of the total region, with a priority on areas that have higher probability of containing AoI.Thus, Re is identical to Rn but without the reachability rewards of r reach T L and r reach R (which are irrelevant in the exploitation context).</p>
<p>V. EXPERIMENTS</p>
<p>We evaluate our approach against existing techniques for exploration and consider the impacts of the different choices made in our design.We first describe our experimental setup.We describe simulated experiments that measure adaptive exploration via both AoI and total coverage achieved by the UAV.We then present a physical experiment to show the practical feasibility of our model.Finally, we present an ablation study of our approach.</p>
<p>A. Experimental Setup</p>
<p>We trained our approach on 35 randomly simulated maps of AoIs of different shapes and distributions.Without retraining the model, we tested it on 20 new simulated maps, as well as 5 maps taken from Planet Satellite.This second experiment represents an application in which the UAV must detect illegal gold mining activities in Amazon rainforest in Peru (explained in more detail in the next section).We built a random scenario generator to create the training and testing maps, without overlapping.The maps are structured as 180×180 cell grids.The scenario generator randomly places the AoI within the boundaries of the map, using a normal distribution with values for the standard deviation and mean based on environment parameters.Testing is done on a set of maps not seen during training.This allows us to study generalization, i.e. how well our learned policies perform in previously unseen environments.To measure the impact of real noise on our solution, we tested our approach on a real grassy field where the drone searched for plastic tarps, representing AoI.</p>
<p>Action Space.The agent has 5 actions: move forward, move backward, move left, move right, and hover.</p>
<p>Training.To train the proposed approach, we train each policy separately to excel in its respective task of exploration or exploitation.Then, we leverage transfer learning of DRL to train the entire model, balancing between the two tasks.This effectively speeds up our training process.First, we train the Target-Directed Navigation policy πn.This task uses the network architecture in Fig. 2 and Recurrent-DDQN for learning πn.Since our environment is partially observable, and since a single episode of training can reach hundreds of steps, using Recurrent-DDQN helps the RL agent avoid overestimation and allows it to learn a better policy.We use an input layer with a shape of [5,31], followed by two fully-connected layers with 64 and 10 units respectively.The 5 entries represent the last 5 time steps, which is important for the LSTM layer to handle the partial observability of the environment.We have an additional input layer for the flattened mt whose shape is [5,625].That information is processed through two fully-connected layers with 100 units each, which is then concatenated to the 10 output units from above.This new vector is the input into another LSTM layer of 110 units, and then through an output layer of 5 units, representing the 5 possible actions.We trained this policy with the Adam optimizer, with a learning rate of .001, a batch size of 32, and an experience buffer with a maximum length of 2000.The randomness variable Epsilon in Recurrent-DDQN starts at 0.95 and is multiplied by 0.99 at each update step until it is &lt; .01.We also used a discount factor of 0.95.The training results of this task are shown in Fig. 3. Notice that the inevitable unstable training of DDQN [12] has been drastically decreased by using the stacked LSTM layers.</p>
<p>We used Recurrent-A2C to train Region Exploration because of A2C's ability to solve continuous environments, which policygradient methods excel at compared to Q-learning methods.We use the network in Fig. 2 In the Region Exploration network, the architecture shown is for the actor network, which outputs probabilities for each action.There is also a value network, identical to the actor network, which outputs a single number representing the value assigned to the input.We used the Adam optimizer with a learning rate of .00001and a batch size of 1.The training results can be seen in Fig. 3. Once again, we see the significance of included the embedded LSTM stack.</p>
<p>Once the two tasks have been pre-trained separately, to improve how each network interacts with the other, we load the individuallytrained network parameters and run 100 full episodes, using the full model architecture shown in Fig. 2. We then freeze the weights and use them for the testing phase.</p>
<p>Baselines.We tested our approach against 4 baselines; 1) Sweeping policy in which the UAV navigates the map in a zigzag fashion.The UAV begins in the top left, travels to the opposite border horizontally, moves down one space, travels back to the left border, and continues on in this fashion without covering any previously covered cell, until the battery threshold is reached; 2) Random policy which uses the same proposed tasks of Target-Directed exploration and AoI exploitation, but instead of using the DRL policies, randomly picks an action at each time step.This baseline serves to illustrate the success of the DRL networks within our model framework; 3) Curiosity model which explores the entire map using a curiosity reward function as in [28]; 4) Robustness driven exploration which uses probabilistic metric temporal logic and Monte-Carlo algorithm to build a navigation plan for the UAV in unknown environments.This approach has been introduced in [3] to navigate Amazonian forests looking for illegal gold mining activities.We compare our results against this approach using the simulated maps for AoI and the illegal mining detection task using the Planet Satellite maps.</p>
<p>B. Testing Results</p>
<p>We developed a framework for landscape output simulation, which can be used to simulate the output of a neural network on any given landscape image.Like a neural network, this simulation is designed to return a tensor of probabilities for the classification of map features.The map has two classes of features: area of interest (AoI) and non-AoI with adjustable parameters for the density, size, and shape of the AoIs.The simulator can generate maps with the option to add salt-and-pepper noise to the image.(so that situations like "out of focus" or "lights of reflection" can also be simulated) and to make the AoIs gradients or solids, depending on the desired application.Hence, our simulator can be easily adapted to investigate different types of environments.The developed simulator can be used as an alternative black-box tool for probability tensor outputs that may be used in further tasks 1 .We simulated 20 maps of size 180 × 180 for the AoI exploration task.We describe the details of the environments used and then analyze the results of our approach against the baselines.</p>
<p>1) Task 1: AoI Exploration: For this task, AoI with different shapes and densities are placed randomly across the map.The agent's task is to explore the map efficiently and follow encountered AoI when possible.The agent is given 18, 000 time steps to complete it's mission.We note that the battery threshold of a UAV will be largely dependent on hardware specifications and environment size.Based on the physical time constraints in our drone experiments (discussed further in Task 3), 18000 is an appropriate upper bound limit.Furthermore, we show results for AoI coverage throughout the episode in order to demonstrate the success of our approach at different time cutoffs (Fig. 6).A representative flight path generated by our model in this task is shown in Fig. 4.</p>
<p>Table I lists the results for our approach and the baselines tested on 20 maps for this task.Each map was tested 5 times.Results are averaged across all 100 tests.Note that the curiosity model achieved the least AoI coverage in every trial because this model must unsystematically explore the map without using smaller, tractable regions.This results in the drone repeatedly exploring the same areas, which significantly reduces overall coverage.Table I also includes the total map coverage for each model to analyze efficiency.A greater total coverage relative to AoI coverage indicates that more time and resources were spent traversing and imaging irrelevant (i.e.non-AoI) areas.This is particularly notable for the sweeping baseline, which explores near the same amount of the total map as it does specifically AoIs.</p>
<p>In Fig. 6, we show the average AoI coverage at each time step during an episode.These results reveal that our model clearly outperforms the baselines in terms of AoI coverage at every point throughout the mission, further indicating its success.This also suggests that a different battery threshold could be considered without significantly altering the results.2) Task 2: Illegal Mining Detection: We tested our approach on the problem of mapping mercury-based Small-scale Gold Mining (ASGM) in the Amazonian rainforest as described in [3].This application is particularly important as ASGM is a significant contributor to deforestation and environmental degradation in the Amazon [3].Satellite monitoring for ASGM is not possible in cloudy and rainy weather, which is very common in areas like the Amazon rainforest.UAVs can overcome those issues.However, the UAV field of view is significantly smaller than that of a satellite.To use UAVs to collect information in Amazon, acquisition of image data must account for limited flight time, the required storage, and the classification burden of the collected images.In this task, the UAV must explore an area in search of ASGM.The environment is unknown and the only input to the agent is the on-board recognition system.The goal is to maximize the probability of detecting ASGM relative to the exploration effort expended.We tested our approach over five 8 × 8km 2 regions in Peru (Delta, Colorado, Madre de Dios, Inambari,La Pampa) [3].We simulated a flight path over these regions using Planet satellite images.The agent's altitude was kept fixed by setting the field of view to 5 × 5m 2 .</p>
<p>Table I lists the results for our approach and the baselines tested on 20 maps for this task.Each map was tested 5 times, and results were averaged across the 100 episodes.Our approach explored more mining areas than all other models.It also had a smaller standard deviation, indicating it's consistency and stability relative to the others.We show the AoI coverage collected throughout an episode in Fig. 6.On average, our model clearly outperforms the baselines at each time step in this task.Because of the Sweeping policy's high variance, it did achieve similar AoI coverage to our model in some cases.As shown in Table I, the Sweeping policy explored on average 40% of the whole map in order to cover 60% of AoI while our model only explored 20% of the whole map to achieve 79% AoI coverage.This result demonstrates that, even when our model requires more steps, it more accurately and efficiently navigates the region.Since we do consider the limited storage capacity of the UAV, this is a relevant metric.Furthermore, the high variance of the Sweeping model makes it impractical in real-world deployment.When the UAV happens to begin near the highest concentration of mining AoI, it more quickly covers a larger percent.However, when it happens to begin further away from concentrated mining AoI, it is unable to fully explore it within the given time constraints.Since we assume no prior knowledge about the environment, it is important that a model be able to consistently cover AoI, regardless of its start position.Fig. 6 indicates that our model is able to do this, while the Sweeping baseline is not.We show the generated paths for one region -La Pampa -in Fig. 5.The black areas in the map (top right) represent the areas occupied by ASGM while the white represents the forest.Clearly, the agent spent more time exploring ASGM areas and avoided the forest.3) Task 3: Real Experiment: Finally, we deployed our approach in a physical experiment in order to demonstrate its feasibility in a real-world setting.In this test, the drone must explore a soccer field in search of plastic tarps as AoI.The drone used for this task had a Raspberry Pi 3B on-board, equipped with a standard Raspberry Pi camera and image thresholding software to identify colors in HSV format.The testing environment was 50 × 50m 2 in size.We scale this to correspond to a single egocentric map mt in our simulations.The drone flew at an altitude of 10m.This altitude gives a field of view of approximately 10 × 10m 2 , representing the second level of egocentrism of ot.Fig. 7 shows a view of the map mt for this experiment.The drone's task was to use the Target-Directed Exploration policy πn to travel from its start position S to the top right corner, marked TL.This corresponds to exploring one region and moving to the next in our full simulations.</p>
<p>As a baseline, we first test the time taken on a straight path between S and TL.With our drone specifications, this task took 1 minute 8 seconds on average.Similarly, we determined that the drone can systematically cover the entire map -as in the Sweeping  baseline -in 21 minutes 16 seconds.We tested our model under the same drone conditions, averaged over 10 flight missions.The average time to reach TL using πn was 1 minute 15 seconds.The purple path in Fig. 7 represents the drone's path while the shade around the path represents its vision range.Clearly, the drone successfully prioritized the path that had more AoI compared to the straight path to the target.The time taken further indicates that the path generated by our model is still more efficient than traversing every grid.Although this is a simplified experiment compared to our full simulations, it proves the applicability of our approach in Fig. 7: The drone path (in purple) from the start position S to its target T L .The blue tarps represent the AoI, and the desaturated purple around the path represents the drone's vision range.</p>
<p>real world cases with noisy measurements.</p>
<p>C. Ablation Study</p>
<p>We performed ablation studies on our approach to pinpoint what parts of our proposed method contribute to the performance.We consider AoI coverage and total map coverage during testing as metrics for comparison.We perform the ablation on the simulated AoI maps from the AoI exploration task.</p>
<p>1) Egocentric Map m: We check if the agent's training on the egocentric map m is useful for the adaptive exploration model.We test this by comparing to a model that was not given access to egocentric map m information.Fig. 8 shows the difference in AoI coverage collected throughout an episode with and without the egocentric map m.Training with egocentric maps clearly helps the model achieve significantly higher performance in terms of AoI coverage.The agent trained without access to the egocentric map failed to cover above 40% of the AoI on every map.Table II shows the results for AoI coverage after the same 18, 000 time steps from before.The model without the egocentric reached only 34.5% in 18, 000 steps while the egocentric map increased the AoI coverage to 81.4%.</p>
<p>2) Visited Vector V : We also study the impact of using the visited vector Vt as an input to the networks for Target-Directed Exploration and AoI Exploitation (Fig. 9).We consider three different levels of comparison: no visited information, binary visited information, and the full Vt we include in our model.As shown in Fig. 9 and Table II, the model trained without Vt generated exploration policies with notably low AoI coverage throughout the episode and a higher  variance from map to map.Using binary values to represent whether a cell has been visited or not does decrease performance, though we note that this difference is not nearly as significant.Overall, these tests allow us to conclude that including the information about previously visited areas in the training for both πn and πr improves the agent's ability to cover as much of the map as possible while following the AoI when detected.The information from Vt thus helps the agent identify and explore unvisited areas of the map, which pushes the agent towards exploring new areas, ultimately achieving the objective of adaptive exploration.</p>
<p>3) Recurrent Layers (LSTM): We next investigate the effect of stacking recurrent layers into the nueral networks for both RL algorithms -DDQN and A2C.As before, Table II and   its performance in terms of the total coverage and AoI coverage.We refer back to Fig. 3, which further reveals the importance of the LSTM layers in terms of model training.Overall, the LSTM stack provides the DDQN and A2C networks with a long term memory to "remember" places visited previously in the current episode.Specifically in Target-Directed Exploration, this allow the agent to rationalize its actions in order to decrease distance to the target region with each step.Furthermore, in both tasks the LSTM helps the agent avoid getting stuck in one area, and continue to move to places it hasn't seen before.</p>
<p>VI. CONCLUSION</p>
<p>In this paper, we presented an adaptive exploration approach for UAVs searching for areas of interest (AoI) in unknown environments using Deep Reinforcement Learning (DRL).The developed approach started by decomposing the environment into small regions to make this complex problem tractable for RL algorithms.We then employed two deep neural networks, each selecting an action according to one of two navigation tasks: Target-Directed Exploration and AoI Exploitation.The trade-off between these two tasks was controlled using an efficient information gain function which repeatedly computed the best target region to search.We trained the Target-Directed Exploration and AoI Exploitation tasks using Recurrent-DDQN and Recurrent-A2C algorithms, respectively.We tested the proposed approach in 3 different tasks: simulated AoI exploration, ASGM detection, and a real experiment using a drone exploring a field in search of several tarps.We compared our approach results in each task against 4 different baselines from literature.The results showed that our approach outperforms other models in terms of search time, AoI coverage, and overall efficiency.In future work, we intend to add more complexity to the environment by introducing obstacles and adversarial force such as in military settings.We also plan to extend the solution to consider a swarm of UAVs using multi-agent RL algorithms.</p>
<p>Fig. 1 :
1
Fig. 1: Abstraction of the environment map into regions.Red areas on the map are simulated AoIs.The blue squares represent the egocentric maps mt.The enlarged square shows the AoI distribution information stored in the map mt.The red square around the agent represents its current observation ot.</p>
<p>Fig. 2 :
2
Fig. 2: The network architecture for training Recurrent-DDQN and Recurrent-A2C in Target-Directed Exploration and Region Exploitation.The information gain function determines which network to prioritize at time step t.</p>
<p>Fig. 3 :
3
Fig. 3: Training performance of Recurrent-DDQN vs DDQN in Target-Directed Exploration task (top) and A2C training in AoI Exploitation (bottom)</p>
<p>Fig. 4 :
4
Fig. 4: Sample path generated by the UAV (left).Darker areas represent places visited more than once.Map simulation (right).Red areas on the map represent abstracted AoIs.</p>
<p>Fig. 5 :
5
Fig. 5: Representative sample of the generated path in the illegal mining detection task (top left).Darker areas represent places visited more than once.Map classification (top right).Dark areas represent AoIs related to mining.Satellite map of the area from Amazonian forests in Peru (bottom).</p>
<p>Fig. 6 :
6
Fig. 6: AoI coverage achieved by each model throughout a single mission for the simulated AoI exploration task(top) and the mining detection task (bottom).</p>
<p>Fig. 8 :
8
Fig. 8: Average AoI coverage achieved throughout the episode with each configuration of the egocentric map m.</p>
<p>Fig. 9: Average AoI coverage achieved throughout the episode with each configuration of the Visited Vector V.</p>
<p>Fig. 10 :
10
Fig. 10: Average AoI coverage achieved throughout the episode with and without using an LSTM stack.</p>
<p>TABLE I :
I
Coverage achieved in 18,000 steps.We report both AoI coverage and total coverage for each model as an indicator of its efficiency.Each value is an average and standard deviation computed from 100 episodes
Simulated AoI DetectionIllegal Mining DetectionModelAverage CoverageAverage CoverageOur ModelAoI Coverage:0.81385± 0.067400.79779± 0.06462Total Coverage0.3051± 0.039590.2038± 0.1394SweepingAoI Coverage:0.55502±0.071470.6023± 0.1936Total Coverage:0.5883± 0.13490.40253± 0.29538RandomAoI Coverage:0.3558± 0.06530.1043± 0.07419Total Coverage:0.1708± 0.040450.1213± 0.0893Curiosity [28]AoI Coverage:0.08192± 0.05250.07299± 0.07833Total Coverage:0.03809± 0.015870.02892± 0.02504RDE [3]AoI Coverage:0.83807±0.073540.63522± 0.05159Total Coverage:0.65466±0.032540.63530± 0.05145</p>
<p>TABLE II :
II
Ablation Study: Average map coverage and AoI coverage at the 18,000 step threshold.Each value is an average and standard deviation computed from 50 episodes
ModelAoI CoverageTotal CoverageNo V0.3214± 0.05430.1671± 0.0169No m0.3449± 0.06480.1401± 0.0185No LSTM0.2723± 0.08610.0802± 0.0261Full Model0.8135± 0.06740.3051± 0.0396
https://github.com/RL-WFU/Drone Simulation.git</p>
<p>Learning exploration policies for navigation. T Chen, S Gupta, A Gupta, 2019</p>
<p>A fully-autonomous aerial robot for search and rescue applications in indoor environments using learning-based techniques. C Sampedro Pérez, A Rodríguez Ramos, H Bavle, A Carrio, P De La Puente, P Campoy, Journal of Intelligent &amp; Robotic Systems. 952018</p>
<p>Robustness-driven exploration with probabilistic metric temporal logic. X Liu, P Shi, S Alqahtani, V P Pauca, M Silman, 2019</p>
<p>Trajectory optimization for autonomous flying base station via reinforcement learning. H Bayerlein, P De Kerret, D Gesbert, 2018 IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC). 2018</p>
<p>Autonomous navigation of uav by using real-time model-based reinforcement learning. N Imanberdiyev, C Fu, E Kayacan, I Chen, 2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV). 2016</p>
<p>Reinforcement learning for autonomous uav navigation using function approximation. H X Pham, H M La, D Feil-Seifer, L Van Nguyen, 2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR). 2018</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, 2018A Bradford BookCambridge, MA, USA</p>
<p>Nonmyopic active learning of gaussian processes: An exploration-exploitation approach. A Krause, C Guestrin, Proceedings of the 24th International Conference on Machine Learning, ICML '07. the 24th International Conference on Machine Learning, ICML '07New York, NY, USAAssociation for Computing Machinery2007</p>
<p>Adaptive multi-robot wide-area exploration and mapping. K H Low, J M Dolan, P Khosla, AAMAS '08. Richland, SCInternational Foundation for Autonomous Agents and Multiagent Systems2008</p>
<p>Active learning is planning: Nonmyopic -bayes-optimal active learning of gaussian processes. T N Hoang, K H Low, P Jaillet, M Kankanhalli, Machine Learning and Knowledge Discovery in Databases (T. Calders, F. Esposito. E Hüllermeier, R Meo, Berlin, Heidelberg; Berlin HeidelbergSpringer2014</p>
<p>Bayesian optimisation for informative continuous path planning. R Marchant, F Ramos, 2014 IEEE International Conference on Robotics and Automation (ICRA). 2014</p>
<p>Deep recurrent q-learning for partially observable mdps. M Hausknecht, P Stone, 2015</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation1997</p>
<p>Temporal logic motion planning in unknown environments. A I M Ayala, S B Andersson, C Belta, 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2013</p>
<p>Minimum-violation scltl motion planning for mobility-ondemand. C Vasile, J Tumova, S Karaman, C Belta, D Rus, 2017 IEEE International Conference on Robotics and Automation (ICRA). 2017</p>
<p>Receding horizon "next-best-view" planner for 3d exploration. A Bircher, M Kamel, K Alexis, H Oleynikova, R Siegwart, 2016 IEEE International Conference on Robotics and Automation (ICRA). 2016</p>
<p>Efficient autonomous exploration planning of large-scale 3-d environments. M Selin, M Tiger, D Duberg, F Heintz, P Jensfelt, IEEE Robotics and Automation Letters. 422019</p>
<p>Guiding autonomous exploration with signal temporal logic. F S Barbosa, D Duberg, P Jensfelt, J Tumova, IEEE Robotics and Automation Letters. 442019</p>
<p>Task allocation in uncertain environments using a quadtree and flow network. S Alqahtani, I Riley, S Taylor, R Gamble, R Mailler, 2018 International Conference on Unmanned Aircraft Systems (ICUAS). </p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, 2016</p>
<p>Learning to navigate in complex environments. P Mirowski, R Pascanu, F Viola, H Soyer, A J Ballard, A Banino, M Denil, R Goroshin, L Sifre, K Kavukcuoglu, D Kumaran, R Hadsell, 2017</p>
<p>Learning to fly by crashing. D Gandhi, L Pinto, A Gupta, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2017</p>
<p>Semi-parametric topological memory for navigation. N Savinov, A Dosovitskiy, V Koltun, 2018</p>
<p>Unsupervised human detection with an embedded vision system on a fully autonomous uav for search and rescue operations. E Lygouras, N Santavas, A Taitzoglou, K Tarchanidis, A Mitropoulos, A Gasteratos, Sensors. 192019</p>
<p>Deep reinforcement learning with double q-learning. H Van Hasselt, A Guez, D Silver, 2015</p>
<p>Neural map: Structured memory for deep reinforcement learning. E Parisotto, R Salakhutdinov, 2017</p>
<p>Mapnet: An allocentric spatial memory for mapping environments. J F Henriques, A Vedaldi, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018</p>
<p>Large-scale study of curiosity-driven learning. Y Burda, H Edwards, D Pathak, A Storkey, T Darrell, A A Efros, 2018</p>            </div>
        </div>

    </div>
</body>
</html>
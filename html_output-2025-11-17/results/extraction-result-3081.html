<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3081 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3081</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3081</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267365459</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.00157v4.pdf" target="_blank">Large Language Models for Mathematical Reasoning: Progresses and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence. In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems. However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings. This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field. This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain. To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3081.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3081.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits step-by-step intermediate reasoning traces from LLMs, improving performance on multi-step mathematical and logical problems by making latent reasoning explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (GPT-3, GPT-3.5, GPT-4, PaLM, LLaMA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a prompt engineering strategy to pre-trained autoregressive and encoder-decoder LLMs to generate intermediate rationales (textual scratchpad) before the final answer; used across commercial and open-source LLMs in cited evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought', 'few-shot chain-of-thought', 'zero-shot chain-of-thought']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt-based technique that instructs the model to output a sequence of reasoning steps (a rationale) before the final answer. Implemented in few-shot (provide CoT exemplars) or zero-shot variants (instruction like "explain your reasoning").</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single primary style but often combined with ensembles (self-consistency) or tool-usage; paper reports CoT as a foundational reasoning style that is then augmented by diverse strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Math word problems and benchmarks (e.g., GSM8K, MATH, SVAMP, MATHVISTA examples cited)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step arithmetic and algebraic problems requiring intermediate reasoning steps; benchmarks used to evaluate emergent chain-like reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported qualitatively: CoT substantially improves LLM performance on many multi-step math benchmarks compared to direct answer prompting; exact accuracies are reported in cited works (Wei et al., 2022 and follow-ups) but not enumerated in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Survey contrasts CoT alone vs CoT combined with aggregation (self-consistency), external tools, and fine-tuning; CoT + aggregation/tooling generally improves robustness and accuracy versus single CoT traces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT is a major prompting advance enabling better multi-step reasoning; however, single CoT traces can be brittle and variable across runs, motivating aggregation (self-consistency), verifiers, and tool-augmented approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT outputs can be inconsistent across runs for the same prompt/question; instruction-tuned models show improved CoT robustness but may degrade on more complex problems; adversarial perturbations can still break CoT-based performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3081.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency decoding for Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time ensemble technique that samples multiple independent Chain-of-Thought reasoning paths from an LLM and aggregates final answers to increase probability of the correct solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (applied typically to GPT-family and similar autoregressive LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied at decoding time by sampling multiple CoT chains and using a voting/consensus mechanism over final answers instead of relying on a single sample; compatible with models that can produce diverse CoT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['self-consistency (ensemble of CoT)', 'sampling-based aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate many CoT chains via stochastic decoding, extract their final answers, and pick the most frequent answer (or apply other consensus rules) to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>uses diversity of sampled reasoning paths (multiple similar style CoT traces) rather than changing reasoning style; considered a strategy to leverage diverse outputs from the same style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and other multistep math benchmarks (as used in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks consisting of multi-step grade-school math word problems where sampling multiple reasoning trajectories can expose correct solutions among noisy outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Qualitatively reported to improve accuracy over single CoT samples in cited work (Wang et al., 2023); exact numeric gains are reported in the original paper but not provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to single-chain CoT, self-consistency consistently improves correctness by aggregating multiple reasoning traces; contrasted with alternative approaches like verifier models and tool use in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregation over multiple CoT paths is an effective way to increase reliability without changing model weights; leverages diversity of outputs to boost final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Relies on model's ability to produce diverse correct paths; if model consistently generates incorrect yet similar chains, self-consistency will not help; sampling increases compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3081.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-of-Thought prompting (PoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that separates symbolic/computational steps from reasoning by eliciting program-like outputs (code or program fragments) which are then executed by an external computer to obtain exact numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>primarily Codex / code-capable LLMs and models with code-generation abilities (applied to GPT-family code models in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM is prompted to output a program or computation sequence (e.g., Python) that encodes the reasoning and arithmetic; the program is executed externally to yield precise computation results, reducing arithmetic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['program-of-thought (program synthesis + execution)', 'separation of reasoning and computation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt LLM to express the reasoning as executable code/program; delegate arithmetic/computation to a runtime (external executor) to avoid model-native numerical mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>introduces a different style (programmatic) distinct from textual CoT; uses diverse reasoning style (programs) vs textual rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Numerical and math word problems requiring exact computation (benchmarks cited include MATH-like and arithmetic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks benefit from precise computation and algorithmic steps which can be expressed and executed as code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported qualitatively: PoT reduces arithmetic and execution errors and improves numeric accuracy compared to plain textual CoT in cited studies (Chen et al., 2023a); no numeric values reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to plain CoT, PoT reduces computation errors by delegating arithmetic to execution, often yielding more accurate numerical answers; survey also contrasts PoT with program-synthesis prompts and code-based verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Disentangling computation from reasoning via program outputs is effective for numerical reliability; combining program generation with execution is a strong approach for math-heavy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Requires robust code-generation; syntax or semantic errors in generated programs can still produce failures; adds dependency on an external executor and engineering overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3081.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-based self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit code-based self-verification (GPT-4 Code Interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot prompting technique that instructs GPT-4 Code Interpreter to emit and execute code to self-verify intermediate steps and final answers, improving correctness on challenging math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Code Interpreter (GPT-4 with execution environment)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A version of GPT-4 capable of generating and executing code; the method asks the model to write code that verifies or recomputes steps to confirm answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['code-based self-verification', 'program synthesis + execution + self-checks']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model produces code that recomputes or checks intermediate results; executed code either validates or corrects the model's textual reasoning, forming an internal verification loop.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Combines textual CoT-style reasoning with programmatic verification — two distinct yet complementary reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Challenging math word problems and multi-step MWP (cases in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Problems where errors commonly arise during computation or step aggregation; benefit from code-level checks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported to further boost GPT-4 Code Interpreter performance on hard MWPs compared to non-verified CoT; survey reports effectiveness qualitatively but does not provide numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to pure CoT or PoT without verification, code-based self-verification improved correctness, especially on computation-heavy problems; positioned as complementary to PoT and external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly asking a code-capable LLM to self-verify by executing computations increases reliability on complex math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Depends on availability of execution environment and correct code generation; verification loops add latency and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3081.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-augmented (Python REPL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-augmented reasoning using external Python REPL (LPML)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integrating Chain-of-Thought prompts with an external Python REPL (via a markup language) to perform exact computation and correct intermediate errors, improving ChatGPT's math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LPML: llm-prompting markup language for mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT and similar conversational LLMs augmented with a Python REPL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The LLM generates annotated CoT reasoning that includes code or computation directives interpreted/executed by a REPL; the markup language standardizes the interaction between text CoT and executable tool calls.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought + external tool (Python REPL)', 'LLM-to-tool markup-mediated execution']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM outputs include segments intended for execution in an external environment; results feed back into the reasoning trace to correct or confirm steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Combines textual reasoning and programmatic/tool-based computation — a deliberately diverse multi-modal reasoning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>SVAMP and other MWP benchmarks (tool-augmented evaluations mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step MWPs where precise calculation or symbolic manipulation is required and where textual-only CoT is error-prone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported qualitatively to correct many CoT arithmetic errors and improve ChatGPT reasoning when integrated via LPML; specific numeric improvements are cited in the original tool-augmentation papers but not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Tool-augmented CoT outperforms plain CoT in cited demonstrations; compared with PoT and code-based self-verification, tool-augmentation via REPL offers a lightweight external execution interface.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding an external executor (Python REPL) to CoT prompts increases numerical correctness and reduces arithmetic mistakes from text-only reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Tool integration requires careful prompt/tool protocol; incorrect interpretation of tool outputs by the LLM can still produce erroneous final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3081.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Backward vs Forward reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backward (reverse/backward-chaining) versus forward reasoning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical finding that LLMs (GPT-4, GPT-3.5, PaLM-2, LLaMA in cited work) show significantly lower accuracy when asked to perform backward reasoning (construct a problem from a target answer or reason backward) compared to forward reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fill in the blank: Exploring and enhancing LLM capabilities for backward reasoning in math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, PaLM-2, LLaMA (as analyzed in Deb et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>These are large pre-trained LLMs evaluated in a study contrasting forward (standard solving) and backward (reverse or backward-chaining) reasoning modes; the survey reports the cited finding without running original experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['forward reasoning (standard problem solving)', 'backward reasoning (reverse-generation / backward-chaining)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Forward: derive answer from problem statement; Backward: given target or partial information, infer missing premises or generate problems that lead to a solution — requires different inference direction and often more planning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Comparison of two distinct reasoning directions/styles rather than variants of the same style; demonstrates sensitivity to reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MWP generation and backward reasoning evaluations (experiments in cited paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks that require generating or reasoning in reverse (from solution to premises) vs standard forward solving of MWP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Cited result: significant drop in accuracy for backward reasoning across analyzed LLMs compared to forward reasoning; survey does not provide numeric accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct contrast: forward >> backward in accuracy for models tested (GPT-4, GPT-3.5, PaLM-2, LLaMA); indicates that some reasoning directions are inherently harder for current LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are weaker at backward/reverse reasoning relative to forward problem solving; method directionality matters and can expose limitations even in powerful models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Backward reasoning failures persist even for top-tier models; some models may perform better on forward reasoning despite being larger, showing lack of uniform generalization across reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3081.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minerva</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minerva (math-specialized fine-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LLM approach that trains on diverse math datasets with intermediate step annotations to elicit step-by-step solutions and improve complex mathematical problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving quantitative reasoning problems with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Minerva (fine-tuned decoder-only LLM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language models fine-tuned on datasets annotated with intermediate steps/rationales (e.g., synthetic and human CoT data) to improve generation of stepwise mathematical solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['fine-tuning to generate intermediate steps (scratchpad)', 'chain-of-thought style fine-tuning']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models are fine-tuned to produce intermediate computations/steps as explicit output (scratchpad) during generation, improving capability on longer and more complex math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Employs a single style (stepwise/scratchpad) but achieves diversity by training on heterogeneous annotated datasets; contrasted with purely prompt-based CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Complex quantitative reasoning tasks and benchmarks (e.g., MATH and GSM8K contexts cited)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring multi-step derivations, symbolic manipulation, and sustained logical reasoning across many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported qualitatively: fine-tuning on intermediate-step data (Minerva) substantially improves problem-solving and ability to produce coherent intermediate rationale versus non-fine-tuned baselines; survey does not list numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Fine-tuning to produce scratchpads competes with prompt-based CoT; survey positions Minerva as an effective weight-updating approach compared to purely prompting frozen LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training LLMs to output intermediate steps (rather than only prompting) is an effective route to boost mathematical reasoning, especially on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Fine-tuned models can overfit to annotation styles and may rely on dataset biases; generalization across datasets/grades remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3081.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2 generator-evaluator loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 sequential fine-tuning as generator-evaluator-generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training pipeline where PaLM 2 is fine-tuned sequentially to generate solutions, then to evaluate (score) solutions, and then to generate again using evaluator feedback, improving correctness discrimination and final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving large language model fine-tuning for solving math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's large language model family (PaLM 2) that was experimentally fine-tuned in sequential stages for generation and evaluation to improve mathematical answer correctness as reported by Liu et al. (2023c).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['generator model', 'verifier/evaluator fine-tuning', 'generator conditioned on evaluator feedback']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Pipeline trains separate or same-arch models to first produce solution candidates, then learn to assign probabilities/grades to candidates (verifier), and then re-generate conditioned on evaluator signals to improve final answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Combines generation and verification (two complementary styles) within a training/fine-tuning pipeline; thus uses diverse roles rather than a single reasoning style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Math word problems and multi-attempt generation tasks (as discussed in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tasks where model both proposes candidate solutions and must accurately judge correctness; evaluator-guided re-generation aims to improve final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Survey reports that sequential fine-tuning (generator → evaluator → generator) improved correctness in PaLM 2 in cited experiments; no specific numeric metrics given in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to a single generator-only fine-tune, incorporating an evaluator and re-generation improves the model's ability to produce and select correct answers; aligns with other verifier-based approaches (Cobbe et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning an explicit verification/evaluation component and using it in a generation loop improves final-answer correctness and helps models discriminate correct from incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with evaluators, models may still generate correct solutions without reliably identifying them; multiple attempts help but do not fully solve calibration/verification challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3081.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3081.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PROMPTPG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PROMPTPG (policy-gradient selection of in-context examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that dynamically learns to select effective in-context examples for few-shot prompting by optimizing a selection policy via policy gradients interacting with an LLM API, improving few-shot stability on semi-structured math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (few-shot API use) in cited application</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Does not change model weights; learns a policy for selecting which examples to include in the prompt for in-context learning, using reinforcement learning to maximize downstream task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['in-context example selection (learned)', 'few-shot prompting optimization']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Policy gradient agent interacts with LLM API to try candidate sets of in-context examples, receives task reward (accuracy), and learns to choose example subsets that improve few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Operates within the in-context learning paradigm but seeks diversity by adaptively selecting examples; does not change reasoning style but modifies prompt composition to elicit better reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TABMWP (tabular math word problems) and other semi-structured MWP datasets</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Semi-structured math problems involving tables and multiple-format contexts where in-context exemplar choice strongly affects few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported to improve few-shot GPT-3 performance and stability on TABMWP by learning prompt example selection; survey gives qualitative description without numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to heuristic or random in-context example selection, PROMPTPG yields better few-shot performance by learning selection policies via policy gradient interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatically learning which in-context exemplars to present to an LLM can mitigate few-shot instability and boost performance on complex semi-structured math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Requires many API interactions for policy learning (compute/costly); learned policies may not generalize across datasets without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mathematical Reasoning: Progresses and Challenges', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. <em>(Rating: 2)</em></li>
                <li>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. <em>(Rating: 2)</em></li>
                <li>LPML: llm-prompting markup language for mathematical reasoning. <em>(Rating: 2)</em></li>
                <li>Fill in the blank: Exploring and enhancing LLM capabilities for backward reasoning in math word problems. <em>(Rating: 2)</em></li>
                <li>Solving quantitative reasoning problems with language models. <em>(Rating: 2)</em></li>
                <li>Improving large language model fine-tuning for solving math problems. <em>(Rating: 2)</em></li>
                <li>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. <em>(Rating: 2)</em></li>
                <li>Solving math word problems by combining language models with symbolic solvers. <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 1)</em></li>
                <li>PROMPTPG (as cited under Lu et al. 2023b - Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning). <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3081",
    "paper_id": "paper-267365459",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that elicits step-by-step intermediate reasoning traces from LLMs, improving performance on multi-step mathematical and logical problems by making latent reasoning explicit.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "mention",
            "model_name": "various LLMs (GPT-3, GPT-3.5, GPT-4, PaLM, LLaMA, etc.)",
            "model_description": "Applied as a prompt engineering strategy to pre-trained autoregressive and encoder-decoder LLMs to generate intermediate rationales (textual scratchpad) before the final answer; used across commercial and open-source LLMs in cited evaluations.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought",
                "few-shot chain-of-thought",
                "zero-shot chain-of-thought"
            ],
            "reasoning_methods_description": "Prompt-based technique that instructs the model to output a sequence of reasoning steps (a rationale) before the final answer. Implemented in few-shot (provide CoT exemplars) or zero-shot variants (instruction like \"explain your reasoning\").",
            "diversity_of_methods": "single primary style but often combined with ensembles (self-consistency) or tool-usage; paper reports CoT as a foundational reasoning style that is then augmented by diverse strategies.",
            "reasoning_task_name": "Math word problems and benchmarks (e.g., GSM8K, MATH, SVAMP, MATHVISTA examples cited)",
            "reasoning_task_description": "Multi-step arithmetic and algebraic problems requiring intermediate reasoning steps; benchmarks used to evaluate emergent chain-like reasoning behavior.",
            "performance_by_method": "Reported qualitatively: CoT substantially improves LLM performance on many multi-step math benchmarks compared to direct answer prompting; exact accuracies are reported in cited works (Wei et al., 2022 and follow-ups) but not enumerated in this survey.",
            "comparison_of_methods": "Survey contrasts CoT alone vs CoT combined with aggregation (self-consistency), external tools, and fine-tuning; CoT + aggregation/tooling generally improves robustness and accuracy versus single CoT traces.",
            "key_findings": "CoT is a major prompting advance enabling better multi-step reasoning; however, single CoT traces can be brittle and variable across runs, motivating aggregation (self-consistency), verifiers, and tool-augmented approaches.",
            "counter_examples_or_negative_results": "CoT outputs can be inconsistent across runs for the same prompt/question; instruction-tuned models show improved CoT robustness but may degrade on more complex problems; adversarial perturbations can still break CoT-based performance.",
            "uuid": "e3081.0",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency decoding for Chain-of-Thought",
            "brief_description": "An inference-time ensemble technique that samples multiple independent Chain-of-Thought reasoning paths from an LLM and aggregates final answers to increase probability of the correct solution.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "various LLMs (applied typically to GPT-family and similar autoregressive LLMs)",
            "model_description": "Applied at decoding time by sampling multiple CoT chains and using a voting/consensus mechanism over final answers instead of relying on a single sample; compatible with models that can produce diverse CoT outputs.",
            "model_size": null,
            "reasoning_methods": [
                "self-consistency (ensemble of CoT)",
                "sampling-based aggregation"
            ],
            "reasoning_methods_description": "Generate many CoT chains via stochastic decoding, extract their final answers, and pick the most frequent answer (or apply other consensus rules) to improve robustness and accuracy.",
            "diversity_of_methods": "uses diversity of sampled reasoning paths (multiple similar style CoT traces) rather than changing reasoning style; considered a strategy to leverage diverse outputs from the same style.",
            "reasoning_task_name": "GSM8K and other multistep math benchmarks (as used in cited work)",
            "reasoning_task_description": "Benchmarks consisting of multi-step grade-school math word problems where sampling multiple reasoning trajectories can expose correct solutions among noisy outputs.",
            "performance_by_method": "Qualitatively reported to improve accuracy over single CoT samples in cited work (Wang et al., 2023); exact numeric gains are reported in the original paper but not provided in this survey.",
            "comparison_of_methods": "Compared to single-chain CoT, self-consistency consistently improves correctness by aggregating multiple reasoning traces; contrasted with alternative approaches like verifier models and tool use in the survey.",
            "key_findings": "Aggregation over multiple CoT paths is an effective way to increase reliability without changing model weights; leverages diversity of outputs to boost final-answer accuracy.",
            "counter_examples_or_negative_results": "Relies on model's ability to produce diverse correct paths; if model consistently generates incorrect yet similar chains, self-consistency will not help; sampling increases compute cost.",
            "uuid": "e3081.1",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Program-of-Thought",
            "name_full": "Program-of-Thought prompting (PoT)",
            "brief_description": "A prompting strategy that separates symbolic/computational steps from reasoning by eliciting program-like outputs (code or program fragments) which are then executed by an external computer to obtain exact numeric answers.",
            "citation_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "mention_or_use": "mention",
            "model_name": "primarily Codex / code-capable LLMs and models with code-generation abilities (applied to GPT-family code models in cited work)",
            "model_description": "LM is prompted to output a program or computation sequence (e.g., Python) that encodes the reasoning and arithmetic; the program is executed externally to yield precise computation results, reducing arithmetic errors.",
            "model_size": null,
            "reasoning_methods": [
                "program-of-thought (program synthesis + execution)",
                "separation of reasoning and computation"
            ],
            "reasoning_methods_description": "Prompt LLM to express the reasoning as executable code/program; delegate arithmetic/computation to a runtime (external executor) to avoid model-native numerical mistakes.",
            "diversity_of_methods": "introduces a different style (programmatic) distinct from textual CoT; uses diverse reasoning style (programs) vs textual rationales.",
            "reasoning_task_name": "Numerical and math word problems requiring exact computation (benchmarks cited include MATH-like and arithmetic tasks)",
            "reasoning_task_description": "Tasks benefit from precise computation and algorithmic steps which can be expressed and executed as code.",
            "performance_by_method": "Reported qualitatively: PoT reduces arithmetic and execution errors and improves numeric accuracy compared to plain textual CoT in cited studies (Chen et al., 2023a); no numeric values reported in survey.",
            "comparison_of_methods": "Compared to plain CoT, PoT reduces computation errors by delegating arithmetic to execution, often yielding more accurate numerical answers; survey also contrasts PoT with program-synthesis prompts and code-based verification.",
            "key_findings": "Disentangling computation from reasoning via program outputs is effective for numerical reliability; combining program generation with execution is a strong approach for math-heavy tasks.",
            "counter_examples_or_negative_results": "Requires robust code-generation; syntax or semantic errors in generated programs can still produce failures; adds dependency on an external executor and engineering overhead.",
            "uuid": "e3081.2",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Code-based self-verification",
            "name_full": "Explicit code-based self-verification (GPT-4 Code Interpreter)",
            "brief_description": "A zero-shot prompting technique that instructs GPT-4 Code Interpreter to emit and execute code to self-verify intermediate steps and final answers, improving correctness on challenging math problems.",
            "citation_title": "Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification.",
            "mention_or_use": "mention",
            "model_name": "GPT-4 Code Interpreter (GPT-4 with execution environment)",
            "model_description": "A version of GPT-4 capable of generating and executing code; the method asks the model to write code that verifies or recomputes steps to confirm answers.",
            "model_size": null,
            "reasoning_methods": [
                "code-based self-verification",
                "program synthesis + execution + self-checks"
            ],
            "reasoning_methods_description": "Model produces code that recomputes or checks intermediate results; executed code either validates or corrects the model's textual reasoning, forming an internal verification loop.",
            "diversity_of_methods": "Combines textual CoT-style reasoning with programmatic verification — two distinct yet complementary reasoning styles.",
            "reasoning_task_name": "Challenging math word problems and multi-step MWP (cases in cited work)",
            "reasoning_task_description": "Problems where errors commonly arise during computation or step aggregation; benefit from code-level checks.",
            "performance_by_method": "Reported to further boost GPT-4 Code Interpreter performance on hard MWPs compared to non-verified CoT; survey reports effectiveness qualitatively but does not provide numeric results.",
            "comparison_of_methods": "Compared to pure CoT or PoT without verification, code-based self-verification improved correctness, especially on computation-heavy problems; positioned as complementary to PoT and external tools.",
            "key_findings": "Explicitly asking a code-capable LLM to self-verify by executing computations increases reliability on complex math tasks.",
            "counter_examples_or_negative_results": "Depends on availability of execution environment and correct code generation; verification loops add latency and complexity.",
            "uuid": "e3081.3",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Tool-augmented (Python REPL)",
            "name_full": "Tool-augmented reasoning using external Python REPL (LPML)",
            "brief_description": "Integrating Chain-of-Thought prompts with an external Python REPL (via a markup language) to perform exact computation and correct intermediate errors, improving ChatGPT's math reasoning.",
            "citation_title": "LPML: llm-prompting markup language for mathematical reasoning.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT and similar conversational LLMs augmented with a Python REPL",
            "model_description": "The LLM generates annotated CoT reasoning that includes code or computation directives interpreted/executed by a REPL; the markup language standardizes the interaction between text CoT and executable tool calls.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought + external tool (Python REPL)",
                "LLM-to-tool markup-mediated execution"
            ],
            "reasoning_methods_description": "LLM outputs include segments intended for execution in an external environment; results feed back into the reasoning trace to correct or confirm steps.",
            "diversity_of_methods": "Combines textual reasoning and programmatic/tool-based computation — a deliberately diverse multi-modal reasoning pipeline.",
            "reasoning_task_name": "SVAMP and other MWP benchmarks (tool-augmented evaluations mentioned)",
            "reasoning_task_description": "Multi-step MWPs where precise calculation or symbolic manipulation is required and where textual-only CoT is error-prone.",
            "performance_by_method": "Reported qualitatively to correct many CoT arithmetic errors and improve ChatGPT reasoning when integrated via LPML; specific numeric improvements are cited in the original tool-augmentation papers but not enumerated here.",
            "comparison_of_methods": "Tool-augmented CoT outperforms plain CoT in cited demonstrations; compared with PoT and code-based self-verification, tool-augmentation via REPL offers a lightweight external execution interface.",
            "key_findings": "Adding an external executor (Python REPL) to CoT prompts increases numerical correctness and reduces arithmetic mistakes from text-only reasoning.",
            "counter_examples_or_negative_results": "Tool integration requires careful prompt/tool protocol; incorrect interpretation of tool outputs by the LLM can still produce erroneous final answers.",
            "uuid": "e3081.4",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Backward vs Forward reasoning",
            "name_full": "Backward (reverse/backward-chaining) versus forward reasoning evaluation",
            "brief_description": "An empirical finding that LLMs (GPT-4, GPT-3.5, PaLM-2, LLaMA in cited work) show significantly lower accuracy when asked to perform backward reasoning (construct a problem from a target answer or reason backward) compared to forward reasoning.",
            "citation_title": "Fill in the blank: Exploring and enhancing LLM capabilities for backward reasoning in math word problems.",
            "mention_or_use": "mention",
            "model_name": "GPT-4, GPT-3.5, PaLM-2, LLaMA (as analyzed in Deb et al., 2023)",
            "model_description": "These are large pre-trained LLMs evaluated in a study contrasting forward (standard solving) and backward (reverse or backward-chaining) reasoning modes; the survey reports the cited finding without running original experiments.",
            "model_size": null,
            "reasoning_methods": [
                "forward reasoning (standard problem solving)",
                "backward reasoning (reverse-generation / backward-chaining)"
            ],
            "reasoning_methods_description": "Forward: derive answer from problem statement; Backward: given target or partial information, infer missing premises or generate problems that lead to a solution — requires different inference direction and often more planning.",
            "diversity_of_methods": "Comparison of two distinct reasoning directions/styles rather than variants of the same style; demonstrates sensitivity to reasoning style.",
            "reasoning_task_name": "MWP generation and backward reasoning evaluations (experiments in cited paper)",
            "reasoning_task_description": "Tasks that require generating or reasoning in reverse (from solution to premises) vs standard forward solving of MWP.",
            "performance_by_method": "Cited result: significant drop in accuracy for backward reasoning across analyzed LLMs compared to forward reasoning; survey does not provide numeric accuracies.",
            "comparison_of_methods": "Direct contrast: forward &gt;&gt; backward in accuracy for models tested (GPT-4, GPT-3.5, PaLM-2, LLaMA); indicates that some reasoning directions are inherently harder for current LLMs.",
            "key_findings": "LLMs are weaker at backward/reverse reasoning relative to forward problem solving; method directionality matters and can expose limitations even in powerful models.",
            "counter_examples_or_negative_results": "Backward reasoning failures persist even for top-tier models; some models may perform better on forward reasoning despite being larger, showing lack of uniform generalization across reasoning styles.",
            "uuid": "e3081.5",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Minerva",
            "name_full": "Minerva (math-specialized fine-tuned LLM)",
            "brief_description": "A fine-tuned LLM approach that trains on diverse math datasets with intermediate step annotations to elicit step-by-step solutions and improve complex mathematical problem solving.",
            "citation_title": "Solving quantitative reasoning problems with language models.",
            "mention_or_use": "mention",
            "model_name": "Minerva (fine-tuned decoder-only LLM variants)",
            "model_description": "Large language models fine-tuned on datasets annotated with intermediate steps/rationales (e.g., synthetic and human CoT data) to improve generation of stepwise mathematical solutions.",
            "model_size": null,
            "reasoning_methods": [
                "fine-tuning to generate intermediate steps (scratchpad)",
                "chain-of-thought style fine-tuning"
            ],
            "reasoning_methods_description": "Models are fine-tuned to produce intermediate computations/steps as explicit output (scratchpad) during generation, improving capability on longer and more complex math problems.",
            "diversity_of_methods": "Employs a single style (stepwise/scratchpad) but achieves diversity by training on heterogeneous annotated datasets; contrasted with purely prompt-based CoT.",
            "reasoning_task_name": "Complex quantitative reasoning tasks and benchmarks (e.g., MATH and GSM8K contexts cited)",
            "reasoning_task_description": "Benchmarks requiring multi-step derivations, symbolic manipulation, and sustained logical reasoning across many steps.",
            "performance_by_method": "Reported qualitatively: fine-tuning on intermediate-step data (Minerva) substantially improves problem-solving and ability to produce coherent intermediate rationale versus non-fine-tuned baselines; survey does not list numeric scores.",
            "comparison_of_methods": "Fine-tuning to produce scratchpads competes with prompt-based CoT; survey positions Minerva as an effective weight-updating approach compared to purely prompting frozen LLMs.",
            "key_findings": "Training LLMs to output intermediate steps (rather than only prompting) is an effective route to boost mathematical reasoning, especially on complex tasks.",
            "counter_examples_or_negative_results": "Fine-tuned models can overfit to annotation styles and may rely on dataset biases; generalization across datasets/grades remains limited.",
            "uuid": "e3081.6",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PaLM2 generator-evaluator loop",
            "name_full": "PaLM 2 sequential fine-tuning as generator-evaluator-generator",
            "brief_description": "A training pipeline where PaLM 2 is fine-tuned sequentially to generate solutions, then to evaluate (score) solutions, and then to generate again using evaluator feedback, improving correctness discrimination and final answers.",
            "citation_title": "Improving large language model fine-tuning for solving math problems.",
            "mention_or_use": "mention",
            "model_name": "PaLM 2",
            "model_description": "Google's large language model family (PaLM 2) that was experimentally fine-tuned in sequential stages for generation and evaluation to improve mathematical answer correctness as reported by Liu et al. (2023c).",
            "model_size": null,
            "reasoning_methods": [
                "generator model",
                "verifier/evaluator fine-tuning",
                "generator conditioned on evaluator feedback"
            ],
            "reasoning_methods_description": "Pipeline trains separate or same-arch models to first produce solution candidates, then learn to assign probabilities/grades to candidates (verifier), and then re-generate conditioned on evaluator signals to improve final answer quality.",
            "diversity_of_methods": "Combines generation and verification (two complementary styles) within a training/fine-tuning pipeline; thus uses diverse roles rather than a single reasoning style.",
            "reasoning_task_name": "Math word problems and multi-attempt generation tasks (as discussed in the cited work)",
            "reasoning_task_description": "Tasks where model both proposes candidate solutions and must accurately judge correctness; evaluator-guided re-generation aims to improve final-answer accuracy.",
            "performance_by_method": "Survey reports that sequential fine-tuning (generator → evaluator → generator) improved correctness in PaLM 2 in cited experiments; no specific numeric metrics given in the survey text.",
            "comparison_of_methods": "Compared to a single generator-only fine-tune, incorporating an evaluator and re-generation improves the model's ability to produce and select correct answers; aligns with other verifier-based approaches (Cobbe et al.).",
            "key_findings": "Learning an explicit verification/evaluation component and using it in a generation loop improves final-answer correctness and helps models discriminate correct from incorrect outputs.",
            "counter_examples_or_negative_results": "Even with evaluators, models may still generate correct solutions without reliably identifying them; multiple attempts help but do not fully solve calibration/verification challenges.",
            "uuid": "e3081.7",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PROMPTPG",
            "name_full": "PROMPTPG (policy-gradient selection of in-context examples)",
            "brief_description": "A method that dynamically learns to select effective in-context examples for few-shot prompting by optimizing a selection policy via policy gradients interacting with an LLM API, improving few-shot stability on semi-structured math tasks.",
            "citation_title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (few-shot API use) in cited application",
            "model_description": "Does not change model weights; learns a policy for selecting which examples to include in the prompt for in-context learning, using reinforcement learning to maximize downstream task accuracy.",
            "model_size": null,
            "reasoning_methods": [
                "in-context example selection (learned)",
                "few-shot prompting optimization"
            ],
            "reasoning_methods_description": "Policy gradient agent interacts with LLM API to try candidate sets of in-context examples, receives task reward (accuracy), and learns to choose example subsets that improve few-shot performance.",
            "diversity_of_methods": "Operates within the in-context learning paradigm but seeks diversity by adaptively selecting examples; does not change reasoning style but modifies prompt composition to elicit better reasoning.",
            "reasoning_task_name": "TABMWP (tabular math word problems) and other semi-structured MWP datasets",
            "reasoning_task_description": "Semi-structured math problems involving tables and multiple-format contexts where in-context exemplar choice strongly affects few-shot performance.",
            "performance_by_method": "Reported to improve few-shot GPT-3 performance and stability on TABMWP by learning prompt example selection; survey gives qualitative description without numeric results.",
            "comparison_of_methods": "Compared to heuristic or random in-context example selection, PROMPTPG yields better few-shot performance by learning selection policies via policy gradient interactions.",
            "key_findings": "Automatically learning which in-context exemplars to present to an LLM can mitigate few-shot instability and boost performance on complex semi-structured math tasks.",
            "counter_examples_or_negative_results": "Requires many API interactions for policy learning (compute/costly); learned policies may not generalize across datasets without retraining.",
            "uuid": "e3081.8",
            "source_info": {
                "paper_title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification.",
            "rating": 2,
            "sanitized_title": "solving_challenging_math_word_problems_using_gpt4_code_interpreter_with_codebased_selfverification"
        },
        {
            "paper_title": "LPML: llm-prompting markup language for mathematical reasoning.",
            "rating": 2,
            "sanitized_title": "lpml_llmprompting_markup_language_for_mathematical_reasoning"
        },
        {
            "paper_title": "Fill in the blank: Exploring and enhancing LLM capabilities for backward reasoning in math word problems.",
            "rating": 2,
            "sanitized_title": "fill_in_the_blank_exploring_and_enhancing_llm_capabilities_for_backward_reasoning_in_math_word_problems"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models.",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Improving large language model fine-tuning for solving math problems.",
            "rating": 2,
            "sanitized_title": "improving_large_language_model_finetuning_for_solving_math_problems"
        },
        {
            "paper_title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.",
            "rating": 2,
            "sanitized_title": "dynamic_prompt_learning_via_policy_gradient_for_semistructured_mathematical_reasoning"
        },
        {
            "paper_title": "Solving math word problems by combining language models with symbolic solvers.",
            "rating": 1,
            "sanitized_title": "solving_math_word_problems_by_combining_language_models_with_symbolic_solvers"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "PROMPTPG (as cited under Lu et al. 2023b - Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning).",
            "rating": 1,
            "sanitized_title": "promptpg_as_cited_under_lu_et_al_2023b_dynamic_prompt_learning_via_policy_gradient_for_semistructured_mathematical_reasoning"
        }
    ],
    "cost": 0.019404499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Mathematical Reasoning: Progresses and Challenges
16 Sep 2024</p>
<p>Janice Ahn 
The Pennsylvania State University ♢ Temple University</p>
<p>Rishu Verma 
The Pennsylvania State University ♢ Temple University</p>
<p>Renze Lou 
The Pennsylvania State University ♢ Temple University</p>
<p>Di Liu diliu@temple.edu 
The Pennsylvania State University ♢ Temple University</p>
<p>Rui Zhang 
The Pennsylvania State University ♢ Temple University</p>
<p>Wenpeng Yin wenpeng@psu.edu 
The Pennsylvania State University ♢ Temple University</p>
<p>Large Language Models for Mathematical Reasoning: Progresses and Challenges
16 Sep 2024BD661AAF62C3ED722553D01E84190733arXiv:2402.00157v4[cs.CL]
Mathematical reasoning serves as a cornerstone for assessing the fundamental cognitive capabilities of human intelligence.In recent times, there has been a notable surge in the development of Large Language Models (LLMs) geared towards the automated resolution of mathematical problems.However, the landscape of mathematical problem types is vast and varied, with LLM-oriented techniques undergoing evaluation across diverse datasets and settings.This diversity makes it challenging to discern the true advancements and obstacles within this burgeoning field.This survey endeavors to address four pivotal dimensions: i) a comprehensive exploration of the various mathematical problems and their corresponding datasets that have been investigated; ii) an examination of the spectrum of LLM-oriented techniques that have been proposed for mathematical problem-solving; iii) an overview of factors and concerns affecting LLMs in solving math; and iv) an elucidation of the persisting challenges within this domain.To the best of our knowledge, this survey stands as one of the first extensive examinations of the landscape of LLMs in the realm of mathematics, providing a holistic perspective on the current state, accomplishments, and future challenges in this rapidly evolving field.</p>
<p>Introduction</p>
<p>Mathematical reasoning is crucial to human intelligence, driving ongoing efforts in the AI community to autonomously tackle math challenges.This pursuit inherently calls for an augmentation of AI capabilities, delving into the intricate realms of textual comprehension, image interpretation, tabular analysis, symbolic manipulation, operational logic, and a nuanced grasp of world knowledge.As the AI landscape evolves, the endeavor to empower machines with a comprehensive understanding of diverse mathematical facets becomes not only a testament to technological prowess but also a pivotal stride towards achieving a more generalized and adept AI.</p>
<p>In recent times, the landscape of AI has been reshaped by the ascendancy of Large Language Models (LLMs) as formidable tools for automating intricate tasks.Notably, LLMs have proven to be potent assets in unraveling the nuances of mathematical problem-solving (Romera-Paredes et al., 2023;Imani et al., 2023).Their language capabilities fuel focused exploration in utilizing them for mathematical reasoning, uncovering fresh insights into the synergy between language and logic.</p>
<p>However, amid this progress, the current state of LLM-oriented research in mathematics presents a complex panorama.Diverse mathematical problem types pose a formidable challenge, exacerbated by the varied evaluation metrics, datasets, and settings employed in the assessment of LLM-oriented techniques (Testolin, 2023;Lu et al., 2023c).The lack of a unified framework hampers our ability to gauge the true extent of progress achieved and impedes a coherent understanding of the challenges that persist in this evolving field.</p>
<p>This survey endeavors to cast a spotlight on the multifaceted landscape of LLMs in the realm of mathematics.We plan to traverse four crucial dimensions: a meticulous exploration of math problem types and the datasets associated with them; an in-depth analysis of the evolving techniques employed by LLMs in mathematical problem-solving; an examination of factors that affect the LLMs solving math problems; and a critical discussion on the persisting challenges that loom over this burgeoning field.</p>
<p>To our knowledge, this survey marks one of the first comprehensive examinations of LLMs specifically tailored for mathematics.By weaving together insights from various dimensions, we aim to provide a holistic understanding of the current state of affairs in LLM-driven mathematical reasoning, shedding light on achievements, challenges, and the uncharted territories that await exploration in this captivating intersection of language and logic.</p>
<p>Related Work</p>
<p>To the best of our knowledge, the existing literature on summarizing mathematical research, particularly within the context of LLMs, remains limited.Notably, Frieder et al. (2023a) compared two ChatGPT versions (9-January-2023 and 30-January-2023) and GPT-4 for four math-related problems: producing proofs, filling holes in proofs, acting as a mathematical search engine and computation.More importantly, they summarized some insightful strategies regarding how LLMs can help mathematicians and advocated a more collaborative approach, incorporating human expertise and LLM automation, for theorem proving.Chang et al. (2023) conducted a comprehensive evaluation of LLMs, incorporating an examination of their performance in mathematical problem-solving, albeit with a relatively brief exploration of the mathematical field.Conversely, both (Testolin, 2023) and (Lu et al., 2023c) delved into the application of Deep Learning in the domain of mathematical reasoning.Our work distinguishes itself on three fronts: firstly, we concentrate on LLMs, providing a more in-depth analysis of their various advancements; secondly, beyond merely reporting progress, we engage in a thorough discussion of the challenges inherent in this trajectory; and thirdly, we extend our scrutiny to encompass the perspective of mathematics pedagogy.In doing so, we contribute a nuanced perspective that seeks to broaden the understanding of LLMs in the context of mathematical research.</p>
<p>The only work contemporaneous with ours is (Liu et al., 2023b).In comparison, our contribution lies in: i) not only introducing various methods but also paying more attention to various factors affecting model performance; ii) taking a broader perspective on the progress of LLM in the field of mathematics, elucidating not only from the AI perspective but also from the perspective of education.It emphasizes that the pursuit of model performance alone, while neglecting human factors, is something that needs attention.</p>
<p>Math Problems &amp; Datasets</p>
<p>This section concisely overviews prominent mathematical problem types and associated datasets, spanning ARITHMETIC, MATH WORD PROB-LEMS, GEOMETRY, AUTOMATED THEOREM PROVING, and MATH IN VISION CONTEXT.</p>
<p>Arithmetic</p>
<p>This category of problems entails pure mathematical operations and numerical manipulation, devoid of the need for the model to interpret text, images, or other contextual elements.An illustrative example is presented below, where "Q" denotes questions and "A" for answers.</p>
<p>Q: 21 + 97 A: 118</p>
<p>The dataset MATH-140 (Yuan et al., 2023) contains 401 arithmetic expressions for 17 groups.</p>
<p>Math Word Problems</p>
<p>MATH WORD PROBLEMS (MWP) are mathematical exercises or scenarios presented in the form of written or verbal descriptions rather than straightforward equations in ARITHMETIC.These problems require individuals to decipher the information provided, identify relevant mathematical concepts, and formulate equations or expressions to solve the given problem.MWP often reflect realworld situations, allowing individuals to apply mathematical principles to practical contexts.Solving these problems typically involves critical thinking, problem-solving skills, and the application of mathematical operations to find a solution.</p>
<p>MWP invariably comprise a question (Q) and its corresponding final answer (A) (referred to as Question-Answer).However, the presence or absence of additional clues can give rise to various versions of these problems.Variations may emerge based on factors such as the availability of an equation (E; referred to as Question-Equation-Answer) or the provision of a step-by-step rationale (R; Question-Rationale-Answer) to guide the problemsolving process.</p>
<p>Question-Answer.The instance of this type of MWP consists of a question (Q) and the final answer (A), such as: Q: Lily received $20 from her mum.After spending $10 on a storybook and $2.5 on a lollipop, how much money does she have left?A: $7.5  (Koncel-Kedziorski et al., 2015) 508 E ADDSUB (Hosseini et al., 2014) 395 E Only addition and subtraction MULTIARITH (Roy and Roth, 2015) 600 E Multi-step reasoning DRAW-1K (Upadhyay and Chang, 2017) 1K E MATH23K (Wang et al., 2017) 23K E Chinese APE210K (Zhao et al., 2020) 210K E Chinese K6 (Yang et al., 2023) 600 E Chinese; grade 1-6 CM17K (Qin et al., 2021) 17K M H Chinese; grade 6-12</p>
<p>Question-Rationale-Answer CARP (Zhang et al., 2023a) 4.9K M Chinese GSM8K (Cobbe et al., 2021) 8.5K M Linguistically diverse MATH (Hendrycks et al., 2021) 12.5K H Problems are put into difficulty levels 1-5 PRM800K (Lightman et al., 2023) 12K H MATH w/ step-wise labels MATHQA (Amini et al., 2019) 37K C GRE examinations; have quality concern AQUA (Ling et al., 2017) 100K C GRE&amp;GMAT questions ARB (Sawada et al., 2023) 105 C Contest problems and university math proof GHOSTS (Frieder et al., 2023b) 709 C THEOREMQA-MATH (Chen et al., 2023b) 442 C Theorem as rationale LILA (Mishra et al.,  Question-Equation-Answer.Compared with Question-Answer, this MWP type provides the equation solution, such as Q: Jack had 8 pens and Mary had 5 pens.Jack gave 3 pens to Mary.How many pens does Jack have now?
E: 8 − 3 A: 5 (optional)
Question-Rationale-Answer. This type of MWP includes answers and reasoning paths, akin to the Chain-of-Thought method, which explicates reasoning steps rather than defining problem types (Wei et al., 2022).The rationale guides correct problem-solving and serves as a valuable reference for model training, including fine-tuning and few-shot learning.</p>
<p>Q: Beth bakes 4, or 2 dozen batches of cookies in a week.If these cookies are shared amongst 16 people equally, how many cookies does each person consume?R: Beth bakes 4 2 dozen batches of cookies for a total of 4 * 2 =&lt;&lt; 4 * 2 = 8 &gt;&gt; 8 dozen cookies.There are 12 cookies in a dozen and she makes 8 dozen cookies for a total of 12 * 8 =&lt;&lt; 12 * 8 = 96 &gt;&gt; 96 cookies.She splits the 96 cookies equally amongst 16 people so they each eat 96/16 =&lt;&lt; 96/16 = 6 &gt;&gt; 6 cookies.</p>
<p>A: 6</p>
<p>Table 1 lists most datasets that are summarized in three categories: Question-Answer, Question-Equation-Answer, and Question-Rationale-Answer. In addition to the above three MWP types of conventional styles, recent work studied MWP in given tables and even MWP generation.</p>
<p>Tabular MWP.TABMWP (Lu et al., 2023b) is the first dataset to study MWP over tabular context on open domains and is the largest in terms of data size.Each problem in TABMWP is accompanied by a tabular context, which is represented in three formats: an image, a semi-structured text, and a structured table.T : Table 2 Q: Henrik bought 2.5 kilograms of oval beads.How much did he spend?(Unit: $) A: 5 MWP Generation.Instead of deriving the answer for a given math question, this type of mathematical reasoning tries to generate MWP questions.</p>
<p>For example, Wang et al. (2021) fine-tuned GPT-2 (Radford et al., 2019) on equation-to-MWP instances for MWP generation.The effectiveness of GPT-3's question-generation capabilities was assessed by Zong and Krishnamachari (2023), who instructed the model to generate a question similar to a provided MWP question.Deb et al. (2023) analyzed a group of LLMs (GPT-4, GPT-3.5, PaLM-2 (Anil et al., 2023), and LLaMa (Touvron et al., 2023a)), and found a significant drop in accuracy for backward reasoning compared to forward reasoning.Norberg et al. (2023) used GPT-4 to rewrite human-written MWP, reporting optimal readability, lexical diversity, and cohesion scores, although GPT-4 rewrites incorporated more low-frequency words.</p>
<p>Geometry</p>
<p>Compared with MWP, GEOMETRY problems involve a distinct set of challenges.While MWP often requires logical reasoning and arithmetic operations, geometry problems demand a spatial understanding of shapes, sizes, and their interrelationships.Solving geometry problems typically entails applying geometric principles, theorems, and formulas to analyze and deduce properties of geometric figures.Furthermore, current geometry approaches mainly rely on symbolic methods and NAME SIZE GEOSHADER (Alvin et al., 2017) 102 GEOS (Seo et al., 2015) 186 GEOS++ (Sachan et al., 2017) 1.4K GEOS-OS (Sachan and Xing, 2017) 2.2K GEOMETRY3K (Lu et al., 2021) 3K GEOQA (Chen et al., 2021a) 5K UNIGEO (Chen et al., 2022) 14.5K</p>
<p>Automated theorem proving</p>
<p>In the specialized area of Automated Theorem Proving (ATP), the inherent challenges are unique and encompass a wide spectrum, akin to those found in distinct mathematical fields.ATP's core focus is on autonomously constructing proofs for specified conjectures, requiring a blend of logical analysis and a profound grasp of formal languages, supported by an extensive knowledge base.Its application is crucial in areas like the validation and development of both software and hardware systems.</p>
<p>For example, the MINIF2F dataset (Zheng et al., 2022) stands out in ATP, featuring a series of complex Olympiad-level mathematical problems, designed to evaluate theorem-proving systems including Metamath (Yu et al., 2023), Lean (Han et al., 2022), and Isabelle (Wenzel et al., 2008).In a similar vein, the HOList benchmark (Bansal et al., 2019), with its comprehensive array of theorem statements from various corpora, sets a sequential proving challenge for ATP systems, where each theorem must be proved using only the lemmas preceding it.Additionally, the COQGYM dataset (Yang and Deng, 2019) provides a broad ATP environment, showcasing a rich collection of more than 71,000 proofs penned by humans, all within the framework of the Coq proof assistant.These datasets illustrate the diverse methodologies and skillsets necessary in ATP, reflecting the multifaceted nature of solving mathematical problems.</p>
<p>Math in vision-language context</p>
<p>CHARTQA (Masry et al., 2022), with 9.6K humanwritten questions and 23.1K model-generated questions have explored a variety of complex reasoning questions that involve several logical and arithmetic operations over charts.MATHVISTA (Lu et al., 2023a): size: 6K; it features seven types of mathematical reasoning: algebraic reasoning, arithmetic reasoning, geometry reasoning, logical reasoning, numeric common sense, scientific reasoning, and statistical reasoning.In addition, fine-grained metadata are available, including question type, answer type, language, source, category, task, grade level, and visual context.</p>
<p>Methodologies</p>
<p>We summarize these methods into three progressive levels: i) Prompting frozen LLMs, ii) Strategies enhancing frozen LLMs, and iii) Fine-tuning LLMs.</p>
<p>Prompting frozen LLMs</p>
<p>We organize prior work by typical LLMs.</p>
<p>GPT-3.Zong and Krishnamachari (2023) evaluated the use of GPT-3, a 175B parameter transformer model for three related challenges pertaining to math word problems: i) classifying word problems, ii) extracting equations from word problems, and iii) generating word problems.</p>
<p>ChatGPT.Shakarian et al. (2023) reported the first independent evaluation of ChatGPT on MWP, and found that ChatGPT's performance changes dramatically based on the requirement to show its work.Cheng and Zhang (2023) assessed Chat-GPT, OpenAI's latest conversational chatbot and LLM, on its performance in elementary-grade arithmetic and logic problems, and found that Chat-GPT performed better than previous models such as InstructGPT (Ouyang et al., 2022) and Minerva (Lewkowycz et al., 2022).</p>
<p>GPT-4.Wu et al. (2023) adapted and evaluated several existing prompting methods to the usage of GPT-4, including a vanilla prompt, Programof-Thoughts prompt (Chen et al., 2023a), and Program Synthesis prompt (Drori et al., 2022).The study by Gu (2023) investigated the capability of GPT-4 to actively engage in math-oriented brainstorming sessions.This includes tasks like identifying new research problems, refining problem formulations, and suggesting potential methods or unconventional solutions, all achieved through iterative ideation with a human partner-a common practice in collaborative brainstorming with other professionals.</p>
<p>Strategies enhancing frozen LLMs</p>
<p>Preprocessing the math question.An et al. (2023a) explored ChatGPT for the dataset SVAMP and observed that substituting numerical expressions with English expressions can elevate the performance.</p>
<p>More advanced prompts.Chain-of-thought (Wei et al., 2022), the first time to steer the LLMs to do step-by-step math reasoning, Self-Consistency (Wang et al., 2023) tried multiple Chain-of-Thought reasoning paths and leverage the consistency mechanism to discover a more probable answer.Zhou et al. (2023a) proposed a novel and effective prompting method, explicit codebased self-verification, to further boost the mathematical reasoning potential of GPT-4 Code Interpreter.This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers.</p>
<p>Using external tool.Yamauchi et al. (2023) employed an external tool, specifically the Python REPL, to correct errors in Chain-of-Thought.Their demonstration highlighted that integrating Chainof-Thought and Python REPL using a markup language improves the reasoning capabilities of ChatGPT.In a related context, He-Yueya et al. (2023) introduced an approach that merges an LLM, Codex (Chen et al., 2021b), capable of progressively formalizing word problems into variables and equations, with an external symbolic solver adept at solving the generated equations.Program-of-Thought (Chen et al., 2023a) separates the computational aspect from the reasoning by utilizing a Language Model (primarily Codex) to articulate the reasoning procedure as a program.The actual computation is delegated to an external computer, responsible for executing the generated programs to arrive at the desired answer.</p>
<p>Improving the whole interaction.Wu et al. (2023) introduced MathChat, a conversational framework designed for chat-based LLMs.In this framework, math problems from the MATH dataset are resolved through a simulated conversation between the model and a user proxy agent.</p>
<p>Considering more comprehensive factors in evaluation.While accuracy is crucial in evaluating LLMs for math problem-solving, it shouldn't be the sole metric.Other important dimensions include: i) Confidence Provision: Imani et al. (2023)'s "MathPromper" boosts LLM performance and confidence by generating algebraic expressions, providing diverse prompts, and evaluating consensus among multiple runs.ii) Verifiable Explanations: Gaur and Saunshi (2023) used concise, verifiable explanations to assess LLM reasoning, revealing their proficiency in zero-shot solving of symbolic MWPand their ability to produce succinct explanations.</p>
<p>Fine-tuning LLMs</p>
<p>Learning to select in-context examples.As indicated by prior research, few-shot GPT-3's performance is susceptible to instability and may decline to near chance levels due to the reliance on incontext examples.This instability becomes more pronounced when dealing with intricate problems such as TABMWP.In addressing this issue, Lu et al. (2023b) introduced PROMPTPG, which can autonomously learn to select effective in-context examples through policy gradient interactions with the GPT-3 API, eliminating the need for manually designed heuristics.</p>
<p>Generating intermediate steps. Nye et al. (2021) initiated the fine-tuning of decoder-only</p>
<p>LLMs, ranging from 2M to 137B in size.Their approach involved training these models to solve integer addition and polynomial evaluation by generating intermediate computation steps into a designated "scratchpad."In a related effort, Zhang et al. (2023b) introduced a fine-tuning strategy for GPT-2 or T5, enabling them to produce step-bystep solutions with a combination of textual and mathematical tokens leading to the final answer.Additionally, Yang et al. (2023) applied a step-bystep strategy in fine-tuning a series of GLM models (Zeng et al., 2023), specifically tailored for solving distinct Chinese mathematical problems.Minerva, developed by Lewkowycz et al. (2022), enhances LLMs' ability to generate intermediate steps in complex math problems.Its fine-tuning of diverse datasets enables nuanced, step-by-step problemsolving, demonstrating advanced handling of intricate mathematical concepts.</p>
<p>Learning an answer verifier.OpenAI researchers, per Cobbe et al. (2021), fine-tuned a GPT-3 model of 175B as a verifier, assigning probabilities to solution candidates.In exploring reexamination processes for MWP solving, Bin et al. (2023) introduced Pseudo-Dual Learning, involving solving and reexamining modules.For MWP solution, Zhu et al. (2023) developed a cooperative reasoning-induced PLM, with GPT-J (Wang and Komatsuzaki, 2021) generating paths and DeBERTa-large (He et al., 2021) supervising evaluation.Google researchers, as per Liu et al. (2023c), observed improved correctness in LLMs with multiple attempts, which hints that LLMs might generate correct solutions while struggling to differentiate between accurate and inaccurate ones.They sequentially fine-tuned their PaLM 2 model (Anil et al., 2023) as a solution generator, evaluator, and generator again.</p>
<p>Learning from enhanced dataset.Emulating the error-driven learning process observed in human learning, An et al. (2023b) conducted finetuning on various open-source LLMs within the LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b), CodeLLaMA (Rozière et al., 2023), WizardMath (Luo et al., 2023), MetaMath (Yu et al., 2023), and Llemma (Azerbayev et al., 2023) families.This fine-tuning utilized mistakecorrection data pairs generated by GPT-4.To mitigate over-reliance on knowledge distillation from LLM teachers, Liang et al. (2023a) finetuned LLaMA-7B on existing mathematical problem datasets that exhibit diverse annotation styles.In a related approach, Raiyan et al. ( 2023) demonstrated that training on linguistic variants of problem statements and implementing a voting mechanism for candidate predictions enhance the mathematical reasoning and overall robustness of the model.</p>
<p>Teacher-Student knowledge distillation.Liang et al. (2023b) utilized GPT-3 to coach a more efficient MWP solver (RoBERTa-based encoderdecoder (Liu et al., 2019)).They shifted the focus from explaining existing exercises to identifying the student model's learning needs and generating new, tailored exercises.The resulting smaller LLM achieves competitive accuracy on the SVAMP dataset with significantly fewer parameters compared to state-of-the-art LLMs.</p>
<p>Finetuning on many datasets.Mishra et al. (2022) conducted fine-tuning on a series of GPT-Neo2.7Bcausal language models (Black et al., 2021) using LILA, a composite of 20 existing math datasets.Similarly, Yue et al. (2023) created "Math-Instruct", a meticulously curated instruction tuning dataset.Comprising 13 math datasets with intermediate Chain-of-Thought and Program-of-Thought rationales, this dataset was used to finetune Llama (Touvron et al., 2023a,b;Rozière et al., 2023) models across different scales.The resulting models demonstrate unprecedented potential in cross-dataset generalization.</p>
<p>Math solver ensemble.Yao et al. (2023) incorporated a problem typing subtask that combines the strengths of the tree-based solver and the LLM solver (ChatGLM-6B (Zeng et al., 2023)).</p>
<p>Analysis</p>
<p>5.1 LLMs's robustness in math Patel et al. (2021) provided strong evidence that the pre-LLM MWP solvers, mostly LSTM-equipped encoder-decoder models, rely on shallow heuristics to achieve high performance on some simple benchmark datasets, then introduced a more challenging dataset, SVAMP, created by applying carefully chosen variations over examples sampled from preceding datasets.Stolfo et al. (2023) observed that, among non-instruction-tuned LLMs, the larger ones tend to be more sensitive to changes in the ground-truth result of a MWP, but not necessarily more robust.However, a different behavior exists in the instruction-tuned GPT-3 models, which show a remarkable improvement in both sensitivity and robustness, although the robustness reduces when problems get more complicated.Wei et al. (2023) assessed the robustness of several top-performing LLMs by augmenting the original problems in the curated CMATH dataset with distracting information.Their findings reveal that GPT-4 can maintain robustness while other models fail.Zhou et al. (2023b) proposed a new dataset RO-BUSTMATH to evaluate the robustness of LLMs in math-solving ability.Extensive experiments show that (i) Adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy; (ii) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (iii) We can improve the robustness of LLMs by using adversarial samples in few-shot prompts.</p>
<p>Factors in influencing LLMs in math</p>
<p>The comprehensive evaluation conducted by Yuan et al. (2023) encompasses OpenAI's GPT series, including GPT-4, ChatGPT2, and GPT-3.5, along with various open-source LLMs.This analysis methodically examines the elements that impact the arithmetic skills of LLMs, covering aspects such as tokenization, pre-training, prompting techniques, interpolation and extrapolation, scaling laws, Chain of Thought (COT), and In-Context Learning (ICL).</p>
<p>Tokenization.This research underscores tokenization's critical role in LLMs' arithmetic performance (Yuan et al., 2023).Models like T5, lacking specialized tokenization for arithmetic, are less effective than those with advanced methods, such as Galactica (Taylor et al., 2022) and LLaMA, which show superior accuracy in arithmetic tasks.This indicates that token frequency in pre-training and the method of tokenization are key to arithmetic proficiency.</p>
<p>Pre-training Corpus.Enhanced arithmetic skills in LLMs correlate with the inclusion of code and LATEX in pre-training data (Yuan et al., 2023).Galactica, heavily utilizing LATEX, excels in arithmetic tasks, while models like Code-DaVinci-002, better at reasoning, lags in arithmetic, highlighting a distinction between arithmetic and reasoning skills.</p>
<p>Prompts.The nature of input prompts greatly affects LLMs' arithmetic performance (Liu et al., 2023a;Lou et al., 2023).Without prompts, performance drops (Yuan et al., 2023).Models like Chat-GPT, which respond well to instructional systemlevel messages, demonstrate the importance of prompt type.Instruction tuning in pre-training also emerges as a significant factor (Yue et al., 2023).</p>
<p>Model Scale.There's a noted correlation between parameter count and arithmetic capability in LLMs (Yuan et al., 2023).Larger models generally perform better, but a performance plateau is observed, as shown by Galactica's similar outcomes at 30B and 120B parameters.However, this doesn't always mean superior performance, with smaller models like ChatGPT occasionally outperforming larger ones.</p>
<p>Perspectives of mathematics pedagogy</p>
<p>While machine learning emphasizes LLMs' problem-solving abilities in mathematics, in practical education, their primary role is to aid learning.Thus, the focus shifts from mere mathematical performance to a crucial consideration of LLMs' understanding of students' needs, capabilities, and learning methods.</p>
<p>Advantages of deploying LLMs in math education.Educators have observed the following benefits of leveraging LLMs for math education.(i) LLMs foster critical thinking and problem-solving skills, as they provide comprehensive solutions and promote rigorous error analysis (Matzakos et al., 2023); (ii) Educators and students prefer LLMgenerated hints because of their detailed, sequential format and clear, coherent narratives (Gattupalli et al., 2023); (iii) LLMs introduce a conversational style in problem-solving, an invaluable asset in math education (Gattupalli et al., 2023); (iv) The impact of LLMs extends beyond mere computational assistance, offering deep insights and understanding spanning diverse disciplines like Algebra, Calculus, and Statistics (Rane, 2023).</p>
<p>Disadvantages of deploying LLMs in math education.(i) Potential for misinterpretation.Misinterpretation of students' queries or errors in providing explanations by LLMs could lead to confusion.</p>
<p>Inaccurate responses might result in the reinforcement of misconceptions, impacting the quality of education (Yen and Hsu, 2023).(ii) Limited understanding of individual learning styles.LLMs may struggle to cater to diverse learning styles, as they primarily rely on algorithms and might not fully grasp the unique needs of each student.Some learners may benefit more from hands-on activities or visual aids that LLMs may not adequately address.Gattupalli et al. (2023) proposed that hints produced by GPT-4 could be excessively intricate for younger students who have shorter attention spans.(iii) Privacy and data security issues.Deploying LLMs involves collecting and analyzing substantial amounts of student data.Privacy concerns may arise if proper measures are not in place to safeguard this data from unauthorized access or misuse.</p>
<p>Challenges</p>
<p>Data-driven &amp; limited generalization.The prevailing trend in current research revolves around the curation of extensive datasets.Despite this emphasis, there is a noticeable lack of robust generalization across various datasets, grade levels, and types of math problems.Examining how humans acquire math-solving skills suggests that machines may need to embrace continual learning to enhance their capabilities.</p>
<p>LLMs' brittleness in math reasoning.The fragility of LLMs in mathematical reasoning is evident across three dimensions.Firstly, when presented with questions expressed in varying textual forms (comprising words and numbers), LLMs exhibit inconsistent performance.Secondly, for identical questions, an LLM may yield different final answers through distinct reasoning paths during multiple trials.Lastly, pre-trained math-oriented LLMs are susceptible to attacks from adversarial inputs, highlighting their vulnerability in the face of manipulated data.</p>
<p>Human-oriented math interpretation.The current LLM-oriented math reasoning, such as chainof-thoughts, does not take into account the needs and comprehension abilities of users, such as students.As an example, Yen and Hsu (2023) discovered that GPT-3.5 had a tendency to misinterpret students' questions in the conversation, resulting in a failure to deliver adaptive feedback.Additionally, research conducted by Gattupalli et al. (2023) revealed that GPT-4 frequently overlooks the practical comprehension abilities of younger students.It tends to generate overly intricate hints that even confuse those students.Consequently, there is a pressing need for increased AI research that actively incorporates human factors into its design, ensuring future developments align more closely with the nuanced requirements of users.</p>
<p>Conclusion</p>
<p>This survey on LLMs for Mathematics delves into various aspects of LLMs in mathematical reasoning, including their capabilities and limitations.The paper discusses different types of math problems, datasets, and the persisting challenges in the domain.It highlights the advancements in LLMs, their application in educational settings, and the need for a human-centric approach in math education.We hope this paper will guide and inspire future research in the LLM community, fostering further advancements and practical applications in diverse mathematical contexts.</p>
<p>GPT4V &amp; Bard.Lu et al. (2023a) presented MATHVISTA, a benchmark of evaluating mathematical reasoning in visual context, conducted a comprehensive, quantitative evaluation of three LLMs (i.e, ChatGPT, GPT-4, Claude-2(Bai et al., 2022)), two proprietary large multimodal models (LMMs) (i.e., GPT4V, Bard), and seven open-source LMMs, with Chain-of-Thought and Program-of-Thought.Multiple.Wei et al. (2023) evaluated a variety of popular LLMs, including both commercial and open-source options, aiming to provide a benchmark tool for assessing the following question: to what grade level of Chinese elementary school math do the abilities of popular LLMs correspond?</p>
<p>Table 1 :
1
Datasets for Math Word Problems.
2022)132KHIncorporates 20 existing datasetsMATH-INSTRUCT (Yue et al., 2023)260KHInstruction-following styleTABMWP (Lu et al., 2023b)38KHTabular MWP; below the College level
E = Elementary, M = Middle School, H = High School, C = College, H = Hybrid</p>
<p>Table 2 :
2
Table for the tabular MWP example.</p>
<p>Table 3 :
3
Geometry datasets
predefined search heuristics, highlighting the spe-cialized strategies required in this domain (Trinhet al., 2024). This contrast in problem-solvingapproaches highlights the multifaceted nature ofmathematical challenges and the varied skill setsrequired in different mathematical domains. Anexample can be seen as follows and Table 3 listsmainstream datasets.ahbcQ: a=7 inches; b=24 inches; c=25 inches;h=6.72 inches; What is its area? (Unit:square inches)A: 84</p>
<p>Synthesis of solutions for shaded area geometry problems. Chris Alvin, Sumit Gulwani, Rupak Majumdar, Supratik Mukhopadhyay, Proceedings of the Thirtieth International Florida Artificial Intelligence Research Society Conference. the Thirtieth International Florida Artificial Intelligence Research Society ConferenceMarco Island, Florida, USAAAAI Press2017. 2017. May 22-24, 2017</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Does chatgpt comprehend the place value in numbers when solving math word problems?. Jisu An, Junseok Lee, Gahgene Gweon, Proceedings of the Workshop "Towards the Future of AI-augmented Human Tutoring in Math Learning" co-located with The 24th International Conference on Artificial Intelligence in Education (AIED 2023). the Workshop "Towards the Future of AI-augmented Human Tutoring in Math Learning" co-located with The 24th International Conference on Artificial Intelligence in Education (AIED 2023)Tokyo, Japan2023a. July 3, 20233491CEUR Workshop Proceedings</p>
<p>Learning from mistakes makes LLM better reasoner. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen, CoRR, abs/2310.206892023b</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, 2023Palm 2 technical report. CoRR, abs/2305.10403</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, Llemma: An open language model for mathematics. 2023</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, CoRR, abs/2204.05862Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan2022Neel Nanda, Catherine Olsson</p>
<p>Holist: An environment for machine learning of higher-order theorem proving. Kshitij Bansal, Sarah M Loos, Markus N Rabe, Christian Szegedy, Stewart Wilcox, 2019</p>
<p>Solving math word problems with reexamination. Yi Bin, Wenhao Shi, Yujuan Ding, Yang Yang, See-Kiong Ng, CoRR, abs/2310.095902023</p>
<p>Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, 2021</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, CoRR, abs/2307.03109A survey on evaluation of large language models. 2023</p>
<p>Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, Xiaodan Liang, Proceedings of EMNLP. EMNLP2022</p>
<p>Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P Xing, Liang Lin, Findings of ACL/IJCNLP, volume ACL/IJCNLP 2021. 2021a</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Joshua Achiam, Vedant Misra, Evan Morikawa, Alec RadfordJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021b. Evaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2023a</p>
<p>Theoremqa: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, Proceedings of EMNLP. EMNLP2023b</p>
<p>Analyzing Chat-GPT's mathematical deficiencies: Insights and contributions. Vincent Cheng, Yu Zhang, Proceedings of the 35th Conference on Computational Linguistics and Speech Processing. the 35th Conference on Computational Linguistics and Speech Processing2023ROCLING 2023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Dinesh Garg, and Parag Singla. 2023. Fill in the blank: Exploring and enhancing LLM capabilities for backward reasoning in math word problems. Aniruddha Deb, Neeva Oza, Sarthak Singla, Dinesh Khandelwal, CoRR, abs/2310.01991</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and fewshot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Proceedings of the National Academy of Sciences. the National Academy of Sciences2022119e2123433119</p>
<p>Large language models for mathematicians. Simon Frieder, Julius Berner, Philipp Petersen, Thomas Lukasiewicz, Internationale Mathematische Nachrichten. 2542023a</p>
<p>Mathematical capabilities of chatgpt. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, CoRR, abs/2301.138672023b</p>
<p>Exploring pre-service teachers' perceptions of large language models-generated hints in online mathematics learning. Sai Gattupalli, William Lee, Danielle Allessio, Danielle Crabtree, Ivon Arroyo, Beverly Woolf, Beverly Woolf, 2023</p>
<p>Reasoning in large language models through symbolic math word problems. Vedant Gaur, Nikunj Saunshi, Findings of ACL. 2023</p>
<p>Llms as potential brainstorming partners for math and science problems. Sophia Gu, CoRR, abs/2310.106772023</p>
<p>Proof artifact cotraining for theorem proving with language models. Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, Stanislas Polu, Proceedings of ICLR. ICLR2022</p>
<p>Deberta: decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Proceedings of ICLR. ICLR2021</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, CoRR, abs/2304.091022023</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of NeurIPS. NeurIPS2021</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of EMNLP. EMNLPACL2014</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, Proceedings of ACL. ACL2023</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , Trans. Assoc. Comput. Linguistics. 32015</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of NAACL. NAACL2016</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Mint: Boosting generalization in mathematical reasoning via multi-view fine-tuning. Zhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao, Qingkai Zeng, Xiangliang Zhang, Dong Yu, CoRR, abs/2307.079512023a</p>
<p>Let GPT be a math tutor: Teaching math word problem solvers with customized exercise generation. Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, Ashwin Kalyan, CoRR, abs/2305.143862023b</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yura Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, CoRR, abs/2305Jan. 2023. 20050Bowen Baker, Teddy Lee</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of ACL. ACL2017</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023a</p>
<p>Mathematical language models: A survey. Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, CoRR, abs/2312.076222023bAimin Zhou, and Liang He</p>
<p>Roberta: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, CoRR, abs/1907.116922019</p>
<p>Improving large language model fine-tuning for solving math problems. Yixin Liu, Avi Singh, C Daniel Freeman, John D Co-Reyes, Peter J Liu, CoRR, abs/2310.100472023c</p>
<p>Is prompt all you need? no. a comprehensive and broader view of instruction learning. Renze Lou, Kai Zhang, Wenpeng Yin, arXiv:2303.104752023arXiv preprint</p>
<p>Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, CoRR, abs/2310.022552023a</p>
<p>Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, Song-Chun Zhu, Proceedings of ACL/IJCNLP. ACL/IJCNLP2021</p>
<p>Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan, Proceedings of ICLR. ICLR2023b</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, Proceedings of ACL. ACL2023c</p>
<p>Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, CoRR, abs/2308.095832023</p>
<p>Chartqa: A benchmark for question answering about charts with visual and logical reasoning. Ahmed Masry, Xuan Do, Jia Long, Shafiq R Qing Tan, Enamul Joty, Hoque, Findings of ACL. 2022</p>
<p>Learning mathematics with large language models: A comparative study with computer algebra systems and other tools. Nikolaos Matzakos, Spyridon Doukakis, Maria Moundridou, International Journal of Emerging Technologies in Learning (iJET). 18202023</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, Proceedings of ACL. ACL2020</p>
<p>LILA: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Ashwin Kalyan, Proceedings of EMNLP. EMNLP2022</p>
<p>Rewriting math word problems with large language models. Kole Norberg, Husni Almoubayyed, Stephen E Fancsali, Logan De Ley, Kyle Weldon, Proceedings of the Workshop on Empowering Education with LLMsthe Next-Gen Interface and Content Generation 2023 co-located with 24th International Conference on Artificial Intelligence in Education (AIED 2023). the Workshop on Empowering Education with LLMsthe Next-Gen Interface and Content Generation 2023 co-located with 24th International Conference on Artificial Intelligence in Education (AIED 2023)Tokyo, JapanApril Murphy, and Steven Ritter. 2023. July 7, 20233487</p>
<p>Show your work: Scratchpads for intermediate computation with language models. I Maxwell, Anders Nye, Guy Johan Andreassen, Henryk Gur-Ari, Jacob Michalewski, David Austin, David Bieber, Aitor Dohan, Maarten Lewkowycz, David Bosma, Charles Luan, Augustus Sutton, Odena, CoRR, abs/2112.001142021</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022In NeurIPS</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of NAACL-HLT. NAACL-HLT2021</p>
<p>Neural-symbolic solver for math word problems with auxiliary tasks. Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng Tang, Liang Lin, Proceedings of ACL/IJCNLP. ACL/IJCNLP2021</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 201919</p>
<p>Math word problem solving by generating linguistic variants of problem statements. Md Nafis Syed Rifat Raiyan, Shah Faiyaz, Jawad Md, Mohsinul Kabir, Kabir, Mahmud Hasan, Md Kamrul Hasan, CoRR, abs/2306.138992023</p>
<p>Enhancing mathematical capabilities through chatgpt and similar generative artificial intelligence: Roles and challenges in solving mathematical problems. Nitin Rane, 10.2139/ssrn.4603237SSRN Electronic Journal. 2023</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, Nature. 2023</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of EMNLP. EMNLP2015</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Martin, CoRR, abs/2308.12950Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023</p>
<p>From textbooks to knowledge: A case study in harvesting axiomatic knowledge from textbooks to solve geometry problems. Mrinmaya Sachan, Avinava Dubey, Eric P Xing, Proceedings of EMNLP. EMNLP2017</p>
<p>Learning to solve geometry problems from natural language demonstrations in textbooks. Mrinmaya Sachan, Eric P Xing, Proceedings of <em>SEM @ACM. </em>SEM @ACM2017</p>
<p>ARB: advanced reasoning benchmark for large language models. Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, Aran Komatsuzaki, CoRR, abs/2307.136922023</p>
<p>Solving geometry problems: Combining text and diagram interpretation. Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, Clint Malcolm, Proceedings of EMNLP. EMNLP2015</p>
<p>An independent evaluation of chatgpt on mathematical word problems (MWP). Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, Lakshmivihari Mareedu, Proceedings of the AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering (AAAI-MAKE 2023). the AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering (AAAI-MAKE 2023)Hyatt Regency; California, USASan Francisco Airport2023. March 27-29, 20233433CEUR Workshop Proceedings</p>
<p>Bernhard Schölkopf, and Mrinmaya Sachan. 2023. A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Proceedings of ACL. ACL</p>
<p>Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, CoRR, abs/2211.09085Galactica: A large language model for science. 2022</p>
<p>Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models. Alberto Testolin, CoRR, abs/2303.077352023</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, CoRR, abs/2302.13971</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, 10.1038/s41586-023-06747-5CoRR, abs/2307.09288Nature. Trieu Trinh, Yuhuai Wu2023bQuoc Le, He He, and Thang Luong. 2024. Solving olympiad geometry without human demonstrations</p>
<p>Annotating derivations: A new evaluation strategy and dataset for algebra word problems. Shyam Upadhyay, Ming-Wei Chang, Proceedings of EACL. EACL2017</p>
<p>Gpt-j-6b: A 6 billion parameter autoregressive language model. Ben Wang, Aran Komatsuzaki, 2021</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, Proceedings of ICLR. ICLR2023</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, Proceedings of EMNLP. EMNLP2017</p>
<p>Math word problem generation with mathematical consistency and problem context constraints. Zichao Wang, Andrew S Lan, Richard G Baraniuk, Proceedings of EMNLP. EMNLP2021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of NeurIPS. NeurIPS2022</p>
<p>CMATH: can your language model pass chinese elementary school math test?. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, Bin Wang, CoRR, abs/2306.166362023</p>
<p>The isabelle framework. Makarius Wenzel, Lawrence C Paulson, Tobias Nipkow, Theorem Proving in Higher Order Logics: 21st International Conference. TPHOLs; Montreal, CanadaSpringer2008. 2008. August 18-21, 200821</p>
<p>An empirical study on challenging math problem solving with GPT-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang, CoRR, abs/2306.013372023</p>
<p>LPML: llm-prompting markup language for mathematical reasoning. Ryutaro Yamauchi, Sho Sonoda, Akiyoshi Sannai, Wataru Kumagai, CoRR, abs/2309.130782023</p>
<p>Jinfeng Bai, and Jie Tang. 2023. GPT can solve mathematical problems without a calculator. Kaiyu Yang, Jia Deng ; Zhen, Ming Yang, Qingsong Ding, Zhihuan Lv, Zehai Jiang, Yuyi He, Guo, CoRR, abs/2309.032412019Learning to prove theorems via interacting with proof assistants</p>
<p>Solving math word problem with problem type classification. Jie Yao, Zihao Zhou, Qiufeng Wang, Proceedings of NLPCC. NLPCC202314304</p>
<p>Three questions concerning the use of large language models to facilitate mathematics learning. An-Zi Yen, Wei-Ling Hsu, CoRR, abs/2310.136152023</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, CoRR, abs/2309.122842023</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, CoRR, abs/2304.020152023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, CoRR, abs/2309.056532023</p>
<p>GLM-130B: an open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, Proceedings of ICLR. ICLR2023</p>
<p>Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen, arXiv:2306.02408Evaluating and improving tool-augmented computation-intensive math reasoning. 2023aarXiv preprint</p>
<p>Interpretable math word problem solution generation via step-by-step planning. Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi Feng, Andrew S Lan, Proceedings of ACL. ACL2023b</p>
<p>Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu, Ape210k: A large-scale and template-rich dataset of math word problems. 2020</p>
<p>Kunhao Zheng, Jesse Michael Han, Stanislas Polu, Minif2f: a cross-system benchmark for formal olympiad-level mathematics. 2022</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, abs/2304.063642023CoRR</p>
<p>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, CoRR, abs/2308.079212023a</p>
<p>Mathattack: Attacking large language models towards math solving ability. Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang, CoRR, abs/2309.016862023b</p>
<p>Solving math word problems via cooperative reasoning induced language models. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, Proceedings of ACL. ACL2023</p>
<p>Solving math word problems concerning systems of equations with GPT-3. Mingyu Zong, Bhaskar Krishnamachari, Proceedings of AAAI. AAAI2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-549 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-549</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-549</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-cb5e3f085caefd1f3d5e08637ab55d39e61234fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cb5e3f085caefd1f3d5e08637ab55d39e61234fc" target="_blank">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate, and shows how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions.</p>
                <p><strong>Paper Abstract:</strong> Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's"hands and eyes,"while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e549.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e549.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan: Do As I Can, Not As I Say (Grounding Language in Robotic Affordances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that composes a pretrained large language model's (LLM) high-level procedural suggestions with learned, language-conditioned affordance value functions (Q-functions) over low-level skills to produce grounded, executable, multi-step plans for a mobile manipulator in real kitchens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretrained transformer LLM (PaLM 540B in the main experiments) used in scoring mode to assign probabilities p(l_pi | i) over candidate textual skill descriptions; LLM is not given visual input, relies on prompt engineering and few-shot examples; FLAN and smaller PaLM variants were also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-kitchen multi-step instruction following (101 tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A mobile manipulator in mock and real office kitchens is given high-level natural language instructions (single primitives to 10+ step requests). The system must decompose the instruction into a sequence of available low-level skills (pick, place, navigate, open/close, etc.), select feasible skills given the current scene, and execute them until completion. Evaluation uses 15 objects and 5 semantic locations across 101 instructions spanning several families (NL nouns/verbs, embodiment, long-horizon, crowd-sourced).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning; instruction following; object manipulation + navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial (combined): LLM provides procedural sequencing and semantic mappings of nouns/attributes; affordance/value functions provide object-relational and spatial feasibility (presence, reachability, location).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM pretraining on large text corpora for procedural/semantic knowledge; learned language-conditioned value functions and BC/RL policies trained from robot demonstrations and simulation for perceptual/world grounding; prompt engineering and in-context examples for elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>LLM scoring of enumerated skill textual labels via prompt engineering (few-shot/structured prompt); iterative planning loop that appends chosen skill to prompt; chain-of-thought prompting variant uses generative decoding of explanations before scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Procedural knowledge implicit in LLM weights and expressed as probabilities over natural-language skill labels; world knowledge/affordances represented explicitly as learned Q-values p(c_pi | s, l_pi) conditioned on language embeddings and RGB observations; policies map language embeddings + vision to low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Planning success rate (human-rated validity of plan) and execution success rate (human-rated success of executed plan).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>PaLM-SayCan: mock kitchen planning success 84%, execution success 74%; real kitchen planning 81%, execution 60% (over 101 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>LLM provided reasonable procedural decompositions and ordering (approach->pick->bring), semantic selection of appropriate objects for abstract requests (e.g., choose a sponge to clean rather than suggesting a vacuum), and selection of object types matching abstract nouns (e.g., 'fruit' -> apple). The affordances reliably filtered out skills whose preconditions were not met (e.g., 'pick X' when X not present).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>LLM-only failures included making suggestions infeasible for the embodiment (e.g., 'go to store'); LLM struggled with negation and ambiguous references; early termination on long-horizon tasks (omitting later steps) was common; affordance misclassification produced embodiment failures (35% of errors attributed to affordances); occasional high-value but failing skills during execution (lack of closed-loop failure handling).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No VF (remove affordance weighting): planning 67% (mock). Generative baseline (LLM generative plan then project via USE): planning 74%. BC NL (policy conditioned on full NL instruction, no planning LLM): 0% execution. BC USE (instruction projected to nearest known command via USE): 9% execution overall. FLAN-SayCan: planning 70%, execution 61%.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing affordance grounding (No VF) reduced planning from 84% to 67%; replacing PaLM with FLAN reduced planning from 84% to 70% and execution from 74% to 61%; replacing scoring with generative decoding+projection reduced interpretability and lowered/varied performance; BC NL (no LLM planner) yielded 0% success, demonstrating necessity of LLM for decomposition; smaller LLM sizes (8B, 62B) performed worse than 540B PaLM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large language models encode rich procedural and object-relational knowledge that can produce useful high-level plans even without sensory input, but they are not grounded to a specific embodiment or scene. Combining LLM task-grounding probabilities with learned perceptual affordances (value functions) yields interpretable, feasible plans and substantially improves planning and execution success in embodied manipulation/navigation tasks. LLM improvements (bigger/better models and chain-of-thought prompting) translate to improved embodied planning when appropriately grounded.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e549.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e549.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Affordance Value Functions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-Conditioned Affordance Value Functions (Skill Q-functions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>TD-trained, language-conditioned Q-value functions that estimate the probability p(c_pi | s, l_pi) that executing a given skill (described by a short text label) will succeed from the current visual/robot state; used as world grounding for LLM proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MT-Opt style RL value functions / Q-network</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learned via temporal-difference RL (MT-Opt style) with sparse binary reward (1 on successful completion of the language-specified skill, 0 otherwise). Networks process RGB images with convolutional layers and accept language embeddings (from a sentence encoder) to output Q-values in [0,1]. Trained with prioritized replay and sim-to-real (RetinaGAN) bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Skill feasibility / affordance estimation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>For each candidate low-level skill (text label), estimate success probability from the current RGB observation and robot state so the planner can prefer feasible actions given the embodiment and scene.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>affordance estimation; grounding for manipulation and navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial + procedural: encodes whether specific objects are present, reachable, and whether a skill sequence is likely to succeed from current spatial configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Learned from robot demonstration data and RL training (simulation + real), using language-conditioned supervision (success/failure labels per language-command).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Querying the learned Q-network with current RGB observation and language embedding of the skill label; used multiplicatively with LLM task probabilities in planning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Explicit scalar Q-values per (state, skill label) pair forming an affordance vector across all skills; Q-values are computed from visual features concatenated with language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contribution to planning/execution success; per-family planning drops when removed (measured via overall planning/execution rates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Inclusion of affordances improved mock-kitchen planning from 67% (No VF) to 84% (PaLM-SayCan). Removing affordances particularly harmed embodiment-family planning (planning dropped to 18% without VF).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Correctly assigned high values to 'pick' skills when the named object is visible and in reach; correctly scored navigation skills when target locations were valid; generated affordance maps that matched scene content (visualized in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Occasional misclassification of affordances leading to choosing infeasible skills (contributed to 35% of total errors); sometimes reported high value for a skill that subsequently failed during closed-loop execution (system lacked fully robust failure detection/replanning).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No VF baseline (selecting max LLM score without affordances) performed worse (67% planning). Generative baseline (LLM generative plan + projection) also inferior and less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing affordance module reduced total planning success (84% -> 67%) and drastically reduced performance on embodiment tests (64% -> 18% planning in that family).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learned, language-conditioned affordance value functions supply the necessary perceptual grounding that LLMs lack: they convert visual/veridical scene information into explicit skill feasibility scores that—when combined multiplicatively with LLM-provided task probabilities—produce feasible, interpretable plans for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e549.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e549.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Task-Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model Skill-Scoring for Task Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an LLM in scoring mode to compute p(l_pi | i) — the probability each candidate low-level skill's textual label is an appropriate next step for a given high-level instruction — thereby providing task-grounding (which skills are semantically relevant).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (primary); FLAN and smaller PaLM variants also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B (PaLM main), also 137B (FLAN), 62B and 8B variants evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained transformer models used in a 'scoring' interface to compute likelihoods of fixed candidate textual completions; prompted with examples and constrained set of admissible skill labels so outputs map directly to executable skills. Also used generative decoding in chain-of-thought variant.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Skill selection (task-grounding) within iterative planner</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given high-level instruction and history of selected skills, the LLM scores each available skill label for how much it progresses the instruction; the planner multiplies this with affordance to choose the next skill, iterating until termination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural decomposition; planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (semantic mapping of words to actions/objects and ordering of steps)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Pretraining on large text corpora; prompt engineering and in-context examples in the plan-generation prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Zero-shot/few-shot prompting and scoring interface (likelihood of candidate completions). For reasoning tasks, generative chain-of-thought decoding used followed by scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit procedural and semantic knowledge encoded in model parameters, output as probability distribution over natural-language skill labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Effect on planning success rate when combined with affordances; ablation comparisons across LLM sizes and types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using PaLM scoring within SayCan yields 84% planning success (mock); replacing PaLM with FLAN reduces planning to 70%; removing LLM (BC NL) yields 0% success, demonstrating LLM's central role in decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully proposed correct next-step skills for many abstract, long-horizon queries; understood ordering and semantic intent (e.g., bring water+apple for 'recover from a workout'), handled attribute-based queries when supported by prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without affordance grounding, LLM proposed infeasible actions (e.g., suggesting to use unavailable tools); struggled with negation and ambiguous attribute references; sometimes terminated early in long multi-step plans.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Generative LLM baseline (produce plan then project to nearest skill via USE) performed worse/less interpretable (planning 74% vs 84% with PaLM-SayCan). BC NL (policy conditioned on full NL instruction) gave 0%.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Smaller or differently fine-tuned LLMs lowered planning success (PaLM 540B > FLAN > smaller PaLMs); chain-of-thought prompting mitigated some LLM reasoning failures (negation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs contain usable procedural and object-relational knowledge for planning but lack perception; using an LLM to score a constrained set of natural-language skill labels extracts this knowledge in a way that can be combined with perceptual affordances to produce feasible, interpretable embodied plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e549.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e549.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought Variant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting Integrated with SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modification to the LLM elicitation pipeline where the LLM first generatively produces an intermediate 'Explanation' (chain-of-thought), which is then appended to the prompt and used in scoring to improve reasoning about attributes and negation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (chain-of-thought prompting applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses generative decoding to elicit explanatory intermediate reasoning steps, then leverages the explanation in the scoring prompt to better rank candidate skill labels; draws on chain-of-thought prompting techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Attribute-based and negation-sensitive instruction decomposition (example: 'bring a fruit-flavored drink without caffeine')</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate natural-language explanation of constraints/criteria, then produce and score candidate skills respecting those constraints to select appropriate items/steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural reasoning; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (reasoning about object attributes and negative constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM pretraining plus in-context chain-of-thought examples inserted into prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Generative chain-of-thought decoding followed by scoring of candidate skill descriptions including the generated explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Short natural-language reasoning traces (explanations) making explicit attribute filters and rationale that guide selection of skills.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Qualitative improvement on negation and attribute-based queries (examples reported); no large-scale numeric breakdown provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Demonstrated successful rollouts on negation/attribute tasks in examples (e.g., correctly selecting a lime soda for 'fruit-flavored drink without caffeine'); explicit quantitative gains not reported but shown as effective in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Handled negation and multi-attribute selection better than vanilla scoring approach; useful for filtering candidate objects by complex semantic conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Not quantified; chain-of-thought may depend on model size and prompt quality; computationally heavier (generative step prior to scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Vanilla SayCan (no chain-of-thought) struggled on negation tasks; chain-of-thought variant improved those examples.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Including 'Explanation' in the prompt and using generative output before scoring improved reasoning in the demonstrated cases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Eliciting explicit intermediate reasoning from LLMs can remedy some of their semantic/reasoning limitations (e.g., negation), enabling better attribute-based selection in embodied planning even though the LLM still lacks direct sensory input; the affordance module remains necessary for perceptual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e549.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e549.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan Open-Source Colab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source SayCan Colab Tabletop Environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A publicly released Colab demo that implements a SayCan-like pipeline in a tabletop UR5 environment using CLIPort low-level policy, ViLD object detector as affordance proxy, and GPT-3 as the LLM to demonstrate the approach in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 is used as the open-source LLM to generate code-structured/natural-language step outputs; CLIPort supplies pick-and-place low-level policies (no learned value function), and ViLD object detector provides affordance-like signals (object presence) in lieu of learned Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tabletop pick-and-place planning demo (open-source)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>UR5 robot in a simulated tabletop environment with colored blocks and bowls must follow LLM-produced step descriptions; CLIPort executes low-level pick-and-place; ViLD provides visual detection signals to approximate affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; instruction following; short-horizon planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (procedural plans from GPT-3; object presence from ViLD detections)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>GPT-3 pretraining for procedural suggestions; ViLD trained for open-vocabulary object detection; CLIPort trained for pick-and-place policies.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>Generative decoding from GPT-3 producing step outputs and code-like plan structures; object detector used as affordance proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Plans output as natural-language or structured 'pick up X and place at Y' sequences; visual affordance approximated by detector scores rather than learned Q-functions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Demonstration / qualitative; no quantitative metrics reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Qualitative demonstration available in the Colab; no numeric results reported in the paper for the open-source demo.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Shows feasibility of integrating an LLM with perception and low-level policies in a simple simulated domain; demonstrates constrained natural-language outputs mapped to executable primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Affordances approximated with an object detector (ViLD) rather than learned Q-values, so behavioral realism and robustness are limited compared to main paper's learned affordance approach.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>N/A (demo environment to illustrate concept).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>N/A for the open-source demo in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The open-source Colab demonstrates the generality of the SayCan idea: an LLM without sensory input can propose procedural plans that, when combined with perception-derived affordance signals and executable low-level policies, yield actionable behaviors in simulated tabletop domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Value function spaces: Skill-centric state abstractions for long-horizon reasoning <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>CLIPort: What and Where Pathways for Robotic Manipulation <em>(Rating: 2)</em></li>
                <li>Open-vocabulary object detection via vision and language knowledge distillation <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling Language Modeling with Pathways <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-549",
    "paper_id": "paper-cb5e3f085caefd1f3d5e08637ab55d39e61234fc",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "SayCan",
            "name_full": "SayCan: Do As I Can, Not As I Say (Grounding Language in Robotic Affordances)",
            "brief_description": "A system that composes a pretrained large language model's (LLM) high-level procedural suggestions with learned, language-conditioned affordance value functions (Q-functions) over low-level skills to produce grounded, executable, multi-step plans for a mobile manipulator in real kitchens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "model_description": "A pretrained transformer LLM (PaLM 540B in the main experiments) used in scoring mode to assign probabilities p(l_pi | i) over candidate textual skill descriptions; LLM is not given visual input, relies on prompt engineering and few-shot examples; FLAN and smaller PaLM variants were also evaluated.",
            "task_name": "Real-kitchen multi-step instruction following (101 tasks)",
            "task_description": "A mobile manipulator in mock and real office kitchens is given high-level natural language instructions (single primitives to 10+ step requests). The system must decompose the instruction into a sequence of available low-level skills (pick, place, navigate, open/close, etc.), select feasible skills given the current scene, and execute them until completion. Evaluation uses 15 objects and 5 semantic locations across 101 instructions spanning several families (NL nouns/verbs, embodiment, long-horizon, crowd-sourced).",
            "task_type": "multi-step planning; instruction following; object manipulation + navigation",
            "knowledge_type": "procedural + object-relational + spatial (combined): LLM provides procedural sequencing and semantic mappings of nouns/attributes; affordance/value functions provide object-relational and spatial feasibility (presence, reachability, location).",
            "knowledge_source": "LLM pretraining on large text corpora for procedural/semantic knowledge; learned language-conditioned value functions and BC/RL policies trained from robot demonstrations and simulation for perceptual/world grounding; prompt engineering and in-context examples for elicitation.",
            "has_direct_sensory_input": false,
            "elicitation_method": "LLM scoring of enumerated skill textual labels via prompt engineering (few-shot/structured prompt); iterative planning loop that appends chosen skill to prompt; chain-of-thought prompting variant uses generative decoding of explanations before scoring.",
            "knowledge_representation": "Procedural knowledge implicit in LLM weights and expressed as probabilities over natural-language skill labels; world knowledge/affordances represented explicitly as learned Q-values p(c_pi | s, l_pi) conditioned on language embeddings and RGB observations; policies map language embeddings + vision to low-level actions.",
            "performance_metric": "Planning success rate (human-rated validity of plan) and execution success rate (human-rated success of executed plan).",
            "performance_result": "PaLM-SayCan: mock kitchen planning success 84%, execution success 74%; real kitchen planning 81%, execution 60% (over 101 tasks).",
            "success_patterns": "LLM provided reasonable procedural decompositions and ordering (approach-&gt;pick-&gt;bring), semantic selection of appropriate objects for abstract requests (e.g., choose a sponge to clean rather than suggesting a vacuum), and selection of object types matching abstract nouns (e.g., 'fruit' -&gt; apple). The affordances reliably filtered out skills whose preconditions were not met (e.g., 'pick X' when X not present).",
            "failure_patterns": "LLM-only failures included making suggestions infeasible for the embodiment (e.g., 'go to store'); LLM struggled with negation and ambiguous references; early termination on long-horizon tasks (omitting later steps) was common; affordance misclassification produced embodiment failures (35% of errors attributed to affordances); occasional high-value but failing skills during execution (lack of closed-loop failure handling).",
            "baseline_comparison": "No VF (remove affordance weighting): planning 67% (mock). Generative baseline (LLM generative plan then project via USE): planning 74%. BC NL (policy conditioned on full NL instruction, no planning LLM): 0% execution. BC USE (instruction projected to nearest known command via USE): 9% execution overall. FLAN-SayCan: planning 70%, execution 61%.",
            "ablation_results": "Removing affordance grounding (No VF) reduced planning from 84% to 67%; replacing PaLM with FLAN reduced planning from 84% to 70% and execution from 74% to 61%; replacing scoring with generative decoding+projection reduced interpretability and lowered/varied performance; BC NL (no LLM planner) yielded 0% success, demonstrating necessity of LLM for decomposition; smaller LLM sizes (8B, 62B) performed worse than 540B PaLM.",
            "key_findings": "Large language models encode rich procedural and object-relational knowledge that can produce useful high-level plans even without sensory input, but they are not grounded to a specific embodiment or scene. Combining LLM task-grounding probabilities with learned perceptual affordances (value functions) yields interpretable, feasible plans and substantially improves planning and execution success in embodied manipulation/navigation tasks. LLM improvements (bigger/better models and chain-of-thought prompting) translate to improved embodied planning when appropriately grounded.",
            "uuid": "e549.0",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Affordance Value Functions",
            "name_full": "Language-Conditioned Affordance Value Functions (Skill Q-functions)",
            "brief_description": "TD-trained, language-conditioned Q-value functions that estimate the probability p(c_pi | s, l_pi) that executing a given skill (described by a short text label) will succeed from the current visual/robot state; used as world grounding for LLM proposals.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MT-Opt style RL value functions / Q-network",
            "model_size": null,
            "model_description": "Learned via temporal-difference RL (MT-Opt style) with sparse binary reward (1 on successful completion of the language-specified skill, 0 otherwise). Networks process RGB images with convolutional layers and accept language embeddings (from a sentence encoder) to output Q-values in [0,1]. Trained with prioritized replay and sim-to-real (RetinaGAN) bootstrapping.",
            "task_name": "Skill feasibility / affordance estimation",
            "task_description": "For each candidate low-level skill (text label), estimate success probability from the current RGB observation and robot state so the planner can prefer feasible actions given the embodiment and scene.",
            "task_type": "affordance estimation; grounding for manipulation and navigation",
            "knowledge_type": "object-relational + spatial + procedural: encodes whether specific objects are present, reachable, and whether a skill sequence is likely to succeed from current spatial configuration.",
            "knowledge_source": "Learned from robot demonstration data and RL training (simulation + real), using language-conditioned supervision (success/failure labels per language-command).",
            "has_direct_sensory_input": true,
            "elicitation_method": "Querying the learned Q-network with current RGB observation and language embedding of the skill label; used multiplicatively with LLM task probabilities in planning loop.",
            "knowledge_representation": "Explicit scalar Q-values per (state, skill label) pair forming an affordance vector across all skills; Q-values are computed from visual features concatenated with language embeddings.",
            "performance_metric": "Contribution to planning/execution success; per-family planning drops when removed (measured via overall planning/execution rates).",
            "performance_result": "Inclusion of affordances improved mock-kitchen planning from 67% (No VF) to 84% (PaLM-SayCan). Removing affordances particularly harmed embodiment-family planning (planning dropped to 18% without VF).",
            "success_patterns": "Correctly assigned high values to 'pick' skills when the named object is visible and in reach; correctly scored navigation skills when target locations were valid; generated affordance maps that matched scene content (visualized in figures).",
            "failure_patterns": "Occasional misclassification of affordances leading to choosing infeasible skills (contributed to 35% of total errors); sometimes reported high value for a skill that subsequently failed during closed-loop execution (system lacked fully robust failure detection/replanning).",
            "baseline_comparison": "No VF baseline (selecting max LLM score without affordances) performed worse (67% planning). Generative baseline (LLM generative plan + projection) also inferior and less interpretable.",
            "ablation_results": "Removing affordance module reduced total planning success (84% -&gt; 67%) and drastically reduced performance on embodiment tests (64% -&gt; 18% planning in that family).",
            "key_findings": "Learned, language-conditioned affordance value functions supply the necessary perceptual grounding that LLMs lack: they convert visual/veridical scene information into explicit skill feasibility scores that—when combined multiplicatively with LLM-provided task probabilities—produce feasible, interpretable plans for embodied agents.",
            "uuid": "e549.1",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "LLM Task-Scoring",
            "name_full": "Language Model Skill-Scoring for Task Grounding",
            "brief_description": "Using an LLM in scoring mode to compute p(l_pi | i) — the probability each candidate low-level skill's textual label is an appropriate next step for a given high-level instruction — thereby providing task-grounding (which skills are semantically relevant).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM (primary); FLAN and smaller PaLM variants also evaluated",
            "model_size": "540B (PaLM main), also 137B (FLAN), 62B and 8B variants evaluated",
            "model_description": "Pretrained transformer models used in a 'scoring' interface to compute likelihoods of fixed candidate textual completions; prompted with examples and constrained set of admissible skill labels so outputs map directly to executable skills. Also used generative decoding in chain-of-thought variant.",
            "task_name": "Skill selection (task-grounding) within iterative planner",
            "task_description": "Given high-level instruction and history of selected skills, the LLM scores each available skill label for how much it progresses the instruction; the planner multiplies this with affordance to choose the next skill, iterating until termination.",
            "task_type": "procedural decomposition; planning",
            "knowledge_type": "procedural + object-relational (semantic mapping of words to actions/objects and ordering of steps)",
            "knowledge_source": "Pretraining on large text corpora; prompt engineering and in-context examples in the plan-generation prompt.",
            "has_direct_sensory_input": false,
            "elicitation_method": "Zero-shot/few-shot prompting and scoring interface (likelihood of candidate completions). For reasoning tasks, generative chain-of-thought decoding used followed by scoring.",
            "knowledge_representation": "Implicit procedural and semantic knowledge encoded in model parameters, output as probability distribution over natural-language skill labels.",
            "performance_metric": "Effect on planning success rate when combined with affordances; ablation comparisons across LLM sizes and types.",
            "performance_result": "Using PaLM scoring within SayCan yields 84% planning success (mock); replacing PaLM with FLAN reduces planning to 70%; removing LLM (BC NL) yields 0% success, demonstrating LLM's central role in decomposition.",
            "success_patterns": "Successfully proposed correct next-step skills for many abstract, long-horizon queries; understood ordering and semantic intent (e.g., bring water+apple for 'recover from a workout'), handled attribute-based queries when supported by prompting.",
            "failure_patterns": "Without affordance grounding, LLM proposed infeasible actions (e.g., suggesting to use unavailable tools); struggled with negation and ambiguous attribute references; sometimes terminated early in long multi-step plans.",
            "baseline_comparison": "Generative LLM baseline (produce plan then project to nearest skill via USE) performed worse/less interpretable (planning 74% vs 84% with PaLM-SayCan). BC NL (policy conditioned on full NL instruction) gave 0%.",
            "ablation_results": "Smaller or differently fine-tuned LLMs lowered planning success (PaLM 540B &gt; FLAN &gt; smaller PaLMs); chain-of-thought prompting mitigated some LLM reasoning failures (negation).",
            "key_findings": "LLMs contain usable procedural and object-relational knowledge for planning but lack perception; using an LLM to score a constrained set of natural-language skill labels extracts this knowledge in a way that can be combined with perceptual affordances to produce feasible, interpretable embodied plans.",
            "uuid": "e549.2",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Chain-of-Thought Variant",
            "name_full": "Chain-of-Thought Prompting Integrated with SayCan",
            "brief_description": "A modification to the LLM elicitation pipeline where the LLM first generatively produces an intermediate 'Explanation' (chain-of-thought), which is then appended to the prompt and used in scoring to improve reasoning about attributes and negation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM (chain-of-thought prompting applied)",
            "model_size": "540B",
            "model_description": "Uses generative decoding to elicit explanatory intermediate reasoning steps, then leverages the explanation in the scoring prompt to better rank candidate skill labels; draws on chain-of-thought prompting techniques.",
            "task_name": "Attribute-based and negation-sensitive instruction decomposition (example: 'bring a fruit-flavored drink without caffeine')",
            "task_description": "Generate natural-language explanation of constraints/criteria, then produce and score candidate skills respecting those constraints to select appropriate items/steps.",
            "task_type": "procedural reasoning; instruction following",
            "knowledge_type": "procedural + object-relational (reasoning about object attributes and negative constraints)",
            "knowledge_source": "LLM pretraining plus in-context chain-of-thought examples inserted into prompt.",
            "has_direct_sensory_input": false,
            "elicitation_method": "Generative chain-of-thought decoding followed by scoring of candidate skill descriptions including the generated explanation.",
            "knowledge_representation": "Short natural-language reasoning traces (explanations) making explicit attribute filters and rationale that guide selection of skills.",
            "performance_metric": "Qualitative improvement on negation and attribute-based queries (examples reported); no large-scale numeric breakdown provided in main text.",
            "performance_result": "Demonstrated successful rollouts on negation/attribute tasks in examples (e.g., correctly selecting a lime soda for 'fruit-flavored drink without caffeine'); explicit quantitative gains not reported but shown as effective in case studies.",
            "success_patterns": "Handled negation and multi-attribute selection better than vanilla scoring approach; useful for filtering candidate objects by complex semantic conditions.",
            "failure_patterns": "Not quantified; chain-of-thought may depend on model size and prompt quality; computationally heavier (generative step prior to scoring).",
            "baseline_comparison": "Vanilla SayCan (no chain-of-thought) struggled on negation tasks; chain-of-thought variant improved those examples.",
            "ablation_results": "Including 'Explanation' in the prompt and using generative output before scoring improved reasoning in the demonstrated cases.",
            "key_findings": "Eliciting explicit intermediate reasoning from LLMs can remedy some of their semantic/reasoning limitations (e.g., negation), enabling better attribute-based selection in embodied planning even though the LLM still lacks direct sensory input; the affordance module remains necessary for perceptual grounding.",
            "uuid": "e549.3",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "SayCan Open-Source Colab",
            "name_full": "Open-source SayCan Colab Tabletop Environment",
            "brief_description": "A publicly released Colab demo that implements a SayCan-like pipeline in a tabletop UR5 environment using CLIPort low-level policy, ViLD object detector as affordance proxy, and GPT-3 as the LLM to demonstrate the approach in simulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": null,
            "model_description": "GPT-3 is used as the open-source LLM to generate code-structured/natural-language step outputs; CLIPort supplies pick-and-place low-level policies (no learned value function), and ViLD object detector provides affordance-like signals (object presence) in lieu of learned Q-values.",
            "task_name": "Tabletop pick-and-place planning demo (open-source)",
            "task_description": "UR5 robot in a simulated tabletop environment with colored blocks and bowls must follow LLM-produced step descriptions; CLIPort executes low-level pick-and-place; ViLD provides visual detection signals to approximate affordances.",
            "task_type": "object manipulation; instruction following; short-horizon planning",
            "knowledge_type": "procedural + object-relational (procedural plans from GPT-3; object presence from ViLD detections)",
            "knowledge_source": "GPT-3 pretraining for procedural suggestions; ViLD trained for open-vocabulary object detection; CLIPort trained for pick-and-place policies.",
            "has_direct_sensory_input": null,
            "elicitation_method": "Generative decoding from GPT-3 producing step outputs and code-like plan structures; object detector used as affordance proxy.",
            "knowledge_representation": "Plans output as natural-language or structured 'pick up X and place at Y' sequences; visual affordance approximated by detector scores rather than learned Q-functions.",
            "performance_metric": "Demonstration / qualitative; no quantitative metrics reported in paper.",
            "performance_result": "Qualitative demonstration available in the Colab; no numeric results reported in the paper for the open-source demo.",
            "success_patterns": "Shows feasibility of integrating an LLM with perception and low-level policies in a simple simulated domain; demonstrates constrained natural-language outputs mapped to executable primitives.",
            "failure_patterns": "Affordances approximated with an object detector (ViLD) rather than learned Q-values, so behavioral realism and robustness are limited compared to main paper's learned affordance approach.",
            "baseline_comparison": "N/A (demo environment to illustrate concept).",
            "ablation_results": "N/A for the open-source demo in the paper.",
            "key_findings": "The open-source Colab demonstrates the generality of the SayCan idea: an LLM without sensory input can propose procedural plans that, when combined with perception-derived affordance signals and executable low-level policies, yield actionable behaviors in simulated tabletop domains.",
            "uuid": "e549.4",
            "source_info": {
                "paper_title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Value function spaces: Skill-centric state abstractions for long-horizon reasoning",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "CLIPort: What and Where Pathways for Robotic Manipulation",
            "rating": 2
        },
        {
            "paper_title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "rating": 2
        },
        {
            "paper_title": "PaLM: Scaling Language Modeling with Pathways",
            "rating": 1
        }
    ],
    "cost": 0.022587999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</h1>
<p>${ }^{1}$ Michael Ahn<em>, Anthony Brohan</em>, Noah Brown<em>, Yevgen Chebotar</em>, Omar Cortes<em>, Byron David</em>, Chelsea Finn<em>, Chuyuan Fu ${ }^{1}$, Keerthana Gopalakrishnan</em>, Karol Hausman<em>, Alex Herzog ${ }^{1}$, Daniel Ho ${ }^{1}$, Jasmine Hsu</em>, Julian Ibarz<em>, Brian Ichter</em>, Alex Irpan<em>, Eric Jang</em>, Rosario Jauregui Ruano<em>, Kyle Jeffrey</em>, Sally Jesmonth<em>, Nikhil J Joshi</em>, Ryan Julian<em>, Dmitry Kalashnikov</em>, Yuheng Kuang<em>, Kuang-Huei Lee</em>, Sergey Levine<em>, Yao Lu</em>, Linda Luu<em>, Carolina Parada</em>, Peter Pastor ${ }^{1}$, Jornell Quiambao<em>, Kanishka Rao</em>, Jarek Rettinghouse<em>, Diego Reyes</em>, Pierre Sermanet<em>, Nicolas Sievers</em>, Clayton Tan<em>, Alexander Toshev</em>, Vincent Vanhoucke<em>, Fei Xia</em>, Ted Xiao<em>, Peng Xu</em>, Sichun Xu<em>, Mengyuan Yan ${ }^{1}$, Andy Zeng</em><br>*Robotics at Google, ${ }^{\dagger}$ Everyday Robots</p>
<h4>Abstract</h4>
<p>Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website, the video, and open sourced code in a tabletop domain can be found at say-can.github.io.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: LLMs have not interacted with their environment and observed the outcome of their responses, and thus are not grounded in the world. SayCan grounds LLMs via value functions of pretrained skills, allowing them to execute real-world, abstract, long-horizon commands on robots.</p>
<h2>1 Introduction</h2>
<p>Recent progress in training large language models (LLMs) has led to systems that can generate complex text based on prompts, answer questions, or even engage in dialogue on a wide range of topics. These models absorb vast quantities of knowledge from text corpora mined from the web,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and we might wonder whether knowledge of everyday tasks that is encoded in such models can be used by robots to perform complex tasks in the real world. But how can embodied agents extract and harness the knowledge of LLMs for physically grounded tasks?</p>
<p>This question poses a major challenge. LLMs are not grounded in the physical world and they do not observe the consequences of their generations on any physical process [1]. This can lead LLMs to not only make mistakes that seem unreasonable or humorous to people, but also to interpret instructions in ways that are nonsensical or unsafe for a particular physical situation. Figure 1 shows an example - a kitchen robot capable of executing skills such as "pick up the sponge" or "go to the table" may be asked for help cleaning up a spill ("I spilled my drink, can you help?"). A language model may respond with a reasonable narrative that is not feasible or useful for the robot. "You could try using a vacuum cleaner" is impossible if there is no vacuum in the scene or if the robot is incapable of using one. With prompt engineering, a LLM may be capable of splitting the high-level instruction into sub-tasks, but it cannot do so without the context of what the robot is capable of given its abilities and the current state of the robot and the environment.</p>
<p>Motivated by this example, we study the problem of how to extract the knowledge in LLMs for enabling an embodied agent, such as a robot, to follow high-level textual instructions. The robot is equipped with a repertoire of learned skills for "atomic" behaviors that are capable of low-level visuomotor control. We make use of the fact that, in addition to asking the LLM to simply interpret an instruction, we can use it to score the likelihood that an individual skill makes progress towards completing the high-level instruction. Then, if each skill has an affordance function that quantifies how likely it is to succeed from the current state (such as a learned value function), its value can be used to weight the skill's likelihood. In this way, the LLM describes the probability that each skill contributes to completing the instruction, and the affordance function describes the probability that each skill will succeed - combining the two provides the probability that each skill will perform the instruction successfully. The affordance functions make the LLM aware of the current scene, and constraining the completions to the skill descriptions makes the LLM aware of the robot's capabilities. Furthermore, this combination results in a fully explainable sequence of steps that the robot will execute to accomplish an instruction - an interpretable plan that is expressed through language.</p>
<p>Our method, SayCan, extracts and leverages the knowledge within LLMs in physically-grounded tasks. The LLM (Say) provides a task-grounding to determine useful actions for a high-level goal and the learned affordance functions (Can) provide a world-grounding to determine what is possible to execute upon the plan. We use reinforcement learning (RL) as a way to learn languageconditioned value functions that provide affordances of what is possible in the world. We evaluate the proposed approach on 101 real-world robotic tasks that involve a mobile robot accomplishing a large set of language instructions in a real kitchen in a zero-shot fashion. Our experiments validate that SayCan can execute temporally-extended, abstract instructions. Grounding the LLM in the real-world via affordances nearly doubles the performance over the non-grounded baselines. Additionally, by evaluating the performance of the system with different LLMs, we show that a robot's performance can be improved simply by enhancing the underlying language model.</p>
<h1>2 Preliminaries</h1>
<p>Large Language Models. Language models seek to model the probability $p(W)$ of a text $W=\left{w_{0}, w_{1}, w_{2}, \ldots, w_{n}\right}$, a sequence of strings $w$. This is generally done through factorizing the probability via the chain rule to be $p(W)=\Pi_{j=0}^{n} p\left(w_{j} \mid w_{&lt;j}\right)$, such that each successive string is predicted from the previous. Recent breakthroughs initiated by neural network-based Attention architectures [2] have enabled efficient scaling of so-called Large Language Models (LLMs). Such models include Transformers [2], BERT [3], T5 [4], GPT-3 [5], Gopher [6], LAMDA [7], FLAN [8], and PaLM [9], each showing increasingly large capacity (billions of parameters and terabytes of text) and subsequent ability to generalize across tasks.</p>
<p>In this work, we utilize the vast semantic knowledge contained in LLMs to determine useful tasks for solving high-level instructions.</p>
<p>Value functions and RL. Our goal is to be able to accurately predict whether a skill (given by a language command) is feasible at a current state. We use temporal-difference-based (TD) reinforcement learning to accomplish this goal. In particular, we define a Markov decision process (MDP) $\mathcal{M}=(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ and $\mathcal{A}$ are state and action spaces, $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}_{+}$is a state-transition probability function, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is a reward function and $\gamma$ is a discount factor.</p>
<p>The goal of TD methods is to learn state or state-action value functions (Q-function) $Q^{\pi}(s, a)$, which represents the discounted sum of rewards when starting from state $s$ and action $a$, followed by the actions produced by the policy $\pi$, i.e. $Q^{\pi}(s, a)=\mathbb{E}<em t="t">{a\sim \pi(a \mid s)} \sum</em>} R\left(s_{t}, a_{t}\right)$. The Q-function, $Q^{\pi}(s, a)$ can be learned via approximate dynamic programming approaches that optimize the following loss: $L_{T D}(\theta)=\mathbb{E<em a_prime="a^{\prime">{(s, a, s^{\prime}) \sim \mathcal{D}} \mid R(s, a)+\gamma \mathbb{E}</em>$ is the dataset of states and actions and $\theta$ are the parameters of the Q-function.} \sim \pi} Q_{\theta}^{\pi}\left(s^{\prime}, a^{\prime}\right)-Q_{\theta}^{\pi}(s, a)]$, where $\mathcal{D</p>
<p>In this work, we utilize TD-based methods to learn said value function that is additionally conditioned on the language command and utilize those to determine whether a given command is feasible from the given state. It is worth noting that in the undiscounted, sparse reward case, where the agent receives the reward of 1.0 at the end of the episode if it was successful and 0.0 otherwise, the value function trained via RL corresponds to an affordance function [10] that specifies whether a skill is possible in a given state. We leverage that intuition in our setup and express affordances via value functions of sparse reward tasks.</p>
<h1>3 SayCan: Do As I Can, Not As I Say</h1>
<p>Problem Statement. Our system receives a user-provided natural language instruction $i$ that describes a task that the robot should execute. The instruction can be long, abstract, or ambiguous. We also assume that we are given a set of skills $\Pi$, where each skill $\pi \in \Pi$ performs a short task, such as picking up a particular object, and comes with a short language description $\ell_{\pi}$ (e.g., "find a sponge") and an affordance function $p\left(c_{\pi} \mid s, \ell_{\pi}\right)$, which indicates the probability of $c$-ompleting the skill with description $\ell_{\pi}$ successfully from state $s$. Intuitively, $p\left(c_{\pi} \mid s, \ell_{\pi}\right)$ means "if I ask the robot to do $\ell_{\pi}$, will it do it?". In RL terminology, $p\left(c_{\pi} \mid s, \ell_{\pi}\right)$ is the value function for the skill if we take the reward to be 1 for successful completion and 0 otherwise.
As mentioned above, $\ell_{\pi}$ denotes the textual label of skill $\pi$ and $p\left(c_{\pi} \mid s, \ell_{\pi}\right)$ denotes the probability that skill $\pi$ with textual label $\ell_{\pi}$ successfully completes if executed from state $s$, where $c_{\pi}$ is a Bernoulli random variable. The LLM provides us with $p\left(\ell_{\pi} \mid i\right)$, the probability that a skill's textual label is a valid next step for the user's instruction. However, what we are interested in is the probability that a given skill successfully makes progress toward actually completing the instruction, which we denote as $p\left(c_{i} \mid i, s, \ell_{\pi}\right)$. Assuming that a skill that succeeds makes progress on $i$ with probability $p\left(\ell_{\pi} \mid i\right)$ (i.e., its probability of being the right skill), and a skill that fails makes progress with probability zero, we can factorize this as $p\left(c_{i} \mid i, s, \ell_{\pi}\right) \propto p\left(c_{\pi} \mid s, \ell_{\pi}\right) p\left(\ell_{\pi} \mid i\right)$. This corresponds to multiplying the probability of the language description of the skill given the instruction $p\left(\ell_{\pi} \mid i\right)$, which we refer to as task-grounding, and the probability of the skill being possible in the current state of the world $p\left(c_{\pi} \mid s, \ell_{\pi}\right)$, which we refer to as world-grounding.
Connecting Large Language Models to Robots. While large language models can draw on a wealth of knowledge learned from copious amounts of text, they will not necessarily break down high-level commands into low-level instructions that are suitable for robotic execution. If a language model were asked "how would a robot bring me an apple", it may respond "a robot could go to a nearby store and purchase an apple for you". Though this response is a reasonable completion for the prompt, it is not necessarily actionable to an embodied agent, which may have a narrow and fixed set of abilities. Therefore, to adapt language models to our problem statement, we must somehow inform them that we specifically want the high-level instruction to be broken down into sequences of available low-level skills. One approach is careful prompt engineering [5, 11], a technique to coax a language model to a specific response structure. Prompt engineering provides examples in the context text ("prompt") for the model that specify the task and the response structure which the model will emulate; the prompt used in this work is shown in Appendix D. 3 along with experiments ablating it. However, this is not enough to fully constrain the output to admissible primitive skills for an embodied agent, and indeed at times it can produce inadmissible actions or language that is not formatted in a way that is easy to parse into individual steps.
Scoring language models open an avenue to constrained responses by outputting the probabilities assigned by a language model to fixed outputs. A language model represents a distribution over potential completions $p\left(w_{k} \mid w_{&lt;k}\right)$, where $w_{k}$ is a word that appears at a $k^{\text {th }}$ position in a text. While typical generation applications (e.g., conversational agents) sample from this distribution or decode the maximum likelihood completion, we can also use the model to score a candidate completion selected from a set of options. Formally in SayCan, given a set of low-level skills $\Pi$, their language descriptions $\ell_{\Pi}$ and an instruction $i$, we compute the probability of a language description of a skill $\ell_{\pi} \in \ell_{\Pi}$ making progress towards executing the instruction $i: p\left(\ell_{\pi} \mid i\right)$, which corresponds to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A value function module (a) is queried to form a value function space of action primitives based on the current observation. Visualizing "pick" value functions, in (b) "Pick up the red bull can" and "Pick up the apple" have high values because both objects are in the scene, while in (c) the robot is navigating an empty space, and thus none of the pick up actions receive high values.</p>
<p>querying the model over potential completions. The optimal skill according to the language model is computed via $$l_0 = \arg \max_{l_0 \in \ell_0} p(\ell_\pi|i)$$. Once selected, the process proceeds by iteratively selecting a skill and appending it to the instruction. Practically, in this work we structure the planning as a dialog between a user and a robot, in which a user provides the high level-instruction (e.g., "How would you bring me a coke can?") and the language model responds with an explicit sequence ("I would: 1. $$l_\pi$", e.g., "I would: 1. find a coke can, 2. pick up the coke can, 3. bring it to you").</p>
<p>This has the added benefit of interpretability, as the model not only outputs generative responses, but also gives a notion of likelihood across many possible responses. Figure 3 (and Appendix Figure 12 in more detail) shows this process of forcing the LLM into a language pattern, where the set of tasks are the skills the low-level policy is capable of and prompt engineering shows plan examples and dialog between the user and the robot. With this approach, we are able to effectively extract knowledge from the language model, but it leaves a major issue: while the decoding of the instruction obtained in this way always consists of skills that are available to the robot, these skills may not necessarily be appropriate for executing the desired high-level task in the <em>specific</em> situation that the robot is currently in. For example, if I ask a robot to "bring me an apple", the optimal set of skills changes if there is no apple in view or if it already has one in its hand.</p>
<p><strong>SayCan.</strong> The key idea of SayCan is to ground large language models through value functions – affordance functions that capture the log likelihood that a particular skill will be able to succeed in the current state. Given a skill $$l_0 \in \Pi$$, its language description $$l_\pi$$ and its corresponding value function, which provides $$p(c_\pi|s, l_\pi)$$, the probability of c-ompletion for the skill described by $$l_\pi$$ in state $$s$$, we form an affordance space $${p(c_\pi|s, l_\pi)}<em>{l</em>\pi \in \Pi}$$. This value function space captures affordances across all skills [12] (see Figure 2). For each skill, the affordance function and the LLM probability are then multiplied together and ultimately the most probable skill is selected, i.e. $$l_0 = \arg \max_{l_\pi \in \Pi} p(c_\pi|s, l_\pi)p(l_\pi|i)$$. Once the skill is selected, the corresponding policy is executed by the agent and the LLM query is amended to include $$l_\pi$$ and the process is run again until a termination token (e.g., "done") is chosen. This process is shown in Figure 3 and described in Algorithm 1. These two mirrored processes together lead to a probabilistic interpretation of SayCan, where the LLM provides probabilities of a skill being useful for the high-level instruction and the affordances provide probabilities of successfully executing each skill. Combining these two probabilities together provides a probability that this skill furthers the execution of the high-level instruction commanded by the user.</p>
<h2>4 Implementing SayCan in a Robotic System</h2>
<p><strong>Language-Conditioned Robotic Control Policies.</strong> To instantiate SayCan, we must provide it with a set of skills, each of which has a policy, a value function, and a short language description (e.g., "pick up the can"). These skills, value functions, and descriptions can be obtained in a variety of different ways. In our implementation, we train the individual skills either with image-based behavioral cloning, following the BC-Z method [13], or reinforcement learning, following MTOpt [14]. Regardless of how the skill's policy is obtained, we utilize value functions trained via TD backups as the affordance model for that skill. While we find that the BC policies achieve higher success rates at the current stage of our data collection process, the value functions provided by the RL policies are crucial as an abstraction to translate control capabilities to a semantic understanding of the scene. In order to amortize the cost of training many skills, we utilize multi-task BC and multi-task RL, respectively, where instead of training a separate policy and value function per skill, we train multi-task policies and models that are <em>conditioned</em> on the language description. Note, however,</p>
<h1>Algorithm 1 SayCan</h1>
<p>Given: A high level instruction $i$, state $s_{0}$, and a set of skills $\Pi$ and their language descriptions $\ell_{\Pi}$</p>
<p>1: $n=0, \pi=\emptyset$
2: while $\ell_{\pi_{n-1}} \neq$ "done" do
3: $\quad \mathcal{C}=\emptyset$
4: for $\pi \in \Pi$ and $\ell_{\pi} \in \ell_{\Pi}$ do
$5: \quad p_{\pi}^{\mathrm{LLM}}=p\left(\ell_{\pi} \mid i, \ell_{\pi_{n-1}}, \ldots, \ell_{\pi_{0}}\right)$
6: $\quad p_{\pi}^{\text {affordance }}=p\left(c_{\pi} \mid s_{n}, \ell_{\pi}\right)$
7: $\quad p_{\pi}^{\text {combined }}=p_{\pi}^{\text {affordance }} p_{\pi}^{\mathrm{LLM}}$
8: $\quad \mathcal{C}=\mathcal{C} \cup p_{\pi}^{\text {combined }}$
9: end for
10: $\pi_{n}=\arg \max <em n="n">{\pi \in \Pi} \mathcal{C}$
11: $\quad$ Execute $\pi</em>$
12: $\quad n=n+1$
13: end while
}\left(s_{n}\right)$ in the environment, updating state $s_{n+1<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Given a high-level instruction, SayCan combines probabilities from a LLM (the probability that a skill is useful for the instruction) with the probabilities from a value function (the probability of successfully executing said skill) to select the skill to perform. This emits a skill that is both possible and useful. The process is repeated by appending the skill to the response and querying the models again, until the output step is to terminate. Appendix Figures 12 and 2 focus on the LLM and VFS components.
that this description only corresponds to low level skills - it is still the role of the LLM in SayCan to interpret the high-level instruction and break it up into individual low level skill descriptions.</p>
<p>To condition the policies on language, we utilize a pre-trained large sentence encoder language model [15]. We freeze the language model parameters during training and use the embeddings generated by passing in text descriptions of each skill. These text embeddings are used as the input to the policy and value function that specify which skill should be performed (see the details of the architectures used in the Appendix C.1). Since the language model used to generate the text embeddings is not necessarily the same as the language model used for planning, SayCan is able to utilize different language models well suited for different abstraction levels - understanding planning with respect to many skills as opposed to expressing specific skills more granularly.
Training the Low-Level Skills. We utilize both BC and RL policy training procedures to obtain the language-conditioned policies and value functions, respectively. To complete the description of the underlying MDP that we consider, we provide the reward function as well as the skill specification that is used by the policies and value functions. As mentioned previously, for skill specification we use a set of short, natural language descriptions that are represented as language model embeddings. We utilize sparse reward functions with reward values of 1.0 at the end of an episode if the language command was executed successfully, and 0.0 otherwise. The success of language command execution is rated by humans where the raters are given a video of the robot performing the skill, together with the given instruction. If two out of the three raters agree that the skill was accomplished successfully, the episode is labelled with a positive reward.</p>
<p>To learn language-conditioned BC policies at scale in the real world, we build on top of BC-Z [13] and use a similar policy-network architecture (shown in Fig. 10). To learn a language-conditioned RL policy, we use MT-Opt [14] in the Everyday Robots simulator using RetinaGAN sim-to-real transfer [16]. We bootstrap the performance of simulation policies by utilizing simulation demonstrations to provide initial successes, and then continuously improve the RL performance with online</p>
<p>data collection. We use a network architecture similar to MT-Opt (shown in Fig. 9). The action space of our policies includes the six degrees of freedom of the end-effector pose as well as gripper open and close commands, x-y position and yaw orientation delta of the mobile base of the robot, and the terminate action. Additional details on data collection and training are in Appendix Section C.2.</p>
<p>Robotic System and Skills. For the control policies, we study a diverse set of manipulation and navigation skills using a mobile manipulator robot. Inspired by common skills one might pose to a robot in a kitchen environment, we propose 551 skills that span seven skill families and 17 objects, which include picking, placing and rearranging objects, opening and closing drawers, navigating to various locations, and placing objects in a specific configurations. In this study we utilize the skills that are most amenable to more complex behaviors via composition and planning as well as those that have high performance at the current stage of data collection; for more details, see Appendix D.</p>
<h1>5 Experimental Evaluation</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The experiments were performed in an office kitchen and a mock kitchen mirroring this setup, with 5 locations and 15 objects. The robot is a mobile manipulator with policies trained from an RGB observation.</p>
<p>Experimental Setup. We evaluate SayCan with a mobile manipulator and a set of object manipulation and navigation skills in two office kitchen environments. Figure 4 shows the environment setup and the robot. We use 15 objects commonly found in an office kitchen and 5 known locations with semantic meaning (two counters, a table, a trash can, and the user location). We test our method in two environments: a real office kitchen and a mock environment mirroring the kitchen, which is also the environment in which the robot's skills were trained. The robot used is a mobile manipulator from Everyday Robots ${ }^{2}$ with a 7 degree-of-freedom arm and a two-fingered gripper. The LLM used is 540B PaLM [9] unless stated otherwise for LLM ablations. We refer to SayCan with PaLM as PaLM-SayCan.</p>
<p>Instructions. To evaluate PaLM-SayCan, we test across 101 instructions from 7 instruction families, summarized in Table 1 and enumerated in Appendix E.1. These were developed to test various aspects of SayCan and were inspired by crowd sourcing via Amazon Mechanical Turk and in-person kitchen users, as well as benchmarks such as ALFRED [17] and BEHAVIOR [18]. The instructions span multiple axes of variation: time-horizon (from single primitives to $10+$ in a row), language complexity (from structured language to fully crowd-sourced requests), and embodiment (variations over the robot and environment state). Table 1 details examples for each family.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Instruction Family</th>
<th style="text-align: left;">Num</th>
<th style="text-align: left;">Explanation</th>
<th style="text-align: left;">Example Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NL Single Primitive</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">NL queries for a single primitive</td>
<td style="text-align: left;">Let go of the coke can</td>
</tr>
<tr>
<td style="text-align: left;">NL Nouns</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">NL queries focused on abstract nouns</td>
<td style="text-align: left;">Bring me a fruit</td>
</tr>
<tr>
<td style="text-align: left;">NL Verbs</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">NL queries focused on abstract verbs</td>
<td style="text-align: left;">Restock the rice chips on the far counter</td>
</tr>
<tr>
<td style="text-align: left;">Structured Language</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">Structured language queries, mirror NL Verbs</td>
<td style="text-align: left;">Move the rice chips to the far counter.</td>
</tr>
<tr>
<td style="text-align: left;">Embodiment</td>
<td style="text-align: left;">11</td>
<td style="text-align: left;">Queries to test SayCan's understanding of the <br> current state of the environment and robot</td>
<td style="text-align: left;">Put the coke on the counter. (starting <br> from different completion stages)</td>
</tr>
<tr>
<td style="text-align: left;">Crowd-Sourced</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">Queries in unstructured formats</td>
<td style="text-align: left;">My favorite drink is redbull, bring one</td>
</tr>
<tr>
<td style="text-align: left;">Long-Horizon</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">Long-horizon queries that require many steps <br> of reasoning</td>
<td style="text-align: left;">I spilled my coke on the table, throw it <br> away and bring me something to clean</td>
</tr>
</tbody>
</table>
<p>Table 1: List of instruction family definitions: We evaluate the algorithm on 101 instructions. We group the instructions into different families, with each family focusing on testing one aspect of the proposed method.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Metrics. To understand the performance of the proposed method we measure two main metrics. The first is plan success rate, which measures whether the skills selected by the model are correct for the instruction, regardless of whether or not they actually successfully executed. We ask 3 human raters to indicate whether the plan generated by the model can achieve the instruction, and if 2 out of 3 raters agree that the plan is valid, it is marked a success. Note that for many instructions there may be multiple valid solutions. For example if the instruction is to "bring a sponge and throw away the soda can", the plan can choose to bring sponge first or throw away the soda can first.</p>
<p>The second metric is execution success rate, which measures whether the full PaLM-SayCan system actually performs the desired instruction successfully. We ask 3 human raters to watch the robot execution. The raters are asked to answer the question "whether the robot achieves the task specified by the task string?" We mark an execution successful if 2 out of 3 raters agree that it is successful.</p>
<h1>5.1 Results</h1>
<p>Table 2 shows the performance of PaLM-SayCan across 101 tasks. In the mock kitchen, PaLMSayCan achieved a planning success rate of $84 \%$ and an execution rate of $74 \%$. We also investigate PaLM-SayCan out of the lab setting and in the real kitchen to verify the performance of the policies and value functions in this setting. We find a reduction of planning performance by $3 \%$ and execution by $14 \%$, indicating PaLM-SayCan and the underlying policies generalize reasonably well to the full kitchen. The full task list and results can be found in the Appendix Table 5, and videos of experiment rollouts and the decision making process can be found on the project website: say-can. github. io.</p>
<p>Figure 5 shows two long-horizon queries and the resulting rollouts. These tasks require PaLMSayCan to plan many steps without error and for the robot to navigate and interact with a significant portion of the kitchen. Each query requires PaLM-SayCan to understand context implicit within the instruction. In Figure 5a, the algorithm must understand the operator has asked for something to "recover from a workout", i.e. something healthy, and thus it brings water and an apple rather than, e.g., a soda and chips. Furthermore, the algorithm must understand ordering and history, that it has already brought a drink and now must bring a snack before terminating. In Figure 5b, PaLM-SayCan must track which objects are the "them" that need to be disposed of and where the sponge should be brought.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Timelapse of rollouts to two long-horizon queries. The robot interacts with a large portion of the kitchen environment and successfully performs sequences of manipulation and navigation skills.</p>
<p>Figure 6 highlights PaLM-SayCan's decision making, along with its interpretability. The decision making process can be understood as it solves instructions by visualizing what the two sides of the algorithm output. This allows a user to understand what options PaLM-SayCan is considering as language completions and what it believes is possible. We find that sequence order is understood (approaching objects before picking them up and picking them up before bringing them). Figure 6 shows that though the query mentions a coke, PaLM-SayCan understands that the important object is something to clean and brings a sponge. Appendix E. 6 shows additional rollouts with complex decisions, embodiment grounding, and long-horizon tasks in Figures 14-17 as well as failures in Figure 16. We believe such real-time and clear interpretability opens avenues to more interactive operation.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of PaLM-SayCan's decision making, where the top combined score chooses the correct skill.</p>
<p>When comparing the performance of different instruction families in Table 2 (see Table 1 for an explanation of families), we see that the natural language nouns performed worse than natural language verbs, due to the number of nouns possible ( 15 objects and 5 locations) versus number of verbs (6). The structured language tasks (created to ablate the performance loss of spelling out the solution versus understanding the query) were planned correctly $93 \%$ of the time, while their natural language verb counterparts were planned correctly $100 \%$. This indicates the language model effectively parses the queries. The embodiment tasks were planned correctly $64 \%$ of the time, generally with failures as a result of affordance function misclassification. PaLM-SayCan planned and executed crowd-sourced natural queries with performance on par with other instruction families. PaLM-SayCan performed worst on the most challenging long-horizon tasks, where most failures were a result of early termination by the LLM (e.g., bringing one object but not the second). We also find that PaLM-SayCan struggles with negation (e.g., "bring me a snack that isn't an apple") and ambiguous references (e.g. asking for drinks with caffeine), which is a known issue inherited from underlying language models [19]. Overall, $65 \%$ of the errors were LLM failures and $35 \%$ were affordance failures.
Returning to our initial example, "I spilled something, can you help?", an ungrounded language model would respond with statements like "I can call you a cleaner" or "I can vacuum that up for you", which given our robot are unreasonable. We have shown that PaLM-SayCan responds "I would: 1. find a sponge, 2. pick up the sponge, 3. bring it to you, 4. done" and is able execute this sequence on the robot in a real kitchen. This requires long-horizon reasoning over a required order, an abstract understanding of the instruction, and knowledge of both the environment and robot's capabilities.
Ablating Language. To study the importance of the LLM, we conduct two ablation experiments using the language-conditioned policy (see Sections 4-4). In $B C N L$ we feed the full instruction $i$ into the policy - this approach is representative of standard RL or BC-based instruction following methods [13, 20, 21, 22]. In $B C$ USE we project the high-level instruction into the set of known language commands via the Universal Sentence Encoder (USE) embeddings [15] by embedding the instruction, all the tasks, and the combinatorial set of sequences tasks (i.e., we consider "pick coke can" as well as " 1 . find coke can, 2. pick coke can" and so on), and selecting the highest cosine similarity instruction. The results in Table 2 illustrate the necessity of the language grounding where $B C N L$ achieves $0 \%$ in all tasks and $B C U S E$ achieves $60 \%$ for single primitives, but $0 \%$ otherwise.
Ablating Value Functions. Table 2 illustrates the necessity of the affordance grounding. We compare PaLM-SayCan to (1) No VF, which removes the value function grounding (i.e., choosing the maximum language score skill) and to (2) Generative, which uses the generative output of the LLM and then projects each planned skill to its maximal cosine similarity skill via USE embeddings. The latter in effect compares to [23], which loses the explicit option probabilities, and thus is less interpretable and cannot be combined with affordance probabilities. For Generative we also tried BERT embeddings [3], but found poor performance. The No VF and Generative approaches performed similarly, achieving $67 \%$ and $74 \%$ planning success rate respectively, and worse than PaLM-SayCan's $84 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Mock Kitchen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Kitchen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">No Affordance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">No LLM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM- <br> SayCan</td>
<td style="text-align: center;">PaLM- <br> SayCan</td>
<td style="text-align: center;">PaLM- <br> SayCan</td>
<td style="text-align: center;">PaLM- <br> SayCan</td>
<td style="text-align: center;">No VF</td>
<td style="text-align: center;">Gen.</td>
<td style="text-align: center;">BC NL</td>
<td style="text-align: center;">BC USE</td>
</tr>
<tr>
<td style="text-align: center;">Family</td>
<td style="text-align: center;">Num</td>
<td style="text-align: center;">Plan</td>
<td style="text-align: center;">Execute</td>
<td style="text-align: center;">Plan</td>
<td style="text-align: center;">Execute</td>
<td style="text-align: center;">Plan</td>
<td style="text-align: center;">Plan</td>
<td style="text-align: center;">Execute</td>
<td style="text-align: center;">Execute</td>
</tr>
<tr>
<td style="text-align: center;">NL Single</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">87\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">87\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">60\%</td>
</tr>
<tr>
<td style="text-align: center;">NL Nouns</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">47\%</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">40\%</td>
<td style="text-align: center;">53\%</td>
<td style="text-align: center;">53\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">NL Verbs</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">87\%</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Structured</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">87\%</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">47\%</td>
<td style="text-align: center;">93\%</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Embodiment</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">64\%</td>
<td style="text-align: center;">55\%</td>
<td style="text-align: center;">64\%</td>
<td style="text-align: center;">55\%</td>
<td style="text-align: center;">18\%</td>
<td style="text-align: center;">36\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Crowd Sourced</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">87\%</td>
<td style="text-align: center;">87\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">80\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Long-Horizon</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">47\%</td>
<td style="text-align: center;">73\%</td>
<td style="text-align: center;">47\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">0\%</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">84\%</td>
<td style="text-align: center;">74\%</td>
<td style="text-align: center;">81\%</td>
<td style="text-align: center;">60\%</td>
<td style="text-align: center;">67\%</td>
<td style="text-align: center;">74\%</td>
<td style="text-align: center;">0\%</td>
<td style="text-align: center;">9\%</td>
</tr>
</tbody>
</table>
<p>Table 2: Success rates of instructions by family. PaLM-SayCan achieves a planning success rate of $84 \%$ and execution success rate of $74 \%$ in the training environment and $81 \%$ planning and $60 \%$ execution in a real kitchen. No VF uses the maximum score skill from the LLM, Generative (Gen.) uses a generative LLM and then projects to the nearest skill via USE embeddings, $B C$ NL uses the policy with the natural language instruction, and $B C$ USE uses the policy with the natural language instruction projected to the nearest skill via USE embeddings.
Ablating the Language Model. SayCan is able to improve with improved language models. The LLM used herein was PaLM [9], a 540B parameter model. In this section we ablate over 8B, 62B, and 540B parameter models as well as the 137B parameter FLAN model [8] which is finetuned on a "instruction answering" dataset. Appendix Table 6 shows each model on a set of generative problems, where we find that generally larger models perform better, though the difference between the 62B and 540B model is small. Results in other works, such as Chain of Thought Prompting [24], indicate this difference may be more pronounced on more challenging problems - this is shown in Section 5.2. We also find that PaLM outperforms FLAN. Though FLAN was fine-tuned on instruction answering, the broader and improved dataset for PaLM may make up for this difference in training.</p>
<p>While it is expected that the generative performance of the language model will improve with better language models, it is unclear how the LLM size influences the final robotics success rate. Table 3 shows PaLM 540B and FLAN on robot running the full SayCan algorithm. The results show that the system using PaLM with affordance grounding (PaLM-SayCan) chooses the correct sequence of skills $84 \%$ of the time and executes them successfully $74 \%$ of the time, reducing errors by half compared to FLAN. This is particularly exciting because it represents the first time we can see how an improvement in language models translates to a similar improvement in robotics. This result indicates a potential future where the fields of language processing and robotics can collaboratively improve each other and scale together.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">PaLM-SayCan</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">FLAN-SayCan</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Family</td>
<td style="text-align: left;">Num</td>
<td style="text-align: left;">Plan</td>
<td style="text-align: left;">Execute</td>
<td style="text-align: left;">Plan</td>
<td style="text-align: left;">Execute</td>
</tr>
<tr>
<td style="text-align: left;">NL Single</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$67 \%$</td>
<td style="text-align: left;">$67 \%$</td>
</tr>
<tr>
<td style="text-align: left;">NL Nouns</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">$67 \%$</td>
<td style="text-align: left;">$47 \%$</td>
<td style="text-align: left;">$60 \%$</td>
<td style="text-align: left;">$53 \%$</td>
</tr>
<tr>
<td style="text-align: left;">NL Verbs</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$93 \%$</td>
<td style="text-align: left;">$80 \%$</td>
<td style="text-align: left;">$67 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Structured</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">$93 \%$</td>
<td style="text-align: left;">$87 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$87 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Embodiment</td>
<td style="text-align: left;">11</td>
<td style="text-align: left;">$64 \%$</td>
<td style="text-align: left;">$55 \%$</td>
<td style="text-align: left;">$64 \%$</td>
<td style="text-align: left;">$55 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Crowd Sourced</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">$87 \%$</td>
<td style="text-align: left;">$87 \%$</td>
<td style="text-align: left;">$73 \%$</td>
<td style="text-align: left;">$67 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Long-Horizon</td>
<td style="text-align: left;">15</td>
<td style="text-align: left;">$73 \%$</td>
<td style="text-align: left;">$47 \%$</td>
<td style="text-align: left;">$47 \%$</td>
<td style="text-align: left;">$33 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">101</td>
<td style="text-align: left;">$84 \%$</td>
<td style="text-align: left;">$74 \%$</td>
<td style="text-align: left;">$70 \%$</td>
<td style="text-align: left;">$61 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Success rates of instructions by family. SayCan achieves a planning success rate of $84 \%$ and execution success rate of $74 \%$ with PaLM and FLAN achieves $70 \%$ planning and $61 \%$ success. SayCan scales and improves with improved LLMs.</p>
<h1>5.2 Case Studies of New Capabilities of PaLM-SayCan</h1>
<p>PaLM-SayCan enables new capabilities. First, we show that it is very easy to incorporate new skills into the system, and use drawer manipulation as an example. Second, we show by leveraging chain of thought reasoning, we are able to solve complex tasks that require reasoning. Finally we show the system can work with multilingual queries, without explicitly being designed to.
Adding Skills: Drawer Manipulation (Appendix E.3). SayCan is capable of integrating new skills by simply adding the new skills as options for the LLM and providing accompanying value</p>
<p>functions and add an example in the prompt with that skill. For example, with the skills open, close, and go to the drawer, SayCan is capable of solving tasks such as "restock the coke and pepsi into the drawer". Over 21 queries we found a planning rate of $100 \%$ and an execution rate of $33 \%$ (due to failures of the chained manipulation policy), with no loss in performance for other instructions.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A sequence of the robot taking rice chips from the drawers. Note the robot only has one arm, so it needs to plan a long sequence to first take rice chips out of drawer and place on the counter, and then pick it up again after closing the drawer.</p>
<h1>CHAIN-OF-THOUGHT PLANNING ROLLOUT WITH PALM-SAYCAN.</h1>
<p>Human: Can you bring a fruit-flavored drink without caffeine?
Explanation: The user has asked for a drink that is fruit-flavored and does not have caffeine, I will bring the lime soda.
Robot: 1. find a lime soda, 2. pick up the lime soda, 3. bring it to you, 4. put down the lime soda, 5. done
Human: Bring me the spicy chips.
Explanation: The user has asked for chips that are spicy, I will bring the Jalapeno chips.
Robot: 1. find the jalapeno chips, 2. pick up the jalapeno chips, 3. bring it to you, 4. put down the jalapeno chips, 5 .done</p>
<p>Human: The snack is too small, bring me something more filling.
Explanation: The user has asked for a snack that is more filling, I will bring the multigrain chips.
Robot: 1. find the multigrain chips, 2. pick up the multigrain chips, 3. bring it to you, 4. put down the multigrain chips, 5. done</p>
<p>Table 4: Chain-of-thought planning rollout with PaLM-SayCan. The highlighted part is the chain of thought generated by PaLM-SayCan.</p>
<p>Chain of Thought Reasoning. SayCan can be integrated with recent work improving LLM reasoning, such as Chain of Thought [24]. One limitation of vanilla SayCan is that it doesn't handle tasks that involves negation. This is inherited from underline language models, and studied in the NLP community [19]. However, we found by using chain-of-thought prompting [24] we can improve SayCan on this front.</p>
<p>For chain-of-thought prompting-based SayCan, we need to modify the prompt to include a part called "Explanation". We also slightly change how we use the language model. Instead of directly using the scoring interface to rank possible options, we first use the generative decoding of LLM to create an explanation, and then use the scoring mode, by including the explanation into the prompt. The full prompt is shown in Appendix E. 4 Listing 3.</p>
<p>A few successful rollouts of the model at evaluation time is shown in Table 4. As we can see, with chain of thought prompting, the model can handle negations and tasks that require reasoning.</p>
<p>Multilingual Queries (Appendix E.5). While not explicitly designed to work with multilingual queries, PaLM-SayCan is able to handle them. The LLM was trained on multilingual corpora and thus SayCan can handle multilingual queries other than English. The results of SayCan on multilingual queries are summarized in Table. 8, and there is almost no performance drop in planning success rate when changing the queries from English to Chinese, French and Spanish.</p>
<p>Closed-Loop Planning. As presented herein, SayCan only receives environmental feedback through value functions at the current decision step, meaning if a skill fails or the environment changes, the necessary feedback may not be available. Owing to the extendibility and the natural language interface, Huang et al. [25] builds upon SayCan to enable closed-loop planning by leveraging environment feedback (from e.g., success detectors, scene descriptors, or even human feedback) through an inner monologue.</p>
<h1>6 Open Source Environment</h1>
<p>We have open-sourced an implementation of SayCan in a Google Colab notebook at say-can. github.io/#open-source. The environment is shown in Figure 8 and is a tabletop with a UR5 robot and randomly generated sets of colored blocks and bowls. The low-level policy is implemented with CLIPort [26], which is trained to output a pick and place location. Due to the lack of a value function for this policy, the affordances are implemented with a ViLD object detector [27]. GPT-3 is used as the open source language model [5]. Steps are output in the form "pick up the object and place it in location", leveraging the ability of LLMs to output code structures.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: We have open sourced a Colab with a tabletop environment, a UR5 robot, and CLIPort-based policy.</p>
<h2>7 Related Work</h2>
<p>Grounding Language Models. A significant body of work has focused on grounding language [28, 29]. Recent works have studied how to ground modern language models, by training them to accept additional environment inputs [30, 31, 32, 33, 34] or to directly output actions [35, 36, 37]. Others grounded language in an environment through prompt engineering [24]. Concurrently with SayCan, Huang et al. [23] use prompt engineering to extract temporally extended plans, but without any additional mechanism to ensure grounding, roughly corresponding to the "Generative" baseline in our experiments. The above methods are all trained without interaction with a physical environment, thus limiting their ability to reason over embodied interactions. One approach to grounding language models in interaction is by learning downstream networks with pre-trained LLM representations [38, 22, 21, 39, 40, 41, 42, 43]. Another approach finetunes language models with interactive data, such as rewards or ranking feedback of the interaction [11, 44, 45]. In our work, SayCan is able to ground language models in the given environment through previously-trained value functions, enabling general, long-horizon behaviors in a zero-shot manner, i.e., without additional training.
Learning Language-Conditioned Behavior. There is a long history of research studying how to connect language and behavior [46, 47, 48, 49, 50]. A large number of prior works have learned language-conditioned behavior via imitation learning [51, 22, 20, 13, 26, 37] or reinforcement learning [52, 53, 49, 54, 55, 56, 21, 41]. Most of these prior works focus on following low-level instructions, such as for pick-and-place tasks and other robotic manipulation primitives [20, 22, 56, 21, 13, 26], though some methods address long-horizon, compound tasks in simulated domains [57, 58, 54]. Like these latter works, we focus on completing temporally extended tasks. However, a central aspect of our work is to solve such tasks by extracting and leveraging the knowledge in large language models. While prior works have studied how pre-trained language embeddings can improve generalization to new instructions [38, 22, 21] and to new low-level tasks [13], we extract much more substantial knowledge from LLMs by grounding them within the robot's affordances. This allows robots to use language models for planning.
Task Planning and Motion Planning. Task and motion planning [59, 60] is a problem of sequencing tasks to solve a high-level problem, while ensuring the feasibility given an embodiment (task</p>
<p>planning [61, 62, 63]; motion planning [64]). Classically, this problem has been solved through symbolic planning [61, 63] or optimization [65, 66], but these require explicit primitives and constraints. Machine learning has recently been applied to enable abstract task specification, allow general primitives, or relax constraints [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]. Others learn to hierarchically solve such long-horizon problems [79, 80, 12, 81, 54]. SayCan leverages an LLM's semantic knowledge about the world for interpreting instructions and understanding how to execute them. The use of LLMs and generality of learned low-level policies enables long-horizon, abstract tasks that scale effectively to the real world, as demonstrated in our robot experiments.</p>
<h1>8 Conclusions, Limitations and Future Work</h1>
<p>We presented SayCan, a method that enables leveraging and grounding the rich knowledge in large language models to complete embodied tasks. For real-world grounding, we leverage pre-trained skills, which are then used to condition the model to choose natural language actions that are both feasible and contextually appropriate. More specifically, we use reinforcement learning as a way to learn value functions for the individual skills that provide affordances of what is possible in the world, and then use textual labels for these skills as potential responses that are scored by a language model. This combination results in a symbiotic relationship where the skills and their value functions can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about how to complete a task. We evaluated the proposed approach on a number of real-world robotic tasks that involve a mobile manipulator robot accomplishing a large set of long-horizon natural language instructions in a real kitchen. We also demonstrated an exciting property of our method where a robot's performance can be improved simply by enhancing the underlying language model.
While SayCan presents a viable way to ground language models in agents' affordances, it has a number of limitations. First, we expect this method to inherit the limitations and biases of LLMs [82, 83], including the dependence on the training data. Secondly, we observe that even though SayCan allows the users to interact with the agents using natural language commands, the primary bottleneck of the system is in the range and capabilities of the underlying skills. To illustrate this, we present representative failure cases in Appendix E. Future work that extends the repertoire of skills and improves their robustness would mitigate this limitation. In addition, at the current stage, the system is not easily able to react to situations where individual skills fail despite reporting a high value, though this could potentially be addressed by appropriate prompting of the language model for a correction.</p>
<p>There are many other potential avenues for future work. A natural question that this work raises is how the information gained through grounding the LLM via real-world robotic experience can be leveraged to improve the language model itself, both in terms of its factuality and its ability to perform common-sense reasoning about real-world environments and physics. Furthermore, since our method uses generic value functions to score affordances, it is intriguing to consider what other sources of grounding could be incorporated in the same manner, such as non-robotic contexts.</p>
<p>In the future, it is also interesting to examine whether natural language is the right ontology to use to program robots: natural language naturally incorporates contextual and semantic cues from the environment, and provides a level of abstraction which enables robots do decide on how to execute a strategy based on its own perception and affordances. At the same time, as opposed to, e.g., hindsight goal images [84], it requires supervision and it might not be the most descriptive medium for certain tasks.</p>
<p>Lastly, SayCan presents a particular way of connecting and factorizing the challenges of language understanding and robotics, and many further extensions can be proposed. Ideas such as combining robot planning and language [85], using language models as a pre-training mechanism for policies [44] and many other ways of combining language and interaction [21, 22, 38, 86] are exciting avenues for future research.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Fred Alcober, Yunfei Bai, Matt Bennice, Maarten Bosma, Justin Boyd, Bill Byrne, Kendra Byrne, Noah Constant, Pete Florence, Laura Graesser, Rico Jonschkowski, Daniel Kappler, Hugo Larochelle, Benjamin Lee, Adrian Li, Maysam Moussalem, Suraj Nair, Jane Park, Evan Rapoport, Krista Reymann, Jeff Seto, Dhruv Shah, Ian Storz, Razvan Surdulescu, Tom Small, and Vincent Zhao for their help and support in various aspects of the project.</p>
<h1>References</h1>
<p>[1] E. M. Bender and A. Koller. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.
[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[4] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, pages 1-67, 2019.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[6] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
[7] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[8] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[9] A. Chowdhery, S. Narang, J. Devlin, et al. Palm: Scaling language modeling with pathways. 2022. URL https://storage.googleapis.com/pathways-language-model/ PaLM-paper.pdf.
[10] J. J. Gibson. The theory of affordances. The Ecological Approach to Visual Perception, 1977.
[11] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Preprint, 2022.
[12] D. Shah, P. Xu, Y. Lu, T. Xiao, A. Toshev, S. Levine, and B. Ichter. Value function spaces: Skill-centric state abstractions for long-horizon reasoning. ICLR, 2022. URL https://arxiv.org/pdf/2111.03189.pdf.
[13] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991-1002. PMLR, 2021.
[14] D. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv, 2021.
[15] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. GuajardoCespedes, S. Yuan, C. Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018.
[16] D. Ho, K. Rao, Z. Xu, E. Jang, M. Khansari, and Y. Bai. Retinagan: An object-aware approach to sim-to-real transfer. 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 10920-10926, 2021.</p>
<p>[17] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages $10740-10749,2020$.
[18] S. Srivastava, C. Li, M. Lingelbach, R. Martín-Martín, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen, S. Buch, K. Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pages 477-490. PMLR, 2022.
[19] A. Hosseini, S. Reddy, D. Bahdanau, R. D. Hjelm, A. Sordoni, and A. Courville. Understanding by understanding not: Modeling negation in language models. arXiv preprint arXiv:2105.03519, 2021.
[20] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor. Languageconditioned imitation learning for robot manipulation tasks. ArXiv, abs/2010.12083, 2020.
[21] S. Nair, E. Mitchell, K. Chen, B. Ichter, S. Savarese, and C. Finn. Learning languageconditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pages 1303-1315. PMLR, 2021.
[22] C. Lynch and P. Sermanet. Grounding language in play. 2020.
[23] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.
[24] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[25] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.
[26] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, 2022.
[27] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.
[28] J. M. Siskind. Grounding language in perception. Artificial Intelligence Review, 1994.
[29] T. Winograd. Understanding natural language. Cognitive psychology, 1972.
[30] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
[31] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.
[32] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 2019.
[33] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 2021.
[34] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021.
[35] A. Suglia, Q. Gao, J. Thomason, G. Thattai, and G. Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021.</p>
<p>[36] A. Pashevich, C. Schmid, and C. Sun. Episodic transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.
[37] P. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.
[38] F. Hill, S. Mokra, N. Wong, and T. Harley. Human instruction-following with deep reinforcement learning via transfer-learning from text. arXiv preprint arXiv:2005.09382, 2020.
[39] V. Blukis, C. Paxton, D. Fox, A. Garg, and Y. Artzi. A persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, 2022.
[40] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.
[41] A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020.
[42] R. Zellers, A. Holtzman, M. Peters, R. Mottaghi, A. Kembhavi, A. Farhadi, and Y. Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. arXiv preprint arXiv:2106.00188, 2021.
[43] P. C. Humphreys, D. Raposo, T. Pohlen, G. Thornton, R. Chhaparia, A. Muldal, J. Abramson, P. Georgiev, A. Goldin, A. Santoro, et al. A data-driven approach for learning to control computers. arXiv preprint arXiv:2202.08137, 2022.
[44] M. Reid, Y. Yamada, and S. S. Gu. Can wikipedia help offline reinforcement learning. arXiv preprint arXiv:2201.12122, 2022.
[45] S. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch. Pretrained language models for interactive decision-making. arXiv preprint arXiv:2202.01771, 2022.
[46] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. 012006.
[47] T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In HRI 2010, 2010.
[48] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. volume 2, 01 2011.
[49] J. Luketina, N. Nardelli, G. Farquhar, J. N. Foerster, J. Andreas, E. Grefenstette, S. Whiteson, and T. Rocktäschel. A survey of reinforcement learning informed by natural language. In IJCAI, 2019.
[50] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 2020.
[51] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In $A A A I, 2016$.
[52] D. K. Misra, J. Langford, and Y. Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In EMNLP, 2017.
[53] K. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari, W. Czarnecki, M. Jaderberg, D. Teplyashin, M. Wainwright, C. Apps, D. Hassabis, and P. Blunsom. Grounded language learning in a simulated 3d world. ArXiv, abs/1706.06551, 2017.
[54] Y. Jiang, S. Gu, K. Murphy, and C. Finn. Language as an abstraction for hierarchical deep reinforcement learning. In NeurIPS, 2019.
[55] G. Cideron, M. Seurin, F. Strub, and O. Pietquin. Self-educated language agent with hindsight experience replay for instruction following. ArXiv, abs/1910.09451, 2019.</p>
<p>[56] P. Goyal, S. Niekum, and R. Mooney. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. ArXiv, abs/2007.15543, 2020.
[57] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. ArXiv, abs/1706.05064, 2017.
[58] J. Andreas, D. Klein, and S. Levine. Modular multitask reinforcement learning with policy sketches. ArXiv, abs/1611.01796, 2017.
[59] L. P. Kaelbling and T. Lozano-Pérez. Hierarchical planning in the now. In Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.
[60] S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel. Combined task and motion planning through an extensible planner-independent interface layer. In 2014 IEEE international conference on robotics and automation (ICRA), 2014.
[61] R. E. Fikes and N. J. Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial intelligence, 1971.
[62] E. D. Sacerdoti. A structure for plans and behavior. Technical report, SRI International, Menlo Park California Artificial Intelligence Center, 1975.
[63] D. Nau, Y. Cao, A. Lotem, and H. Munoz-Avila. Shop: Simple hierarchical ordered planner. 1999.
[64] S. M. LaValle. Planning algorithms. 2006.
[65] M. Toussaint. Logic-geometric programming: An optimization-based approach to combined task and motion planning. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.
[66] M. A. Toussaint, K. R. Allen, K. A. Smith, and J. B. Tenenbaum. Differentiable physics and stable modes for tool-use and manipulation planning. 2018.
[67] D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese. Neural task programming: Learning to generalize across hierarchical tasks. In 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018.
[68] D. Xu, R. Martín-Martín, D.-A. Huang, Y. Zhu, S. Savarese, and L. F. Fei-Fei. Regression planning networks. Advances in Neural Information Processing Systems, 32, 2019.
[69] D.-A. Huang, S. Nair, D. Xu, Y. Zhu, A. Garg, L. Fei-Fei, S. Savarese, and J. C. Niebles. Neural task graphs: Generalizing to unseen tasks from a single video demonstration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.
[70] B. Eysenbach, R. R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and reinforcement learning. Advances in Neural Information Processing Systems, 2019.
[71] N. Savinov, A. Dosovitskiy, and V. Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.
[72] B. Ichter, P. Sermanet, and C. Lynch. Broadly-exploring, local-policy trees for long-horizon task planning. Conference on Robot Learning (CoRL), 2021.
[73] C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. A joint model of language and perception for grounded attribute learning. arXiv preprint arXiv:1206.6423, 2012.
[74] T. Silver, R. Chitnis, N. Kumar, W. McClinton, T. Lozano-Perez, L. P. Kaelbling, and J. Tenenbaum. Inventing relational state and action abstractions for effective and efficient bilevel planning. arXiv preprint arXiv:2203.09634, 2022.
[75] C. R. Garrett, C. Paxton, T. Lozano-Pérez, L. P. Kaelbling, and D. Fox. Online replanning in belief space for partially observable task and motion problems. In 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020.</p>
<p>[76] Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi, and A. Farhadi. Visual semantic planning using deep successor representations. In Proceedings of the IEEE international conference on computer vision, 2017.
[77] D. K. Misra, J. Sung, K. Lee, and A. Saxena. Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. The International Journal of Robotics Research, 2016.
[78] B. Wu, S. Nair, L. Fei-Fei, and C. Finn. Example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks. In 5th Annual Conference on Robot Learning, 2021.
[79] S. Nair and C. Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. ArXiv, abs/1909.05829, 2020.
[80] F. Xia, C. Li, R. Martín-Martín, O. Litany, A. Toshev, and S. Savarese. Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.
[81] C. Li, F. Xia, R. Martin-Martin, and S. Savarese. Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. In Conference on Robot Learning, 2020.
[82] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021.
[83] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[84] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan, B. Eysenbach, R. C. Julian, C. Finn, and S. Levine. Actionable models: Unsupervised offline reinforcement learning of robotic skills. ArXiv, abs/2104.07749, 2021.
[85] S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics. 2014.
[86] S. I. Wang, P. Liang, and C. D. Manning. Learning language games through interaction. arXiv preprint arXiv:1606.02447, 2016.
[87] T. Xiao, E. Jang, D. Kalashnikov, S. Levine, J. Ibarz, K. Hausman, and A. Herzog. Thinking while moving: Deep reinforcement learning with concurrent control. In International Conference on Learning Representations, 2019.
[88] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. In ICLR, 2016.</p>
<h1>A Version Control</h1>
<p>v1 $\rightarrow$ v2: Added PaLM results. Added study about new capabilities (drawer manipulation, chain of thought prompting, multilingual instructions). Added an ablation study of language model size. Added an open-source version of SayCan on a simulated tabletop environment. Improved readability.</p>
<h2>B Contributions</h2>
<h2>B. 1 By Type</h2>
<ul>
<li>Designed and built distributed robot learning infrastructure: Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Byron David, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Alex Irpan, Eric Jang, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Yao Lu, Peter Pastor, Kanishka Rao, Nicolas Sievers, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan.</li>
<li>Designed, implemented, or trained the underlying manipulation policies: Yevgen Chebotar, Keerthana Gopalakrishnan, Karol Hausman, Julian Ibarz, Alex Irpan, Eric Jang, Nikhil Joshi, Ryan Julian, Kuang-Huei Lee, Yao Lu, Kanishka Rao, and Ted Xiao.</li>
<li>Designed or implemented the data generation and curation or collected data: Noah Brown, Omar Cortes, Jasmine Hsu, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Linda Luu, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Clayton Tan, and Sichun Xu.</li>
<li>Designed or implemented SayCan: Karol Hausman, Brian Ichter, Sergey Levine, Alexander Toshev, and Fei Xia.</li>
<li>Managed or advised on the project: Chelsea Finn, Karol Hausman, Eric Jang, Sally Jesmonth, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Alexander Toshev, and Vincent Vanhoucke.</li>
<li>Ran evaluations or experiments: Noah Brown, Omar Cortes, Brian Ichter, Rosario Jauregui Ruano, Kyle Jeffrey, Linda Luu, Jornell Quiambao, Jarek Rettinghouse, Diego Reyes, Clayton Tan, and Fei Xia.</li>
<li>Scaled simulation infrastructure: Nikhil J Joshi, Yao Lu, Kanishka Rao, and Ted Xiao.</li>
<li>Wrote the paper: Chelsea Finn, Karol Hausman, Brian Ichter, Alex Irpan, Sergey Levine, Fei Xia, and Ted Xiao.</li>
<li>Created open-source environment: Brian Ichter, Andy Zeng.</li>
</ul>
<h2>B. 2 By Person</h2>
<p>Michael Ahn developed the deployment system that enabled the ability to scale up data collection on real robots.
Anthony Brohan implemented the logging system for the project and designed and implemented the data labeling pipelines.
Noah Brown led and coordinated the real-robot operations including data collection with teleoperators, evaluations and the real-world setup.
Yevgen Chebotar designed and implemented multiple offline RL methods allowing the manipulation policies to process data coming from different sources.
Omar Cortes collected data on the robots and ran and supervised real-world evaluations.
Byron David developed simulation assets and performed system identification.
Chelsea Finn advised on the project, helped set the research direction and wrote parts of the paper.
Chuyuan Fu developed deformable objects and experimented with sim-to-real techniques.
Keerthana Gopalakrishnan provided multiple infrastructure contributions that allowed for scalable learning of manipulation policies.
Karol Hausman co-led the project as well as developed SayCan, helped set the research direction, trained the underlying manipulation policies, and wrote the paper.
Alex Herzog developed the teleoperation tools and implemented multiple infrastructure tools that allowed for continuous robot operation.
Daniel Ho helped develop sim-to-real pipelines for manipulation policies.</p>
<p>Jasmine Hsu provided logging and monitoring infrastructure tools as well as data labeling pipelines. Julian Ibarz provided multiple contributions that enabled scaling learning algorithms for manipulation policies, and helped set the research direction.
Brian Ichter initiated and led the SayCan algorithm, combined the manipulation and navigation skills, ran experiments for the paper, created the open-sourced SayCan Colab, and wrote the paper. Alex Irpan set up and led the autonomous data collection effort as well as verified the data collected by the robots, and wrote parts of the paper.
Eric Jang helped set the research and team direction, managed the data for learning, developed the behavioral cloning manipulation policies, and wrote parts of the paper.
Rosario Jauregui Ruano collected data on the robots and ran and supervised real-world evaluations. Kyle Jeffrey collected data on the robots and ran and supervised real-world evaluations.
Sally Jesmonth was the program manager for the project.
Nikhil J Joshi developed a number of simulation and infrastructure tools that allowed to scale up simulation training.
Ryan Julian developed multi-modal network architectures and trained manipulation policies.
Dmitry Kalashnikov contributed infrastructure pieces that enabled training from logged data.
Yuheng Kuang implemented the logging system for the project and designed and implemented the data labeling pipelines
Kuang-Huei Lee made improvements to training algorithms for manipulation policies.
Sergey Levine advised on the project, helped set the research direction, developed SayCan, and wrote parts of the paper.
Yao Lu led and designed the robot learning infrastructure for the project providing most of the tools and improving manipulation policies.
Linda Luu ran multiple evaluations, collected data and helped establish real-robot operations.
Carolina Parada advised on the project, managed the team, helped write the paper, and helped set the research direction.
Peter Pastor provided infrastructure tools that allowed for continuous robot operations.
Jornell Quiambao collected data on the robots and ran and supervised real-world evaluations.
Kanishka Rao co-led the project, managed the team, helped set the research direction and contributed to training manipulation policies.
Jarek Rettinghouse collected data on the robots and ran and supervised real-world evaluations.
Diego Reyes collected data on the robots and ran and supervised real-world evaluations.
Pierre Sermanet set up the crowd compute rating pipeline.
Nicolas Sievers provided simulation assets and environments used for simulation training.
Clayton Tan collected data on the robots and ran and supervised real-world evaluations and helped establish real-robot operations.
Alexander Toshev advised on the project, developed SayCan, helped write the paper, and helped set research direction.
Vincent Vanhoucke advised on the project, managed the team, and helped write the paper.
Fei Xia developed, implemented, and led on-robot SayCan, ran the experiments for the paper, created the demos, and wrote the paper.
Ted Xiao led the scaling of manipulation skills, designed and developed learning from simulation for manipulation skills, and developed multi-modal network architectures.
Peng Xu was the engineering lead for integrating manipulation and navigation and developed the underlying infrastructure for SayCan.
Sichun Xu developed remote teleoperation tools that allowed scaling up data collection in simulation.
Mengyuan Yan implemented infrastructure and learning tools that allowed for learning manipulation policies from different data sources.
Andy Zeng provided codebase and simulation assets for the open-source SayCan Colab.</p>
<h1>B. 3 Corresponding Emails:</h1>
<p>{ichter,xiafei,karolhausman}@google.com</p>
<h2>C RL and BC Policies</h2>
<h2>C. 1 RL and BC Policy Architecture</h2>
<p>The RL models use an architecture similar to MT-Opt [14], with slight changes to support natural language inputs (see Fig. 9 for the network diagram). The camera image is first processed by 7 con-</p>
<p>volutional layers. The language instruction is embedded by the LLM, then concatenated with the robot action and non-image parts of the state, such as the gripper height. To support asynchronous control, inference occurs while the robot is still moving from the previous action. The model is given how much of the previous action is left to execute [87]. The conditioning input goes through FC layers, then tiled spatially and added to the conv. volume, before going through 11 more convolutional layers. The output is gated through a sigmoid, so the Q-value is always in $[0,1]$.
The BC models use an architecture similar to BC-Z [13] (see Fig. 10 for the network diagram). The language instruction is embedded by a universal sentence encoder [15], then used to FiLM condition a Resnet-18 based architecture. Unlike the RL model, we do not provide the previous action or gripper height, since this was not necessary to learn the policy. Multiple FC layers are applied to the final visual features, to output each action component (arm position, arm orientation, gripper, and the termination action).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Network architecture in RL policy</p>
<h1>C. 2 RL and BC Policy Training</h1>
<p>RL training. In addition to using demonstrations in the BC setup, we also learn languageconditioned value functions with RL. For this purpose, we complement our real robot fleet with a simulated version of the skills and environment. To reduce the simulation-to-real gap we transform robot images via RetinaGAN [16] to look more realistic while preserving genera object structure. In order to learn a language-conditioned RL policy, we utilize MT-Opt [14] in the Everyday Robots simulator using said simulation-to-real transfer. We bootstrap the performance of simulation policies by utilizing simulation demonstrations to provide initial successes, and then continuously improve the RL performance with online data collection in simulation. Standard image augmentations (random brightness and contrast) as well as random cropping were applied. The $640 \times 512$ input image was padded by 100 pixels left-right and 40 pixels top-down, then cropped back down to a $640 \times 512$ image, so as to allow for random spatial shifts without limiting the field of view. We use a network architecture similar to MT-Opt (shown in Fig. 9).
The RL model is trained using 16 TPUv3 chips and for about 100 hours, as well as a pool of 3000 CPU workers to collect episodes and another 3000 CPU workers to compute target Q-values. Computing target Q-values outside the TPU allows the TPU to be used solely for computing gradient updates. Episode rewards are sparse and always 0 or 1 , so the Q-function is updated using a log loss. Models were trained using prioritized experience replay [88], where episode priority was tuned to encourage replay buffer training data for each skill to be close to $50 \%$ success. Episodes were</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Network architecture in BC policy
sampled proportionally to their priority, defined as $1+10 \cdot|p-0.5|$, where $p$ is the average success rate of episodes in the replay buffer.</p>
<p>BC training. We use 68000 teleoperated demonstrations that were collected over the course of 11 months using a fleet of 10 robots. The operators use VR headset controllers to track the motion of their hand, which is then mapped onto the robot's end-effector pose. The operators can also use a joystick to move the robot's base. We expand the demonstration dataset with 276000 autonomous episodes of learned policies which are later success-filtered and included in BC training, resulting in an additional 12000 successful episodes. To additionally process the data, we also ask the raters to mark the episodes as unsafe (i.e., if the robot collided with the environment), undesirable (i.e., if the robot perturbed objects that were not relevant to the skill) or infeasible (i.e., if the skill cannot be done or is already accomplished). If any of these conditions are met, the episode is excluded from training.
To learn language-conditioned BC policies at scale in the real world, we build on top of BC-Z [13] and use a similar policy-network architecture (shown in Fig. 10). It is trained with an MSE loss for the continuous action components, and a cross-entropy loss for the discrete action components. Each action component was weighted evenly. Standard image augmentations (random brightness and contrast) as well as random cropping were used. The $640 \times 512$ input image was padded by 100 pixels left-right and 40 pixels top-down, then cropped back down to a $640 \times 512$ image, so as to allow for random spatial shifts without limiting the field of view. For faster iteration speeds with negligible training performance reduction, image inputs were down sampled to half-size ( $256 \times 320$ images). Affordance value functions were trained with full-size images, since half-size images did not work as well when learning $Q\left(s, a, \ell_{\pi}\right)$. The BC model is trained using 16 TPUv3 chips and trained for about 27 hours.</p>
<h1>C. 3 RL and BC Policy Evaluations</h1>
<p>In order to obtain the best possible manipulation capabilities for use in SayCan, we use a separate evaluation protocol for iterating on the RL and BC policies in the Mock Office Kitchen stations. Evaluations are divided by skill (pick up, knock over, place upright, open/close drawers, move object close to another one), and within each skill, 18-48 skills are sampled from a predetermined set of three objects. Object positions are randomized on each episode, with one or two objects serving as a distractor.</p>
<p>The episode ends when 50 actions have been taken or the policy samples a terminate action. A human operator supervises multiple robots performing evaluation and performs scene resets as needed, and records each episode as a success or failure. Models whose per-skill performance outperforms prior models are "graduated" to the same evaluation protocol in the real kitchen, and then integrated into SayCan. We found that despite the domain shift from Mock Office Kitchen stations to the actual</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://everydayrobots.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1892 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1892</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1892</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-38.html">extraction-schema-38</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <p><strong>Paper ID:</strong> paper-276767120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.01508v1.pdf" target="_blank">E NABLING AI S CIENTISTS TO R ECOGNIZE I NNOVATION : A D OMAIN -A GNOSTIC A LGORITHM FOR A SSESSING N OVELTY</a></p>
                <p><strong>Paper Abstract:</strong> In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea’s local density with its adjacent neighbors’ densities. We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment. Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820) and biomedical research (AUROC=0.765) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1892.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1892.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RND_eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Neighbor Density empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's empirical evaluation comparing the proposed Relative Neighbor Density (RND) algorithm against LLM-based judges and absolute local-density metrics across computer science, biomedical, and mixed cross-domain test sets, using AUROC as the main performance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>automated system evaluation / empirical benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>LLM-based novelty judgments (with/without literature search and with guideline/tournament variants), absolute local density (Historical Dissimilarity; Overall Novelty components), citation-based Contemporary Impact (as used in ON), journal/conference prestige and temporal heuristics used as validation proxies</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Constructed automated validation labels: positive = recent articles from top venues (NeurIPS 2024 accepted or Nature Medicine Aug 2024–Feb 2025 'Article' type), negative = historically highly-cited papers from the same venues (top cited NeurIPS 2015–2020; top cited Nature Medicine past 15 years). (Note: this is an internal, temporal/top-venue proxy rather than long-term citation trajectories.)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>Binary novel vs non-novel labels (positive = 'novel' recent top-venue papers; negative = 'non-novel' historically highly-cited papers), plus continuous novelty percentiles (0–100) produced by RND/HD/ON</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>NeurIPS test set: positives = 80 accepted (oral/spotlight) 2024 NeurIPS papers (with Program Chair novelty comments); negatives = 99 most cited NeurIPS papers 2015–2020. Nature Medicine test set: positives = 66 'Article' type papers Aug 2024–Feb 2025 (excl. phase 2/3 trials); negatives = 99 most-cited Nature Medicine articles over past 15 years. Mixed (cross-domain) set = combination of the above; literature databases: ~25.36M PubMed abstracts and ~2.64M arXiv abstracts used for embeddings and neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>RND AUROC: NeurIPS = 0.820, Nature Medicine = 0.765, Mixed = 0.795. Historical Dissimilarity (HD) / Absolute Local Density AUROC: NeurIPS ≈ 0.851, Nature Medicine ≈ 0.757, Mixed ≈ 0.362–0.395 (large cross-domain drop). LLM-based: without integrated literature search AUROC ≈ 0.50 (random); LLM+literature-search (Sonnet-3.7) AUROC ≈ 0.80 on NeurIPS but ≈ 0.60 on Nature Medicine and Mixed. Paper also states RND outperformed benchmarks on cross-domain (RND 0.795 vs benchmarks mean 0.597).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Quantified gaps: HD suffers a dramatic cross-domain AUROC drop from ~0.85 (single-domain CS) to ~0.36 (mixed) — a difference of ≈0.49; RND maintains high cross-domain AUROC and thus reduces that gap (RND mixed 0.795 vs HD mixed 0.362 → gap ~0.433). LLM (Sonnet-3.7) drops from ~0.80 (NeurIPS) to ~0.60 (biomed/mixed) → gap ~0.20. RND vs benchmark cross-domain improvement reported as 0.795 vs 0.597 → improvement 0.198.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Validation leverages temporal publication patterns (recent top-venue papers treated as novel; older highly-cited papers treated as non-novel). The paper notes that human-labeled novelty sets become outdated as ideas become established; no multi-year citation trajectory or delayed-recognition ground-truth (e.g., 10-year citations) was measured in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td>Performance differs by field: LLM+search performs much better in computer science than in biomedicine (internal knowledge of model matters); HD and RND both perform acceptably within single domains (HD closely matches RND in NeurIPS and Nature Medicine), but HD fails in cross-domain settings while RND remains robust (domain-invariant). The authors attribute this to domain-specific citation patterns, publication velocities, and semantic densities.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td>Described as a domain-dependent, non-linear degradation for absolute local-density and some LLM methods: HD shows near-adequate single-domain performance but a catastrophic drop in cross-domain (threshold/interaction effect rather than a simple linear decline). RND yields stable percentile-based scores (theoretical uniformity) across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>RND: AUROC 0.820 (NeurIPS), 0.765 (Nature Medicine), 0.795 (Mixed). HD: AUROC ≈0.851 (NeurIPS), ≈0.757 (Nature Medicine), ≈0.362–0.395 (Mixed). LLMs: without literature search AUROC ≈0.5; Sonnet-3.7 with literature search ≈0.80 (NeurIPS) and ≈0.60 (Nature Medicine/Mixed); Deepseek-r1 and Sonnet-3.7 outperform GPT-4o when external knowledge is provided. ON (Overall Novelty) and HD correlated with human labels in prior work but ON's cross-domain robustness not demonstrated here.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Mechanisms cited for proxy failures: (1) LLMs are sensitive to input perturbations and rely on internal domain knowledge; without external retrieval they fail. (2) Absolute local-density metrics depend on arbitrary choices (number of neighbors, historical/contemporary corpus boundaries) and are sensitive to domain-specific semantic density and citation/publication dynamics. (3) Literature database biases and embedding quality can influence metrics. RND's relative comparison across neighbor densities reduces dependence on absolute density scales and hence mitigates domain effects.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Proposed corrections include the RND algorithm (relative neighbor-density percentile scoring) which aims to be domain-invariant; augmenting LLMs with literature search (feeding top-k relevant papers) improves within-domain performance; use RND as a reward signal in RL to train reasoning models; careful parameter tuning (P and Q) to balance variance vs sensitivity. The paper also proposes an automated temporal/top-venue validation methodology to avoid manual labels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td>Exceptions noted: HD (absolute local density) performs comparably to RND within single domains (NeurIPS and Nature Medicine), and Sonnet-3.7 with literature search attains high AUROC (~0.80) on NeurIPS — i.e., proxies can work well in homogeneous, single-domain settings.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Supports a domain-sensitive Proxy-to-Ground-Truth Gap view: the evidence shows that commonly used proxies (absolute local density, LLM-only judgments) can systematically misestimate novelty when applied across domains, producing large gaps in cross-domain evaluation, while domain-normalized/relative metrics (RND) reduce this gap. Thus the results support the hypothesis that proxy performance is domain-dependent and that relative, distributional corrections can mitigate systematic undervaluation or misranking across fields.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1892.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1892.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM_judges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models used as novelty judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discussion and experimental evaluation of using LLMs (GPT-4o, Sonnet-3.7, Deepseek-r1) as autonomous novelty judges, with and without external literature search or structured guidelines/tournament designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>automated system evaluation / empirical assessment of LLM judgment reliability</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>LLM-generated novelty ratings (plain LLM judgement; LLM + literature search of top-k relevant papers; LLM with NeurIPS review guideline; LLM via Swiss tournament design)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Same constructed test labels as above (recent top-venue papers = novel; historically highly-cited = non-novel).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>Binary novel vs non-novel labels used for evaluation; LLM outputs mapped to scores/binary decisions (0 or 1 in LLM+search setting; continuous/overall scores when using review-guideline simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>Same NeurIPS, Nature Medicine, Mixed test sets; LLM baselines run three times to account for output variability; literature search provided top-10 relevant papers retrieved via embedding-based search.</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>LLM without external search: AUROC ≈ 0.50 (random). LLM + literature search (Sonnet-3.7): AUROC ≈ 0.80 on NeurIPS but ≈ 0.60 on Nature Medicine and Mixed. Sonnet-3.7 and Deepseek-r1 outperform GPT-4o when external knowledge is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Observed domain gap for LLM+search ~0.20 drop in AUROC between NeurIPS (~0.80) and biomed/mixed (~0.60). LLM methods thus show moderate loss of accuracy across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not directly evaluated; paper notes human review guidelines and tournament designs may not generalize over time and are sensitive to prompt/input phrasing; no long-term recognition trajectories assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td>LLM performance strongly depends on model's internal knowledge of the domain: better in computer science (where the model likely has more exposure) than in biomedical domain; adding external search reduces but does not eliminate domain differences.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td>Performance decline with domain mismatch appears non-linear and sensitive to availability of external literature context — large step-change when literature retrieval is absent, moderate degradation across domains when present.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>As above: Sonnet-3.7 + search ~0.80 (CS), ~0.60 (biomed); Deepseek-r1 similar pattern; GPT-4o substantially worse unless augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Causes: sensitivity to input phrasing, reliance on model's pretraining/domain knowledge, and lack of integrated, comprehensive retrieval leading to inconsistent judgments; weak grounding causes contradictory ratings for semantically similar phrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Feeding LLMs with top-k retrieved relevant papers markedly improves accuracy; using review-guidelines or tournament formats without literature degrades results; combining RND-derived signals as reward during training is proposed as a way to improve LLM generation and judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td>Sonnet-3.7 with literature search achieves high accuracy (~0.80) on NeurIPS, showing LLMs can perform well when given appropriate external evidence and on familiar domains.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>Findings support the idea that LLM-based proxies can be unreliable and domain-sensitive, thus contributing evidence that proxy-to-ground-truth gaps arise from model biases and lack of robust, cross-domain grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1892.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1892.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different evaluation metrics (citations, peer review, journal prestige, automated systems) perform when assessing scientific discoveries of varying novelty or transformational nature, including comparisons between early/proxy metrics and later ground-truth impact measures.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>validation_proxy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal / venue / citation-based validation proxy methodology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's automated validation method: positive (novel) samples chosen as recent articles from top journals/conferences; negative (non-novel) samples chosen as historically highly-cited papers — used to create scalable test sets without manual expert labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>methodological proposal / automated validation design</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metrics_studied</strong></td>
                            <td>Journal/conference prestige (top venues), recency (recent issues), citation counts (highly-cited older papers) as proxies for novelty vs non-novelty</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Operational ground truth constructed from temporal/top-venue heuristics: 'recent top-venue publications' = novel; 'historically highly-cited older papers' = non-novel. (This is treated as the ground-truth labels for evaluation but is itself a proxy.)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_classification</strong></td>
                            <td>Binary: 'novel' positives (recent top-venue) vs 'non-novel' negatives (older highly-cited).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_characteristics</strong></td>
                            <td>NeurIPS positives drawn from Program Chair-accepted papers with explicit novelty comments; negatives were top 99 cited NeurIPS papers 2015–2020. Nature Medicine positives = 'Article' type Aug 2024–Feb 2025; negatives top 99 cited Nature Medicine papers over past 15 years.</td>
                        </tr>
                        <tr>
                            <td><strong>key_quantitative_findings</strong></td>
                            <td>Using this validation approach, RND and HD achieved AUROC ≳0.75 within single domains; LLM methods performed poorly without search. The approach enabled creation of larger, automated test sets and revealed cross-domain failures of absolute-density metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap_magnitude</strong></td>
                            <td>Not a quantified gap per se, but authors acknowledge this validation choice simplifies the classification problem and may over-separate positives and negatives; they note that none of the methods achieved saturated AUROCs even under this simplified scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Explicit use of temporal separation: positives are recent papers; negatives are older, highly-cited works — the method assumes novelty is more likely in recent top-venue outputs and that past novel ideas become non-novel over time as they attract citations.</td>
                        </tr>
                        <tr>
                            <td><strong>field_specific_findings</strong></td>
                            <td>The paper argues that such temporal/top-venue proxies are easier to construct in some domains (e.g., CS with public review comments) and that manual expert labels are less scalable and become outdated; proxies interact with domain-specific publication and citation dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_shape</strong></td>
                            <td>Not modeled as a functional form; the authors caution that proxy separation may be 'easier' than real-world borderline novelty distinctions, so observed performance may overestimate practical discriminability.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>This is a validation design rather than an evaluator; its use showed that methods that look good in single-domain settings may fail in mixed-domain settings (e.g., HD).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_identified</strong></td>
                            <td>Rationale: recent top-venue papers are likely frontier work; older highly-cited papers indicate that the idea area has become crowded. Limitations: high citation count can reflect utility rather than novelty; top-venue selection excludes borderline or incremental cases and may bias the test set.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_approaches</strong></td>
                            <td>Authors propose complementing this automated proxy with more nuanced negative sampling (e.g., rejected papers or incremental works) in future work to better represent subtle novelty distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_exceptions</strong></td>
                            <td>Authors note that their negative sampling (historical highly-cited) may be too easily distinguishable from positives and therefore does not cover subtle cases where proxy metrics might fail.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>This methodological contribution emphasizes that commonly used proxies (journal prestige, recency, citation counts) are practical but imperfect stand-ins for 'ground-truth' novelty; the paper uses them to demonstrate that different metrics diverge, supporting the existence of proxy-truth gaps and the need for domain-invariant measures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas <em>(Rating: 2)</em></li>
                <li>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation <em>(Rating: 1)</em></li>
                <li>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex <em>(Rating: 1)</em></li>
                <li>Evaluating the World Model Implicit in a Generative Model <em>(Rating: 1)</em></li>
                <li>Pubmed and beyond: biomedical literature search in the age of artificial intelligence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1892",
    "paper_id": "paper-276767120",
    "extraction_schema_id": "extraction-schema-38",
    "extracted_data": [
        {
            "name_short": "RND_eval",
            "name_full": "Relative Neighbor Density empirical evaluation",
            "brief_description": "This paper's empirical evaluation comparing the proposed Relative Neighbor Density (RND) algorithm against LLM-based judges and absolute local-density metrics across computer science, biomedical, and mixed cross-domain test sets, using AUROC as the main performance metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_type": "automated system evaluation / empirical benchmark",
            "proxy_metrics_studied": "LLM-based novelty judgments (with/without literature search and with guideline/tournament variants), absolute local density (Historical Dissimilarity; Overall Novelty components), citation-based Contemporary Impact (as used in ON), journal/conference prestige and temporal heuristics used as validation proxies",
            "ground_truth_measure": "Constructed automated validation labels: positive = recent articles from top venues (NeurIPS 2024 accepted or Nature Medicine Aug 2024–Feb 2025 'Article' type), negative = historically highly-cited papers from the same venues (top cited NeurIPS 2015–2020; top cited Nature Medicine past 15 years). (Note: this is an internal, temporal/top-venue proxy rather than long-term citation trajectories.)",
            "discovery_type_classification": "Binary novel vs non-novel labels (positive = 'novel' recent top-venue papers; negative = 'non-novel' historically highly-cited papers), plus continuous novelty percentiles (0–100) produced by RND/HD/ON",
            "sample_characteristics": "NeurIPS test set: positives = 80 accepted (oral/spotlight) 2024 NeurIPS papers (with Program Chair novelty comments); negatives = 99 most cited NeurIPS papers 2015–2020. Nature Medicine test set: positives = 66 'Article' type papers Aug 2024–Feb 2025 (excl. phase 2/3 trials); negatives = 99 most-cited Nature Medicine articles over past 15 years. Mixed (cross-domain) set = combination of the above; literature databases: ~25.36M PubMed abstracts and ~2.64M arXiv abstracts used for embeddings and neighbor retrieval.",
            "key_quantitative_findings": "RND AUROC: NeurIPS = 0.820, Nature Medicine = 0.765, Mixed = 0.795. Historical Dissimilarity (HD) / Absolute Local Density AUROC: NeurIPS ≈ 0.851, Nature Medicine ≈ 0.757, Mixed ≈ 0.362–0.395 (large cross-domain drop). LLM-based: without integrated literature search AUROC ≈ 0.50 (random); LLM+literature-search (Sonnet-3.7) AUROC ≈ 0.80 on NeurIPS but ≈ 0.60 on Nature Medicine and Mixed. Paper also states RND outperformed benchmarks on cross-domain (RND 0.795 vs benchmarks mean 0.597).",
            "proxy_truth_gap_magnitude": "Quantified gaps: HD suffers a dramatic cross-domain AUROC drop from ~0.85 (single-domain CS) to ~0.36 (mixed) — a difference of ≈0.49; RND maintains high cross-domain AUROC and thus reduces that gap (RND mixed 0.795 vs HD mixed 0.362 → gap ~0.433). LLM (Sonnet-3.7) drops from ~0.80 (NeurIPS) to ~0.60 (biomed/mixed) → gap ~0.20. RND vs benchmark cross-domain improvement reported as 0.795 vs 0.597 → improvement 0.198.",
            "temporal_pattern": "Validation leverages temporal publication patterns (recent top-venue papers treated as novel; older highly-cited papers treated as non-novel). The paper notes that human-labeled novelty sets become outdated as ideas become established; no multi-year citation trajectory or delayed-recognition ground-truth (e.g., 10-year citations) was measured in this work.",
            "field_specific_findings": "Performance differs by field: LLM+search performs much better in computer science than in biomedicine (internal knowledge of model matters); HD and RND both perform acceptably within single domains (HD closely matches RND in NeurIPS and Nature Medicine), but HD fails in cross-domain settings while RND remains robust (domain-invariant). The authors attribute this to domain-specific citation patterns, publication velocities, and semantic densities.",
            "relationship_shape": "Described as a domain-dependent, non-linear degradation for absolute local-density and some LLM methods: HD shows near-adequate single-domain performance but a catastrophic drop in cross-domain (threshold/interaction effect rather than a simple linear decline). RND yields stable percentile-based scores (theoretical uniformity) across domains.",
            "automated_system_performance": "RND: AUROC 0.820 (NeurIPS), 0.765 (Nature Medicine), 0.795 (Mixed). HD: AUROC ≈0.851 (NeurIPS), ≈0.757 (Nature Medicine), ≈0.362–0.395 (Mixed). LLMs: without literature search AUROC ≈0.5; Sonnet-3.7 with literature search ≈0.80 (NeurIPS) and ≈0.60 (Nature Medicine/Mixed); Deepseek-r1 and Sonnet-3.7 outperform GPT-4o when external knowledge is provided. ON (Overall Novelty) and HD correlated with human labels in prior work but ON's cross-domain robustness not demonstrated here.",
            "mechanism_identified": "Mechanisms cited for proxy failures: (1) LLMs are sensitive to input perturbations and rely on internal domain knowledge; without external retrieval they fail. (2) Absolute local-density metrics depend on arbitrary choices (number of neighbors, historical/contemporary corpus boundaries) and are sensitive to domain-specific semantic density and citation/publication dynamics. (3) Literature database biases and embedding quality can influence metrics. RND's relative comparison across neighbor densities reduces dependence on absolute density scales and hence mitigates domain effects.",
            "correction_approaches": "Proposed corrections include the RND algorithm (relative neighbor-density percentile scoring) which aims to be domain-invariant; augmenting LLMs with literature search (feeding top-k relevant papers) improves within-domain performance; use RND as a reward signal in RL to train reasoning models; careful parameter tuning (P and Q) to balance variance vs sensitivity. The paper also proposes an automated temporal/top-venue validation methodology to avoid manual labels.",
            "counterexamples_or_exceptions": "Exceptions noted: HD (absolute local density) performs comparably to RND within single domains (NeurIPS and Nature Medicine), and Sonnet-3.7 with literature search attains high AUROC (~0.80) on NeurIPS — i.e., proxies can work well in homogeneous, single-domain settings.",
            "supports_or_challenges_theory": "Supports a domain-sensitive Proxy-to-Ground-Truth Gap view: the evidence shows that commonly used proxies (absolute local density, LLM-only judgments) can systematically misestimate novelty when applied across domains, producing large gaps in cross-domain evaluation, while domain-normalized/relative metrics (RND) reduce this gap. Thus the results support the hypothesis that proxy performance is domain-dependent and that relative, distributional corrections can mitigate systematic undervaluation or misranking across fields.",
            "uuid": "e1892.0"
        },
        {
            "name_short": "LLM_judges",
            "name_full": "Large Language Models used as novelty judges",
            "brief_description": "Discussion and experimental evaluation of using LLMs (GPT-4o, Sonnet-3.7, Deepseek-r1) as autonomous novelty judges, with and without external literature search or structured guidelines/tournament designs.",
            "citation_title": "",
            "mention_or_use": "use",
            "study_type": "automated system evaluation / empirical assessment of LLM judgment reliability",
            "proxy_metrics_studied": "LLM-generated novelty ratings (plain LLM judgement; LLM + literature search of top-k relevant papers; LLM with NeurIPS review guideline; LLM via Swiss tournament design)",
            "ground_truth_measure": "Same constructed test labels as above (recent top-venue papers = novel; historically highly-cited = non-novel).",
            "discovery_type_classification": "Binary novel vs non-novel labels used for evaluation; LLM outputs mapped to scores/binary decisions (0 or 1 in LLM+search setting; continuous/overall scores when using review-guideline simulations).",
            "sample_characteristics": "Same NeurIPS, Nature Medicine, Mixed test sets; LLM baselines run three times to account for output variability; literature search provided top-10 relevant papers retrieved via embedding-based search.",
            "key_quantitative_findings": "LLM without external search: AUROC ≈ 0.50 (random). LLM + literature search (Sonnet-3.7): AUROC ≈ 0.80 on NeurIPS but ≈ 0.60 on Nature Medicine and Mixed. Sonnet-3.7 and Deepseek-r1 outperform GPT-4o when external knowledge is provided.",
            "proxy_truth_gap_magnitude": "Observed domain gap for LLM+search ~0.20 drop in AUROC between NeurIPS (~0.80) and biomed/mixed (~0.60). LLM methods thus show moderate loss of accuracy across domains.",
            "temporal_pattern": "Not directly evaluated; paper notes human review guidelines and tournament designs may not generalize over time and are sensitive to prompt/input phrasing; no long-term recognition trajectories assessed.",
            "field_specific_findings": "LLM performance strongly depends on model's internal knowledge of the domain: better in computer science (where the model likely has more exposure) than in biomedical domain; adding external search reduces but does not eliminate domain differences.",
            "relationship_shape": "Performance decline with domain mismatch appears non-linear and sensitive to availability of external literature context — large step-change when literature retrieval is absent, moderate degradation across domains when present.",
            "automated_system_performance": "As above: Sonnet-3.7 + search ~0.80 (CS), ~0.60 (biomed); Deepseek-r1 similar pattern; GPT-4o substantially worse unless augmented.",
            "mechanism_identified": "Causes: sensitivity to input phrasing, reliance on model's pretraining/domain knowledge, and lack of integrated, comprehensive retrieval leading to inconsistent judgments; weak grounding causes contradictory ratings for semantically similar phrasings.",
            "correction_approaches": "Feeding LLMs with top-k retrieved relevant papers markedly improves accuracy; using review-guidelines or tournament formats without literature degrades results; combining RND-derived signals as reward during training is proposed as a way to improve LLM generation and judgment.",
            "counterexamples_or_exceptions": "Sonnet-3.7 with literature search achieves high accuracy (~0.80) on NeurIPS, showing LLMs can perform well when given appropriate external evidence and on familiar domains.",
            "supports_or_challenges_theory": "Findings support the idea that LLM-based proxies can be unreliable and domain-sensitive, thus contributing evidence that proxy-to-ground-truth gaps arise from model biases and lack of robust, cross-domain grounding.",
            "uuid": "e1892.1"
        },
        {
            "name_short": "validation_proxy",
            "name_full": "Temporal / venue / citation-based validation proxy methodology",
            "brief_description": "The paper's automated validation method: positive (novel) samples chosen as recent articles from top journals/conferences; negative (non-novel) samples chosen as historically highly-cited papers — used to create scalable test sets without manual expert labeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_type": "methodological proposal / automated validation design",
            "proxy_metrics_studied": "Journal/conference prestige (top venues), recency (recent issues), citation counts (highly-cited older papers) as proxies for novelty vs non-novelty",
            "ground_truth_measure": "Operational ground truth constructed from temporal/top-venue heuristics: 'recent top-venue publications' = novel; 'historically highly-cited older papers' = non-novel. (This is treated as the ground-truth labels for evaluation but is itself a proxy.)",
            "discovery_type_classification": "Binary: 'novel' positives (recent top-venue) vs 'non-novel' negatives (older highly-cited).",
            "sample_characteristics": "NeurIPS positives drawn from Program Chair-accepted papers with explicit novelty comments; negatives were top 99 cited NeurIPS papers 2015–2020. Nature Medicine positives = 'Article' type Aug 2024–Feb 2025; negatives top 99 cited Nature Medicine papers over past 15 years.",
            "key_quantitative_findings": "Using this validation approach, RND and HD achieved AUROC ≳0.75 within single domains; LLM methods performed poorly without search. The approach enabled creation of larger, automated test sets and revealed cross-domain failures of absolute-density metrics.",
            "proxy_truth_gap_magnitude": "Not a quantified gap per se, but authors acknowledge this validation choice simplifies the classification problem and may over-separate positives and negatives; they note that none of the methods achieved saturated AUROCs even under this simplified scenario.",
            "temporal_pattern": "Explicit use of temporal separation: positives are recent papers; negatives are older, highly-cited works — the method assumes novelty is more likely in recent top-venue outputs and that past novel ideas become non-novel over time as they attract citations.",
            "field_specific_findings": "The paper argues that such temporal/top-venue proxies are easier to construct in some domains (e.g., CS with public review comments) and that manual expert labels are less scalable and become outdated; proxies interact with domain-specific publication and citation dynamics.",
            "relationship_shape": "Not modeled as a functional form; the authors caution that proxy separation may be 'easier' than real-world borderline novelty distinctions, so observed performance may overestimate practical discriminability.",
            "automated_system_performance": "This is a validation design rather than an evaluator; its use showed that methods that look good in single-domain settings may fail in mixed-domain settings (e.g., HD).",
            "mechanism_identified": "Rationale: recent top-venue papers are likely frontier work; older highly-cited papers indicate that the idea area has become crowded. Limitations: high citation count can reflect utility rather than novelty; top-venue selection excludes borderline or incremental cases and may bias the test set.",
            "correction_approaches": "Authors propose complementing this automated proxy with more nuanced negative sampling (e.g., rejected papers or incremental works) in future work to better represent subtle novelty distinctions.",
            "counterexamples_or_exceptions": "Authors note that their negative sampling (historical highly-cited) may be too easily distinguishable from positives and therefore does not cover subtle cases where proxy metrics might fail.",
            "supports_or_challenges_theory": "This methodological contribution emphasizes that commonly used proxies (journal prestige, recency, citation counts) are practical but imperfect stand-ins for 'ground-truth' novelty; the paper uses them to demonstrate that different metrics diverge, supporting the existence of proxy-truth gaps and the need for domain-invariant measures.",
            "uuid": "e1892.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas",
            "rating": 2
        },
        {
            "paper_title": "Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation",
            "rating": 1
        },
        {
            "paper_title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
            "rating": 1
        },
        {
            "paper_title": "Evaluating the World Model Implicit in a Generative Model",
            "rating": 1
        },
        {
            "paper_title": "Pubmed and beyond: biomedical literature search in the age of artificial intelligence",
            "rating": 1
        }
    ],
    "cost": 0.01681775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025</p>
<p>Yao Wang wang-yao24@mails.tsinghua.edu.cn 
Department of Automation
Tsinghua University</p>
<p>Mingxuan Cui mingxuan.cui@mail.nankai.edu.cn 
Nankai University</p>
<p>Arthur Jiang arthursjiang@gmail.com 
Yidu Technology 
ENABLING AI SCIENTISTS TO RECOGNIZE INNOVATION: A DOMAIN-AGNOSTIC ALGORITHM FOR ASSESSING NOVELTY
10 Mar 2025014BE4AA0DE10575620E8518F64631CFarXiv:2503.01508v2[cs.AI]
In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery.This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea's local density with its adjacent neighbors' densities.We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment.Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820)and biomedical research (AUROC=0.765)domains.Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s.0.597) on cross-domain evaluation.These results validate RND as a generalizable solution for automated novelty assessment in scientific research.</p>
<p>Introduction</p>
<p>In the pursuit of Artificial General Intelligence (AGI), automating scientific research and knowledge discovery presents both a formidable challenge and an exciting opportunity, as it will be groundbreaking to expand the boundaries of human knowledge by leveraging scalable computing resources.Therefore, as the capabilities of large language models (LLMs) continue to improve, researchers have started to explore their use in automating various aspects of the research process, including the generation of novel ideas, as exemplified by the AI scientist concept (Lu et al. [2024]).</p>
<p>A key task for any AI-based scientist is the generation of novel research ideas, a task traditionally performed by human scientists during their brainstorming phase.While LLMs have shown promise in generating a large pool of ideas quickly and cost-effectively, akin to the initial stages of human research, the real challenge lies in evaluating these ideas for their novelty.Traditionally, novelty in scientific research has been assessed through peer review and expert evaluations, where domain specialists judge the originality of an idea based on their experience and familiarity with existing literature.However, such assessments are inherently subjective, time-consuming, and inconsistent across reviewers.Moreover, as the volume of scientific output grows exponentially, manual novelty assessment struggles to keep pace.Automated methods are therefore crucial for filtering out redundant ideas and promoting genuinely innovative directions.</p>
<p>Existing approaches primarily fall into two categories: (1) leveraging large language models (LLMs) as judges and (2) using absolute local density-based novelty metrics.</p>
<p>The most straightforward approach is to use LLMs as judges to evaluate the novelty of ideas.Si. et al. adopted a Swiss system tournament design to evaluate ideas by using LLM as judge (Si et al. [2024]), which was further applied in Nova: An Iterative Planning and Search Approach to enhance Novelty and Diversity of LLM-Generated Ideas (Hu et al. [2024]).To improve LLM's accuracy of judgment, Lu include NeurIPS review guideline and Semantic Scholar API as tools (Lu et al. [2024], Su et al. [2024]): the NeurIPS review guideline was served as both chain-of-thoughts prompts and few-shot examples, while search API enabled LLM to search top 10 relevant papers to determine the novelty of given idea.</p>
<p>An alternative approach relies on the absolute local density in semantic embedding space to measure novelty.Su. et al. introduced the concept of Historical Dissimilarity and Contemporary Dissimilarity, calculated as the average Euclidean distance (local density) between a generated abstract's embedding and the embeddings of the five most similar abstracts from a historical database and a contemporary database, respectively.In combination with the citation-based Contemporary Impact metric, they developed the Overall Novelty (ON) metric (Su et al. [2024]).Their study validated that ON correlates well with human-labeled novelty, demonstrating its effectiveness as a novelty assessment measure.</p>
<p>Though the aforementioned approaches provide potential solutions on idea novelty evaluation, there are major challenges when considering practical issues.</p>
<p>First, the reliability of using-LLM-as-judge remains questionable, even with external knowledge or tools.Studies have demonstrated that auto-regressive LLMs like GPT-4o (Hurst et al. [2024]) produce outputs sensitive to input perturbations (Zhuo et al. [2023], Singh et al. [2024]).In novelty assessment specifically, this means identical research ideas phrased differently might receive contradictory novelty ratings.While recent reasoning models, such as DeepSeek-r1 (Guo et al. [2025]) and Sonnet-3.7 (Anthropic [2025]), show improved reasoning capabilities, their reliability for scientific novelty judgment remains unvalidated.</p>
<p>Second, the absolute local density-based metric from (Su et al. [2024]) shows significant limitations across diverse research contexts.By relying on just 5 most similar abstracts from history and contemporary databases as reference points, the metric's validity becomes highly dependent on arbitrary choices: the size of paper collections, the temporal boundaries defining 'history' versus 'contemporary,' and the selection criteria for inclusion.Different research domains also exhibit varying citation patterns, publication velocities, and semantic densities, which undermined the metric's generalizability across research domains.</p>
<p>Last but not least, the validation methodology used to assess novelty evaluators themselves was significantly lacking.In most cases, the validation of novelty metric depends on small test sets manually labeled by human experts within a specific domain (Hu et al. [2024], Lu et al. [2024], Su et al. [2024], Si et al. [2024]).Such validations are difficult to scale across different research areas, as they are often highly specialized and tailored to particular fields of study.What's worse, manually produced novelty labels rapidly become outdated: as scientific research advances continuously, ideas labeled "novel" today quickly become established knowledge, rendering static human-labeled validation sets increasingly inaccurate over time.</p>
<p>To address these challenges, in this paper, we establish comprehensive semantic embedding databases for novelty assessment.These databases incorporate over 30 million publications from two distinct domains: Pubmed, the leading biomedical literature search engine with nearly 36 million articles (Jin et al. [2024]), and Arxiv, which contains more than 2.3 million scholarly articles across eight subject areas (Cornell Tech [2023]).</p>
<p>Based on these resources, we propose the Relative Neighbor Density (RND) algorithm, which measures novelty by analyzing the distribution patterns of semantic neighbors rather than simple absolute local density.This approach proves more reliable than LLM-based judgments and more generalizable than existing absolute local density-based metrics across different research domains.We also develop an automated validation methodology that leverages temporal publication patterns to evaluate novelty without requiring expert manual labeling.Our extensive evaluations using test sets from computer science, biomedical science, and cross-domain contexts demonstrate that our proposed algorithm maintains accuracy within specific domains while scaling effectively across diverse research areas.</p>
<p>Our main contributions are:</p>
<p>• A novel neighbor density-based Relative Neighbor Density (RND) algorithm for assessing research idea novelty that is robust across domains, which holds domain-invariant property</p>
<p>Related Works</p>
<p>Assessing the novelty of research ideas is a fundamental challenge in automating scientific discovery.Various approaches have been explored in recent years, leveraging Large Language Models (LLMs) and semantic similarity measures to evaluate idea originality.This section reviews existing methods, highlighting their strengths and limitations.</p>
<p>LLMs for Novelty Assessment</p>
<p>Recent work has demonstrated promising results in using LLMs as autonomous judges for research novelty.Si et al. (Si et al. [2024]) evaluated this approach using ICLR submissions, converting them into standardized project proposals and conducting pairwise comparisons between accepted and rejected papers(Table 7).Their Swiss tournament system iteratively paired proposals based on accumulated scores, with Claude-3.5-Sonnetachieving 71.4% accuracy in predicting paper acceptance.As a control measure, they included human expert reranking, which revealed notable discrepancies between automated and human judgments.</p>
<p>Lu et al. (Lu et al. [2024]) expanded this concept with their AI Scientist framework, integrating idea generation, evaluation, and refinement.Their system employs chain-of-thought prompting and external knowledge retrieval via Semantic Scholar API to enhance assessment quality.While showing promise in matching human-level performance, these LLM-based approaches face fundamental challenges in reliability and consistency, as highlighted by studies showing their sensitivity to input variations (Zhuo et al. [2023], Singh et al. [2024]).</p>
<p>Absolute Local Density-based Metrics</p>
<p>An alternative approach focuses on semantic local density to evaluate novelty.Su et al. (Su et al. [2024]) used the Historical Dissimilarity (HD), which is the average Euclidean distance between the generated abstract embedding and embeddings of the 5 most similar abstracts in historical literature base.We denote it as "Absolute Local Density" because the average distance is a metric of local density, and they use the value of density directly.In addition to HD, they also used Contemporary Dissimilarity (CD), which calculate in same algorithm in contemporary literature base, which is also an absolute local density-based metric.</p>
<p>Based on HD and CD, they developed Overall Novelty (ON) metric as below,
ON = HD × CI CD (1)
The main challenge of local density-based metric is that the density values vary across different domains.In case of evaluating ideas from mixed research domains, the variance would cause a severe degrade in the accuracy.</p>
<p>Validating Novelty Metrics</p>
<p>A critical limitation in existing research is the lack of scalable validation methodologies.Current approaches (Lu et al. [2024], Su et al. [2024], Hu et al. [2024], Si et al. [2024]) typically rely on small-size literature database and manually labeled test set created by domain experts.For instance, Su et al. (Su et al. [2024]) constructed an "ecosystem" for computer science (CS) using information extracted from 85,217 papers-a dataset that represents only a small fraction of the CS literature available on platforms like arXiv.While their analysis demonstrated promising correlations between novelty scores and human labels across 100 manually evaluated abstracts, the methodology's reliance on domain-specific expertise significantly constrains its generalizability, which is echoed by reviewer's comments that only validating CS is "relatively simple" (Openreview Reviewer 2X9t [2025]).</p>
<p>Furthermore, the scalability challenge persists across all existing frameworks.The creation of test sets currently requires expert labeling, making large-scale evaluation prohibitively resource-intensive.This limitation underscores the need for automated approaches to test set creation that maintain fidelity while eliminating the requirement for extensive manual labeling.</p>
<p>Method</p>
<p>Problem Description</p>
<p>Given a set of ideas I,
I = {idea i }, i ∈ <a href="2">1, N </a>
Where idea i is a sequence of words or characters in nature language.N ≥ 1 represents the number of ideas whose novelty needs to be assessed.</p>
<p>The objective is to design a mapping F from idea space to a score in real value space
F (idea i ) = score i , where idea i ∈ I, score i ∈ R (3)
The novelty score score should be monotonic, meaning that for any two ideas idea i and idea j , if idea i is more novel than idea j , then their corresponding scores must satisfy:
∀ idea i , idea j ∈ I, idea i ≻ idea j ⇒ F (idea i ) &gt; F (idea j )(4)
where idea i ≻ idea j denotes that idea i is considered more novel than idea j based on a given novelty criterion.</p>
<p>Semantic Embedding &amp; Literature Database</p>
<p>Each published literature's abstract, which is also a sequence of words or characters in natural language, is denoted as
a j .
The semantic embedding model is a mapping function G , which maps ideas and abstracts into embedding vectors:
G (idea i ) = v i , where v i ∈ R dims ,(5)G (a j ) = v j , where v j ∈ R dims (6)
Thus, the preprocessed literature semantic database is represented as a set A:
A = {(a j , v j ) | j ∈ [1, M ]}(7)
We collected 36 million academic articles from the PubMed Download API (National Library of Medicine [2025]) and 2.6 million papers from the ArXiv dataset (Cornell University [2025]).Among all fetched documents, only those with both a non-empty title and abstract were considered valid for the experiment, resulting in 25,360,114 papers from PubMed and 2,643,057 papers from ArXiv.</p>
<p>For each paper, two semantic embedding vectors were generated-one from its title and another from its abstract-using the M3-Embedding model Chen et al. [2024].The embedding vector dimension, denoted as dims, is 1024.All texts and embedding vectors were stored in Elasticsearch Version 8 for efficient retrieval.</p>
<p>Algorithm</p>
<p>For each idea idea i and its embedding v i , we first find its P nearest neighbors using k-Nearest Neighbors (KNN) search:
{v 1 , v 2 , . . . , v P } = KN N (v i , P, A) (8) where v j is the j-th nearest neighbor of v i , j ∈ [1, P ].
For the idea itself v i and each neighbor v j , the neighbor density (ND) is defined as below
N D = 1 Q Q k=1 d(v, v k )(9)
where v k denotes the k-th nearest article's embedding vector in the literature corpus A and d(•, •) is the cosine distance between two vectors.</p>
<p>We define the set S i that contains the neighbor density values of idea i 's neighbors:
S i = {N D j | j ∈ [1, P ]}(10)
Finally, we compute the novelty score score i for idea i as:
score i = |{N D ∈ S i | N D ≤ N D i }| |S i | × 100 (11)
The selection of values of P and Q is a trade-off between reliability of estimation and other cost.A lower values of P will cause biased estimation of novelty score; while higher value of P will increase computing cost in O(P • Q) complexity.Similarly, lower Q will also cause unreliable estimation of local density.Meanwhile, a higher Q value will not only be computational costly, but also sacrifice sensitivity as ND converges to its expectation as Q increase, losing information local density.Refer to Appendix A for detailed analysis on the effects of P and Q.</p>
<p>Based on analysis and empirical experiments, we set P = 100 and Q = 50.</p>
<p>The pseudoscope implement calculation of ND is presented in Algorithm 1; and the pseudoscope of complete algorithm is provided in Algorithm 2.</p>
<p>Algorithm</p>
<p>Validation without Human Labeling</p>
<p>As a novelty evaluation algorithm, the most challenging point in past research is to find a reliable labeled test set to evaluate the algorithm.Therefore, we propose a new method to construct a convincing test set instead of relying on human experts to annotate it.</p>
<p>For the positive samples (a.k.a novel ideas) in the test set, we select recent articles from top journals or conferences.For the negative samples (a.k.a.non-novel ideas), highly cited articles published before the last few years were selected, also from the research domain's top journals or conferences.The fundamental principles behind such methodology were: high-quality novel ideas are more likely to be published in recent issues and top journals or conferences; while after time passes, the at-the-time novel ideas were more likely to attract attention and related works, thus become non-innovative at present.</p>
<p>In this way, we can make positive and negative samples have a more obvious difference in novelty in a relatively objective and recognized way.We have two test sets: NeurIPS, which represents the most advanced research results in the field of computer science, and Nature Medicine, which represents the most cutting-edge papers in the medical field.The sample year distribution of the test sets can be found in Table 1 NeurIPS test set: The initial corpus consists of papers that are Accept (oral) or Accept (spotlight) by Program Chairs at the 2024 NeurIPS conference, which represents the latest research results in computer science.Furthermore, we select articles from the initial corpus that explicitly mention that the papers have obvious novelty in the comments of Program Chairs to form the positive samples of the NeurIPS test set.The comments and decision information of Program Chairs can be obtained on the OpenReview.netwebsite.At the same time, we use the Semantic Scholar API to obtain the 99 most cited papers published in the NeurIPS conference from 2015 to 2020 to form the negative samples of the test set.The titles of all samples are presented in Table 4 Nature Medicine test set: The positive samples of the Nature Medicine test set consist of articles classified as "Article" type, published in Nature Medicine from August 2024 to February 2025, according to the classification on the nature.comwebsite.Articles related to phase 2 or phase 3 trials were excluded.And we used the same method as the negative samples of the NeurIPS test set to obtain 99 articles of Nature Medicine with the highest citation count in the past 15 years as negative samples of the test set.The titles of all samples are presented in</p>
<p>Baseline</p>
<p>To evaluate our algorithm, we selected all existing novelty assessment algorithms as baselines, categorized into two groups: LLM-based and non-LLM-based.Non-LLM-based algorithms, including Relative Neighbor Density(Ours), Historical Dissimilarity(HD), and Overall Novelty(ON), rely solely on literature search and mathematical calculations.Since the output of the literature search for the same query remains consistent, we conducted a single test to assess the algorithm's performance.In contrast, for LLM-based algorithms, due to the inherent variability of LLM outputs, we ran three tests for each algorithm, calculated the average result, and included the standard deviation in the table.The full experimental results of the LLM-based method are provided in Table D.1.</p>
<p>For all methods, we use the abstracts of the papers in the test set as "ideas" for testing.</p>
<p>Historical Dissimilarity: Identify the five most relevant papers based on their embeddings and compute the Euclidean distance between the embedding of the idea and the embeddings of the abstracts of these five papers.The final novelty score is obtained by averaging these distance values.</p>
<p>Overall Novelty: The historical database contains papers from 2011 to 2021, and the contemporary database contains papers from 2021 to 2025.The score calculation method refers to equation 1.</p>
<p>LLM + literature search: Provide LLM with the titles and abstracts of the 10 most relevant papers to the given idea.The model then assesses whether the core concepts of these papers significantly overlap with the idea(Table 8).If substantial overlap is detected, the idea is deemed non-novel and assigned a score of 0. If no significant overlap is found, the idea is considered novel and assigned a score of 1.</p>
<p>LLM with guideline: Utilize the NeurIPS 2024 review guidelines to assist LLM in evaluating the novelty of ideas(Table 6).The final score is determined based on the "Overall" score provided in the review assessment.</p>
<p>LLM with tournament: First, the idea is transformed into the Standardized Project Proposal format(Table 7).Next, the novelty of all standardized ideas is assessed using the Swiss tournament method, where ideas are iteratively compared in a structured competition.Finally, each idea is assigned a score based on the number of wins it accumulates throughout the tournament.</p>
<p>Accuracy Evaluation</p>
<p>As shown in Table 2, our enhanced neighbor density-based novelty measurement algorithm outperforms all baseline models on both the Nature Medicine and Mixed test sets, while also demonstrating strong performance on the NeurIPS test set.2: Validation of Different methods, measured by AUROC.HD: Historical Dissimilarity (section 2.2).ON: Overall Novelty (section 2.2).LLM + literature search: supplementing LLM with 10 relevant papers, which were searched by idea's embedding from our literature database using semantic embedding.LLM with guideline: using NeurIPS 2024 review guideline to help LLM judge the novelty of ideas, which is not applicable to Nature Medicine.Therefore, the results of Nature Medicine and Mixed are marked as not applicable.LLM with tournament: a Swiss system tournament design to evaluate ideas by using LLM as judge.</p>
<p>Model</p>
<p>By comparing the results of various LLM-related algorithms, we observe a key similarity between Sonnet-3.7 with guideline and Sonnet-3.7 with tournament: both methods provide very limited external knowledge to the LLM, with no existing literature being fed into the model.As a result, the model's judgment of novelty is highly inaccurate.In contrast, the LLM + literature search method inputs the 10 most relevant papers to the idea, significantly improving the accuracy of the model's judgment.Moreover, the accuracy of the LLM + literature search method is much higher in the field of computer science compared to the field of biomedicine, highlighting the significant impact of the model's internal knowledge on the judgment outcomes, even with the addition of external knowledge.Additionally, Sonnet-3.7 (Anthropic [2025]) and Deepseek-r1 (Guo et al. [2025]) show much higher AUROC scores than GPT-4o, indicating that when external knowledge is provided, the performance of the inference model greatly surpasses that of the autoregressive model.However, we observed that the Historical Dissimilarity (HD) metric closely matched the performance of our proposed method on the Nature Medicine and NeurIPS test set.In contrast, on the Mixed test set, there was a significant disparity, with our method achieving an AUROC of 0.795, while HD only reached 0.362.This prompted us to further investigate the underlying reasons for this substantial difference.</p>
<p>The score distributions provided by the Historical Dissimilarity (HD) metric on the NeurIPS and Nature Medicine test set, as shown in Figure 1, are markedly different.This disparity implies that some negative samples from NeurIPS would be evaluated as more novel than some positive samples from Nature Medicine under this evaluation system, highlighting HD's limited generalization ability across domains.In contrast, the score distributions of our method on both test sets are nearly identical, indicating that our scores are absolute and unaffected by the specific discipline or field.This means that our scores are universally comparable across domains.This result underscores the robust cross-domain evaluation capability of our method, making it applicable for researchers in any field.</p>
<p>Sensitivity Study</p>
<p>Sensitivity of Hyper-Parameter</p>
<p>Since our method has two key parameters: P and Q, we conducted experiments to understand the contribution of each parameter to our algorithm.As illustrated in the left panel of Figure 2, the AUROC on each test set increases as P grows.However, when P &gt; 50, the improvement in AUROC becomes marginal compared to the significant gain observed when increasing P from 10 to 50.This suggests that the marginal benefit of further increasing P diminishes while simultaneously incurring substantial computational costs, given that the algorithm's time complexity is O(P • Q).Additionally, the poor performance observed when P = 10 can be attributed to the biased estimation of novelty scores when P is too small, a phenomenon influenced by multiple factors.For a more detailed explanation, please refer to Appendix A.1.</p>
<p>The right panel of Figure 2 demonstrates that when P remains constant, both excessively small and large values of Q negatively impact the algorithm's performance.This is due to the inaccuracy in local density estimation when Q is too small and the significant reduction in algorithm sensitivity when Q is too large.For further details, please refer to Appendix A.</p>
<p>Sensitivity of Design</p>
<p>In our Relative Neighbor Density algorithm, the notion "relative", i.e. comparing idea's local density with its neighbor's local densities, plays an important role.Moreover, other distance metric, such as Euclidean distance, could also be used in our algorithm.To understand the sensitivity of the current design, we conducted experiments by changing the design of the "relative" notion, and distance metric.The result presented in Table 3</p>
<p>Case Study</p>
<p>We visualize the neighbors of both a novel and a non-novel idea in the embedding vector space to demonstrate the superiority of our algorithm.Figures 3 and 4 show the visualization results of the embedding vectors of an idea and its neighbors on a two-dimensional plane, after dimensionality reduction using t-Distributed Stochastic Neighbor Embedding (t-SNE).While t-SNE excels at preserving the local structure of the data, it does not reliably retain the global structure(Van der Maaten and Hinton [2008]).As a result, the distance between the idea and its P adjacent neighbors is not accurately preserved, but distances between these P neighbors and their Q nearest neighbors is well preserved.</p>
<p>We first use Attention is All You Need (Vaswani et al. [2017]), a highly cited article, as a non-novel idea from the current perspective.Figure 3 clearly illustrates that there is a dense cluster of neighbors around the idea.In contrast, the neighbors around the idea's P neighbors are relatively few and sparse.</p>
<p>Next, we use Evaluating the World Model Implicit in a Generative Model (Vafa et al. [2025]), an article considered highly novel by the NeurIPS 2024 Program Chairs, as an example of a novel idea, based on their comments (openreview [2025]).In Figure 4, it is evident that the idea's local neighbor density is much sparser than its P nearest neighbors.</p>
<p>The experimental results demonstrate that the novelty of an idea is reflected in the local structure of the most similar documents to the idea within the embedding vector space, which supports the correctness of our algorithm in principle.Furthermore, a key difference between Figures 3 and 4 is that Figure 3 shows multiple neighboring clusters centered around the idea's P nearest neighbors, suggesting that the vector density of the two images in the embedding space is notably different.This highlights that the novelty of an idea cannot be determined solely by local density of the idea but must also take into account the vector density surrounding the idea.This is also clearly reflected in the experimental results for the Mixed test set in Table 3.</p>
<p>Discussion</p>
<p>In this work, we proposed a novel neighbor density-based metric for assessing research idea novelty, addressing the limitations of LLM judgment and absolute local density-based metrics.By leveraging large-scale literature embeddings from both biomedical sciences and computer science, our approach ensures robust reliability and cross-domain generalizability.Additionally, we introduced a scalable validation framework that eliminates reliance on expert labeling, enabling objective and reproducible novelty assessment.</p>
<p>Why a Non-LLM Novelty Assessment Algorithm is Necessary?</p>
<p>Assessing the novelty of a research idea is inherently difficult, subjective, and resource-intensive.While LLMs have the potential to assist in this process, their effectiveness is limited by the challenges outlined in the Introduction.Our experiments (see Table 2) highlight these issues: without an integrated search tool, even the most advanced reasoning models' performance was comparable to random guessing (AUROC =0.5).When a search tool was introduced, Sonnet-3.7 achieved similar accuracy on the NeurIPS test set (AUROC =0.8) but experienced significant degradation (AUROC =0.6) on both the Nature Medicine and cross-domain test sets.In contrast, our proposed RND algorithm can produce more reliable and consistent results, as seen in Table 2. Our algorithm is better at distinguishing genuinely novel ideas from the large pool of candidates from mixing research domains (AUROC =0.78 v.s Other's AUROC&lt;=0.6).Such cross-domain novelty assessment capability is crucial to AI scientist, as more and more innovation happened in inter-discipline of research domains.</p>
<p>Recent advancements in reinforcement learning for reasoning models, such as those demonstrated in Deepseek-R1 (Guo et al. [2025]), suggest the potential of rule-based reward systems to guide model development.Our RND algorithm could serve as a sophisticated rewarding mechanism, potentially enhancing reasoning model's capabilities in generating novel scientific ideas and advancing the role of AI in scientific innovation.</p>
<p>Accuracy in Each Domain</p>
<p>In</p>
<p>Domain-invariant Accuracy</p>
<p>However, when tested in the cross-domain test set (the Mixed test set), which includes ideas from both computer science and biomedicine, the performance of HD significantly degraded, with its AUROC dropping to 0.362.In contrast, the AUROC of our proposed algorithm remained robust at 0.795, similar to its performance in the single-domain test sets.</p>
<p>As demonstrated in the Table 3, the relative position of idea's local density among all of its neighbor's local density is crucial for comparing novelty across different domains.</p>
<p>We argue that RND algorithm hold domain-invariant property, i.e. the distribution of novelty scores produced by RND is identical regardless of the tested domain, which explained why our relative density-based approach succeeds in cross-domain scenarios.According to the mathematical reasoning in Appendix A.3, we concluded the distribution of novelty score S is only subject to P (the number of neighbors considered); thus it is invariant to the validation domain.Furthermore, in Figure 1 (right panel) and 5, the actual distribution of scores echoed the theoretical analysis.Such domain-invariant property is crucial for conducting multi-disciplinary scientific research, where ideas from diverse fields must be compared and evaluated effectively.</p>
<p>Why Validation Methods Differ Between Novel and Non-Novel?</p>
<p>When building our test set, an obvious approach might be to use symmetrical sources -for example, using accepted NeurIPS papers as novel samples and rejected NeurIPS papers (specifically those rejected for lacking novelty) as non-novel samples.However, this approach presents significant limitations.Firstly, very few top-tier venues publicly release review comments with explicit novelty assessments, making such data scarce and difficult to generalize across domains.Secondly, papers may be rejected for "lack of novelty" due to incremental advances or methodological similarities, even when addressing previously unexplored topics.</p>
<p>Instead, our definition of novelty relies on how extensively similar ideas have been studied in the literature.Following this definition, we selected highly-cited papers from recent years as our non-novel samples, as these papers represent ideas that have been thoroughly explored and extended by numerous subsequent works.While high citation count itself can indicate either novelty or utility, papers that are both recent and highly-cited typically represent research areas that have quickly become crowded with similar work, making the original contributions less novel by our working definition.Further details on our sampling methodology can be found in Section 4.1.</p>
<p>Limitations &amp; Future Work</p>
<p>Several limitations of our work warrant further exploration.</p>
<p>First, the algorithm relies heavily on large-scale literature databases with semantic embeddings.Biases in the literature database could potentially influence novelty assessments, especially if certain areas of research are underrepresented or if publication biases exist within fields.</p>
<p>Second, the algorithm's performance is also dependent on the quality of semantic embeddings for representing complex scientific concepts.While the M3 model demonstrated effectiveness, domain-specific fine-tuning could potentially improve performance.Future work should investigate specialized embedding models for scientific literature that better capture the complex semantics of scientific abstracts, particularly for technical terminology and methodological nuances.</p>
<p>Third, our validation methodology, while avoiding the need for expert labeling, relied on non-novel samples that may be too easily distinguishable from novel ones.By using historical highly-cited papers as non-novel examples, rather than borderline cases such as recently rejected papers or incremental work from current journals, we created a simplified assessment scenario compared to the subtle distinctions scientists face in real research settings.However, the fact that none of the tested algorithms achieved saturated AUROCs even in this relatively straightforward scenario demonstrates the fundamental challenge of novelty assessment and validates our comparative analysis.</p>
<p>Looking ahead, we envision several promising directions for future work:</p>
<ol>
<li>
<p>Integration with AI Research Workflows: Incorporating our novelty evaluation algorithm into end-to-end AI scientist workflows would enable autonomous research ideation and evaluation.This integration would allow AI systems to independently generate research hypotheses, assess their novelty using our domain-invariant RND algorithm, and prioritize the most promising directions for further investigation.Such integration could accelerate scientific discovery by efficiently navigating complex multi-disciplinary research landscapes where human intuition about novelty is often limited.</p>
</li>
<li>
<p>Enhancing Reasoning Model: As highlighted in our discussion, current reasoning models struggle with reliable novelty assessment across domains.We propose utilizing our RND algorithm as a sophisticated reward mechanism within reinforcement learning frameworks for training reasoning models for AI research.By providing domain-invariant novelty signals during training, we could potentially guide models to generate more innovative scientific ideas while maintaining scientific validity.</p>
</li>
</ol>
<p>These advancements would further enhance AI's role in scientific research by accelerating idea generation, refining research hypotheses, and potentially uncovering interdisciplinary connections that might otherwise remain unexplored.</p>
<p>Appendix A Algorithm Analysis</p>
<p>A.1 Effects of Parameter P</p>
<p>The novelty score is computed as
score i = |{N D ∈ S i | N D ≤ N D i }| P × 100,(12)
where S i = {N D j | j = 1, 2, . . ., P } is the set of neighbor densities for the P nearest neighbors of the idea i, and N D i is the neighbor density for idea i itself.</p>
<p>Empirical Cumulative Distribution Function (ECDF) Interpretation</p>
<p>Define the empirical cumulative distribution function (ECDF) for the set S i as
F P (x) = 1 P P j=1 1 {N Dj ≤x} ,
where 1 {N Dj ≤x} is the indicator function that is 1 if N D j ≤ x and 0 otherwise.Then, by definition, the novelty score can be written as score i = 100
• F P (N D i ). (13)</p>
<p>Consistency of the ECDF</p>
<p>Let F (x) be the true cumulative distribution function of the neighbor densities (assumed to be i.i.d.samples from a distribution F ).By the Glivenko-Cantelli theorem, the ECDF F P (x) converges uniformly to F (x) as P → ∞:
sup x |F P (x) − F (x)| P →∞ − −−− → 0.
Thus, for a sufficiently large P , we have
F P (N D i ) ≈ F (N D i ).
(14) This shows that the score, being proportional to F P (N D i ), converges to 100 • F (N D i ), which is the true quantile of N D i in the distribution of neighbor densities.</p>
<p>Variance and Sensitivity with Finite P</p>
<p>For a finite sample size P , F P (N D i ) is a random variable whose variance depends on P .Under the assumption of i.i.d.sampling,
Var(F P (N D i )) = F (N D i )(1 − F (N D i )) P .
Thus, the standard deviation is proportional to 1 √ P .This quantifies that:</p>
<p>• Smaller P : The variance Var(F P (N D i )) is larger, leading to a noisier (less reliable) estimation of the quantile, and hence of the novelty score.• Larger P : The variance decreases, yielding a more accurate estimation of the true quantile F (N D i ).</p>
<p>The novelty score becomes less sensitive to random fluctuations when P is large, as the empirical quantile is a better estimator of the true quantile.</p>
<ol>
<li>Discreteness of the Score for Small P When P is small, the possible values of F P (N D i ) are discrete, specifically:
F P (N D i ) ∈ 0, 1 P , 2 P , . . . , 1 .
For instance, if P = 1, then F 1 (N D i ) can only be 0 or 1, corresponding to a score of either 0% or 100%.This coarse granularity can result in a biased or uninformative measure of novelty.As P increases, the steps 1 P become finer, allowing the score to capture more subtle differences in the density distribution.</li>
</ol>
<p>Conclusion</p>
<p>The parameter P affects the final novelty score in two major ways:</p>
<ol>
<li>Accuracy: As P increases, the empirical cumulative distribution F P (x) better approximates the true cumulative distribution F (x), leading to a more accurate quantile estimate F (N D i ). 2. Variance: The variance of the estimate F P (N D i ) is proportional to 1 P .Thus, a larger P reduces the variability of the score, making it less sensitive to random noise.In summary, a higher P leads to a more robust and sensitive measure of novelty, while a smaller P results in a discrete and noisier estimate.</li>
</ol>
<p>A.2 Effects of Parameter Q</p>
<p>The neighbor density (ND) is given by:
N D = 1 Q Q k=1 d k ,(15)
where
d k = d(v, v k )
represents the distance between the point v and its k-th nearest neighbor.</p>
<p>Assuming that d k are independent and identically distributed (i.i.d.) random variables with mean E[d k ] = µ d , we compute the expectation of ND:
E[N D] = E 1 Q Q k=1 d k = 1 Q Q k=1 E[d k ] = 1 Q Qµ d = µ d .(16)
The variance of ND is given by:
Var(N D) = Var 1 Q Q k=1 d k . (17)
Using the property that the variance of the mean of Q i.i.d.random variables is:
Var 1 Q Q k=1 d k = 1 Q 2 Q k=1 Var(d k ).(18)
Since each d k has variance σ 2 d , we obtain:
Var(N D) = Qσ 2 d Q 2 = σ 2 d Q .(19)</p>
<p>A.2.1 Interpretation of Variance Scaling</p>
<p>The derived formula:
Var(N D) = σ 2 d Q (20)
shows that:</p>
<p>• As Q increases, the variance of ND decreases.</p>
<p>• Specifically, variance scales inversely with Q, meaning that larger Q results in a more stable estimate of ND.</p>
<p>• When Q → ∞, Var(N D) → 0, indicating that ND converges to its expected value µ d , which would cause lost of information on local density.• For small Q, ND exhibits higher variability, making it more sensitive to local fluctuations.</p>
<p>A.3 Domain-Invariant</p>
<p>A.3.1 Theoretical Analysis</p>
<p>Consider a test set in which each idea is assigned a neighborhood density defined as
N D = 1 Q Q i=1 d(idea, a i ),(21)
where a i denotes the ith nearest article in the literature corpus and d(•, •) is the cosine distance.</p>
<p>Let F (x) be the cumulative distribution function (CDF) of the neighborhood densities in the literature corpus.The percentile score for an idea is then defined by S = F (N D).</p>
<p>(22) By the probability integral transform, if N D is drawn from a distribution with CDF F (x), then S ∼ U(0, 1).</p>
<p>(
)23
In practice, F (x) is estimated empirically using P articles from the neighborhood.The empirical CDF is given by
FP (x) = 1 P P j=1 1{N D j ≤ x},(24)
where N D j is the neighborhood density of the jth article, and 1{•} is the indicator function.Since
P j=1 1{N D j ≤ x} ∼ Binomial(P, F (x)),(25)
we have
E[ FP (x)] = F (x) and Var[ FP (x)] = F (x)(1 − F (x)) P .(26)
Now, consider two literature corpora: a medical corpus with density distribution F M (x) and a computer science corpus with density distribution F C (x).For an idea in the test set, define its scores as
ŜM = FM (N D M ) and ŜC = FC (N D C ),(27)
where N D M and N D C are the neighborhood densities computed using the respective corpora.According to equation 26, we have
E[ Ŝ] = F (N D) and Var[ Ŝ] = F (N D)(1 − F (N D)) P . (28)
where F (N D) ∼ U(0, 1), which implies
ŜM d = ŜC .(29)
Furthermore, note that the variance of the empirical estimate FP (x) is solely a function of P :
Var[ FP (x)] = F (x)(1 − F (x)) P .(30)
Thus, when P changes (e.g., P = 50, 100, or 500), the change in variance-and hence the fluctuation in the score-is proportional to 1 P and is independent of the corpus.In other words,
∆ Var ∝ 1 P ,(31)
which holds for both the medical and the computer science datasets.</p>
<p>Therefore, we conclude that:
ŜM d = ŜC and ∆ Ŝ ∝ 1 P . (32)
This establishes that the scoring distributions for the test set are identical across corporas, and the effect of changing P on the score variation is equivalent for all datasets.Task description: You are a researcher who is reviewing a paper that was submitted to a computer science venue.Be critical and cautious in your decision.If a paper is bad or you are unsure, give it bad scores and reject it.Below is a description of the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.Reviewer guidelines: 1. Summary: Briefly summarize the paper and its contributions.This is not the place to critique the paper; the authors should generally agree with a well-written summary.2. Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: -Originality: Are the tasks or methods new?Is the work a novel combination of well-known techniques?(This can be valuable!)Is it clear how this work differs from previous contributions?-Quality: Is the submission technically sound?Are claims well-supported (e.g., by theoretical analysis or experimental results)?Are the methods used appropriately?Is this a complete piece of work or a work in progress?Are the authors careful and honest about evaluating both the strengths and weaknesses of their work?-Clarity: Is the submission clearly written?Is it well organized?(If not, please make constructive suggestions for improving its clarity.)Does it adequately inform the reader?(Note that a superbly written paper provides enough information for an expert reader to reproduce its results.)-Significance: Are the results important?Are others (researchers or practitioners) likely to use the ideas or build on them?Does the submission address a difficult task in a better way than previous work?Does it advance the state of the art in a demonstrable way?Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?3. Questions: Please list and carefully describe any questions and suggestions for the authors.Think of the things where a response from the author can change your opinion, clarify confusion, or address a limitation.This can be very important for a productive rebuttal and discussion phase with the authors.4. Ethical concerns: If there are ethical issues with this paper, please flag the paper for an ethics review.5. Overall: Please provide an "overall score" for this submission.Choices: -10: Award quality: Technically flawless paper with groundbreaking impact on one or more areas, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical considerations.</p>
<p>A.3.2 Experimental Evidence</p>
<p>-9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area and excellent impact on multiple areas, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-8: Strong Accept: Technically strong paper, with novel ideas, excellent impact on at least one area or high-toexcellent impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.</p>
<p>-7: Accept: Technically solid paper, with high impact on at least one sub-area or moderate-to-high impact on more than one area, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.</p>
<p>-6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, and ethical considerations.</p>
<p>-5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation.Please use sparingly.</p>
<p>-4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation.Please use sparingly.</p>
<p>-3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, and incompletely addressed ethical considerations.</p>
<p>-2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility, and mostly unaddressed ethical considerations.</p>
<p>-1: Very Strong Reject: For instance, a paper with trivial results or unaddressed ethical considerations Provided paper: Here is the paper you are asked to review: {paper} Output: Return a JSON object: <JSON> template <JSON> Role: You are an ambitious AI PhD student who is looking to publish a paper that will contribute significantly to the field.Task description: You have an idea and you want to check if it is novel or not.I.e., not overlapping significantly with existing literature or already well explored.Be a harsh critic for novelty, ensure there is a sufficient contribution in the idea for a new conference or workshop paper.You will be given the titles and abstracts of the 10 papers most relevant to your idea.Decide a paper idea is novel if after sufficient searching, you have not found a paper that significantly overlaps with your idea.Decide a paper idea is not novel, if you have found a paper that significantly overlaps with your idea.Set your decision to True if you think the idea is novel, set it to False if you think the idea is not novel.Your Idea: This is the idea you need to judge for novelty: {Idea} Top 10 relevant papers: {papers} Output: Return only True or False, dont return any other words.</p>
<p>Figure 1 :
1
Figure 1: Comparison of HD &amp; Our score distributions in different domains.1: In the right panel, the upper and lower bounds of the score exceeded the actual score range ([0, 100]) because of linear interpolation.2: to make the horizontal axis comparable, we scaled the Historical Dissimilarity scores by ×100.</p>
<p>Figure 2 :
2
Figure 2: Comparison of AUROC of RND algorithm with different parameters.left: AUROC with different P value when Q=50. right: AUROC with different Q value when P=100</p>
<p>Figure 3 :
3
Figure 3: Neighbor Distribution of a Non-novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 4 :
4
Figure 4: Neighbor Distribution of a Novel Idea in Embedding Space (t-SNE processed).</p>
<p>Figure 5 :
5
Figure 5: Score Distribution of RND algorithm with different P value.</p>
<ol>
<li>
<p>The Consensus Molecular Subtypes of Colorectal Cancer4.Fratricide-resistant CD7-CAR T cells in T-ALL.4. High-performance medicine: the convergence of human and artificial intelligence 5. International multicenter validation of AI-driven ultrasound detection of ovarian cancer.5.Understanding the tumor immune microenvironment (TIME) for effective therapy 6. Donor-derived GD2-specific CAR T cells in relapsed or refractory neuroblastoma.6.Intestinal microbiota metabolism of L-carnitine, a nutrient in red meat, promotes atherosclerosis 7. Single-nucleus chromatin accessibility and transcriptomic map of breast tissues of women of diverse exosomes educate bone marrow progenitor cells toward a pro-metastatic phenotype through MET 9. Echocardiographic screening for heart failure and optimization of the care pathway for individuals with pacemakers: a randomized controlled trial.9. Signatures of T cell dysfunction and exclusion predict cancer immunotherapy response Continued on next page Nature Medicine Test Set Positive Negative 10.Population-based, first-tier genomic newborn screening in the maternity ward.10.Neutralizing antibody levels are highly predictive of immune protection from symptomatic SARS-CoV-2 infection 11.Allogeneic CD5-specific CAR-T therapy for relapsed/refractory T-ALL: a phase 1 trial.11.Ischemia and reperfusion-from mechanism to translation 12. Transplantation of a genetically modified porcine heart into a live human.12. Mechanisms of fibrosis: therapeutic translation for fibrotic disease 13.A multi-modal single-cell and spatial expression map of metastatic breast cancer biopsies across clinicopathological features.13.Metabolite profiles and the risk of developing diabetes 14. ctDNA-based molecular residual disease and survival in resectable colorectal cancer.14.Mechanisms of NAFLD development and therapeutic strategies 15.Antifungal heteroresistance causes prophylaxis failure and facilitates breakthrough Candida parapsilosis infections.15.Inflammasomes: mechanism of action, role in disease, and therapeutics 16.Subcutaneous weekly semaglutide with automated insulin delivery in type 1 diabetes: a doubleblind, randomized, crossover trial.16.Chronic inflammation in the etiology of disease across the life span 17.Combined endurance and resistance exercise training in heart failure with preserved ejection fraction: a randomized controlled trial.17.Mutational Landscape of Metastatic Cancer Revealed from Prospective Clinical Sequencing of 10,000 Patients 18. Multi-omic profiling a defined bacterial consortium for treatment of recurrent Clostridioides difficile infection.18. Antibody responses to SARS-CoV-2 in patients with COVID-19 19.An organotypic atlas of human vascular cells.19.ABT-199, a potent and selective BCL-2 inhibitor, achieves antitumor activity while sparing platelets 20.Lipid profiling identifies modifiable signatures of cardiometabolic risk in children and adolescents with obesity.20.Clinical and immunological assessment of asymptomatic SARS-CoV-2 infections 21.Ferric carboxymaltose for anemia in late pregnancy: a randomized controlled trial.21.Extrapulmonary manifestations of COVID-19 22. Effects of conditional cash transfers on tuberculosis incidence and mortality according to race, ethnicity and socioeconomic factors in the 100 Million Brazilian Cohort.22.A guide to deep learning in healthcare 23.Phenome-wide associations of sleep characteristics in the Human Phenotype Project.23.A global survey of potential acceptance of a COVID-19 vaccine 24.Proteomic signatures improve risk prediction for common and rare diseases.24.The emerging role of lncRNAs in cancer 25.Remotely delivered weight management for people with long COVID and overweight: the randomized wait-list-controlled ReDIRECT trial.25.SARS-CoV-2 Entry Genes Are Most Highly Expressed in Nasal Goblet and Ciliated Cells within Human Airways 26.Sustained effect of prasinezumab on Parkinson's disease motor progression in the open-label extension of the PASADENA trial.26.Gut microbiota metabolism of dietary fiber influences allergic airway disease and hematopoiesis 27.Collaboration between clinicians and visionlanguage models in radiology report generation.27.The immunology of stroke: from mechanisms to translation 28.Oral obeldesivir provides postexposure protection against Marburg virus in nonhuman primates.28.Asthma phenotypes: the evolution from clinical to molecular approaches Continued on next page Nature Medicine Test Set Positive Negative 29.Digital consults in heart failure care: a randomized controlled trial.</p>
</li>
<li>
<p>Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy 58.Semaglutide in patients with overweight or obesity and chronic kidney disease without diabetes: a randomized double-blind placebo-controlled clinical trial.58.Identification of the molecular basis of doxorubicin-induced cardiotoxicity 59.Intracerebroventricular B7-H3-targeting CAR T cells for diffuse intrinsic pontine glioma: a phase 1 trial.59.New from NPG: Genome-wide association study identifies five new schizophrenia loci 60.AI-based selection of individuals for supplemental MRI in population-based breast cancer screening: the randomized ScreenTrustMRI trial.60.Senolytics Improve Physical Function and Increase Lifespan in Old Age 61.A toolbox for surfacing health equity harms and biases in large language models.61.Subtypes of Pancreatic Ductal Adenocarcinoma and Their Differing Responses to Therapy 62. Partitioned polygenic risk scores identify distinct types of metabolic dysfunction-associated steatotic liver disease.62.A purified membrane protein from Akkermansia muciniphila or the pasteurized bacterium improves metabolism in obese and diabetic mice 63.Multi-omics-based mapping of decidualization resistance in patients with a history of severe preeclampsia.63.The NALP3/NLRP3 Inflammasome Instigates Obesity-Induced Autoinflammation and Insulin Resistance 64.Electronic nudges for sustained influenza vaccination uptake in older adults: the nationwide randomized NUDGE-FLU-2 trial.64.IgE and mast cells in allergic disease 65.A time-stratified, case-crossover study of heat exposure and perinatal mortality from 16 hospitals in sub-Saharan Africa.65. Brown adipose tissue activity controls triglyc-bonemarrow-derived stromal cells to pulmonary alveoli protects against acute lung injury 87.A single-cell atlas of the peripheral immune response in patients with severe COVID-19 88.Determinants of response and resistance to CD19 chimeric antigen receptor (CAR) T cell therapy of chronic lymphocytic leukemia 89.Cancer epigenetics reaches mainstream oncology 90.Real-time tracking of self-reported symptoms to predict potential COVID-19 91.Metformin alters the gut microbiome of individuals with treatment-naive type 2 diabetes, contributing to the therapeutic effects of the drug 92.Synaptic plasticity and depression: new insights from stress and rapid-acting antidepressants 93.Matrix-embedded cells control osteoclast formation 94. Targeting EZH2 in cancer 95.Comprehensive molecular characterization of clinical responses to PD-1 inhibition in metastatic gastric cancer 96.Identification of miR-34a as a potent inhibitor of prostate cancer progenitor cells and metastasis by directly repressing CD44 97.Phenotype molding of stromal cells in the lung tumor microenvironment 98. Key roles of adjuvants in modern vaccines 99.AI in health and medicine Table 5: Titles of Novel (Positive) and Non-novel (Negative) Papers in Nature Medicine Test Set C Prompt C.1 Prompt for LLM with NeurIPS 2024 Review Guideline Prompt</p>
</li>
</ol>
<p>score ← |{N D∈D|N D≤N D Idea }| |D| × 100 10: Return score
1 Find Neighbors and Calculate Neighbor Density1: function NEIGHBOR(Input, P, Q)2:v Input ← GET_EMBEDDING(Input)▷ Using M3-Embedding model3:C ← []4:neighbors ← GET_NEIGHBORS(v Input , max(P, Q))▷ Find max(P, Q) nearest neighbors5:neighbors_f or_count ← neighbors[: Q]▷ Only use the Q nearest neighbors to calculate density6:neighbors_f or_distribution ← neighbors[: P ] ▷ The P nearest neighbors are used to calculate distribution7:for each paper in neighbors_f or_count do8:v paper ← GET_EMBEDDING(paper)9:distance ← 1 -COSINE_SIMILARITY(v Input , v paper )10:C.Append(distance)11:end for12:N D Input ← MEAN(C)13:return N D Input , neighbors_f or_distribution14: end functionAlgorithm 2 Calculate Novelty Score of Given Idea1: Input: Idea2: Output: A score in the range of 0 to 1003: D ← []4: N D Idea , neighbors ← NEIGHBOR(Idea, P, Q)5: for paper in neighbors do6:N D paper , _ ← NEIGHBOR(paper, P, Q)7:D.Append(N D paper )8: end for9:</p>
<p>Table 5
5Test setLabelCounttotal 2024-2025 2019-2023 2014-2018 -2014NeurIPSPositive Negative80 9980 00 310 680 0Nature MedicinePositive Negative66 9966 00 290 320 38</p>
<p>Table 1 :
1
Count of Data in Different Time Ranges for NeurIPS and Nature Medicine Test Sets.Positive: novel samples, Negative: non-novel samples.</p>
<p>Table 3 :
3
validate our statement.AUROC Comparison for Different Design .Absolute Local Density: Use the idea's local density as novelty score.(density calculated by mean distances between idea and idea's P first level neighbors).Euclidean distance: replace the cosine distance with Euclidean in RND
Test setAUROCRelative Neighbor Density(Ours) Absolute Local Density Euclidean distanceNeurIPS0.8200.8510.815Nature Medicine0.7650.7570.753Mixed0.7950.3950.78</p>
<p>NeurIPS test set, the neighbor density-based RND algorithm, absolute local density-based HD algorithm and Sonnet-3.7 with literature search tools achieved AUROC better than 0.8.When it comes to another domain (Nature Medicine
Idea First level neighbor Second level neighborNeighbor: Learning Knowledge Graph-based World Models of Textual Environments60Idea: Evaluating the World Model Implicit in a Generative Model40Neighbor: Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation20Neighbor: DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning0204060801007550250255075
test set for biomedical research), only RND and HD achieved AUROC at approximately 0.7; the other algorithms, including reasoning model such as Sonnet-3.7,degraded to AUROC 0.6.The strong performance of HD in the two respective domains suggests that measuring the local density (average semantic distance between a given idea and its nearest neighbors) in the historical literature database can effectively indicate the novelty of that idea in a single research domain.Since our proposed algorithm also incorporates semantic density, it exhibited similar accuracy level.</p>
<p>Table 6 :
6
Prompt for LLM with NeurIPS 2024 Review Guideline C.2 Prompt for Standardized Project Proposals For model selection, if any version of Claude is mentioned, change it to the latest version of Claude (Claude-3.5);if any version of LLaMA is mentioned, change it to the latest version LLaMA-3.Do not make any other model changes.Now directly generate the edited student idea to match the format of the template.</p>
<p>Table 7 :
7
Prompt for Standardized Project Proposals C.3 Prompt for LLM with Literature Search Prompt</p>
<p>Table 8 :
8
Prompt for LLM with Literature Search</p>
<p>Long-term cardiovascular outcomes of COVID-19 83.Ketone body β-hydroxybutyrate blocks the NLRP3 inflammasome-mediated inflammatory disease 84.Large language models in medicine 85.In vivo photodynamic therapy using upconversion nanoparticles as remote-controlled nanotransducersContinued on next pagePromptRole: You are a writing assistant specialized in editing academic writing.Task: I will give you a student's research idea and an idea template.Your task is to edit the student's idea to follow the template's format.Student idea: Title {title} Main Idea {paper} Template: 1. Title: A concise statement of the main research question to be used as the paper title.2. Problem Statement: Clearly define the problem your research intends to address.Explain clearly why this problem is interesting and important.3. Motivation: Explain why existing methods are not good enough to solve the problem, and explain the inspiration behind the new proposed method.You should also motivate why the proposed method would work better than existing baselines on the problem.4. Proposed Method: Explain how the proposed method works, describe all the essential steps. 5.Step-by-Step Experiment Plan: Break down every single step of the experiments, make sure every step is executable.Cover all essential details such as the datasets, models, and metrics to be used.If the project involves prompting, give some example prompts for each step.6. Test Case Examples: Give at least two concrete examples.The first example should show how the baseline method fails on the test case.If there are multiple baselines, give examples for all of them.The second example should show how the proposed method succeeds on the test case.For each test case, include the input (test example and the full prompt) and the expected output.You should also provide an explanation for why the outputs from the proposed prompt are better.If the proposed method has multiple steps, break them down into intermediate steps.7. Fallback Plan: Propose some alternative plans for what should the students do if the proposed method doesn't manage to satisfy the success criteria.For example, you can suggest additional analysis to help debug why the proposed method didn't work, which could inform alternative new methods, or just turn the project into an analysis paper instead by offering some interesting ablation and insights.Requirement: Make sure that you only edit the wording and formatting, including things like punctuation, capitalization, linebreaks, and bullet points.Also make sure to edit any informal wording and phrasing to use vocabulary that sounds like the template's writing style.No other changes are allowed beyond these.You should use tab as indentation and make sure to use appropriate nested indentation for sub-bullets.All bullets should have a clear hierarchy so people can easily differentiate the sub-bullets.Only leave empty lines between sections and remove any extra line breaks.If many bullet points are clustered together in a paragraph, separate them clearly with indentation and appropriate bullet point markers.Change to a new line for each new bullet point.For the fallback plan, do not list a bunch of bullet points.Instead, condense them into one coherent paragraph.For line breaks, avoid Raw String Literals or Double Backslashes when using " n", and change them to spaces or tabs.For in-line citations, if the citation mentioned the author's last name (like "(Si et al., 2023)" or "(An et al., 2024)"), you should keep them there; but if the citation is just a number (like "[1]" or "[3,4,5]"), you should just remove it and do some necessary rephrasing to make the sentence still sound coherent without the references.Apart from minor rephrasing and changing formatting, do not change any content of the idea.You must preserve the exact meaning of the original idea, do not change, remove, or add any other details.Do not drop any sections (including test case examples).Do not rename any models, datasets, or methods.Do not drop clarification or examples in brackets and do not drop any data source mentions (e.g., Chatbot Arena or Wildchat)!Note that when indexing test case examples, each test case example could have multiple steps of inputs and outputs and you shouldn't give separate indices to them.Each test case example should be a whole set of input-output pairs for the baseline(s) and proposed method.For the proposed method section, avoid any big changes.If the section comes in as a coherent paragraph, you don't have to break it down into bullet points.If the section is already in bullet points, you should keep it that way.If the section is a mix of both, you should keep the bullet points and the coherent paragraph as they are.Keep all the clarification and examples mentioned in all the sections and do not remove any of them (including those in brackets).D Result in Detail
Claude 3.7 sonnet and claude code. Anthropic, 2025. Feb 28th, 2025</p>
<p>Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu, 2024</p>
<p>Cornell Tech, Arxiv annual report 2023. 2023. Feb 25th, 2025</p>
<p>Arxiv dataset. 2025. Jan 10, 2025Cornell University</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.14255October 2024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Pubmed and beyond: biomedical literature search in the age of artificial intelligence. Qiao Jin, Robert Leaman, Zhiyong Lu, EBioMedicine. 1002024</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. September 2024</p>
<p>Evaluating the world model implicit in a generative model. 2025. Jan 10, 2025. 2025. Feb 26th, 2025National Library of Medicine. Pubmed download</p>
<p>Official review of submission1763. 2025. March 7th, 2025Openreview Reviewer 2X9t</p>
<p>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109September 2024</p>
<p>Robustness of LLMs to Perturbations in Text. Ayush Singh, Navpreet Singh, Shubham Vatsal, arXiv:2407.08989July 2024</p>
<p>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.09403October 2024</p>
<p>Evaluating the world model implicit in a generative model. Keyon Vafa, Justin Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan, Advances in Neural Information Processing Systems. 372025</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, Yuan-Fang Li, arXiv:2301.12868March 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2034 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2034</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2034</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-276741579</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.00735v1.pdf" target="_blank">LADDER: S ELF -I MPROVING LLM S T HROUGH R ECURSIVE P ROBLEM D ECOMPOSITION</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2034.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2034.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LADDER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning through Autonomous Difficulty-Driven Example Recursion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-generation and self-improvement framework that has an LLM recursively generate progressively easier variants of complex problems, verify solutions with a numerical checker, and then apply reinforcement learning (GRPO) on the resulting variant tree to bootstrap problem-solving capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated difficulty-based ordering (recursive variant generation)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>For each root problem the base LLM generates a tree of variants by sampling 3–5 mathematical transformations (from a transformation library) and producing batches of 10 variants per prompt; recursion is applied to create simpler variants (tree depth capped: 3 for Llama experiments, 2 for MIT experiments). Diversity was increased using temperature cycling (0.8–1.4) and persona-based prompting (e.g., 'think like Euler'), and the generation process was filtered for unsolvable or degenerate outputs. A group of generated variants per training question forms a natural difficulty gradient used as the curriculum for GRPO reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama 3.2 3B (training/ablation experiments); Qwen 2.5 7B Deepseek-R1 Distilled (MIT Integration Bee experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Mathematical indefinite integration (benchmarks: undergraduate integrals and MIT Integration Bee qualifying exam)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Integrals ranged from simple single-technique antiderivatives to compositional problems requiring multiple techniques and novel substitutions; curriculum trees produced intermediate tasks that reduced complexity stepwise. Typical variant trees: root → level1 → level2 (depth 2–3); number of generated variants: ~500 per training problem in Llama experiments (≈5,000 total from 10 roots), 9,000 variants for MIT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>pass@1 sampling (base), pass@10 sampling (base), RL without variants (RL trained only on original training problems)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Llama 3.2 3B on undergraduate test set: pass@1 baseline 1%, pass@10 baseline 2%, RL without variants ≤3% (collapsed to 0% within 30 steps), LADDER (RL with variants) 82% accuracy (test); MIT Integration Bee (Qwen 2.5 7B): base model 50% → LADDER 73% accuracy; comparison models: GPT-4o 42%, OpenAI o1 80% (o1 not using numerical checker).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>LADDER learning curve showed rapid gains from step 0 to ~250 steps before plateauing at ~82% (Llama experiments). RL without variants failed quickly—never exceeded ~3% and collapsed to 0% within ~30 training steps—showing the curriculum enabled stable learning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Variants were generated from a small training set (10 problems) yet LADDER generalized to a 100-problem undergraduate test set (82%); for MIT experiments historical exams 2010–2024 were used to generate variants and none of the 2025 test items appeared in the variant set, yet LADDER achieved 73%, indicating strong out-of-distribution generalization within the integration domain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Diversity measures: exact-match overlap between generated variants and test set was minimal (5 matches among ~5,000 variants). Techniques to improve diversity (batch size =10, temperature cycling 0.8–1.4, persona prompting) were required; naive single-variant prompts produced repetitive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Curriculum encodes prerequisites implicitly via recursive generation of simpler variants (parent→child), but the paper did not implement or evaluate an explicit automatic prerequisite graph or adaptive prerequisite selection.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — the method explicitly generates intermediate/bridging tasks via recursive variant trees (depth 2–3). These intermediate tasks were effective: LADDER succeeded where RL on root problems alone failed, indicating usefulness of intermediate variants for learning techniques and bootstrapping more complex solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Generated variants sometimes deviated from intended difficulty (many harder than parent); ~8% of variants in Llama experiments were effectively unsolvable (symbolic solver >5s). Models could produce degenerate or trivially incorrect outputs exploited verification (e.g., outputting integration symbol), necessitating filters. Variant quality control consumed substantial compute.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported generation scales and costs: Llama experiments produced ~500 variants per training problem (≈5,000 variants total); MIT used ~9,000 variants. Verification used adaptive quadrature with 2s timeout per attempt and up to 3 retries. RL hyperparameters: KL coefficient 0.001; batch sizes varied (MIT used batch size 128). TTRL per unsolved test question used ~800 variants and 100 RL steps. The paper notes substantial wasted verification compute due to hard/unsolvable variants but does not provide wall-clock timings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human expert scoring or human-curated curriculum comparisons were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Recursive LLM-generated curricula (LADDER) reliably create a smooth difficulty gradient that enables RL (GRPO) to learn complex integration skills that fail under training on root problems alone, producing large gains (Llama 3B: 1–2% → 82%; Qwen 2.5 7B: 50% → 73%). However, variant generation quality control is a bottleneck: some variants are harder than intended and waste compute, and the curriculum is non-adaptive in the current implementation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2034.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2034.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time extension of LADDER that generates problem-specific variant trees at inference and applies short, targeted RL runs to adapt the model to a single hard test instance, then rolls back parameters for the next instance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated difficulty-based ordering applied at test-time (on-the-fly variant synthesis per test question)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>For each difficult test question TTRL generates a focused tree of variants (≈800 variants per unsolved test question in MIT experiments, recursion limited to 2 levels), then runs GRPO for a short number of steps (100 RL steps reported) using the problem-specific variants as the curriculum; if any variant-guided update produces a correct solution for the original question (verified numerically), the question is considered solved. After finishing, model parameters are rolled back for the next test question.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen 2.5 7B Deepseek-R1 Distilled (applied after LADDER tuning); experiments also applied TTRL to base pre-LADDER model as ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Mathematical indefinite integration (MIT Integration Bee qualifying problems)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Focused on single highly-crafted MIT integration problems; complexity varied per question — number of RL steps required to solve an unsolved test question ranged from 3 to 30 steps, indicating different levels of compositional difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Repeated pass@k sampling (naive test-time sampling), applying TTRL directly to base (pre-LADDER) model as ablation, and LADDER-only (no TTRL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On MIT Integration Bee (Qwen 2.5 7B): LADDER alone 73% accuracy; LADDER + TTRL increased to 90% accuracy. Ablation: applying TTRL directly to base (pre-LADDER) model for 100 steps failed to solve the same questions (no improvement), indicating dependency on prior LADDER training.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Per-question RL steps to success varied between 3 and 30; TTRL used 100 RL steps in experiments and typically solved additional 3–4 questions per exam (raising score from 73% to 90%). TTRL was more effective than naive repeated sampling (pass attempts) at test-time.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>TTRL is targeted to individual test questions (on-the-fly adaptation) rather than global generalization; when applied after LADDER it solved previously unsolved, hard test questions, but TTRL applied to base model without prior LADDER exposure produced no successful generalization to those questions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not explicitly quantified for TTRL beyond reporting ~800 variants generated per question; the paper notes TTRL's on-the-fly synthesis can explore novel integrals and expand training distribution for that specific question.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>TTRL implicitly constructs intermediate variants that serve as prerequisites for the target question; the paper reports that solving more complex questions often required generating and learning from a chain of solved variants, but no explicit prerequisite discovery algorithm was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — TTRL generates intermediate variants per test question (depth 2 trees, ≈800 variants) which serve as the curriculum for per-question RL; these were effective in enabling the model to solve several hard test questions it previously failed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>TTRL success depended on prior LADDER exposure; applying TTRL alone to the base model failed. Also, variant generation quality at test-time can produce many irrelevant or harder variants that waste compute.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Per-unsolved-question TTRL used ≈800 generated variants and 100 RL steps; number of RL steps to success per question ranged 3–30. Paper claims TTRL scales test-time compute and can be parallelized, but reports no wall-clock times or FLOP counts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human expert evaluation reported for TTRL outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>TTRL provides an effective test-time curriculum: by generating problem-specific variants and performing short RL on them, the model improved from 73% to 90% on the MIT Integration Bee. However, TTRL requires prior exposure from LADDER to be effective and incurs extra per-question computation (variant gen + RL).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2034.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2034.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Variant Generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-enabled Recursive Variant Generation (transformation library, batch prompts, persona/temperature cycling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete method for creating curricula by instructing an LLM to apply mathematical transformations to produce easier or equivalent problem variants, using batching, temperature cycling, and persona prompts to increase diversity and recursively produce a tree of intermediate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated with rule-guided transformations and stochastic sampling</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Construct a transformation library (simple → complex operations). For each integral sample 3–5 transformations and prompt the LLM to produce variants in batches of 10 variants per prompt; vary sampling temperature between 0.8–1.4 and rotate 'persona' prompts to encourage different mathematical perspectives. Apply recursion to child variants to build trees (depth capped at 2–3). Post-generate filtering and numerical verification remove degenerate/unsolvable variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Base LLMs used for generation: Llama 3.2 3B (development), GPT-4o was used as a source for synthetic training problems (not for variant generation experiments reported as main results); Qwen 2.5 7B used in MIT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Mathematical integration (symbolic/numeric), but method presented as generalizable to other verifiable domains (e.g., programming with unit tests, formal proof assistants).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Variant generation aimed to produce stepwise simpler tasks spanning small algebraic simplifications up to removing nested functions; tree depth 2–3 created chains of intermediate tasks to bridge complex composites.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Naive single-variant prompts (smaller batch sizes), no-persona/no-temperature-cycling generation (both noted to be inferior), and hand-selected training-only data (RL without variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Effect on downstream RL: when using this LLM-generated curriculum, Llama 3.2 3B rose to 82% test accuracy; naive generation (smaller batch sizes, no persona/temperature cycling) produced repetitive variants and poorer RL performance (quantitative drop not enumerated beyond qualitative statements). Approximately 8% of generated variants were unsolvable in Llama experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Batch generation of 10 per prompt and persona/temperature strategies increased diversity and gave faster/stabler RL convergence; exact comparative numeric timelines aside from overall LADDER learning curve (rapid up to ~250 steps) were not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>The LLM-generated curriculum produced variants that generalized beyond training roots (10 training problems → success across 100-test problem set); exact ablation numbers on alternative generation strategies are qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Measured via exact-match overlap with test set (5 matches among ~5,000 variants) and by qualitative assessment of repetitiveness; larger batch sizes, temperature cycling, and persona prompts significantly increased variant diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Variant generation deliberately constructs simpler variants that serve as prerequisites, but no algorithmic identification (e.g., dependency graphs) was used; prerequisite relationships are implicit in the parent→child variant structure.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes; recursive generation explicitly creates intermediate tasks (variants) intended to bridge difficulty between root and solvable leaves; tree depth and branching factor controlled to produce effective intermediates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Variants could be poorly calibrated (harder than parent), produce unsolvable cases (~8%), or be repetitive if generation parameters not tuned. The paper also notes LLM exploitation of verification (degenerate outputs) requiring filter rules.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generation volumes reported: ~500 variants per training problem (Llama experiments), ≈9,000 total for MIT experiments; generation quality-control and numerical verification (2s timeout per attempt, up to 3 retries) incurred nontrivial compute overhead but exact compute/time costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human curation/annotation of generated variants reported; variant quality-control was automated (timeouts, filters, numerical verification).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Carefully-engineered LLM prompting (batching, temperature cycling, personas) plus a transformation library and recursive generation reliably yields diverse intermediate tasks that form an effective curriculum for RL; however, quality control issues (unsolvable/harder-than-intended variants) and non-adaptivity are notable limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2034.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2034.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines & Comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pass@k sampling and RL without variants baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines used to evaluate curriculum benefit: pass@1 and pass@10 sampling of the base model, and reinforcement learning applied only to the small set of root training problems (no variants).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>pass@k: sampling-based inference; RL without variants: no curriculum (training only on original problems)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>pass@k uses multiple sampled outputs at inference to increase correct-answer probability (pass@1 and pass@10 reported). RL without variants performed GRPO training on only the small training set (10 root integrals) with identical hyperparameters to LADDER but without the generated variant curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama 3.2 3B (reported baselines for undergraduate integration experiments); comparative model values reported for MIT exam (GPT-4o, OpenAI o1) but not trained in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Mathematical integration benchmarks used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same integration problems as LADDER experiments; baseline RL without variants struggled on compositional problems requiring multiple techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>These are themselves the baseline methods used for comparison to LADDER and TTRL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Llama 3.2 3B undergraduate test: pass@1 1%, pass@10 2%; RL without variants: never exceeded ~3% and collapsed to 0% within ~30 steps. MIT: baseline Qwen 2.5 7B before LADDER 50% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>RL without variants collapsed quickly (to 0% within ~30 steps) whereas LADDER converged to high accuracy over ~250 steps, indicating a stable curriculum speeds and stabilizes learning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>RL without variants failed to generalize beyond the small training set; pass@k sampling provided negligible gains (1→2%).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable for pass@k; RL without variants had no generated-task diversity and therefore poor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>None — baselines do not construct prerequisites or intermediate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No — RL without variants had no intermediate tasks; pass@k is an inference-time sampling strategy, not a curriculum.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Pass@k sampling alone cannot compensate for lack of learned techniques; naive RL without an appropriate difficulty gradient leads to collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>pass@10 increases inference-sample cost relative to pass@1 but provided minimal performance benefit; RL without variants used the same RL hyperparameters as LADDER but produced no beneficial outcomes, representing wasted training compute under identical settings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human evaluation reported for baselines beyond reporting standard model scores.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Simple inference strategies (pass@k) or RL without a curriculum are insufficient: pass@10 only increased accuracy from 1% to 2%, and RL without variants failed (collapsed), demonstrating that a structured, difficulty-graded curriculum is critical for stable RL-based learning on compositional integration tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2034.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2034.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRPO (used RL algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group Relative Policy Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL optimization technique used in the paper that estimates baselines from grouped sample rewards instead of a separate critic, used to train the LLM policy on variant trees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>reinforcement learning-based optimization (group-based advantage estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>GRPO samples a group of outputs per question and computes an advantage for each sample using group mean and std of rewards; objective maximizes expected group-normalized advantage. Used with simple rule-based rewards (accuracy and format) and a KL-coefficient of 0.001. GRPO was applied both in LADDER (training) and in TTRL (test-time per-question RL).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Applied to LLM policy optimization in mathematical integration tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Optimizes policy to produce correct symbolic integration solutions; rewards are sparse (binary correctness) and formatting reward helps reliable verification.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Not compared to other RL algorithms in the paper (only GRPO was used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used with LADDER curriculum, GRPO achieved large performance gains: Llama 3.2 3B 82% test accuracy and Qwen 2.5 7B LADDER 73% on MIT exam; with TTRL (GRPO applied at test-time) final MIT score rose to 90%. No ablation comparing alternative RL algorithms was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>GRPO runs with identical hyperparameters produced rapid learning under LADDER (convergence to plateau ~250 steps) while GRPO without variant curricula collapsed, indicating sensitivity of GRPO training to curriculum design.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Used GRPO trained on variant curricula generalized to held-out test sets (see LADDER results); no cross-algorithm generalization comparisons conducted.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not directly applicable to GRPO itself; success depended on diversity of variant curricula fed into GRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>GRPO does not perform prerequisite discovery; it optimizes policy given the curriculum provided by variant generation.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>GRPO consumes intermediate tasks (variants) for policy updates but does not itself generate them.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>No algorithmic limitations of GRPO were reported beyond sensitivity to the absence of a smooth curriculum (training collapse when trained only on the small root set).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>GRPO chosen for efficiency (no separate critic), but paper provides limited numerical compute cost details; used with KL coefficient 0.001 and varying batch sizes (e.g., 128 for MIT experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human evaluation of GRPO outputs beyond numerical verification of solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>GRPO served as an effective RL learner when supplied with a well-constructed LLM-generated curriculum, but GRPO training collapses if the curriculum lacks a smooth difficulty gradient (e.g., training only on hard root tasks).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can teach themselves to program better. <em>(Rating: 2)</em></li>
                <li>Self-Improvement and Automated Curriculum Generation in LLMs. <em>(Rating: 1)</em></li>
                <li>Self-Taught Reasoner (STaR) <em>(Rating: 1)</em></li>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: A framework and survey <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2034",
    "paper_id": "paper-276741579",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "LADDER",
            "name_full": "Learning through Autonomous Difficulty-Driven Example Recursion",
            "brief_description": "A curriculum-generation and self-improvement framework that has an LLM recursively generate progressively easier variants of complex problems, verify solutions with a numerical checker, and then apply reinforcement learning (GRPO) on the resulting variant tree to bootstrap problem-solving capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated difficulty-based ordering (recursive variant generation)",
            "curriculum_method_description": "For each root problem the base LLM generates a tree of variants by sampling 3–5 mathematical transformations (from a transformation library) and producing batches of 10 variants per prompt; recursion is applied to create simpler variants (tree depth capped: 3 for Llama experiments, 2 for MIT experiments). Diversity was increased using temperature cycling (0.8–1.4) and persona-based prompting (e.g., 'think like Euler'), and the generation process was filtered for unsolvable or degenerate outputs. A group of generated variants per training question forms a natural difficulty gradient used as the curriculum for GRPO reinforcement learning.",
            "llm_model_used": "Llama 3.2 3B (training/ablation experiments); Qwen 2.5 7B Deepseek-R1 Distilled (MIT Integration Bee experiments)",
            "domain_environment": "Mathematical indefinite integration (benchmarks: undergraduate integrals and MIT Integration Bee qualifying exam)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Integrals ranged from simple single-technique antiderivatives to compositional problems requiring multiple techniques and novel substitutions; curriculum trees produced intermediate tasks that reduced complexity stepwise. Typical variant trees: root → level1 → level2 (depth 2–3); number of generated variants: ~500 per training problem in Llama experiments (≈5,000 total from 10 roots), 9,000 variants for MIT experiments.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "pass@1 sampling (base), pass@10 sampling (base), RL without variants (RL trained only on original training problems)",
            "performance_metrics": "Llama 3.2 3B on undergraduate test set: pass@1 baseline 1%, pass@10 baseline 2%, RL without variants ≤3% (collapsed to 0% within 30 steps), LADDER (RL with variants) 82% accuracy (test); MIT Integration Bee (Qwen 2.5 7B): base model 50% → LADDER 73% accuracy; comparison models: GPT-4o 42%, OpenAI o1 80% (o1 not using numerical checker).",
            "learning_speed_comparison": "LADDER learning curve showed rapid gains from step 0 to ~250 steps before plateauing at ~82% (Llama experiments). RL without variants failed quickly—never exceeded ~3% and collapsed to 0% within ~30 training steps—showing the curriculum enabled stable learning.",
            "generalization_performance": "Variants were generated from a small training set (10 problems) yet LADDER generalized to a 100-problem undergraduate test set (82%); for MIT experiments historical exams 2010–2024 were used to generate variants and none of the 2025 test items appeared in the variant set, yet LADDER achieved 73%, indicating strong out-of-distribution generalization within the integration domain.",
            "task_diversity_analysis": "Diversity measures: exact-match overlap between generated variants and test set was minimal (5 matches among ~5,000 variants). Techniques to improve diversity (batch size =10, temperature cycling 0.8–1.4, persona prompting) were required; naive single-variant prompts produced repetitive outputs.",
            "prerequisite_identification": "Curriculum encodes prerequisites implicitly via recursive generation of simpler variants (parent→child), but the paper did not implement or evaluate an explicit automatic prerequisite graph or adaptive prerequisite selection.",
            "intermediate_task_generation": "Yes — the method explicitly generates intermediate/bridging tasks via recursive variant trees (depth 2–3). These intermediate tasks were effective: LADDER succeeded where RL on root problems alone failed, indicating usefulness of intermediate variants for learning techniques and bootstrapping more complex solutions.",
            "llm_limitations_observed": "Generated variants sometimes deviated from intended difficulty (many harder than parent); ~8% of variants in Llama experiments were effectively unsolvable (symbolic solver &gt;5s). Models could produce degenerate or trivially incorrect outputs exploited verification (e.g., outputting integration symbol), necessitating filters. Variant quality control consumed substantial compute.",
            "computational_cost": "Reported generation scales and costs: Llama experiments produced ~500 variants per training problem (≈5,000 variants total); MIT used ~9,000 variants. Verification used adaptive quadrature with 2s timeout per attempt and up to 3 retries. RL hyperparameters: KL coefficient 0.001; batch sizes varied (MIT used batch size 128). TTRL per unsolved test question used ~800 variants and 100 RL steps. The paper notes substantial wasted verification compute due to hard/unsolvable variants but does not provide wall-clock timings.",
            "human_expert_evaluation": "No human expert scoring or human-curated curriculum comparisons were reported.",
            "key_findings_summary": "Recursive LLM-generated curricula (LADDER) reliably create a smooth difficulty gradient that enables RL (GRPO) to learn complex integration skills that fail under training on root problems alone, producing large gains (Llama 3B: 1–2% → 82%; Qwen 2.5 7B: 50% → 73%). However, variant generation quality control is a bottleneck: some variants are harder than intended and waste compute, and the curriculum is non-adaptive in the current implementation.",
            "uuid": "e2034.0"
        },
        {
            "name_short": "TTRL",
            "name_full": "Test-Time Reinforcement Learning",
            "brief_description": "A test-time extension of LADDER that generates problem-specific variant trees at inference and applies short, targeted RL runs to adapt the model to a single hard test instance, then rolls back parameters for the next instance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated difficulty-based ordering applied at test-time (on-the-fly variant synthesis per test question)",
            "curriculum_method_description": "For each difficult test question TTRL generates a focused tree of variants (≈800 variants per unsolved test question in MIT experiments, recursion limited to 2 levels), then runs GRPO for a short number of steps (100 RL steps reported) using the problem-specific variants as the curriculum; if any variant-guided update produces a correct solution for the original question (verified numerically), the question is considered solved. After finishing, model parameters are rolled back for the next test question.",
            "llm_model_used": "Qwen 2.5 7B Deepseek-R1 Distilled (applied after LADDER tuning); experiments also applied TTRL to base pre-LADDER model as ablation.",
            "domain_environment": "Mathematical indefinite integration (MIT Integration Bee qualifying problems)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Focused on single highly-crafted MIT integration problems; complexity varied per question — number of RL steps required to solve an unsolved test question ranged from 3 to 30 steps, indicating different levels of compositional difficulty.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Repeated pass@k sampling (naive test-time sampling), applying TTRL directly to base (pre-LADDER) model as ablation, and LADDER-only (no TTRL).",
            "performance_metrics": "On MIT Integration Bee (Qwen 2.5 7B): LADDER alone 73% accuracy; LADDER + TTRL increased to 90% accuracy. Ablation: applying TTRL directly to base (pre-LADDER) model for 100 steps failed to solve the same questions (no improvement), indicating dependency on prior LADDER training.",
            "learning_speed_comparison": "Per-question RL steps to success varied between 3 and 30; TTRL used 100 RL steps in experiments and typically solved additional 3–4 questions per exam (raising score from 73% to 90%). TTRL was more effective than naive repeated sampling (pass attempts) at test-time.",
            "generalization_performance": "TTRL is targeted to individual test questions (on-the-fly adaptation) rather than global generalization; when applied after LADDER it solved previously unsolved, hard test questions, but TTRL applied to base model without prior LADDER exposure produced no successful generalization to those questions.",
            "task_diversity_analysis": "Not explicitly quantified for TTRL beyond reporting ~800 variants generated per question; the paper notes TTRL's on-the-fly synthesis can explore novel integrals and expand training distribution for that specific question.",
            "prerequisite_identification": "TTRL implicitly constructs intermediate variants that serve as prerequisites for the target question; the paper reports that solving more complex questions often required generating and learning from a chain of solved variants, but no explicit prerequisite discovery algorithm was evaluated.",
            "intermediate_task_generation": "Yes — TTRL generates intermediate variants per test question (depth 2 trees, ≈800 variants) which serve as the curriculum for per-question RL; these were effective in enabling the model to solve several hard test questions it previously failed.",
            "llm_limitations_observed": "TTRL success depended on prior LADDER exposure; applying TTRL alone to the base model failed. Also, variant generation quality at test-time can produce many irrelevant or harder variants that waste compute.",
            "computational_cost": "Per-unsolved-question TTRL used ≈800 generated variants and 100 RL steps; number of RL steps to success per question ranged 3–30. Paper claims TTRL scales test-time compute and can be parallelized, but reports no wall-clock times or FLOP counts.",
            "human_expert_evaluation": "No human expert evaluation reported for TTRL outputs.",
            "key_findings_summary": "TTRL provides an effective test-time curriculum: by generating problem-specific variants and performing short RL on them, the model improved from 73% to 90% on the MIT Integration Bee. However, TTRL requires prior exposure from LADDER to be effective and incurs extra per-question computation (variant gen + RL).",
            "uuid": "e2034.1"
        },
        {
            "name_short": "LLM-based Variant Generation",
            "name_full": "LLM-enabled Recursive Variant Generation (transformation library, batch prompts, persona/temperature cycling)",
            "brief_description": "A concrete method for creating curricula by instructing an LLM to apply mathematical transformations to produce easier or equivalent problem variants, using batching, temperature cycling, and persona prompts to increase diversity and recursively produce a tree of intermediate tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated with rule-guided transformations and stochastic sampling",
            "curriculum_method_description": "Construct a transformation library (simple → complex operations). For each integral sample 3–5 transformations and prompt the LLM to produce variants in batches of 10 variants per prompt; vary sampling temperature between 0.8–1.4 and rotate 'persona' prompts to encourage different mathematical perspectives. Apply recursion to child variants to build trees (depth capped at 2–3). Post-generate filtering and numerical verification remove degenerate/unsolvable variants.",
            "llm_model_used": "Base LLMs used for generation: Llama 3.2 3B (development), GPT-4o was used as a source for synthetic training problems (not for variant generation experiments reported as main results); Qwen 2.5 7B used in MIT experiments.",
            "domain_environment": "Mathematical integration (symbolic/numeric), but method presented as generalizable to other verifiable domains (e.g., programming with unit tests, formal proof assistants).",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Variant generation aimed to produce stepwise simpler tasks spanning small algebraic simplifications up to removing nested functions; tree depth 2–3 created chains of intermediate tasks to bridge complex composites.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Naive single-variant prompts (smaller batch sizes), no-persona/no-temperature-cycling generation (both noted to be inferior), and hand-selected training-only data (RL without variants).",
            "performance_metrics": "Effect on downstream RL: when using this LLM-generated curriculum, Llama 3.2 3B rose to 82% test accuracy; naive generation (smaller batch sizes, no persona/temperature cycling) produced repetitive variants and poorer RL performance (quantitative drop not enumerated beyond qualitative statements). Approximately 8% of generated variants were unsolvable in Llama experiments.",
            "learning_speed_comparison": "Batch generation of 10 per prompt and persona/temperature strategies increased diversity and gave faster/stabler RL convergence; exact comparative numeric timelines aside from overall LADDER learning curve (rapid up to ~250 steps) were not provided.",
            "generalization_performance": "The LLM-generated curriculum produced variants that generalized beyond training roots (10 training problems → success across 100-test problem set); exact ablation numbers on alternative generation strategies are qualitative.",
            "task_diversity_analysis": "Measured via exact-match overlap with test set (5 matches among ~5,000 variants) and by qualitative assessment of repetitiveness; larger batch sizes, temperature cycling, and persona prompts significantly increased variant diversity.",
            "prerequisite_identification": "Variant generation deliberately constructs simpler variants that serve as prerequisites, but no algorithmic identification (e.g., dependency graphs) was used; prerequisite relationships are implicit in the parent→child variant structure.",
            "intermediate_task_generation": "Yes; recursive generation explicitly creates intermediate tasks (variants) intended to bridge difficulty between root and solvable leaves; tree depth and branching factor controlled to produce effective intermediates.",
            "llm_limitations_observed": "Variants could be poorly calibrated (harder than parent), produce unsolvable cases (~8%), or be repetitive if generation parameters not tuned. The paper also notes LLM exploitation of verification (degenerate outputs) requiring filter rules.",
            "computational_cost": "Generation volumes reported: ~500 variants per training problem (Llama experiments), ≈9,000 total for MIT experiments; generation quality-control and numerical verification (2s timeout per attempt, up to 3 retries) incurred nontrivial compute overhead but exact compute/time costs not reported.",
            "human_expert_evaluation": "No human curation/annotation of generated variants reported; variant quality-control was automated (timeouts, filters, numerical verification).",
            "key_findings_summary": "Carefully-engineered LLM prompting (batching, temperature cycling, personas) plus a transformation library and recursive generation reliably yields diverse intermediate tasks that form an effective curriculum for RL; however, quality control issues (unsolvable/harder-than-intended variants) and non-adaptivity are notable limitations.",
            "uuid": "e2034.2"
        },
        {
            "name_short": "Baselines & Comparisons",
            "name_full": "Pass@k sampling and RL without variants baselines",
            "brief_description": "Baselines used to evaluate curriculum benefit: pass@1 and pass@10 sampling of the base model, and reinforcement learning applied only to the small set of root training problems (no variants).",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "pass@k: sampling-based inference; RL without variants: no curriculum (training only on original problems)",
            "curriculum_method_description": "pass@k uses multiple sampled outputs at inference to increase correct-answer probability (pass@1 and pass@10 reported). RL without variants performed GRPO training on only the small training set (10 root integrals) with identical hyperparameters to LADDER but without the generated variant curriculum.",
            "llm_model_used": "Llama 3.2 3B (reported baselines for undergraduate integration experiments); comparative model values reported for MIT exam (GPT-4o, OpenAI o1) but not trained in this work.",
            "domain_environment": "Mathematical integration benchmarks used in the paper.",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Same integration problems as LADDER experiments; baseline RL without variants struggled on compositional problems requiring multiple techniques.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "These are themselves the baseline methods used for comparison to LADDER and TTRL.",
            "performance_metrics": "Llama 3.2 3B undergraduate test: pass@1 1%, pass@10 2%; RL without variants: never exceeded ~3% and collapsed to 0% within ~30 steps. MIT: baseline Qwen 2.5 7B before LADDER 50% (reported).",
            "learning_speed_comparison": "RL without variants collapsed quickly (to 0% within ~30 steps) whereas LADDER converged to high accuracy over ~250 steps, indicating a stable curriculum speeds and stabilizes learning.",
            "generalization_performance": "RL without variants failed to generalize beyond the small training set; pass@k sampling provided negligible gains (1→2%).",
            "task_diversity_analysis": "Not applicable for pass@k; RL without variants had no generated-task diversity and therefore poor performance.",
            "prerequisite_identification": "None — baselines do not construct prerequisites or intermediate tasks.",
            "intermediate_task_generation": "No — RL without variants had no intermediate tasks; pass@k is an inference-time sampling strategy, not a curriculum.",
            "llm_limitations_observed": "Pass@k sampling alone cannot compensate for lack of learned techniques; naive RL without an appropriate difficulty gradient leads to collapse.",
            "computational_cost": "pass@10 increases inference-sample cost relative to pass@1 but provided minimal performance benefit; RL without variants used the same RL hyperparameters as LADDER but produced no beneficial outcomes, representing wasted training compute under identical settings.",
            "human_expert_evaluation": "No human evaluation reported for baselines beyond reporting standard model scores.",
            "key_findings_summary": "Simple inference strategies (pass@k) or RL without a curriculum are insufficient: pass@10 only increased accuracy from 1% to 2%, and RL without variants failed (collapsed), demonstrating that a structured, difficulty-graded curriculum is critical for stable RL-based learning on compositional integration tasks.",
            "uuid": "e2034.3"
        },
        {
            "name_short": "GRPO (used RL algorithm)",
            "name_full": "Group Relative Policy Optimization",
            "brief_description": "An RL optimization technique used in the paper that estimates baselines from grouped sample rewards instead of a separate critic, used to train the LLM policy on variant trees.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "reinforcement learning-based optimization (group-based advantage estimation)",
            "curriculum_method_description": "GRPO samples a group of outputs per question and computes an advantage for each sample using group mean and std of rewards; objective maximizes expected group-normalized advantage. Used with simple rule-based rewards (accuracy and format) and a KL-coefficient of 0.001. GRPO was applied both in LADDER (training) and in TTRL (test-time per-question RL).",
            "llm_model_used": null,
            "domain_environment": "Applied to LLM policy optimization in mathematical integration tasks.",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Optimizes policy to produce correct symbolic integration solutions; rewards are sparse (binary correctness) and formatting reward helps reliable verification.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Not compared to other RL algorithms in the paper (only GRPO was used).",
            "performance_metrics": "When used with LADDER curriculum, GRPO achieved large performance gains: Llama 3.2 3B 82% test accuracy and Qwen 2.5 7B LADDER 73% on MIT exam; with TTRL (GRPO applied at test-time) final MIT score rose to 90%. No ablation comparing alternative RL algorithms was reported.",
            "learning_speed_comparison": "GRPO runs with identical hyperparameters produced rapid learning under LADDER (convergence to plateau ~250 steps) while GRPO without variant curricula collapsed, indicating sensitivity of GRPO training to curriculum design.",
            "generalization_performance": "Used GRPO trained on variant curricula generalized to held-out test sets (see LADDER results); no cross-algorithm generalization comparisons conducted.",
            "task_diversity_analysis": "Not directly applicable to GRPO itself; success depended on diversity of variant curricula fed into GRPO.",
            "prerequisite_identification": "GRPO does not perform prerequisite discovery; it optimizes policy given the curriculum provided by variant generation.",
            "intermediate_task_generation": "GRPO consumes intermediate tasks (variants) for policy updates but does not itself generate them.",
            "llm_limitations_observed": "No algorithmic limitations of GRPO were reported beyond sensitivity to the absence of a smooth curriculum (training collapse when trained only on the small root set).",
            "computational_cost": "GRPO chosen for efficiency (no separate critic), but paper provides limited numerical compute cost details; used with KL coefficient 0.001 and varying batch sizes (e.g., 128 for MIT experiments).",
            "human_expert_evaluation": "No human evaluation of GRPO outputs beyond numerical verification of solutions.",
            "key_findings_summary": "GRPO served as an effective RL learner when supplied with a well-constructed LLM-generated curriculum, but GRPO training collapses if the curriculum lacks a smooth difficulty gradient (e.g., training only on hard root tasks).",
            "uuid": "e2034.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can teach themselves to program better.",
            "rating": 2
        },
        {
            "paper_title": "Self-Improvement and Automated Curriculum Generation in LLMs.",
            "rating": 1
        },
        {
            "paper_title": "Self-Taught Reasoner (STaR)",
            "rating": 1
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "rating": 2
        }
    ],
    "cost": 0.0160315,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LADDER: SELF-IMPROVING LLMS THROUGH RECURSIVE PROBLEM DECOMPOSITION
March 6, 2025</p>
<p>Toby Simonds 
Tufa Labs</p>
<p>Akira Yoshiyama 
Tufa Labs</p>
<p>LADDER: SELF-IMPROVING LLMS THROUGH RECURSIVE PROBLEM DECOMPOSITION
March 6, 2025D769F5E67930E3B1F8FBC2B255AA2641arXiv:2503.00735v3[cs.LG]
We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems.Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants.We demonstrate LADDER's effectiveness on the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination.We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time.TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance.These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) has emerged as a highly effective approach for training Large Language Models (LLMs), yet its success hinges critically on the availability of appropriate training tasks [5,9,10,12].A fundamental challenge lies in obtaining verifiable tasks that match the model's current capabilities.For RL to be effective, tasks must form a gradient of difficulties that allows for incremental learning progress [8].When tasks exceed the model's current abilities, the training process not only stalls but can lead to catastrophic collapse, resulting in degraded performance.This challenge is particularly acute in domains requiring complex reasoning, where the gap between simple and advanced tasks can be substantial [4,12].</p>
<p>We propose Learning through Autonomous Difficulty-Driven Example Recursion (LADDER), a framework that enables LLMs to autonomously improve their problem-solving capabilities through strategic self-guided learning.The key insight is that models can bootstrap their own learning by recursively generating and solving progressively simpler variants of complex problems.For each challenging problem, LADDER prompts the model to create multiple easier variants, forming a natural difficulty gradient.This process continues recursively, with each variant spawning simpler sub-variants, until reaching problems the model can reliably solve.The solutions to these simpler problems then provide stepping stones for tackling progressively harder variants.We demonstrate that this self-bootstrapping approach achieves dramatic improvements beyond what's possible through standard techniques like pass@k sampling -enabling models to reliably solve problems that were previously far beyond their capabilities.</p>
<p>Unlike previous approaches requiring carefully curated datasets or human feedback, LADDER leverages the model's existing capabilities to create a natural difficulty gradient, allowing for systematic improvement through reinforcement learning with verifiable rewards.The framework requires only a reliable verification mechanism -in our case, numerical integration for checking solutions.This enables the model to assess its own progress and guide its learning trajectory without human intervention.</p>
<p>We demonstrate LADDER's effectiveness on mathematical integration tasks, achieving remarkable improvements across multiple benchmarks.Using this approach, we improve a Llama 3B model's accuracy from 1% to 82% on undergraduatelevel integration problems.When applied to the challenging 2025 MIT Integration Bee examination, LADDER enables a 7B parameter model to achieve 73% accuracy, significantly outperforming much larger models, such as GPT-4o (42%), and typical human performance (15-30%).These results showcase how strategic problem decomposition and verified self-learning can achieve substantial capability improvements without relying on architectural scaling or human supervision.</p>
<p>Building on LADDER's self-improvement framework, we propose Test-Time Reinforcement Learning (TTRL), a novel approach that extends these principles to inference time.TTRL dynamically generates problem variants during test-time and applies reinforcement learning to refine the model's solutions, effectively creating a micro-learning process for each test instance.By leveraging the same verification mechanisms used in training, TTRL enables the model to further improve its performance.When applied to the 2025 MIT Integration Bee, TTRL boosts accuracy from 73% -with just LADDER -to 90%, demonstrating how scaling test-time compute through strategic problem decomposition can yield substantial performance improvements.We achieve state of the art accuracy, outperforming significantly larger models, such as OpenAI's o1.Thus, we make the following contributions:</p>
<p>• We propose a novel framework for autonomous model improvement through recursive problem decomposition and self-guided learning via reinforcement learning with GRPO.</p>
<p>• We develop a systematic method for generating and verifying problem variants that create natural difficulty gradients, requiring only numerical verification.</p>
<p>• We demonstrate significant empirical improvements on mathematical reasoning tasks, improving a Llama 3B model from 2% to 82% on undergraduate integration problems and achieving 73% accuracy on the MIT Integration Bee with a 7B model, matching SoTA performance</p>
<p>• We introduce Test-Time Reinforcement Learning (TTRL), a method for scaling compute at inference time through variant generation and reinforcement learning, boosting performance on the MIT Integration Bee from 73% to 90%.</p>
<p>Related Work</p>
<p>Self-Improvement and Automated Curriculum Generation in LLMs.Haluptzok et al. (2023) present a self-play setup where a code-focused LLM continually invents and solves new programming puzzles, checks solutions with a Python interpreter, and fine-tunes on correct results [3].We build upon the generate → solve → verify → learn cycle, adding a more explicit curricular element to guide the model's step-by-step improvement.</p>
<p>Recent advances such as STaR (Self-Taught Reasoner) have illustrated that LLMs can improve their reasoning skills by learning from their own generated chain-of-thoughts-effectively acting as both student and teacher [15].Step" [6] demonstrated that breaking down verification into explicit steps and allocating additional computation to each verification component can significantly improve performance.Similarly, Tree of Thoughts [14] showed how systematically exploring multiple reasoning paths during inference can lead to better problem-solving capabilities.These approaches typically focus on increasing the length and deliberation of model outputs, either through structured prompting, such as s1, or systematic exploration of solution spaces, such as self-consistency [7,12].Our work differs fundamentally from these approaches by introducing dynamic learning at test-time -rather than just increasing reasoning length or exploring multiple paths, TTRL enables the model to actually improve its capabilities through practice on related problems.This represents a shift from static inference-time techniques to dynamic adaptation through targeted learning.</p>
<p>Reinforcement Learning Approaches for LLM Self-Improvement.In recent years, reinforcement learning has emerged as a pivotal strategy for enabling language models to self-improve through iterative self-correction and feedback.Notably, OpenAI's o1 model uses reinforcement learning to refine its chain-of-thought reasoning-learning to "think" step by step [9].Similarly, DeepSeek's R1 model leverages a reinforcement learning framework that minimizes human supervision by generating and self-evaluating its own training data [2].</p>
<p>Importantly, recent work has shown that RL approaches may generalize much better than Supervised Fine-Tuning (SFT), which appears to memorize instead [1,13].RL approaches have also shown to be highly effective in generalizing to out-of-distribution tasks, particularly in competitive mathematics [13].These results strongly suggest that reinforcement learning offers a more promising path to self-improvement than supervised fine-tuning.</p>
<p>These innovations underscore a paradigm shift-from externally curated curricula and human-labeled data toward autonomous, feedback-driven self-improvement in language models.LADDER is a structured framework designed to enhance LLM problem-solving capabilities through recursive problem decomposition and reinforcement learning.We demonstrate the effectiveness of LADDER in the domain of mathematical integration.LADDER consists of the following components:</p>
<p>• Variant Generation: A structured method for generating trees of progressively simpler variants of complex problems, establishing a natural difficulty gradient.</p>
<p>• Solution Verification: A numerical integration method to verify the solution of an integral.</p>
<p>• Reinforcement Learning: The protocol used to train the base model on the variant trees.</p>
<p>LADDER is a two-step algorithm.First, the training set of integrals is collected and a tree of variants is generated for each integral as described in section 3.1.2.Second, we perform the reinforcement learning protocol described in section 3.1.4on a base model using the set of variant trees as the training set.The resulting model should then have significantly improved and generalized mathematical integration capabilities, and can then be prompted to solve new integrals.See Algorithm 1 for LADDER pseudocode.V LADDER is the set of variants generated for all questions in the train set of integrals and a i is the answer to the ith test question.</p>
<p>The number of variants N generated per question is a hyperparameter, which we vary for different experiments.We experiment with LADDER in sections 3.2.1 and 3.2.2.</p>
<p>Variant Generation</p>
<p>The variant generation process required careful design to ensure sufficient diversity and appropriate difficulty scaling.We found that naive approaches to variant generation -simply asking the model to generate easier versions -led to repetitive patterns and limited diversity.Through experimentation, we developed a multi-stage process that dramatically improved the quality and variety of generated variants.</p>
<p>Algorithm 1 LADDER 1: Q train , Q test ← train and test sets of integrals 2: N ← number of variants to generate 3: G(q, j) : generates the jth variant of integral q
4: V LADDER ← {v i,j ∈ G(Q train [i], j) | 1 ≤ i ≤ |Q train |, 1 ≤ j ≤ N } 5: π θLADDER ← GRPO(π θbase , V LADDER ) 6: for i ← 1, |Q test | do 7: a i ∼ π θLADDER (• | Q test [i]) 8: end for Algorithm 2 TTRL 1: Q test ← test set of integrals 2:
N ← number of variants to generate 3: G(q, j) : generates the jth variant of integral q 4: for i ← 1, |Q test | do 5:
V TTRL,i ← {v j ∈ G(Q test [i], j) | 1 ≤ j ≤ N } 6:
π θTTRL,i ← GRPO(π θbase , V TTRL,i )</p>
<p>7:
a i ∼ π θTTRL,i (• | Q test [i]) 8: end for
First, we used the base model to generate an extensive set of mathematical transformations that could be applied to integrals, categorized by their impact on problem difficulty.These included operations ranging from simple (e.g., reducing exponent values, simplifying denominators) to more complex (e.g., introducing nested functions, combining multiple function types).This transformation library served as a foundation for guiding variant generation.</p>
<p>Second, for each integral, we randomly sampled 3-5 transformations and provided them as explicit suggestions to the variant generation model.Critically, we found that generating variants in batches of 10 per prompt significantly improved diversity -smaller batch sizes led to more repetitive outputs, while larger batches reduced quality.To further enhance variation, we employed two key techniques:</p>
<ol>
<li>Temperature cycling: We dynamically varied the sampling temperature between 0.8 and 1.4 across prompts.</li>
</ol>
<p>This helped balance between creativity and mathematical validity.</p>
<p>Persona-based prompting:</p>
<p>We prompted the model to adopt different mathematical perspectives (e.g., "think like Euler focusing on series", "approach like Gauss looking for patterns").</p>
<p>The combination of batch generation, varied temperatures, and rotating personas proved crucial led to significantly more diverse variants.Without them the model would often converge to generating very similar and often repeating variants.</p>
<p>Third, to ensure sufficient coverage of the difficulty space, we applied this generation process recursively, generating a tree of variants, each a simpler integral than its parent.We found that this also helped improve variant diversity.This recursive approach helped build natural difficulty gradients -each problem could spawn multiple easier variants, which in turn could generate even simpler problems.We capped tree depth at three to maintain problem relevance.The following is an example of one root-to-leaf path in a variant tree:</p>
<p>Original problem:
x 2 + 1 x 4 + 2x 2 + 1 dx Level 1 variant: x 2 x 4 + 1 dx Level 2 variant: 1 x 2 + 1 dx Level 3 variant: 1 x 2 dx
Quality control presented a significant challenge in our variant generation process.While we prompted the model to generate "easier" or "equivalent" variants, the actual difficulty of the resulting integrals often deviated substantially from the intended level.Small perturbations in coefficients or function composition could transform seemingly simple integrals into much harder ones -for instance, changing a coefficient from 1 to 2 in a rational function could introduce complex roots that make the integral significantly more challenging.</p>
<p>For our Llama experiments, approximately 8% of generated variants were effectively unsolvable (i.e. it took a symbolic system more than 5 seconds to solve), and many more were substantially harder than their parent problems despite being intended as simpler variants.While these harder variants were naturally ignored during training through zero reward signals, they consumed significant computational resources in verification attempts and represented wasted generation capacity.Future work could explore methods to better constrain variant generation to maintain intended difficulty levels while preserving mathematical diversity.</p>
<p>We apply the above three-step variant generation method to generate the sets of variant integrals in both LADDER and TTRL.V LADDER is the set of variants for all questions in the train set Q train and V TTRL,i is the set of variants for the ith question in the test set, Q test [i].See Algorithms 1 and 2.</p>
<p>Solution Verification</p>
<p>Integral variant generation occurs at the beginning of LADDER.Throughout the process of performing RL in LADDER we must also perform solution verification.In order to do so, we developed a robust numerical verification framework that balanced accuracy with computational efficiency.The key challenge was ensuring reliable verification across different types of integrals while handling edge cases and potential numerical instabilities.</p>
<p>For each candidate solution to an integral, we employed a multi-point numerical comparison approach.We randomly sampled five points from the domain [-10, 10] and evaluated both the candidate solution and the original integral over small intervals of length 0.1 centered at these points.This interval length was chosen empirically to minimize numerical integration errors while maintaining sensitivity to local solution behavior.The use of multiple small intervals, rather than a single large one, helped detect both local and global errors in the solutions.</p>
<p>Our verification protocol included several key components:</p>
<p>• Singularity handling: When a sampled point was near a singularity (detected through rapid value changes or numerical overflow), we automatically resampled to a new point.This adaptive sampling ensured robust verification even for functions with challenging behavior.• Numerical precision: Solutions were deemed correct if their values exhibited a relative difference of 10 −2 or less compared to numerical integration results across all test intervals.We found empirically that this tolerance level effectively balanced false positives and negatives.• Timeout management: To handle computationally intensive integrals, we implemented a 2-second timeout for each verification attempt.If timeout occurred, new evaluation points were sampled, with up to three retries before marking the solution as unverifiable.• Edge case detection: Early iterations revealed that models could exploit verification weaknesses by outputting trivial solutions (like the integration symbol itself) or degenerate forms.We added specific filters to detect and reject such cases, ensuring solutions demonstrated genuine understanding.</p>
<p>All numerical integrations were performed using adaptive quadrature methods, with automatic precision adjustment based on the integrand's complexity.This approach provided reliable results even for oscillatory and highly nonlinear functions, while maintaining reasonable computational efficiency.</p>
<p>We note, however, that the numerical verifier may not be 100% accurate.We opted for a numerical approach rather than a symbolic one because many problems beyond integration can be numerically verified despite lacking a symbolic solver.</p>
<p>We apply the above solution verification in our RL Protocol in section 3.1.4and experiments in sections 3.2.1,3.2.2 and 3.2.3.In our MIT experiments, final benchmark results are verified directly against the official solutions.</p>
<p>Reinforcement Learning Protocol</p>
<p>We decided to employ Group Relative Policy Optimization (GRPO) in both LADDER and TTRL [11].GRPO does not use a separate critic model and instead estimates the baseline from group scores, improving efficiency and reducing memory.For each question q, GRPO samples a group of outputs o 1 , o 2 , ..., o G from the old policy π θ old and then optimizes the policy model π θ by maximizing the following objective:
J GRPO (θ) = E q ∼ P (Q), {o i } G i=1 ∼ π θ old (O | q) 1 G
where ϵ and β are hyperparameters, and A i is the advantage, computed using a group of rewards r 1 , r 2 , ..., r G corresponding to the outputs within each group:
A i = r i − mean {r 1 , r 2 , . . . , r G } std {r 1 , r 2 , . . . , r G }(3)
We adopted a simple, rule-based reward model.The reward model is kept simple in order to be straightforward to apply to other verifiable domains in the future.The reward model consists of two rewards:</p>
<p>• Accuracy reward: Using the solution verification method described in 3.1.3,we evaluate whether the response is correct or not.The model is required to provide the final answer in a specified format (i.e. in <ANSWER> </ANSWER> tags), enabling reliable rule-based verification of correctness.</p>
<p>• Format reward: In addition to the accuracy reward model, we employ a format reward that encourages the model to put its answer in <ANSWER> </ANSWER> tags.</p>
<p>We perform the RL runs with a KL divergence coefficient of 0.001.Training batch size and number of epochs differ among our experiments.</p>
<p>Test-Time Reinforcement Learning</p>
<p>Test-Time Reinforcement Learning (TTRL) extends our LADDER framework to inference time, enabling dynamic adaptation to challenging problems directly during testing.Upon encountering a difficult integration problem, TTRL generates a focused set of related variants and conducts targeted reinforcement learning specifically for that problem.By recursively decomposing the challenging problem into simpler variants and learning from this focused curriculum, TTRL allows the model to rapidly develop problem-specific expertise without hand-crafting variants or architectural modifications.This provides a novel approach to scaling compute at test-time -rather than simply increasing output sampling or model size, TTRL leverages compute to dynamically improve the model's problem-solving capabilities through focused practice on variants of the specific challenge at hand.</p>
<p>The same three components of LADDER are also used in Test-Time Reinforcement Learning (TTRL).For each question at test-time, there are two steps.First, we generate a tree of variants for the test question at hand.Second, we perform the reinforcement learning protocol described in section 3.1.4on a base model using the variant tree as the training set.The resulting model should then have significantly improved mathematical integration capabilities tuned to the test question at hand.The test question is then answered using the tuned model, and finally the model is rolled back to its original parameters for the next test question.See Algorithm 2 for TTRL pseudocode, where V TTRL,i is the set of variants generated for the ith test question and a i is the answer to the ith test question.</p>
<p>As in LADDER, the number of variants generated per question is a hyperparameter, which we vary for different experiments.We experiment with TTRL in section 3.2.3.</p>
<p>Experiments</p>
<p>Llama 3B Experiments</p>
<p>We first experiment with LADDER using a Llama 3.2 3B parameter model as our base architecture to allow us to more quickly run ablation experiments.To establish our evaluation dataset, we developed a comprehensive collection of 110 indefinite integration problems, combining questions sourced from university-level mathematics curricula with synthetically generated problems using GPT-4o.To ensure appropriate difficulty calibration, we benchmarked each problem to verify it was solvable by GPT-4o but beyond the capabilities of the base Llama 3.2 3B model.This was done purposefully to benchmark performance on problems that can still be solved by an LLM but are outside the scope of Llama 3.2 3B.</p>
<p>We randomly split this collection into a training set of 10 problems and a test set of 100 problems.we generated approximately 500 variants for each of our 10 training problems.To validate the diversity of our variant set, we performed exact matching against our test set and found minimal overlap (only 5 matches among 5,000 variants).The small training set was deliberately chosen to demonstrate our method's effectiveness in generating useful variants from limited examples.Approximately half of the problems were sourced from curriculum textbooks, with the remainder synthetically generated by GPT-4o.Notably, the synthetic problems were indistinguishable from curriculum-sourced ones.During training, models were prompted to express solutions in sympy algebraic notation without integration constants, ensuring consistent evaluation across different but mathematically equivalent expressions of the same solution.This standardization was crucial for reliable verification of solution correctness during the training process.</p>
<p>We performed two RL experiments with Llama 3.2 3B.The first experiment trained on only the 10 question training set (RL w/o variants).The second experiment trained on only variants generated from the 10 question training set (RL w/ variants, i.e.LADDER).Both runs used identical hyper parameters and were continued until performance plateaued.</p>
<p>MIT Integration Bee (LADDER)</p>
<p>Building off the findings from our Llama 3B experiemtns, we extended our methodology to the 2025 MIT Integration Bee qualifying examination, an annual competition that attracts both undergraduate and graduate students from MIT, with participants often having competitive mathematics backgrounds.The qualifying exam, which serves to select 16 finalists, contains 20 questions and has a typical qualifying threshold of 73%, though most participants score between 15-30%, with 50% to 73% out of 20 considered strong performance.Students are given 20 minutes to attempt the entire exam.</p>
<p>Using the DeepSeek-R1 distilled Qwen 2.5 7B model, we applied our variant generation approach to historical qualifying exams from 2010-2024.Our variant generation followed a two-level tree structure -first generating variants from each source problem, then generating additional variants from those first-level variants.We capped recursion at depth two, as preliminary experiments showed third-level variants became trivially simple and lost mathematical relevance to the original problems.With this approach we generated 9,000 variants.</p>
<p>Through experimentation with different difficulty distributions, we settled on prompting the model to generate 70% easier and 30% equivalent variants than their parent problems.While we initially attempted to include more challenging variants, we found this skew toward easier problems created more effective learning trajectories.Notably, many "equivalent" variants still introduced novel mathematical patterns while maintaining similar difficulty levels, contributing to the model's generalization capabilities.</p>
<p>We verified that none of the 2025 exam questions appeared in our variant set, though we note this precaution was primarily for methodological rigor rather than necessity, as our model never accessed the test set or solutions during training.We apply the same hyperparameters as in the 3B experiments, except for modifying the batch size to 128.</p>
<p>MIT Integration Bee (LADDER + TTRL)</p>
<p>After applying LADDER, there remain certain questions which the tuned model continues to fail to answer.For each of these questions we further apply TTRL.For each unsolved problem, we generate a tree of variants following the same process illustrated in Figure 1, but limited to two levels of depth and approximately 800 total variants.Using these problem-specific variants, we conduct 100 steps of reinforcement learning with identical RL parameters as in the MIT Integration Bee LADDER setup.The problem is considered "solved" if the model produces a solution at any point during this process, as verified by our numerical integration framework.</p>
<p>TTRL's approach of on-the-fly data synthesis means it can handle novel integrals better by expanding its training setfocused on that problem -at test time.This is especially important for challenges such as the MIT Integration Bee, where many integrals are cleverly constructed and might be out-of-distribution relative to standard calculus problems.</p>
<p>Further, TTRL provides a systematic approach to scaling performance through additional compute.Rather than simply increasing sampling or temperature parameters, TTRL enables active improvement of problem-solving capabilities through focused practice at test-time.</p>
<p>Results</p>
<p>B</p>
<p>Llama 3B Experiments</p>
<p>Our experiments evaluated whether generating easier variants could enable effective reinforcement learning in mathematical integration tasks on Llama 3.2 3B.We tested four conditions: base model with pass@1 samping, base model with pass@10 sampling, RL without variants (only on the training set), and LADDER (RL with variants from the training set).</p>
<p>In Figure 2, we observe a consistent and rapid improvement from step 0 to 250, with the model's performance quickly ascending before ultimately plateauing.This swift progression suggests an effective learning mechanism, characterized by substantial performance gains that stabilize around 82% accuracy as the model approaches its performance ceiling.</p>
<p>LADDER achieved 82% accuracy on the test set, significantly outperforming both the base model with pass@1 sampling (1%) and with pass@10 sampling (2%).RL without variants consistently failed, with performance never exceeding 3% before collapsing to 0% within 30 training steps, highlighting the necessity of a smooth difficulty gradient for successful training (see Figure 4).</p>
<p>Analysis of the remaining unsolved problems reveals nuanced challenges in the model's mathematical integration capabilities.The model demonstrates difficulty with integrals requiring multiple technique combinations and novel substitution patterns.These complex scenarios highlight limitations in the current approach, suggesting that while the variant generation successfully teaches individual techniques, synthesizing these methods for more intricate problems remains a challenge.</p>
<p>We observed a clear upward trend in performance as the number of generated variants increased, with no definitive plateau in the graph (see Figure 3).While the rate of improvement appears to slow beyond 6,000 variants, the data suggests that continued scaling may still yield further gains.This indicates that generating more variants could enhance performance, though with diminishing returns relative to earlier stages of training.Rather than identifying a strict optimal corpus size, our findings highlight a trade-off between computational cost and incremental improvements, suggesting that additional scaling may remain beneficial depending on resource availability.</p>
<p>The stark improvement from base model pass@10 (2%) to LADDER (82%) demonstrates genuine acquisition of mathematical abilities rather than merely improved output sampling.The consistent failure of RL without variants under identical hyperparameters confirms that success stems from the carefully constructed difficulty gradient rather than the RL algorithm itself, suggesting potential applications to other complex reasoning tasks where direct training proves ineffective.</p>
<p>MIT integration Bee (LADDER)</p>
<p>Applying our methodology to the MIT Integration Bee led to a significant improvement in model performance.LADDER improved the accuracy of the Deepseek-R1 Qwen 2.5 7B base model from a baseline of 50% to 73% on the 2025 qualifying examination, significantly outperforming other existing models, including GPT-4o (42%).See Figure 5.</p>
<p>For comparison, most human participants score between 3-6 points out of 20 (15-30%), with scores above 7-12 (35-73%) considered strong performances.With a 73% accuracy, our model meets the qualification threshold of 14/20.However, while LADDER achieved substantial gains, it still did not reach the performance of o1 (80%).</p>
<p>The consistent performance gap between our trained model and base models highlights the effectiveness of our variantbased recursive training approach in improving mathematical reasoning.Despite the relatively modest parameter count of 7B, our base model was able to significantly outperform larger models that did not undergo targeted recursive training.This suggests that structured self-improvement methodologies can lead to substantial gains in problem-solving ability without requiring massive increases in model size.</p>
<p>MIT Integration Bee (LADDER + TTRL)</p>
<p>After applying LADDER to the 7B base model, we further employed Test-Time Reinforcement Learning (TTRL) for 100 steps on questions that LADDER failed to answer correctly.Of the remaining incorrect responses, TTRL successfully solved 3-4 additional problems, increasing performance from 73% to 90% (Figure 6).This improvement places our model's performance well above the typical qualification threshold of 73% for the MIT Integration Bee, setting a new state-of-the-art for mid-sized LLMs on this benchmark.In Figure 6, we compare LADDER pass@100 with TTRL, demonstrating that TTRL is a more effective test-time scaling method than naively attempting a test question many times.TTRL's application of recursive problem decomposition on the test questions themselves allows the initial LADDER model to correctly solve some of the hardest questions on the MIT Integration Bee qualifying examination.TTRL enables our relatively small 7B LADDER model to outperform OpenAI's o1 model at pass@1.</p>
<p>The number of RL steps required to solve each test question previously incorrectly answered varied, ranging from as few as 3 steps to as many as 30.This variation likely reflects the complexity of each integral, with more complex problems requiring a greater number of solved variants to develop a useful learning trajectory.The questions that remained unsolved even after LADDER + TTRL are particularly complex for undergraduate students and are included in the appendix.</p>
<p>To assess the necessity of LADDER in TTRL's effectiveness, we also applied TTRL directly to the base model (pre-LADDER).After 100 steps, the base model remained unable to correctly answer any of the questions that LADDER + TTRL successfully solved.Performance gains from TTRL only occurred when applied to the LADDER-trained model, suggesting that the RL protocol in LADDER provides essential exposure to the distribution of integrals, enabling TTRL to later learn further.Future research could explore whether TTRL alone, with sufficiently high compute constraints, could achieve comparable results.</p>
<p>We note that while our LADDER+TTRL model surpasses o1's score on the MIT Integration Bee, this comparison should be taken as a general baseline rather than a strict head-to-head evaluation.Unlike our approach, o1 does not have access to a numerical checker, meaning it operates under different constraints.Our results highlight the effectiveness of self-improvement via recursive problem decomposition and reinforcement learning rather than suggesting a direct superiority over o1's approach.</p>
<p>Discussion</p>
<p>Our results demonstrate a potentially transformative approach to AI self-improvement through recursive problem decomposition and solution verification.The key insight is that models can direct their own learning trajectory by generating and solving progressively more challenging problems, requiring only a formal verifier rather than human guidance or curated datasets.This represents a significant shift from traditional supervised learning or RLHF approaches, suggesting a path toward more autonomous AI systems capable of extending their own capabilities.</p>
<p>The effectiveness of this self-directed learning is evident in our empirical results: improvements from 1% to 82% on undergraduate integration problems and from 50% to 73% on the MIT Integration Bee, achieved with LADDER, without human feedback or model scaling.With additional variant-based learning at test-time (TTRL), we achieve a state-of-the-art score of 90%.The 7B parameter model's ability to outperform much larger architectures suggests that improvements in AI capabilities may come not just from scaling compute or parameters, but from enabling models to structure their own learning progression.This methodology's recursive capability makes it particularly powerful.When faced with problems beyond their current abilities, models can autonomously decompose them into simpler variants, creating their own curriculum that adapts to their evolving capabilities.The implications extend beyond mathematical reasoning -suggesting a general framework for AI systems that can bootstrap more sophisticated capabilities from simpler ones, guided only by formal verification mechanisms rather than human oversight.</p>
<p>A Form of Test-Time Compute Scaling</p>
<p>Test-time compute scaling represents a promising new frontier in enhancing model capabilities beyond traditional approaches like architectural scaling or increased parameter counts.LADDER and TTRL introduce a fundamentally different paradigm: rather than relying on larger models or more extensive pre-training, we can achieve substantial performance improvements by enabling models to strategically practice and learn during inference.This approach is particularly powerful because it allows models to develop problem-specific expertise dynamically, adapting their capabilities to the exact challenges they encounter.</p>
<p>The dominant approach to test-time compute scaling today concentrates on increasing output token length, allowing models to engage in more extensive step-by-step reasoning.However, this approach faces fundamental challenges with extremely difficult problems that require extensive exploration.The sequential nature of token generation creates memory constraints and potential bottlenecks, as each token must be generated and processed in sequence while maintaining the entire chain of reasoning in context.</p>
<p>TTRL offers a fundamentally different approach that mirrors human learning strategies.Just as humans often need to study and practice similar problems before tackling a particularly challenging question, TTRL enables models to systematically explore and learn from related problems before attempting the target task.This approach is inherently more parallelizable than sequential token generation -variant problems can be generated and solved independently across multiple compute units, with their insights aggregated to improve performance on the original problem.While our current implementation explored variants sequentially, the framework naturally extends to distributed training approaches, potentially offering significant speedups through parallel exploration of the problem space.</p>
<p>This parallelizability presents a key advantage over traditional test-time scaling approaches.Where increasing output length faces fundamental sequential constraints, TTRL's variant generation and learning process can be distributed across multiple GPUs or machines.Each compute unit can independently explore different regions of the problem space, generating and solving variants while sharing insights through model updates.This mirrors distributed training approaches in the pre-training phase, but applied dynamically at test-time to specific challenging problems.</p>
<p>Extension to Other Verifiable Domains</p>
<p>While we demonstrated our approach using numerical integration, the underlying principle can extend to a broad range of formal reasoning tasks through appropriate verification tools.It is likely that any domain where question variants can be generated and which has a verifier-generator gap can leverage our approach.These domains share the critical property we identified in integration: a clear generator-verifier gap where solution verification is more straightforward than solution generation.This suggests our approach of iterative variant generation and verified learning could provide a general framework for improving formal reasoning capabilities in language models.Competitive programming is an example of another possible domain in which our method may be effective.An LLM could generate a solution to a programming problem and verify it with provided unit tests.In formal mathematics, tools like the Lean proof assistant offer another avenue for verification.Lean can rigorously verify mathematical proofs, providing a reliable signal for whether a model's reasoning is correct.</p>
<p>More generally, this approach has potential applications in planning and agent-based tasks, where models can teach themselves by successfully executing simpler tasks before building up to more complex challenges.This progressive learning approach, combined with reliable verification at each step, provides a robust framework developing advanced reasoning capabilities.</p>
<p>Future Work</p>
<p>A core challenge in optimizing LADDER lies in the precise calibration of variant difficulty and curriculum sequencing.While our current approach uses static parameters for variant generation, an adaptive strategy could dynamically adjust the difficulty gradient based on model performance.Rather than generating a fixed number of variants at predetermined difficulty levels, the framework could analyze solution success rates to determine optimal branching points and difficulty steps.This dynamic calibration would ensure each generated variant meaningfully contributes to the learning trajectory while avoiding redundant or inappropriately difficult examples that waste computational resources.The balance between exploration of novel problem patterns and exploitation of known solution strategies remains to be optimized.</p>
<p>Conclusion</p>
<p>We proposed LADDER, a framework for improving language models' problem-solving capabilities through recursive problem decomposition and reinforcement learning with verifiable rewards, as well as its extension to test-time, TTRL.By enabling models to generate and learn from progressively simpler variants of complex problems, LADDER demonstrates how strategic self-directed learning can achieve significant improvements without relying on architectural scaling or human supervision.The dramatic improvements in mathematical reasoning capabilities -from 1% to 82% on undergraduate integration problems and achieving 90% accuracy on the MIT Integration Bee through TTRLdemonstrate the effectiveness of this approach.</p>
<p>Beyond mathematical integration, our work builds upon three insights in general AI development: (1) the power of recursive problem decomposition for tackling complex tasks, (2) the importance of verifiable rewards for guiding self-improvement, and (3) the potential of scaling compute through strategic practice at test-time.The success of both LADDER during training and TTRL at inference time reinforces that focusing on how models interact with their environment is just as important as architectural innovations for advancing AI capabilities.</p>
<p>The principles demonstrated in this paper could likely extend to any domain with clear verification mechanisms, from program synthesis to formal theorem proving.By providing a framework for models to bootstrap their own capabilities through carefully constructed learning trajectories, we move closer to AI systems that can systematically extend their abilities into increasingly complex domains.</p>
<p>Figure 1 :
1
Figure 1: Example of variant generation for an integration problem.Each level represents progressively simpler variants of the original problem.The tree structure ensures each variant has exactly one parent, maintaining a clear progression of difficulty.</p>
<p>Figure 3 :
3
Figure 2: LADDER training progression.</p>
<p>Figure 4 :
4
Figure 4: Comparison of LLM scores by training approach.</p>
<p>C l a u d e -3 5 GFigure 5 :
55
Figure 5: Comparison of LLM scores on the 2025 MIT Integration Bee (7B).Average across 3 runs.</p>
<p>LADDER pass@ 1 LADDERFigure 6 :
16
Figure 6: TTRL score improvement on 2025 MIT Integration Bee.</p>
<p>Figure 7 :
7
Figure 7: Example of variant generation for twin prime problem.</p>
<p>Appendix: MIT Integration Bee Failed QuestionsAfter applying LADDER and TTRL, there were two questions on the 2025 MIT Integration Bee qualifying exam which continued to be answered incorrectly.The questions incorrectly answered are among the most complex questions on the exam, and for an undergraduate student are very difficult to solve.Question 12:
Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, Yi Ma, Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Language models can teach themselves to program better. Patrick Haluptzok, Matthew Bowers, Adam Tauman, Kalai , 2023</p>
<p>On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. Tomasz Korbak, Hady Elsahar, Germán Kruszewski, Marc Dymetman, 2022</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. 2025</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, 2025</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, 2020</p>
<p>Learning to reason with llms. Openai, 2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, 2022</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, 2025</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267627926</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07197v4.pdf" target="_blank">GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse fields, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By translating node representation into tokens, GraphTranslator empowers an LLM to make predictions based on language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results demonstrate the effectiveness of our proposed GraphTranslator on zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended tasks through language instructions. Our code is available at: https://github.com/alibaba/GraphTranslator.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Translator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTranslator - Translator module (token projection / soft prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based module that converts pre-trained graph model node embeddings into a fixed set of token embeddings (soft prompt) that a frozen LLM can consume; uses learnable query tokens with self- and cross-attention to extract language-relevant features from node embeddings and outputs 32 token embeddings which are linearly projected to the LLM word-embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Soft-prompt token projection (Translator)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node embedding h_v (from a frozen GM), the Translator uses a transformer encoder with K learnable query tokens that (1) interact via self-attention, (2) cross-attend to the node embedding to extract language-relevant features, producing a sequence of token embeddings (32 tokens of dim 768 in experiments). Those token embeddings are linearly projected into the LLM's word-embedding dimensionality and concatenated with instruction text as a soft prompt to the frozen LLM for generation/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs) — node-centric graphs where each node has textual attributes (e.g., user/product text, paper title+abstract)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Fixed-length tokenized representation; compressed summary of node+neighborhood+model bias; designed to bridge modality gap; more compact and lower-noise than raw attribute concatenation; interpretable as a soft prompt to LLMs; potential information loss due to compression into fixed token set; requires alignment data and two-stage training to map into LLM semantic space.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Zero-shot node classification (Taobao: Lifestage, Cat Owner, Vehicle Owner) and ArXiv Top-k classification; Graph Question Answering (GQA) multi-turn open-ended dialogue tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics reported: Legality Rate (%), Accuracy (%), Recall (% or macro-recall), Macro-F1 (%), Top-k classification accuracy (Top-1/Top-3/Top-5) and human rating counts for GQA (A/B/C/D). Representative reported results: GraphTranslator (full two-stage) — ArXiv Top-1: 28.28%, Top-3: 37.63%, Top-5: 39.88% (Table 2); in GQA, GraphTranslator obtained 210 'A' quality ratings vs Vanilla LLM 203 (counts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to raw-text prompting baselines (LLM+attribute, LLM+attribute+neighbors) and text-similarity baselines (BERT/RoBERTa), GraphTranslator produced higher legality rates and higher classification and recall in experiments; e.g., ArXiv Top-1: GraphTranslator 28.28% vs Vanilla LLM ≈17.90% (reported baseline). Ablation: Stage1-only Top-1 ≈8.22%, Stage2-only Top-1 ≈16.94%, showing two-stage training outperforms single-stage variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on high-quality alignment data generated by Producer; fixed-length token compression can lose fine-grained/topological details (node degree, full neighborhood structure); needs careful two-stage training and sufficient alignment pairs; quality depends on downstream frozen LLM vocabulary/embedding dimensionality and linear projection; experiments used ChatGLM2-6B — performance may change with larger/smaller LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Producer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Producer module (LLM-based graph-to-text constructor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that uses an LLM (with chain-of-thought style prompting) to generate a textual description for each node by concatenating: (1) node attribute summary, (2) sampled neighbor attribute summary, and (3) model-inferred commonality (what the graph model smooths/shared info), producing (node embedding, textual description) alignment pairs for Translator training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-generated concatenated node descriptions (node + neighbor + model commonality)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node v, the Producer prompts an LLM to: (a) summarize the node's textual attributes into a concise node description, (b) summarize attributes of sampled neighbors N(v) into a neighbor description, and (c) infer/summarize the commonality between the node and neighbors (model bias), then concatenates these three components into a single textual description t_v. These descriptions paired with frozen node embeddings form alignment data for Translator training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Aims to be succinct and low-noise by summarization; captures both local node features and sampled neighborhood semantics plus inferred model biases; human-readable text suitable for LLM consumption; quality depends on LLM used to generate descriptions; can omit explicit topological features unless explicitly prompted.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used to train Translator and evaluated indirectly via downstream tasks: zero-shot node classification and graph question answering (GQA). Also evaluated qualitatively for conversational multi-turn consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Indirect evaluation via downstream metrics (Legality Rate, Accuracy, Recall, Macro-F1, Top-k); qualitative GQA human ratings (counts of A/B/C/D). Reported outcomes: Producer-generated descriptions led to higher legality rates and better multi-turn GQA performance (GraphTranslator achieved 210 'A' ratings vs Vanilla LLM 203). Exact numeric contribution isolated to Producer alone not separately reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to feeding raw node attribute text or raw attribute+neighbor concatenation directly to LLM, Producer-generated summaries are lower-noise and produced better LLM responses when used with the Translator; the paper reports GraphTranslator (which uses Producer) outperforms LLM+attribute and LLM+attribute+neighbors baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Producer quality is pivotal — limited by LLM used (authors used ChatGLM2-6B and note larger LLMs like ChatGPT could improve descriptions); current Producer omits some explicit topology (e.g., node degree) and may not capture full graph structural details; generating high-quality descriptions at web-scale may be costly; risk of introducing LLM generation artifacts or biases into alignment data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+attr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla LLM prompt with node textual attributes (LLM+attribute)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that directly appends node textual attributes to an instruction and feeds the resulting raw text prompt into a frozen LLM (ChatGLM2-6B) to answer/classify without any embedding-to-token translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Raw attribute concatenation (node text -> LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>No explicit graph-to-text transformation beyond using the node's original textual attributes; the original node attribute text is concatenated with the task instruction and provided as prompt to the LLM for generation or class prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple and faithful to original text (low transformation), but often noisy and long (especially if including neighbor text), can overwhelm LLM attention and produce noisy or hallucinated outputs; does not compress or emphasize graph-induced semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Zero-shot node classification; Graph QA (directly feeding raw text into LLM for multi-turn dialog baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Same suite: Legality Rate, Accuracy, Recall, Macro-F1, Top-k. Representative reported result: ArXiv Top-1 accuracy (Vanilla LLM) ≈ 17.90% (paper table), which is lower than GraphTranslator's 28.28% Top-1. Authors report lower legality/accuracy in some Taobao tasks compared to GraphTranslator.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Performed worse than GraphTranslator, which used Producer summaries + Translator soft prompts. Adding neighbor text (LLM+attr+N()) improved some metrics but still underperformed GraphTranslator due to noisy and redundant information in raw concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High prompt length and noisy neighbor information can degrade LLM performance; lacks compression or encoding of graph-structural signals; prone to hallucination when reasoning about implicit graph-derived facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+attr+N()</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla LLM prompt with node and neighbor textual attributes (LLM+attribute+neighbors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that concatenates the node's textual attributes together with sampled neighbor attribute texts into the instruction fed to an LLM, attempting to provide local graph context purely as raw text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Raw attribute + neighbor concatenation (node+neighbors -> LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Concatenate the node text and the sampled neighbors' texts (N(v)) and append to the task instruction; no learned translation of embeddings; directly used as LLM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Provides more contextual information than node-only raw text but is more verbose and noisier; increases prompt length and potential redundancy; may contain contradictory or extraneous information.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Zero-shot node classification, Graph QA baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics: Legality Rate, Accuracy, Recall, Macro-F1. Authors report LLM+attr+N() often improved legality rates over LLM+attr alone but still underperformed GraphTranslator; e.g., for some Taobao tasks recall improved but was lower than GraphTranslator which better compresses implicit signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>LLM+attr+N() outperforms LLM+attr in some cases (more context) but is outperformed by GraphTranslator which uses Producer summaries and soft token prompts that reduce noise and compress graph semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No learned alignment between embedding and LLM input space; longer prompts can disturb LLM attention; sensitive to which neighbors are sampled and to raw-text noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5227.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5227.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT/RoBERTa similarity-based text classifier baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only baselines that compute similarity between node text descriptions and label text (using prompting) with pre-trained encoders (BERT, RoBERTa) or their fine-tuned variants to produce zero-shot predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Text similarity / classifier (BERT / RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode node textual descriptions (or generated descriptions) and class label descriptions using BERT/RoBERTa; compute similarity (or directly fine-tune) to determine the predicted class; no graph embedding translation involved.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Relies solely on textual attributes and pre-trained language encoder semantics; straightforward and interpretable but lacks graph structural signals; limited in handling complex zero-shot scenarios compared to LLM-guided approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Zero-shot node classification (Taobao tasks reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics in Table 1: Example — Taobao Lifestage accuracy: BERT 34.73%, RoBERTa 33.10% (authors report these baseline numbers); macro-F1 and recall values also provided in table (see paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Performed poorly relative to GraphTranslator and some LLM-based methods; authors note BERT-based methods struggle in complex zero-shot scenarios since they rely only on similarity computations and cannot handle instruction-following open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit incorporation of neighbor or model bias information unless included in the text; limited zero-shot generalization to unseen labels and tasks that require reasoning beyond surface text similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs <em>(Rating: 2)</em></li>
                <li>GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking <em>(Rating: 2)</em></li>
                <li>PRODIGY: Enabling In-context Learning Over Graphs <em>(Rating: 2)</em></li>
                <li>Explanations as Features: LLM-Based Features for Text-Attributed Graphs <em>(Rating: 2)</em></li>
                <li>Natural Language is All a Graph Needs <em>(Rating: 1)</em></li>
                <li>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5227",
    "paper_id": "paper-267627926",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Translator",
            "name_full": "GraphTranslator - Translator module (token projection / soft prompt)",
            "brief_description": "A transformer-based module that converts pre-trained graph model node embeddings into a fixed set of token embeddings (soft prompt) that a frozen LLM can consume; uses learnable query tokens with self- and cross-attention to extract language-relevant features from node embeddings and outputs 32 token embeddings which are linearly projected to the LLM word-embedding space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Soft-prompt token projection (Translator)",
            "representation_description": "For each node embedding h_v (from a frozen GM), the Translator uses a transformer encoder with K learnable query tokens that (1) interact via self-attention, (2) cross-attend to the node embedding to extract language-relevant features, producing a sequence of token embeddings (32 tokens of dim 768 in experiments). Those token embeddings are linearly projected into the LLM's word-embedding dimensionality and concatenated with instruction text as a soft prompt to the frozen LLM for generation/prediction.",
            "graph_type": "Text-Attributed Graphs (TAGs) — node-centric graphs where each node has textual attributes (e.g., user/product text, paper title+abstract)",
            "representation_properties": "Fixed-length tokenized representation; compressed summary of node+neighborhood+model bias; designed to bridge modality gap; more compact and lower-noise than raw attribute concatenation; interpretable as a soft prompt to LLMs; potential information loss due to compression into fixed token set; requires alignment data and two-stage training to map into LLM semantic space.",
            "evaluation_task": "Zero-shot node classification (Taobao: Lifestage, Cat Owner, Vehicle Owner) and ArXiv Top-k classification; Graph Question Answering (GQA) multi-turn open-ended dialogue tasks.",
            "performance_metrics": "Metrics reported: Legality Rate (%), Accuracy (%), Recall (% or macro-recall), Macro-F1 (%), Top-k classification accuracy (Top-1/Top-3/Top-5) and human rating counts for GQA (A/B/C/D). Representative reported results: GraphTranslator (full two-stage) — ArXiv Top-1: 28.28%, Top-3: 37.63%, Top-5: 39.88% (Table 2); in GQA, GraphTranslator obtained 210 'A' quality ratings vs Vanilla LLM 203 (counts).",
            "comparison_to_other_representations": "Compared to raw-text prompting baselines (LLM+attribute, LLM+attribute+neighbors) and text-similarity baselines (BERT/RoBERTa), GraphTranslator produced higher legality rates and higher classification and recall in experiments; e.g., ArXiv Top-1: GraphTranslator 28.28% vs Vanilla LLM ≈17.90% (reported baseline). Ablation: Stage1-only Top-1 ≈8.22%, Stage2-only Top-1 ≈16.94%, showing two-stage training outperforms single-stage variants.",
            "limitations_or_challenges": "Relies on high-quality alignment data generated by Producer; fixed-length token compression can lose fine-grained/topological details (node degree, full neighborhood structure); needs careful two-stage training and sufficient alignment pairs; quality depends on downstream frozen LLM vocabulary/embedding dimensionality and linear projection; experiments used ChatGLM2-6B — performance may change with larger/smaller LLMs.",
            "uuid": "e5227.0",
            "source_info": {
                "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Producer",
            "name_full": "Producer module (LLM-based graph-to-text constructor)",
            "brief_description": "A pipeline that uses an LLM (with chain-of-thought style prompting) to generate a textual description for each node by concatenating: (1) node attribute summary, (2) sampled neighbor attribute summary, and (3) model-inferred commonality (what the graph model smooths/shared info), producing (node embedding, textual description) alignment pairs for Translator training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "LLM-generated concatenated node descriptions (node + neighbor + model commonality)",
            "representation_description": "For each node v, the Producer prompts an LLM to: (a) summarize the node's textual attributes into a concise node description, (b) summarize attributes of sampled neighbors N(v) into a neighbor description, and (c) infer/summarize the commonality between the node and neighbors (model bias), then concatenates these three components into a single textual description t_v. These descriptions paired with frozen node embeddings form alignment data for Translator training.",
            "graph_type": "Text-Attributed Graphs (TAGs)",
            "representation_properties": "Aims to be succinct and low-noise by summarization; captures both local node features and sampled neighborhood semantics plus inferred model biases; human-readable text suitable for LLM consumption; quality depends on LLM used to generate descriptions; can omit explicit topological features unless explicitly prompted.",
            "evaluation_task": "Used to train Translator and evaluated indirectly via downstream tasks: zero-shot node classification and graph question answering (GQA). Also evaluated qualitatively for conversational multi-turn consistency.",
            "performance_metrics": "Indirect evaluation via downstream metrics (Legality Rate, Accuracy, Recall, Macro-F1, Top-k); qualitative GQA human ratings (counts of A/B/C/D). Reported outcomes: Producer-generated descriptions led to higher legality rates and better multi-turn GQA performance (GraphTranslator achieved 210 'A' ratings vs Vanilla LLM 203). Exact numeric contribution isolated to Producer alone not separately reported.",
            "comparison_to_other_representations": "Compared to feeding raw node attribute text or raw attribute+neighbor concatenation directly to LLM, Producer-generated summaries are lower-noise and produced better LLM responses when used with the Translator; the paper reports GraphTranslator (which uses Producer) outperforms LLM+attribute and LLM+attribute+neighbors baselines.",
            "limitations_or_challenges": "Producer quality is pivotal — limited by LLM used (authors used ChatGLM2-6B and note larger LLMs like ChatGPT could improve descriptions); current Producer omits some explicit topology (e.g., node degree) and may not capture full graph structural details; generating high-quality descriptions at web-scale may be costly; risk of introducing LLM generation artifacts or biases into alignment data.",
            "uuid": "e5227.1",
            "source_info": {
                "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM+attr",
            "name_full": "Vanilla LLM prompt with node textual attributes (LLM+attribute)",
            "brief_description": "A baseline that directly appends node textual attributes to an instruction and feeds the resulting raw text prompt into a frozen LLM (ChatGLM2-6B) to answer/classify without any embedding-to-token translation.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Raw attribute concatenation (node text -&gt; LLM)",
            "representation_description": "No explicit graph-to-text transformation beyond using the node's original textual attributes; the original node attribute text is concatenated with the task instruction and provided as prompt to the LLM for generation or class prediction.",
            "graph_type": "Text-Attributed Graphs (TAGs)",
            "representation_properties": "Simple and faithful to original text (low transformation), but often noisy and long (especially if including neighbor text), can overwhelm LLM attention and produce noisy or hallucinated outputs; does not compress or emphasize graph-induced semantics.",
            "evaluation_task": "Zero-shot node classification; Graph QA (directly feeding raw text into LLM for multi-turn dialog baseline).",
            "performance_metrics": "Same suite: Legality Rate, Accuracy, Recall, Macro-F1, Top-k. Representative reported result: ArXiv Top-1 accuracy (Vanilla LLM) ≈ 17.90% (paper table), which is lower than GraphTranslator's 28.28% Top-1. Authors report lower legality/accuracy in some Taobao tasks compared to GraphTranslator.",
            "comparison_to_other_representations": "Performed worse than GraphTranslator, which used Producer summaries + Translator soft prompts. Adding neighbor text (LLM+attr+N()) improved some metrics but still underperformed GraphTranslator due to noisy and redundant information in raw concatenation.",
            "limitations_or_challenges": "High prompt length and noisy neighbor information can degrade LLM performance; lacks compression or encoding of graph-structural signals; prone to hallucination when reasoning about implicit graph-derived facts.",
            "uuid": "e5227.2",
            "source_info": {
                "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM+attr+N()",
            "name_full": "Vanilla LLM prompt with node and neighbor textual attributes (LLM+attribute+neighbors)",
            "brief_description": "A baseline that concatenates the node's textual attributes together with sampled neighbor attribute texts into the instruction fed to an LLM, attempting to provide local graph context purely as raw text.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Raw attribute + neighbor concatenation (node+neighbors -&gt; LLM)",
            "representation_description": "Concatenate the node text and the sampled neighbors' texts (N(v)) and append to the task instruction; no learned translation of embeddings; directly used as LLM prompt.",
            "graph_type": "Text-Attributed Graphs (TAGs)",
            "representation_properties": "Provides more contextual information than node-only raw text but is more verbose and noisier; increases prompt length and potential redundancy; may contain contradictory or extraneous information.",
            "evaluation_task": "Zero-shot node classification, Graph QA baseline comparisons.",
            "performance_metrics": "Reported metrics: Legality Rate, Accuracy, Recall, Macro-F1. Authors report LLM+attr+N() often improved legality rates over LLM+attr alone but still underperformed GraphTranslator; e.g., for some Taobao tasks recall improved but was lower than GraphTranslator which better compresses implicit signals.",
            "comparison_to_other_representations": "LLM+attr+N() outperforms LLM+attr in some cases (more context) but is outperformed by GraphTranslator which uses Producer summaries and soft token prompts that reduce noise and compress graph semantics.",
            "limitations_or_challenges": "No learned alignment between embedding and LLM input space; longer prompts can disturb LLM attention; sensitive to which neighbors are sampled and to raw-text noise.",
            "uuid": "e5227.3",
            "source_info": {
                "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "BERT-sim",
            "name_full": "BERT/RoBERTa similarity-based text classifier baseline",
            "brief_description": "Text-only baselines that compute similarity between node text descriptions and label text (using prompting) with pre-trained encoders (BERT, RoBERTa) or their fine-tuned variants to produce zero-shot predictions.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Text similarity / classifier (BERT / RoBERTa)",
            "representation_description": "Encode node textual descriptions (or generated descriptions) and class label descriptions using BERT/RoBERTa; compute similarity (or directly fine-tune) to determine the predicted class; no graph embedding translation involved.",
            "graph_type": "Text-Attributed Graphs (TAGs)",
            "representation_properties": "Relies solely on textual attributes and pre-trained language encoder semantics; straightforward and interpretable but lacks graph structural signals; limited in handling complex zero-shot scenarios compared to LLM-guided approaches.",
            "evaluation_task": "Zero-shot node classification (Taobao tasks reported).",
            "performance_metrics": "Reported metrics in Table 1: Example — Taobao Lifestage accuracy: BERT 34.73%, RoBERTa 33.10% (authors report these baseline numbers); macro-F1 and recall values also provided in table (see paper Table 1).",
            "comparison_to_other_representations": "Performed poorly relative to GraphTranslator and some LLM-based methods; authors note BERT-based methods struggle in complex zero-shot scenarios since they rely only on similarity computations and cannot handle instruction-following open-ended tasks.",
            "limitations_or_challenges": "No explicit incorporation of neighbor or model bias information unless included in the text; limited zero-shot generalization to unseen labels and tasks that require reasoning beyond surface text similarity.",
            "uuid": "e5227.4",
            "source_info": {
                "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "PRODIGY: Enabling In-context Learning Over Graphs",
            "rating": 2,
            "sanitized_title": "prodigy_enabling_incontext_learning_over_graphs"
        },
        {
            "paper_title": "Explanations as Features: LLM-Based Features for Text-Attributed Graphs",
            "rating": 2,
            "sanitized_title": "explanations_as_features_llmbased_features_for_textattributed_graphs"
        },
        {
            "paper_title": "Natural Language is All a Graph Needs",
            "rating": 1,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
            "rating": 1,
            "sanitized_title": "graphprompt_unifying_pretraining_and_downstream_tasks_for_graph_neural_networks"
        }
    ],
    "cost": 0.01357875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks
28 Feb 2024</p>
<p>Mengmei Zhang zhangmengmei@bestpay.com.cn 
Mingwei Sun sunmingwei.smw@alibaba-inc.com 
Peng Wang 
Shen Fan fanshen.fs@alibaba-inc.com 
Yanhu Mo 
Xiaoxiao Xu xiaoxiao.xuxx@alibaba-inc.com 
Hong Liu 
Cheng Yang 
Chuan Shi chuanshi1978@gmail.com 
Xi- Aoxiao Xu 
† 2024 Graphtrans </p>
<p>Alibaba Group Holding Limited
China Telecom Bestpay China</p>
<p>Alibaba Group Holding Limited</p>
<p>Alibaba Group Holding Limited</p>
<p>Alibaba Group Holding Limited</p>
<p>Alibaba Group Holding Limited</p>
<p>Alibaba Group Holding Limited</p>
<p>Alibaba Group Holding Limited</p>
<p>Peng Cheng Laboratory China</p>
<p>Peng Cheng Laboratory China</p>
<p>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks
28 Feb 2024A5475CBB69E9DE09B95BA5B1C1805D5010.1145/3589334.3645682arXiv:2402.07197v4[cs.AI]• Computing methodologies → Natural language generation; • Information systems → Wrappers (data mining) Large Language Model, Graph Neural Network
Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse fields, especially for open-ended tasks.While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form.Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and openended tasks, with LLM as a node feature enhancer or as a standalone predictor.To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM.To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information.By translating node representation into tokens, Graph-Translator empowers an LLM to make predictions based on language instructions, providing a unified perspective for both predefined and open-ended tasks.Extensive results demonstrate the effectiveness of our proposed GraphTranslator on zero-shot node classification.The graph question answering experiments reveal</p>
<p>INTRODUCTION</p>
<p>Graph is commonly used to model many real-world relationships, such as social networks [22,28], citation networks [14] and the e-commerce networks [41].In recent years, graph models (GMs), such as graph neural networks (GNNs) [8,14], which combine node feature information with the graph structure using neural networks, have achieved state-of-the-art performance on a wide range of realworld applications.Despite great success, GMs are restricted to tasks within pre-defined format (e.g., node classification).They only identify pre-defined classes that are present in training phase, which inevitably makes it challenging for GMs to generalize to unseen categories and concepts.</p>
<p>Recently, the emergence of large language models (LLMs) like ChatGPT 1 has brought about a paradigm shift in natural language processing (NLP) research, showcasing their impressive emergent abilities [30] for flexible open-ended tasks based on natural language instructions.Such development of LLMs also has revolutionized research of diverse modality [15,16,32].Taking image as an example, LLMs largely facilitates the visual-centric open-ended tasks, such as instructed image-to-text generation and visual question answering tasks, transforming how we interpret and interact with visual information.</p>
<p>Similarly, for graphs, beyond the foundational pre-defined tasks, there is a strong need for empowering the open-ended tasks.Especially for the text-attributed graphs, which are commonly used in social media and e-commercial, supporting tasks that can be customized by users with text instructions and yield interpretable responses, will greatly enhance the user experience and expand the business scope.To this end, several works have applied LLMs for graph recently, which can be categorized into two classes [2] as showed in Figure 1 (a)-1 and (a)-2: first, leveraging LLMs to enhance nodes' text attributes with their massive knowledge and then generating predictions through GMs [2,3,5,10,33]; second, regarding node as token or text then employing LLM as standalone predictor [2,7,27,35].However, when it comes to practical industrial scenarios, there is a dilemma: The former methods, using LLM as the enhancer of GM predictor, can produce accurate prediction on pre-defined tasks, but fails to process open-ended tasks and lacks interactivity and explainability.The latter methods, employing a LLM as sole predictor, can handle the open-ended tasks while may bring the hallucinations [37] and high cost of LLMs, which is unbearable for pre-defined tasks.It naturally raises a question: Can we build a model that can solve both pre-defined and open-ended tasks?</p>
<p>To answer this question, we propose to align the pre-trained GM to LLM, where GM focuses on the pre-defined tasks, and LLM serves as an interface of GM for open-ended tasks.However, it is non-trivial to align GM to LLM for facing two challenges: (1) There exists a significant modality gap between the pre-trained GM and LLM, due to their differences in data format and processing mechanisms.LLMs only operate on sequences of tokens representing natural language text, and they are trained to understand and generate human-readable text.While graph models process structured graph data and output node embeddings.These embeddings capture the structure and features of graph but are not inherently interpretable as natural language.(2) There lacks alignment data for bridging GM and LLM.Without natural alignment data, it's difficult to train models to understand and translate between the two modalities (node embeddings and textual tokens) effectively.Directly converting may result in loss of information or introducing noise.Ideally, for seamless alignment, there should be a dataset with pairs of node embeddings and corresponding textual descriptions, allowing the model to learn the intricate alignment between GM and LLM.</p>
<p>In this paper, to address above challenges, we propose a novel framework called GraphTranslator to align the pre-trained GM to LLM, solving both pre-defined and open-ended tasks.Specifically, for the challenge of modality gap, GraphTranslator introduces a Translator module which converts node embedding into token embedding space.To achieve this, the Translator module learns a set of graph queries to extract the language information of node embeddings, then performs generative learning for adapting to LLM.For the second challenge of lacking alignment data, we introduce a Producer that capable of constructing (node embedding, textual description) pairs through the powerful generation ability of LLMs.</p>
<p>To seamlessly textualize the information encoded in node embeddings, we generate the description step by step, including briefing node's attribute, summarizing neighbor attribute, then reasoning their commonality.After training on the alignment data, as presented in Figure 1 (b), a frozen LLM, equipped with Translator, can handle various open-ended tasks based on the node embedding and instructions.In conclusion, in our GraphTranslator , pre-defined tasks can be tackled efficiently by the customized GM, while LLMs further provide GM with interactivity and interpretability.</p>
<p>We highlight our contributions as follows:</p>
<p>• We propose a novel model GraphTranslator that aligns graph model to large language model, providing a unified perspective for both pre-defined and open-ended tasks.</p>
<p>METHODOLOGY</p>
<p>In this section, we first present the notations and problem settings used in our model named GraphTranslator, then introduce the architecture of GraphTranslator along with the training strategies.</p>
<p>Notations and Problem Settings</p>
<p>Text-Attributed Graphs We mainly focus on the ubiquitous text-attributed graphs (TAGs), where nodes represent textual entities such as documents or sentences, and edges denote the relationships between them [10].The representation learning on TAGs has attracted attention for past years and is applied to broad applications, ranging from text classification [11] to fake news detection [18].Formally, we define a TAG as G = V, , {  }  ∈ V , where V is a set of  nodes, and  ∈ {0, 1}  × is the adjacency matrix of graph.For each node , it is associated with a sequential text feature, denoted as   .Here we use a subset of   nodes for training GraphTranslator, denoted as V  ⊂ V.</p>
<p>Pre-defined Tasks In the current landscape of the graph domain, numerous graph models are mainly designed and trained for pre-defined tasks, which refer to tasks that are explicitly defined and specified in advance.These tasks typically have well-defined input and output specifications, along with clear evaluation metrics.</p>
<p>When training graph models, researchers or engineers will define these tasks in advance and provide datasets associated with them to train the models.This allows models to focus on solving specific problems and achieve high performance on these tasks, such as node/graph classification [34], link prediction [36,39], node clustering, etc.On the one hand, these well-formalized tasks provide a benchmark for model evaluation, on the other hand, these tasks often serve as the core function of real-world graph systems, requiring high levels of efficiency and precision, such as daily update in e-commerce system.</p>
<p>Open-ended Tasks On the contrary, open-ended tasks offer greater flexibility, characterized by the absence of explicit task specifications or evaluation criteria.Models designed for open-ended tasks often depend on autonomous learning and creative problemsolving approaches.In real-world scenarios, new tasks often emerge with evolving business requirements, such as classifying new labels or tasks driven entirely by human instructions.The computer vision community has also adopted the language instruction paradigm for tasks like image-to-text generation [15,16].However, current graph models are constrained by predefined tasks and fail to adopt to open-ended task customization guided by language instructions like LLMs.</p>
<p>Overall Architecture</p>
<p>The primary goal of our GraphTranslator is to align graph models to LLMs, to leverage the emergent capabilities of LLMs for open-ended tasks.Specifically, GraphTranslator consists of four components:</p>
<p>(1) Frozen graph model (GM), is pre-trained on a large-scale graph, such as e-commerce graphs with billions of nodes, yielding embeddings for all nodes that encoding the graph's information for downstream tasks.We use the pre-trained GraphSAGE model as an example in this work.(2) Frozen LLM, is trained on broad text corpus, showcasing emergent abilities when the number of parameters reach a certain scale.We employ the pre-trained ChatGLM2-6B for demonstration.(3) Producer, is designed to construct the alignment data for training the Translator module, i.e., (node embedding, textual description) pairs.(4) Translator module, is designed to project the GM learned node embeddings into LLM space, eliminating the modality gap between GM and LLM. Figure 2 illustrates the pipeline of our GraphTranslator.With the node embedding from pretrained graph model, the Producer first textualizes target node, neighbors and their commonality, for constructing alignment pairs.In first-stage training, the Translator is trained with the (node embedding, text description) pairs for alignment.In order to facilitate the node embedding to follow instructions better, we bridge the Translator with LLM, then finetuning the Translator with the pairs.To the end, this framework can be generalized to unseen node representation in inference phase, solving open-ended tasks by conversation.</p>
<p>Frozen Graph Model</p>
<p>The representation learning of TAGs has been extensively studied.Given a TAG G = V, , {  }  ∈ V , the typical graph neural networks (GNNs) [8,14,26] is denoted as   (,  ), where  is set of learnable parameters,  is the node features processed by shallow methods such as bag-of-words (BoW) [9] or skip-gram [20].Taking the GraphSAGE [8] as an example, typically, GraphSAGE samples a fixed-size neighbors N () around target node , then concatenates the node's previous layer embedding   −1  with the aggregated neighborhood vectors {  −1  , ∀ ∈ N ()} by:
𝒉 𝑘 𝑣 = 𝜎 (𝑾 𝑘 • CONCAT(𝒉 𝑘 −1 𝑣 ∪AGGREGATE 𝑘 {𝒉 𝑘 −1 𝑢 , ∀𝑢 ∈ N (𝑣)}).
(1) Finally, the pre-trained GM   * encodes the local graph information of  and yields node embedding   =   * (,  )  .</p>
<p>Frozen Large Language Model</p>
<p>LLMs are trained on extensive text corpora, acquiring a substantial amount of knowledge.It's worth noting that LLMs reveal their capabilities only when they reach a certain parameter scale [30].To prevent the potential issues of catastrophic forgetting and the unbearable training costs associated with handling a large number of parameters, here we keep the LLM parameters fixed.Specifically, we employ ChatGLM2-6B, which is open-source bilingual (Chinese-English) language model.ChatGLM2-6B employs a specific type of autoregressive blank infilling task, which aligns with the typical design philosophy of most pretraining tasks.This approach involves a "disrupt and reconstruct" strategy, wherein portions of the original text are masked (disrupted) and subsequently predicted (reconstructed).After extensive training on large-scale corpora, these LLMs acquire the ability to retain a considerable amount of knowledge and provide reasonable answers to human queries.</p>
<p>Producer Module</p>
<p>To align GM and LLM, we construct the alignment data  = {(  ,   )}   =1 , where   outline the information encoded within node embedding   for each node  ∈ V  .This process is not only related to graph data but also intertwined with the design of the GMs, so we employ LLM to construct high-quality description text with Chain-of-Thought (CoT).Taking GraphSAGE as an example, we have devised a pipeline that guides LLM in constructing descriptive information along three key dimensions: • Node Information: Generally, node attributes, such as text or numerical data, are considered as the features for each node.In GMs, these attributes are often transformed into node features, which is achieved by Bag-of-Word or word embeddings models.Therefore, the Producer uses LLM to summarize and analyze the attributes of each node  in the training set, yielding node description, denoted as    .• Neighbor Information: GraphSAGE also consider neighboring information.GraphSAGE randomly samples a subset of neighboring nodes N () and aggregate their representations, yielding the neighbor embedding.Node and neighbor information are further fused through weighted summation or concatenation.So our Producer employs LLM to summarize the attributes of the sampled neighbors N (), resulting in the neighbor information description, denoted as  N ()  .</p>
<p>• Model Information: Given that most GNNs tend to uncover similarities between nodes and their neighbors for smoothing purposes, we instruct LLM to summarize the shared information.Therefore, the Producer further utilizes LLM to infer the commonalities between node  and its neighbors N () based on    and </p>
<p>N (𝑣) 𝑣</p>
<p>, resulting in commonality information denoted as    .Through this carefully designed pipeline, we guide LLM step by step to construct high-quality embedding description text   for each node  ∈ V  , by concatenating node self information, neighbor information, and the model bias, termed   = {   , </p>
<p>N (𝑣) 𝑣</p>
<p>,    }.</p>
<p>Translator Module</p>
<p>There exists modality gap between the trained GMs and LLMs, so LLMs fail to interpret node representations.Namely, the sizes of the node embedding and the input token of LLM are different, and they have different feature space.To resolve this discrepancy, inspired by [15], we introduce the Translator module, which aims to align GM and LLM by converting the learned node embedding into token representations.A naive solution is applying a simple trainable projection matrix can convert   into language embedding tokens, aligning their dimensionality with that of the word embedding space within the language model.While the simple transformation is hard to extract and translate the complex information contained within node representations to natural language, and may struggle to generalize to unseen nodes.In our Translator module, for a pair (  ,   ) in alignment data, we utilize two encoders, denoted as   (•) and   (•), to extract their language features for alignment.For textual description   , we leverage the text encoder   (  ) (e.g., BERT [4]) to extract the language features   =   (  ), where   (•) contains 12 layers of Transformer blocks.For node embedding   , we also adopt a transformer-based network   (•) with  learnable token embeddings as input, termed query tokens  =    =1 , and output  features   = { , }  =1 and   =   (,   ), extracting the information of   that is most related to   .To achieve this, as inspired by [15], the query tokens  are designed to interact with each another using self-attention layers, interface with node embedding   through cross-attention layers, and communicate with description   by sharing the selfattention layers between   and   .</p>
<p>Model Training</p>
<p>Inspired by [15], we train the lightweight Translator module following a two-stage training paradigm, bridging the gap between graph and LLMs step by step.In the first stage, we train the Translator module for extracting   from the node embedding   most relevant to   .In the second stage, we perform the generative learning by connecting the output of the Translator to the frozen LLM.We continue to training the Translator such that its output can be understood by LLM.</p>
<p>Stage 1: Training the Translator for GM-text alignment.</p>
<p>In training, we keep the pretrained node representations frozen and only train Translator module.We jointly optimize three objectives to align   and t , which is the [CLS] token embedding of   .First, the contrastive objective aligns   and t by maximizing their mutual information.We first compute the pairwise similarity between t and each token in   , and select the highest one as the similarity score, then contrast the similarity of a positive pair against those of negative pairs.Second, the generative objective aims to train the Translator module for generating text based on the given embedding   .The essential information of given   is extracted by query tokens  = {  }  =1 in   , then is seamlessly relayed to text tokens in   through the shared self-attention layers.Now we replace the [CLS] token with [DEC] token for the generation task.By optimizing the cross entropy loss between the generated text and the actual description   , the  is forced to capture more details in   related to the   .Third, the matching objective aims to learn the fine-grained alignment.We concatenate each token  , ∈   ( ∈ [1 • • • ]) with the [CLS] token t of   , then feed them into a binary classifier and compute the matching score by averaging the logits across all queries.More details can be found in [15].</p>
<p>Stage 2: Training the Translator for GM-LLM alignment.</p>
<p>We use a linear layer to project the output of Translator module, i.e., token embeddings   , into the same dimension with the word embedding of LLM.And the projected embeddings, which can be regarded as a soft prompt, are concatenated with human instructions as the input of LLM.Then we perform generative learning to tune the parameters of Translator with alignment data.In this way, the node embedding   can be aligned with the pre-trained LLM word embedding.</p>
<p>EXPERIMENT 3.1 Experimental Setting</p>
<p>3.1.1Dataset.We conducted experiments of the proposed Graph-Translator on real-world datasets, including the industrial dataset Taobao and the widely used benchmark dataset ArXiv: • Taobao dataset is a subset extracted from the Taobao e-commerce platform.It consists of 980,000 nodes representing unique Taobao users.The associated attributes include user behaviors like purchases, searches, browsing, favorites, and cart additions.The extracted graph includes 1,790,000 edges, indicating social connections between users.</p>
<p>• ArXiv dataset is a graph constructed by a collection of ArXiv research papers with 169,343 nodes and 1,166,243 edges.Each node represents an individual research paper with textual attributes including the paper's title and abstract, and the edges reflect the paper citation relationship.For training the graph model, we divide nodes into 90,941 training nodes and 29,799 validation nodes following [12].In consideration of LLM's inference speed, our test set contains 4,000 data points for 40 computer science categories, chosen proportionally based on the labels in the public split.</p>
<p>3.1.2Baselines.We compare our model with several pre-trained transformer-based language models.In the zero-shot scenario, we calculate the similarity between node description and the labeled text with a prompt, and predict the most similar class.</p>
<p>• BERT [4] and BERT * : BERT is a pre-trained transformer leveraging masked language modeling for bidirectional context representation from unlabeled text.BERT * further refines this by fine-tuning on given datasets, improving adaptability and knowledge for downstream tasks.</p>
<p>• RoBERTa [17] and RoBERTa * : RoBERTa is a variant of the BERT model which incorporates additional training techniques and data augmentation strategies to improve the model's performance further.Similar to BERT * , we finetune RoBERTa to obtain RoBERTa * .To further examine the effectiveness of our GraphTranslator , we compare with the methods only using text to query LLM:</p>
<p>• LLM+  : It directly appends the original attribute description   to instruction, serving as input for ChatGLM2-6B.</p>
<p>• LLM+  + N () : It simply merges the vanilla text attribute of node and neighbors to instruction to serve as input for ChatGLM2-6B.</p>
<p>Model Details.</p>
<p>Graph Model.In Taobao dataset, we utilize GraphSAGE [8] to generate node embeddings.The GraphSAGE model employs a 2layer aggregation and samples 10 neighbors for each layer.The dimensions of the intermediate layer isset to 768.For the ArXiv dataset, we configure the intermediate dimension as 1024, while maintaining the remaining settings identical to the Taobao dataset.Large Language Model.The LLM model employed in this research is ChatGLM2-6B, which possesses a total parameter size of approximately 6 billion.The model is composed of 28 Transformer blocks, each with a hidden size of 4096 and 32 attention heads.Additionally, the feed-forward network incorporates an intermediate layer dimension of 13,696.The vocabulary size is set at 65,024, while the maximum sequence length is capped at 32,768.</p>
<p>Translator.The Translator module is implemented based on a BERT model, BERT-base-Chinese for the Taobao dataset and BERTbase-English for the ArXiv dataset.It includes 12 layers of Transformer blocks, with alternate layers conducting cross-attention between node embeddings and query tokens, which includes 32 tokens with the dimension of 768.The attention head's number is set to 12.The maximum sequence length is set at 512, with a vocabulary size of 21,128 for the Taobao dataset and 30,522 for the ArXiv dataset.The final output of the Translator contains 32 embeddings with the dimension of 768.</p>
<p>Experiment Environment.</p>
<p>All experiments are conducted on a Linux server with four GPU (Tesla V100, Memory 32G) and CPU (Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz),and its operating system is Ubuntu 20.04.We implement the proposed GraphTranslator with deep learning library PyTorch and PyTorch Geometric.The Python and PyTorch versions are 3.8 and 1.12, respectively.</p>
<p>Zero-shot Node Classification</p>
<p>Zero-shot classification, as an emergent capability of LLMs, allows the model to predict the class that has not been seen during the training phase.Our GraphTranslator , aligned to LLMs, is expected 1.In terms of interests and hobbies, this user pays much attention to carrelated products, indicating that he is a person who likes cars and technology.</p>
<ol>
<li>In terms of personality, his attention to car-related products indicates that he is a person who likes challenges and has the spirit of adventure; He bought a variety of auto parts, which shows that he is a detail-oriented and patient person;</li>
</ol>
<p>His collection of model cars shows that he is a collector with good taste.</p>
<ol>
<li>In terms of life needs, he purchased automobile-related goods,</li>
</ol>
<p>indicating that he has the need to use cars; Purchased auto parts, indicating that he has the need to maintain the car; The purchase of a model car indicates that he has a need to show the car.</p>
<p>Please summarize the common interests and preferences of these friends.</p>
<p>According to the information provided, these friends have the following interests in common:</p>
<ol>
<li>Like to buy a variety of goods, including auto related goods, auto parts, car models, etc. 2. Like collecting car related goods, such as car models, auto parts, etc.Therefore, the common interest preference of these friends is to buy a variety of goods and collect car-related goods.</li>
</ol>
<p>Why does this user become friends with these people?</p>
<p>1.In terms of similarity of hobbies, what this user has in common with his friends is that they are interested in cars, technology and collecting.As a result, they are able to become friends and share their experiences and insights with each other in these areas.2. In this user's hobby, he focuses on auto-related goods, auto parts, and car models.These hobbies take time and effort to collect and maintain.Therefore, his friends also need to have similar interests in order to be friends with him.If his friends are also not interested in the goods and areas he likes, then it is difficult for them to become friends, let alone maintain friendship.</p>
<p>Please summarize the user's interests.</p>
<p>This user has a wide range of interests and hobbies, involving a number of categories, including shoe bags, belt accessories, car accessories, massage equipment, lighting systems, tablet computers, etc.In addition, users' search and favorite records also involve a number of product categories, including lighting systems, car bulbs, tablets, mobile phones, new energy vehicles, etc. Judging from this user's shopping behavior, he/she has diversified interests and likes to try different goods and services.</p>
<p>(1232 words in total)</p>
<p>Among these friends, most of them prefer to buy electronic products, such as mobile phones, tablets, cars, etc., and are also very concerned about peripheral products such as charging banks and mobile phone holders.In addition, many people also like to travel and photography, like to buy some unique ornaments and decorations to decorate the room.In terms of their interests and hobbies, these friends pay more attention to technology and fashion, and like to try new electronic products and travel experiences.</p>
<p>As can be seen from the user's behavior information, there may be many reasons for this user to become friends with these people.First of all, these users have common interests with this user, such as e-commerce platforms, shopping, travel and photography, etc., and the common topics and interests make them easier to get to know.</p>
<p>Secondly, these users and this user have similar preferences in shopping behavior, such as buying shoes and bags, belt accessories, massage equipment, car interior accessories, etc., which makes it easier for them to discover each other and form friendships.</p>
<p>Finally, these users may learn about this user through social media and other channels, and then take the initiative to approach and communicate with each other, and finally become friends.</p>
<p>Why does this user become friends with these people?</p>
<p>Please summarize the common interests and preferences of these friends.The effectiveness is evaluated by utilizing several metrics: (1) Legality Rate, [40], we use legality rate to measure the ratio that model produces valid answers.(2) Accuracy stands for the percentage of the correct predictions over total predicted samples.(3) Recall is calculated as the positive class recall rate in the 2 class task and macro-recall rate in the muli-class task.(4) F1-score is defined as the harmonic mean of precision and recall.The higher F1-score and legality rate hint the better effectiveness.( 5) Top-k classification accuracy represents the percentage of the correct labels which are among the top-k predicted labels (only for Arxiv dataset).Note that the LLM-based models typically make predictions within the format of text rather than discrete labels, so we employ regular expression matching to extract the predicted class from response for evaluation.More details can be found in the Appendix D.</p>
<p>The experimental results on Taobao and ArXiv are presented in Table 1, where we have the following observations: • Our model GraphTranslator achieves better performance than most of baselines, which indicates GM can greatly benefit from LLMs within our GraphTranslator.The BERT-based method performs poorly, as it relies solely on similarity calculations and is unable to handle complex zero-shot scenarios.Our GraphTranslator model performs better than Vanilla LLM, including LLM+  and LLM+  + N () , since LLM directly processes the raw text that contains both node and neighbor attribute, bringing noises and excessive complexity.It demonstrates the superior of GraphTranslator to extract the graph information using the soft prompt translated from node embedding.</p>
<p>• In LLM-based methods, our approach achieves the highest legality rate.The reason may be that LLM+  only inputs the raw text attribute   of nodes into the LLM, which contains limited information and poses significant reasoning challenges for the LLM.Then LLM+  + N () enriches it with the attributes of neighbors N (), and gain the higher legality rate but still suffer from the noisy and redundant text.Especially for Taobao dataset, it is challenging for LLM to derive answers of instructions from the intricate shopping history within user attribute.While our GraphTranslator introduces a Producer module to succinctly summarize   ,  N () and its commonalities, providing rich content while reducing noise.And GraphTranslator takes node representations translated by the Translator, serving as a soft graph prompt that can encapsulate more intricate details than discrete text.Moreover, GraphTranslator projects input embeddings into fixed-length tokens, facilitating the comprehension of LLM for graph information.</p>
<p>• Our GraphTranslator exhibits a particularly notable improvement in recall for positive instances.Taking the Cat Owner prediction on the Taobao dataset as an example, LLM+  + N () method requires explicit product terms in the text, such as "cat food" or "cat litter", to predict as a positive instance.However, the translated node embedding in GraphTranslator encodes and compresses the information, where the products implicitly related to cats are also summarized as cat product explicitly.Hence, GraphTranslator is more adept at accurately identifying the positive instances, which is crucial for industrial applications.</p>
<p>Graph Question Answering (GQA)</p>
<p>To further reveal the potential and commercial value of our Graph-Translator across a wide range of open-ended applications, we showcase the graph question answering (GQA) experiments on Taobao dataset.We query the LLM in a multi-turn dialogue format to deeply investigate the capability of our GraphTranslator to extract, explain and reason the unseen node embedding.</p>
<p>To provide the quantitative analysis of GraphTranslator, we build an evaluation set by randomly sampling 100 nodes and constructing three questions as follows: (1) User Understanding: "Please summarize the user's interests." (2)Friends Understanding: "Please summarize the common interests and preference of these friends." (3)Friendship Analysis: "Why does this user become friends with these people?" For GraphTranslator the translated user embedding is only concatenated with the first question, serving a soft prompt.As a comparison, we directly feed the text attributes of user and neighbors to ChatGLM2-6B.More details of prompts can be found in the Appendix D. As questions are open-ended, we employ both human volunteers and ChatGPT (GPT-3.5-turbo-16k)as the evaluators to perform quantitative analysis.Following [29], we gather question-answering pairs of each test sample and use the four-level rating system:</p>
<p>• Rating-A: The answer is correct and concise, the information is correct, and the reasoning is accurate.• Rating-B: The answer is reasonable, with minor errors or imperfections.</p>
<p>• Rating-C: The answer is relevant to the question, but has obvious errors or inaccuracies in the content.• Rating-D: The response is irrelevant or completely invalid.</p>
<p>The comparison results are presented in Figure 3, where we have following observations:</p>
<p>• Only provided unseen node embedding as prompt for LLM, our GraphTranslator gets 210 A in total, showcasing strong performance comparable to Vanilla LLM which gets 203.This is because that GraphTranslator are trained on the low-noise descriptions generated by our Producer, thus the trained Translator can extract high-quality information from the node embedding for multi-turn dialogue.</p>
<p>• One can observe that our GraphTranslator consistently achieves better performance in Q1 and Q3, especially for the challenging question Q3 which requires model to understand graph and reason why these people become friends.It demonstrates the superior of GraphTranslator to extract graph information based on node embedding.</p>
<p>A detailed case is showed in Figure 4. We analyse several basic abilities which are reflected in the model's response, and have the following observations:</p>
<p>• Graph understanding: Taking the summarization of user's and friends' interests (i.e., car) as examples, the Vanilla LLM can understand the instructions, but struggles to extract the key characteristics.Specifically, in response A1, Vanilla LLM lists numerous specific products as marked by the red strikethrough, which belong to friends, not users.And by A2, Vanilla LLM summaries too many hobbies, but only car is the true shared interest.The reason is that the lengthy prompt in Q1 (over 1000 words) may disturb the attention mechanism of LLM.On the other hand, as highlighted in green, our GraphTranslator identifies the primary interests and hobbies in A1, analyzes the personality and life needs, and in A2, successfully concludes that their common interest is cars.It demonstrates the superior ability of GraphTranslator to extract and interpret graph information using the soft prompt translated from node embedding.</p>
<p>• Reasoning ability: The question Q3 requires models to understand graph information and reason why these people become friends.The Vanilla LLM captures much noise information of friends, tending to give a general explanation.Our GraphTranslator provides explanations from three perspectives, i.e., the similarity of hobbies, the maintenance of friendship, and friend influence, which are all centered around the user and friends' shared interest of cars.The final answer is fundamentally accurate, and it presents an explicit and logical reasoning process.</p>
<p>• Multi-turn dialogue ability: By providing graph information only in the first question Q1, we can observe the improvement in our model's multi-turn dialogue capability compared to Vanilla LLM.Our GraphTranslator maintains a consistent graph-centric response throughout the conversation.This capability is attributed to the low-noise descriptions generated by our Producer, thus the trained Translator can extract high-quality information from the node embedding for multi-turn dialogue.</p>
<p>CONCLUSIONS</p>
<p>In this paper, we propose a novel framework to align graph models (GMs) to LLM, named GraphTranslator, aiming to utilize the extended interface of LLMs to offer various open-ended tasks for GM.GraphTranslator introduces a Translator module to eliminate the modality gap, by converting node embeddings learned by GM to a set of tokens.For further training, a Producer module is designed to generate the alignment data, through seamlessly textualizing the information encoded in node embeddings.We evaluate our method on real-world datasets for open-ended tasks.The experimental results demonstrate the effectiveness of GraphTranslator on zero-shot node classification.The preliminary graph question answering experiments indicate the capability of our GraphTranslator to extract, explain and reason the graph information, revealing the potential and commercial value.</p>
<p>A DISCUSSION</p>
<p>In this paper, we investigate the alignment of graph models (GMs) with Large Language Models (LLMs) to augment the capability of GMs in addressing open-ended tasks.While the preliminary experiments reveal our GraphTranslator potential on open-ended applications, there are several limitations listed as follows:</p>
<p>• The Producer plays a pivotal role in dictating the overall quality of the Translator model.In future work, more topology information encoded in node embeddings, like node degree, can be included in the description in order to reduce information loss.Moreover, due to limited resources, we employed ChatGLM2-6B to generate descriptions in Producer.In future work, we can use larger-scale LLMs such as ChatGPT, to improve the quality of the generated text descriptions.And integrating novel LLM utilities and techniques, such as with Chain-of-Thought [31] and AutoPrompt [23], also could further improve the performance.• In the experiment, we only have labels for quantitative analysis in our zero-shot node classification, and for the GQA task, we merely showcase our GraphTranslator performance through specific cases.</p>
<p>To offer a complete and quantitative evaluation of model capabilities in open-ended tasks, such as graph understanding, explaining, reasoning, and multi-round conversation, it's important to develop an evaluation dataset and devise corresponding metrics in future work.</p>
<p>B RELATED WORK B.1 Graph Representation Learning</p>
<p>Graph representation learning, including earlier shallow graph embedding [22] and graph neural networks [8,14,26], aims to capture and encode these relationships in a way that facilitates various downstream tasks.Traditional graph models primarily focus on supervised learning, requiring a substantial amount of labeled data for tasks such as node classification.These methods achieve strong performance when sufficient labeled data is available, but tend to underperform in scenarios with limited labeled data.Inspired by pretrained language models, the new paradigm of "Pre-train, Prompt, and Predict" has been recognized for its effectiveness in addressing few-shot downstream tasks [6,13,19,24,25,38,42].Despite their advancements, achieving zero-shot learning and open-ended tasks remains challenging.</p>
<p>B.2 Large Language Model for Graph</p>
<p>The NLP landscape has recently been revolutionized by language modeling (LM), which is one of the major approaches to advancing the language intelligence of machines [4].The goal of LM is to model the generative likelihood of text for predicting future (or masked) token probabilities.Recently, researchers have observed that when the model sizes of the pre-trained LMs up to a certain scale, the LMs will showcase some remarkable capabilities, named emergent abilities [30].Here we use the term "large language models" (LLMs) to denote such language models that have the extensive number of billions parameters, and have been pre-trained on vast corpora of data [1].These LLMs, like ChatGPT and GPT4 [21], can effectively follow language even multi-modal instructions, aligned with human intent to complete various real-world tasks.More recently, applying LLMs for graph domain has received several preliminary research experimental trials already, which can be categorized into two classes [2]: The first, LLMs-as-Enhancers [3,5,10], augments node text attributes with LLM knowledge, and still utilizes GMs for predictions.They can produce accurate predictions on traditional tasks, but fail to make full use of the capabilities of text generation, instruction following for open-ended tasks.The second, LLMs-as-Predictors, treats nodes as tokens or text and uses LLMs as the standalone predictor, which often can generate imaginative content but training/inferring LLMs can be time-consuming.This can actually hinder their widespread deployment for web-scale pre-defined tasks.</p>
<p>C EXPERIMENT DETAILS C.1 Code</p>
<p>In the development of our work, we have built upon the efforts and open-source contributions of several pioneering projects, for which we extend our heartfelt gratitude.</p>
<p>• LAVIS (https://github.com/salesforce/LAVIS):The logical architecture of LAVIS library served as the foundation for our code development.</p>
<p>• ChatGLM (https://github.com/THUDM/ChatGLM-6B):An opensource LLM with the amazing language capabilities.• BLIP2 (https://arxiv.org/abs/2301.12597):our model is inspired from BLIP2.</p>
<p>In the spirit of open science and collaboration, we are also excited to share the source code of our work with the community: https: //github.com/alibaba/GraphTranslator.</p>
<p>C.2 Training Details.</p>
<p>• Pre-training Graph Model Phase.In the pre-training phase, we employ link prediction as the self-supervised task for pre-training the graph model.For each batch in the Taobao dataset, we randomly select 1024 positive and negative edges.In the case of the ArXiv dataset, we sample 65,536 edges for each batch, and the ratio of positive and negative examples is set to 1:1 in both datasets.We use Adam as the optimizer and set the learning rate and weight decay to 1e-4 and 1e-3 for the Taobao dataset.For the Arxiv dataset, we set the learning rate to 0.01 and omit the weight decay.</p>
<p>• Pre-training LLM Phase.We adopt the well-pretrained ChatGLM2-6B2 , which is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B.</p>
<p>• Stage 1 Training Phase.We utilize the 274,168 and 90,941 pairs of node representation and textual description for Taobao and ArXiv datasets respectively.We employ a batch size of 16 and train the GraphTranslator for 8 epochs.We adopt the AdamW optimizer and set the learning rate and weight decay to 1e-6 and 0.05 respectively.Furthermore, to accelerate the training process and enlarge the batch size of gradient backpropagation, we incorporate the technique of gradient accumulation with 32 steps.The implementation of gradient accumulation results in an augmented batch size of 512 for gradient backpropagation.</p>
<p>• Stage 2 Training Phase.We maintain the same number of pairs as in stage 1.Due to memory limitations, we employ a batch size of 8  Taobao Based on the product information, please describe the characteristics of this user, and the common characteristics of his friends in interests, hobbies, personality traits, and life needs.</p>
<p>ArXiv</p>
<p>Please summarize the topic and content of the paper and its cited papers in English.</p>
<p>and train the GraphTranslator for 3 epochs.We adopt the AdamW optimizer, and set the learning rate and weight decay to 1e-6 and 0.05 respectively.We also incorporate gradient accumulation technique during the Stage 2, which results in an augmented batch size of 256 for gradient backpropagation.</p>
<p>C.3 Further Experiment</p>
<p>To validate the effectiveness of training strategies, we compare our GraphTranslator with its variants, "Stage 1 Only" and "Stage 2 Only".Taking Arxiv dataset as an example, the results are presented in the Table 2.We observe that the training of stage 1, despite effectively aligning graph embeddings and texts, fails to map graph embeddings into the semantic space of the LLM.As a result, the LLM is hard to understand semantic information and it exhibits significantly lower performance compared to GraphTranslator.On the other hand, although stage 2 bridges the gap between graph embedding and LLM directly, it lacks the understanding between embeddings and texts, thus contributing to the sub-optimal performance.In conclusion, the two-phase training of GraphTranslator is crucial for enabling LLM to comprehend the graph information, ultimately leading to the optimal results.</p>
<p>D PROMPT DESIGN</p>
<p>The prompts of Taobao and ArXiv datasets are presented in the Table 3, 4, 5.For the prompts of the Taobao dataset, we translate them into English with Youdao3 .The prompts for the inference stage are shown in Table 5, we force the model to answer the number corresponding to the label in the Taobao dataset.Therefore, we employ regular expression matching to identify the relevant labels based on the digital number.For the ArXiv dataset, we extract labels based on the textual descriptions associated with each category.</p>
<p>Figure 1 :
1
Figure 1: Intuitive illustration of our GraphTranslator (a) Comparisons of GraphTranslator with popular paradigms of applying LLMs to graphs.Unlike using LLM as enhancer or sole predictor, GraphTranslator bridges LLM and GM, handling both pre-defined and open-ended tasks.(b) Demonstrations of tasks in GraphTranslator, where GM is leveraged for pre-defined tasks, and the LLM is extended as the interface of GM for open-ended tasks.</p>
<p>Figure 2 :
2
Figure 2: The overall framework of our GraphTranslator, which aligns GM to LLM by Translator for open-ended tasks.We train the lightweight Translator module following a two-stage paradigm, with the alignment data generated by our Producer.</p>
<p>Figure 3 :
3
Figure 3: Quantitative analysis of graph question answering.The order of response quality ranking is as follows: A &gt; B &gt; C &gt; D.</p>
<p>to classify the nodes to unseen classes.We conduct zero-shot node classification on four tasks as follows: (1) Taobao Lifestage prediction aims to reason the life stage of user to three categories [Single, Married, Parented].(2) Taobao Cat Owner prediction is to infer whether the user has cat.(3) Taobao Vehicle Owner prediction is to + Please summarize the user's interests.</p>
<p>3 .…
3
In terms of friend influence, the user and his friends can influence each other.For example, if his friends have experience in car maintenance, he can ask them for advice on how to better maintain his car; If his friends have experience with model cars, he can ask them for advice on how to make his own model cars.By learning from and sharing with each other, they can get to know each other better and achieve greater success in life.Based on the following information about the browsing, collecting and purchasing behavior of a user and his friends on the e-commerce platform, analyze why the user became friends with these people.Here is some information about their behavior: This user purchased […4 items…]; Search and collect […4 items…]; Browse […3 items…].He has several friends, and their purchase information is as follows: Friend 53: Purchased […3 items…], Searched and collected […4 items…], Browsed […3 items…].Friend 11:......</p>
<p>Figure 4 :
4
Figure 4: A case of graph question answering on Taobao Dataset.</p>
<p>Table 1 :
1
Results on zero-shot node classification.LLM+  LLM+  + N () GraphTranslator
Dataset BERT RoBERTa BERT  Taobao Metric Legality Rate (%) 100.00 100.00 100.00 Accuracy (%) 34.73 33.10 32.97100.00 34.5350.10 33.4655.57 34.5958.80 35.33(Lifestage)Recall (%)34.7333.1032.9734.5333.4634.5935.33Macro-F1 (%)27.1724.5625.0625.7331.6332.6032.62Legality Rate (%) 100.00100.00100.00100.0031.2045.4398.97TaobaoAccuracy (%)51.1350.8749.0348.7751.9258.5550.99(Cat Owner)Recall (%)87.4060.4063.2711.7312.8245.5695.69Macro-F1 (%)43.7350.4247.9840.6221.0552.9666.14Legality Rate (%) 100.00100.00100.00100.0063.9786.1794.60TaobaoAccuracy (%)47.5347.9347.3748.7346.7449.0949.40(Vehicle Owner)Recall (%)59.0054.7351.5364.6063.0161.2983.27Macro-F1 (%)46.8347.6947.2847.4154.6255.1561.87Legality Rate(%) 100.00100.00100.00100.0099.1599.4097.8ArXivTop-1 Acc (%) Top-3 Acc (%)1.63 7.633.55 11.9814.53 29.606.95 16.5314.07 26.9817.90 28.4328.48 37.62Top-5 Acc (%)28.0022.9338.3023.7542.4637.9939.87GraphTranslatorVanilla LLM
* RoBERTa *</p>
<p>Table 2 :
2
Impact of Stage 1 and Stage 2
Legality Rate Top-1 Top-3 Top-5GraphTranslator (Stage1 Only)98.288.2216.94 25.92GraphTranslator (Stage2 Only)99.6016.94 27.54 41.24GraphTranslator97.8028.28 37.63 39.88</p>
<p>Table 3 :
3
Prompts For the ProducerUser Behavior Description: <User Behavior Description>.Please summarize the characteristics of this user according to the product behavior information.The answer format is: What kind of characteristics does the user have in terms of interests, hobbies, personality traits, and life needs Neighbor behavior summary Neighbor Behavior Description: <Neighbor Behavior Description>.Please summarize most of the similarities that this user's friends have based on the product behavior information.The answer format is: What do several friends of this user have in common in interests, hobbies, personality traits, and life needs?ArXiv Paper SummaryThe title and abstract of this paper are as follows: <Title text> \t <Abstract text>.pleasesummarizethis paper and list five keywords of this paper.Neighbor Paper SummaryThe paper title and abstract are provided as follows: <Title text> \t <Abstract text>.\n <Title text> \t <Abstract text>.... \n Please summarize the topic and content of these papers.All answers are in English and No Chinese in your answer
Dataset StepPromptTaobao User behavior summary</p>
<p>Table 4 :
4
Prompts For the Training Stage
Dataset Prompt
https://huggingface.co/THUDM/chatglm2-6b
https://fanyi.youdao.com/
ACKNOWLEDGMENTSThis work is supported by Alibaba Group through Alibaba Innovative Research Program.Thanks to our colleagues at Alibaba for their help on engineering implementation of our GraphTranslator, including Bei Yang and Yudong Luo.Question: You are a home marketing analyst, you must use only one character to answer the guess about the user's family situation based on the user's purchase information for yourself or for the family, the answer format is [X] : Answer if the user is a single person who is not in a relationship[1], answer if the user is a childless person who is in a relationship and has no children[2], answer if the user is a married and childless person who has children in the family[3].Answer:
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021. 2021</p>
<p>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Haifang Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang, ArXiv abs/2307.033932023. 2023</p>
<p>Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction. Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S Dhillon, ICLR. 2022</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXiv:2308.02565Simteg: A frustratingly simple approach improves textual graph learning. 2023. 2023arXiv preprint</p>
<p>Universal Prompt Tuning for Graph Neural Networks. Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, Lei Chen, 2022258866085</p>
<p>GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, ArXiv abs/2305.150662023. 2023</p>
<p>Inductive Representation Learning on Large Graphs. William L Hamilton, Zhitao Ying, Jure Leskovec, Neural Information Processing Systems. 2017</p>
<p>Distributional Structure. S Zellig, Harris, The philosophy of linguistics. 1954</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, Bryan Hooi, arXiv:2305.19523Explanations as Features: LLM-Based Features for Text-Attributed Graphs. 2023. 2023arXiv preprint</p>
<p>Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification. Linmei Hu, Tianchi Yang, Chuan Shi, Houye Ji, Xiaoli Li, Conference on Empirical Methods in Natural Language Processing. 2019</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 332020. 2020</p>
<p>PRODIGY: Enabling In-context Learning Over Graphs. Qian Huang, Hongyu Ren, Peng Chen, Gregor Krvzmanc, Dajun Daniel, Percy Zeng, Jure Liang, Leskovec, ArXiv abs/2305.126002023. 2023</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ArXiv abs/2301.125972023. 2023</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, ArXiv abs/2304.08485Visual Instruction Tuning. 2023. 2023258179774</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Finegrained Fact Verification with Kernel Graph Attention Network. Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu, Annual Meeting of the. Association for Computational Linguistics2019</p>
<p>GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang, Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023. 2023</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, NIPS. 2013</p>
<p>DeepWalk: online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven S Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014. 2014</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, arXiv:2010.159802020. 2020arXiv preprint</p>
<p>GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks. Mingchen Sun, Kaixiong Zhou, Xingbo He, Ying Wang, Xin Wang, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022. 2022</p>
<p>All in One: Multi-Task Prompting for Graph Neural Networks. Xiangguo Sun, Hongtao Cheng, Jia Li, Bo Liu, Jihong Guan, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023. 2023</p>
<p>Graph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, stat. 10502017. 2017</p>
<p>Can Language Models Solve Graph Problems in Natural Language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, ArXiv abs/2305.100372023. 2023</p>
<p>Community Preserving Network Embedding. Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, Shiqiang Yang, AAAI. 2017</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022. 2022arXiv preprint</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai Hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Trans. Mach. Learn. Res. 20222022. 2022</p>
<p>Chain of Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, ArXiv abs/2201.119032022. 2022</p>
<p>Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, ArXiv abs/2303.04671Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. 2023. 2023</p>
<p>Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications. Han Xie, Da Zheng, Jun Ma, Houyu Zhang, N Vassilis, Xiang Ioannidis, Qing Song, Sheng Ping, Carl J Wang, Yi Yang, Belinda Xu, Trishul Zeng, Chilimbi, KDD. 2023</p>
<p>How Powerful are Graph Neural Networks. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, International Conference on Learning Representations. 2019</p>
<p>Natural Language is All a Graph Needs. Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang, ArXiv abs/2308.071342023. 2023260887732</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 2018. 201831</p>
<p>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, ArXiv abs/2309.012192023. 2023</p>
<p>Heterogeneous Region Embedding with Prompt Learning. Silin Zhou, Dan He, Lisi Chen, Shuo Shang, Peng Han, AAAI Conference on Artificial Intelligence. 2023</p>
<p>Progresses and challenges in link prediction. Tao Zhou, Iscience. 24112021. 2021</p>
<p>Xuanhe Zhou, Guoliang Li, Zhiyuan Liu, LLM As DBA. 2023260775770</p>
<p>AliGraph: A Comprehensive Graph Neural Network Platform. Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, Jingren Zhou, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019. 2019</p>
<p>SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. Yun Zhu, Jianhao Guo, Siliang Tang, ArXiv abs/2302.124492023. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
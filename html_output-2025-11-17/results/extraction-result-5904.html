<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5904 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5904</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5904</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-b408198f2c891720738c385272afeeebcd747268</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b408198f2c891720738c385272afeeebcd747268" target="_blank">HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations.</p>
                <p><strong>Paper Abstract:</strong> The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5904",
    "paper_id": "paper-b408198f2c891720738c385272afeeebcd747268",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003568,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution</h1>
<p>Ehsan Kamalloo ${ }^{<em> \dagger}$ Aref Jafari ${ }^{</em> \dagger}$ Xinyu Zhang ${ }^{\dagger}$<br>Nandan Thakur ${ }^{\dagger}$ Jimmy Lin ${ }^{\dagger}$<br>${ }^{\dagger}$ David R. Cheriton School of Computer Science, University of Waterloo<br>ekamalloo@uwaterloo.ca</p>
<h4>Abstract</h4>
<p>The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-theloop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have paved the way for the emergence of generative informationseeking search engines such as Bing Chat, Google Bard, and perplexity.ai, where search results are formulated in natural language text, incorporating references to the relevant web pages from which they are derived. This approach aims to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Question</h2>
<p>What was Octavia E. Butler's first novel?</p>
<h2>Quotes</h2>
<p>[1] Survivor is a science fiction novel by American writer Octavia E. Butler. First published in 1978 as part of Butler's "Patternist series"...
[2] Butler's first work published was "Crossover" in the 1971 Clarion Workshop anthology... Starting in 1974, Butler worked on a series of novels that would later be collected as the Patternist series... The first novel, "Patternmaster" (1976), eventually became the last installment in the series' internal chronology...</p>
<h2>Answer</h2>
<p>Octavia E. Butler's first novel was "Patternmaster" which was published in 1976 and was also the first installment in her "Patternist series" [2].</p>
<p>Informative? Yes
Attributable? Yes
Table 1: An example taken from HAGRID that includes a question along with a list of relevant passages (quotes), an answer generated by GPT3.5 (§3.3), and informativeness and attributability evaluated by human annotators (§3.4).
provide users with contextually rich responses. Yet, LLMs are known to generate text lacking sufficient grounding to knowledge sources (Dziri et al., 2022; Ji et al., 2023), thereby posing risks of misinformation and even worse, hallucination (Maynez et al., 2020; Raunak et al., 2021). This problem becomes particularly critical within search engines where such inaccuracies can erode user trust and potentially spread misinformation (Metzler et al., 2021; Shah and Bender, 2022). Building models that are capable of incorporating citations that link to some supporting evidence is a vital step toward understanding the behaviour of LLMs, allowing users to easily verify the factu-</p>
<p>ality of model outputs. The development of such models further fosters interpretable LLMs and attributable outputs (Rashkin et al., 2023), thus reinforcing the transparency and reliability of LLMs.</p>
<p>A significant obstacle in building generative search models equipped with citations is the lack of accessible and openly available datasets. The data used by commercial search engines for training their generative information-seeking models are typically proprietary and not accessible to the public, thereby hindering their widespread use in the open-source community.</p>
<p>In this paper, we introduce a new dataset for generative information-seeking scenarios to address these limitations. Our dataset is constructed on top of MIRACL (Zhang et al., 2022), an information retrieval dataset that consists of information-seeking questions along with a set of manually labeled relevant passages (quotes). We collect attributed explanations for each question by eliciting prompts from an LLM, i.e., GPT-3.5 (Ouyang et al., 2022), based on the given relevant passages. The explanations adhere to an in-context citation style, similar to scientific articles, that references the supporting quotes. We next ask human annotators to judge the explanations based on two criteria, (i) informativeness: whether the explanation provides a direct answer to the question, and (ii) attributability: whether the explanation is attributable to the source passages. We name our dataset HAGRID, representing Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset. An example question along with its relevant passage and the generated answer is presented in Table 1.</p>
<p>HAGRID consists of two subsets: training and development, enabling researchers to train and evaluate future information-seeking models with attribution capabilities. In particular, we seek to establish a dataset for building open-source end-to-end search models capable of retrieving candidate quotes and generating attributable answers based on input queries, which are key ingredients in retrieval-augmented generative models (Lewis et al., 2020; Izacard and Grave, 2021; Borgeaud et al., 2022). In contrast to existing datasets (Liu et al., 2023a; Gao et al., 2023), our emphasis on both openness and the integration of human annotations makes HAGRID a valuable and unique resource in this area. HAGRID is publicly released under the Apache 2.0 License. We hope that open- sourcing of the dataset spurs innovation and further advancements in the rapidly growing area of generative search.</p>
<h2>2 Related Work</h2>
<p>Explainability. Understanding why models behave in certain ways is crucial in deploying them in real-world applications (Doshi-Velez and Kim, 2017). A common approach for explainability in NLP is to provide human-understandable explanations for particular outputs of a black-box model (Camburu et al., 2018). Numerous attempts were made in many language understanding tasks including text classification (Camburu et al., 2018; Liu et al., 2019), question answering (Abujabal et al., 2017; Rajani et al., 2019), fact verification (Atanasova et al., 2020; Kotonya and Toni, 2020), and summarization (Li et al., 2021) to generate rationales that explain models' outputs. While these explanations are in line with our goal in this paper, they are not necessarily attributable (Jacovi and Goldberg, 2020). Moreover, several benchmarks (DeYoung et al., 2020; Mathew et al., 2021) were proposed to evaluate the generated rationales. Towards this goal, Narang et al. (2020) built a general-purpose T5 model that generates explanations for its predictions.</p>
<p>Attributability. Rashkin et al. (2023) formalize an attributable statement to identified sources such that it can be entailed from some underlying corpus by a generic hearer. Thus, attributability is a specific form of explainability within the constraints of a given source. WebGPT (Nakano et al., 2021) and GopherCite (Menick et al., 2022) are two recent closed-source models that are capable of generating references to their supporting evidence. From the data perspective, several QA datasets (Geva et al., 2021; Bohnet et al., 2022) provide pointers to text snippets supporting the gold answer. Moreover, two recent works (Liu et al., 2023a; Gao et al., 2023) focus on verifying citations in generated text based on a given set of quotes, which closely aligns with our objective in this paper. Specifically, Liu et al. (2023a) focus on closed-source proprietary search engines, whereas our goal is to use publicly available data to allow for building open-source end-to-end search models. Similarly, ALCE (Gao et al., 2023), a concurrent work to HAGRID, shares a similar goal, albeit with two notable differences. First, Gao et al. (2023) derive questions from QA datasets</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: HAGRID’s data collection workflow. We first take a question and positive passages (quotes) from MIRACL, and reformat them into a prompt to instruct an LLM to generate answers with in-context citations (See an example in Figure 2). The answers generated by the LLM are evaluated by human annotators based on two criteria: informativeness (whether they correctly fulfill the question), and attributability (whether the source quotes appearing in the answers support the answers).
that consist of question and answer pairs but lack annotations for quotes. To identify quotes, a retrieval model is adopted to determine relevant passages by matching them with gold answers. However, automated answer equivalence is shown to be fallible, especially for long-form answers [Kamalloo2023; Xu2023] that could lead to accepting irrelevant quotes or rejecting legitimate quotes. Second, ALCE automatically determines the generated answer correctness as well as the citation quality, in contrast with HAGRID where we employ human annotators to validate these important criteria, thereby minimizing the risk of propagating any tool errors into the dataset.</p>
<p>Using LLMs for Dataset Creation. Due to the substantial costs, time constraints, and potential biases associated with human data collection, researchers sometimes resort to leveraging machines to reduce the human involvement. With the advent of proficient LLMs, machine-assisted techniques have become viable to some degree [Saunders2022; Wiegreffe2022] and in various downstream tasks including natural language inference [liu2022], instructionfollowing [wang2023; Honovich2023], and information retrieval [Boni2022; Jeronymo2023], data collection has evolved into a collaborative effort between models and humans.</p>
<h2>3 Data Collection</h2>
<h3>3.1 Task Formulation</h3>
<p>We characterize the task of attributable information-seeking as the following: given a query $Q$ and $n$ text snippets $\mathcal{S}=s_{1}, \cdots, s_{n}$ that are relevant to $Q$, the goal is to formulate an answer $A$ to $Q$ such that statements in $A$ are cited to their supporting source snippets $s_{i}$ based on which they are generated. Specifically, answer $A$ is composed of $m$ sentences $a_{1}, \cdots, a_{m}$; each ends with a reference $\left[r_{a_{j}}\right]$ where $r_{a_{j}}$ is a set of integers referring to the indexes of snippets in $\mathcal{S}$; that is, $r_{a_{j}} \in{1 . . n}$. Note that although certain cases such as "according to [1]..." or "...supply chain [2]" are not explicitly covered by this formulation, such sentences can often be rewritten to follow the specified format. Also, generic sentences like "Below is an explanation." do not require citation, but they are not common in information-seeking scenarios. The examples</p>
<p>of cited answers are shown in Figure 1. Our objective is to curate contextualized summary answers derived from a list of text snippets, while also providing the corresponding snippets from which the answers originate.</p>
<h3>3.2 Datasets</h3>
<p>Numerous datasets have been designed for information-seeking scenarios in open-domain QA (Joshi et al. 2017; Lee et al. 2019; inter alia) and information retrieval (Bajaj et al. 2018; Soboroff et al. 2019; Voorhees et al. 2021; inter alia). In this work, we opt to equip existing retrieval datasets with attribution rather than constructing a dataset from scratch. This is primarily motivated by the fact that existing retrieval datasets already contain high-quality queries with judged text snippets, but they typically lack the rationales for annotated answers. By leveraging existing queries, we streamline the data collection process, enabling us to focus on attributability.</p>
<p>MIRACL (Zhang et al., 2022), a multilingual information retrieval (IR) dataset containing queries over Wikipedia articles for 18 diverse languages. The evaluated retrieval task setting is monolingual, i.e., both the query and document are of the same language. The dataset was created using human annotators following a setup similar to TYDI QA (Clark et al., 2020). Unlike prior work that segments Wikipedia articles into fixed 100-word passages (Karpukhin et al., 2020; Clark et al., 2020; Asai et al., 2021), MIRACL split documents based on natural discourse units using two consecutive newlines. The dataset represents a standard ad hoc retrieval task, where passages have been marked relevant for each query. In this work, we focus on working using the English subset of MIRACL and leave out other languages for future work. There are 32.8 M passages, 2,863 queries in the training set, and 799 queries in the development set of the MIRACL English subset.</p>
<h3>3.3 Answer Generation</h3>
<p>In contrast to QA datasets such as SQuAD (Rajpurkar et al., 2016), Natural Questions (Kwiatkowski et al., 2019), or ELI5 (Fan et al., 2019), questions in MIRACL do not have gold answers. While gold answers could be obtained via human annotations, the effort would be costly and prohibitively time-consuming. Instead, in our work, we use an existing off-the-shelf LLM to elicit answers because of their ability to effec-
tively generate explanatory answers (Wiegreffe et al., 2022).</p>
<p>We input all the positive passages for each query (with at least one relevant passage) in the MIRACL dataset into an LLM. This setup is inspired by retrieval-augmented generation (Lewis et al., 2020), wherein generation is conditioned not only on the query but also on the retrieved passages. The relevant passages, derived from the English Wikipedia in MIRACL, will be referred to as "Quotes." As reported in Table 2, nearly 3 quotes on average are provided for each query. We instruct GPT-3.5, i.e., gpt-3.5-turbo-0301 (OpenAI, 2022), to generate an answer to a question in a zero-shot fashion. We do not prepare any demonstrations or instructions for prompting with GPT-3.5. We provide an instruction, and a list of quotes as contexts and ask the LLM to reference answers within brackets [ ] in the IEEE format. The complete instruction used is provided in Figure 2. We also explored several instructions in the prompt to guide the LLM in generating both short and long answers, leading us to collect multiple answers per query. However, we found no significant differences between these generated answers. All the quotes can easily fit within the GPT-3.5 context window size of 4,096 tokens.</p>
<p>We further post-processed model responses to verify the format of model responses using regular expressions and filtered out the ones that violate the specified format.</p>
<h3>3.4 Human Annotation</h3>
<p>For human assessment, we hired 4 specialist annotators with 1+ year of experience with text data annotation on our team. Each annotator was interviewed prior to being hired and was verified to be a fluent and efficient annotator. To minimize any potential biases and ensure consistency in the annotation process, our team implemented a carefully designed onboarding procedure with training sessions specifically tailored to this task. The annotators were remunerated with an hourly rate of $\$ 15.2$ USD. In total, the project required approximately 1,400 annotation hours to complete.</p>
<p>Before proceeding with answer annotation, we initially decomposed answers into sentences. This is in large part to simplify the task as individual sentences are easier to read and evaluate, thus accelerating the data annotation process. It also al-</p>
<p>lows for collecting fine-grained annotations. If a sentence lacks citations, we group it with the following sentence that includes a citation, as the citation may pertain to all the grouped sentences. Following this pre-processing step, we asked our human evaluators to assess two criteria in generated responses:</p>
<ul>
<li>Informativeness checks whether a generated answer provides a useful response to the question. More precisely, if at least one sentence within an answer is labelled informative, the entire answer is deemed informative. In essence, this criterion is identical to perceived utility in <em>Liu et al. (2023a)</em>. Notably, informativeness encompasses a broader scope, compared to correctness in <em>Gao et al. (2023); Liu et al. (2023b)</em> because it ensures accuracy as well as relevance by taking additional information in the answers into account.</li>
<li>Attributability measures whether factual claims in a generated answer can be supported by corresponding quotes. An answer sentence would be labelled attributable only if it is fully supported by a cited quote. In cases where multiple citations appear in an answer sentence, all cited quotes must contain ample evidence to validate the sentence. When all sentences within an answer are labelled attributable, the answer is deemed attributable. We observed that annotating attributability takes 3-5x longer than annotating informativeness since annotators should carefully read all cited quotes to arrive at a decision. This is why, we were not able to collect annotations for all generated answers due to budget constraints.</li>
</ul>
<h3>3.5 Statistics</h3>
<p>Table 2 provides an overview of HAGRID in the answer generation phase, prior to human annotation. The training and development sets contain 1,922 and 716 questions, respectively. Using GPT-3.5, we generate around 3,214 (1.7 per question on average) and 1,318 (1.8 on average) answers for train and development sets accordingly. Moreover, 6,577 and 3,305 citations (2.0 and 2.5 per answer on average) were generated within answers.</p>
<p>The statistics of the annotation results are reported in Table 3. All the generated answers have been manually evaluated for informativeness, while around 24% (754) and 88% (1,157) of the answers have been evaluated for attributability on the training and development sets, respectively. The distributions of both informativeness and attributability are greatly consistent between the training and development sets (Informative: 84% and 90% answers marked "yes"; Attributable: 73% and 71% answers marked "yes", respectively for training and development sets).</p>
<h2>4 HAGRID Analysis</h2>
<p>This section presents an in-depth analysis of the HAGRID dataset and discusses our main observations. Our aim is two-fold: (1) examining the content of answers with respect to the two criteria, introduced in §3.4, and (2) how quotes are cited in answers.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 3: Comparative analyses of the y-axis distribution with respect to different answer labels: attributable vs. unattributable and informative vs. uninformative. Each curve is a KDE plot to represent distribution. The horizontal dash lines depict the first, second, and third quartiles.</p>
<table>
<thead>
<tr>
<th></th>
<th>Train</th>
<th>Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td>非 Questions</td>
<td>1,922</td>
<td>716</td>
</tr>
<tr>
<td>Avg. 非 Quotes per Question</td>
<td>2.7</td>
<td>2.9</td>
</tr>
<tr>
<td>非 Generated Answers</td>
<td>3,214</td>
<td>1,318</td>
</tr>
<tr>
<td>Avg. Answer per Question</td>
<td>1.7</td>
<td>1.8</td>
</tr>
<tr>
<td>非 Citations</td>
<td>6,577</td>
<td>3,305</td>
</tr>
<tr>
<td>Avg. Citation per Answer</td>
<td>2.0</td>
<td>2.5</td>
</tr>
</tbody>
</table>
<p>Table 2: The statistics of the answers generated by LLM, prior to human annotation. Given a question, the model generates multiple answers (Generated Answers, and each answer may be accompanied by more than one cited quotes (Citations).</p>
<p>Unattributable and uninformative answers often correspond to higher number of quotes. Figure 3a depicts the impact of the number of associated quotes with respect to informativeness and attributability. The mean distribution of the number of quotes for unattributable and uninformative answers is higher than that of attributable and informative answers. Hence, as the number of associated quotes grows, LLMs tend to become fallible in generating informative and attributable answers.</p>
<p>Attributable answers are semantically close to their referring quotes. Figure 3b plots the distribution of semantic similarity, measured using BERTScore [zhang2020bertscore], between answer sentences and corresponding cited sentences in quotes. Looking at the median of the distribution, informative answers are only slightly more</p>
<table>
<thead>
<tr>
<th></th>
<th>Total</th>
<th>Yes</th>
<th></th>
<th>No</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Coverage vs. Density between generated answers and their cited quotes. Answers tend to be extractive, thus words are frequently copied from quotes into the answers.</p>
<p>quotes which subsume answer words. Answers with larger chunks of text copied from their quotes will result in higher density. Figure 4 illustrates the coverage and density distributions. While coverage largely falls between 0.5 and 1.0, density is more varied. These results indicate that generated answers tend to use words from their associated quotes and thus, are mostly extractive.</p>
<p>When the number of quotes is small, quotes are cited nearly evenly. We analyze the citation frequency to determine which quotes are referenced in the generated answers. The findings are illustrated in Figure 5 where the x-axis represents the number of associated quotes and the y-axis shows the percentage of the cited quotes. When the number of quotes remains below 5, the indices of cited quotes are distributed evenly in general. However, when the number of quotes surpasses 5, the top-3 quotes receive higher citations, while the lower-ranked quotes are cited far less frequently.</p>
<h2>5 Conclusion</h2>
<p>Generative search with the ability to cite supporting sources has gained a lot of traction lately. However, the absence of accessible high-quality data inhibits progress in building open-source information-seeking models. In this paper, we seek to bridge this gap in the community by introducing HAGRID, a new dataset for building end-to-end generative retrieval models. Our dataset is collected via a human-machine collaboration that starts with generating explanatory answers to</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Frequency of citation indices based on different numbers of given quotes; <strong>x-axis</strong>: the number of given quotes in the prompt; <strong>y-axis</strong>: the frequency of indices in the citation. The citations percentage that are larger than or equal to 10% are marked on the figure.</p>
<p>information-seeking queries from GPT-3.5, followed by a human assessment of correctness and attributability of the generated answers. HAGRID facilitates the development of open-source models for information-seeking scenarios. Our human study has shed light on the room for improvement, i.e., around 40% of GPT-3.5 generated answers are not informative and over 20% fail to demonstrate attribution to the quotes. Moving forward, future research endeavors may focus on building more accurate models, aimed at mitigating the errors commonly encountered in current LLMs.</p>
<h2>Limitations</h2>
<p>The scope of our dataset is on information-seeking scenarios that mainly inquire about factual statements that usually do not warrant creative or complex reasoning. Thus, more challenging questions with multi-hop reasoning (yang2018happiness), discrete reasoning (dua2019detection), etc. are not covered in this study.</p>
<p>Another limitation is that HAGRID covers only English. While the source dataset, MIRACL, is multilingual and encompasses 18 languages, we leave non-English languages, either high-resource or low-resource, for future work.</p>
<h2>Ethical Statement</h2>
<p>Although we do not foresee any major risk or negative societal impact of our dataset, models con-</p>
<p>structed on HAGRID may inadvertently produce biased outputs due to the reported tendencies of LLMs to generate stereotypes or biases. Therefore, care must be exercised for responsible deployment of such models in real-world applications.</p>
<h2>References</h2>
<p>Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, and Gerhard Weikum. 2017. QUINT: Interpretable question answering over knowledge bases. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 61-66, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Akari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2021. XOR QA: Cross-lingual open-retrieval question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 547-564, Online. Association for Computational Linguistics.</p>
<p>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating fact checking explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352-7364, Online. Association for Computational Linguistics.</p>
<p>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv preprint arXiv:1611.09268.</p>
<p>Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2022.</p>
<p>Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.</p>
<p>Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Unsupervised dataset generation for information retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22, pages 2387-2392. Association for Computing Machinery.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, JeanBaptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206-2240. PMLR.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.</p>
<p>Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454-470.</p>
<p>Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443-4458, Online. Association for Computational Linguistics.</p>
<p>Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022. On the origin of hallucinations in conversational models: Is it the datasets or the models? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5271-5285, Seattle, United States. Association for Computational Linguistics.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558-3567, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. arXiv preprint arXiv:2305.14627.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719,</p>
<p>New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409-14428, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880, Online. Association for Computational Linguistics.</p>
<p>Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198-4205, Online. Association for Computational Linguistics.</p>
<p>Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large language models as efficient dataset generators for information retrieval. arXiv preprint arXiv:2301.01820.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. 2023. Evaluating opendomain question answering in the era of large</p>
<p>language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5591-5606, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Neema Kotonya and Francesca Toni. 2020. Explainable automated fact-checking for public health claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7740-7754, Online. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096, Florence, Italy. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wentau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems, volume 33, pages 9459-9474. Curran Associates Inc.</p>
<p>Haoran Li, Arash Einolghozati, Srinivasan Iyer, Bhargavi Paranjape, Yashar Mehdad, Sonal Gupta, and Marjan Ghazvininejad. 2021. EASE: Extractive-abstractive summarization with explanations. arXiv preprint arXiv:2105.06982.</p>
<p>Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. 2022. WANLI: Worker and AI collaboration for natural language inference dataset creation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6826-6847, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards explainable NLP: A generative explanation framework for text classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570-5581, Florence, Italy. Association for Computational Linguistics.</p>
<p>Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848.</p>
<p>Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023b. WebGLM: Towards an efficient web-enhanced question answering system with human preferences. arXiv preprint arXiv:2306.07906.</p>
<p>Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. HateXplain: A benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1486714875 .</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy</p>
<p>Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147.</p>
<p>Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search: making domain experts out of dilettantes. SIGIR Forum, 55(1):1-27.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browserassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546.</p>
<p>OpenAI. 2022. Introducing ChatGPT.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, pages 2773027744. Curran Associates, Inc.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Florence, Italy. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process-
ing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring attribution in natural language generation models. Computational Linguistics, pages 1-66.</p>
<p>Vikas Raunak, Arul Menezes, and Marcin Junczys-Dowmunt. 2021. The curious case of hallucinations in neural machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172-1183, Online. Association for Computational Linguistics.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.</p>
<p>Chirag Shah and Emily M. Bender. 2022. Situating search. In Proceedings of the 2022 Conference on Human Information Interaction and Retrieval, CHIIR '22, pages 221-232. Association for Computing Machinery.</p>
<p>Ian Soboroff, Shudong Huang, and Donna Harman. 2019. TREC 2019 news track overview. In TREC.</p>
<p>Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. TREC-COVID: Constructing a pandemic information retrieval test collection. SIGIR Forum, 54(1):1-12.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi.</p>
<ol>
<li>Reframing human-AI collaboration for generating free-text explanations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 632-658, Seattle, United States. Association for Computational Linguistics.</li>
</ol>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32253245, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations.</p>
<p>Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David AlfonsoHermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2022. Making a MIRACL: Multilingual information retrieval across a continuum of languages. arXiv preprint arXiv:2210.09984.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal Contribution
${ }^{\dagger}$ HAGRID is released at https://github.com/ project-miracl/hagrid.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
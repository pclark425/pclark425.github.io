<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2723 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2723</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2723</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-c8fc50bc2cd673fa4e5c5ac581f6fe3fcdaf6b8d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c8fc50bc2cd673fa4e5c5ac581f6fe3fcdaf6b8d" target="_blank">Learning Knowledge Graph-based World Models of Textual Environments</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work focuses on the task of building world models of text-based game environments and frames this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduces both a transformer-based multi-task architecture and a loss function to train it.</p>
                <p><strong>Paper Abstract:</strong> World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2723.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2723.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Worldformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Worldformer (multi-task transformer world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task transformer-based world model for text games that encodes observations and an explicit knowledge graph, predicts graph differences between steps, and autoregressively decodes both the next-step graph-difference and the set of contextually valid actions using Set-of-Sequences losses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Worldformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multi-encoder (BERT-like text encoder, BERT-like graph encoder) + aggregator feeding two GPT-2-like autoregressive decoders (one for graph-difference, one for valid actions). Trained multi-task with a Set-of-Sequences (SOS) loss and predicts G_{t+1}-G_t rather than full G_{t+1}.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td>380M</td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>JerichoWorld (derived from Jericho text-games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A collection of text-adventure (interactive fiction) games (27 training games, 9 test games) where agents perceive textual observations, maintain knowledge graphs of state and must generate valid natural-language actions to change state; tasks: predict next-step knowledge graph and next-step valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Set of <subject, relation, object> triples (knowledge graph) encoded by a graph encoder (BERT-like); aggregated into a state vector via an aggregator and accessed via cross-attention by decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Discovered locations, objects, attributes, relations and the agent's current location and inventory — represented as triples (e.g., you, have, lantern).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not a fixed numeric capacity; training uses a fixed graph vocabulary (7,002 tokens); dataset statistics: mean full graph ~8.71 triples/state, mean graph-difference ~3.42 triples/state (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Full-graph encoding: G_t is encoded each step by the graph encoder and made available to decoders via cross-attention (i.e., attention-based retrieval from encoded graph representation).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated every action-step by predicting the graph difference G_{t+1}-G_t; the predicted add-tuples are applied to G_t (and deletions inferred) to form G_{t+1}.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Track world state across partial observability, narrow action space (generate contextually relevant valid actions), and provide look-ahead by predicting state changes to bias action generation/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>State-weighted overall metrics reported across 9 test games: Knowledge-graph prediction (graph-level EM overall 39.15%*, graph-level F1 41.06%*; token-level EM 51.32%, token-level F1 52.45%). Valid-action prediction: exact match (EM) overall 23.22%, F1 overall 25.54%. (Table 1; * indicates statistically top result.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Comparable single-task Seq2Seq baseline (which encodes graphs but lacks Worldformer components) achieved much lower KG prediction: graph-level EM overall 14.29%, token-level EM 18.80%; valid-action EM 18.10% (Table 1). Ablations removing Worldformer components (graph-difference, multi-tasking, SOS loss) caused significant drops—largest drop when graph-difference simplification removed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper reports three key reasons memory (graph-based) is effective: (1) predicting graph differences (G_{t+1}-G_t) drastically reduces prediction target size and improves tractability; (2) multi-task training (jointly predicting next graph-difference and valid actions) shares information between acting and mapping and improves performance on both tasks; (3) treating outputs as permutation-invariant Sets-of-Sequences (SOS loss) better matches the structure of graphs/actions and improves decoding. Graph memory enables look-ahead (e.g., inferring chest contents before observation) and mitigates partial observability and combinatorial action space.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not many direct failure modes are claimed for the Worldformer itself, but general limitations noted include: prediction difficulty increases with larger target sets (motivating graph-difference), label imbalance in valid actions across games (affects action prediction), and vocabulary/decoding difficulty for actions (action vocabulary ~11,056). Extraction-based methods (Rules/QA) suffer from over-extraction and fail when information is hidden in observations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>The paper finds the best configuration is: graph-based memory encoded by a graph encoder, predict graph-differences (G_{t+1}-G_t), multi-task train jointly with valid-action decoder, and train decoders with the SOS loss — this combination (the Worldformer) outperforms single-task and extractive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Knowledge Graph-based World Models of Textual Environments', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2723.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2723.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT (QA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (Question-Answering based graph construction, ALBERT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that populates a knowledge graph by asking questions about observations using a QA transformer (ALBERT variant) and applying some hand-authored rules to construct triples; used as a strong extractive baseline for graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT (QA-based extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an ALBERT-style QA model (pretrained + finetuned on SQuAD and Jericho-QA) to answer structured questions about the current observation; answers are converted into graph triples via rules.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>JerichoWorld / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same benchmark as used in the paper: mapping observations to knowledge graphs and valid actions across text-adventure games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory constructed via question-answer extraction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Set of <subject, relation, object> triples assembled from extracted QA answers and hand rules (i.e., an explicit symbolic knowledge graph).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Entities and relations mentioned in immediate observations (objects, locations, attributes) extracted from text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not explicitly stated; uses a fixed graph vocabulary of 7,002 tokens (same as other sequence models).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Extraction from the current textual observation via QA model; assembled triples become the graph available to downstream modules.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Re-extract/populate graph from observations (question answering) at each step and apply some rules to update the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To populate a symbolic knowledge graph representing current visible state, used for downstream reasoning or agent policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Strong token-level performance: token-level EM overall 53.58% and token-level F1 overall 55.74%; graph-level EM overall 32.79%, F1 overall 35.48% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Extraction-based QA yields high token overlap (good token-level metrics) because it highlights input text, but tends to over-extract (extracting text not strictly part of graph) which hurts graph-level exact-match metrics in some cases; also vulnerable when world state is hidden and not directly described in text (e.g., chest contents revealed only after opening).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Prone to over-extraction, brittle when information is not explicitly described in the observation, and cannot perform 'look-ahead' inference about unobserved facts as effectively as learned generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Knowledge Graph-based World Models of Textual Environments', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2723.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2723.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rules (OpenIE + hand rules)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based graph extraction (OpenIE + hand-authored rules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline extraction pipeline using OpenIE and hand-authored rules to extract graph triples from observations to populate a knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Constrained Reinforcement Learning for Natural Language Action Spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rules-based extractor</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses off-the-shelf open information extraction (OpenIE) plus additional hand-authored rules to extract subject-relation-object triples from the observation text and update the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>JerichoWorld / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same dataset/benchmark mapping observations into knowledge graphs and valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory populated via rule-based information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Explicit set of <s,r,o> triples created by extraction rules and OpenIE.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Entities, relations and attributes directly extractable from observation text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified; uses same graph token vocabulary in downstream comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Extraction from current observation; no learned retrieval mechanism described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Apply extraction rules to each observation to add triples; update via rules per step.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Populate an explicit symbolic world model to assist action selection and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Poorer performance in experiments: graph-level EM overall 4.70%, graph-level F1 7.25%; token-level EM overall 13.08%, token-level F1 17.50% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Rule-based extraction can be useful when information is explicitly present and well-formed, but performs poorly relative to learned extractive (QA) and generative (Worldformer) approaches; limited by brittleness and inability to infer unobserved facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Sensitive to irregularities in text, brittle due to hand rules, and low coverage when descriptions are non-standard or when important facts are latent/hidden.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Knowledge Graph-based World Models of Textual Environments', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2723.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2723.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA-W</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATA-World (adapted Graph-Aided Transformer Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adapted Graph-Aided Transformer Agent architecture that encodes text and graphs and uses a single decoder to predict add/del operations on the graph (i.e., predicting both additions and deletions in one Seq2Seq pass).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA-W</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same encoder structure as Worldformer (text + graph encoders) but single-task: one decoder trained with standard Seq2Seq loss to output add/del graph operations (predicts (G_{t+1}-G_t) ∪ (G_t-G_{t+1})).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>JerichoWorld / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure mapping tasks; graph prediction and action prediction across many IF games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Explicit triples (add/del) predicted as tokens by a single Seq2Seq decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>World state as triples including additions and deletions between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not explicitly specified; uses same triple tokenizer and graph vocabulary (7,002 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph encoded by graph encoder and attended by decoder during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Predicts add/delete tuples directly; updates graph by applying predicted operations per step.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Track and update world state for downstream decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Intermediate performance: graph-level EM overall 24.06%, graph-level F1 25.19%; token-level EM overall 35.31%, token-level F1 37.10% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Predicting a smaller set of operations (add/del) is more tractable than predicting full graphs; GATA-W outperforms basic Seq2Seq because it reduces the per-step prediction target size, but multi-tasking and SOS loss in Worldformer yield further improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Single-task decoding of add/del still larger target than graph-difference-only approach; performance lags behind Worldformer which uses multi-task and SOS losses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Knowledge Graph-based World Models of Textual Environments', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2723.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2723.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seq2Seq graph/action predictor (JerichoWorld baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single-task encoder-decoder baseline that encodes observation and graph with a BERT-like encoder and decodes the next graph or actions with a GPT-2-like autoregressive decoder trained with standard Seq2Seq cross-entropy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling worlds in text.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Seq2Seq baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>BERT-like bidirectional encoder over observation+graph and a GPT-2-like autoregressive decoder; trained single-task with left-to-right cross-entropy (no SOS loss, no multi-tasking, no graph-difference simplification).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>JerichoWorld / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same dataset: per-step mapping from observation and current graph to next-graph or valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based input (provided as part of input context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph represented as tokenized triples fed to encoder (sequence representation, not explicitly permutation-invariant).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Current state's graph triples and textual observation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Uses the same graph/token vocabularies; no explicit capacity limit beyond encoder input length.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Encoder processes provided graph tokens; decoder conditions on encoded context when generating outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Decoder generates full G_{t+1} (or action set) directly; no explicit graph-difference update simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Used as contextual input to predict next graph or valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Lower performance compared to Worldformer: KG prediction graph-level EM overall 14.29%, token-level EM 18.80%; valid-action EM overall 18.10% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Seq2Seq is sensitive to the average number of valid actions (label imbalance); tends to learn common navigation actions before fine-grained actions. Treating graphs as plain sequences and not exploiting permutation invariance and graph-difference makes prediction harder.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Does not exploit graph-difference simplification or SOS permutation invariance, resulting in worse performance and higher difficulty when the per-step target size is larger.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Knowledge Graph-based World Models of Textual Environments', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2723.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2723.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM (GPT-2 tuned on ClubFloyd action traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2-based model fine-tuned on a human-created dataset (ClubFloyd) of observation-action pairs for valid action generation; used as an external-data tuned baseline for action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 style generative language model (fine-tuned on ClubFloyd corpus of human text-game transcripts) used to generate actions conditioned on observations (and prior action/next observation in its training setup).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>JerichoWorld (valid action prediction evaluated across test games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Action-generation task: given state observation and context, predict the set of valid natural-language actions that will change the world-state.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not applicable (no structured memory). Reported valid-action performance: overall EM 13.79% and F1 19.11% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>CALM shows high variance across games: performs better on some games whose genres match the ClubFloyd training data; not explicitly using structured graph memory may limit ability to predict fine-grained actions that depend on latent world state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Knowledge Graph-based World Models of Textual Environments', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic belief graphs to generalize on text-based games. <em>(Rating: 2)</em></li>
                <li>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Graph Constrained Reinforcement Learning for Natural Language Action Spaces. <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games. <em>(Rating: 2)</em></li>
                <li>Modeling worlds in text. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2723",
    "paper_id": "paper-c8fc50bc2cd673fa4e5c5ac581f6fe3fcdaf6b8d",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Worldformer",
            "name_full": "Worldformer (multi-task transformer world model)",
            "brief_description": "A multi-task transformer-based world model for text games that encodes observations and an explicit knowledge graph, predicts graph differences between steps, and autoregressively decodes both the next-step graph-difference and the set of contextually valid actions using Set-of-Sequences losses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Worldformer",
            "agent_description": "Multi-encoder (BERT-like text encoder, BERT-like graph encoder) + aggregator feeding two GPT-2-like autoregressive decoders (one for graph-difference, one for valid actions). Trained multi-task with a Set-of-Sequences (SOS) loss and predicts G_{t+1}-G_t rather than full G_{t+1}.",
            "base_model_size": "380M",
            "game_benchmark_name": "JerichoWorld (derived from Jericho text-games)",
            "game_description": "A collection of text-adventure (interactive fiction) games (27 training games, 9 test games) where agents perceive textual observations, maintain knowledge graphs of state and must generate valid natural-language actions to change state; tasks: predict next-step knowledge graph and next-step valid actions.",
            "uses_memory": true,
            "memory_type": "graph-based memory (knowledge graph)",
            "memory_structure": "Set of &lt;subject, relation, object&gt; triples (knowledge graph) encoded by a graph encoder (BERT-like); aggregated into a state vector via an aggregator and accessed via cross-attention by decoders.",
            "memory_content": "Discovered locations, objects, attributes, relations and the agent's current location and inventory — represented as triples (e.g., you, have, lantern).",
            "memory_capacity": "Not a fixed numeric capacity; training uses a fixed graph vocabulary (7,002 tokens); dataset statistics: mean full graph ~8.71 triples/state, mean graph-difference ~3.42 triples/state (reported).",
            "memory_retrieval_strategy": "Full-graph encoding: G_t is encoded each step by the graph encoder and made available to decoders via cross-attention (i.e., attention-based retrieval from encoded graph representation).",
            "memory_update_strategy": "Updated every action-step by predicting the graph difference G_{t+1}-G_t; the predicted add-tuples are applied to G_t (and deletions inferred) to form G_{t+1}.",
            "memory_usage_purpose": "Track world state across partial observability, narrow action space (generate contextually relevant valid actions), and provide look-ahead by predicting state changes to bias action generation/planning.",
            "performance_with_memory": "State-weighted overall metrics reported across 9 test games: Knowledge-graph prediction (graph-level EM overall 39.15%*, graph-level F1 41.06%*; token-level EM 51.32%, token-level F1 52.45%). Valid-action prediction: exact match (EM) overall 23.22%, F1 overall 25.54%. (Table 1; * indicates statistically top result.)",
            "performance_without_memory": "Comparable single-task Seq2Seq baseline (which encodes graphs but lacks Worldformer components) achieved much lower KG prediction: graph-level EM overall 14.29%, token-level EM 18.80%; valid-action EM 18.10% (Table 1). Ablations removing Worldformer components (graph-difference, multi-tasking, SOS loss) caused significant drops—largest drop when graph-difference simplification removed.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Paper reports three key reasons memory (graph-based) is effective: (1) predicting graph differences (G_{t+1}-G_t) drastically reduces prediction target size and improves tractability; (2) multi-task training (jointly predicting next graph-difference and valid actions) shares information between acting and mapping and improves performance on both tasks; (3) treating outputs as permutation-invariant Sets-of-Sequences (SOS loss) better matches the structure of graphs/actions and improves decoding. Graph memory enables look-ahead (e.g., inferring chest contents before observation) and mitigates partial observability and combinatorial action space.",
            "memory_limitations": "Not many direct failure modes are claimed for the Worldformer itself, but general limitations noted include: prediction difficulty increases with larger target sets (motivating graph-difference), label imbalance in valid actions across games (affects action prediction), and vocabulary/decoding difficulty for actions (action vocabulary ~11,056). Extraction-based methods (Rules/QA) suffer from over-extraction and fail when information is hidden in observations.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "The paper finds the best configuration is: graph-based memory encoded by a graph encoder, predict graph-differences (G_{t+1}-G_t), multi-task train jointly with valid-action decoder, and train decoders with the SOS loss — this combination (the Worldformer) outperforms single-task and extractive baselines.",
            "uuid": "e2723.0",
            "source_info": {
                "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Q*BERT (QA)",
            "name_full": "Q*BERT (Question-Answering based graph construction, ALBERT variant)",
            "brief_description": "A baseline that populates a knowledge graph by asking questions about observations using a QA transformer (ALBERT variant) and applying some hand-authored rules to construct triples; used as a strong extractive baseline for graph construction.",
            "citation_title": "How to avoid being eaten by a grue: Structured exploration strategies for textual worlds.",
            "mention_or_use": "use",
            "agent_name": "Q*BERT (QA-based extractor)",
            "agent_description": "Uses an ALBERT-style QA model (pretrained + finetuned on SQuAD and Jericho-QA) to answer structured questions about the current observation; answers are converted into graph triples via rules.",
            "base_model_size": null,
            "game_benchmark_name": "JerichoWorld / text-adventure games",
            "game_description": "Same benchmark as used in the paper: mapping observations to knowledge graphs and valid actions across text-adventure games.",
            "uses_memory": true,
            "memory_type": "graph-based memory constructed via question-answer extraction",
            "memory_structure": "Set of &lt;subject, relation, object&gt; triples assembled from extracted QA answers and hand rules (i.e., an explicit symbolic knowledge graph).",
            "memory_content": "Entities and relations mentioned in immediate observations (objects, locations, attributes) extracted from text.",
            "memory_capacity": "Not explicitly stated; uses a fixed graph vocabulary of 7,002 tokens (same as other sequence models).",
            "memory_retrieval_strategy": "Extraction from the current textual observation via QA model; assembled triples become the graph available to downstream modules.",
            "memory_update_strategy": "Re-extract/populate graph from observations (question answering) at each step and apply some rules to update the graph.",
            "memory_usage_purpose": "To populate a symbolic knowledge graph representing current visible state, used for downstream reasoning or agent policies.",
            "performance_with_memory": "Strong token-level performance: token-level EM overall 53.58% and token-level F1 overall 55.74%; graph-level EM overall 32.79%, F1 overall 35.48% (Table 1).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Extraction-based QA yields high token overlap (good token-level metrics) because it highlights input text, but tends to over-extract (extracting text not strictly part of graph) which hurts graph-level exact-match metrics in some cases; also vulnerable when world state is hidden and not directly described in text (e.g., chest contents revealed only after opening).",
            "memory_limitations": "Prone to over-extraction, brittle when information is not explicitly described in the observation, and cannot perform 'look-ahead' inference about unobserved facts as effectively as learned generative models.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2723.1",
            "source_info": {
                "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Rules (OpenIE + hand rules)",
            "name_full": "Rule-based graph extraction (OpenIE + hand-authored rules)",
            "brief_description": "A baseline extraction pipeline using OpenIE and hand-authored rules to extract graph triples from observations to populate a knowledge graph.",
            "citation_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces.",
            "mention_or_use": "use",
            "agent_name": "Rules-based extractor",
            "agent_description": "Uses off-the-shelf open information extraction (OpenIE) plus additional hand-authored rules to extract subject-relation-object triples from the observation text and update the knowledge graph.",
            "base_model_size": null,
            "game_benchmark_name": "JerichoWorld / text-adventure games",
            "game_description": "Same dataset/benchmark mapping observations into knowledge graphs and valid actions.",
            "uses_memory": true,
            "memory_type": "graph-based memory populated via rule-based information extraction",
            "memory_structure": "Explicit set of &lt;s,r,o&gt; triples created by extraction rules and OpenIE.",
            "memory_content": "Entities, relations and attributes directly extractable from observation text.",
            "memory_capacity": "Not specified; uses same graph token vocabulary in downstream comparisons.",
            "memory_retrieval_strategy": "Extraction from current observation; no learned retrieval mechanism described.",
            "memory_update_strategy": "Apply extraction rules to each observation to add triples; update via rules per step.",
            "memory_usage_purpose": "Populate an explicit symbolic world model to assist action selection and reasoning.",
            "performance_with_memory": "Poorer performance in experiments: graph-level EM overall 4.70%, graph-level F1 7.25%; token-level EM overall 13.08%, token-level F1 17.50% (Table 1).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Rule-based extraction can be useful when information is explicitly present and well-formed, but performs poorly relative to learned extractive (QA) and generative (Worldformer) approaches; limited by brittleness and inability to infer unobserved facts.",
            "memory_limitations": "Sensitive to irregularities in text, brittle due to hand rules, and low coverage when descriptions are non-standard or when important facts are latent/hidden.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2723.2",
            "source_info": {
                "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "GATA-W",
            "name_full": "GATA-World (adapted Graph-Aided Transformer Agent)",
            "brief_description": "An adapted Graph-Aided Transformer Agent architecture that encodes text and graphs and uses a single decoder to predict add/del operations on the graph (i.e., predicting both additions and deletions in one Seq2Seq pass).",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "mention_or_use": "use",
            "agent_name": "GATA-W",
            "agent_description": "Same encoder structure as Worldformer (text + graph encoders) but single-task: one decoder trained with standard Seq2Seq loss to output add/del graph operations (predicts (G_{t+1}-G_t) ∪ (G_t-G_{t+1})).",
            "base_model_size": null,
            "game_benchmark_name": "JerichoWorld / text-adventure games",
            "game_description": "Text-adventure mapping tasks; graph prediction and action prediction across many IF games.",
            "uses_memory": true,
            "memory_type": "graph-based memory (knowledge graph)",
            "memory_structure": "Explicit triples (add/del) predicted as tokens by a single Seq2Seq decoder.",
            "memory_content": "World state as triples including additions and deletions between steps.",
            "memory_capacity": "Not explicitly specified; uses same triple tokenizer and graph vocabulary (7,002 tokens).",
            "memory_retrieval_strategy": "Graph encoded by graph encoder and attended by decoder during generation.",
            "memory_update_strategy": "Predicts add/delete tuples directly; updates graph by applying predicted operations per step.",
            "memory_usage_purpose": "Track and update world state for downstream decision making.",
            "performance_with_memory": "Intermediate performance: graph-level EM overall 24.06%, graph-level F1 25.19%; token-level EM overall 35.31%, token-level F1 37.10% (Table 1).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Predicting a smaller set of operations (add/del) is more tractable than predicting full graphs; GATA-W outperforms basic Seq2Seq because it reduces the per-step prediction target size, but multi-tasking and SOS loss in Worldformer yield further improvements.",
            "memory_limitations": "Single-task decoding of add/del still larger target than graph-difference-only approach; performance lags behind Worldformer which uses multi-task and SOS losses.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2723.3",
            "source_info": {
                "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Seq2Seq baseline",
            "name_full": "Seq2Seq graph/action predictor (JerichoWorld baseline)",
            "brief_description": "Single-task encoder-decoder baseline that encodes observation and graph with a BERT-like encoder and decodes the next graph or actions with a GPT-2-like autoregressive decoder trained with standard Seq2Seq cross-entropy loss.",
            "citation_title": "Modeling worlds in text.",
            "mention_or_use": "use",
            "agent_name": "Seq2Seq baseline",
            "agent_description": "BERT-like bidirectional encoder over observation+graph and a GPT-2-like autoregressive decoder; trained single-task with left-to-right cross-entropy (no SOS loss, no multi-tasking, no graph-difference simplification).",
            "base_model_size": null,
            "game_benchmark_name": "JerichoWorld / text-adventure games",
            "game_description": "Same dataset: per-step mapping from observation and current graph to next-graph or valid actions.",
            "uses_memory": true,
            "memory_type": "graph-based input (provided as part of input context)",
            "memory_structure": "Graph represented as tokenized triples fed to encoder (sequence representation, not explicitly permutation-invariant).",
            "memory_content": "Current state's graph triples and textual observation.",
            "memory_capacity": "Uses the same graph/token vocabularies; no explicit capacity limit beyond encoder input length.",
            "memory_retrieval_strategy": "Encoder processes provided graph tokens; decoder conditions on encoded context when generating outputs.",
            "memory_update_strategy": "Decoder generates full G_{t+1} (or action set) directly; no explicit graph-difference update simplification.",
            "memory_usage_purpose": "Used as contextual input to predict next graph or valid actions.",
            "performance_with_memory": "Lower performance compared to Worldformer: KG prediction graph-level EM overall 14.29%, token-level EM 18.80%; valid-action EM overall 18.10% (Table 1).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Seq2Seq is sensitive to the average number of valid actions (label imbalance); tends to learn common navigation actions before fine-grained actions. Treating graphs as plain sequences and not exploiting permutation invariance and graph-difference makes prediction harder.",
            "memory_limitations": "Does not exploit graph-difference simplification or SOS permutation invariance, resulting in worse performance and higher difficulty when the per-step target size is larger.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2723.4",
            "source_info": {
                "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "CALM",
            "name_full": "CALM (GPT-2 tuned on ClubFloyd action traces)",
            "brief_description": "A GPT-2-based model fine-tuned on a human-created dataset (ClubFloyd) of observation-action pairs for valid action generation; used as an external-data tuned baseline for action prediction.",
            "citation_title": "Keep CALM and explore: Language models for action generation in text-based games.",
            "mention_or_use": "use",
            "agent_name": "CALM",
            "agent_description": "GPT-2 style generative language model (fine-tuned on ClubFloyd corpus of human text-game transcripts) used to generate actions conditioned on observations (and prior action/next observation in its training setup).",
            "base_model_size": null,
            "game_benchmark_name": "JerichoWorld (valid action prediction evaluated across test games)",
            "game_description": "Action-generation task: given state observation and context, predict the set of valid natural-language actions that will change the world-state.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": "Not applicable (no structured memory). Reported valid-action performance: overall EM 13.79% and F1 19.11% (Table 1).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": null,
            "memory_limitations": "CALM shows high variance across games: performs better on some games whose genres match the ClubFloyd training data; not explicitly using structured graph memory may limit ability to predict fine-grained actions that depend on latent world state.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2723.5",
            "source_info": {
                "paper_title": "Learning Knowledge Graph-based World Models of Textual Environments",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games.",
            "rating": 2,
            "sanitized_title": "learning_dynamic_belief_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "How to avoid being eaten by a grue: Structured exploration strategies for textual worlds.",
            "rating": 2,
            "sanitized_title": "how_to_avoid_being_eaten_by_a_grue_structured_exploration_strategies_for_textual_worlds"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces.",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games.",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Modeling worlds in text.",
            "rating": 2,
            "sanitized_title": "modeling_worlds_in_text"
        }
    ],
    "cost": 0.018737,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Knowledge Graph-based World Models of Textual Environments</h1>
<p>Prithviraj Ammanabrolu<br>School of Interactive Computing<br>Georgia Institute of Technology<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>School of Interactive Computing<br>Georgia Institute of Technology<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.</p>
<h2>1 Introduction</h2>
<p>World models, often in the form of probabilistic generative models, are used in conjunction with model-based reinforcement learning to improve a learning agent's ability to operate in various environments [33, 7]. They are inspired by human cognitive processes [15], with a key hypothesis being that the ability to predict how the world will change in response to one's actions will help you better plan what actions to take [12]. Evidence towards this hypothesis comes in the form of studies showing that simulating trajectories using internal learned models of the world improves sample efficiency in learning to operate in an environment [12, 30].
Text-based games, in which players perceive and interact with the world entirely through textual natural language, are environments that provide new challenges for world model approaches. Textbased games are structured as long puzzles or quests that can only be solved by navigating and interacting with potentially hundreds of locations, characters, and objects. The puzzle-like structures to the games are exacerbated by two factors. First, these environments are partially observable, i.e. observations of the world are incomplete. Second, the agent faces a combinatorially-sized action space of the order of $\mathcal{O}\left(10^{14}\right)$ possible actions at every step. For these reasons model-free reinforcement learning in text-based games is extremely data inefficient.
Prior work on text-based game playing agents repeatedly demonstrated that providing agents with structured memory in the form of knowledge graphs (sets of $\langle s, r, o\rangle$ tuples such that $s$ is a subject, $r$ is a relation, and $o$ is an object) is critical in enabling them to operate in and model these worlds [3, $21,1,2]$ —aiding in both the challenges mentioned. These works all rely on extracting information</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two subsequent states in Zork1 consisting of: textual observations, world knowledge graphs, valid actions, and actions taken.
about one's surroundings while navigating novel environments, either through rules [3, 2], questionanswering [5], or transformer-based extraction [1, 21]. This lifted representation helps agents remember aspects of the world that become unobservable as the agent navigates the environment. However, we hypothesize that agents that rely on lifted representations of the world will benefit from more than just memorization but the ability to predict how the graph state representation will change. For example, by inferring that a locked chest is likely to contain treasure before it is actually revealed provides an agent with a form of look-ahead that will potentially enable it to bias its actions towards opening such a chest. We introduce an approach to this knowledge representation problem that effectively simplifies it by exploiting the inherent structure of knowledge graphs-framing it to be the task of inferring the difference in knowledge graphs between subsequent states given an action.
A consequence of the combinatorially-sized action space in text-based games is that that the set of contextually relevant actions-i.e. those that are most likely to affect change in the environment-are overwhelmed by the irrelevant actions. For example, it is not illegal to try to climb a tree when there are no trees present, and the game engine will just respond with feedback that nothing happens. An aspect of world modeling that has not been considered for other games is inferring which actions are valid in a particular context. We hypothesize that both the challenges mentioned are closely linked and present world models that multi-task learn to tackle both simultaneously-i.e. to answer the questions of "What actions can I perform?" and "How will the world change if I perform a particular action?".</p>
<p>Our work has four core contributions. (1) We first show how changes in the world can be represented in the form of differences between subsequent knowledge graph state representations. (2) We present the Worldformer, a novel multi-task transformer based architecture that learns to simultaneously generate both the set of graph differences and the set of contextually relevant actions. (3) We introduce a loss function and a training methodology that enable more effective training of the Worldformer by exploiting the fact that knowledge graphs and natural language valid actions can be represented permutation invariant Sets of Sequences-wherein the ordering of tokens within an item in the set matters but the set itself lacks ordering. (4) A zero-shot ablation study conducted on diverse set of never-before-seen text games shows the significance of each of the prior three contributions in outperforming strong existing baselines.</p>
<h1>2 Related Work</h1>
<p>We will focus on three main areas of related work: world modeling and model-based reinforcement learning, and world modeling and knowledge graphs in text games, and general (knowledge) graph construction techniques.
World modeling via model-based reinforcement learning often serves to learn transition models of an environment to allow for simulation without actually interacting with the environment [7]. Ha and Schmidhuber [12] use Variational Autoencoders (VAEs) combined with recurrent neural networks to learn compressed state representations over time of visual reinforcement learning environments [8]. This model is then used to simulate an environment and learn a control policy in it. Other contemporary works attempt to also learn dynamics models using raw pixels in the</p>
<p>context of games such as Atari [26, 16], and Super Mario Bros. [11] as well as 3D simulations [16] and robotics [37, 35]. We note that in all of these works, in addition to the state space being raw pixels-the action space is fixed and orders of magnitude smaller than in text games.</p>
<p>In textual environments, the traditional state representations of choice have been raw text encodings via recurrent neural networks [24, 14, 13] but have since shifted towards transformer [34] and knowledge graph-based representations [3, 1]. Knowledge graphs have been shown to be aid in the challenges of: (1) knowledge representation [3, 1], enabling neuro-symbolic reasoning approaches over graph-based state representations [29]; (2) combinatorial state-action spaces [2, 1]; and (3) incorporating external knowledge sources for commonsense reasoning [3, 21, 22, 9]. Two of these works are perhaps closest in spirit to ours. Yao et al. [39] train a GPT-2 model [27] to decode valid actions based on human text game transcripts found online, showing that improved valid action generation ability results in better control policies. Ammanabrolu et al. [5] frame knowledge graph construction in text games as a question-answering problem where agents ask questions to identify common objects in the world and their attributes, showing that improved knowledge graph quality results in better control policies. We note that of these works handles only one or another of the sub-tasks required for world modeling in these environments.</p>
<p>A core aim in the field of graph representation learning is representing graphs as continuous vectors while maintaining their inherent structure [17, 38]. Approaches to this task when applied to knowledge graphs, attempt to exploit the inherent structure of knowledge graphs to create more accurate continuous vector graph representations in the form of embeddings [36]. Building on these representation learning works is the task of automated knowledge base construction attempts to create links in knowledge graphs given text [25]. Li et al. [19] approach the graph generation problem as a sequential process, first learning a generative model of the graph and then iteratively adding nodes and edges to it using the learned model-this work does not consider cases where the graphs are conditioned on text, however. These works focus solely on graph construction and do not include the inherent interactive action-based components featured in world modeling and text games.</p>
<h1>3 Background</h1>
<p>Dataset. We use the JerichoWorld Dataset [4]. ${ }^{1}$ It contains 24,198 mappings between rich natural language observations and: (1) knowledge graphs in the form of a set of tuples $\langle s, r, o\rangle$ (such that $s$ is a subject, $r$ is a relation, and $o$ is an object) that reflect the world state in the form of a map; (2) a set of natural language actions that are guaranteed to cause a change in that particular world state. An example of the mapping between rich natural language observations and structured knowledge is illustrated in Figure 1. The training data is collected across 27 text games in multiple genres and contains a further 7,836 heldout instances over 9 additional games in the test set.</p>
<p>Each instance of the dataset takes the form of a tuple of the form $\left\langle S_{t}, A, S_{t+1}, R\right\rangle$ where $S_{t}$ and $S_{t+1}$ are two subsequent states with $A$ being the action used to transition between states and $R$ the observed reward. As mentioned earlier, each of the states in the tuple contain information regarding the observation $O_{t} \in S_{t}$, ground truth knowledge graph $G_{t} \in S_{t}$, and valid actions for that state $V_{t} \in S_{t}$. This data was collected by oracle agents, i.e. agents that can perfectly solve a game, exploring using a mix of an oracle and random policy to ensure high coverage of a game's state space. A full sample is found in Appendix A.1.</p>
<p>Tasks. Given this dataset, we focus on two tasks within it as formally defined by JerichoWorld. As mentioned in Section 1, a successful world model will be able to accomplish both of these tasks.</p>
<ol>
<li>Knowledge Graph Generation: this task involves predicting the graph at time step $t+1$ : $G_{t+1} \in S_{t+1}$ given the textual observations, valid actions, and graph at time step $t$ : $O_{t}, V_{t}, G_{t} \in S_{t}$, and action $A$ for all samples in the dataset.</li>
<li>Valid Action Generation: this task is formally defined as predicting the set of sequences of valid actions at time step $t+1: V_{t+1} \in S_{t+1}$ given the textual observations, valid actions, and graph at time step $t: O_{t}, V_{t}, G_{t} \in S_{t}$, and action $A$ for all samples in the dataset.
<sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The transformation between subsequent world knowledge graphs $G_{t}$ and $G_{t+1}$ based on the states in Figure 1. The green (Gr) outlined portions in the center are additions to $G_{t}$ to get $G_{t+1}$ (i.e. $G_{t+1}-G_{t}$ ) and the red (R) portions similarly represent deletions to $G_{t}$ (i.e. $G_{t}-G_{t+1}$ ).</p>
<h1>4 The Worldformer</h1>
<p>This section describes the core methodological contributions of our work in creating world models for text games. We first show how knowledge graph generation can be simplified to predicting the graph difference between agent steps. We then describe the Worldformer, a transformer-based architecture, and end-to-end training method—including an objective function-that treats both of the world modeling tasks as a Set of Sequences generation problem.</p>
<h3>4.1 Knowledge Graph Difference Generation</h3>
<p>Figure 2 describes the gist of our simplification of the knowledge graph generation problem. Recall that knowledge graphs are directed graphs that are stored the form of a set of tuples as $\langle s, r, o\rangle$ such that $s$ is a subject, $r$ is a relation, and $o$ is an object. Let the knowledge graphs representing the world state at two subsequent steps be $G_{t}$ and $G_{t+1}$. At every step, tuples are either added or deleted from the graph $G_{t}$ to update the belief state about the world and turn it into graph $G_{t+1}$. Using this observation, we can simplify the knowledge graph generation problem. Instead of predicting $G_{t+1}$ given $G_{t}$ and prior context, we can instead predict the differences between the two graphs.
In Figure 2, between steps $t$ and $t+1$, we see that $G_{t+1}-G_{t}$ is the set of tuples that are added to $G_{t}$ and $G_{t}-G_{t+1}$ the set of tuples are are deleted from $G_{t}$. Together they make up the graph differences. Here, we make a second key observation that allows for yet further simplification of the problem. This observation is based on generally applicable properties of such worlds: (1) locations are fixed and unique, i.e. the positions of locations with respect to each other does not change; (2) objects and characters can only be in one location at a time; and (3) contradicting object attributes can be identified using a lexical dictionary such as WordNet [20], e.g. an object cannot be both open and closed at the same time. These properties let us uniquely identify the triples to be deleted from the graph $G_{t}-G_{t+1}$ given triples to be added to the graph $G_{t+1}-G_{t}$. Additional implementation details can be found in Appendix A.2.
Taken together, the Knowledge Graph Generation task can be cast as follows: predict the nodes to be added to the graph $G_{t}$ at time step $t: G_{t+1}-G_{t}$ (a much smaller set than $G_{t+1}$ by itself) to transform it into graph $G_{t+1}$ given the textual observations, valid actions, and graph at time step $t$ : $O_{t}, V_{t}, G_{t} \in S_{t}$, and action $A$ for all samples in the dataset.</p>
<h3>4.2 Multi-task Architecture</h3>
<p>The Worldformer is a multi-task world model that simultaneously learns to perform both knowledge graph and valid action generation. It is built on the hypothesis that each of these tasks contains information crucial to the other-the valid actions that can be executed at any timestep are entirely dependent on the current state and vice versa the state knowledge graph updates on the basis of the previously executed action.
Figure 3 describes the architecture of the Worldformer. The inputs to the architecture are textual observations, valid actions, and graph at time step $t: O_{t}, V_{t}, G_{t} \in S_{t} . O_{t}$ and $V_{t}$ are encoded through a bidirectional text encoder into $\mathbf{O}<em _mathbf_t="\mathbf{t">{\mathbf{t}}$. In our work, we used an architecture similar to BERT [10] with the original pre-trained weights that are then fine-tuned using a masked language model (MLM) loss on observations taken from the training data. $\mathbf{O}</em>$ is the output of the final hidden layer. The}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The Worldformer architecture. The text encoder (B) and graph encoder (R) have similar architecture but different pre-training strategies. Both the decoders are not pre-trained and have identical architectures. Solid black lines indicate gradient flow.</p>
<p>Graph encoder receives $G_t$ and encodes it into $\mathbf{G_t}$. It is also similar to BERT, but is pre-trained on knowledge graphs found in the training data using a MLM loss with a phrase-level masking scheme where whole components of a $\langle s, r, o\rangle$ graph triple (individual underlined portions in Figure 3) are masked at once. Again, $\mathbf{G_t}$ is the output of the final hidden layer.</p>
<p>$\mathbf{O_t}$ and $\mathbf{G_t}$ are passed into a representation aggregator which then sends the combined encoded state representation $\mathbf{S_t}$ to one of two autoregressive decoders that have the same general internal architecture as GPT-2 [27]. During training, the first decoder is conditioned on $\mathbf{S_t}$ directly and $\mathbf{O_t}$ through cross-attention and takes in the valid actions of the next state $V_{t+1}$ as input, learning to predict the same input sequence shifted to the right as sequence-to-sequence models do. Similarly, the second decoder is conditioned on $\mathbf{S_t}$ directly and $\mathbf{G_t}$ through cross-attention and takes in the knowledge graph of the next state $G_{t+1}$ as input.</p>
<h3>4.3 Set of Sequences Generation and Training</h3>
<p>We observe that both the knowledge graph difference $G_{t+1}-G_t$ and the valid actions $V_{t+1}$ are both Sets of Sequences where the ordering of the sequence of tokens within an action or a graph triple matters but the ordering of all the actions and triples does not. Standard autoregressive decoding used in sequence-to-sequence (Seq2Seq) models [32] does not account for such permutation invariance. We frame the graph and action prediction tasks as a generation of a Set of Sequences (SOS) problem—expanding on the simple set prediction problem definition proposed by works such as Deep Sets [40] to account for the specific structure of Sets of Sequences. This problem structure is used to then formulate a training methodology that lets autoregressive decoders better account for the SOS structure.</p>
<p>For both of the decoders in Figure 3, we are given a target sequence $Y = {y_1, \ldots, y_M}$ and some input context via the encoders $X$. Standard autoregressive techniques factor the distribution over the target sequence into a chain of conditional probabilities with a causal left to right structure.</p>
<p>$$P(Y|X; \theta) = \prod_{i=1}^{M+1} p(y_{i}|y_{0:i-1},X; \theta) \tag{1}$$</p>
<p>Where $\theta$ represents the overall network parameters. This can then be used to formulate a maximum likelihood training loss with cross-entropy at every step.</p>
<p>$$\mathcal{L}<em i="1">{seq} = \log P(Y|X; \theta) = \sum</em>$$}^{M+1} \log p(y_{i}|y_{0:i-1},X; \theta) \tag{2</p>
<p>In our setting, we can group elements in $Y$ into its set of sequences form</p>
<p>$$
\begin{gathered}
Y_{\text {sos }}=\left{y_{1}^{\prime} \ldots y_{M^{\prime}}^{\prime}\right}, y_{i}^{\prime} \in V_{t+1} \text { or } y_{i}^{\prime} \in G_{t+1}-G_{t}, M^{\prime} \leq M \
\text { where } y_{i}^{\prime}=\left{y_{k} \ldots y_{k+l}\right}, \sum_{j} \operatorname{len}\left(y_{j}^{\prime}\right)=M
\end{gathered}
$$</p>
<p>Via the decoders, we seek to learn a transformation from $\mathbf{S}<em _sos="{sos" _text="\text">{\mathbf{t}} \in \mathbb{R}^{d}$ (the input $d$-dimensional state representation vector) and $Y</em>$.
Combining this definition of permutation invariant functions with Eq. 2, 3, we can factorize the distribution over the output Set of Sequences as the following chain of probabilities:}} \in \mathcal{Y}$ (decoder inputs in the space of all possible decoder inputs $\mathcal{Y}$ ) that map to the permutation invariant target set of sequences $Y_{\text {sos }}$. This function can then be defined as $f: \mathbb{R}^{d} \cup 2^{\mathcal{Y}} \rightarrow 2^{\mathcal{Y}}$ as the permutation invariance of part of the domain and range of this function makes it the power set of $\mathcal{Y</p>
<p>$$
\begin{aligned}
P\left(Y_{\text {sos }} \mid X ; \theta\right) &amp; =\prod_{i=1}^{M+1} p\left(y_{i}^{\prime} \mid X ; \theta\right) \
p\left(y_{i}^{\prime} \mid X ; \theta\right) &amp; =\prod_{k=l}^{l+n} p\left(y_{k} \mid y_{l: k-1}, X ; \theta\right) \
&amp; \text { where } l=\sum_{j&lt;i} \operatorname{len}\left(y_{j}^{\prime}\right), n=\operatorname{len}\left(y_{i}^{\prime}\right)
\end{aligned}
$$</p>
<p>With the key intuition here being that Eq. 4a factorizes the distribution such that each element of $Y_{\text {sos }}$ is independent of other elements in the set, but tokens of an element $y_{i}^{\prime}$ in the set are conditioned on preceding tokens within the element (Eq. 4b).
This in turn gives us a maximum likelihood Set of Sequences loss that can be used to train a model to output a Set of Sequences.</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _sos="{sos" _text="\text">{\text {sos }}=\log P\left(Y</em> \mid X ; \theta\right) \
&amp; =\sum_{i=1}^{M+1} \sum_{k=l}^{l+n} \log p\left(y_{k} \mid y_{l: k-1}, X ; \theta\right) \
&amp; \text { where } l=\sum_{j&lt;i} \operatorname{len}\left(y_{j}^{\prime}\right), n=\operatorname{len}\left(y_{i}^{\prime}\right)
\end{aligned}
$$}} \mid X ; \theta\right) &amp; =\sum_{i=1}^{M+1} \log p\left(y_{i}^{\prime</p>
<p>In our formulation, we have observation sequences at timestep $t: O_{t}, V_{t}$ encoded into $\mathbf{O}<em t="t">{\mathbf{t}}$, graph $G</em>}$ encoded into $\mathbf{G<em _mathbf_t="\mathbf{t">{\mathbf{t}}$, and all of them combined into $\mathbf{S}</em>$. Across the two decoders, this gives us a combined loss:}}$, with the output Sets of Sequences at timestep $t+1$ being the graph difference $G_{t+1}-G_{t}$ and valid actions $V_{t+1</p>
<p>$$
\mathcal{L}<em t_1="t+1">{\text {world }}=\log P\left(G</em>}-G_{t} \mid \mathbf{S<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{G}</em>}} ; \theta\right)+\log P\left(V_{t+1} \mid \mathbf{S<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{O}</em> ; \theta\right)
$$}</p>
<p>This loss is used to multi-task train the Worldformer simultaneously across the two tasks.</p>
<h1>5 Evaluation</h1>
<p>We evaluate the Worldformer by comparing it on both of the tasks across 9 never-before-seen testing games against strong baselines. We further present ablation studies in each task to determine the relative importance of each of the techniques presented in the previous section.
Metrics. Across both the tasks, we use the same metrics as defined in JerichoWorld [4]. For knowledge graph generation, we report two types of metrics (Exact Match or EM and F1) operating on two different levels-at a graph tuple level and another at a token level. The graph level metrics are based on matching the set of $\langle$ subject, relation, object $\rangle$ triples within the graph, all three tokens in a particular triple must match a triple within the ground truth graph to count as a true positive. The</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Expt.</th>
<th style="text-align: center;">Met- <br> rics</th>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">zork1</th>
<th style="text-align: center;">lib.</th>
<th style="text-align: center;">det.</th>
<th style="text-align: center;">bal.</th>
<th style="text-align: center;">pent.</th>
<th style="text-align: center;">ztun</th>
<th style="text-align: center;">ludi.</th>
<th style="text-align: center;">deep.</th>
<th style="text-align: center;">temp.</th>
<th style="text-align: center;">overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">886</td>
<td style="text-align: center;">654</td>
<td style="text-align: center;">434</td>
<td style="text-align: center;">990</td>
<td style="text-align: center;">276</td>
<td style="text-align: center;">462</td>
<td style="text-align: center;">2210</td>
<td style="text-align: center;">630</td>
<td style="text-align: center;">1294</td>
<td style="text-align: center;">7836</td>
</tr>
<tr>
<td style="text-align: center;">Knowledge Graph Prediction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rules</td>
<td style="text-align: center;">Gr.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">3.72</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">1.39</td>
<td style="text-align: center;">9.17</td>
<td style="text-align: center;">6.44</td>
<td style="text-align: center;">4.94</td>
<td style="text-align: center;">5.10</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">4.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">12.87</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">11.90</td>
<td style="text-align: center;">10.22</td>
<td style="text-align: center;">10.06</td>
<td style="text-align: center;">8.37</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">3.36</td>
<td style="text-align: center;">7.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tok.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">6.08</td>
<td style="text-align: center;">10.33</td>
<td style="text-align: center;">7.51</td>
<td style="text-align: center;">32.53</td>
<td style="text-align: center;">16.48</td>
<td style="text-align: center;">14.40</td>
<td style="text-align: center;">14.47</td>
<td style="text-align: center;">3.34</td>
<td style="text-align: center;">7.42</td>
<td style="text-align: center;">13.08</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">8.42</td>
<td style="text-align: center;">26.74</td>
<td style="text-align: center;">10.23</td>
<td style="text-align: center;">36.09</td>
<td style="text-align: center;">23.36</td>
<td style="text-align: center;">21.74</td>
<td style="text-align: center;">18.48</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">9.44</td>
<td style="text-align: center;">17.50</td>
</tr>
<tr>
<td style="text-align: center;">Q*BERT</td>
<td style="text-align: center;">Gr.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">24.56</td>
<td style="text-align: center;">29.14</td>
<td style="text-align: center;">34.45</td>
<td style="text-align: center;">41.22</td>
<td style="text-align: center;">28.96</td>
<td style="text-align: center;">22.17</td>
<td style="text-align: center;">41.44</td>
<td style="text-align: center;">4.42</td>
<td style="text-align: center;">36.84</td>
<td style="text-align: center;">32.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">24.88</td>
<td style="text-align: center;">31.46</td>
<td style="text-align: center;">36.23</td>
<td style="text-align: center;">41.85</td>
<td style="text-align: center;">30.12</td>
<td style="text-align: center;">26.26</td>
<td style="text-align: center;">46.74</td>
<td style="text-align: center;">4.66</td>
<td style="text-align: center;">39.86</td>
<td style="text-align: center;">35.48</td>
</tr>
<tr>
<td style="text-align: center;">(Question <br> Answering)</td>
<td style="text-align: center;">Tok.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">43.93</td>
<td style="text-align: center;">49.78</td>
<td style="text-align: center;">60.28</td>
<td style="text-align: center;">85.81</td>
<td style="text-align: center;">65.02</td>
<td style="text-align: center;">49.44</td>
<td style="text-align: center;">57.58</td>
<td style="text-align: center;">9.31</td>
<td style="text-align: center;">48.98</td>
<td style="text-align: center;">53.58 $55.74^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">48.31</td>
<td style="text-align: center;">52.76</td>
<td style="text-align: center;">63.21</td>
<td style="text-align: center;">86.18</td>
<td style="text-align: center;">69.54</td>
<td style="text-align: center;">49.82</td>
<td style="text-align: center;">60.95</td>
<td style="text-align: center;">9.84</td>
<td style="text-align: center;">49.17</td>
<td style="text-align: center;">55.74 $55.74^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">Seq2Seq</td>
<td style="text-align: center;">Gr.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">12.44</td>
<td style="text-align: center;">18.42</td>
<td style="text-align: center;">26.86</td>
<td style="text-align: center;">8.19</td>
<td style="text-align: center;">22.18</td>
<td style="text-align: center;">16.89</td>
<td style="text-align: center;">12.94</td>
<td style="text-align: center;">8.38</td>
<td style="text-align: center;">16.48</td>
<td style="text-align: center;">14.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">12.96</td>
<td style="text-align: center;">18.89</td>
<td style="text-align: center;">29.48</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">23.54</td>
<td style="text-align: center;">16.89</td>
<td style="text-align: center;">14.18</td>
<td style="text-align: center;">10.47</td>
<td style="text-align: center;">18.52</td>
<td style="text-align: center;">15.54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tok.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">18.01</td>
<td style="text-align: center;">20.26</td>
<td style="text-align: center;">35.86</td>
<td style="text-align: center;">17.60</td>
<td style="text-align: center;">25.48</td>
<td style="text-align: center;">17.19</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">13.25</td>
<td style="text-align: center;">22.48</td>
<td style="text-align: center;">18.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">21.12</td>
<td style="text-align: center;">20.84</td>
<td style="text-align: center;">35.86</td>
<td style="text-align: center;">18.86</td>
<td style="text-align: center;">27.72</td>
<td style="text-align: center;">17.87</td>
<td style="text-align: center;">15.42</td>
<td style="text-align: center;">13.25</td>
<td style="text-align: center;">24.34</td>
<td style="text-align: center;">19.96</td>
</tr>
<tr>
<td style="text-align: center;">GATA-W</td>
<td style="text-align: center;">Gr.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">22.30</td>
<td style="text-align: center;">24.72</td>
<td style="text-align: center;">21.72</td>
<td style="text-align: center;">23.68</td>
<td style="text-align: center;">22.81</td>
<td style="text-align: center;">27.00</td>
<td style="text-align: center;">24.55</td>
<td style="text-align: center;">23.76</td>
<td style="text-align: center;">24.52</td>
<td style="text-align: center;">24.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">25.34</td>
<td style="text-align: center;">26.47</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">26.54</td>
<td style="text-align: center;">27.63</td>
<td style="text-align: center;">27.00</td>
<td style="text-align: center;">24.55</td>
<td style="text-align: center;">23.76</td>
<td style="text-align: center;">24.92</td>
<td style="text-align: center;">25.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tok.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">33.09</td>
<td style="text-align: center;">33.88</td>
<td style="text-align: center;">25.64</td>
<td style="text-align: center;">34.64</td>
<td style="text-align: center;">37.71</td>
<td style="text-align: center;">35.81</td>
<td style="text-align: center;">35.94</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">40.89</td>
<td style="text-align: center;">35.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">33.93</td>
<td style="text-align: center;">34.86</td>
<td style="text-align: center;">25.80</td>
<td style="text-align: center;">38.68</td>
<td style="text-align: center;">39.59</td>
<td style="text-align: center;">38.88</td>
<td style="text-align: center;">37.16</td>
<td style="text-align: center;">32.48</td>
<td style="text-align: center;">43.97</td>
<td style="text-align: center;">37.10</td>
</tr>
<tr>
<td style="text-align: center;">Worldformer</td>
<td style="text-align: center;">Gr.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">21.62</td>
<td style="text-align: center;">34.39</td>
<td style="text-align: center;">41.05</td>
<td style="text-align: center;">50.41</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">41.56</td>
<td style="text-align: center;">40.10</td>
<td style="text-align: center;">41.87</td>
<td style="text-align: center;">42.43</td>
<td style="text-align: center;">39.15*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">24.44</td>
<td style="text-align: center;">34.39</td>
<td style="text-align: center;">44.53</td>
<td style="text-align: center;">52.43</td>
<td style="text-align: center;">34.30</td>
<td style="text-align: center;">42.20</td>
<td style="text-align: center;">41.65</td>
<td style="text-align: center;">42.74</td>
<td style="text-align: center;">45.17</td>
<td style="text-align: center;">41.06*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tok.</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">42.88</td>
<td style="text-align: center;">41.98</td>
<td style="text-align: center;">54.39</td>
<td style="text-align: center;">62.22</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">50.80</td>
<td style="text-align: center;">51.29</td>
<td style="text-align: center;">50.04</td>
<td style="text-align: center;">53.81</td>
<td style="text-align: center;">51.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">48.12</td>
<td style="text-align: center;">41.98</td>
<td style="text-align: center;">59.13</td>
<td style="text-align: center;">62.22</td>
<td style="text-align: center;">49.00</td>
<td style="text-align: center;">52.24</td>
<td style="text-align: center;">51.29</td>
<td style="text-align: center;">50.04</td>
<td style="text-align: center;">54.96</td>
<td style="text-align: center;">52.45</td>
</tr>
<tr>
<td style="text-align: center;">Valid Action Prediction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Seq2Seq</td>
<td style="text-align: center;">Act</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">16.65</td>
<td style="text-align: center;">15.13</td>
<td style="text-align: center;">18.19</td>
<td style="text-align: center;">18.19</td>
<td style="text-align: center;">23.39</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">20.10</td>
<td style="text-align: center;">14.71</td>
<td style="text-align: center;">20.34</td>
<td style="text-align: center;">18.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">17.85</td>
<td style="text-align: center;">16.88</td>
<td style="text-align: center;">21.12</td>
<td style="text-align: center;">18.23</td>
<td style="text-align: center;">25.87</td>
<td style="text-align: center;">15.13</td>
<td style="text-align: center;">20.86</td>
<td style="text-align: center;">14.86</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">19.44</td>
</tr>
<tr>
<td style="text-align: center;">CALM</td>
<td style="text-align: center;">Act</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">18.67</td>
<td style="text-align: center;">11.18</td>
<td style="text-align: center;">17.37</td>
<td style="text-align: center;">10.04</td>
<td style="text-align: center;">13.77</td>
<td style="text-align: center;">11.29</td>
<td style="text-align: center;">15.49</td>
<td style="text-align: center;">10.31</td>
<td style="text-align: center;">13.13</td>
<td style="text-align: center;">13.79</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">18.90</td>
<td style="text-align: center;">25.49</td>
<td style="text-align: center;">34.42</td>
<td style="text-align: center;">12.16</td>
<td style="text-align: center;">34.40</td>
<td style="text-align: center;">9.95</td>
<td style="text-align: center;">20.94</td>
<td style="text-align: center;">7.84</td>
<td style="text-align: center;">18.57</td>
<td style="text-align: center;">19.11</td>
</tr>
<tr>
<td style="text-align: center;">Worldformer</td>
<td style="text-align: center;">Act</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">23.08</td>
<td style="text-align: center;">22.55</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">29.08</td>
<td style="text-align: center;">27.05</td>
<td style="text-align: center;">20.71</td>
<td style="text-align: center;">21.36</td>
<td style="text-align: center;">24.04</td>
<td style="text-align: center;">22.80</td>
<td style="text-align: center;">23.22*</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">23.50</td>
<td style="text-align: center;">26.52</td>
<td style="text-align: center;">25.28</td>
<td style="text-align: center;">32.89</td>
<td style="text-align: center;">31.32</td>
<td style="text-align: center;">23.66</td>
<td style="text-align: center;">22.27</td>
<td style="text-align: center;">26.12</td>
<td style="text-align: center;">25.66</td>
<td style="text-align: center;">25.54*</td>
</tr>
</tbody>
</table>
<p>Table 1: Results across both the tasks for the models specified. Overall indicates a size weighted average. All experiments were performed over three random seeds, with standard deviations not exceeding $\pm 3.2$ in any of the overall categories for KG prediction and $\pm 1.2$ for valid action prediction. Bolded results indicate highest overall scores. Asterisk (*) indicates when the top result is significantly higher ( $p&lt;0.05$ with an ANOVA test followed by a post-hoc pair-wise Tukey test) over all alternatives. $\dagger$ indicates this result is not significantly higher than Worldformer.
token level metrics operate on measuring unigram overlap in the graphs, any relations or entities in the predicted tokens that match the ground truth count towards a true positive.</p>
<p>For valid action generation, we adapt the graph level Exact Match (EM) and F1 metrics as described in the previous task to actions. In other words, positive EM or F1 happens only when all tokens in a predicted valid action match one in the gold standard set. In all cases, EM checks for accuracy or direct overlap between the predictions and ground truth, while F1 is a harmonic mean of predicted precision and recall.</p>
<h1>5.1 Knowledge Graph Generation</h1>
<p>We compare the Worldformer to 4 baselines taken from contemporary knowledge graph-based world modeling approaches in text games. All sequence models use a fixed graph vocabulary of size 7002 that contains all unique relations and entities at train and test times. Additional details and hyperparameters for the models are found in Appendix A.2.
Rules. Following Ammanabrolu and Hausknecht [2], we extract graph information from the observation using information extraction tools such as OpenIE [6] in addition to some hand-authored rules to account for the irregularities of text games.
Question-Answering. (QA) This baseline comes from the Q*BERT agent described in [5]. It is trained on both the SQuAD 2.0 [28] the Jericho-QA text game question answering dataset [5] on the same set of training games as found in Worldformer. It uses the ALBERT [18] variant of the BERT [10] natural language transformer to answer questions and populate the knowledge graph via a few hand-authored rules from the answers. Examples of questions asked include: "What is my current location?", "What objects are around me?".
Seq2Seq. This single-task model is provided as a baseline by JerichoWorld and performs sequence learning by encoding the observation and graph with a single bidirectional BERT-based encoder</p>
<p>and using an autoregressive GPT-2-based decoder to decode the next graph. It is trained using the standard Seq2Seq cross-entropy loss (Eq. 2).</p>
<p>GATA-World. We adapt the Graph-Aided Transformer Agent [1] to our task. It consists of the same encoder structure as the Worldformer but contains one decoder that performs single-task Seq2Seq learning to decode both the set of tuples that must be added as well as deleted from the graph in the form of: $\langle$ add, node1, node2, relation $\rangle$ or $\langle$ del, node1, node2, relation $\rangle$. This is equivalent to predicting $\left(G_{t+1}-G_{t}\right) \cup\left(G_{t}-G_{t+1}\right)$. It is trained with the Seq2Seq cross-entropy loss (Eq. 2).</p>
<p>Table 1 describes the results in this task over all the games. We see that on the graph level metrics, the Worldformer performs significantly better than all other other baselines. On the token level metrics, the Worldformer and QA method are comparable-the difference between these two methods are statistically non-significant $(p=0.18)$ with each other but both significantly $(p&lt;0.05)$ higher than all others. The QA method, and other extractive methods, highlight portions of the input observation that form the graph and are particularly well suited for the token level metrics. The JerichoWorld developers note that these approaches are prone to over-extraction, i.e. extracting more text than is strictly relevant from the input observation aiding token level overlap but resulting in a sharp drop in terms of the graph level metrics [4]. Additional failure modes of such extraction based approaches occur when the text descriptions are incomplete or hidden-e.g. the contents of a chest are revealed through the textual observation only when it is opened by a player. The Worldformer is able to make a informed guess as to the contents of the chest due to its training, providing a form of look ahead that the Rules and QA systems cannot.</p>
<p>Table 3 present the results of an ablation study testing the relative importance of the three main components of the Worldformer: graph difference prediction, multi-task training, and the SOS loss. We note that a model without any of these components is equivalent to the Seq2Seq approach described previously. We see significant drops in performance, particularly on the graph level metrics, when any single one of these components are removed. This indicates that all three components are necessary for the Worldformer to achieve state-of-the-art performance.</p>
<p>In particular, we note that the largest performance drop was when Worldformer did not use the graph difference simplification. In this case, the KG prediction task is simplified to predicting only; the length of the set of sequences $G_{t+1}-G_{t}$ is much smaller than $G_{t+1}$. There are on average 3.42 triples or 10.42 tokens per state across the JerichoWorld test dataset for $G_{t+1}-G_{t}$ but a mean of 8.71 triples or 26.13 tokens per state for $G_{t+1}$. This also explains the increased performance of the GATA-W over the baseline Seq2Seq agent-this agent only needs to predict on average 5.04 rules or 20.16 tokens across the testing games. Predicting a smaller number of triples and tokens per state makes the problem relatively more tractable for world modeling agents.</p>
<h3>5.2 Valid Action Generation</h3>
<p>Similarly to the other task, we compare the Worldformer to an existing baseline for valid action prediction. All models use a fixed vocabulary of size 11,056 at train and test times.</p>
<p>Seq2Seq. This single-task model is provided as a baseline by JerichoWorld and is identical to the Seq2Seq model described in the previous task but is single-task trained to predict valid actions.</p>
<p>CALM. A complementary dataset of observation-action pairs created by humans on the ClubFloyd online Interactive Narrative forum ${ }^{2}$ appears in both Ammanabrolu and Hausknecht [2] and Yao et al. [39] with the latter using it to tune a GPT-2 model for valid action prediction using a GPT-2 based Seq2Seq valid action model dubbed CALM. ${ }^{3}$ This model takes in $O_{t}, A, O_{t+1}$ and targets $V_{t+1}$.</p>
<p>In Table 1, we see that the Worldformer significantly outperforms the Seq2Seq baseline on all the games and CALM overall. Each valid action in a text game requires at most 5 tokens. This combined with an average of 10.30 valid actions per test state means that for every state we would need to generate about 52 tokens. Yet further, the vocabulary size for actions is 11,056 , larger than the graph</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>vocabulary of 7,002 . This increase in task difficulty explains the relative decrease in the magnitude of performance metrics between KG and valid action prediction tasks. Both the Seq2Seq model and CALM-which is trained on a different dataset-are comparable on F1 scores but Seq2Seq is better overall for exact matches. CALM also has relatively higher variance in performance across the test games than the other two methods-e.g. on some games such as zork1 and detective it outperforms the Seq2Seq and is not too far off the Worldformer especially in terms of F1 score. This would appear to indicate that the Club Floyd dataset of text game transcripts that CALM was trained on is better suited for transfer to certain games than others due to differences in training set genre similarities.</p>
<p>Table 3 presents an ablation study that tests the two main components of the Worldformer for this task: multi-task learning, and SOS loss. As with the KG prediction task, we observe significant drops in performance when either of these components are taken away-suggesting that they are relatively critical components. The JerichoWorld developers note that there is a correlation between performance of the baseline Seq2Seq model to the average number of valid actions for the testing game (see Appendix Table 4). They attribute this to label imbalance in the dataset, stating that the model likely learns a common set of actions found across all games such as navigation actions before learning more fine-grained actions. E.g. ztuu, deephome, and balances have a high number of gold standard average valid actions while pentari, ludicorp, detective, and temple which have a low number of average valid actions. While the latter set of games have generally higher performance on both the Seq2Seq and Worldformer models, the gap is significantly less pronounced with the Worldformer. We hypothesize that this is due to the multi-task training of the Worldformer-encoder representations now contain enough information regarding the next knowledge graph to alleviates the label imbalance of the actions and enable prediction of more fine-grained actions.</p>
<h1>6 Conclusions</h1>
<p>We presented the Worldformer, a state-of-the-art world model for text games that: maps worlds by predicting the difference in knowledge graphs between subsequent states, multi-task learns to map a world and act in it simultaneously, and frames all of these tasks as a Set of Sequences generation problem. Its state-of-the-art performance and an ablation study have three potential implications: (1) the simplification of the knowledge representation problem into that of predicting knowledge graph differences between subsequent states is a critical step in making the problem more tractable; (2) performance improvements due to multi-task training imply that acting in and mapping these worlds is inherent a highly correlated problem and benefits from being solved jointly; and (3) the performance boosts due to the SOS loss suggest that accounting for this property of graphs and actions enables more effective training than if we were to treat them as simple sequences.</p>
<h2>7 Broader Impacts</h2>
<p>We view text-games as an platform on which to teach agents how to communicate effectively using natural language, to plan via sequential decision making in situations that may not be anticipated. We seek to enable agents to more efficiently model such worlds, helping them produce more contextually relevant language in such situation. As stated in JerichoWorld-and further verified by us-data is collected from games containing situations of non-normative language usage-describing situations that fictional characters may engage in that are potentially inappropriate, and on occasion impossible, for the real world such as running a troll through with a sword. Instances of such scenarios are mitigated by careful curation of the games that the data is collected by both the authors of Jericho [13] and JerichoWorld [4]. This is based on manual vetting and (existing) crowd-sourced reviews on the popular interactive narrative forum IFDB (https://ifdb.org/).
Broadly speaking, the most relevant downstream task for this work is model-based reinforcement learning. It is applicable to many sequential tasks, some of which cannot be anticipated. World modeling for text environments is more suited for domains in which change in the world is affected via language, which mitigates physical risks-downstream lines of work are not directly relevant to robotics-but not cognitive and emotional risks, as any system capable of generating natural language is capable of accidental or intentional non-normative and biased language use [23, 31].</p>
<h1>References</h1>
<p>[1] A. Adhikari, X. Yuan, M.-A. Côté, M. Zelinka, M.-A. Rondeau, R. Laroche, P. Poupart, J. Tang, A. Trischler, and W. Hamilton. Learning dynamic belief graphs to generalize on text-based games. Advances in Neural Information Processing Systems, 33, 2020.
[2] P. Ammanabrolu and M. Hausknecht. Graph Constrained Reinforcement Learning for Natural Language Action Spaces. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1x6w0EtwH.
[3] P. Ammanabrolu and M. O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, 2019.
[4] P. Ammanabrolu and M. O. Riedl. Modeling worlds in text. OpenReview Preprint, 2021. URL https://openreview.net/forum?id=Y1YtS9MZA75.
[5] P. Ammanabrolu, E. Tien, M. Hausknecht, and M. O. Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. arXiv preprint arXiv:2006.07409, 2020.
[6] G. Angeli, J. Premkumar, M. Jose, and C. D. Manning. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015.
[7] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26-38, 2017. doi: 10.1109/ MSP.2017.2743240.
[8] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.
[9] S. Dambekodi, S. Frazier, P. Ammanabrolu, and M. O. Riedl. Playing text-based games with common sense. arXiv preprint arXiv:2012.02757, 2020.
[10] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.
[11] M. Guzdial, B. Harrison, B. Li, and M. Riedl. Crowdsourcing Open Interactive Narrative. In $F D G, 2015$.
[12] D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https: //proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf.
[13] M. Hausknecht, P. Ammanabrolu, M.-A. Côté, and X. Yuan. Interactive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020. URL https://arxiv.org/abs/1909.05398.
[14] J. He, J. Chen, X. He, J. Gao, L. Li, L. Deng, and M. Ostendorf. Deep reinforcement learning with a natural language action space. In ACL, 2016.
[15] D. Jancke. Orientation formed by a spot's trajectory: a two-dimensional population approach in primary visual cortex. Journal of Neuroscience, 20(14):RC86-RC86, 2000.
[16] T. Kipf, E. van der Pol, and M. Welling. Contrastive learning of structured world models. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=H1gax6VtDB.
[17] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017.
[18] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS.
[19] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.</p>
<p>[20] G. A. Miller. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39-41, 1995.
[21] K. Murugesan, M. Atzeni, P. Shukla, M. Sachan, P. Kapanipathi, and K. Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811, 2020.
[22] K. Murugesan, M. Atzeni, P. Kapanipathi, P. Shukla, S. Kumaravel, G. Tesauro, K. Talamadupula, M. Sachan, and M. Campbell. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence, 2021.
[23] M. S. A. Nahian, S. Frazier, M. Riedl, and B. Harrison. Learning norms from stories: A prior for value aligned agents. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, AIES '20, page 124-130, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450371100. doi: 10.1145/3375627.3375825. URL https://doi.org/10.1145/3375627. 3375825 .
[24] K. Narasimhan, T. D. Kulkarni, and R. Barzilay. Language understanding for text-based games using deep reinforcement learning. In EMNLP, pages 1-11, 2015.
[25] F. Niu, C. Zhang, C. Ré, and J. Shavlik. Elementary: Large-scale knowledge-base construction via machine learning and statistical inference. International Journal on Semantic Web and Information Systems (IJSWIS), 8(3):42-73, 2012.
[26] J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-Conditional Video Prediction using Deep Networks in Atari Games. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, R. Garnett, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2845-2853. Curran Associates, Inc., 2015.
[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language Models are Unsupervised Multitask Learners. 2019.
[28] P. Rajpurkar, R. Jia, and P. Liang. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://www.aclweb.org/anthology/P18-2124.
[29] C. Sautier, D. J. Agravante, and M. Tatsubori. State Prediction in TextWorld with a PredicateLogic Pointer Network Architecture. In In Workshop on Knowledge-based Reinforcment Learning at IJCAI-20, 2020. URL https://kbrl.github.io/papers/08-KBRL.pdf.
[30] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.
[31] E. Sheng, K.-W. Chang, P. Natarajan, and N. Peng. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP), pages 3407-3412. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1339. URL https://www.aclweb. org/anthology/D19-1339.
[32] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems Volume 2, NIPS'14, page 3104-3112, Cambridge, MA, USA, 2014. MIT Press.
[33] R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.
[35] N. Wahlström, T. B. Schön, and M. P. Deisenroth. From pixels to torques: Policy learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.
[36] Q. Wang, Z. Mao, B. Wang, and L. Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724-2743, 2017.</p>
<p>[37] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ a1afc58c6ca9540d057299ec3016d726-Paper.pdf.
[38] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4-24, 2021. doi: 10.1109/TNNLS.2020.2978386.
[39] S. Yao, R. Rao, M. Hausknecht, and K. Narasimhan. Keep CALM and explore: Language models for action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704. URL https://www.aclweb.org/anthology/2020.emnlp-main. 704.
[40] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ f22e4747da1aa27e363d86d40ff442fe-Paper.pdf.</p>
<h1>A Appendix</h1>
<h2>A. 1 Dataset</h2>
<p>An example of a single state from a $\left\langle S_{t}, A, S_{t+1}, R\right\rangle$ tuple taken from the JerichoWorld dataset. This dataset is licensed using the MIT license.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Game</span><span class="o">:</span><span class="w"> </span><span class="n">ztuu</span>
<span class="n">Location</span><span class="o">:</span><span class="w"> </span><span class="n">Cultural</span><span class="w"> </span><span class="n">Complex</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">imposing</span><span class="w"> </span><span class="n">ante</span><span class="o">-</span><span class="n">room</span><span class="o">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">apparently</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">cultural</span><span class="w"> </span><span class="n">center</span>
<span class="w">    </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GUE</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">adorned</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ghastly</span><span class="w"> </span><span class="n">style</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GUE</span><span class="s1">&#39;s &quot;Grotesque Period.&quot; With leering gargoyles,</span>
<span class="s1">    cartoonish friezes depicting long-forgotten scenes of GUE history, and primitive statuary of pointy-</span>
<span class="s1">    headed personages unknown (perhaps very, very distant progenitors of the Flatheads), the place would</span>
<span class="s1">    have been best left undiscovered. North of here, a large hallway passes under the roughly hewn</span>
<span class="s1">    inscription &quot;Convention Center.&quot; To the east, under a fifty-story triumphal arch, a passageway the</span>
<span class="s1">    size of a large city boulevard opens into the Royal Theater. A relatively small and unobtrusive sign</span>
<span class="s1">    (perhaps ten feet high) stands nearby. South, a smaller and more dignified (i.e. post-Dimwit) path</span>
<span class="s1">    leads into what is billed as the &quot;Hall of Science.&quot; You can see a pair of razor-like gloves here.</span>
<span class="s1">Observation: You put on the razor-like gloves.</span>
<span class="s1">Inventory:</span>
<span class="s1">    You are carrying:</span>
<span class="s1">        a brass lantern (providing light)</span>
<span class="s1">        a pair of glasses</span>
<span class="s1">        four candy bars:</span>
<span class="s1">            a ZM$100000</span>
<span class="s1">            a Multi-Implementeers</span>
<span class="s1">            a Forever Gores</span>
<span class="s1">            a Baby Rune</span>
<span class="s1">            a cheaply-made sword</span>
<span class="s1">Prev Act: put on gloves</span>
<span class="s1">Inventory Objects:</span>
<span class="s1">    candy: Which do you mean, the ZM$100000, the Multi Implementeers, the Forever Gores or the Baby Rune?</span>
<span class="s1">    Implementeers: The profiles on the wrapper of this delicacy look more like Moe, Larry, and Curly than</span>
<span class="s1">            those of your favorite Implementeers (presumably, Marc, Mike, and David.)</span>
<span class="s1">    Forever/Gores: The wrapper of this bar pictures the Milky Way, but the stars are all blood red. Kids</span>
<span class="s1">        love them.</span>
<span class="s1">    sword: This is a cheaply made sword of no antiquity whatsoever. With regard to grues or other</span>
<span class="s1">            underworldly denizens, your weapon is as likely to engender laughter as fear.</span>
<span class="s1">    rune: The label is covered with mystical runes, the meanings of which elude you.</span>
<span class="s1">    glasses: The owner of these glasses had an indeterminate vision problem, because the lenses have both</span>
<span class="s1">        been crushed underfoot. The vision problem, of course, has been solved.</span>
<span class="s1">    lantern: The lantern, while of the cheapest construction, appears functional enough for the moment.</span>
<span class="s1">        Your best hope is that it stays that way. It looks like the lamp has gone through a few cycles of</span>
<span class="s1">            impact revitalization.</span>
<span class="s1">Inventory Attributes:</span>
<span class="s1">    glasses: clothing</span>
<span class="s1">    gloves: clothing</span>
<span class="s1">    sword: animate, equip</span>
<span class="s1">    lantern: animate, equip</span>
<span class="s1">Surrounding Objects:</span>
<span class="s1">    gargoyles: Unless you are inordinately masochistic, the less time spent examining the artwork, the</span>
<span class="s1">        better.</span>
<span class="s1">    east: You see nothing special about the east wall.</span>
<span class="s1">    tunnel: The tunnel leads west.</span>
<span class="s1">    gloves: The razor like gloves would be very attractive for an axe murderer. And they&#39;</span><span class="n">re</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">size</span><span class="o">.</span>
<span class="w">    </span><span class="n">south</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="w"> </span><span class="n">special</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">south</span><span class="w"> </span><span class="n">wall</span><span class="o">.</span>
<span class="w">    </span><span class="n">sign</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">sign</span><span class="w"> </span><span class="n">indicates</span><span class="w"> </span><span class="n">today</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">performance</span><span class="o">,</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="o">(</span><span class="k">in</span><span class="w"> </span><span class="n">honor</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">festivities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Convention</span>
<span class="w">        </span><span class="n">Center</span><span class="o">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="s2">&quot;A Massacre on 34th Street.&quot;</span>
<span class="n">Surrounding</span><span class="w"> </span><span class="n">Attributes</span><span class="o">:</span>
<span class="w">    </span><span class="n">gloves</span><span class="o">:</span><span class="w"> </span><span class="n">clothing</span>
<span class="w">    </span><span class="n">tunnel</span><span class="o">:</span><span class="w"> </span><span class="n">animate</span>
<span class="w">    </span><span class="n">sign</span><span class="o">:</span><span class="w"> </span><span class="n">animate</span>
<span class="n">Graph</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="n">sign</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="o">,</span><span class="w"> </span><span class="n">Cultural</span><span class="w"> </span><span class="n">Complex</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">Forever</span><span class="w"> </span><span class="n">Gores</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">ZM$100000</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">Baby</span>
<span class="w">        </span><span class="n">Rune</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">tunnel</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="o">,</span><span class="w"> </span><span class="n">Cultural</span><span class="w"> </span><span class="n">Complex</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="o">,</span><span class="w"> </span><span class="n">Cultural</span><span class="w"> </span><span class="n">Complex</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">brass</span><span class="w"> </span><span class="n">lantern</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span>
<span class="w">            </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">glasses</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">decoration</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="o">,</span><span class="w"> </span><span class="n">Cultural</span><span class="w"> </span><span class="n">Complex</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">cheaply</span><span class="o">-</span><span class="n">made</span><span class="w"> </span><span class="n">sword</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span>
<span class="w">            </span><span class="n">Multi</span><span class="o">-</span><span class="n">Implementeers</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">you</span><span class="o">,</span><span class="w"> </span><span class="n">have</span><span class="o">,</span><span class="w"> </span><span class="n">razor</span><span class="o">-</span><span class="n">like</span><span class="w"> </span><span class="n">gloves</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">glasses</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">clothing</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">gloves</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">clothing</span><span class="o">],</span>
<span class="w">                </span><span class="o">[</span><span class="n">sword</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">animate</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">tunnel</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">animate</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">sign</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">animate</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">lantern</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">animate</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">sword</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span>
<span class="w">                </span><span class="n">equip</span><span class="o">],</span><span class="w"> </span><span class="o">[</span><span class="n">lantern</span><span class="o">,</span><span class="w"> </span><span class="k">is</span><span class="o">,</span><span class="w"> </span><span class="n">equip</span><span class="o">]</span>
<span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="n">west</span><span class="o">,</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="n">lantern</span><span class="w"> </span><span class="n">off</span><span class="o">,</span><span class="w"> </span><span class="n">east</span><span class="o">,</span><span class="w"> </span><span class="n">south</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">multi</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">forever</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">lantern</span><span class="w"> </span><span class="n">down</span><span class="o">,</span>
<span class="w">    </span><span class="n">put</span><span class="w"> </span><span class="n">rune</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">glasses</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">sword</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">razor</span><span class="w"> </span><span class="n">off</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">glasses</span><span class="o">,</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">glasses</span><span class="o">,</span>
<span class="w">    </span><span class="n">lower</span><span class="w"> </span><span class="n">razor</span><span class="o">,</span><span class="w"> </span><span class="k">throw</span><span class="w"> </span><span class="n">multi</span><span class="o">,</span><span class="w"> </span><span class="k">throw</span><span class="w"> </span><span class="n">lantern</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">multi</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">glasses</span><span class="o">,</span><span class="w"> </span><span class="n">north</span>
</code></pre></div>

<h2>A. 2 Training and Hyperparameters</h2>
<p>All baseline models have hyperparameters taken from their respective works and from the JerichoWorld benchmarks. They are trained accordingly, with the exception of GATA-World. This model uses an architecture identical to that of the Worldformer but is trained to predict add/del</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">No. Samples</th>
<th style="text-align: center;">Input Vocab Size</th>
<th style="text-align: center;">Avg. Obs <br> Token Len.</th>
<th style="text-align: center;">Avg. No. Valid Actions</th>
<th style="text-align: center;">Avg. Graph Triple Len.</th>
<th style="text-align: center;">Avg. Graph Diff Len.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">wishbringer</td>
<td style="text-align: center;">560</td>
<td style="text-align: center;">1043</td>
<td style="text-align: center;">136.54</td>
<td style="text-align: center;">10.35</td>
<td style="text-align: center;">4.00</td>
<td style="text-align: center;">2.17</td>
</tr>
<tr>
<td style="text-align: center;">snacktime</td>
<td style="text-align: center;">168</td>
<td style="text-align: center;">468</td>
<td style="text-align: center;">190.08</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">2.33</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;">tryst205</td>
<td style="text-align: center;">1052</td>
<td style="text-align: center;">871</td>
<td style="text-align: center;">136.24</td>
<td style="text-align: center;">14.30</td>
<td style="text-align: center;">7.81</td>
<td style="text-align: center;">4.33</td>
</tr>
<tr>
<td style="text-align: center;">enter</td>
<td style="text-align: center;">448</td>
<td style="text-align: center;">470</td>
<td style="text-align: center;">219.06</td>
<td style="text-align: center;">18.04</td>
<td style="text-align: center;">14.79</td>
<td style="text-align: center;">4.55</td>
</tr>
<tr>
<td style="text-align: center;">omniquest</td>
<td style="text-align: center;">784</td>
<td style="text-align: center;">460</td>
<td style="text-align: center;">79.96</td>
<td style="text-align: center;">21.50</td>
<td style="text-align: center;">8.02</td>
<td style="text-align: center;">3.05</td>
</tr>
<tr>
<td style="text-align: center;">zork3</td>
<td style="text-align: center;">1142</td>
<td style="text-align: center;">564</td>
<td style="text-align: center;">137.68</td>
<td style="text-align: center;">12.72</td>
<td style="text-align: center;">6.59</td>
<td style="text-align: center;">2.78</td>
</tr>
<tr>
<td style="text-align: center;">zork2</td>
<td style="text-align: center;">584</td>
<td style="text-align: center;">684</td>
<td style="text-align: center;">154.90</td>
<td style="text-align: center;">29.66</td>
<td style="text-align: center;">7.82</td>
<td style="text-align: center;">4.11</td>
</tr>
<tr>
<td style="text-align: center;">inhumane</td>
<td style="text-align: center;">1004</td>
<td style="text-align: center;">409</td>
<td style="text-align: center;">90.24</td>
<td style="text-align: center;">4.31</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">1.61</td>
</tr>
<tr>
<td style="text-align: center;">905</td>
<td style="text-align: center;">504</td>
<td style="text-align: center;">296</td>
<td style="text-align: center;">100.91</td>
<td style="text-align: center;">13.60</td>
<td style="text-align: center;">11.69</td>
<td style="text-align: center;">2.73</td>
</tr>
<tr>
<td style="text-align: center;">loose</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1141</td>
<td style="text-align: center;">140.38</td>
<td style="text-align: center;">2.12</td>
<td style="text-align: center;">10.12</td>
<td style="text-align: center;">5.25</td>
</tr>
<tr>
<td style="text-align: center;">murdac</td>
<td style="text-align: center;">1914</td>
<td style="text-align: center;">251</td>
<td style="text-align: center;">80.76</td>
<td style="text-align: center;">8.67</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">1.85</td>
</tr>
<tr>
<td style="text-align: center;">moonlit</td>
<td style="text-align: center;">684</td>
<td style="text-align: center;">669</td>
<td style="text-align: center;">131.62</td>
<td style="text-align: center;">9.20</td>
<td style="text-align: center;">12.10</td>
<td style="text-align: center;">4.49</td>
</tr>
<tr>
<td style="text-align: center;">dragon</td>
<td style="text-align: center;">894</td>
<td style="text-align: center;">1049</td>
<td style="text-align: center;">182.79</td>
<td style="text-align: center;">13.13</td>
<td style="text-align: center;">11.64</td>
<td style="text-align: center;">5.26</td>
</tr>
<tr>
<td style="text-align: center;">jewel</td>
<td style="text-align: center;">1418</td>
<td style="text-align: center;">657</td>
<td style="text-align: center;">119.08</td>
<td style="text-align: center;">13.82</td>
<td style="text-align: center;">7.21</td>
<td style="text-align: center;">2.80</td>
</tr>
<tr>
<td style="text-align: center;">weapon</td>
<td style="text-align: center;">294</td>
<td style="text-align: center;">481</td>
<td style="text-align: center;">230.41</td>
<td style="text-align: center;">9.65</td>
<td style="text-align: center;">29.79</td>
<td style="text-align: center;">8.14</td>
</tr>
<tr>
<td style="text-align: center;">karn</td>
<td style="text-align: center;">2196</td>
<td style="text-align: center;">615</td>
<td style="text-align: center;">138.87</td>
<td style="text-align: center;">26.36</td>
<td style="text-align: center;">13.24</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr>
<td style="text-align: center;">zenon</td>
<td style="text-align: center;">402</td>
<td style="text-align: center;">401</td>
<td style="text-align: center;">101.52</td>
<td style="text-align: center;">5.97</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">2.06</td>
</tr>
<tr>
<td style="text-align: center;">acorncoart</td>
<td style="text-align: center;">474</td>
<td style="text-align: center;">343</td>
<td style="text-align: center;">323.38</td>
<td style="text-align: center;">20.18</td>
<td style="text-align: center;">36.14</td>
<td style="text-align: center;">4.37</td>
</tr>
<tr>
<td style="text-align: center;">ballyboo</td>
<td style="text-align: center;">2132</td>
<td style="text-align: center;">962</td>
<td style="text-align: center;">127.08</td>
<td style="text-align: center;">15.39</td>
<td style="text-align: center;">7.25</td>
<td style="text-align: center;">2.77</td>
</tr>
<tr>
<td style="text-align: center;">yomomma</td>
<td style="text-align: center;">884</td>
<td style="text-align: center;">619</td>
<td style="text-align: center;">129.06</td>
<td style="text-align: center;">16.11</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;">enchanter</td>
<td style="text-align: center;">1714</td>
<td style="text-align: center;">722</td>
<td style="text-align: center;">133.56</td>
<td style="text-align: center;">45.27</td>
<td style="text-align: center;">14.83</td>
<td style="text-align: center;">3.98</td>
</tr>
<tr>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">2082</td>
<td style="text-align: center;">728</td>
<td style="text-align: center;">166.96</td>
<td style="text-align: center;">25.03</td>
<td style="text-align: center;">15.76</td>
<td style="text-align: center;">6.16</td>
</tr>
<tr>
<td style="text-align: center;">houndark</td>
<td style="text-align: center;">344</td>
<td style="text-align: center;">539</td>
<td style="text-align: center;">162.33</td>
<td style="text-align: center;">6.33</td>
<td style="text-align: center;">13.01</td>
<td style="text-align: center;">4.04</td>
</tr>
<tr>
<td style="text-align: center;">afflicted</td>
<td style="text-align: center;">574</td>
<td style="text-align: center;">762</td>
<td style="text-align: center;">165.13</td>
<td style="text-align: center;">17.34</td>
<td style="text-align: center;">2.91</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;">adventureland</td>
<td style="text-align: center;">870</td>
<td style="text-align: center;">398</td>
<td style="text-align: center;">87.41</td>
<td style="text-align: center;">9.02</td>
<td style="text-align: center;">6.99</td>
<td style="text-align: center;">3.07</td>
</tr>
<tr>
<td style="text-align: center;">reverb</td>
<td style="text-align: center;">722</td>
<td style="text-align: center;">526</td>
<td style="text-align: center;">101.92</td>
<td style="text-align: center;">9.04</td>
<td style="text-align: center;">5.23</td>
<td style="text-align: center;">1.83</td>
</tr>
<tr>
<td style="text-align: center;">night</td>
<td style="text-align: center;">346</td>
<td style="text-align: center;">462</td>
<td style="text-align: center;">49.92</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">10.17</td>
<td style="text-align: center;">1.27</td>
</tr>
<tr>
<td style="text-align: center;">overall train</td>
<td style="text-align: center;">24198</td>
<td style="text-align: center;">11056</td>
<td style="text-align: center;">133.30</td>
<td style="text-align: center;">17.41</td>
<td style="text-align: center;">9.74</td>
<td style="text-align: center;">3.30</td>
</tr>
<tr>
<td style="text-align: center;">Testing games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">deephome</td>
<td style="text-align: center;">630</td>
<td style="text-align: center;">760</td>
<td style="text-align: center;">147.33</td>
<td style="text-align: center;">15.31</td>
<td style="text-align: center;">10.20</td>
<td style="text-align: center;">5.88</td>
</tr>
<tr>
<td style="text-align: center;">balances</td>
<td style="text-align: center;">990</td>
<td style="text-align: center;">452</td>
<td style="text-align: center;">107.15</td>
<td style="text-align: center;">13.04</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">2.41</td>
</tr>
<tr>
<td style="text-align: center;">ludicorp</td>
<td style="text-align: center;">2210</td>
<td style="text-align: center;">503</td>
<td style="text-align: center;">88.32</td>
<td style="text-align: center;">9.27</td>
<td style="text-align: center;">9.47</td>
<td style="text-align: center;">3.66</td>
</tr>
<tr>
<td style="text-align: center;">pentari</td>
<td style="text-align: center;">276</td>
<td style="text-align: center;">472</td>
<td style="text-align: center;">130.34</td>
<td style="text-align: center;">3.72</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">2.75</td>
</tr>
<tr>
<td style="text-align: center;">detective</td>
<td style="text-align: center;">434</td>
<td style="text-align: center;">344</td>
<td style="text-align: center;">105.97</td>
<td style="text-align: center;">5.72</td>
<td style="text-align: center;">2.80</td>
<td style="text-align: center;">1.64</td>
</tr>
<tr>
<td style="text-align: center;">ztuu</td>
<td style="text-align: center;">462</td>
<td style="text-align: center;">607</td>
<td style="text-align: center;">170.89</td>
<td style="text-align: center;">18.39</td>
<td style="text-align: center;">11.97</td>
<td style="text-align: center;">4.32</td>
</tr>
<tr>
<td style="text-align: center;">zork1</td>
<td style="text-align: center;">886</td>
<td style="text-align: center;">697</td>
<td style="text-align: center;">109.70</td>
<td style="text-align: center;">13.02</td>
<td style="text-align: center;">6.46</td>
<td style="text-align: center;">3.80</td>
</tr>
<tr>
<td style="text-align: center;">library</td>
<td style="text-align: center;">654</td>
<td style="text-align: center;">510</td>
<td style="text-align: center;">154.40</td>
<td style="text-align: center;">4.59</td>
<td style="text-align: center;">9.18</td>
<td style="text-align: center;">4.95</td>
</tr>
<tr>
<td style="text-align: center;">temple</td>
<td style="text-align: center;">1294</td>
<td style="text-align: center;">622</td>
<td style="text-align: center;">138.07</td>
<td style="text-align: center;">8.56</td>
<td style="text-align: center;">10.77</td>
<td style="text-align: center;">4.15</td>
</tr>
<tr>
<td style="text-align: center;">overall test</td>
<td style="text-align: center;">7836</td>
<td style="text-align: center;">11056</td>
<td style="text-align: center;">118.92</td>
<td style="text-align: center;">10.30</td>
<td style="text-align: center;">8.71</td>
<td style="text-align: center;">3.42</td>
</tr>
</tbody>
</table>
<p>Table 4: Statistics for JerichoWorld especially showcasing the difference between the average number of graph triples per state, and the average graph difference length per instance.
rules as described in Section 5-i.e. it is trained single-task and has only a graph decoder and no action decoder. The hyperparameters and training methodology for this model match those of the Worldformer described below.</p>
<h1>A.2.1 Worldformer</h1>
<p>Following JerichoWorld [4] models were trained until validation accuracy (picked to be a random $10 \%$ subset of the training data) did not improve for 5 epochs or 96 wall clock hours on a machine with 4 Nvidia GeForce RTX 2080 GPUs, three times with three random seeds. All models decode using beam search with a beam width of 15 at test time until the end-of-sequence tag is reached. The size of the decoding vocabulary for the action decoder is 11056 and for the graph decoder is 7002 . Hyperparameters were not tuned and were taken from other transformer-based text game works [1, 5]. Hyperparameter settings for ablations do not vary from the full Worldformer.</p>
<p>Encoders have an architecture similar to BERT [10] and decoders one similar to GPT-2 [27]-the rest of the hyperparameters are provided in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter type</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Text encoder</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dictionary Tokenizer</td>
<td style="text-align: center;">Sentence piece</td>
</tr>
<tr>
<td style="text-align: center;">Num. layers</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Num. attention heads</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Feedforward network hidden size</td>
<td style="text-align: center;">3072</td>
</tr>
<tr>
<td style="text-align: center;">Input length</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Embedding size</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: center;">Graph encoder</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dictionary Tokenizer</td>
<td style="text-align: center;">Triple tokenizer</td>
</tr>
<tr>
<td style="text-align: center;">Num. layers</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Num. attention heads</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Feedforward network hidden size</td>
<td style="text-align: center;">3072</td>
</tr>
<tr>
<td style="text-align: center;">Input length</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Embedding size</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: center;">Aggregator</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Num. layers</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Num. attention heads</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Feedforward network hidden size</td>
<td style="text-align: center;">4096</td>
</tr>
<tr>
<td style="text-align: center;">Input length</td>
<td style="text-align: center;">2048</td>
</tr>
<tr>
<td style="text-align: center;">Embedding size</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: center;">Action Decoder</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dictionary Tokenizer</td>
<td style="text-align: center;">White space tokenizer</td>
</tr>
<tr>
<td style="text-align: center;">Num. layers</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Num. attention heads</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Feedforward network hidden size</td>
<td style="text-align: center;">3072</td>
</tr>
<tr>
<td style="text-align: center;">Input length</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Embedding size</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: center;">Graph Decoder</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dictionary Tokenizer</td>
<td style="text-align: center;">Triple tokenizer</td>
</tr>
<tr>
<td style="text-align: center;">Num. layers</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Num. attention heads</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Feedforward network hidden size</td>
<td style="text-align: center;">3072</td>
</tr>
<tr>
<td style="text-align: center;">Input length</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">Embedding size</td>
<td style="text-align: center;">768</td>
</tr>
<tr>
<td style="text-align: center;">Common</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Activation</td>
<td style="text-align: center;">gelu</td>
</tr>
<tr>
<td style="text-align: center;">Batch size</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Dropout ratio</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">Gradient clip</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$3 \times 10^{-4}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Hyperparameters used to train the Worldformer. It has a total of $\approx 380$ million trainable parameters. The triple tokenizer splits on individual parts of $\langle s, r, o\rangle$ as described in Section 4.2.</p>
<h2>A. 3 Example Output Graphs and Actions</h2>
<p>Here, we provide 3 examples of graphs and actions generated from the randomly drawn test example instances shown in JerichoWorld to provide a qualitative comparison across the different models.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Game</span><span class="o">:</span><span class="w"> </span><span class="n">ludicorp</span>
<span class="n">State</span><span class="o">:</span>
<span class="w">    </span><span class="n">Location</span><span class="o">:</span><span class="w"> </span><span class="n">Meeting</span><span class="w"> </span><span class="n">Area</span>
<span class="w">        </span><span class="n">A</span><span class="w"> </span><span class="n">door</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">south</span><span class="w"> </span><span class="n">leads</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">garden</span><span class="o">.</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">water</span><span class="w"> </span><span class="n">cooler</span><span class="w"> </span><span class="n">sits</span><span class="w"> </span><span class="n">invitingly</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">corner</span><span class="o">.</span><span class="w"> </span><span class="n">More</span>
<span class="w">        </span><span class="n">doors</span><span class="w"> </span><span class="n">lead</span><span class="w"> </span><span class="n">east</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">west</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Coil</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">wire</span><span class="w"> </span><span class="n">here</span><span class="o">.</span>
<span class="n">Observation</span><span class="o">:</span><span class="w"> </span><span class="n">Dropped</span><span class="o">.</span>
<span class="n">Inventory</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">carrying</span><span class="o">:</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="n">Dragon</span><span class="w"> </span><span class="n">Statue</span>
<span class="w">    </span><span class="n">some</span><span class="w"> </span><span class="n">Plant</span><span class="w"> </span><span class="n">Pots</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="n">Long</span><span class="w"> </span><span class="n">Ladder</span>
</code></pre></div>

<h1>a Gun</h1>
<p>Graph: ["Coil of wire", "in", "Meeting Area"],
["you", "have", "Plant Pots"],
["Water Cooler", "in", "Meeting Area"],
["you", "have", "Dragon Statue"],
["you", "have", "Long Ladder"],
["you", "in", "Meeting Area"],
["you", "have", "Gun"]
Valid Actions: take wire, east, west, south, put dragon down, put pots down, put gun down, put ladder down
Act: take wire
Next State:
Location: Meeting Area
A door to the south leads into the garden. A water cooler sits invitingly in the corner. More doors lead east and west.
Observation: Taken.
Inventory: You are carrying:
a Coil of wire
a Dragon Statue
some Plant Pots
a Long Ladder
a Gun
Graph: ["you", "have", "Coil of wire"],
["you", "have", "Plant Pots"],
["Water Cooler", "in", "Meeting Area"],
["you", "have", "Dragon Statue"],
["you", "have", "Long Ladder"],
["you", "in", "Meeting Area"],
["you", "have", "Gun"]
Valid Actions: put wire down, east, west, south, put dragon down, put pots down, put gun down, put ladder down
Predicted Next State Graphs:
Rules: ["door to the", "in", "South"],
["more doors", "in", "east and west"]
["leads to the", "in", "garden"],
["you are carrying", "have", "some Plant Pots"],
["a water cooler sits", "in", "corner"],
["you are carrying", "have", "Dragon Statue"],
["you are carrying", "have", "a Long Ladder"],
["you", "in", "Meeting Area"],
["you are carrying", "have", "a Gun"]
QA: ["you", "have", "Coil of wire"],
["door", "in", "South"],
["doors", "in", "east and west"]
["you", "have", "some Plant Pots"],
["water cooler", "in", "corner"],
["you", "have", "Dragon Statue"],
["you", "have", "a Long Ladder a Gun"],
["you", "in", "Meeting Area"]
Seq2Seq: ["you", "have", "Pots"],
["you", "in", "Statue"],
["you", "have", "Ladder Gun"],
["you", "in", "Meeting Area"]
GATA-W: ["you", "in", "Coil of wire"],
["you", "have", "Plant Pots"],
["Water Cooler", "in", "Meeting Area"],
["you", "have", "Dragon Statue"],
["you", "in", "Statue"],
["you", "have", "Long Ladder"],
["you", "in", "Meeting Area"],
["you", "have", "Gun"]
Worldformer: ["you", "have", "Coil of wire"],
["you", "have", "Plant Pots"],
["Water Cooler", "in", "Meeting Area"],
["you", "in", "Dragon Statue"],
["you", "have", "Long Ladder"],
["you", "in", "Meeting Area"],
["you", "have", "Gun"]
Predicted Next State Valid Actions:
Seq2Seq Actions: east, west, south, north, pick coil up, pick statue up, put ladder gun down, put meeting area down, put pots down
CALM Actions: northwest, up, take plant, put plant in cooler, take water, take fish, south, wait, take all, north, take dragon, take cooler, southeast, take dragon statue, get all, east, west, take gun, northeast, southwest
Worldformer Actions: east, west, south, north, put coil down, pick statue up, put ladder down, put pots down, put gun down</p>
<div class="codehilite"><pre><span></span><code>===============
</code></pre></div>

<p>Game: pentari
State:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Location</span><span class="o">:</span><span class="w"> </span><span class="n">Armory</span>
<span class="w">    </span><span class="n">Many</span><span class="w"> </span><span class="n">death</span><span class="o">-</span><span class="n">dealing</span><span class="w"> </span><span class="n">weapons</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">every</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">stored</span><span class="w"> </span><span class="n">here</span><span class="o">.</span><span class="w"> </span><span class="n">Several</span><span class="w"> </span><span class="n">tall</span><span class="w"> </span><span class="n">racks</span><span class="w"> </span><span class="n">probably</span><span class="w"> </span><span class="n">beld</span>
<span class="w">        </span><span class="n">spears</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">shorter</span><span class="w"> </span><span class="n">ones</span><span class="w"> </span><span class="n">mounted</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">wall</span><span class="w"> </span><span class="n">stored</span><span class="w"> </span><span class="n">various</span><span class="w"> </span><span class="n">kinds</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">swords</span><span class="o">.</span><span class="w"> </span><span class="n">Other</span>
<span class="w">        </span><span class="n">wall</span><span class="w"> </span><span class="n">mounts</span><span class="o">,</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">empty</span><span class="o">,</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">idea</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">weapons</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">held</span><span class="w"> </span><span class="n">by</span>
<span class="w">        </span><span class="n">them</span><span class="o">.</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">archway</span><span class="w"> </span><span class="n">north</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">partially</span><span class="w"> </span><span class="n">blocked</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">collapsed</span><span class="w"> </span><span class="n">stones</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">rubble</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span>
<span class="w">        </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">jewel</span><span class="w"> </span><span class="n">encrusted</span><span class="w"> </span><span class="n">dagger</span><span class="w"> </span><span class="n">here</span><span class="o">.</span>
<span class="n">Observation</span><span class="o">:</span><span class="w"> </span><span class="n">Armory</span>
<span class="w">    </span><span class="n">Many</span><span class="w"> </span><span class="n">death</span><span class="o">-</span><span class="n">dealing</span><span class="w"> </span><span class="n">weapons</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">every</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">stored</span><span class="w"> </span><span class="n">here</span><span class="o">.</span><span class="w"> </span><span class="n">Several</span><span class="w"> </span><span class="n">tall</span><span class="w"> </span><span class="n">racks</span><span class="w"> </span><span class="n">probably</span><span class="w"> </span><span class="n">beld</span>
<span class="w">        </span><span class="n">spears</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">shorter</span><span class="w"> </span><span class="n">ones</span><span class="w"> </span><span class="n">mounted</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">wall</span><span class="w"> </span><span class="n">stored</span><span class="w"> </span><span class="n">various</span><span class="w"> </span><span class="n">kinds</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">swords</span><span class="o">.</span>
<span class="w">        </span><span class="n">Other</span><span class="w"> </span><span class="n">wall</span><span class="w"> </span><span class="n">mounts</span><span class="o">,</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">empty</span><span class="o">,</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">idea</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">weapons</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">been</span>
<span class="w">        </span><span class="n">held</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">them</span><span class="o">.</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">archway</span><span class="w"> </span><span class="n">north</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">partially</span><span class="w"> </span><span class="n">blocked</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">collapsed</span><span class="w"> </span><span class="n">stones</span><span class="w"> </span><span class="n">and</span>
<span class="w">        </span><span class="n">rubble</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">jewel</span><span class="w"> </span><span class="n">encrusted</span><span class="w"> </span><span class="n">dagger</span><span class="w"> </span><span class="n">here</span><span class="o">.</span>
<span class="n">Inventory</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">carrying</span><span class="w"> </span><span class="n">nothing</span><span class="o">.</span>
<span class="n">Graph</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;jewel encrusted dagger&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="w"> </span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Armory&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;west&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">]</span>
<span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">other</span><span class="o">,</span><span class="w"> </span><span class="n">east</span><span class="o">,</span><span class="w"> </span><span class="n">north</span>
<span class="n">Act</span><span class="o">:</span><span class="w"> </span><span class="n">east</span>
<span class="n">Next</span><span class="w"> </span><span class="n">State</span><span class="o">:</span>
<span class="w">    </span><span class="n">Location</span><span class="o">:</span><span class="w"> </span><span class="n">Main</span><span class="w"> </span><span class="n">Hall</span>
<span class="w">        </span><span class="n">This</span><span class="w"> </span><span class="n">once</span><span class="w"> </span><span class="n">majestic</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">visitors</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">come</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">relax</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">meet</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">formal</span><span class="w"> </span><span class="n">lord</span>
<span class="w">        </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">castle</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">somewhat</span><span class="w"> </span><span class="n">informal</span><span class="w"> </span><span class="n">atmosphere</span><span class="o">.</span><span class="w"> </span><span class="n">Several</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">comfortable</span><span class="w"> </span><span class="n">couches</span><span class="w"> </span><span class="n">are</span>
<span class="w">        </span><span class="n">scattered</span><span class="w"> </span><span class="n">about</span><span class="o">,</span><span class="w"> </span><span class="n">dusty</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">altogether</span><span class="w"> </span><span class="n">squalid</span><span class="o">.</span><span class="w"> </span><span class="n">Many</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">tapestries</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">hang</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span>
<span class="w">        </span><span class="n">walls</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">horribly</span><span class="w"> </span><span class="n">faded</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">age</span><span class="o">.</span><span class="w"> </span><span class="n">Large</span><span class="w"> </span><span class="n">open</span><span class="w"> </span><span class="n">archways</span><span class="w"> </span><span class="n">lead</span><span class="w"> </span><span class="n">east</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">west</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">a</span>
<span class="w">        </span><span class="n">huge</span><span class="w"> </span><span class="n">fireplace</span><span class="w"> </span><span class="n">dominates</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">northern</span><span class="w"> </span><span class="n">wall</span><span class="o">.</span>
<span class="n">Observation</span><span class="o">:</span><span class="w"> </span><span class="n">Main</span><span class="w"> </span><span class="n">Hall</span>
<span class="w">    </span><span class="n">This</span><span class="w"> </span><span class="n">once</span><span class="w"> </span><span class="n">majestic</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">visitors</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="n">come</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">relax</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">meet</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">formal</span>
<span class="w">        </span><span class="n">lord</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">castle</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">somewhat</span><span class="w"> </span><span class="n">informal</span><span class="w"> </span><span class="n">atmosphere</span><span class="o">.</span><span class="w"> </span><span class="n">Several</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">comfortable</span>
<span class="w">        </span><span class="n">couches</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">scattered</span><span class="w"> </span><span class="n">about</span><span class="o">,</span><span class="w"> </span><span class="n">dusty</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">altogether</span><span class="w"> </span><span class="n">squalid</span><span class="o">.</span><span class="w"> </span><span class="n">Many</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">tapestries</span><span class="w"> </span><span class="n">still</span>
<span class="w">        </span><span class="n">hang</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">walls</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">horribly</span><span class="w"> </span><span class="n">faded</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">age</span><span class="o">.</span><span class="w"> </span><span class="n">Large</span><span class="w"> </span><span class="n">open</span><span class="w"> </span><span class="n">archways</span><span class="w"> </span><span class="n">lead</span><span class="w"> </span><span class="n">east</span><span class="w"> </span><span class="n">and</span>
<span class="w">        </span><span class="n">west</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">huge</span><span class="w"> </span><span class="n">fireplace</span><span class="w"> </span><span class="n">dominates</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">northern</span><span class="w"> </span><span class="n">wall</span>
<span class="n">Inventory</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">carrying</span><span class="w"> </span><span class="n">nothing</span><span class="o">.</span>
<span class="n">Graph</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;couch&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;jewel encrusted dagger&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="w"> </span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;east&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;tapestry&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">]</span>
<span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="n">east</span><span class="o">,</span><span class="w"> </span><span class="n">west</span><span class="o">,</span><span class="w"> </span><span class="n">south</span><span class="o">,</span><span class="w"> </span><span class="n">north</span>
<span class="n">Predicted</span><span class="w"> </span><span class="n">Next</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">Graphs</span><span class="o">:</span>
<span class="n">Rules</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Several large comfortable couches&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;many large tapestries&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;many large tapestries&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;age&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;huge fireplace dominates&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;center of room against northern wall&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;death dealing weapons&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;visitors&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;to relax&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;archway north&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;blocked&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;archway north&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;collapsed stones&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;spears&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;several tall racks&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;see encrusted dagger&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you are carrying&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;nothing&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;east&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">]</span>
<span class="n">QA</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;couches&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;tapestries&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;tapestries&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;faded&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;east&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;fireplace&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;center of room&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;weapons&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;visitors&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;spears&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;tall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;nothing&quot;</span><span class="o">]</span>
<span class="n">Seq2Seq</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;couch&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;jewel&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="w"> </span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;large&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">]</span>
<span class="n">GATA</span><span class="o">-</span><span class="n">W</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;couch&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;jewel encrusted dagger&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="w"> </span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;tapestry&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">]</span>
<span class="n">Worldformer</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;couch&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;dagger&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="w"> </span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;Main Hall&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;east&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Armory&quot;</span><span class="o">],</span>
<span class="o">[</span><span class="s2">&quot;faded&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Main Hall&quot;</span><span class="o">]</span>
<span class="n">Predicted</span><span class="w"> </span><span class="n">Next</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span>
<span class="w">    </span><span class="n">Seq2Seq</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="n">east</span><span class="o">,</span><span class="w"> </span><span class="n">west</span><span class="o">,</span><span class="w"> </span><span class="n">south</span><span class="o">,</span><span class="w"> </span><span class="n">north</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">jewel</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">large</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">armory</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">couch</span><span class="w"> </span><span class="n">down</span>
<span class="w">    </span><span class="n">CALM</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="kd">get</span><span class="w"> </span><span class="n">dagger</span><span class="o">,</span><span class="w"> </span><span class="n">northwest</span><span class="o">,</span><span class="w"> </span><span class="n">out</span><span class="o">,</span><span class="w"> </span><span class="n">up</span><span class="o">,</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">exits</span><span class="o">,</span><span class="w"> </span><span class="n">search</span><span class="w"> </span><span class="n">tapestries</span><span class="o">,</span><span class="w"> </span><span class="n">close</span><span class="w"> </span><span class="n">door</span><span class="o">,</span><span class="w"> </span><span class="n">south</span><span class="o">,</span><span class="w"> </span><span class="n">search</span>
<span class="w">        </span><span class="n">fireplace</span><span class="o">,</span><span class="w"> </span><span class="n">enter</span><span class="w"> </span><span class="n">fireplace</span><span class="o">,</span><span class="w"> </span><span class="n">open</span><span class="w"> </span><span class="n">fireplace</span><span class="o">,</span><span class="w"> </span><span class="n">north</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="o">,</span><span class="w"> </span><span class="n">southeast</span><span class="o">,</span><span class="w"> </span><span class="n">east</span><span class="o">,</span><span class="w"> </span><span class="n">west</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">dagger</span><span class="o">,</span>
<span class="w">        </span><span class="n">northeast</span><span class="o">,</span><span class="w"> </span><span class="n">southwest</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Game</span><span class="o">:</span><span class="w"> </span><span class="n">temple</span>
<span class="n">State</span><span class="o">:</span>
<span class="w">    </span><span class="n">Location</span><span class="o">:</span><span class="w"> </span><span class="n">Dead</span><span class="w"> </span><span class="n">End</span>
<span class="w">        </span><span class="n">This</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">town</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">radically</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">parts</span><span class="w"> </span><span class="n">closer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tower</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">roads</span><span class="w"> </span><span class="n">are</span>
<span class="w">            </span><span class="n">narrower</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">paving</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">irregular</span><span class="o">,</span><span class="w"> </span><span class="n">sometimes</span><span class="w"> </span><span class="n">stone</span><span class="w"> </span><span class="n">slabs</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">sometimes</span><span class="w"> </span><span class="n">cobblestones</span><span class="o">.</span>
<span class="w">                </span><span class="n">The</span><span class="w"> </span><span class="n">buildings</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">tall</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">less</span><span class="w"> </span><span class="n">well</span><span class="w"> </span><span class="n">kept</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">before</span><span class="o">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">windows</span><span class="w"> </span><span class="n">or</span>
<span class="w">                </span><span class="n">doors</span><span class="o">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">few</span><span class="w"> </span><span class="n">overhead</span><span class="w"> </span><span class="n">bridges</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">house</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">road</span><span class="w"> </span><span class="n">ends</span><span class="w"> </span><span class="n">here</span><span class="w"> </span><span class="n">and</span>
<span class="w">                </span><span class="n">the</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">way</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">north</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">wrought</span><span class="w"> </span><span class="n">iron</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">Charles</span>
<span class="w">                </span><span class="n">Bristow</span><span class="w"> </span><span class="n">here</span><span class="o">.</span>
<span class="w">    </span><span class="n">Observation</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">cat</span><span class="w"> </span><span class="n">jumps</span><span class="w"> </span><span class="n">aside</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">avoid</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">projectile</span><span class="o">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">moves</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bit</span><span class="w"> </span><span class="n">too</span><span class="w"> </span><span class="n">far</span><span class="o">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">falls</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">but</span>
<span class="w">        </span><span class="n">like</span><span class="w"> </span><span class="n">most</span><span class="w"> </span><span class="n">cats</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">escapes</span><span class="w"> </span><span class="n">unhurt</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">cat</span><span class="w"> </span><span class="n">runs</span><span class="w"> </span><span class="n">off</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">north</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">wrought</span><span class="w"> </span><span class="n">iron</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="n">falls</span><span class="w"> </span><span class="n">down</span>
<span class="w">        </span><span class="n">again</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">hits</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stone</span><span class="w"> </span><span class="n">slabs</span><span class="o">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">hollow</span><span class="w"> </span><span class="n">sound</span><span class="o">,</span><span class="w"> </span><span class="n">much</span><span class="w"> </span><span class="n">like</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">cavity</span>
<span class="w">        </span><span class="n">below</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">slab</span><span class="o">.</span><span class="w"> </span><span class="s1">&#39;I used to have a cat, you know&#39;</span><span class="w"> </span><span class="n">Charles</span><span class="w"> </span><span class="n">remarks</span><span class="o">.</span>
<span class="w">    </span><span class="n">Inventory</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">carrying</span><span class="o">:</span>
<span class="w">        </span><span class="n">a</span><span class="w"> </span><span class="n">vial</span><span class="w"> </span><span class="n">labelled</span><span class="w"> </span><span class="n">Mukhtar</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">Caelestae</span><span class="w"> </span><span class="n">Horriblis</span>
<span class="w">        </span><span class="n">two</span><span class="w"> </span><span class="n">vials</span>
<span class="w">        </span><span class="n">a</span><span class="w"> </span><span class="n">yellow</span><span class="w"> </span><span class="n">paper</span>
<span class="w">        </span><span class="n">a</span><span class="w"> </span><span class="n">hideous</span><span class="w"> </span><span class="n">statue</span>
<span class="w">    </span><span class="n">Graph</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;stone slab&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;slab&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Charles&#39; clothes&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Charles Bristow&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;clothes&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;clothes&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;equip&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;mysterious vial&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;vial labelled Mukhtar&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;overhead bridge&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Caelestae Horriblis&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;yellow paper&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;yellow paper&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;hideous statue&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;hideous statue&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;sky&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;elliptcal building&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;entrance&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Charles Bristow&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;wrought iron key&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">]</span>
<span class="w">    </span><span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">wrought</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">paving</span><span class="o">,</span><span class="w"> </span><span class="n">north</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">mysterious</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">caelestae</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">paper</span>
<span class="w">        </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">statue</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">mukhtar</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">drop</span><span class="w"> </span><span class="n">wrought</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">bridge</span>
<span class="n">Act</span><span class="o">:</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">paper</span><span class="w"> </span><span class="n">down</span>
<span class="n">Next</span><span class="w"> </span><span class="n">State</span><span class="o">:</span>
<span class="w">    </span><span class="n">Location</span><span class="o">:</span><span class="w"> </span><span class="n">Dead</span><span class="w"> </span><span class="n">End</span>
<span class="w">        </span><span class="n">This</span><span class="w"> </span><span class="n">part</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">town</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">radically</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">parts</span><span class="w"> </span><span class="n">closer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">tower</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">roads</span><span class="w"> </span><span class="n">are</span>
<span class="w">            </span><span class="n">narrower</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">paving</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">irregular</span><span class="o">,</span><span class="w"> </span><span class="n">sometimes</span><span class="w"> </span><span class="n">stone</span><span class="w"> </span><span class="n">slabs</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">sometimes</span><span class="w"> </span><span class="n">cobblestones</span><span class="o">.</span>
<span class="w">                </span><span class="n">The</span><span class="w"> </span><span class="n">buildings</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">tall</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">less</span><span class="w"> </span><span class="n">well</span><span class="w"> </span><span class="n">kept</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">before</span><span class="o">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">windows</span><span class="w"> </span><span class="n">or</span>
<span class="w">                </span><span class="n">doors</span><span class="o">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">few</span><span class="w"> </span><span class="n">overhead</span><span class="w"> </span><span class="n">bridges</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">house</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">house</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">road</span><span class="w"> </span><span class="n">ends</span><span class="w"> </span><span class="n">here</span><span class="w"> </span><span class="n">and</span>
<span class="w">                </span><span class="n">the</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">way</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">north</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">also</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">yellow</span><span class="w"> </span><span class="n">paper</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">wrought</span><span class="w"> </span><span class="n">iron</span><span class="w"> </span><span class="n">key</span>
<span class="w">                </span><span class="n">and</span><span class="w"> </span><span class="n">Charles</span><span class="w"> </span><span class="n">Bristow</span><span class="w"> </span><span class="n">here</span><span class="o">.</span>
<span class="w">    </span><span class="n">Observation</span><span class="o">:</span><span class="w"> </span><span class="n">Dropped</span><span class="o">.</span>
<span class="w">    </span><span class="n">Inventory</span><span class="o">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">carrying</span><span class="o">:</span>
<span class="w">        </span><span class="n">a</span><span class="w"> </span><span class="n">vial</span><span class="w"> </span><span class="n">labelled</span><span class="w"> </span><span class="n">Mukhtar</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">Caelestae</span><span class="w"> </span><span class="n">Horriblis</span>
<span class="w">        </span><span class="n">two</span><span class="w"> </span><span class="n">vials</span>
<span class="w">        </span><span class="n">a</span><span class="w"> </span><span class="n">hideous</span><span class="w"> </span><span class="n">statue</span>
<span class="w">    </span><span class="n">Graph</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;yellow paper&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;yellow paper&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;stone slab&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;stone slab&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Charles&#39; clothes&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Charles Bristow&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;clothes&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;clothes&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;equip&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;mysterious vial&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;vial labelled Mukhtar&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;overhead bridge&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Caelestae Horriblis&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;have&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;hideous statue&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;hideous statue&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;animate&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;sky&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;elliptcal building&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;you&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;entrance&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;Charles Bristow&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">],</span>
<span class="w">    </span><span class="o">[</span><span class="s2">&quot;wrought iron key&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;in&quot;</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;Dead End&quot;</span><span class="o">]</span>
<span class="w">    </span><span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span><span class="o">:</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">wrought</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">paving</span><span class="o">,</span><span class="w"> </span><span class="n">north</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">mysterious</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">caelestae</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">all</span><span class="o">,</span><span class="w"> </span><span class="n">put</span>
<span class="w">        </span><span class="n">statue</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">mukhtar</span><span class="w"> </span><span class="n">down</span><span class="o">,</span><span class="w"> </span><span class="n">drop</span><span class="w"> </span><span class="n">wrought</span><span class="w"> </span><span class="n">against</span><span class="w"> </span><span class="n">bridge</span>
<span class="n">Predicted</span><span class="w"> </span><span class="n">Next</span><span class="w"> </span><span class="n">State</span><span class="w"> </span><span class="n">Graphs</span><span class="o">:</span>
</code></pre></div>

<p>Rules:["you", "in", "Dead End"],
["Dead End", "is", "radically different from the parts closer to the tower"],
["roads", "is", "narrower"],
["the paving", "is", "irregular"],
["the buildings", "is", "less well kept"],
["only way", "is", "north"],
["you can see", "in", "yellow paper"],
["you can see", "in", "a wrought iron key and Charles Bristow here"],
["you are carrying", "have", "a vial labelled Mukhtar"],
["you are carrying", "have", "the Caelestae Horriblis two vials"],
["you are carrying", "have", "a hideous statue"]
QA: ["you", "in", "Dead End"],
["Dead End", "is", "animate"],
["road", "is", "animate"],
["paving", "is", "animate"],
["building", "is", "animate"],
["yellow paper", "in", "Dead End"],
["paper", "is", "animate"],
["key", "in", "Dead End"],
["key", "is", "animate"],
["Charles Bristow", "in", "Dead End"],
["Charles Bristow", "is", "animate"],
["you", "have", "a vial"],
["a vial", "is", "animate"],
["you", "have", "Caelestae Horriblis"],
["Caelestae Horriblis", "is", "animate"],
["you", "have", "two vials"],
["you", "have", "a hideous statue"]
["a hideous statue", "is", "animate"]
Seq2Seq: ["you", "in", "Dead End"],
["Dead End", "is", "animate"],
["key", "in", "Dead End"],
["key", "is", "animate"],
["Charles Bristow", "in", "Dead End"],
["Charles Bristow", "is", "animate"],
["you", "have", "vial"],
["you", "have", "two vials"],
["you", "in", "statue"]
GATA-W: ["yellow paper", "in", "Dead End"],
["stone slab", "in", "Dead End"],
["stone slab", "is", "animate"],
["Charles' clothes", "in", "Charles Bristow"],
["clothes", "is", "animate"],
["clothes", "is", "equip"],
["you", "have", "mysterious vial"],
["you", "have", "vial labelled Mukhtar"],
["overhead bridge", "in", "Dead End"],
["you", "in", "Caelestae Horriblis"],
["sky", "in", "Dead End"],
["elliptcal building", "in", "Dead End"],
["you", "in", "Dead End"],
["entrance", "in", "Dead End"],
["Charles Bristow", "in", "Dead End"],
["wrought iron key", "in", "Dead End"]
Worldformer: ["yellow paper", "in", "Dead End"],
["yellow paper", "is", "animate"],
["stone slab", "in", "Dead End"],
["stone slab", "is", "animate"],
["Charles' clothes", "in", "Charles Bristow"],
["clothes", "is", "animate"],
["clothes", "is", "equip"],
["you", "have", "vial"],
["bridge", "in", "Dead End"],
["you", "have", "Caelestae Horriblis"],
["you", "have", "hideous statue"],
["hideous statue", "is", "animate"],
["sky", "in", "Dead End"],
["building", "in", "Dead End"],
["you", "in", "Dead End"],
["entrance", "in", "Dead End"],
["Charles Bristow", "in", "Dead End"],
["wrought iron key", "in", "Dead End"]
Predicted Next State Valid Actions:
Seq2Seq Actions: east, west, south, north, drop vial, take key, take statue, take charles bristow
CALM Actions: put paper down, drop vial, take key, up, down, put paper in cavity, put yellow paper down , take vial, drop key, drop all, open vial, south, get key, put yellow paper in cavity, take all, put vial in cavity, north, east, west, give vial to cat
Worldformer Actions: south, east, west, north, take building, take entrance, drop vial, take charles bristow, take paper, drop vials, take sky, drop key, take statue, take bridge</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://www.allthingsjacq.com/interactive_fiction.html
${ }^{3}$ https://github.com/princeton-nlp/calm-textgame&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
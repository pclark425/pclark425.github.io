<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1703 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1703</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1703</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-258841747</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.findings-emnlp.1019.pdf" target="_blank">Masked Path Modeling for Vision-and-Language Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments by following natural language instructions. A major challenge in VLN is the limited availability of training data, which hinders the models' ability to generalize effectively. Previous approaches have attempted to address this issue by introducing additional supervision during training, often requiring costly human-annotated data that restricts scalability. In this paper, we introduce a masked path modeling (MPM) objective, which pretrains an agent using self-collected data for downstream navigation tasks. Our proposed method involves allowing the agent to actively explore navigation environments without a specific goal and collect the paths it traverses. Subsequently, we train the agent on this collected data to reconstruct the original path given a randomly masked subpath. This way, the agent can actively accumulate a diverse and substantial amount of data while learning conditional action generation. To evaluate the effectiveness of our technique, we conduct experiments on various VLN datasets and demonstrate the versatility of MPM across different levels of instruction complexity. Our results exhibit significant improvements in success rates, with enhancements of 1.32\%, 1.05\%, and 1.19\% on the val-unseen split of the Room-to-Room, Room-for-Room, and Room-across-Room datasets, respectively. Furthermore, we conduct an analysis that highlights the potential for additional improvements when the agent is allowed to explore unseen environments prior to testing.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1703.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1703.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision encoder pretrained on large image–text (caption) pairs using a contrastive objective; used in this work as a frozen visual feature extractor and reported to substantially improve downstream VLN performance versus ImageNet-pretrained encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP vision encoder (used as CLIP-ViT-L-336/14)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A multimodal contrastively pretrained vision encoder that maps images to a joint image–text embedding space; here used as a frozen visual feature extractor (CLIP-ViT-L-336/14) whose outputs are linearly projected and fed into the VLN transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>image-caption (image+text) web corpora</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Described in the paper as "large image-caption data" / web-scraped image-text pairs; specific datasets and sizes are not enumerated in this paper (original CLIP was trained on a very large, web-scale set of image-caption pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN) benchmarks (R2R / R4R / RxR)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D photorealistic indoor navigation in Matterport3D-derived environments where an agent receives a language instruction and panoramic visual observations and must output a sequence of discrete navigation actions to reach a goal viewpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (CLIP is not an agent pretrained in a text-action environment; pretraining involved pairing images with captions)</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions (panoramic view selection / step to next viewpoint; heading/elevation angles used for regression in some objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Not applicable — CLIP provides visual representations that are consumed by the VLN model; action generation is learned by the cross-modal transformer on top of CLIP features during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic views (36 views per panorama in experiments); CLIP features extracted per view plus relative-angle encoding (sin/cos of heading and elevation).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using CLIP as the vision encoder (HAMT+ baseline) improved performance relative to the original HAMT (ImageNet vision encoder): on RxR validation seen the HAMT+ baseline outperformed HAMT by 4.53% SR and 4.57 sDTW, and on RxR validation unseen by 5.5% SR and 4.42 sDTW (numbers reported in the paper as resulting from using CLIP features).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Original HAMT baseline (ImageNet-pretrained vision encoder) — used as comparison; exact HAMT baseline numeric values are reported in paper tables but vary per dataset/split (see tables in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Multimodal pretraining on image-text pairs provides stronger, more aligned visual features for language-conditioned tasks; representation alignment between vision and text reduces domain gap and improves downstream VLN learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>CLIP does not by itself connect visual representations to action generation — action mapping must be learned during downstream finetuning; potential modality-gap remains for action-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an image–text pretrained vision encoder (CLIP) as a frozen feature extractor yields clear, measurable improvements for VLN over ImageNet-pretrained encoders, demonstrating that image-caption (language) pretraining transfers beneficially to 3D embodied navigation when representations are consumed by a downstream action-generating model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Masked Path Modeling for Vision-and-Language Navigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1703.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1703.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLN-focused pretraining approach that uses image–text–action triplets from VLN data with single-step action prediction and masked instruction modeling to integrate action generation into pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards learning a generic agent for vision-and-language navigation via pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PREVALENT (VLN pretraining with image-text-action triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretraining scheme for VLN that trains models on in-domain image–text–action triplets using objectives such as single-step action prediction (classification) and masked instruction modeling to learn to predict actions given language and visual context.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>in-domain VLN data: image-text-action triplets (annotated instructions paired with trajectories/actions)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining data consisted of available VLN datasets (human-annotated instruction-action pairs) and synthetic instruction data in prior work (PREVALENT uses VLN data and some synthesized data); exact sizes not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Agents are trained to follow natural-language instructions in Matterport3D-based indoor environments and output navigation actions to reach goal viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>In the pretraining setting, the action supervision is single-step discrete navigation actions (classification over candidate next viewpoints) and regression of heading/elevation for next-step actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions and optionally continuous heading/elevation regression (SAP/SAR objectives described).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct supervision: pretraining uses image-text-action triplets so the model learns a mapping from multimodal representations (instruction + visual context) to the discrete next-action label (single-step prediction) and to heading/elevation regression.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB visual observations (CLIP/ImageNet visual features), relative-angle encodings; pretraining uses the same modalities as downstream VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Pretraining on in-domain VLN image-text-action triplets provides direct action-supervised signals that reduce the gap between representation learning and action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Scalability limited by scarcity of annotated instruction–action pairs; reliance on human-annotated or synthetic data that may be noisy or limited in quantity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining that explicitly includes action prediction (image–text–action triplets) helps connect learned representations to action generation, but scalability is constrained by the availability and quality of in-domain annotated instruction–action data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Masked Path Modeling for Vision-and-Language Navigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1703.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1703.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT / XLM-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT and XLM-R text encoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pretrained language models used to encode natural language instructions (BERT for English, XLM-R for multilingual instructions) before feeding embeddings into the VLN cross-modal transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BERT (English) / XLM-R (multilingual) text encoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Transformer-based language encoders pretrained on large text corpora (BERT on English corpora; XLM-R on multilingual corpora) that produce instruction embeddings used as inputs to the VLN cross-modal transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large text corpora (Wikipedia / web text for BERT; large multilingual corpora for XLM-R)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Standard pretraining corpora used by these models (not enumerated in this paper); BERT/XLM-R are off-the-shelf pretrained language models providing strong text representations.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN) — R2R / R4R / RxR</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Language-conditioned 3D navigation where the encoded instruction conditions action generation across panoramic visual history and current observation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Text pretraining involved language modeling objectives (masked language modeling), not action supervision; action space in pretraining is not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions in the 3D environment; action generation is learned during VLN finetuning using instruction embeddings from BERT/XLM-R.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No explicit mapping in pretraining; mapping is learned during downstream finetuning by the cross-modal transformer that consumes text embeddings and visual features to predict actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic observations plus relative-angle encodings; text encoder supplies instruction grounding to visual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong pretrained language representations provide robust instruction encodings that facilitate grounding to vision and action during downstream VLN training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Language-only pretraining does not teach action grounding; the agent still needs action-conditioned finetuning to map instructions to executable navigation behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language encoders (BERT / XLM-R) are standard and effective for encoding instructions in VLN, but by themselves do not supply action-generation capabilities — those are learned when combined with visual features and finetuned on VLN objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Masked Path Modeling for Vision-and-Language Navigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1703.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1703.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Image-caption VLN pretraining (Majumdar / Guhur)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Web-scale image–caption pretraining applied to VLN (e.g., Majumdar et al., Guhur et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works pretrained vision–language models on web-scraped image–caption corpora (e.g., Conceptual Captions) to learn general vision–language representations and then finetuned them on VLN, improving transfer but not directly learning action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving vision-and-language navigation with image-text pairs from the web.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Vision–language models pretrained on image–caption corpora (e.g., Conceptual Captions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Models pretrained on large-scale image-caption datasets to learn aligned vision-text representations; subsequently finetuned on in-domain VLN data for navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>web-scraped image–caption datasets (image + text captions)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Examples mentioned include Conceptual Captions and other web-scraped image-caption corpora; the paper does not list sizes here but describes them as web-scale image–text data.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Instruction-conditioned 3D navigation in Matterport3D-derived environments requiring grounding of language to visual scenes and planning/executing navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Pretraining on image-caption corpora involves captions (language supervision), not action labels; action semantics are absent during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions (panoramic candidate selection) learned during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Action mapping occurs during downstream finetuning: the pretrained vision–language representations are adapted and connected to action predictors in the VLN model, rather than explicit mapping present in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic views converted to pretrained visual features; language modality via captions/instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in literature and discussed in this paper as producing improvements when finetuned on VLN, but the present paper does not report specific numeric gains for these exact prior methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale aligned image–text pretraining yields vision–language representations that are more directly adaptable to language-conditioned embodied tasks, reducing representation-domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Lack of action-level supervision in pretraining means these models must still learn to map representations to actions during finetuning; domain mismatch between web images/captions and indoor VLN panoramas can remain.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on web-scale image–caption corpora improves vision–language alignment and helps downstream VLN after finetuning, but such pretraining does not by itself teach action generation or fully close the domain gap to in-domain VLN data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Masked Path Modeling for Vision-and-Language Navigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1703.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1703.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Li & Bansal (2023) proxy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Li and Bansal (2023) — proxy pretraining tasks for VLN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent work proposing three proxy tasks (one reconstructs trajectory semantic information from masked inputs) intended to benefit VLN pretraining; according to this paper, that approach did not fully connect vision and language modalities or exploit self-collected data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving vision-and-language navigation by generating future-view image semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Li & Bansal (2023) proxy-task pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretraining scheme with multiple proxy tasks including masked reconstruction of trajectory semantic information; aimed at improving VLN, but noted here as not successfully connecting vision and language or utilizing self-collected data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>proxy tasks over VLN data / semantic reconstruction objectives (paper mentions masked inputs and semantic reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Details not provided in this paper; described as using self-collected data and proxy tasks but failing to connect vision and language modalities effectively (per the authors' discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Standard VLN navigation tasks requiring mapping of language instructions to navigation actions in Matterport3D-based environments.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Proxy tasks involve reconstructing semantic/trajectory information (not explicit step-wise action supervision in all tasks); action semantics partially target trajectory-level information.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions learned during finetuning; proxy tasks are not full action predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Proxy reconstruction tasks attempt to teach trajectory/semantic prediction but do not provide an explicit, robust mapping from vision+language to actions according to this paper's critique.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Visual features and semantic/trajectory representations; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Proxy tasks that include trajectory/semantic reconstruction can in principle encourage path fidelity and representation alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>According to this paper, these proxy tasks 'fail to connect vision and language modalities' and therefore provide limited benefit for action-conditioned navigation; reliance on self-collected data without explicit action supervision limits effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proxy-task pretraining that lacks explicit, scalable action-supervision or fails to align vision and language modalities may not transfer effectively to action-generation demands of VLN; explicit action-conditioned pretraining (like MPM or PREVALENT) is preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Masked Path Modeling for Vision-and-Language Navigation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards learning a generic agent for vision-and-language navigation via pretraining. <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision. <em>(Rating: 2)</em></li>
                <li>Improving vision-and-language navigation with image-text pairs from the web. <em>(Rating: 2)</em></li>
                <li>Airbert: Indomain pretraining for vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>History aware multimodal transformer for vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>Improving vision-and-language navigation by generating future-view image semantics. <em>(Rating: 1)</em></li>
                <li>Masked trajectory models for prediction, representation, and control. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1703",
    "paper_id": "paper-258841747",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "CLIP",
            "name_full": "CLIP (Contrastive Language–Image Pretraining)",
            "brief_description": "A vision encoder pretrained on large image–text (caption) pairs using a contrastive objective; used in this work as a frozen visual feature extractor and reported to substantially improve downstream VLN performance versus ImageNet-pretrained encoders.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "use",
            "model_agent_name": "CLIP vision encoder (used as CLIP-ViT-L-336/14)",
            "model_agent_description": "A multimodal contrastively pretrained vision encoder that maps images to a joint image–text embedding space; here used as a frozen visual feature extractor (CLIP-ViT-L-336/14) whose outputs are linearly projected and fed into the VLN transformers.",
            "pretraining_data_type": "image-caption (image+text) web corpora",
            "pretraining_data_details": "Described in the paper as \"large image-caption data\" / web-scraped image-text pairs; specific datasets and sizes are not enumerated in this paper (original CLIP was trained on a very large, web-scale set of image-caption pairs).",
            "embodied_task_name": "Vision-and-Language Navigation (VLN) benchmarks (R2R / R4R / RxR)",
            "embodied_task_description": "3D photorealistic indoor navigation in Matterport3D-derived environments where an agent receives a language instruction and panoramic visual observations and must output a sequence of discrete navigation actions to reach a goal viewpoint.",
            "action_space_text": "N/A (CLIP is not an agent pretrained in a text-action environment; pretraining involved pairing images with captions)",
            "action_space_embodied": "Discrete navigation actions (panoramic view selection / step to next viewpoint; heading/elevation angles used for regression in some objectives)",
            "action_mapping_method": "Not applicable — CLIP provides visual representations that are consumed by the VLN model; action generation is learned by the cross-modal transformer on top of CLIP features during finetuning.",
            "perception_requirements": "RGB panoramic views (36 views per panorama in experiments); CLIP features extracted per view plus relative-angle encoding (sin/cos of heading and elevation).",
            "transfer_successful": true,
            "performance_with_pretraining": "Using CLIP as the vision encoder (HAMT+ baseline) improved performance relative to the original HAMT (ImageNet vision encoder): on RxR validation seen the HAMT+ baseline outperformed HAMT by 4.53% SR and 4.57 sDTW, and on RxR validation unseen by 5.5% SR and 4.42 sDTW (numbers reported in the paper as resulting from using CLIP features).",
            "performance_without_pretraining": "Original HAMT baseline (ImageNet-pretrained vision encoder) — used as comparison; exact HAMT baseline numeric values are reported in paper tables but vary per dataset/split (see tables in paper).",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Multimodal pretraining on image-text pairs provides stronger, more aligned visual features for language-conditioned tasks; representation alignment between vision and text reduces domain gap and improves downstream VLN learning.",
            "transfer_failure_factors": "CLIP does not by itself connect visual representations to action generation — action mapping must be learned during downstream finetuning; potential modality-gap remains for action-level control.",
            "key_findings": "Using an image–text pretrained vision encoder (CLIP) as a frozen feature extractor yields clear, measurable improvements for VLN over ImageNet-pretrained encoders, demonstrating that image-caption (language) pretraining transfers beneficially to 3D embodied navigation when representations are consumed by a downstream action-generating model.",
            "uuid": "e1703.0",
            "source_info": {
                "paper_title": "Masked Path Modeling for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PREVALENT",
            "name_full": "PREVALENT",
            "brief_description": "A VLN-focused pretraining approach that uses image–text–action triplets from VLN data with single-step action prediction and masked instruction modeling to integrate action generation into pretraining.",
            "citation_title": "Towards learning a generic agent for vision-and-language navigation via pretraining.",
            "mention_or_use": "use",
            "model_agent_name": "PREVALENT (VLN pretraining with image-text-action triplets)",
            "model_agent_description": "A pretraining scheme for VLN that trains models on in-domain image–text–action triplets using objectives such as single-step action prediction (classification) and masked instruction modeling to learn to predict actions given language and visual context.",
            "pretraining_data_type": "in-domain VLN data: image-text-action triplets (annotated instructions paired with trajectories/actions)",
            "pretraining_data_details": "Pretraining data consisted of available VLN datasets (human-annotated instruction-action pairs) and synthetic instruction data in prior work (PREVALENT uses VLN data and some synthesized data); exact sizes not reported here.",
            "embodied_task_name": "Vision-and-Language Navigation (VLN)",
            "embodied_task_description": "Agents are trained to follow natural-language instructions in Matterport3D-based indoor environments and output navigation actions to reach goal viewpoints.",
            "action_space_text": "In the pretraining setting, the action supervision is single-step discrete navigation actions (classification over candidate next viewpoints) and regression of heading/elevation for next-step actions.",
            "action_space_embodied": "Discrete panoramic navigation actions and optionally continuous heading/elevation regression (SAP/SAR objectives described).",
            "action_mapping_method": "Direct supervision: pretraining uses image-text-action triplets so the model learns a mapping from multimodal representations (instruction + visual context) to the discrete next-action label (single-step prediction) and to heading/elevation regression.",
            "perception_requirements": "Panoramic RGB visual observations (CLIP/ImageNet visual features), relative-angle encodings; pretraining uses the same modalities as downstream VLN.",
            "transfer_successful": true,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Pretraining on in-domain VLN image-text-action triplets provides direct action-supervised signals that reduce the gap between representation learning and action generation.",
            "transfer_failure_factors": "Scalability limited by scarcity of annotated instruction–action pairs; reliance on human-annotated or synthetic data that may be noisy or limited in quantity.",
            "key_findings": "Pretraining that explicitly includes action prediction (image–text–action triplets) helps connect learned representations to action generation, but scalability is constrained by the availability and quality of in-domain annotated instruction–action data.",
            "uuid": "e1703.1",
            "source_info": {
                "paper_title": "Masked Path Modeling for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BERT / XLM-R",
            "name_full": "BERT and XLM-R text encoders",
            "brief_description": "Large pretrained language models used to encode natural language instructions (BERT for English, XLM-R for multilingual instructions) before feeding embeddings into the VLN cross-modal transformer.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding.",
            "mention_or_use": "use",
            "model_agent_name": "BERT (English) / XLM-R (multilingual) text encoders",
            "model_agent_description": "Transformer-based language encoders pretrained on large text corpora (BERT on English corpora; XLM-R on multilingual corpora) that produce instruction embeddings used as inputs to the VLN cross-modal transformer.",
            "pretraining_data_type": "large text corpora (Wikipedia / web text for BERT; large multilingual corpora for XLM-R)",
            "pretraining_data_details": "Standard pretraining corpora used by these models (not enumerated in this paper); BERT/XLM-R are off-the-shelf pretrained language models providing strong text representations.",
            "embodied_task_name": "Vision-and-Language Navigation (VLN) — R2R / R4R / RxR",
            "embodied_task_description": "Language-conditioned 3D navigation where the encoded instruction conditions action generation across panoramic visual history and current observation.",
            "action_space_text": "Text pretraining involved language modeling objectives (masked language modeling), not action supervision; action space in pretraining is not applicable.",
            "action_space_embodied": "Discrete navigation actions in the 3D environment; action generation is learned during VLN finetuning using instruction embeddings from BERT/XLM-R.",
            "action_mapping_method": "No explicit mapping in pretraining; mapping is learned during downstream finetuning by the cross-modal transformer that consumes text embeddings and visual features to predict actions.",
            "perception_requirements": "RGB panoramic observations plus relative-angle encodings; text encoder supplies instruction grounding to visual representations.",
            "transfer_successful": true,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong pretrained language representations provide robust instruction encodings that facilitate grounding to vision and action during downstream VLN training.",
            "transfer_failure_factors": "Language-only pretraining does not teach action grounding; the agent still needs action-conditioned finetuning to map instructions to executable navigation behaviors.",
            "key_findings": "Pretrained language encoders (BERT / XLM-R) are standard and effective for encoding instructions in VLN, but by themselves do not supply action-generation capabilities — those are learned when combined with visual features and finetuned on VLN objectives.",
            "uuid": "e1703.2",
            "source_info": {
                "paper_title": "Masked Path Modeling for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Image-caption VLN pretraining (Majumdar / Guhur)",
            "name_full": "Web-scale image–caption pretraining applied to VLN (e.g., Majumdar et al., Guhur et al.)",
            "brief_description": "Prior works pretrained vision–language models on web-scraped image–caption corpora (e.g., Conceptual Captions) to learn general vision–language representations and then finetuned them on VLN, improving transfer but not directly learning action generation.",
            "citation_title": "Improving vision-and-language navigation with image-text pairs from the web.",
            "mention_or_use": "mention",
            "model_agent_name": "Vision–language models pretrained on image–caption corpora (e.g., Conceptual Captions)",
            "model_agent_description": "Models pretrained on large-scale image-caption datasets to learn aligned vision-text representations; subsequently finetuned on in-domain VLN data for navigation tasks.",
            "pretraining_data_type": "web-scraped image–caption datasets (image + text captions)",
            "pretraining_data_details": "Examples mentioned include Conceptual Captions and other web-scraped image-caption corpora; the paper does not list sizes here but describes them as web-scale image–text data.",
            "embodied_task_name": "Vision-and-Language Navigation (VLN)",
            "embodied_task_description": "Instruction-conditioned 3D navigation in Matterport3D-derived environments requiring grounding of language to visual scenes and planning/executing navigation actions.",
            "action_space_text": "Pretraining on image-caption corpora involves captions (language supervision), not action labels; action semantics are absent during pretraining.",
            "action_space_embodied": "Discrete navigation actions (panoramic candidate selection) learned during finetuning.",
            "action_mapping_method": "Action mapping occurs during downstream finetuning: the pretrained vision–language representations are adapted and connected to action predictors in the VLN model, rather than explicit mapping present in pretraining.",
            "perception_requirements": "RGB panoramic views converted to pretrained visual features; language modality via captions/instructions.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported in literature and discussed in this paper as producing improvements when finetuned on VLN, but the present paper does not report specific numeric gains for these exact prior methods.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale aligned image–text pretraining yields vision–language representations that are more directly adaptable to language-conditioned embodied tasks, reducing representation-domain gap.",
            "transfer_failure_factors": "Lack of action-level supervision in pretraining means these models must still learn to map representations to actions during finetuning; domain mismatch between web images/captions and indoor VLN panoramas can remain.",
            "key_findings": "Pretraining on web-scale image–caption corpora improves vision–language alignment and helps downstream VLN after finetuning, but such pretraining does not by itself teach action generation or fully close the domain gap to in-domain VLN data.",
            "uuid": "e1703.3",
            "source_info": {
                "paper_title": "Masked Path Modeling for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Li & Bansal (2023) proxy tasks",
            "name_full": "Li and Bansal (2023) — proxy pretraining tasks for VLN",
            "brief_description": "A recent work proposing three proxy tasks (one reconstructs trajectory semantic information from masked inputs) intended to benefit VLN pretraining; according to this paper, that approach did not fully connect vision and language modalities or exploit self-collected data.",
            "citation_title": "Improving vision-and-language navigation by generating future-view image semantics.",
            "mention_or_use": "mention",
            "model_agent_name": "Li & Bansal (2023) proxy-task pretraining",
            "model_agent_description": "Pretraining scheme with multiple proxy tasks including masked reconstruction of trajectory semantic information; aimed at improving VLN, but noted here as not successfully connecting vision and language or utilizing self-collected data.",
            "pretraining_data_type": "proxy tasks over VLN data / semantic reconstruction objectives (paper mentions masked inputs and semantic reconstruction)",
            "pretraining_data_details": "Details not provided in this paper; described as using self-collected data and proxy tasks but failing to connect vision and language modalities effectively (per the authors' discussion).",
            "embodied_task_name": "Vision-and-Language Navigation (VLN)",
            "embodied_task_description": "Standard VLN navigation tasks requiring mapping of language instructions to navigation actions in Matterport3D-based environments.",
            "action_space_text": "Proxy tasks involve reconstructing semantic/trajectory information (not explicit step-wise action supervision in all tasks); action semantics partially target trajectory-level information.",
            "action_space_embodied": "Discrete navigation actions learned during finetuning; proxy tasks are not full action predictors.",
            "action_mapping_method": "Proxy reconstruction tasks attempt to teach trajectory/semantic prediction but do not provide an explicit, robust mapping from vision+language to actions according to this paper's critique.",
            "perception_requirements": "Visual features and semantic/trajectory representations; specifics not provided here.",
            "transfer_successful": false,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Proxy tasks that include trajectory/semantic reconstruction can in principle encourage path fidelity and representation alignment.",
            "transfer_failure_factors": "According to this paper, these proxy tasks 'fail to connect vision and language modalities' and therefore provide limited benefit for action-conditioned navigation; reliance on self-collected data without explicit action supervision limits effectiveness.",
            "key_findings": "Proxy-task pretraining that lacks explicit, scalable action-supervision or fails to align vision and language modalities may not transfer effectively to action-generation demands of VLN; explicit action-conditioned pretraining (like MPM or PREVALENT) is preferable.",
            "uuid": "e1703.4",
            "source_info": {
                "paper_title": "Masked Path Modeling for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards learning a generic agent for vision-and-language navigation via pretraining.",
            "rating": 2,
            "sanitized_title": "towards_learning_a_generic_agent_for_visionandlanguage_navigation_via_pretraining"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision.",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Improving vision-and-language navigation with image-text pairs from the web.",
            "rating": 2,
            "sanitized_title": "improving_visionandlanguage_navigation_with_imagetext_pairs_from_the_web"
        },
        {
            "paper_title": "Airbert: Indomain pretraining for vision-and-language navigation.",
            "rating": 2,
            "sanitized_title": "airbert_indomain_pretraining_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "History aware multimodal transformer for vision-and-language navigation.",
            "rating": 2,
            "sanitized_title": "history_aware_multimodal_transformer_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "Improving vision-and-language navigation by generating future-view image semantics.",
            "rating": 1,
            "sanitized_title": "improving_visionandlanguage_navigation_by_generating_futureview_image_semantics"
        },
        {
            "paper_title": "Masked trajectory models for prediction, representation, and control.",
            "rating": 1,
            "sanitized_title": "masked_trajectory_models_for_prediction_representation_and_control"
        }
    ],
    "cost": 0.0191805,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Masked Path Modeling for Vision-and-Language Navigation</p>
<p>Zi-Yi Dou zdou@cs.ucla.edu 
University of California
Los Angeles ♯ Amazon Alexa AI</p>
<p>Feng Gao fenggo@amazon.com 
University of California
Los Angeles ♯ Amazon Alexa AI</p>
<p>Nanyun Peng 
University of California
Los Angeles ♯ Amazon Alexa AI</p>
<p>Masked Path Modeling for Vision-and-Language Navigation
B5140D66A5F23622E9759D8365F3B797
Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments based on natural language instructions.A major challenge in VLN is the limited available training data, which hinders the models' ability to generalize effectively.Previous approaches have attempted to alleviate this issue by using external tools to generate pseudolabeled data or integrating web-scaled imagetext pairs during training.However, these methods often rely on automatically-generated or out-of-domain data, leading to challenges such as suboptimal data quality and domain mismatch.In this paper, we introduce a masked path modeling (MPM) objective.MPM pretrains an agent using self-collected data for subsequent navigation tasks, eliminating the need for external tools.Specifically, our method allows the agent to explore navigation environments and record the paths it traverses alongside the corresponding agent actions.Subsequently, we train the agent on this collected data to reconstruct the original action sequence when given a randomly masked subsequence of the original path.This approach enables the agent to accumulate a diverse and substantial dataset, facilitating the connection between visual observations of paths and the agent's actions, which is the foundation of the VLN task.Importantly, the collected data are in-domain, and the training process avoids synthetic data with uncertain quality, addressing previous issues.We conduct experiments on various VLN datasets and demonstrate the applications of MPM across different levels of instruction complexity.Our results exhibit significant improvements in success rates, with enhancements of 1.3%, 1.1%, and 1.2% on the val-unseen split of the Room-to-Room, Room-for-Room, and Room-across-Room datasets, respectively.Additionally, we underscore the adaptability of MPM as well as the potential for additional improvements when the agent is allowed to explore unseen environments prior to testing. 1 PlusLabNLP/mpm.</p>
<p>Vsion-and-Language Navigation Masked Path Modeling</p>
<p>Figure 1: Vision-and-language navigation agents are trained to predict actions in a real-world environment given a natural language instruction.We incorporate our proposed masked path modeling objective into training, where the agent is trained to reconstruct the original action sequence when given a randomly masked subsequence of the visual inputs of the original path.The figure is adapted from Anderson et al. (2018b).</p>
<p>Introduction</p>
<p>A vision-and-language navigation (VLN) agent is trained to follow natural language instructions and navigate within an environment to achieve a specified goal.This task requires the agent to possess several sophisticated abilities, including understanding and grounding language phrases to visual objects, as well as planning and executing actions in a real-world setting.</p>
<p>The pretraining-then-finetuning paradigm (Peters et al., 2018;Devlin et al., 2019;Chen et al., 2021b;He et al., 2022) has proven to be effective in addressing these challenges in the field of VLN.By utilizing various supervision signals and proposing pretraining objectives, significant improvements have been demonstrated across VLN tasks.Prior works have explored the use of internetscale image-text datasets to acquire grounded vision and language representations.Notable contributions in this area include the works of Majumdar et al. (2020) and Guhur et al. (2021), who leverage web-scraped image-caption corpora to learn general vision-language representations and then finetune the models using in-domain VLN data to adapt the representations specifically for VLN tasks.Similarly, Shen et al. (2022) and Khandelwal et al. (2022) employ the CLIP vision encoder (Radford et al., 2021) pretrained with image-text contrastive loss and showcase the application of such models in various embodied tasks.</p>
<p>While the vision-language pretrained representations improve the alignment between vision and language modalities, it is important to note that there is a domain gap between the pretraining data and VLN data.In addition, these models are unaware of how to connect the learned representations to actions because they are not explicitly trained for generating actions, which is a critical skill for VLN tasks.To address the issue and integrate action generation into pretraining, prior works such as PREVALENT (Hao et al., 2020) HAMT (Chen et al., 2021a) use a single-step action prediction objective based on human-annotated and synthetic image-text-action triplets.However, the scalability of the pretraining objective is limited due to the scarcity of annotated instruction-action pairs.Specifically, they require training on pairs of natural language instruction and action sequences which are costly to obtain in a scalable way.There is also a line of work that utilizes synthetic data for training, either obtaining pseudo-instructions from sampled paths (Fried et al., 2018b;Tan et al., 2019;Hao et al., 2020;Wang et al., 2022b) or generating both the visual environments as well as the language instructions (Kamath et al., 2023;Wang et al., 2023).However, the automatically-generated data cannot be perfect and the noise during data generation can impact the model performance.</p>
<p>In this paper, we present an approach to pretrain VLN models with masked path modeling (MPM), which leverages in-domain path data collected by an agent for self-supervised learning.The proposed objective targets addressing the two major limitations of previous work:</p>
<p>• It collects scalable in-domain pretraining data without data synthesis.During the pretraining phase, the VLN agent explores the environment randomly and gathers navigation paths along with its actions, which are then used for pretraining the agent.Because the agent actively explores different environments, we can collect a rich amount of diverse paths.In addition, the MPM objective only requires the collected path and action information for training, eliminating the need for synthesizing additional signals.</p>
<p>• It explicitly focuses on conditional action generation.Concretely, as shown in Figure 1, to construct the MPM objective, we randomly mask certain viewpoints in the self-collected paths and train the agent to reconstruct the original paths based on the masked ones.MPM is similar to the VLN objective because the agent is trained to output a sequence of actions given specific instructions, with the distinction that the instructions are presented as masked paths rather than natural language instructions.Consequently, MPM effectively prepares the agent for VLN tasks that require conditioned action generation skills.</p>
<p>As a result, our pretraining objective is scalable and well-suited for addressing the VLN task.</p>
<p>We evaluate the proposed method on various VLN datasets with different types of instructions, including Room-to-Room (Anderson et al., 2018c), Room-for-Room (Jain et al., 2019), and Roomacross-Room (Ku et al., 2020).Experimental results demonstrate that MPM can achieve significant improvements in both seen and unseen environments compared with strong baselines.For example, we achieve improvements of 1.32%, 1.05%, and 1.19% on the val-unseen split of the Room-to-Room, Room-for-Room, and Room-across-Room datasets, respectively.In addition, we demonstrate the MPM can be flexibly integrated into different types of models and achieve improvements.Furthermore, an analysis reveals the potential for additional improvements when the agent is allowed to explore unseen environments prior to testing.</p>
<p>Methods</p>
<p>In this section, we first introduce the basic settings and model architecture of vision-and-language navigation, then illustrate the details of each component of our proposed approach.</p>
<p>Background</p>
<p>Training Data.The training data D of VLN consists of parallel instruction-action pairs {(i k , a k )} from different environments.However, it is hard to manually annotate a large amount of instruction-action data for VLN.Therefore, researchers have proposed various data augmentation strategies (Fried et al., 2018b;Tan et al.,  In VLN, the model separately encodes the given instruction with a text encoder, its past history with a hierarchical visual encoder, and its current observations with another vision encoder; the encoded representations are then fed into a joint cross-modal transformer to predict the final action.In MPM, we directly feed a masked subpath to the cross-modal transformer instead of a language instruction and the model is trained to predict the original action given the masked subpath.</p>
<p>All the parameters between VLN and MPM are shared.</p>
<p>2019; Li et al., 2022;Kamath et al., 2023) to provide additional supervision for VLN models.Int this work, we follow a common practice to use a speaker model (Fried et al., 2018b) to generate language instructions given randomly sampled paths and enrich the training data with the generated instruction-action pairs.Specifically, we incorporate the PREVALENT-generated data (Hao et al., 2020) into training.</p>
<p>Base Settings.Using the parallel instructionaction pairs D, a VLN agent is trained to follow a given language instruction and generate a sequence of actions to reach the final destination in a photorealistic environment.Formally, in a given environment e, the navigation agent parameterized by θ learns to model the distribution P (a|i, e; θ), where i and a denote instruction and action variables, respectively.</p>
<p>Model Architecture.In this paper, we employ a history-aware multimodal transformer architecture design following Chen et al. (2021a) as it achieves strong VLN performance across datasets, although it should be noted that our approach is compatible with most existing model architectures in VLN.</p>
<p>Overall, as shown in Figure 2, at each action prediction step, we have a transformer text encoder to encode the given language instruction, a hierarchical vision transformer to encode all the past observations of the agent, and another vision transformer to encode the agent panoramic observation of the current step; then, the three types of representations will be concatenated together and fed into a cross-modal transformer for joint encoding, and the final pooled representation is used for action prediction.</p>
<p>Text Features.Following previous work (Chen et al., 2021a), we use pretrained text encoder to encode the language instructions.We use the standard BERT model (Devlin et al., 2019) to encode the English instructions for the R2R and R4R datasets (Anderson et al., 2018c;Jain et al., 2019) and the XLM-R model (Conneau et al., 2020) to encode the multilingual instructions for the RxR dataset (Ku et al., 2020).</p>
<p>Vision Features.At each step, the agent is given a panoramic observation of its current position in the environment, denoted as {v i } K i=1 .For each view in the panoramic observation, its vision feature is first extracted using a pretrained vision encoder.While many of the previous methods (Anderson et al., 2018b;Chen et al., 2021a) use vision encoders pretrained on ImageNet (Fei-Fei et al., 2009) for image classification, we find that CLIP vision encoder (Radford et al., 2021) achieves stronger performance, which is consistent with the findings of Shen et al. (2022).Therefore, we choose to use CLIP to first extract vision features and then the CLIP features are fed into the transformers for history and observation encoding.</p>
<p>For the current observations, in addition to the CLIP features, we also feed the model with the relative angle of each view v i in the panoramic observation, represented as REL(v i ) = (sinθ i , cosθ i , sinϕ i , cosϕ i ) where θ i and ϕ i are the relative heading and elevation angle to the agent's orientation, respectively.The combined representations {[CLIP(v i ); REL(v i )]} K i=1 are then fed into a transformer to obtain K processed representations.</p>
<p>History Features.The model also keeps track of its past observations with a hierarchical vision transformer, where the panoramic observation at each step is first encoded by a single vector with a vision transformer, and all the panoramic representations are jointly encoded with another transformer along the temporal dimension.We refer to Chen et al. (2021a) for details.</p>
<p>Cross-Modal Interactions.The history features and the current observation features are concatenated as the vision modality, and a dual-stream cross-modal fusion architecture is used to encode both the vision and text modalities and allow for cross-modal information exchange.At each layer, we have a self-attention block for inter-modal interactions and a cross-attention block for vision-text interactions.</p>
<p>Masked Path Modeling</p>
<p>In this part, we illustrate the main idea of our proposed masked path modeling method.We go through the details of the active data collection, model architecture, and training strategies.</p>
<p>General Framework.Masked path modeling is inspired by the masked data modeling pretraining methods in the language and vision communities (Devlin et al., 2019;He et al., 2022), where the general idea is that the model is trained to reconstruct an original input (e.g., a sentence or an image) given parts of the input masked.In VLN, we propose to first ask an agent to perform a sequence of actions and collect a path consisting of several viewpoints p = ⟨p 1 , p 2 , • • • , p n ⟩, then we mask x%2 of the viewpoints in this path and feed the observations along the masked path p m = ⟨p m 1 , p m 2 , • • • , p m k ⟩ to the agent and the agent is trained to perform the same sequence of actions as before to reconstruct the original path p given p m .Note that different from the common masked data modeling methods, the input and output modalities are different in MPM.Specifically, the model inputs are visual observations of the environment while the model outputs are the agent actions.This design can explicitly train the model to connect vision inputs and action outputs, which forms the foundation of the VLN task.</p>
<p>Data Collection.One of the major bottlenecks of VLN is the lack of training data and it is hard to collect large-scale in-domain data for VLN.In masked path modeling, however, the agent can actively collect a great amount of data given an environment for training.During the data collection period, we ask the agent to randomly choose the next viewpoint with equal probabilities at each step.Also, we keep track of all the visited viewpoints, and the agent is not allowed to visit the same viewpoint twice.To control the length of the paths, we utilize pre-computed statistics from the training data regarding agent paths.We then randomly select paths, ensuring that each path length is sampled according to the distribution of path lengths observed in the training data.More sophisticated path collection techniques such as using techniques to encourage the diversity of sampled paths may also be used but here we leave it as future work.During path masking, we ensure that the last viewpoint is not masked so that the agent is always aware of the goal viewpoint.</p>
<p>Model Architecture for MPM.As in Figure 2, the main difference between masked path modeling and the VLN objective is that the input of masked path modeling is a sequence of visual observations instead of a natural language instruction.Therefore, we employ the same architecture as the original HAMT model except that we perform a linear transformation on the CLIP-encoded visual features so as to match the input and model dimensions, and then directly feed transformed features to the crossmodal transformer module.While collecting the visual features along a masked path, we do not use the panoramic view but only the view that the agent currently faces so as to make the pretraining task harder. 3All the module parameters are shared between the masked path modeling and VLN objectives.</p>
<p>Training Strategies.We include our masked path modeling objective into the pretraining and finetuning stages of HAMT (Chen et al., 2021a).Concretely, during pretraining, the agent is jointly pretrained with masked path modeling and standard objectives including masked language modeling and instruction trajectory matching.We also include single-step action prediction and regression (SAP/SAR), and spatial relationship prediction (SPREL) objectives as in Chen et al. (2021a Table 2: Results on the Room-for-Room dataset (Jain et al., 2019).MPM can also improve the model performance in this setting across all the evaluation metrics.The best scores are in bold.</p>
<p>The SAP and SAR objectives ask the model to predict the next action based on instruction, history from the ground-truth demonstration, and the current observation with imitation learning, where SAP formulates the task as a classification task while SAR trains the model to regress the action heading and elevation angles.The SPREL objective trains the model to predict the relative spatial position of two views in a panorama based on only visual features, angle features, or both types of features.We refer readers to Chen et al. (2021a) for more details.</p>
<p>Then, during finetuning, the model is jointly trained with both masked path modeling and the VLN objective with equal loss weights. 5We combine the Asynchronous Advantage Actor-Critic (A3C) reinforcement learning objective (Mnih et al., 2016) and imitation learning objective for the VLN objective following previous work (Tan et al., 2019;Chen et al., 2021b), but only use the imitation learning objective for masked path modeling because it is stable and also it is non-trivial to design step-wise rewards in this setting.</p>
<p>Experiments</p>
<p>In this section, we present our experimental results with the proposed masked path modeling objective.</p>
<p>image classification model while our vision encoder is CLIP.Also, the CLIP vision encoder is frozen during pretraining to save computation time. 5We did not see significant performance differences when tuning the loss weights in our preliminary studies.</p>
<p>Settings</p>
<p>We go through the experimental settings in this part, including our used datasets, evaluation metrics, and implementation details.</p>
<p>Datasets</p>
<p>We evaluate the models on different types of VLN datasets, including the Room-to-Room (R2R) (Anderson et al., 2018c), Room-for-Room (R4R) (Jain et al., 2019) and Room-across-Room (RxR) (Ku et al., 2020) datasets.</p>
<p>R2R.The R2R dataset is built based on Matter-port3D (Chang et al., 2017) and has 7,189 paths, with each path paired with 3 different English instructions and the average length of all the paths is 29.R2R is split into training, validation, and test sets; the validation set consists of two splits: 1) val-seen, where all the paths are sampled from environments that are also seen in the training set, and 2) val-unseen, where paths are sampled from environments that do not appear in the training set so as to test the generalization ability of agents.The paths in the test set are from new environments unseen in the training and validation sets.</p>
<p>R4R.The R4R dataset is an algorithmically produced extension of R2R that concatenates two adjacent tail-to-head paths in R2R as well as their corresponding instructions to form a new instructionpath pair.With this extension, R4R has longer paths and instructions, and the paths are not always the shorted path from the starting point to the goal, making the dataset less biased than R2R.</p>
<p>Model Validation Seen</p>
<p>Validation Unseen Test Unseen SR↑ SPL↑ nDTW↑ sDTW↑ SR↑ SPL↑ nDTW↑ sDTW↑ SR↑ SPL↑ nDTW↑ sDTW↑ HAMT (Chen et al., 2021a)  RxR.The RxR dataset follows the same environment division as that in the R2R dataset.Different from R2R, RxR is a larger dataset that has 16,522 paths in total.In addition, the instructions are multilingual and in three languages, including English, Hindi, and Telugu.The lengths of the instructions in RxR are also much larger than that in R2R (average length: 78 vs. 29).</p>
<p>Evaluation Metrics</p>
<p>We adopt the standard evaluation metrics in VLN (Anderson et al., 2018a) to evaluate models.Specifically, we evaluate models with 1) trajectory lengths (TL): the length of the agent path measured in meters; 2) navigation error (NE): the average distance between the final position of agents and the goal position measured in meters; 3) success rate (SR): the proportion of agents whose final position is within three meters of the target; 4) success rate weighted by normalized inverse path length (SPL): success rate normalized by the ratio between the length of the shortest path and the predicted path.</p>
<p>Because the above metrics are heavily biased towards whether or not the agent can reach the goal position while ignoring the specific path the agents take, Jain et al. (2019) propose the coverage weighted by length score (CLS) metric that measures the path fidelity between the predicted path and target path for the R4R dataset.Similarly, Ku et al. (2020) propose normalized dynamic time warping (nDTW) and success rate weighted by dynamic time warping (sDTW) (Magalhães et al., 2019) for RxR.</p>
<p>Implementation Details</p>
<p>Model Architecture.We build our models upon the HAMT model (Chen et al., 2021a) and follow all of its parameter settings except that we use CLIP-ViT (Radford et al., 2021) pretrained vision encoder and it is not finetuned during training.Specifically, our model consists of a 9-layer text transformer, a 2-layer panoramic transformer for encoding history information, and a 4-layer transformer for cross-modal encoding.In each panoramic observation, there are K = 36 views of images and we use CLIP-ViT-L-336/14 to encode the input images.We denote the HAMT baseline with CLIP-ViT-L-336/14 as HAMT+.</p>
<p>Pretraining.During pretraining, we randomly select proxy tasks including masked path modeling for each mini-batch with a predefined ratio as in Chen et al. (2021a).Different from Chen et al. (2021a), the CLIP-ViT is frozen instead of finetuned during both pretraining and finetuning in order to save computational costs.We train the model for 200k iterations with the AdamW optimizer (Loshchilov and Hutter, 2018) and the learning rate is set to 5e-5 and the batch size is set to 64.It take around 1 day to finish training on 4 NVIDIA Tesla V100 GPUs.</p>
<p>Finetuning.During finetuning, the model is jointly finetuned with the IL+RL and masked path modeling objectives with equal weights.The model is fine-tuned for 300k iterations with a learning rate of 1e-5 and batch size of 8 on a single V100 GPU, taking around 2.5 days to finish. 6The best model is selected according to performance on the val unseen split.We use the same augmented data as Hong et al. (2021) following previous work for the R2R dataset, while no augmented data is used for other datasets.Greedy search is applied in inference following the single-run setting.In  both pretraining and finetuning, the agent samples a batch of paths for MPM from the available environments in Matterport3D and we do not allow the agents to explore test environments.</p>
<p>Main Results</p>
<p>The main results of the baselines and our model are listed in Table 1, 2, and 3. We report both the numbers in the HAMT paper (Chen et al., 2021a) and our reproduced performance.First, it should be noted that because we use the strong CLIP vision encoder, our reproduced HAMT+ baseline can achieve better performance than the original HAMT paper across settings, even surpassing the state-of-the-art end-to-end trained model (Li and Bansal, 2023).Especially, on the RxR datasets, our HAMT+ outperforms HAMT by 4.53% and 4.57 sDTW on the validation seen split and 5.5% success rate and 4.42 sDTW on the validation unseen split.</p>
<p>Built upon a strong baseline, our model can still outperform it across settings.Notably, the performance gains are pronounced when measured with path fidelity metrics (i.e., CLS, nDTW, sDTW) on the long-horizon VLN datasets R4R and RxR in the seen environments, indicating that the masked path modeling objective can encourage the models to faithfully follow the natural language instructions and complete the paths accordingly.This is intuitive as the VLN training objectives can optimize the models towards taking the shortest path to reach the final goal, whereas during masked path modeling, the model is trained to reconstruct the original paths, thus the model can follow the instructions more faithfully.</p>
<p>In unseen environments, our model achieves 1.32%, 2.13%, and 1.51% improvements over the baseline in success rates on the R2R, R4R, and RxR datasets respectively on the validation set, demonstrating the effectiveness of our approach.We attribute these improvements to the fact that our objective allows the model to be trained on a variety of paths and can thus improve the generalization ability of the model in unseen environments.</p>
<p>Analysis</p>
<p>In this part, we perform several analyses to gain insights into MPM.We leave more analysis results in Appendix.</p>
<p>Exploring Unseen Environments.Because we allow the agents to autonomously acquire data and learn without the need for annotated data, we can train the agents in a scalable way.We hypothesize that when trained with masked path modeling, the agent can be familiarized with the explored environments and thus improve its navigation performance, even though it is not explicitly trained with the VLN objective.To verify this, we train the model with masked path modeling on unseen environments in the validation sets with the same hyper-parameters as before and test its VLN performance.As shown in Table 4, performing masked path modeling on unseen environments can significantly improve the model navigation performance, demonstrating the potential of using the objective in a large-scale setting.Especially, exploring unseen environments can bring 4.46% and 3.56 improvements in SR and sDTW on the R4R validation unseen set respectively.</p>
<p>Path Design.When collecting the paths, we make the lengths of the sampled paths follow the distribution of that in the training data so that the paths are similar to the navigation paths.As shown in Table 5, if we switch the path designs between R2R and R4R, the performance gains will drop marginally, indicating that making the paths between MPM and VLN similar can best utilize the MPM objective for VLN.We leave more sophisticated path designs as future work.</p>
<p>MPM during Pretraining.During the pretraining stage, we follow HAMT to train the models with masked language modeling, instruction trajectory matching, single-step action prediction and regression, and spatial relationship prediction tasks.</p>
<p>We also choose to include the masked path modeling objective during pretraining so as to mitigate the difference between pretraining and finetuning.</p>
<p>As shown in Table 6, we can see that including masked path modeling is important as it can well prepare the models for the finetuning stage, although only doing masked path modeling during finetuning can also bring marginal improvements.Notably, not including MPM during pretraining seems to achieve comparable or even better performance than including it on R4R.One possible explanation is that during pretraining the path lengths are similar to those of R2R, thus the pretrained agent may be more suitable for R2R than R4R.</p>
<p>Applications in Other Models and Datasets.</p>
<p>Previously, we mainly experiment with the endto-end HAMT model on navigation tasks with finegrained language instructions.In this part, we implement MPM upon the strong DUET model (Chen et al., 2022).As shown in</p>
<p>Related Work</p>
<p>We overview three lines of related research, including vision-and-language navigation in general, vision-and-language pretraining with a focus on its applications in VLN, as well as pretraining for control and embodied learning.</p>
<p>Vision-and-Language Navigation.Building vision-and-language navigation models has received increasing attention in recent years (Anderson et al., 2018b;Fried et al., 2018a;Wang et al., 2018;Li et al., 2019b;Zhu et al., 2020b;Kurita and Cho, 2021) and various benchmarks have been proposed to evaluate the ability of embodied agents to follow instructions and accomplish specified tasks (Kolve et al., 2017;Anderson et al., 2018a;Savva et al., 2019;Anderson et al., 2018c;Chen et al., 2019;Ku et al., 2020;Shridhar et al., 2020;Padmakumar et al., 2022) (2019) propose to mix imitation learning and A3C (Mnih et al., 2016) and increase the diversity of the synthesized data by adding noise into the environments during data generation.To utilize additional training signals, Ma et al. (2019) propose the self-monitoring agent that improves vision-language alignment with a co-grounding module and progress monitor; Zhu et al. (2020a) propose four self-supervised auxiliary tasks that are beneficial for the task of VLN.There are also works on designing better architectures (Chen et al., 2021a(Chen et al., , 2022) ) for VLN.In terms of data augmentation, in addition to methods (Fried et al., 2018b;Tan et al., 2019;Hao et al., 2020;Dou and Peng, 2022) that synthetic language instructions are generated from randomly sampled paths, there are also works on synthesizing both the environments and the instructions (Wang et al., 2022b;Kamath et al., 2023).However, most of these works require external models to generate imperfect data, which can potentially harm the performance of VLN models.</p>
<p>Vision-and-Language Pretraining.Pretraining models on large web corpora have proven to be highly effective in natural language processing (Peters et al., 2018;Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020), and similar techniques have been applied in computer vision (Chen et al., 2020a(Chen et al., , 2021b;;He et al., 2022;Bao et al., 2022) and vision-language communities (Li et al., 2019a;Chen et al., 2020b;Radford et al., 2021;Kim et al., 2021;Li et al., 2021;Dou et al., 2022).In the field of VLN, researchers have tried to use unimodally or multimodally pretrained language or vision representations (Anderson et al., 2018b;Li et al., 2019b).Notably, the CLIP vision encoder (Radford et al., 2021) pretrained on large image-caption data has proven to be generally effective for vision-andlanguage embodied tasks (Khandelwal et al., 2022;Shen et al., 2022).To jointly learn transferrable vision and language representations for VLN, Majumdar et al. (2020) and Guhur et al. (2021) propose to first pretrain models on large image-caption data such as Conceptual Captions (Sharma et al., 2018) and then adapt the representations for VLN tasks by finetuning the model on in-domain VLN data.While the pretrained representations can be useful, the pretraining process does not explicitly connect the learned representations to output actions.To further integrate action generation into VLN pretraining, researchers have attempted to directly use VLN data for pretraining.For example, PREVALENT (Hao et al., 2020) is pretrained on image-text-action triplets with a single-step action prediction objective and masked instruction modeling objective; Chen et al. (2021a) further propose a single-step regression and spatial relationship prediction objective that introduces more supervisions.However, the pretraining data is limited by the size of VLN data and thus it is difficult to apply their approaches in large-scale settings; Li and Bansal (2023) propose three proxy tasks, one of which is to reconstruct the trajectory semantic information given masked inputs, but they fail to connect vision and language modalities and train on self-collected data.</p>
<p>Pretraining for Control and Embodied Learning.The general idea of pretraining on largescale data has been adopted in embodied tasks, including data collection (Burda et al., 2019), representation learning (Yang and Nachum, 2021), and world model learning (Seo et al., 2023).For example, Pathak et al. (2017) present a curiositydriven self-supervised data collection approach that encourages agents to explore unfamiliarized states; Nair et al. (2022) pretrain representations on human ego-centric video data and adapt the representations on robotic tasks.In terms of using self-collected data for training, Chaplot et al. (2021) propose to build 3D semantic maps and use them to improve action and perception skills.Similar to our work, Wu et al. (2023) propose to pretrain models by reconstructing a full trajectory or given a random subset of the same trajectory, and the pretrained models can be used for different downstream purposes like inverse dynamics, forward dynamics, imitation learning, offline RL, and representation learning.Different from these works, our method explicitly optimizes models for conditioned action generation and the agent can self-collect a rich amount of data for pretraining.</p>
<p>Conclusion</p>
<p>In this paper, we propose masked path modeling, a pretraining objective designed for vision-andlanguage navigation.The objective can utilize scalable data explored from different environments to improve the agent's conditioned action generation ability.We incorporate the objective into a strong baseline and demonstrate improvements across different settings.An analysis further reveals the potential for scaling the objective to large-scale settings.Future directions include designing better exploration strategies as well as investigating applications in more fields.</p>
<p>Limitations</p>
<p>MPM is designed to connect vision and action modalities, while focusing less on the language modality.In addition, we constrain the agent to collect paths from the available environments in Matterport3D, which can limit the diversity of the paths to some degree.We also focus solely on VLN settings and do not investigate its applications in other embodied instruction following tasks.</p>
<p>Figure 2 :
2
Figure 2: We follow Chen et al. (2021a) to design the base model architecture.In VLN, the model separately encodes the given instruction with a text encoder, its past history with a hierarchical visual encoder, and its current observations with another vision encoder; the encoded representations are then fed into a joint cross-modal transformer to predict the final action.In MPM, we directly feed a masked subpath to the cross-modal transformer instead of a language instruction and the model is trained to predict the original action given the masked subpath.All the parameters between VLN and MPM are shared.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Qualitative results for the instruction "Walk across patio, stop at hanging basket chair."</p>
<p>Table 1 :
1
(Anderson et al., 2018c)93 3.34 67.05 61.69 12.70 3.57 67.19 61.94  HAMT+ w/ MPM  10.86 2.43 76.30 72.85 11.99 3.44 68.37 62.59 12.54 3.47 67.79 62.54Results on the Room-to-Room dataset(Anderson et al., 2018c).We incorporate MPM into a strong baseline (HAMT+) and achieve significant improvements across settings.The best scores are in bold.
ModelValidation Seen TL NE↓ SR↑ SPL↑ TL NE↓ SR↑ SPL↑ TL NE↓ SR↑ SPL↑ Validation Unseen Test UnseenHAMT (Chen et al., 2021a) HAMT w/ Li and Bansal (2023) HAMT+ 11.11 2Model 11.15 2.51 --Validation Seen 76 72 --NE↓ SR↑ CLS↑ nDTW↑ sDTW↑ NE↓ SR↑ CLS↑ nDTW↑ sDTW↑ 11.46 2.29 66 61 12.27 3.93 65 60 --68 62 --65 60 Validation UnseenHAMT (Chen et al., 2021a) HAMT+ HAMT+ w/ MPM-4.62 57.29 67.97 --4.29 59.13 70.50-61.01 64.88-41.96 48.286.09 44.6 5.90 44.75 61.84 57.7 5.65 46.88 62.7650.3 54.18 55.2331.8 33.89 35.50</p>
<p>Table 3 :
3
(Ku et al., 2020)om-across-Room dataset(Ku et al., 2020).The best scores are in bold.
HAMT+ HAMT+ w/ MPM59.4 58.9 63.93 59.93 68.59 55.47 62.00 58.05 67.52 53.87 65.3 50.9 56.5 56.0 63.1 48.3 53.12 46.62 59.94 45.19 ----67.73 63.89 71.02 58.86 63.51 59.24 67.71 54.53 60.00 52.52 63.97 51.13ModelR2R Validation Unseen TL NE↓ SR↑ SPL↑ NE↓R4R Validation Unseen SR↑ CLS↑ nDTW↑ sDTW↑HAMT+ w/ MPM HAMT+ w/ MPM-Prexplore 11.37 3.33 69.60 64.69 5.13 51.34 63.72 11.99 3.44 68.37 62.59 5.65 46.88 62.7655.23 57.9535.50 39.06</p>
<p>Table 4 :
4
Pre-exploring the test environments with MPM can further improve the model performance.</p>
<p>Table 5 :
5
MPM performs the best when its collected paths resemble the paths of test environments.Here we only control the lengths of the paths to be similar to the paths of either R2R or R4R.
ModelMPM PT FTTLR2R Validation Unseen NE↓ SR↑ SPL↑ NE↓SR↑R4R Validation Unseen CLS↑ nDTW↑ sDTW↑HAMT+ HAMT+ HAMT+✗ ✗ ✓✗ ✓ ✓11.93 3.34 67.05 61.69 5.90 44.75 61.84 11.84 3.40 67.65 61.74 5.83 46.35 63.66 11.99 3.44 68.37 62.59 5.65 46.88 62.7654.18 56.73 55.2333.89 35.87 35.50</p>
<p>Table 6 :
6
Including MPM during both pretraining (PT) and finetuning (FT) can generally lead to the best performance.</p>
<p>Table 7 :
7
MPM can be flexibly applied to various models and datasets.
ModelR2R Validation Unseen TL NE↓ SR↑ SPL↑NE↓REVERIE Validation Unseen OSR↑ SR↑ CLS↑ RGS↑ RGSPL↑DUET (Chen et al., 2022) 13.94 3.31 DUET w/ MPM 13.37 3.05 72.84 62.09 21.35 52.51 49.11 35.64 72 62 22.11 51.07 46.98 33.7332.15 32.5223.03 23.48</p>
<p>Table 7
7, MPM canstill improve the model performance with this map-based model on both R2R and REVERIE, demon-strating the flexibilty of MPM. It is worth notingthat REVERIE is a remote object grounding taskwhile MPM mainly concerns with navigating to agoal viewpoint or image, thus the object groundingperformance is not improved signficantly, and weleave this as a future work.</p>
<p>. In this line of research, representative works include Fried et al. (2018b) who propose panoramic action space and use a speaker follow to synthesize additional training data.In addition, Tan et al.</p>
<p>Code is available at https://github.com/
We set x=25 in this paper and perform sensitivity analysis in the experiment section.
In the decoding side, the agent still receives panoramic views as in previous work(Fried et al., 2018b;Chen et al., 2021a).
We do not use the masked region modeling objective in HAMT because it requires distilling the knowledge of an
Following the hyper-parameters in https://github. com/cshizhe/VLN-HAMT/blob/main/finetune_src/ scripts/run_r2r.sh.
AcknowledgmentWe thank anonymous reviewers for their insightful feedback.We also thank Cheng-Fu Yang, Sidi Lu and other members from the UCLA NLP group for their helpful feedback and discussions.Zi-Yi is supported by the Amazon Fellowship and DARPA Machine Common Sense (MCS) program under Cooperative Agreement N66001-19-2-4032 and NIH R01HL152270.A Additional Analysis ResultsIn this section, we present more analysis results to provide further insights into our proposed method.The model is generally robust to this hyper-parameter, with 25% achieving the best performance on unseen environments.Impact of the Mask Ratio.The mask ratio is a hyperparameter in masked path modeling and we investigate the optimal ratio in this paragraph.As shown in Figure3, the objective is generally robust to the mask ratio, with 25% leading to the best performance and &lt;=50% bringing improvements on the baseline.Therefore, we choose to randomly mask 25% of the viewpoints along a path in this paper.It is worth noting that because we always keep the goal viewpoint as mentioned in the method section, masking 100% of the paths is equivalent to an image-goal navigation task.Because the model performance can be improved even with 100% viewpoints removed, the improvements of MPM can be partially attribute to multi-task learning, which has proven to be effective inWang et al. (2022a).Performance on Instructions of Different Lengths.We evaluate the performance of both the baseline model and our proposed approach on instructions of varying lengths on the R2R validation set. Figure4illustrates the results, indicating that the benefits of employing MPM are particularly significant when instructions are lengthy.We attribute this observation to the fact that the sampled paths occasionally involve intricate navigation patterns.Consequently, the integration of MPM enables the model to effectively learn how to navigate and follow complex instructions.Similarly, our method can achieve significant improvements when the instructions are relatively short possibly because the sampled paths are sometimes simple.Qualitative Examples.We also sample some examples from the R2R validation set with both the baseline model and our model.As shown in Figure5and 6, our method can encourage the model to closely follow the language instructions even if they are rather lengthy or concise.On the other hand, the baseline model fails to do so: it can generate incorrect paths when the instructions are short and false and unnecessarily complicated paths when the instructions are long.
Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, On evaluation of embodied navigation agents. 2018aarXiv preprint</p>
<p>Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2018b</p>
<p>Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2018c</p>
<p>BEiT: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Songhao Piao, Furu Wei, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). the Conference on Neural Information Processing Systems (NeurIPS)2020</p>
<p>Exploration by random network distillation. Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2019</p>
<p>Matter-port3d: Learning from RGB-D data in indoor environments. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, Proceedings of the International Conference on 3D Vision (3DV). the International Conference on 3D Vision (3DV)2017</p>
<p>SEAL: Self-supervised embodied active learning using exploration and 3d consistency. Devendra Singh Chaplot, Murtaza Dalal, Saurabh Gupta, Jitendra Malik, Russ R Salakhutdinov, Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). the Conference on Neural Information Processing Systems (NeurIPS)2021</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>History aware multimodal transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). the Conference on Neural Information Processing Systems (NeurIPS)2021a</p>
<p>Think global, act local: Dual-scale graph transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2020a</p>
<p>An empirical study of training self-supervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)2021b</p>
<p>UNITER: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020b</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the Conference of the North American Chapter. the Conference of the North American Chapterthe Association for Computational Linguistics2019</p>
<p>FOAM: A followeraware speaker model for vision-and-language navigation. Zi-Yi Dou, Nanyun Peng, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)2022</p>
<p>An empirical study of training end-to-end vision-and-language transformers. Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, Michael Zeng, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>ImageNet: Constructing a large-scale image database. Li Fei-Fei, Jia Deng, Kai Li, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2009</p>
<p>Unified pragmatic models for generating and following instructions. Daniel Fried, Jacob Andreas, Dan Klein, Proceedings of the Conference of the North American Chapter. the Conference of the North American Chapterthe Association for Computational Linguistics2018a</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). the Conference on Neural Information Processing Systems (NeurIPS)2018b</p>
<p>Airbert: Indomain pretraining for vision-and-language navigation. Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>Towards learning a generic agent for vision-and-language navigation via pretraining. Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>A recurrent visionand-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>Stay on the path: Instruction fidelity in vision-andlanguage navigation. Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2019</p>
<p>A new path: Scaling vision-and-language navigation with synthetic instructions and imitation learning. Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh, Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, Zarana Parekh, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>Simple but effective: CLIP embeddings for embodied AI. Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, Aniruddha Kembhavi, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>ViLT: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2021</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, AI2-THOR: An interactive 3D environment for visual AI. 2017arXiv preprint</p>
<p>Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Generative language-grounded policy in vision-and-language navigation with bayes' rule. Shuhei Kurita, Kyunghyun Cho, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Improving vision-andlanguage navigation by generating future-view image semantics. Jialu Li, Mohit Bansal, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2023</p>
<p>EnvEdit: Environment editing for vision-and-language navigation. Jialu Li, Hao Tan, Mohit Bansal, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Align before fuse: Vision and language representation learning with momentum distillation. Junnan Li, Akhilesh Ramprasaath R Selvaraju, Shafiq Deepak Gotmare, Caiming Joty, Steven Xiong, Hoi, Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). the Conference on Neural Information Processing Systems (NeurIPS)2021</p>
<p>Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, VisualBERT: A simple and performant baseline for vision and language. 2019aarXiv preprint</p>
<p>Robust navigation with language pretraining and stochastic sampling. Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah A Smith, Yejin Choi, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2019b</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, RoBERTa: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2018</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Alregib, Zsolt Kira, Richard Socher, Caiming Xiong, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2019</p>
<p>General evaluation for instruction conditioned navigation using dynamic time warping. Gabriel Magalhães, Vihan Jain, Alexander Ku, Eugene Ie, Jason Baldridge, Workshop on Visually Grounded Interaction and Language (ViGIL). 2019</p>
<p>Improving vision-and-language navigation with imagetext pairs from the web. Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2016</p>
<p>R3M: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, Proceedings of the Conference on Robot Learning (CoRL). the Conference on Robot Learning (CoRL)2022</p>
<p>TEACh: Task-driven embodied agents that chat. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2022</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2017</p>
<p>Deep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proceedings of the Conference of the North American Chapter. the Conference of the North American Chapterthe Association for Computational Linguistics2018</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2021</p>
<p>Habitat: A platform for embodied AI research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)2019</p>
<p>Masked world models for visual control. Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, Pieter Abbeel, Proceedings of the Conference on Robot Learning (CoRL). the Conference on Robot Learning (CoRL)2023</p>
<p>Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2018</p>
<p>How much can CLIP benefit vision-and-language tasks?. Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2022</p>
<p>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, Proceedings of the Conference of the North American Chapter. the Conference of the North American Chapterthe Association for Computational Linguistics2019</p>
<p>Towards versatile embodied navigation. Hanqing Wang, Wei Liang, Luc V Gool, Wenguan Wang, Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). the Conference on Neural Information Processing Systems (NeurIPS)2022a</p>
<p>Less is more: Generating grounded navigation instructions from landmarks. Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar, Aleksandra Faust, Izzeddin Gur, Natasha Jaques, Austin Waters, Jason Baldridge, Peter Anderson, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2022b</p>
<p>Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang, Wang , Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Scaling data generation in vision-and-language navigation. Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)2023</p>
<p>Masked trajectory models for prediction, representation, and control. Philipp Wu, Arjun Majumdar, Kevin Stone, Yixin Lin, Igor Mordatch, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2023Pieter Abbeel, and Aravind Rajeswaran</p>
<p>Representation matters: offline pretraining for sequential decision making. Mengjiao Yang, Ofir Nachum, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2021</p>
<p>Vision-language navigation with selfsupervised auxiliary reasoning tasks. Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)2020a</p>
<p>Babywalk: Going farther in vision-and-language navigation by taking baby steps. Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, Fei Sha, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2020b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1169 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1169</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1169</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-268510262</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.09930v1.pdf" target="_blank">Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics</a></p>
                <p><strong>Paper Abstract:</strong> A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning al-gorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks. We also demonstrate that we can harness the learned skills to adapt better than other baselines to five perturbed environments. Finally, qualitative analyses showcase a range of remarkable behaviors: adaptive-intelligent-robotics.github.io/QDAC.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1169.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1169.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3 (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3 world model using a Recurrent State Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent latent dynamics world model (RSSM) used in the paper's model-based variant (QDAC-MB); trained to predict observations, rewards, terminations and the task features phi, and used for latent 'imagination' rollouts to train critics and the skill-conditioned actor via backpropagation through the learned dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3 (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recurrent latent dynamics model (RSSM) combining deterministic recurrent hidden states and stochastic latent variables; the model is trained to predict (reconstruct) observations, rewards, termination flags and the environment features ϕ, and is used to instantiate an imagination MDP whose latent dynamics permit multi-step rollouts. Critic networks (V and successor-features ψ) are trained on imagined trajectories and the actor is optimized by backpropagating through the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent latent dynamics; RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous control locomotion (Brax physics environments; skill-conditioned locomotion tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction / reconstruction losses on predicted observations, rewards, terminations and predicted features ϕ (i.e., standard world-model prediction loss / likelihood or MSE on predicted quantities); critics are trained on model-predicted λ-returns. The paper does not report a specialized numeric fidelity metric for the world model beyond training losses.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Treated as a black-box latent neural model; the paper does not provide interpretability analyses tying latent dimensions to semantic environment aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>none mentioned in this paper (no latent-space visualizations or disentanglement analyses reported; model used for latent rollouts and predictions of ϕ).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Qualitative only: authors report advantages such as small memory footprint of imagined rollouts enabling thousands of on-policy trajectories in parallel and straight-through gradients through dynamics; no numeric compute budgets, wall-clock times, parameter counts, or GPU requirements are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitative improvement in sample efficiency relative to the model-free QDAC is claimed (world model enables massive skill sampling in imagination), but no head-to-head numeric compute vs sample-efficiency trade-off is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>The model-based variant (QDAC-MB) that uses DreamerV3/RSSM achieves better ability to execute some skills (notably the challenging Humanoid Jump task where features are not explicitly in observations) and generally outperforms or improves over model-free QDAC and several baselines on diversity and performance metrics in the paper's experiments; the paper does not isolate numeric performance attributable solely to the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The world model's representation capability (latent dynamics + predicted ϕ) is reported to increase the ability to learn and execute skills that are hard to achieve from raw environment data (e.g., jump features not observable directly), and to improve sample efficiency by enabling large-scale imagined rollouts used to train critics and actor; however the paper provides qualitative/aggregate experimental evidence rather than quantitative ablation isolating model fidelity→policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Paper highlights benefits (sample efficiency, representation power, ability to train with many imagined skill rollouts, straight-through gradients) but does not quantify costs; explicit tradeoffs such as model bias/compounding error, extra training complexity, or compute overhead are discussed only qualitatively (no numeric cost/benefit reported).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses an RSSM (Dreamer family) extended to predict task features ϕ in addition to rewards/observations/termination; constructs an imagination MDP in latent space; trains critics (V and ψ) on λ-returns predicted by the model; performs N imagined rollouts per update sampling many skills uniformly in Z; actor optimization backpropagates through model dynamics; successor-features critic implemented as a distributional critic in their Dreamer-based implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to the model-free QDAC variant and to other baselines: QDAC-MB outperforms model-free QDAC on tasks where world-model representation matters (e.g., Humanoid Jump) and improves sample efficiency via imagined rollouts. The paper reports QDAC-MB generally achieves higher coverage/performance than many baselines, but no direct numerical compute or statistical efficiency comparison isolating world-model overhead is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No explicit numerical guidance on optimal world-model capacity, rollout horizon, or compute/latents is provided. The paper's practical suggestions are: (1) extend the world model to predict task features ϕ used for skill conditioning, (2) sample many skills in imagination to train skill-conditioned critics and actor, and (3) train critics on λ-returns in imagined rollouts—but no hyperparameter recommendations or formal trade-off curves are given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1169.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1169.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World models / latent dynamics models (general reference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of learned environment models (latent dynamics models, RSSMs, VAE-based world models) cited in the paper as a mechanism to improve sample efficiency and to enable large-scale imagination-based training of skill-conditioned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World models (latent dynamics; RSSM family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General description in the paper: compressed spatial and temporal latent representation learned to predict future observations/rewards/termination; used to run simulated rollouts in imagination and to backpropagate through learned dynamics for policy and critic training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (general category; includes RSSM / Dreamer family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>not task-specific in the general mention, but positioned for use in continuous-control locomotion and other domains where sample efficiency and large-scale imagination are valuable</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Described conceptually as being measured by predictive/reconstruction losses on observations, rewards, termination and (optionally) task features; the paper references standard Dreamer-style training objectives but does not provide new fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Characterized as learned latent representations (compressed, not inherently interpretable); no interpretability analyses provided in this paper for the general class.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>none mentioned for the general discussion (references prior work but does not report interpretability methods here).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Paper notes world models allow small-memory imagined rollouts and parallel sampling of many trajectories (improving sample-efficiency), but does not quantify training/inference cost; the general remark implies increased training complexity vs pure model-free methods but potential reduction in environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitative claim: model-based methods improve sample efficiency and allow massively parallel imagined rollouts, enabling training of skill-conditioned function approximators with many skills sampled in imagination; no numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Paper argues world models can improve task performance via better representations and more efficient training (citing Dreamer/Dream to Control literature); no new numeric performance numbers are provided for the general mention beyond the QDAC-MB experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>World models are framed as particularly useful when the agent sees only a small subset of (s,z) combinations in real environment data—the model allows generalization by imagining unseen skill-state combinations and thus improves training of skill-conditioned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Paper lists benefits (compressed representation, reduced environment interactions, straight-through gradients, parallel imagined sampling) but does not provide quantified costs; implicit tradeoffs include model training complexity and potential model bias, which are acknowledged conceptually but not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Recommended design choices in the discussion: use an RSSM-like latent model, extend it to predict task features ϕ when those features are used for skill definition, and use imagined rollouts to train critics/actor (λ-return targets, distributional successor-features critic).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted qualitatively with model-free RL: world models can improve sample-efficiency and representation power for skill learning tasks; paper reports the model-based QDAC-MB variant outperforms model-free baselines on several tasks, especially when features are not explicit in observations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No prescriptive optimal configuration provided for general world models; the paper's practical insight is to predict task features in the model and to leverage imagined rollouts for skill-conditioned training, but it stops short of giving architecture/hyperparameter rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Learning Latent Dynamics for Planning from Pixels <em>(Rating: 2)</em></li>
                <li>Dream to Control: Learning Behaviors by Latent Imagination <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1169",
    "paper_id": "paper-268510262",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DreamerV3 (RSSM)",
            "name_full": "DreamerV3 world model using a Recurrent State Space Model (RSSM)",
            "brief_description": "A recurrent latent dynamics world model (RSSM) used in the paper's model-based variant (QDAC-MB); trained to predict observations, rewards, terminations and the task features phi, and used for latent 'imagination' rollouts to train critics and the skill-conditioned actor via backpropagation through the learned dynamics.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "use",
            "model_name": "DreamerV3 (RSSM)",
            "model_description": "A recurrent latent dynamics model (RSSM) combining deterministic recurrent hidden states and stochastic latent variables; the model is trained to predict (reconstruct) observations, rewards, termination flags and the environment features ϕ, and is used to instantiate an imagination MDP whose latent dynamics permit multi-step rollouts. Critic networks (V and successor-features ψ) are trained on imagined trajectories and the actor is optimized by backpropagating through the world model.",
            "model_type": "latent world model (recurrent latent dynamics; RSSM)",
            "task_domain": "continuous control locomotion (Brax physics environments; skill-conditioned locomotion tasks)",
            "fidelity_metric": "Prediction / reconstruction losses on predicted observations, rewards, terminations and predicted features ϕ (i.e., standard world-model prediction loss / likelihood or MSE on predicted quantities); critics are trained on model-predicted λ-returns. The paper does not report a specialized numeric fidelity metric for the world model beyond training losses.",
            "fidelity_performance": null,
            "interpretability_assessment": "Treated as a black-box latent neural model; the paper does not provide interpretability analyses tying latent dimensions to semantic environment aspects.",
            "interpretability_method": "none mentioned in this paper (no latent-space visualizations or disentanglement analyses reported; model used for latent rollouts and predictions of ϕ).",
            "computational_cost": "Qualitative only: authors report advantages such as small memory footprint of imagined rollouts enabling thousands of on-policy trajectories in parallel and straight-through gradients through dynamics; no numeric compute budgets, wall-clock times, parameter counts, or GPU requirements are reported.",
            "efficiency_comparison": "Qualitative improvement in sample efficiency relative to the model-free QDAC is claimed (world model enables massive skill sampling in imagination), but no head-to-head numeric compute vs sample-efficiency trade-off is reported.",
            "task_performance": "The model-based variant (QDAC-MB) that uses DreamerV3/RSSM achieves better ability to execute some skills (notably the challenging Humanoid Jump task where features are not explicitly in observations) and generally outperforms or improves over model-free QDAC and several baselines on diversity and performance metrics in the paper's experiments; the paper does not isolate numeric performance attributable solely to the world model.",
            "task_utility_analysis": "The world model's representation capability (latent dynamics + predicted ϕ) is reported to increase the ability to learn and execute skills that are hard to achieve from raw environment data (e.g., jump features not observable directly), and to improve sample efficiency by enabling large-scale imagined rollouts used to train critics and actor; however the paper provides qualitative/aggregate experimental evidence rather than quantitative ablation isolating model fidelity→policy performance.",
            "tradeoffs_observed": "Paper highlights benefits (sample efficiency, representation power, ability to train with many imagined skill rollouts, straight-through gradients) but does not quantify costs; explicit tradeoffs such as model bias/compounding error, extra training complexity, or compute overhead are discussed only qualitatively (no numeric cost/benefit reported).",
            "design_choices": "Uses an RSSM (Dreamer family) extended to predict task features ϕ in addition to rewards/observations/termination; constructs an imagination MDP in latent space; trains critics (V and ψ) on λ-returns predicted by the model; performs N imagined rollouts per update sampling many skills uniformly in Z; actor optimization backpropagates through model dynamics; successor-features critic implemented as a distributional critic in their Dreamer-based implementation.",
            "comparison_to_alternatives": "Compared qualitatively to the model-free QDAC variant and to other baselines: QDAC-MB outperforms model-free QDAC on tasks where world-model representation matters (e.g., Humanoid Jump) and improves sample efficiency via imagined rollouts. The paper reports QDAC-MB generally achieves higher coverage/performance than many baselines, but no direct numerical compute or statistical efficiency comparison isolating world-model overhead is provided.",
            "optimal_configuration": "No explicit numerical guidance on optimal world-model capacity, rollout horizon, or compute/latents is provided. The paper's practical suggestions are: (1) extend the world model to predict task features ϕ used for skill conditioning, (2) sample many skills in imagination to train skill-conditioned critics and actor, and (3) train critics on λ-returns in imagined rollouts—but no hyperparameter recommendations or formal trade-off curves are given.",
            "uuid": "e1169.0",
            "source_info": {
                "paper_title": "Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "World models (general)",
            "name_full": "World models / latent dynamics models (general reference)",
            "brief_description": "General class of learned environment models (latent dynamics models, RSSMs, VAE-based world models) cited in the paper as a mechanism to improve sample efficiency and to enable large-scale imagination-based training of skill-conditioned policies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "World models (latent dynamics; RSSM family)",
            "model_description": "General description in the paper: compressed spatial and temporal latent representation learned to predict future observations/rewards/termination; used to run simulated rollouts in imagination and to backpropagate through learned dynamics for policy and critic training.",
            "model_type": "latent world model (general category; includes RSSM / Dreamer family)",
            "task_domain": "not task-specific in the general mention, but positioned for use in continuous-control locomotion and other domains where sample efficiency and large-scale imagination are valuable",
            "fidelity_metric": "Described conceptually as being measured by predictive/reconstruction losses on observations, rewards, termination and (optionally) task features; the paper references standard Dreamer-style training objectives but does not provide new fidelity metrics.",
            "fidelity_performance": null,
            "interpretability_assessment": "Characterized as learned latent representations (compressed, not inherently interpretable); no interpretability analyses provided in this paper for the general class.",
            "interpretability_method": "none mentioned for the general discussion (references prior work but does not report interpretability methods here).",
            "computational_cost": "Paper notes world models allow small-memory imagined rollouts and parallel sampling of many trajectories (improving sample-efficiency), but does not quantify training/inference cost; the general remark implies increased training complexity vs pure model-free methods but potential reduction in environment interactions.",
            "efficiency_comparison": "Qualitative claim: model-based methods improve sample efficiency and allow massively parallel imagined rollouts, enabling training of skill-conditioned function approximators with many skills sampled in imagination; no numeric comparison provided.",
            "task_performance": "Paper argues world models can improve task performance via better representations and more efficient training (citing Dreamer/Dream to Control literature); no new numeric performance numbers are provided for the general mention beyond the QDAC-MB experimental results.",
            "task_utility_analysis": "World models are framed as particularly useful when the agent sees only a small subset of (s,z) combinations in real environment data—the model allows generalization by imagining unseen skill-state combinations and thus improves training of skill-conditioned policies.",
            "tradeoffs_observed": "Paper lists benefits (compressed representation, reduced environment interactions, straight-through gradients, parallel imagined sampling) but does not provide quantified costs; implicit tradeoffs include model training complexity and potential model bias, which are acknowledged conceptually but not measured.",
            "design_choices": "Recommended design choices in the discussion: use an RSSM-like latent model, extend it to predict task features ϕ when those features are used for skill definition, and use imagined rollouts to train critics/actor (λ-return targets, distributional successor-features critic).",
            "comparison_to_alternatives": "Contrasted qualitatively with model-free RL: world models can improve sample-efficiency and representation power for skill learning tasks; paper reports the model-based QDAC-MB variant outperforms model-free baselines on several tasks, especially when features are not explicit in observations.",
            "optimal_configuration": "No prescriptive optimal configuration provided for general world models; the paper's practical insight is to predict task features in the model and to leverage imagined rollouts for skill-conditioned training, but it stops short of giving architecture/hyperparameter rules.",
            "uuid": "e1169.1",
            "source_info": {
                "paper_title": "Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Learning Latent Dynamics for Planning from Pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Dream to Control: Learning Behaviors by Latent Imagination",
            "rating": 1,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        }
    ],
    "cost": 0.01459275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
3 Jun 2024</p>
<p>Luca Grillotti 
Maxence Faldor 
Borja González León 
Antoine Cully 
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
3 Jun 2024B3089391CE3DF8D44C29BD902F29AEF0arXiv:2403.09930v3[cs.LG]
A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations.Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks.However, most approaches return only one solution specialized for a specific problem.We introduce Quality-Diversity Actor-Critic (QDAC), an offpolicy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors.In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills.Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks.We also demonstrate that we can harness the learned skills to adapt better than other baselines to five perturbed environments.Finally, qualitative analyses showcase a range of remarkable behaviors: adaptive-intelligent-robotics.github.io/QDAC.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) has enabled groundbreaking achievements like mastering discrete games (Mnih et al., 2013;Silver et al., 2016) but also continuous control domains for locomotion (Haarnoja et al., 2019;Heess et al., 2017).These milestones have showcased the extraordinary potential of RL algorithms in solving specific problems.</p>
<ul>
<li>Equal contribution 1 Department of Computing, Imperial College London, London, United Kingdom 2 Iconic AI.Correspondence to: Luca Grillotti <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#108;&#117;&#99;&#97;&#46;&#103;&#114;&#105;&#108;&#108;&#111;&#116;&#116;&#105;&#49;&#54;&#64;&#105;&#109;&#112;&#101;&#114;&#105;&#97;&#108;&#46;&#97;&#99;&#46;&#117;&#107;">&#108;&#117;&#99;&#97;&#46;&#103;&#114;&#105;&#108;&#108;&#111;&#116;&#116;&#105;&#49;&#54;&#64;&#105;&#109;&#112;&#101;&#114;&#105;&#97;&#108;&#46;&#97;&#99;&#46;&#117;&#107;</a>.</li>
</ul>
<p>Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).</p>
<p>In contrast, human intelligence is beyond mastering a single task, and adapts to unforeseen environments by combining skills.Empowering artificial agents with diverse skills was shown to improve exploration (Gehring et al., 2021), to facilitate knowledge transfer (Eysenbach et al., 2018), to enable hierarchical problem-solving (Allard et al., 2022), to enhance robustness and adaptation (Kumar et al., 2020;Cully et al., 2015) and finally, to foster creativity (Zahavy et al., 2023;Lehman et al., 2020).</p>
<p>Following this observation, methods have been developed to make agents more versatile, including Goal-Conditioned Reinforcement Learning (GCRL) (Liu et al., 2022), Unsupervised Reinforcement Learning (URL) (Eysenbach et al., 2018;Sharma et al., 2019), and reward design (Margolis &amp; Agrawal, 2022).However, designing algorithms to learn expressive skills that are useful to solve downstream tasks remains a challenge.Reward design requires a lot of manual work and fine-tuning while being very brittle.GCRL and URL try to achieve goals or execute skills while disregarding other objectives like safety or efficiency, leaving a gap in our quest for machines that can execute expressive and optimal skills to solve complex tasks.</p>
<p>Quality-Diversity (QD) optimization (Pugh et al., 2016) is a family of methods, originating from Evolutionary Algorithms, that generate a diverse population of highperforming solutions.QD algorithms have shown promising results in hard-exploration settings (Ecoffet et al., 2021), to recover from damage (Cully et al., 2015) or to reduce the reality gap (Chatzilygeroudis et al., 2018).In particular, QD algorithms have been scaled to challenging, continuous control tasks, by synergizing evolutionary methods with reinforcement learning (Faldor et al., 2023b;Pierrot et al., 2022).Other approaches like SMERL (Kumar et al., 2020) and DOMiNO (Zahavy et al., 2022) share the same objective of finding diverse and near-optimal policies and optimize a quality-diversity trade-off employing a pure reinforcement learning formulation.Most QD algorithms guide the diversity search towards relevant behaviors using a manually defined behavior descriptor function, that meaningfully characterizes solutions for the type of diversity desired (Cully &amp; Demiris, 2018;Mouret &amp; Clune, 2015).Two notable exceptions are AURORA (Grillotti &amp; Cully, 2022b) and SMERL, that learn an unsupervised diversity measure, using an autoencoder architecture and DIAYN respectively.</p>
<p>In this work, we aim to solve the Quality-Diversity problem, i.e. to learn a large number of high-performing and diverse behaviors, where the diversity measure is given as part of the task, as a function of state-action occupancy (see Section 3 for a detailed problem statement).First, we introduce an approximate policy skill improvement update based on successor features, analogous to the classic policy improvement update based on value function (Section 4.1).Second, we show that the policy skill improvement update based on successor features enables the policy to efficiently learn to execute skills with a theoretical justification (see Proposition in Section 4.1).Third, we formalize the goal of Quality-Diversity into a problem that seamlessly unifies value function and successor features critics using constrained optimization to (1) maximize performance, while (2) executing desired skills (see Problem P3 in Section 4.1).Finally, we introduce Quality-Diversity Actor-Critic (QDAC), a practical algorithm that solves this problem by leveraging two independent critics -the value function criticizes the actions made by the actor to improve quality while the successor features criticizes the actions made by the actor to improve diversity (Section 4.2).</p>
<p>We evaluate our approach on six continuous control tasks and show that QDAC achieves 15% more diverse behaviors and 38% higher performance than other baselines (Section 5.4.1).Finally, we show that the skills can be used to adapt to downstream tasks in a few shots or via hierarchical learning (Section 5.4.2).</p>
<p>Background</p>
<p>We consider the reinforcement learning framework (Sutton &amp; Barto, 2018) where an agent interacts with a Markov Decision Process (MDP) to maximize the expected sum of rewards.At each time step t, the agent observes a state s t ∈ S and takes an action a t ∈ A, which causes the environment to transition to a next state s t+1 ∈ S, sampled from the dynamics p(s t+1 | s t , a t ).Additionally, the agent receives a reward r t = r(s t , a t ) and observes features ϕ t = ϕ(s t , a t ) ∈ Φ ⊂ R d .In this work, we assume the features ϕ t are provided by the environment as part of the task, akin to the rewards, and are not learned by the agent.We denote ρ π (s) = lim t→∞ P (s t = s|s 0 , π) the stationary distribution of states under a policy π, which we assume exists and is independent of s 0 (Sutton et al., 1999).</p>
<p>The objective of the agent is to find a policy π that maximizes the expected discounted sum of rewards, or expected return E π [ t γ t r t ].The so-called value-based methods in RL rely on the concept of value function V π (s), defined as the expected return obtained when starting from state s and following policy π thereafter (Puterman, 1994):
V π (s) = E π ∞ i=0 γ i r t+i s t = s .
In this work, the value function is approximated via a neural network parameterized by θ V .Similarly to Mnih et al. (2013), those parameters are optimized by minimizing the Bellman error:
J V (θ V ) = E π V θ V (s t ) − r t − γV θ ′ V (s t+1 ) 2(1)
where θ ′ V are the parameters of a target network, which are updated at a lower pace to improve training stability (Mnih et al., 2015).</p>
<p>In addition to the value function, we also leverage the concept of successor features ψ π (s), which is the expected discounted sum of features obtained when starting from state s and following policy π thereafter (Barreto et al., 2017):
ψ π (s) = E π ∞ i=0 γ i ϕ t+i s t = s .
The successor features captures the expected features under a given policy, offering insights into the agent's future behavior and satisfies a Bellman equation in which ϕ t plays the role of the reward ψ π (s) = E π [ϕ t + γψ π (s t+1 )|s t = s], and can be learned with any RL methods (Dayan, 1993).In this work specifically, the successor features are approximated via a neural network parameterized by θ ψ .Analogously to the value function network, θ ψ is optimized by minimizing the Bellman error:
J ψ (θ ψ ) = E π ψ θ ψ (s t ) − ϕ t − γψ θ ′ ψ (s t+1 ) 2 2 (2)
where θ ′ ψ are the parameters of the corresponding target network.</p>
<p>In practice, we make use of a universal value function approximator V π (s, z) (Schaul et al., 2015) and of a universal successor features approximator ψ π (s, z) (Borsa et al., 2018) that depend on state s but also on the skill z conditioning the policy.The value function quantifies the performance while the successor features characterizes the behavior of the agent.For conciseness, we omit π from the notations ρ π , V π , ψ π and we note π z (a|s) := π(a|s, z).</p>
<p>Problem Statement</p>
<p>In this work, we aim to solve the Quality-Diversity problem, i.e. to learn a policy that can execute a large number of different and high-performing behaviors.In this section, we formalize this intuitive goal into a concrete optimization problem.The behavior of a policy π is characterized by the expected features under the policy's stationary distribution, lim T →∞
1 T T −1 t=0 ϕ t = E π [ϕ(s, a)
] and we define the space of all possible behaviors to be the skill space Z.</p>
<p>Given this definition, we intend to learn a skill-conditioned policy π(a|s, z) that (1) maximizes the expected return, and (2) is subject to the expected features converge to the desired skill z.In other words, we solve the following constrained optimization problem, for all z ∈ Z,
maximize E πz ∞ i=0 γ i r t+i subject to E πz [ϕ(s, a)] = z (P1)
The feature function ϕ can be any arbitrary function of the state of the MDP and of the action taken by the agent.Computing diversity based on the raw observations in highdimensional environments (e.g., pixel observations) may not lead to interesting behaviors.Thus, the features can be thought of as relevant characteristics or events for the type of diversity desired, such as joint positions, contact with the ground, speed and so on.To illustrate the generality of this problem formulation, we now give two examples.Consider a robot whose objective is to minimize energy consumption, and where the features characterize the velocity of the robot
ϕ t = v t = v x (t) v y (t)
⊺ and the skill space Z = R 2 .</p>
<p>For each desired velocity z ∈ Z, π(a|s, z) is expected to (1) minimize energy consumption, while (2) following the desired velocity z in average, lim T →∞</p>
<p>T</p>
<p>T −1 t=0 v t = z.Now consider another example with a legged robot, where the objective is to go forward as fast as possible, and the features characterize which foot is in contact with the ground at each time step.For example, ϕ t = 1 0 ⊺ for a biped robot that is standing on its first leg and with the second leg not touching the ground at time step t.With these features, the i-th component of the skill z (i.e.average features) will be the proportion of time during which the i-th foot of the robot is in contact with the ground, denoted as feet contact rate.In that case, the skill space characterizes the myriad of ways the robot can walk and specifically, how often each leg is being used.Notice that to achieve a feet contact of z = 0.1 0.6 ⊺ , the robot needs to use 10% of the time the first foot and 60% of the time the second foot over a trajectory of multiple time steps.</p>
<p>Methods</p>
<p>In this section, we present Quality-Diversity Actor-Critic (QDAC), a quality-diversity reinforcement learning algorithm that discovers high-performing and diverse skills.First, we present a concrete optimization problem that optimizes for quality and diversity as defined in Section 3. Second, we define the notion of successor features policy iteration that we combine with value function policy iteration to derive a tractable objective for the actor, that solves problem P1 approximately.Third, we derive a practical algorithm that optimizes this objective.</p>
<p>Actor Objective</p>
<p>First, we relax the constraint from Problem P1 using the L 2 norm and δ, a threshold that quantifies the maximum acceptable distance between the desired skill and the expected features.We solve the optimization problem, for all z ∈ Z,
maximize E πz ∞ i=0 γ i r t+i subject to ∥E πz [ϕ(s, a)] − z∥ 2 ≤ δ (P2)
Second, we derive an upper bound for the distance between desired skill and expected features, whose proof is provided in Appendix B. A similar proposition is proven in a more general case in Appendix B.1.The goal is to minimize the bound so that the constraint in Problem P2 is satisfied.</p>
<p>Proposition.Consider an infinite horizon, finite MDP with observable features in Φ.Let π be a policy and let ψ be the discounted successor features.Then, for all skills z ∈ Z, we can derive an upper bound for the distance between z and the expected features under π:
∥E πz [ϕ(s, a)] − z∥ 2 ≤ E πz [∥(1 − γ)ψ(s, z) − z∥ 2 ] (3)maximize E πz [V (s, z)] subject to E πz [∥(1 − γ)ψ(s, z) − z∥ 2 ] ≤ δ (P3)
Finally, we solve Problem P3 using the method of Lagrange multipliers as described by Abdolmaleki et al. (2018;2023).For all states s, and all skills z ∈ Z, we maximize the Lagrangian function, subject to 0 ≤ λ(s, z) ≤ 1,
(1 − λ(s, z)) V (s, z)−λ(s, z)∥(1 − γ)ψ(s, z) − z∥ 2 (4)
The first term in red aims at maximizing the return, while the second term in blue aims at executing the desired skill.</p>
<p>To optimize the actor to be high-performing while executing diverse skills, we use a generalized policy iteration method.The algorithm consists in (1) policy evaluation for both critics V (s, z) and ψ(s, z), and (2) policy improvement via optimization of the Lagrangian function introduced in Equation 4. This formulation combines the classic policy improvement based on value function with a novel policy skill improvement based on successor features.</p>
<p>The Lagrange multiplier λ is optimized to balance the quality-diversity trade-off.
If ∥(1 − γ)ψ(s 1 , z 1 ) − z 1 ∥ 2 ≤
δ is satisfied for (s 1 , z 1 ), we expect λ(s 1 , z 1 ) to decrease to encourage maximizing the return.On the contrary, if the constraint is not satisfied for (s 2 , z 2 ), we expect λ(s 2 , z 2 ) to increase to encourage satisfying the constraint.</p>
<p>Practical Algorithm</p>
<p>The objective in Equation 4 can be optimized with any reinforcement learning algorithm that implements generalized policy iteration.We give two variants of our method, one variant named QDAC, that is model-free and that builds on top of SAC, and one variant named QDAC-MB, that is model-based and that builds on top of DreamerV3.Additional details about QDAC-MB are provided in Appendix C.2.In this section, we detail the model-free variant.</p>
<p>QDAC's model-free pseudocode is provided in Algorithm 1.</p>
<p>At each iteration, a skill z is uniformly sampled for an episode of length T , during which the agent interacts with the environment following skill z with π(•|s, z).At each time step t, the transition is stored in a replay buffer D, augmented with the features ϕ(s t , a t ) and with the current desired skill z.</p>
<p>Then, the Lagrange multiplier is updated to balance the quality-diversity trade-off.The parameters θ λ are optimized so that λ(s, z) increases when the actor is unable to execute the desired skill z, to put more weight on executing the skill.</p>
<p>Conversely, the parameters θ λ are optimized so that λ(s, z) decreases when the actor is able to execute the desired skill z, to put more weight on maximizing the return.The update of the Lagrange multiplier and its role in the actor objective are depicted in Figure 2. In practice, we use a cross-entropy loss to optimize θ λ :
J λ (θ λ ) = Es∼ρ z∼U (Z) [ − (1 − y) log (1 − λ (s, z)) − y log (λ (s, z))]
where
y = 0 if ∥(1 − γ)ψ(s, z) − z∥ 2 ≤ δ 1 otherwise (5)
Finally, the critics V , ψ and the actor π z are trained with a policy iteration step adapted from SAC and following Equation 4. The objective is optimized with stochastic gradient descent using a mini-batch of transitions sampled from the replay buffer.To improve sample efficiency, the transitions from the mini-batch are duplicated with new random skills sampled uniformly in the skill space.Additional information about QDAC's training procedure are provided in Appendix C.1.</p>
<p>Experiments</p>
<p>The goal of our experiments is twofold: (1) evaluate QDAC's ability to learn high-performing and diverse skills based on a wide range of features, (2) evaluate QDAC's ability to harness learned skills to solve downstream tasks.</p>
<p>Tasks</p>
<p>LEARNING DIVERSE HIGH-PERFORMING SKILLS</p>
<p>We evaluate our method on a range of challenging continuous control tasks using the Google Brax (Freeman et al., 2021)
λ ← θ λ − α λ ∇J λ (θ λ )
▷ Update Lagrange multiplier with Eq. 5 θV ← θV − αV ∇JV (θV )</p>
<p>▷ Policy evaluation for value function with Eq. 1
θ ψ ← θ ψ − α ψ ∇J ψ (θ ψ )
▷ Policy evaluation for successor features with Eq. 2 θπ ← θπ + απ∇Jπ(θπ) ▷ Policy improvement with Eq. 4 end for until convergence are traditional benchmark tasks that have been extensively studied in the Quality-Diversity and GCRL literature (Cully et al., 2015;Faldor et al., 2023a;Nilsson &amp; Cully, 2021;Zhu et al., 2021;Finn et al., 2017), while the two last ones are challenging tasks that we introduce in this work.In these locomotion tasks, the objective is to go forward as fast as possible while minimizing energy consumption.</p>
<p>Feet Contact features indicate for each foot of the agent, if the foot is in contact or not with the ground, exactly as defined in DCG-ME's original paper (Faldor et al., 2023a).For example, if the Ant only touches the ground with its second foot at time step t, then ϕ(s t , a t ) = 0 1 0 0 ⊺ .</p>
<p>The diversity of feet contact found by such QD algorithms has been demonstrated to be very useful in downstream tasks such as damage recovery (Cully et al., 2015).The expected features correspond to the proportion of time each foot is in contact with the ground.</p>
<p>Velocity features are two-dimensional vectors indicating the velocity of the agent in the xy-plane, ϕ(s t , a t ) = v x (t) v y (t) ⊺ .We evaluate on the velocity features to show that our method works on classic GCRL tasks.Moreover, the velocity features are interesting because satisfying a velocity that is negative on the x-axis is directly opposite to maximizing the forward velocity reward.</p>
<p>Jump features are one-dimensional vectors indicating the height of the lowest foot.For example, if the left foot of the humanoid is 10 cm above the ground and if its right foot is 3.5 cm above the ground, then the features ϕ(s t , a t ) = 0.035 .The skills derived from the jump features are also challenging to execute because to maintain an average z = 1 T T −1 i=0 ϕ t+i , the agent is forced to oscillate around that value z because of gravity.</p>
<p>Angle features are two-dimensional vectors indicating the angle α of the main body about the z-axis, ϕ(s t , a t ) = cos(α) sin(α)</p>
<p>⊺ .The goal of this task is to go as fast as possible in the x-direction while facing any directions, forcing the agent to sidestep or moonwalk.</p>
<p>HARNESSING SKILLS FOR FEW-SHOT ADAPTATION AND HIERARCHICAL LEARNING</p>
<p>We evaluate our method on few-shot adaptation scenarios with four types of perturbation and on one hierarchical learning task.For each task, the reward is the same but the MDP's dynamics is perturbed.Additional details are available in Appendix D.2.</p>
<p>In few-shot adaption tasks, no re-training is allowed and we evaluate the top-performing skills for each method while varying the perturbation to measure the robustness of the different algorithms, see Appendix D.2 for more details.</p>
<p>Humanoid -Hurdles requires the agent to jump over hurdles of varying heights.Humanoid -Motor Failure requires the agent to adapt to different degrees of failure in the motor controlling its left knee.In Ant -Gravity, the agent needs to adapt to different gravity conditions.Finally, Walker -Friction requires the agent to adapt to varying levels of ground friction.Here, we evaluate the agent's ability to adjust its locomotion strategy to a new perturbed MDP.</p>
<p>In the hierarchical learning task, named Ant -Wall, the agent is faced with navigating around a wall.A meta-controller is trained with Soft Actor-Critic (SAC) to maximize forward movement.Here, we evaluate the ability to use the diversity of skills discovered by QDAC for hierarchical RL.</p>
<p>Baselines</p>
<p>We compare QDAC with two families of methods that both balance a quality-diversity trade-off.The first family consists in evolutionary algorithms that maintain a diverse population of high-performing individuals whereas the second family uses a pure reinforcement learning formulation.Additionally, we perform three ablation studies.</p>
<p>Quality-Diversity via Evolutionary Algorithms</p>
<p>We compare our method with PPGA (Batra et al., 2023), DCG-ME (Faldor et al., 2023a;b) and QD-PG (Pierrot et al., 2022), three evolutionary algorithms that optimize a diverse population of high-performing individuals.PPGA is a stateof-the-art Quality-Diversity algorithm that mixes Proximal Policy Optimization (PPO) (Schulman et al., 2017) with CMA-MAEGA (Fontaine &amp; Nikolaidis, 2023); it alternates between (1) estimating the performance-feature gradients with PPO and (2) maintaining a population of coefficients to linearly combine those performance-feature gradients, those coefficients are optimized to maximize archive improvement.DCG-ME is another state-of-the-art Quality-Diversity algorithm that evolves a population of both high-performing and diverse solutions, and simultaneously distills those solutions into a single skill-conditioned policy.QD-PG is a Quality-Diversity algorithm that uses quality and diversity policy gradients to optimize its population of policies.</p>
<p>Quality-Diversity via Reinforcement Learning</p>
<p>We also compare our method with DOMiNO (Zahavy et al., 2022), SMERL (Kumar et al., 2020) and Reverse SMERL (Zahavy et al., 2022) that balance a quality-diversity trade-off using a pure reinforcement learning formulation.DOMiNO is a reinforcement learning algorithm designed to discover diverse behaviors while preserving near-optimality.Analogous to our method, it characterizes policies' behaviors using successor features.SMERL learns a latent-conditioned policy that maximizes the mutual information between states s and latent variables z, with a threshold to toggle the diversity reward in the objective r + α1(R ≥ R * − ϵ)r.The diversity reward r is measured from the likelihood of a discriminator q(z|s) coming from DIAYN.In other words, SMERL maximizes a weighted combination of environment reward and diversity reward when the policy is near-optimal, and only the environment reward otherwise.Reverse SMERL maximizes a similar reward 1(R &lt; R * − ϵ)r + αr.In other words, Reverse SMERL maximizes a weighted combination of environment reward and diversity reward when the policy is not near-optimal, and only the diversity reward otherwise.</p>
<p>Ablations We perform three additional ablation studies, that we call No-SF, Fixed-λ and UVFA.For No-SF, we remove the successor features representation and use a naive distance to skill instead t γ t ∥ϕ t − z∥ 2 to understand the contribution of the successor features critic to optimize diversity.For Fixed-λ, we remove the Lagrange multiplier and use a fixed trade-off instead to understand the contribution of constrained optimization.UVFA (Schaul et al., 2015) is an algorithm that corresponds to the combination of our two previous ablations, as such we consider it to be an ablation in this work.A summarized description of all baselines under study is provided in Table E.3.</p>
<p>Evaluation Metrics</p>
<p>We evaluate our method using two types of metrics from the Quality-Diversity literature that aim at evaluating the performance and the diversity of the discovered skills: (1) the distance to skill metrics, that evaluate the ability of an agent to execute desired skills, and (2) the performance metrics, that quantify the ability of an agent to maximize return while executing desired skills.Each experiment is replicated 10 times with random seeds.We report the Inter-Quartile Mean (IQM) value for each metric, with the estimated 95% Confidence Interval (CI) (Agarwal et al., 2021).The statistical significance of the results is evaluated using the Mann-Whitney U test (Mann &amp; Whitney, 1947) and the probabilities of improvement are reported in Appendix A.1.</p>
<p>Distance to skill metrics To evaluate the ability of a policy to achieve a given skill z, we estimate the expected distance to skill, denoted d(z), by averaging the euclidean distance between the desired skill z and the observed skill over 10 rollouts, as defined by Faldor et al. (2023a;b).First, we use d(z) to compute distance profiles on Figure 4, which quantify for a given distance d, the proportion of skills in the skill space that have an expected distance to skill smaller than d, computed with the function
d → 1 Nz Nz i=1 1(d(z i ) &lt; d).
Second, we summarize the ability of a policy to execute skills with the distance score,
1 Nz Nz i=1 −d(z i ).
Performance metrics To evaluate the ability of a policy to solve a task given a skill z, we estimate the expected undiscounted return, denoted R(z), by averaging the return over 10 rollouts, as defined by Flageat &amp; Cully (2023); Grillotti et al. (2023).First, we use R(z) to compute performance profiles on Figure 4, which quantify for a given return R, the proportion of skills in the skill space that have an expected return larger than R, after filtering out the skills that are not achieved by the policy.To this end, we compute the expected distance to skill d(z), and discard skills with an ex- pected distance to skill that is larger than a predefined threshold, d(z) &gt; d eval .More precisely, the performance profile is the function
R → 1 Nz Nz i=1 1(d(z i ) &lt; d eval , R(z i ) &gt; R).
Second, we summarize the ability of a policy to maximize return while executing skills, with the performance score,
1 Nz Nz i=1 R(z)1(d(z i ) &lt; d eval ).</p>
<p>Results</p>
<p>The goal of our experiments is to answer two questions: (1) Does QDAC solve the Quality-Diversity problem? (2) Can we harness the high-performing and diverse skills to adapt to perturbed MDP?In Section 5.4.1, we demonstrate that QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks (Fig. 3).In Section 5.4.2, we show that we can harness the learned skills to adapt better than other baselines to five perturbed MDP (Fig. 6).The code is available at: github.com/adaptive-intelligent-robotics/QDAC.</p>
<p>LEARNING DIVERSE HIGH-PERFORMING SKILLS</p>
<p>In this section, we evaluate QDAC with metrics coming from the Quality-Diversity literature, namely the distance to skill and performance metrics.However, DOMiNO and SMERL optimize for diversity, without the focus on executing specific skills.Consequently, the concept of 'distance to skill' does not apply and thus, the traditional QD metrics are not applicable.Nonetheless, we compare our approach with these baselines on adaptation tasks, for which they were initially designed, in Section 5.4.2.</p>
<p>QDAC and QDAC-MB outperform all baselines in executing a wide range of skills (Fig. 4), except on Humanoid Jump where DCG-ME and QD-PG achieve a better distance profile than our model-free variant.Yet, QDAC-MB outper-forms those baselines, due to the representation capabilities of the world model.The jump features are challenging because of the min operator, and because the features are not explicitly available in the observations given to the agent.</p>
<p>Notably, QDAC and QDAC-MB are capable of achieving skills that are contrary to the task reward, as illustrated by the velocity features in Figure 4 and Figure 5, which is not the case for PPGA, DCG-ME and QD-PG.Finally, our approach outperforms DCG-ME that fails to explicitly minimize the expected distance to skill, a common issue among QD algorithms (Flageat &amp; Cully, 2023).</p>
<p>QDAC-MB outperforms Fixed-λ on all tasks, showing the importance of using constrained optimization to solve the QD problem.Furthermore, QDAC-MB achieves better performance than No-SF on all tasks, showing the importance of the successor features critics to optimize diversity.Finally, QDAC significantly outperforms No-SF and UVFA on all tasks.On feet contact tasks, No-SF and UVFA can only execute skills in the corners of the skill space where z = ϕ t , as shown in Figure A.12.This is because these baselines employ a naive approach that consists in minimizing the distance between the features and the desired skill.Thus, they can only execute skills where the legs are always or never in contact with the ground.These comparisons supports the claim that QDAC is capable of accurately executing a diversity of skills and highlight the significance of the policy skill improvement term in blue in Equation 4.</p>
<p>QDAC and QDAC-MB outperform DCG-ME and QD-PG in maximizing return (Fig. 3), as the latter don't achieve many skills in the first place, and the performance score only evaluates the performance of skills successfully executed by the policy.While PPGA achieves a performance score comparable to QDAC, it does so by finding fewer robust policies, albeit with better performance (Fig. 4).Fixed-λ is the only baseline that gets performance scores and profiles comparable to QDAC-MB.However, Fixed-λ covers a smaller range of skills, as evidenced by the edges of the skill space on Figure A.12. Additionally, QDAC-MB outperforms Fixed-λ on the challenging jump task (Fig. 4), due to the necessity of a strong weight on the constraint.Ultimately, using an adaptive λ proves advantageous for our approach.</p>
<p>As summarized in Figure 3, QDAC and QDAC-MB achieve a better quality-diversity trade-off than other baselines, quantified by the distance score and the performance score.</p>
<p>HARNESSING SKILLS FOR FEW-SHOT ADAPTATION AND HIERARCHICAL LEARNING</p>
<p>QDAC and QDAC-MB demonstrate competitive performance in few-shot adaptation and hierarchical RL tasks, see Figure 6.On the hurdles task, when considering hurdle heights strictly greater than 0, QDAC-MB significantly outperforms other baselines by consistently jumping over higher hurdles and showcases remarkable behaviors.On the motor failure task, although performing worse than PPGA, QDAC shows great robustness, especially in the high damage regime.QDAC-MB performs better than QDAC on low damage, but QDAC can adapt to 100% damage strength on the left knee, still achieving more than 5,000 in return.</p>
<p>In other words, QDAC has found a way to continue walking despite not being able to control at all the left knee.QDAC does not seem able to adapt to gravity variations, but QDAC-MB shows competitive performance although performing slightly worse than PPGA and DOMiNO.On Walker -Friction, QDAC outperforms all baselines except PPGA and DCG-ME that achieve marginally better performance.Finally, QDAC's learned skills appear to be the best on the hierarchical RL task, as it achieves significantly higher performance than other baselines.</p>
<p>Our extensive experiments and analyses in Sections 5.4.1 and 5.4.2 firmly establish the efficacy of QDAC and QDAC-MB in addressing the dual challenges of learning highperforming and diverse skills.These methods not only surpass traditional Quality-Diversity algorithms in optimizing for specific pre-defined skills but also demonstrate remarkable adaptability and robustness in perturbed environments.</p>
<p>Related Work</p>
<p>QD optimization (Pugh et al., 2016;Cully &amp; Demiris, 2018) is a family of algorithms that generate large collections of solutions, such as policies, that are both diverse and high-performing.Those methods originate from Novelty Search (Lehman &amp; Stanley, 2011a;b) and Evolutionary Algorithms literature, where the diversity is defined across a population of solutions.Quality-Diversity algorithms have been shown to be competitive with skill discovery reinforcement learning methods (Chalumeau et al., 2022), and promising for adaptation to unforeseen situations (Cully et al., 2015).When considering large neural network policies, the sample efficiency of QD algorithms can be improved by using Evolution Strategies (Fontaine et al., 2020;Colas et al., 2020), RL-based methods (Pierrot et al., 2022;Nilsson &amp; Cully, 2021;Faldor et al., 2023a;Tjanaka et al., 2022;Batra et al., 2023;Xue et al., 2024).The sample efficiency can be further improved by decomposing the policies into several parts and coevolving a sub-population for each part (Xue et al., 2024).However, most QD algorithms output a large number of policies, which can be difficult to deal with in downstream tasks.Similarly to QDAC, DCG-ME addresses that issue by optimizing a single skill-conditioned policy (Faldor et al., 2023a).Finally, approaches like SMERL (Kumar et al., 2020) or DOMiNO (Zahavy et al., 2022) also solve the QD objective employing a pure reinforcement learning formulation.Contrary to QDAC, the policies discovered by DOMiNO are not trained to execute specific target skills.</p>
<p>Most Unsupervised Reinforcement Learning approaches discover diverse behaviors by maximizing an intrinsic reward defined in terms of the discriminability of the trajectories.</p>
<p>Usually, the methods maximize the Mutual Information (MI) between the trajectories and the skills I(τ, z) (Gregor et al., 2016;Sharma et al., 2019;Mazzaglia et al., 2022), simplified to an MI-maximization between skills and states with the following lower bound: I(τ, z) ≥ T t=1 I(s t , z).It has been shown that MI-based algorithms are equivalent to distance-to-skill minimization algorithms (Choi et al., 2021;Gu et al., 2021), and therefore present similarities with our work.However, most URL algorithms maximize an intrinsic reward while disregarding any other objective, making it difficult to discover useful and expressive behaviors.</p>
<p>While diversity can be achieved by maximizing a mutual information objective, it can also be explicitly defined as a distance between behavioral descriptors.Such descriptors can take the form of successor features (Zahavy et al., 2022;2023) or of expected features obtained though entire episodes (Cully et al., 2015;Batra et al., 2023).In this work, we rely on this latter definition, as expressed in Problems P1 and P2.The features ϕ can be defined in different ways.First, they can be a subpart of the state of the agent such as the joint positions and velocities (Zahavy et al., 2022), torso velocity (Cheng et al., 2023), or feet contacts (Cully et al., 2015).In this case, the state of the agent may guide the search towards relevant notions of diversity; however, this requires expert knowledge about the task, and the choice of feature definition strongly influences the quality and diversity of the generated solutions (Tarapore et al., 2016).Second, to avoid hand-defining features, we could define ϕ as the full state (Kumar et al., 2020) or as an unsupervised low-dimensional encoding of it (Grillotti &amp; Cully, 2022b;Mazzaglia et al., 2022;Liu &amp; Abbeel, 2021).In this case, additional techniques can be used to ensure the learned behaviors are relevant, such as adding an extrinsic reward term (Chalumeau et al., 2022), promoting diversity in the neighborhood of relevant solutions (Grillotti &amp; Cully, 2022a), or adding constraints for near-optimality (Zahavy et al., 2022;Kumar et al., 2020).Instead, QDAC constrains the agent's behavior to follow a given hand-defined skill z, and maximizes the performance for all skills z ∈ Z.</p>
<p>Conclusion</p>
<p>In this work, we present QDAC, an actor-critic deep reinforcement learning algorithm, that leverages a value function critic and a successor features critic to learn highperforming and diverse behaviors.In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills.</p>
<p>Empirical evaluations suggest that QDAC outperforms previous Quality-Diversity methods on six continuous control locomotion tasks.Quantitative results demonstrate that QDAC is competitive in adaptation tasks, while qualitative analyses reveal a range of diverse and remarkable behaviors.</p>
<p>In the future, we hope to apply QDAC to other tasks with different properties than the tasks from this paper.For example, it would be interesting to apply QDAC to non-ergodic environments such as Atari games.</p>
<p>Furthermore, like the vast majority of Quality-Diversity algorithms, QDAC uses a manually defined diversity measure to guide the diversity search towards relevant behaviors.An exciting direction for future work would be to learn the feature function in an unsupervised manner to discover task-agnostic skills.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Supplementary Results</p>
<p>A.1. Quantitative Results</p>
<p>Distance</p>
<p>A.2. Heatmaps</p>
<p>In Figures A.12 to A.17, we report the heatmaps for the metrics defined in Section 5.3: the negative distance to skill (in the top row) and the performance (in the bottom row).Each heatmap represents the skill space of the corresponding task.In the first row, the color of each cell represents the negated distance to the closest skill achieved by the policy (the darker the better).In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ); while colorized cells indicate the performance score (i.e. the return) achieved by the agent for the corresponding skill.</p>
<p>A.3. Results without filtering with d eval</p>
<p>In Figures A.18 to A.24, we report the profiles and heatmaps defined in Section 5.3 except that skills that are not successfully executed (i.e.d(z) &gt; d eval ) are not filtered out.</p>
<p>A.4. Archive Profiles and Heatmaps for DOMiNO, SMERL and Reverse SMERL</p>
<p>In Figures A.25 to A.31, we present the archive profiles and heatmaps achieved by DOMiNO, SMERL, and Reverse SMERL using a method analogous to that in (Chalumeau et al., 2022): we generate an archive from the intermediate policies encountered during training, and use this archive to compare against QDAC and QDAC-MB.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in (Chalumeau et al., 2022).The lines represent the IQM for 10 replications, and the shaded areas correspond to the 95% CI.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in (Chalumeau et al., 2022).The heatmap represents the skill space of feet contacts Z = [0, 1] 2 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2 ⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.3. We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in (Chalumeau et al., 2022).The heatmap represents the skill space of feet contacts Z = [0, 1] 4 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2 z3 z4</p>
<p>⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row, Walker Feet Contact Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in (Chalumeau et al., 2022).The heatmap represents the skill space of feet contacts Z = [0, 1] 2 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2 ⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.Humanoid Angle Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in (Chalumeau et al., 2022).The heatmap represents the skill space of body angles Z =] − π, π].This space is discretized into cells, with each cell representing a distinct skill; in this task, the skills refer to the angle of the humanoid body about the z-axis.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.</p>
<p>B. Theoretical Results</p>
<p>Proposition.Consider an infinite horizon, finite MDP with observable features in Φ.Let π be a policy and let ψ be the discounted successor features.Then, for all skills z ∈ Z, we can derive an upper bound for the distance between z and the expected features under π:
∥E πz [ϕ(s, a)] − z∥ 2 ≤ E πz <a href="6">∥(1 − γ)ψ(s, z) − z∥ 2 </a>
Proof.For all states s ∈ S, the Bellman equation for ψ gives:
ψ(s, a, z) = ϕ(s, a) + γE s ′ ∼p(.|s,a) <a href="7">ψ(s ′ , z)</a>
For all skills z ∈ Z and for all sequences of T states (s 0 , a 0 , . . ., s T −1 ) sampled from π z , we have:
1 T T −1 t=0 ϕ(s t , a t ) − z 2 = 1 T T −1 t=0 ψ(s t , a t , z) − γE st+1∼p(.|st,at) [ψ(s t+1 , z)] − z (Equation 7) = 1 T T −1 t=0 ((1 − γ)ψ(s t , a t , z) − z) + γ T T −1 t=0 ψ(s t , a t , z) − E st+1∼p(.|st,at) [ψ(s t+1 , z)] 2 ≤ 1 T T −1 t=0 (1 − γ)ψ(s t , a t , z) − z 2 + γ 1 T T −1 t=0 ψ(s t , a t , z) − E st+1∼p(.|st,at) [ψ(s t+1 , z)] 2 (triangular inequality)
We denote ρ(s) = lim t→∞ P (s t = s|s 0 , π z ) the stationary distribution of states under π z , which we assume exists and is independent of s 0 .Consequently, by taking the right term to the limit as T → ∞:
lim T →∞ 1 T T −1 t=0 ψ(s t , a t , z) − E st+1∼p(.|st,at) [ψ(s t+1 , z)] = lim T →∞ 1 T T −1 t=0 ψ(s t , a t , z) − lim T →∞ 1 T T −1 t=0 E st+1∼p(.|st,at) [ψ(s t+1 , z)] = E s∼ρ a∼π(.|s,z) [ψ(s, a, z)] − E s∼ρ a∼π(.|s,z) E s ′ ∼p(.|s,a) [ψ(s ′ , z)] = E s∼ρ [ψ(s, z)] − E s∼ρ a∼π(.|s,z) s ′ ∼p(.|s,a) [ψ(s ′ , z)] = E s∼ρ [ψ(s, z)] − E s ′ ∼ρ [ψ(s ′ , z)] = 0
Furthermore, by taking the left term to the limit as T → ∞:
lim T →∞ 1 T T −1 t=0 (1 − γ)ψ(s t , a t , z) − z = (1 − γ)E s∼ρ a∼π(.|s,z) [ψ(s, a, z)] − z = (1 − γ)E s∼ρ [ψ(s, z)] − z
Finally, by taking the inequality to the limit as T → ∞, we get:
∥E πz [ϕ(s, a)] − z∥ 2 ≤ ∥(1 − γ)E πz [ψ(s, z)] − z∥ 2 + ∥0∥ 2 ∥E πz [ϕ(s, a)] − z∥ 2 ≤ E πz [∥(1 − γ)ψ(s, z) − z∥ 2 ] (Jensen's inequality)
Proposition B.1.Consider a continuous MDP with a bounded feature space Φ, a skill z ∈ Z, and π a policy such that the sequence 1
T T −1 t=0 ϕ t T ≥1
almost surely 1 converges for trajectories sampled from π z .If we write
ϵ := sup t E πz [∥ϕ t + γψ(s t , a t , z) − ψ(s t+1 , a t+1 , z)∥ 2 ], then: E πz lim T →∞ 1 T T −1 t=0 ϕ t − z 2 ≤ sup t E πz [∥(1 − γ)ψ(s t , a t , z) − z∥ 2 ] + ϵ (8)
Furthermore, it is worth noting that if the MDP dynamics p and π are deterministic, then ϵ = 0.</p>
<p>Proof.Let z ∈ Z.</p>
<p>To make the proof easier to read, we use the following notations:
ψ t := ψ (s t , a t , z)
We define β as follows:
β := sup t E πz <a href="9">∥(1 − γ)ψ t − z∥ 2 </a>
Then we have, for all t,
E πz [∥(1 − γ)ψ t − z∥ 2 ] ≤ β(10)E πz ϕ t + γψ t − ψ t+1 2 ≤ ϵ(11)
The Bellman equation applied to Successor Features (SF) can be written:
ψ t = ϕ t + γE πz ψ t+1 s t , a t(12)
or also:
ϕ t = ψ t − γE πz ψ t+1 s t , a t(13)
We can now derive an upper bound for 1
T T −1 t=0 ϕ t − z 2
. For all sequences of T states s 0:T −1 we have:
1 T T −1 t=0 ϕ t − z 2 = 1 T T −1 t=0 (ϕ t − z) 2 = 1 T T −1 t=0 ψ t − γE πz ψ t+1 s t , a t − z 2 (from Equation 13) = 1 T T −1 t=0 (1 − γ)ψ t + γψ t − γE πz ψ t+1 s t , a t − z 2 = 1 T T −1 t=0 ((1 − γ)ψ t − z) + 1 T T −1 t=0 γψ t − γE πz ψ t+1 s t , a t 2 ≤ 1 T T −1 t=0 ((1 − γ)ψ t − z) 2 + 1 T T −1 t=0 γψ t − γE πz ψ t+1 s t , a t 2 (triangular inequality) Thus, E πz 1 T T −1 t=0 ϕ t − z 2 ≤ E πz 1 T T −1 t=0 ((1 − γ)ψ t − z) 2 + E πz 1 T T −1 t=0 γψ t − γE πz ψ t+1 s t , a t 2 (14)
1 almost sure refers to the almost sure convergence from probability theory where rollouts are sampled from πz.</p>
<p>We consider now the two terms on the right hand-side separately.First of all, we prove that the first term is lower than or equal to β: 10)
E πz 1 T T −1 t=0 ((1 − γ)ψ t − z) 2 ≤ E πz 1 T T −1 t=0 ∥((1 − γ)ψ t − z)∥ 2 (triangular inequality) ≤ 1 T T −1 t=0 E πz [∥((1 − γ)ψ t − z)∥ 2 ] ≤ 1 T T −1 t=0 β (from Equation≤ β(15)
Also, we can prove that the second term of the right-hand side in Equation 14 is lower than or equal to ϵ + η T where lim T →∞ η T = 0.For all sequences of T states s 0:T −1 , we have:
T −1 t=0 γψ t − γE πz ψ t+1 s t , a t = T −1 t=0 γψ t − T −1 t=0 γE πz ψ t+1 s t , a t = γψ 0 − γE πz [ ψ T | s T −1 , a T −1 ] + T −1 t=1 γψ t − T −2 t=0 γE πz ψ t+1 s t , a t = γψ 0 − γE πz [ ψ T | s T −1 , a T −1 ] + T −2 t=0 γψ t+1 − T −2 t=0 γE πz ψ t+1 s t , a t = γψ 0 − γE πz [ ψ T | s T −1 , a T −1 ] + T −2 t=0 γψ t+1 − γE πz ψ t+1 s t , a t
Thus, after dividing by T and applying the norm and expectation, we get:
E πz 1 T T −1 t=0 γψ t − γE πz ψ t+1 s t , a t 2 ≤ E πz 1 T (γψ 0 − γE πz [ ψ T | s T −1 , a T −1 ]) 2 + E πz 1 T T −2 t=0 γψ t+1 − γE πz ψ t+1 s t , a t 2 (triangular inequality) Let η T := E πz 1 T (γψ 0 − γE πz [ ψ T | s T −1 , a T −1 ]
) 2 , we then have: 12)
E πz 1 T T −1 t=0 γψ t − γE πz ψ t+1 s t , a t 2 ≤ η T + E πz 1 T T −2 t=0 γψ t+1 − γE πz ψ t+1 s t , a t 2 (triangular inequality) ≤ η T + E πz 1 T T −2 t=0 γψ t+1 − γE πz ψ t+1 s t , a t 2 ≤ η T + E πz 1 T T −2 t=0 γψ t+1 + ϕ t − ψ t 2 (from Equation≤ η T + 1 T T −2 t=0 E πz γψ t+1 + ϕ t − ψ t 2 ≤ η T + 1 T T −2 t=0 E πz ϕ t + γψ t+1 − ψ t 2 ≤ η T + 1 T T −2 t=0 ϵ ≤ η T + T − 1 T ϵ(16)
After combining the two previously derived Equations 15 and 16, we get:
E πz 1 T T −1 t=0 ϕ t − z 2 ≤ β + η T + T − 1 T ϵ(17)
Now we intend to prove that lim T →∞ η T = 0
η T = E πz 1 T (γψ 0 − γE πz [ ψ T | s T −1 , a T −1 ]) 2 = γ T E πz [∥(ψ 0 − E πz [ ψ T | s T −1 , a T −1 ])∥ 2 ] ≤ γ T E πz [∥ψ 0 ∥ 2 + ∥E πz [ ψ T | s T −1 , a T −1 ]∥ 2 ] (triangular inequality) ≤ γ T (E πz [∥ψ 0 ∥ 2 ] + E πz [∥E πz [ ψ T | s T −1 , a T −1 ]∥ 2 ])
As the space of features Φ is bounded, there exist a ρ &gt; 0 such that for all ϕ ∈ Φ, ∥ϕ∥ 2 ≤ ρ.Hence, for all t,
∥ψ t ∥ 2 = E πz ∞ i=0 γ i ϕ t+i 2 s t , a t ≤ E πz ∞ i=0 γ i ϕ t+i 2 s t , a t ≤ ρ 1−γ . Hence, η T ≤ γ T ρ 1 − γ + E πz [∥E πz [ ψ T | s T −1 , a T −1 ]∥ 2 ] ≤ γ T ρ 1 − γ + E πz [E πz [ ∥ψ T ∥ 2 | s T −1 , a T −1 ]] (Jensen's inequality) ≤ γ T ρ 1 − γ + E πz [∥ψ T ∥ 2 ] (law of total expectation) ≤ γ T ρ 1 − γ + ρ 1 − γ ≤ 1 T 2ργ 1 − γ (18)
Then, knowing that for all T , we have 0 ≤ η T , the squeeze theorem ensures that lim T →∞ η T = 0. Now we will prove that the left-hand side of Equation 14 converges, let
X T := 1 T T −1 t=0 ϕ t − z 2 . For all T , |X T | ≤ 1 T T −1 t=0 ∥ϕ t ∥ 2 + ∥z∥ 2 (triangular inequality) ≤ ρ + ∥z∥ 2
Moreover, z is a fixed variable, which means that |X T | is bounded.In addition, E πz [ρ + ∥z∥ 2 ] &lt; ∞, and the sequence (X T ) T ≥1 converges almost surely (since
1 T T −1 t=0 ϕ t T ≥1
converges almost surely by hypothesis).The dominated convergence theorem then ensures that (E πz [X T ]) T ≥1 converges and:
lim T →∞ E πz [X T ] = E πz lim T →∞ X T(19)
Finally, by taking the Equation 14to the limit as T → ∞, we get:
lim T →∞ E πz       1 T T −1 t=0 ϕ t − z 2 X T       ≤ lim T →∞ β + η T + T − 1 T ϵ E πz lim T →∞ 1 T T −1 t=0 ϕ t − z 2 ≤ β + lim T →∞ η T =0 + lim T →∞ T − 1 T =1 ϵ E πz lim T →∞ 1 T T −1 t=0 ϕ t − z 2 ≤ β + ϵ Algorithm 2λ ← θ λ − α λ ∇J λ (θ λ )
▷ Update Lagrange multiplier with Eq. 5
θ Q,i ← θ Q,i − α Q ∇J Q (θ Q,i ) for i ∈ {1, 2}
▷ Policy evaluation for the Q-networks (Haarnoja et al., 2019)
θ ψ ← θ ψ − α ψ ∇J ψ (θ ψ )
▷ Policy evaluation for successor features with Eq. 2
θ π ← θ π + α π ∇J π (θ π )
▷ Policy improvement with Eq.21
β ← β − α β ∇J β (β)
▷ Adjust temperature as in (Haarnoja et al., 2019)  4. To that end, we use the Soft Actor-Critic (SAC) algorithm with adjusted temperature β (Haarnoja et al., 2019).Then the objective from Equation 4 needs to be slightly modified to be optimized by SAC:
▷ Update target networks θ ′ Q,i ← τ θ Q,i + (1 − τ )θ ′ Q,i for i ∈ {1, 2} θ ′ ψ ← τ θ ψ + (1 − τ )θ ′J π (θ π ) = (1 − λ(s, z)) Q(s, a, z) − λ(s, z)∥(1 − γ)ψ(s, a, z) − z∥ 2 + β log π(a|s, z)
Entropy regularization term used in SAC (20) Using the same notations as (Haarnoja et al., 2019), each action a returned by the policy π can be seen as the function of the state s, the skill z, and a random noise ϵ: a = f θπ (s, z, ϵ).Then the complete form of the actor's objective function is as follows:
J π (θ π ) = (1 − λ(s, z)) Q(s, f θπ (s, z, ϵ), z)−λ(s, z)∥(1 − γ)ψ(s, f θπ (s, z, ϵ), z) − z∥ 2 +β log π(f θπ (s, z, ϵ)|s, z)(21)
In our setup, the policy π outputs a vector µ and a vector of standard deviations σ 1 • • • σ n .The action a is computed as follows:
a = µ + σ 1 ϵ 1 • • • σ n ϵ n , where ϵ ∼ N (0, I).
The Q-network is trained using the exact same procedure as in (Haarnoja et al., 2019), with a clipped double-Q trick (Fujimoto et al., 2018).The successor features network ψ is trained to minimize the Bellman error (see Eq. 7).The targets of the double Q-network and of the successor features network are updated at each iteration using soft target updates, in order to stabilize training (Lillicrap et al., 2015).</p>
<p>Algorithm 2 provides a detailed description of the training procedure of QDAC.</p>
<p>C.2. Expanded Information on QDAC-MB</p>
<p>We provide here additional details on world models, and on our implementation of QDAC's model-based variant.
W ← θ W − α W ∇J W (θ W ) ▷ Update world model ▷ Training steps from a rollout in imagination with skills z ∼ U(Z) θ λ ← θ λ − α λ ∇J λ (θ λ )
▷ Update Lagrange multiplier with Eq. 5 θ
V ← θ V − α V ∇J V (θ V )
▷ Policy evaluation for value function with Eq. 1
θ ψ ← θ ψ − α ψ ∇J ψ (θ ψ )
▷ Policy evaluation for successor features with Eq. 2
θ π ← θ π + α π ∇J π (θ π )
▷ Policy improvement with Eq. 23 end for until convergence C.2.1.WORLD MODELS Learning a skill-conditioned function approximator is challenging because in general, the agent will only see a small subset of possible (s, z) combinations (Schaul et al., 2015;Borsa et al., 2018).In that case, a world model can be used to improve sample efficiency.One key advantage of model-based methods is to learn a compressed spatial and temporal representation of the environment to train a simple policy that can solve the required task (Ha &amp; Schmidhuber, 2018).World models are particularly valuable for conducting simulated rollouts in imagination which can subsequently inform the optimization of the agent's behavior, effectively reducing the number of environment interactions required for learning (Hafner et al., 2019a).Moreover, world models enable to compute straight-through gradients, which backpropagate directly through the learned dynamics (Hafner et al., 2023).Most importantly, the small memory footprint of imagined rollouts enables to sample thousands of on-policy trajectories in parallel (Hafner et al., 2023), making possible to learn skill-conditioned function approximators with massive skill sampling in imagination.</p>
<p>In this work, we use a Recurrent State Space Model (RSSM) from Hafner et al. (2019b).At each iteration, the world model W is trained to learn the transition dynamics, and to predict the observation, reward, and termination condition.An Imagination MDP ( S, A, p, γ), can then be defined from the latent states s ∈ S and from the dynamics p of W. In parallel, DreamerV3 trains a critic network V ( s t ) to regress the λ-return V λ ( s t ) (Sutton &amp; Barto, 2018).Then, the actor is trained to maximize V λ , with an entropy regularization for exploration:
J π (θ π ) = E a∼π(.| s) s ′ ∼ p(.| s,a) H t=1 V λ ( s t ) .</p>
<p>C.2.2. QDAC-MB</p>
<p>QDAC-MB's pseudocode is provided in Algorithm 3. At each iteration, a skill z is uniformly sampled and for T steps, the agent interacts with the environment following skill z with π(•|•, z).At each step, the transition is stored in a dataset D, which is used to perform a world model training step.Then, N skills are uniformly sampled to perform rollouts in imagination, and those rollouts are used to (1) train the two critics V (s, z), ψ(s, z) and (2) train the actor π.</p>
<p>World model training</p>
<p>The dataset is used to train the world model W according to DreamerV3.In addition to the reward r t , we extend the model to estimate the features ϕ t , like shown on Figure C.32.</p>
<p>Critic training</p>
<p>The estimated rewards r t and features ϕ t predicted by the world model are used to predict the value function V and the successor features ψ respectively.Then, similarly to DreamerV3, the value function V and successor features ψ are trained to regress the λ-returns, V λ and ψ λ respectively.The successor features target is defined recursively as follows:
ψ λ ( s t , z) = ϕ t + γ c t (1 − λ) ψ( s t+1 , z) + λψ λ ( s t+1 , z) and ψ λ ( s H , z) = ψ( s H , z)(22)
Actor training For each actor training step, we sample N skills z 1 . . .z N ∈ Z.We then perform N rollouts of horizon H in imagination using the world model and policies π(•|•, z i ).Those rollouts are used to train the critic v, the successor features network ψ, and the actor by backpropagating through the dynamics of the model.The actor maximizes the following objective, with an entropy regularization for exploration, where sg (•) represents the stop gradient function.
J π (θ π ) = E s 1:H ∼W,π z∼U (Z)   H t=1 (1 − sg (λ)) V λ ( s t , z) Performance − sg (λ) ∥(1 − γ)ψ λ ( s t , z) − z∥ 2 Distance to desired skill z  (23)</p>
<p>D.3. Evaluation Metrics Details</p>
<p>In this section, we illustrate how to compute and read the distance and performance profiles in Figure 4.In the Quality-Diversity community, there is a consensus that the best evaluation metric is the "distance/performance profile" (Flageat et al., 2022;Grillotti et al., 2023;Grillotti &amp; Cully, 2022a;Batra et al., 2023).This metric is also being used in skill learning for robotics (Margolis et al., 2022).</p>
<p>The profiles are favored because it effectively captures the essence of what QD algorithms aim to achieve: not just finding a single optimal solution but exploring a diverse set of high-quality solutions.For a given distance d, the distance profile shows the proportion of skills with distance to skill lower than d.For a given performance p, the performance profile shows the proportion of skills with a performance higher than p.The bigger the area under the curve, the better the algorithm is.</p>
<p>The profiles have similarities with the cumulative distribution functions in probability.All the skills that are not successfully executed by the policy are filtered out before computing the performance heatmap and profile.This figure also illustrates how to read the performance of the highest-performing skill (maximal performance of the agent) and the distance to skill of the best executed skill (minimal distance to skill of the agent).</p>
<p>E. Baselines details
(R ≥ R * − ϵ)rt) ✗ ✗ Reverse SMERL ‡ γ t (1(R &lt; R * − ϵ)rt + λrt) ✗ ✗ No-SF (1 − λ) γ t rt − λ γ t ∥ϕ t − z∥ 2 ✓ ✓ Fixed-λ (1 − λ) γ t rt − λ (1 − γ) γ t ϕ t − z 2 ✓ ✗ UVFA
(1 − λ) γ t rt − λ γ t ∥ϕ t − z∥ 2 ✓ ✗ † DCG-ME and QD-PG learn diverse skills with mechanisms that are not visible in their objective function.‡ see Section E.4 for a detailed explanation of the reward used in SMERL and Reverse SMERL.</p>
<p>E.1. DCG-ME</p>
<p>DCG-ME (Faldor et al., 2023a) is a QD algorithm based on MAP-Elites (Mouret &amp; Clune, 2015), that combines evolutionary methods with reinforcement learning to improve sample efficiency.DCG-ME addresses these challenges through two key innovations: First, it enhances the Policy Gradient variation operator with a descriptor-conditioned critic.This allows for a more nuanced exploration of the solution space by guiding the search towards uncharted territories of high diversity and performance.Second, by utilizing actor-critic training paradigms, DCG-ME learns a descriptor-conditioned policy that encapsulates the collective knowledge of the population into a singular, versatile policy capable of exhibiting a wide range of behaviors without incurring additional computational costs.</p>
<p>E.2. QD-PG</p>
<p>QD-PG (Pierrot et al., 2022) is a QD algorithm based on MAP-Elites (Mouret &amp; Clune, 2015) that integrates Policy Gradient methods with QD approaches, aiming to generate a varied collection of neural policies that perform well within continuous control environments.The core innovation of QD-PG lies in its Diversity Policy Gradient (DPG), a mechanism designed to enhance diversity among policies in a sample-efficient manner.This is achieved by leveraging information available at each time step to nudge policies toward greater diversity.The policies in the MAP-Elites grid are subjected to two gradient-based mutation operators, specifically tailored to augment both their quality (performance) and diversity.This dual-focus approach not only addresses the exploration-exploitation dilemma inherent in learning but also enhances robustness by producing multiple effective solutions for the given problem.</p>
<p>The diversity policy gradient is based on the maximization of an intrinsic reward defined as: rt = J j=1 ∥ϕ(s t ) − ϕ(s j )∥ 2 , where the J states s j are coming from an archive of past encountered states.</p>
<p>E.3. DOMiNO</p>
<p>DOMiNO (Zahavy et al., 2022) considers a set of policies (π i ) i∈[1,N ] , and intends to maximize simultaneously their quality while also maximizing the diversity of those that are near-optimal.Thus each policy π i maximizes the following objective:</p>
<p>(1 − λ) γ t r i t + λ γ t ri t where r i t is the task reward (also called extrinsic reward) and ri t is the diversity reward.Also, λ i refers to the Lagrange multiplier of policy π i ; it is used to balance between (1) the maximization of the extrinsic reward when the policy is not near-optimal, and (2) the maximization of diversity when the policy is near-optimal.The definition of near-optimality is given by the first policy π 1 .</p>
<p>The first policy π 1 only maximizes the expected sum of extrinsic rewards without considering any diversity.Hence, r1 = 0 and its Lagrange multiplier λ 1 is always equal to 0. Its average extrinsic reward V 1 e is estimated empirically and used to define when the other policies are near-optimal.The other policies (π i ) i≥2 are considered near-optimal when their average extrinsic reward V i e is higher than α V 1 e (considering V 1 e is positive) where α is a constant between 0 and 1.If a policy is not near-optimal, its Lagrange coefficient λ i decreases to focus on maximizing the task reward; if it is near-optimal, the coefficient increases to give more importance to the diversity reward ri .For policies (π i ) i≥2 , the diversity reward balances between repulsion and attraction of the average features ψ i experienced by the policies:
ri t = 1 − l i l 0 3 ϕ i t • ( ψ i − ψ j * )
where j * = arg min j ψ i − ψ j 2 and l 0 is a constant.</p>
<p>E.4. SMERL and Reverse SMERL</p>
<p>To estimate the optimal return R M (π * M ) required by SMERL, we apply the same method as Kumar et al. (2020).We trained SAC on each environment and used SAC performance R SAC as the the optimal return value for each environment.Similarly to Kumar et al. (2020), we choose λ = 2.0 by taking the best value when evaluated on HalfCheetah environment.</p>
<p>We use SMERL with continuous DIAYN with a Gaussian discriminator (Choi et al., 2021), so that the policy learns a continuous range of skills instead of a finite number of skills (Kumar et al., 2020).Finally, we use DIAYN + prior (Eysenbach et al., 2018;Chalumeau et al., 2022) to guide SMERL and Reverse SMERL towards relevant skills as explained in DIAYN's original paper.</p>
<p>With a Gaussian discriminator q(z DIAYN |s) = N (z DIAYN |µ(s), Σ(s)), the intrinsic reward is of the form r = log q(z DIAYN |s) − log p(z DIAYN ) ∝ ∥µ(s) − z DIAYN ∥ 2 2 up to an additive and a multiplicative constant, as demonstrated by Choi et al. (2021).Replacing the state s with the prior information ϕ(s) in the discriminator gives r ∝ − ∥µ(ϕ(s)) − z DIAYN ∥ 2 2 .Consequently, we can see that the intrinsic reward from DIAYN corresponds to executing a latent skill z DIAYN (i.e.achieving a latent goal) in the unsupervised space defined by the discriminator q(z DIAYN |ϕ(s)).Indeed, the intrinsic reward is analogous to the reward used in GCRL of the form r ∝ − ∥ϕ(s) − g∥ 2 (Liu et al., 2022).Moreover, the bijection between the latent skills (i.e.latent goals) and the features (i.e.goals) is given by z DIAYN ∼ q(z DIAYN |ϕ(s)).</p>
<p>E.5. No-SF</p>
<p>We can show that the constraint in QDAC's objective function is easier to satisfy than No-SF's constraint.</p>
<p>For all skills z ∈ Z and for all sequences of states (s t ) t≥0 , we have:
(1 − γ) ∞ t=0 γ t ϕ t − z 2 = (1 − γ) ∞ t=0 γ t ϕ t − ∞ t=0 γ t z 2 = (1 − γ) ∞ t=0 γ t (ϕ t − z) 2 ≤ (1 − γ) ∞ t=0 γ t ∥ϕ t − z∥ 2
Thus, we have the following inequality:
∥(1 − γ)ψ(s, z) − z∥ 2 ≤ (1 − γ) ∞ t=0 γ t ∥ϕ t − z∥ 2
At each timestep t, No-SF tries to satisfy ϕ t = z, whereas QDAC approximately tries to satisfy lim T →∞ 1 T T t=0 ϕ t = z, which is less restrictive.</p>
<p>Figure 1
1
Figure 1.a) QDAC's architecture: the agent π(a|s, z) learns high-performing and diverse behaviors with a dual critics optimization V (s, z) and ψ(s, z) which are balanced with a Lagrange multiplier λ(s, z).b) Example of diverse behaviors on a set of challenging continuous control tasks.c) Few-shot adaptation tasks and hierarchical learning tasks using the diversity of skills learned by QDAC.</p>
<p>Figure 2 .
2
Figure2.The Lagrange multiplier is optimized to balance the quality-diversity trade-off, see Eq. 5. a) If the expected features (1 − γ)ψ(s, z) is in the neighborhood of z, then λ(s, z) decreases to focus on maximizing the return.b) Otherwise, λ(s, z) increases to focus on executing z. c) After the Lagrange multiplier is updated, the policy is optimized according to the objective.</p>
<p>Figure 3 .
3
Figure 3. Distance and performance scores normalized and aggregated across all tasks.The values correspond to the IQM while the error bars represent IQM 95% CI.</p>
<p>Figure 4 .
4
Figure 4. (top) Distance profiles and (bottom) performance profiles for each task defined in Section 5.3.The lines represent the IQM for 10 replications, and the shaded areas correspond to the 95% CI. Figure D.33 illustrates how to read distance and performance profiles.</p>
<p>Figure 5 .
5
Figure 5. Ant Velocity Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.The heatmap represents the skill space Z = [−5 m/s, 5 m/s] 2 , of target velocities.This space is discretized into cells, with each cell representing a distinct skill z = vx vy ⊺ .In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ).The heatmaps for other tasks are presented in section A.2.</p>
<p>Figure 6 .
6
Figure6.Performance for each algorithm in environments with different levels of perturbations after few-shot adaptation or after hierarchical learning.The lines represent the IQM for 10 replications, and the shaded areas correspond to the 95% CI.</p>
<p>Figure A. 7 .Figure A. 8 .
78
Figure A.7. Probabilities of improvement of QDAC over all other baselines, aggregated across all tasks, as defined by Agarwal et al. (2021).</p>
<p>Figure A.9. IQM for distance and performance scores per task.</p>
<p>Figure A.11. Per-task probabilities of improvement (as defined by Agarwal et al. (2021)) of QDAC-MB over all other baselines.</p>
<p>Figure A.12. Humanoid Feet Contact Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.The heatmap represents the skill space of feet contacts Z = [0, 1] 2 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2 ⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.</p>
<p>Figure A.14. Walker Feet Contact Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.The heatmap represents the skill space of feet contacts Z = [0, 1] 2 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2 ⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.</p>
<p>Figure A.18. (top) Distance profiles and (bottom) performance profiles for each task defined in Section 5.3 similar to Figure 4 except that skills that are not successfully executed (i.e.d(z) &gt; d eval ) are not filtered out.The lines represent the IQM for 10 replications, and the shaded areas correspond to the 95% CI. Figure D.33 illustrates how to read distance and performance profiles.</p>
<p>Figure A.19. Humanoid Feet Contact Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.In the bottom row, the skills that are not successfully executed (i.e.d(z) &gt; d eval ) are not filtered out.</p>
<p>FigureA.25.(top)  Distance profiles and (bottom) performance profiles for each task defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in(Chalumeau et al., 2022).The lines represent the IQM for 10 replications, and the shaded areas correspond to the 95% CI. Figure D.33 illustrates how to read distance and performance profiles.</p>
<p>FigureA.25.(top)  Distance profiles and (bottom) performance profiles for each task defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in(Chalumeau et al., 2022).The lines represent the IQM for 10 replications, and the shaded areas correspond to the 95% CI. Figure D.33 illustrates how to read distance and performance profiles.</p>
<p>FigureA.27.Ant Contact Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in(Chalumeau et al., 2022).The heatmap represents the skill space of feet contacts Z = [0, 1] 4 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2 z3 z4⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row,</p>
<p>FigureA.28.Walker Feet Contact Heatmaps of (top)  negative distance to skill, (bottom) performance defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in(Chalumeau et al., 2022).The heatmap represents the skill space of feet contacts Z = [0, 1] 2 .This space is discretized into cells, with each cell representing a distinct skill z = z1 z2⊺ , where zi is the proportion of time that leg i touches the ground over an entire episode.In the bottom row, empty cells</p>
<p>FigureA.29.Humanoid Jump Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in(Chalumeau et al., 2022).The heatmap represents the skill space of jumping skills Z = [0, 0.25].This space is discretized into cells, with each cell representing a distinct skill; in this task, the skills refer to the average of the lowest foot heights over an entire episode.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.</p>
<p>FigureA.31.Humanoid Angle Heatmaps of (top) negative distance to skill, (bottom) performance defined in Section 5.3.We present here the results from DOMiNO, SMERL and Reverse SMERL using a method analogous to that in(Chalumeau et al., 2022).The heatmap represents the skill space of body angles Z =] − π, π].This space is discretized into cells, with each cell representing a distinct skill; in this task, the skills refer to the angle of the humanoid body about the z-axis.In the bottom row, empty cells show which skills are not successfully executed (i.e.d(z) &gt; d eval ), while colorized cells indicate the performance score obtained for the corresponding skill.</p>
<p>Expanded Information on QDACThe policy parameters θ π are optimized to maximize the objective function from Equation</p>
<p>Detailed training procedure of QDAC input Parameters θ π , θ V , θ ψ , θ λ ▷ Initial parameters for the actor, critics and Lagrange multiplier D ← ∅ ▷ Initialize an empty replay buffer repeat z ∼ U (Z) ▷ Sample skill uniformly from skill space for T steps do ▷ Environment steps a t ∼ π(a t |s t , z) ▷ Sample action from policy s t+1 ∼ p(s t+1 |s t , a t , z) ▷ Sample transition from the environment D ← D ∪ {(s t , a t , r(s t , a t ), ϕ(s t , a t ), s t+1 , z)} ▷ Store transition in the replay buffer ▷ Training steps θ</p>
<p>Algorithm 3 QDAC-MB input Parameters θ π , θ V , θ ψ , θ λ , θ W ▷ Initial parameters for the actor, critics, Lagrange multiplier and world model D ← ∅ ▷ Initialize an empty replay buffer repeat z ∼ U (Z) ▷ Sample skill uniformly from skill space for T steps do ▷ Environment steps a t ∼ π(a t | s t , z) ▷ Sample action from policy s t+1 ∼ p(s t+1 |s t , a t , z) ▷ Sample transition from the environment D ← D ∪ {(s t , a t , r(s t , a t ), ϕ(s t , a t ), s t+1 )} ▷ Store transition in the replay buffer θ</p>
<p>Figure C.32. Imagination rollout performed within the world model W. Each individual rollout i generates on-policy transitions following skill zi, starting from a state s1 for a fixed number of steps H.The world model predicts ri and ϕ i that enable to compute Vi and ψ i respectively.
111112222HHHH12H</p>
<p>Table E.3.Comparison of the main features for the different algorithms Algorithm Objective Function Model-based Lagrange Multiplier λQDAC (1 − λ) γ t rt − λ (1 − γ) γ t ϕ t − z 2 ✗ ✓ QDAC-MB (1 − λ) γ t rt − λ (1 − γ) γ t ϕ t − z 2
✓✓DCG-ME  † QD-PG  †γ t exp(− γ t rt or∥z−z ′ ∥ 2 l γ t rt)rt✗ ✗✗ ✗DOMiNO(1 − λ) γ t rt + λ γ t rt✗✓SMERL  ‡γ t (rt + λ1
For all adaptation tasks, the reward stays the same but the dynamics of the MDP is changed.The goal is to leverage the diversity of skills to adapt to unforeseen situations.For all few-shot adaptation tasks, we evaluate all skills for each replication of each method and select the best one to solve the adaptation task.In Figure6, the lines represent the IQM for the 10 replications and the shaded areas correspond to the 95% CI.On Humanoid -Hurdles, we use the jump features to jump over hurdles varying in height from 0 to 50 cm.On Humanoid -Motor Failure, we use the feet contact features to find the best way to continue walking forward despite the damage.In this experiment, we scale the action corresponding to the torque of the left knee (actuator 10) by the damage strength (x-axis of Figure6) ranging from 0.0 (no damage) to 1.0 (maximal damage).On Ant -Gravity, we use the feet contact features to find the best way to continue walking forward despite the change in gravity.In this experiment, we scale the gravity by a coefficient ranging from 0.5 (low gravity) to 3.0 (high gravity).On Walker -Friction, we use the feet contact features to find the best way to continue walking forward despite the change in friction.In this experiment, we scale the friction by a coefficient ranging from 0.0 (low friction) to 5.0 (high friction).D.2.2. HIERARCHICAL LEARNINGFor the hierarchical learning task, we learn a meta-controller that selects the skills of the policy in order to adapt to the new task.On Ant -Wall, the meta-controller is trained with SAC to select the velocity skills that enables to go around the wall and move forward as fast as possible in order to maximize performance.F. HyperparametersWe provide here all the hyperparameters used for QDAC and all baselines.QDAC-MB uses the same hyperparameters as the ones used by DreamerV3(Hafner et al., 2023), hence we provide here only the parameters mentioned in this work.The implementation of QDAC-MB is based on the implementation of DreamerV3.Its successor features network is also implemented as a distributional critic.In our implementation, the Lagrange multiplier network λ is only conditioned on the skill z, as we noticed no difference in performance.The hyperparameters for SMERL, Reverse SMERL and DCG-ME are exactly the same as in their original papers, where they were fine-tuned on similar locomotion tasks.We also tried using hidden layers of size 512 for those baselines, in order to give them an architecture that is closer to QDAC.We noticed a statistically significant decrease in performance for Reverse SMERL and DCG-ME, and no statistically significant change in performance for SMERL.Each algorithm is run until convergence for 10 7 environment steps.The hyperparameters for DOMiNO are based on the ones suggested byZahavy et al. (2022)and fine-tuned for our tasks.
Maximum a posteriori policy optimisation. A Abdolmaleki, J T Springenberg, Y Tassa, R Munos, N Heess, M A Riedmiller, 6th International Conference on Learning Representations, ICLR 2018. Vancouver, BC, CanadaApril 30 -May 3, 2018. 2018Conference Track Proceedings. OpenReview.net</p>
<p>On Multi-objective Policy Optimization as a Tool for Reinforcement Learning: Case Studies in Offline RL and Finetuning. A Abdolmaleki, S H Huang, G Vezzani, B Shahriari, J T Springenberg, S Mishra, D Tb, A Byravan, K Bousmalis, A Gyorgy, C Szepesvari, R Hadsell, N Heess, M Riedmiller, arXiv:2106.08199August 2023. </p>
<p>Deep Reinforcement Learning at the Edge of the Statistical Precipice. R Agarwal, M Schwarzer, P S Castro, A C Courville, M Bellemare, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Hierarchical quality-diversity for online damage recovery. M Allard, S C Smith, K Chatzilygeroudis, A Cully, 10.1145/3512290.3528751Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '22. the Genetic and Evolutionary Computation Conference, GECCO '22New York, NY, USAAssociation for Computing MachineryJuly 2022</p>
<p>Successor features for transfer in reinforcement learning. A Barreto, W Dabney, R Munos, J J Hunt, T Schaul, H Van Hasselt, D Silver, Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates IncDecember 2017</p>
<p>Proximal policy gradient arborescence for quality diversity reinforcement learning. S Batra, B Tjanaka, M C Fontaine, A Petrenko, S Nikolaidis, G S Sukhatme, 10.48550/arXiv.2305.137952023</p>
<p>Universal Successor Features Approximators. D Borsa, A Barreto, J Quan, D J Mankowitz, H V Hasselt, R Munos, D Silver, T Schaul, September 2018</p>
<p>Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery. F Chalumeau, R Boige, B Lim, V Macé, M Allard, A Flajolet, A Cully, T Pierrot, September 2022</p>
<p>Reset-free trial-and-error learning for robot damage recovery. K Chatzilygeroudis, V Vassiliades, J.-B Mouret, 10.1016/j.robot.2017.11.010.URLhttps://www.sciencedirect.com/science/article/pii/S0921889017302440Robotics and Autonomous Systems. 0921-88901002018</p>
<p>Learning diverse skills for local navigation under multi-constraint optimality. J Cheng, M Vlastelica, P Kolev, C Li, G Martius, Intrinsically-Motivated and Open-Ended Learning Workshop @NeurIPS2023. 2023</p>
<p>Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning. J Choi, A Sharma, H Lee, S Levine, S S Gu, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLRJuly 2021</p>
<p>Scaling map-elites to deep neuroevolution. C Colas, V Madhavan, J Huizinga, J Clune, Proceedings of the 2020 Genetic and Evolutionary Computation Conference. the 2020 Genetic and Evolutionary Computation Conference2020</p>
<p>Quality and diversity optimization: A unifying modular framework. A Cully, Y Demiris, 10.1109/TEVC.2017.2704781IEEE Transactions on Evolutionary Computation. 2222018</p>
<p>Robots that can adapt like animals. A Cully, J Clune, D Tarapore, J.-B Mouret, 10.1038/nature14422arXiv:1407.3501Nature. 0028-08365217553May 2015cs, q-bio</p>
<p>Improving generalization for temporal difference learning: The successor representation. P Dayan, A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, 10.1038/s41586-020-03157-9doi: 10.1038/ s41586-020-03157-9Neural Computation. 1476-4687541993. February 2021Nature</p>
<p>Diversity is All You Need: Learning Skills without a Reward Function. B Eysenbach, A Gupta, J Ibarz, S Levine, September 2018</p>
<p>MAP-Elites with Descriptor-Conditioned Gradients and Archive Distillation into a Single Policy. M Faldor, F Chalumeau, M Flageat, A Cully, 10.1145/3583131.3590503Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '23. the Genetic and Evolutionary Computation Conference, GECCO '23New York, NY, USAJuly 2023aAssociation for Computing Machinery. ISBN 9798400701191</p>
<p>Synergizing quality-diversity with descriptor-conditioned reinforcement learning. M Faldor, F Chalumeau, M Flageat, A Cully, 2023b</p>
<p>Model-agnostic metalearning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, AustraliaAugust 201770JMLR.org</p>
<p>Uncertain quality-diversity: Evaluation methodology and new methods for quality-diversity in uncertain domains. M Flageat, A Cully, 10.1109/TEVC.2023.3273560IEEE Transactions on Evolutionary Computation. 2023</p>
<p>Benchmarking Quality-Diversity Algorithms on Neuroevolution for Reinforcement Learning. M Flageat, B Lim, L Grillotti, M Allard, S C Smith, A Cully, arXiv:2211.02193November 2022</p>
<p>Covariance matrix adaptation map-annealing. M Fontaine, S Nikolaidis, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2023</p>
<p>Covariance matrix adaptation for the rapid illumination of behavior space. M C Fontaine, J Togelius, S Nikolaidis, A K Hoover, 10.1145/3377930.3390232Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO '20. the 2020 Genetic and Evolutionary Computation Conference, GECCO '20New York, NY, USAJune 2020Association for Computing Machinery</p>
<p>Brax -A Differentiable Physics Engine for Large Scale Rigid Body Simulation. C D Freeman, E Frey, A Raichuk, S Girgin, I Mordatch, O Bachem, June 2021</p>
<p>Addressing function approximation error in actor-critic methods. S Fujimoto, H Hoof, D Meger, International conference on machine learning. PMLR2018</p>
<p>Hierarchical Skills for Efficient Exploration. J Gehring, G Synnaeve, A Krause, N Usunier, November 2021</p>
<p>. K Gregor, D J Rezende, D Wierstra, arXiv:1611.07507Variational Intrinsic Control. November 2016</p>
<p>Relevance-guided unsupervised discovery of abilities with quality-diversity algorithms. L Grillotti, A Cully, 10.1145/3512290.3528837Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '22. the Genetic and Evolutionary Computation Conference, GECCO '22New York, NY, USAAssociation for Computing MachineryJuly 2022a</p>
<p>Unsupervised behavior discovery with quality-diversity optimization. L Grillotti, A Cully, 10.1109/TEVC.2022.3159855IEEE Transactions on Evolutionary Computation. 2662022b</p>
<p>Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains. L Grillotti, M Flageat, B Lim, A Cully, 10.1145/3583131.3590498Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '23. the Genetic and Evolutionary Computation Conference, GECCO '23New York, NY, USAJuly 2023Association for Computing Machinery. ISBN 9798400701191</p>
<p>S S Gu, M Diaz, D C Freeman, H Furuta, S K S Ghasemipour, A Raichuk, B David, E Frey, E Coumans, O Bachem, Braxlines: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization, October 2021. </p>
<p>. D Ha, J World Schmidhuber, Models, 10.5281/zenodo.1207631arXiv:1803.10122March 2018cs, stat</p>
<p>Soft Actor-Critic Algorithms and Applications. T Haarnoja, A Zhou, K Hartikainen, G Tucker, S Ha, J Tan, V Kumar, H Zhu, A Gupta, P Abbeel, S Levine, arXiv:1812.05905January 2019cs, stat</p>
<p>Dream to Control: Learning Behaviors by Latent Imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, September 2019a</p>
<p>Learning Latent Dynamics for Planning from Pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningPMLRMay 2019b</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Emergence of Locomotion Behaviours in Rich Environments. N Heess, D Tb, S Sriram, J Lemmon, J Merel, G Wayne, Y Tassa, T Erez, Z Wang, A Eslami, M Riedmiller, D Silver, July 2017</p>
<p>One solution is not all you need: few-shot extrapolation via structured MaxEnt RL. S Kumar, A Kumar, S Levine, C Finn, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates IncDecember 2020</p>
<p>Abandoning objectives: Evolution through the search for novelty alone. J Lehman, K O Stanley, Evolutionary computation. 1922011a</p>
<p>Evolving a diversity of virtual creatures through novelty search and local competition. J Lehman, K O Stanley, Proceedings of the 13th annual conference on Genetic and evolutionary computation. the 13th annual conference on Genetic and evolutionary computation2011b</p>
<p>The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities. J Lehman, J Clune, D Misevic, C Adami, L Altenberg, J Beaulieu, P J Bentley, S Bernard, G Beslon, D M Bryson, N Cheney, P Chrabaszcz, A Cully, S Doncieux, F C Dyer, K O Ellefsen, R Feldt, S Fischer, S Forrest, A Fŕenoy, C Gagńe, L Le Goff, L M Grabowski, B Hodjat, F Hutter, L Keller, C Knibbe, P Krcah, R E Lenski, H Lipson, R Maccurdy, C Maestre, R Miikkulainen, S Mitri, D E Moriarty, J.-B Mouret, A Nguyen, C Ofria, M Parizeau, D Parsons, R T Pennock, W F Punch, T S Ray, M Schoenauer, E Schulte, K Sims, K O Stanley, F Taddei, D Tarapore, S Thibault, R Watson, W Weimer, J Yosinski, 10.1162/artl_a_00319Artificial Life. 1064-5462262May 2020</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015arXiv preprint</p>
<p>Behavior from the void: Unsupervised active pre-training. H Liu, P Abbeel, Advances in Neural Information Processing Systems. 202134</p>
<p>Goal-Conditioned Reinforcement Learning: Problems and Solutions. M Liu, M Zhu, W Zhang, 10.24963/ijcai.2022/770July 20226</p>
<p>On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics. H B Mann, D R Whitney, 1947</p>
<p>Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior. G B Margolis, P Agrawal, arXiv:2212.03238December 2022cs, eess</p>
<p>Rapid Locomotion via Reinforcement Learning. G B Margolis, G Yang, K Paigwar, T Chen, P Agrawal, arXiv:2205.02824May 2022</p>
<p>Learning and Adapting Skills in Imagination. P Mazzaglia, T Verbelen, B Dhoedt, A Lacoste, S Rajeswar, Choreographer, October 2022</p>
<p>Playing Atari with Deep Reinforcement Learning, December. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.56022013</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Illuminating search spaces by mapping elites. J.-B Mouret, J Clune, arXiv:1504.04909April 2015cs, qbio</p>
<p>Policy gradient assisted MAP-Elites. O Nilsson, A Cully, 10.1145/3449639.3459304Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '21. the Genetic and Evolutionary Computation Conference, GECCO '21New York, NY, USAAssociation for Computing MachineryJune 2021</p>
<p>Diversity policy gradient for sample efficient qualitydiversity optimization. T Pierrot, V Macé, F Chalumeau, A Flajolet, G Cideron, K Beguir, A Cully, O Sigaud, N Perrin-Gilbert, 10.1145/3512290.3528845Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '22. the Genetic and Evolutionary Computation Conference, GECCO '22New York, NY, USAAssociation for Computing MachineryJuly 2022</p>
<p>Quality Diversity: A New Frontier for Evolutionary Computation. J K Pugh, L B Soros, K O Stanley, 10.3389/frobt.2016.00040Frontiers in Robotics and AI. 2296- 914432016</p>
<p>Markov Decision Processes: Discrete Stochastic Dynamic Programming. M L Puterman, 1994John Wiley &amp; Sons, Inc., USA1st edition</p>
<p>Universal Value Function Approximators. T Schaul, D Horgan, K Gregor, D Silver, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningPMLRJune 2015</p>
<p>Proximal Policy Optimization Algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347August 2017</p>
<p>Dynamics-Aware Unsupervised Discovery of Skills. A Sharma, S Gu, S Levine, V Kumar, K Hausman, September 2019</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, 10.1038/nature16961Nature. 1476-46875297587January 2016Nature Publishing Group</p>
<p>Reinforcement learning: an introduction. Adaptive computation and machine learning series. R S Sutton, A G Barto, 2018The MIT PressCambridge, Massachusettssecond edition edition</p>
<p>How do different encodings influence the performance of the map-elites algorithm. R S Sutton, D Mcallester, S Singh, Y Mansour, D Tarapore, J Clune, A Cully, J Mouret, 10.1145/2908812.2908875Proceedings of the 2016 on Genetic and Evolutionary Computation Conference. T Friedrich, F Neumann, A M Sutton, the 2016 on Genetic and Evolutionary Computation ConferenceDenver, CO, USAACM1999. July 20 -24, 2016. 201612Advances in Neural Information Processing Systems</p>
<p>Approximating gradients for differentiable quality diversity in reinforcement learning. B Tjanaka, M C Fontaine, J Togelius, S Nikolaidis, 10.1145/3512290.3528705Proceedings of the Genetic and Evolutionary Computation Conference, GECCO '22. the Genetic and Evolutionary Computation Conference, GECCO '22New York, NY, USAJuly 2022Association for Computing Machinery</p>
<p>Sample-efficient quality-diversity by cooperative coevolution. K Xue, R.-J Wang, P Li, D Li, J Hao, C Qian, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality. T Zahavy, Y Schroecker, F Behbahani, K Baumli, S Flennerhag, S Hou, S Singh, September 2022</p>
<p>Diversifying AI: Towards Creative Chess with AlphaZero. T Zahavy, V Veeriah, S Hou, K Waugh, M Lai, E Leurent, N Tomasev, L Schut, D Hassabis, S Singh, arXiv:2308.09175August 2023</p>
<p>Model-Assisted Policy Optimization for Goal-Oriented Tasks. M Zhu, M Liu, J Shen, Z Zhang, S Chen, W Zhang, D Ye, Y Yu, Q Fu, W Yang, Mapgo, arXiv:2105.06350May 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2625 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2625</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2625</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735" target="_blank">Benchmarking Large Language Models As AI Research Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes MLAgentBench, a suite of ML tasks for benchmarking AI research agents, and designs an LLM-based research agent to automatically perform experimentation loops in such an environment, finding that a GPT-4-based research agent can feasibly build compelling ML models over many tasks in MLAgentBench, displaying highly interpretable plans and actions.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2625.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2625.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLAgentBench agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM-based automated experimentation agent (MLAgentBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-model-driven agent designed to perform end-to-end machine learning experimentation by reading/writing files, executing Python code, editing scripts, reasoning (reflection, research plan, fact-check), and iterating until producing a final submission evaluated by automatic evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLAgentBench LM-based agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An agent architecture that prompts a large language model to produce a step-wise Rationale and a structured Action (JSON) at each timestep. Key capabilities: (1) inspect and summarize files (Understand File), (2) edit scripts (Edit Script / Edit Script Segment), (3) execute arbitrary Python scripts (Execute Script), (4) undo edits, (5) maintain an explicit Research Plan and Status and Fact Check entries to reduce hallucination, (6) produce interpretable stepwise reflections and next actions. The agent is implemented on a sandboxed environment (MLAgentBench) with 13 concrete ML tasks; it iteratively modifies code/data and runs experiments until submission or time/action limits.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LM-based Automated Experimentation Agent / AI Scientist (experimental)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning experimentation (computer vision, NLP, graphs, tabular regression/classification, time-series, code runtime optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>A suite of 13 ML tasks where the agent must improve baseline model performance (or improve runtime) by interacting with a workspace: tasks include improving CIFAR-10 CNN accuracy, finetuning BERT for imdb sentiment, ogbn-arxiv node classification, house-price regression, multiple Kaggle challenges (spaceship-titanic, parkinsons-disease, fathomnet, identify-contrails), CLRS algorithmic reasoning, BabyLM language-model pretraining over 10M words, and code speedups for LLaMA inference and vectorization.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Heterogeneous complexity spanning well-established low-dimensional tasks (e.g., house-price tabular regression) to open-ended, high-dimensional and recent challenges (Kaggle competitions, BabyLM). Complexity factors: varied data modalities (images, text, graphs, time-series), differing evaluation metrics (accuracy, MAE, SMAPE, MCRMSE, MAP@20, perplexity, wall-clock time), recency (some datasets postdate LM pretraining), open-ended design/implementation search space (model architecture, hyperparameters, feature engineering), and debugging complexity when editing code. Quantitative measures available: 13 tasks with per-task success rates reported; action budgets (max 50 actions, max 5 hours per run; GPT-4 runs limited to 30 actions).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Each task provided starter files and datasets (training and test with test labels withheld for submissions) or starter code; some tasks have baseline starter code (baseline metrics recorded), while others require building from scratch (imdb, some Kaggle tasks). Datasets vary in size and recency (canonical datasets like CIFAR-10 are standard and likely in pretraining data; several Kaggle challenges and BabyLM are recent and potentially out-of-distribution relative to LMs). No new laboratory data generation required; all tasks used pre-provided datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Runs execute Python training/validation jobs (minutes per run). Experimental protocol: 8 independent runs per agent-model pairing. Time limits: typically up to 5 hours and up to 50 actions (30 actions for GPT-4 due to cost). Token usage: total run of entire benchmark with GPT-4-turbo once consumed ~6 million tokens (~$60 at the time reported); Claude v3 Opus agents spent more tokens and wall-clock time (slower API). GPT-4-turbo reported to spend ~51% fewer tokens than average agents. Authors estimate expected monetary cost to accomplish a task given 26% average success ~ $231. Exact compute (GPU hours) for training tasks is task-dependent and not uniformly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Mixed: tasks are well-scoped with explicit evaluators (clear numeric metrics) but open-ended in how to achieve improvements (continuous hyperparameter/architecture choices, discrete code edits, stochastic training outcomes). Deterministic steps (file edits) combine with stochastic training runs; clear evaluation metrics exist per task (accuracy, MAE, perplexity, wall-clock time), enabling automatic success/failure detection (success defined as >=10% improvement over starter baseline). Domain knowledge helpful (ML expertise) but agent attempts to supply it via LM priors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Per-task evaluator metric (e.g., test accuracy, MAE, SMAPE, MCRMSE, MAP@20, perplexity, wall-clock time). Primary aggregated success definition: run is a 'success' if final submission improves performance metric by >=10% over the baseline in starter code.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Varied by LM and task. Reported averages and extremes from the paper: Claude v3 Opus agent achieved the highest average success rate across models: 37.5% (over 13 tasks, 8 trials each). Example per-task rates (Claude v3 Opus, Table 3): house-price 100%, spaceship-titanic 100%, ogbn-arxiv 87.5%, cifar10 62.5%, identify-contrails 0.0% (paper notes many tasks vary from 100% on older datasets to 0% on recent Kaggle challenges). GPT-4 achieved different tradeoffs: lower average success rate but larger average percentage improvement on tasks where it did succeed. Overall average success across all evaluated models reported ~26% (for some model/agent combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed failure modes documented in the paper: (1) Hallucination — claiming improvements or facts without executing code; (2) Bad Plan — incorrect experimental plans that don't lead to progress; (3) Response Format Error — LM outputs invalid JSON actions that cannot be parsed; (4) Submission Format Error — agent produces submission files in wrong format; (5) Getting stuck debugging complex edits (agent stalls on fixes); (6) Long-horizon planning deficiencies and re-planning failures; (7) Performance regressions when running longer (many agents degrade performance over steps). Additional limitations: struggles on recent datasets likely unseen during LM pretraining (e.g., BabyLM: 0% success).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Factors that improved agent performance included: (a) using Claude v3 Opus in this evaluation (best success rate), (b) explicit structured prompting requiring Research Plan and Status and Fact Check to reduce hallucinations and provide interpretable plans, (c) compound actions (Understand File, Edit Script) that reduce low-level prompting burden, (d) well-specified task descriptions and starter files with evaluators, and (e) contained sandbox allowing iterative code execution and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper compares multiple LMs and agent frameworks. Key comparative findings: Claude v3 Opus agent achieved highest average success rate (37.5%); GPT-4-based agents produced larger mean metric improvements when they succeeded but were more prone to hallucination and poor planning. Framework comparison (Table 5): with GPT-4-turbo, 'Ours' agent averaged 26.0% success vs AutoGPT 2.9% and LangChain 1.0%; with Claude v3 Opus, 'Ours' averaged 37.5% vs AutoGPT 13.5% and LangChain 33.7% (LangChain with Claude v3 was competitive on some tasks). Running longer tended to degrade performance for most models except Claude v3 Opus.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No human researcher baseline reported. The paper uses a 'starter code' baseline (provided baseline model or trivial predictor) and defines success relative to improving that baseline by >=10%. For tasks without runnable baselines, trivial or random prediction baselines are used (e.g., imdb random baseline ~50%/0.5 accuracy).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2625.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT (Significant-Gravitas / Auto-GPT project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source project aiming to create general-purpose autonomous agents by chaining LLM calls with tool use and external APIs; here adapted and benchmarked as an agent framework on MLAgentBench tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGPT (adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoGPT is an open-source autonomous agent framework that coordinates iterative LLM calls, tool invocations, and planning to pursue high-level goals; the paper adapts AutoGPT (with available tools restricted to the MLAgentBench environment) and evaluates its ability to perform ML experimentation across the 13 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>General-purpose Autonomous Agent / AI Scientist framework (adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning experimentation (same MLAgentBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Applied to the same 13 ML experimentation tasks in MLAgentBench — the adapted AutoGPT attempts to inspect starter files, edit/run code, and produce final submissions for ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same range as MLAgentBench tasks; AutoGPT is intended for open-ended multi-step tasks and faces the same heterogeneous modality and search-space complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the same starter files and datasets provided by MLAgentBench; no external web or package installation allowed in benchmark runs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Evaluated under same protocol (8 runs per configuration, same action/time limits). Specific token/time efficiency data not provided per AutoGPT beyond aggregated comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, multi-step, mixed deterministic (file edits) and stochastic (training) structure with explicit evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Same per-task evaluator metrics and success = >=10% improvement over starter baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported in Table 5: AutoGPT (GPT-4-turbo) average success rate 2.9%; AutoGPT (Claude v3 Opus) average success rate 13.5% across the 13 tasks (8 trials each). Performance varied by task but was substantially lower than the authors' proposed agent in most cases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>AutoGPT adaptation showed poor performance on many ML tasks in this benchmark—likely due to lack of the specialized Research Plan/Status and Fact Check scaffolding, inability to robustly plan/replan for long-horizon ML experiments, and parsing/formatting/tool-invocation fragility in this constrained environment.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>When AutoGPT succeeded, factors likely included simpler tasks with well-defined starter code and straightforward edits (e.g., tasks where baseline code needed minor parameter tuning), but overall the paper reports limited success.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared directly to the authors' agent and LangChain in Table 5: AutoGPT performed much worse than the authors' structured LM agent; with Claude v3 Opus AutoGPT (13.5% avg) was better than AutoGPT with GPT-4-turbo (2.9%) but still below the authors' agent (37.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided; same starter-code baselines apply. No human performance comparisons were reported for AutoGPT adaptation in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2625.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain (ReAct variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain (zero-shot-react-description / ReAct-style agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework for building language-model agents; the authors used a LangChain/ReAct-style baseline (zero-shot-react-description) and evaluated it on MLAgentBench for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LangChain (zero-shot-react-description adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LangChain is a modular agent framework that composes LLM prompts with programmatic tool calls. The evaluated variant used ReAct-style prompting (reasoning + action) but without the Research Plan/Status and Fact Check scaffolding the authors designed.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Agent Framework / ReAct-style LM Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning experimentation (MLAgentBench tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Adapted to perform the 13 ML tasks by using the available file-system and execution tools; expected to generate plans and execute actions via ReAct prompts but lacks additional planning/status bookkeeping.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same as other agents; performance sensitive to long-horizon planning and execution correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses MLAgentBench starter files and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Same experimental budget and environmental constraints as other agents; per-agent token/time breakdowns reported globally (LangChain runs compared in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended with clear evaluators but requires robust step-wise planning and verification to succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Per-task evaluator metrics; success = >=10% improvement over baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported in Table 5: LangChain (GPT-4-turbo) average success rate 1.0%; LangChain (Claude v3 Opus) average success rate 33.7% across tasks. The paper notes LangChain with Claude v3 Opus was surprisingly competitive on several tasks, sometimes because being simpler avoided incorrect submission-format edits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Missing higher-level Research Plan/Status and Fact Check entries made LangChain-pruned variants more error-prone on complex tasks; common issues included invalid JSON/actions and submission format mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity sometimes helped avoid dangerous edits (e.g., leaving submission format intact) leading to competitive performance on tasks that required minimal change.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>LangChain with Claude v3 Opus achieved 33.7% average success (Table 5), close to the authors' agent (37.5%), while LangChain with GPT-4-turbo performed poorly (1.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2625.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientist (Adam)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Scientist 'Adam' (King et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific automated discovery system that generated functional genomics hypotheses and experimentally tested them in the laboratory using lab automation (applied to yeast Saccharomyces cerevisiae).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist 'Adam'</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A closed-loop automated discovery system that generates hypotheses (functional genomics), plans experiments, and executes those experiments via laboratory automation; integrates hypothesis generation, experiment design, and wet-lab execution to validate predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery System / Automated Experimentation Platform</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Biology — functional genomics (yeast Saccharomyces cerevisiae)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Autonomously generate and test hypotheses about gene function and interactions in yeast using high-throughput laboratory protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Domain-specific biological experimentation involving wet-lab protocols, combinatorially large hypothesis spaces (genes and interactions), experimental stochasticity, and resource-constrained laboratory runs. Complexity is high due to laboratory procedures and biological variability.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relied on biological assay data generated by the system's experiments; data generation occurred via automated laboratory execution (not pre-existing datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Includes wet-lab execution time and laboratory resources; computational modeling/hypothesis generation components are less emphasized in the paper. The MLAgentBench paper references Adam as prior art but does not provide compute numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Closed-loop experimental structure (open scientific discovery problem but with procedural steps and measurable experimental outcomes); stochastic biological outcomes and high cost per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Scientific hypotheses validated experimentally (qualitative/quantitative lab assay results); original Adam papers reported ability to propose and test non-trivial hypotheses, but MLAgentBench does not restate quantitative success measures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in MLAgentBench paper (Adam cited as an example of domain-tailored automated scientific discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed in this paper; general challenges for such systems include domain specificity, experimental cost, and limited generalization outside designed lab pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Tight integration with laboratory automation, domain-specific instrumentation, and carefully engineered hypothesis-testing pipelines enabled closed-loop discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as contrast: Adam represents highly tailored automated discovery whereas MLAgentBench aims toward more general, flexible LM-based research assistants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in the MLAgentBench paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2625.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoML (Automated Machine Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that automate parts of the ML pipeline (model selection, hyperparameter tuning, architecture search) to reduce human engineering in building performant models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automl: A survey of the state-of-the-art</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML / Neural Architecture Search (general)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encompasses techniques such as hyperparameter optimization, neural architecture search, and pipeline automation; often uses search strategies (e.g., Bayesian optimization, evolutionary algorithms) and can be computationally intensive but domain-general for ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AutoML / Neural Architecture Search</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning model search and optimization (across vision, NLP, tabular, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automates selection of model architectures, hyperparameters, and training pipelines to optimize predefined evaluation metrics (accuracy, MAE, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Search over typically large, mixed discrete-continuous spaces (architectures, hyperparameters), multi-objective in some settings (accuracy vs. latency), computationally expensive (many training runs), and stochastic (training noise).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on available training/validation data; data volumes vary by problem. Does not generally require new data generation beyond standard supervised learning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Often high (many model trainings); survey referenced in paper documents various compute strategies; specific compute numbers not provided in MLAgentBench paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined optimization problems with clear evaluation metrics but large search spaces; can be automated because objective functions are measurable.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Model performance metrics (accuracy, MAE, validation loss), often compared to human-designed baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantitatively reported in this paper; AutoML cited as an established area demonstrating practical automation of ML workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Computational cost, overfitting to validation sets, and limited ability to handle open-ended research decisions beyond measurable optimization objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Well-defined objective, availability of validation metric, and ability to parallelize evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>AutoML contrasted with the LM-based open-ended agent approach of this paper; MLAgentBench emphasizes broader decision-making beyond pure AutoML optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>AutoML systems are typically compared to human-engineered models; no direct human baselines reported in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2625.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoML-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent approach that repeatedly prompts LMs with dataset/model information and predicts training outcomes to accelerate hyperparameter tuning and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automl-gpt: Automatic machine learning with gpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML-GPT (concurrent work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses LMs to model/predict training logs and guide efficient hyperparameter search by prompting with data and model cards and anticipating training behavior; focuses on AutoML-style hyperparameter tuning rather than open-ended code editing.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LM-assisted AutoML / Hyperparameter Tuning System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Hyperparameter tuning and AutoML within ML tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given data and model descriptions, predict training curves and recommend hyperparameter changes to improve efficiency of search.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Still large hyperparameter spaces, but approach reduces search cost by using LM priors to predict likely outcomes; complexity depends on model size and hyperparameter dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on historical training logs / model cards; MLAgentBench references it as concurrent work but does not provide datasets for it.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Less than exhaustive search if LM predictions are accurate; paper cites it as complementary work but provides no compute numbers here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-posed optimization/estimation problem with measurable metrics; not fully open-ended research discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Hyperparameter search efficiency (reduced number of expensive training runs) and final model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in MLAgentBench paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed here; general risk is LM prediction inaccuracies leading to missed optima.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Leveraging LM priors and historic training patterns to reduce expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Positioned by the authors as complementary to MLAgentBench's open-ended agent evaluation; AutoML-GPT focuses on efficient tuning rather than full experimentation autonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2625.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLCopilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLcopilot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that prompts LMs with past experiences and knowledge to predict categorized hyperparameter settings (e.g., high/low values) for ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLCopilot (concurrent work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LM-oriented assistant that uses prior runs/experience to recommend coarse hyperparameter choices or settings to guide ML experimentation, rather than performing end-to-end code edits and experiment execution.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LM-assisted Hyperparameter Recommendation System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning hyperparameter recommendation and guidance</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provides recommended hyperparameter categories (e.g., low/high weight decay) based on prior knowledge and task descriptions to speed manual or automated tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Less granular than full AutoML; reduces dimensionality by categorization but still relies on correct mapping from task characteristics to hyperparameter choices.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on prior experience / metadata; MLAgentBench mentions it as related work without numeric evaluation here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Low relative to full AutoML since it provides recommendations rather than running many trainings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Semi-structured recommendation problem with actionable suggestions but not full optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Improved tuning efficiency or final model performance if recommendations are followed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in MLAgentBench paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Coarse recommendations may miss precise optima; effectiveness depends on quality of prior experience encoded in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Effective when prior examples are representative of the current task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Contrasted by the authors as narrower in scope than their open-ended agent which must implement code edits and run experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2625.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager (LLM-driven embodied agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-ended embodied agent that uses LLMs to play Minecraft by decomposing long-term goals into executable steps and learning affordances in the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-based agent designed for an embodied environment (Minecraft) that iteratively plans, executes, and learns environment affordances to accomplish complex tasks, illustrating LLM agent capabilities in an interactive domain.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Embodied LM Agent / Generative Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Interactive game environment (Minecraft) — sequential decision-making and skill acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Open-ended tasks in a simulated environment that require long-horizon planning, tool use, and environment interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to long-horizon planning, partial observability, and large discrete action spaces; success measured in task completion/skill acquisition in game environment.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Environment-provided observations and affordance learning from interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Environment simulation and many LLM calls; MLAgentBench cites Voyager as related work but provides no performance data here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and sequential, with emergent behaviors from iterated planning and affordance learning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion and learned behaviors in the environment (not quantified in MLAgentBench paper).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Long-term planning and hallucination risks are common in such agents, similar to those observed by the MLAgentBench authors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Modularity for learning affordances and decomposing tasks into subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Cited as an example of LLM-based agents applied outside pure ML experimentation to emphasize breadth of agent research.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2625.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2625.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (language grounding for robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system combining language model affordance priors with robot low-level controllers to ground instructions into physically executable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines LLM-derived high-level intent scoring with robot affordances and low-level controllers to select feasible actions for real-world robotic tasks; an example of grounding LMs to physical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LM-grounded Robotics Execution System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Robotics — grounding language instructions to robot actions</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Mapping high-level instructions to sequences of physically executable motor commands using affordance models and LLM guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to real-world uncertainty, safety constraints, continuous control spaces, and need for robust affordance estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires environment-specific affordance models and robot perception data; MLAgentBench only mentions SayCan as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Real-time perception and control pipelines; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Sequential decision-making with real-world stochasticity and safety-critical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion and safety/feasibility in robotic execution; not reported in MLAgentBench.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Affordance misestimation, safety concerns, and mismatch between language priors and physical capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration of LLM guidance with grounded affordance models and robust controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Included as an example of successful grounding of LLM agents in the physical world; MLAgentBench contrasts domain-specific integrated systems like SayCan with their more general software-focused agent.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist <em>(Rating: 2)</em></li>
                <li>The automation of science <em>(Rating: 2)</em></li>
                <li>Automl: A survey of the state-of-the-art <em>(Rating: 2)</em></li>
                <li>Automl-gpt: Automatic machine learning with gpt <em>(Rating: 2)</em></li>
                <li>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 1)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2625",
    "paper_id": "paper-c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "MLAgentBench agent",
            "name_full": "LM-based automated experimentation agent (MLAgentBench)",
            "brief_description": "A language-model-driven agent designed to perform end-to-end machine learning experimentation by reading/writing files, executing Python code, editing scripts, reasoning (reflection, research plan, fact-check), and iterating until producing a final submission evaluated by automatic evaluators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MLAgentBench LM-based agent",
            "system_description": "An agent architecture that prompts a large language model to produce a step-wise Rationale and a structured Action (JSON) at each timestep. Key capabilities: (1) inspect and summarize files (Understand File), (2) edit scripts (Edit Script / Edit Script Segment), (3) execute arbitrary Python scripts (Execute Script), (4) undo edits, (5) maintain an explicit Research Plan and Status and Fact Check entries to reduce hallucination, (6) produce interpretable stepwise reflections and next actions. The agent is implemented on a sandboxed environment (MLAgentBench) with 13 concrete ML tasks; it iteratively modifies code/data and runs experiments until submission or time/action limits.",
            "system_type": "LM-based Automated Experimentation Agent / AI Scientist (experimental)",
            "problem_domain": "Machine learning experimentation (computer vision, NLP, graphs, tabular regression/classification, time-series, code runtime optimization)",
            "problem_description": "A suite of 13 ML tasks where the agent must improve baseline model performance (or improve runtime) by interacting with a workspace: tasks include improving CIFAR-10 CNN accuracy, finetuning BERT for imdb sentiment, ogbn-arxiv node classification, house-price regression, multiple Kaggle challenges (spaceship-titanic, parkinsons-disease, fathomnet, identify-contrails), CLRS algorithmic reasoning, BabyLM language-model pretraining over 10M words, and code speedups for LLaMA inference and vectorization.",
            "problem_complexity": "Heterogeneous complexity spanning well-established low-dimensional tasks (e.g., house-price tabular regression) to open-ended, high-dimensional and recent challenges (Kaggle competitions, BabyLM). Complexity factors: varied data modalities (images, text, graphs, time-series), differing evaluation metrics (accuracy, MAE, SMAPE, MCRMSE, MAP@20, perplexity, wall-clock time), recency (some datasets postdate LM pretraining), open-ended design/implementation search space (model architecture, hyperparameters, feature engineering), and debugging complexity when editing code. Quantitative measures available: 13 tasks with per-task success rates reported; action budgets (max 50 actions, max 5 hours per run; GPT-4 runs limited to 30 actions).",
            "data_availability": "Each task provided starter files and datasets (training and test with test labels withheld for submissions) or starter code; some tasks have baseline starter code (baseline metrics recorded), while others require building from scratch (imdb, some Kaggle tasks). Datasets vary in size and recency (canonical datasets like CIFAR-10 are standard and likely in pretraining data; several Kaggle challenges and BabyLM are recent and potentially out-of-distribution relative to LMs). No new laboratory data generation required; all tasks used pre-provided datasets.",
            "computational_requirements": "Runs execute Python training/validation jobs (minutes per run). Experimental protocol: 8 independent runs per agent-model pairing. Time limits: typically up to 5 hours and up to 50 actions (30 actions for GPT-4 due to cost). Token usage: total run of entire benchmark with GPT-4-turbo once consumed ~6 million tokens (~$60 at the time reported); Claude v3 Opus agents spent more tokens and wall-clock time (slower API). GPT-4-turbo reported to spend ~51% fewer tokens than average agents. Authors estimate expected monetary cost to accomplish a task given 26% average success ~ $231. Exact compute (GPU hours) for training tasks is task-dependent and not uniformly reported.",
            "problem_structure": "Mixed: tasks are well-scoped with explicit evaluators (clear numeric metrics) but open-ended in how to achieve improvements (continuous hyperparameter/architecture choices, discrete code edits, stochastic training outcomes). Deterministic steps (file edits) combine with stochastic training runs; clear evaluation metrics exist per task (accuracy, MAE, perplexity, wall-clock time), enabling automatic success/failure detection (success defined as &gt;=10% improvement over starter baseline). Domain knowledge helpful (ML expertise) but agent attempts to supply it via LM priors.",
            "success_metric": "Per-task evaluator metric (e.g., test accuracy, MAE, SMAPE, MCRMSE, MAP@20, perplexity, wall-clock time). Primary aggregated success definition: run is a 'success' if final submission improves performance metric by &gt;=10% over the baseline in starter code.",
            "success_rate": "Varied by LM and task. Reported averages and extremes from the paper: Claude v3 Opus agent achieved the highest average success rate across models: 37.5% (over 13 tasks, 8 trials each). Example per-task rates (Claude v3 Opus, Table 3): house-price 100%, spaceship-titanic 100%, ogbn-arxiv 87.5%, cifar10 62.5%, identify-contrails 0.0% (paper notes many tasks vary from 100% on older datasets to 0% on recent Kaggle challenges). GPT-4 achieved different tradeoffs: lower average success rate but larger average percentage improvement on tasks where it did succeed. Overall average success across all evaluated models reported ~26% (for some model/agent combinations).",
            "failure_modes": "Observed failure modes documented in the paper: (1) Hallucination — claiming improvements or facts without executing code; (2) Bad Plan — incorrect experimental plans that don't lead to progress; (3) Response Format Error — LM outputs invalid JSON actions that cannot be parsed; (4) Submission Format Error — agent produces submission files in wrong format; (5) Getting stuck debugging complex edits (agent stalls on fixes); (6) Long-horizon planning deficiencies and re-planning failures; (7) Performance regressions when running longer (many agents degrade performance over steps). Additional limitations: struggles on recent datasets likely unseen during LM pretraining (e.g., BabyLM: 0% success).",
            "success_factors": "Factors that improved agent performance included: (a) using Claude v3 Opus in this evaluation (best success rate), (b) explicit structured prompting requiring Research Plan and Status and Fact Check to reduce hallucinations and provide interpretable plans, (c) compound actions (Understand File, Edit Script) that reduce low-level prompting burden, (d) well-specified task descriptions and starter files with evaluators, and (e) contained sandbox allowing iterative code execution and verification.",
            "comparative_results": "Paper compares multiple LMs and agent frameworks. Key comparative findings: Claude v3 Opus agent achieved highest average success rate (37.5%); GPT-4-based agents produced larger mean metric improvements when they succeeded but were more prone to hallucination and poor planning. Framework comparison (Table 5): with GPT-4-turbo, 'Ours' agent averaged 26.0% success vs AutoGPT 2.9% and LangChain 1.0%; with Claude v3 Opus, 'Ours' averaged 37.5% vs AutoGPT 13.5% and LangChain 33.7% (LangChain with Claude v3 was competitive on some tasks). Running longer tended to degrade performance for most models except Claude v3 Opus.",
            "human_baseline": "No human researcher baseline reported. The paper uses a 'starter code' baseline (provided baseline model or trivial predictor) and defines success relative to improving that baseline by &gt;=10%. For tasks without runnable baselines, trivial or random prediction baselines are used (e.g., imdb random baseline ~50%/0.5 accuracy).",
            "uuid": "e2625.0"
        },
        {
            "name_short": "AutoGPT",
            "name_full": "Auto-GPT (Significant-Gravitas / Auto-GPT project)",
            "brief_description": "An open-source project aiming to create general-purpose autonomous agents by chaining LLM calls with tool use and external APIs; here adapted and benchmarked as an agent framework on MLAgentBench tasks.",
            "citation_title": "Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous.",
            "mention_or_use": "use",
            "system_name": "AutoGPT (adaptation)",
            "system_description": "AutoGPT is an open-source autonomous agent framework that coordinates iterative LLM calls, tool invocations, and planning to pursue high-level goals; the paper adapts AutoGPT (with available tools restricted to the MLAgentBench environment) and evaluates its ability to perform ML experimentation across the 13 tasks.",
            "system_type": "General-purpose Autonomous Agent / AI Scientist framework (adaptation)",
            "problem_domain": "Machine learning experimentation (same MLAgentBench tasks)",
            "problem_description": "Applied to the same 13 ML experimentation tasks in MLAgentBench — the adapted AutoGPT attempts to inspect starter files, edit/run code, and produce final submissions for ML tasks.",
            "problem_complexity": "Same range as MLAgentBench tasks; AutoGPT is intended for open-ended multi-step tasks and faces the same heterogeneous modality and search-space complexity.",
            "data_availability": "Uses the same starter files and datasets provided by MLAgentBench; no external web or package installation allowed in benchmark runs.",
            "computational_requirements": "Evaluated under same protocol (8 runs per configuration, same action/time limits). Specific token/time efficiency data not provided per AutoGPT beyond aggregated comparisons.",
            "problem_structure": "Open-ended, multi-step, mixed deterministic (file edits) and stochastic (training) structure with explicit evaluators.",
            "success_metric": "Same per-task evaluator metrics and success = &gt;=10% improvement over starter baseline.",
            "success_rate": "Reported in Table 5: AutoGPT (GPT-4-turbo) average success rate 2.9%; AutoGPT (Claude v3 Opus) average success rate 13.5% across the 13 tasks (8 trials each). Performance varied by task but was substantially lower than the authors' proposed agent in most cases.",
            "failure_modes": "AutoGPT adaptation showed poor performance on many ML tasks in this benchmark—likely due to lack of the specialized Research Plan/Status and Fact Check scaffolding, inability to robustly plan/replan for long-horizon ML experiments, and parsing/formatting/tool-invocation fragility in this constrained environment.",
            "success_factors": "When AutoGPT succeeded, factors likely included simpler tasks with well-defined starter code and straightforward edits (e.g., tasks where baseline code needed minor parameter tuning), but overall the paper reports limited success.",
            "comparative_results": "Compared directly to the authors' agent and LangChain in Table 5: AutoGPT performed much worse than the authors' structured LM agent; with Claude v3 Opus AutoGPT (13.5% avg) was better than AutoGPT with GPT-4-turbo (2.9%) but still below the authors' agent (37.5%).",
            "human_baseline": "Not provided; same starter-code baselines apply. No human performance comparisons were reported for AutoGPT adaptation in this paper.",
            "uuid": "e2625.1"
        },
        {
            "name_short": "LangChain (ReAct variant)",
            "name_full": "LangChain (zero-shot-react-description / ReAct-style agent)",
            "brief_description": "A framework for building language-model agents; the authors used a LangChain/ReAct-style baseline (zero-shot-react-description) and evaluated it on MLAgentBench for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LangChain (zero-shot-react-description adaptation)",
            "system_description": "LangChain is a modular agent framework that composes LLM prompts with programmatic tool calls. The evaluated variant used ReAct-style prompting (reasoning + action) but without the Research Plan/Status and Fact Check scaffolding the authors designed.",
            "system_type": "Agent Framework / ReAct-style LM Agent",
            "problem_domain": "Machine learning experimentation (MLAgentBench tasks)",
            "problem_description": "Adapted to perform the 13 ML tasks by using the available file-system and execution tools; expected to generate plans and execute actions via ReAct prompts but lacks additional planning/status bookkeeping.",
            "problem_complexity": "Same as other agents; performance sensitive to long-horizon planning and execution correctness.",
            "data_availability": "Uses MLAgentBench starter files and datasets.",
            "computational_requirements": "Same experimental budget and environmental constraints as other agents; per-agent token/time breakdowns reported globally (LangChain runs compared in Table 5).",
            "problem_structure": "Open-ended with clear evaluators but requires robust step-wise planning and verification to succeed.",
            "success_metric": "Per-task evaluator metrics; success = &gt;=10% improvement over baseline.",
            "success_rate": "Reported in Table 5: LangChain (GPT-4-turbo) average success rate 1.0%; LangChain (Claude v3 Opus) average success rate 33.7% across tasks. The paper notes LangChain with Claude v3 Opus was surprisingly competitive on several tasks, sometimes because being simpler avoided incorrect submission-format edits.",
            "failure_modes": "Missing higher-level Research Plan/Status and Fact Check entries made LangChain-pruned variants more error-prone on complex tasks; common issues included invalid JSON/actions and submission format mistakes.",
            "success_factors": "Simplicity sometimes helped avoid dangerous edits (e.g., leaving submission format intact) leading to competitive performance on tasks that required minimal change.",
            "comparative_results": "LangChain with Claude v3 Opus achieved 33.7% average success (Table 5), close to the authors' agent (37.5%), while LangChain with GPT-4-turbo performed poorly (1.0%).",
            "human_baseline": "Not provided.",
            "uuid": "e2625.2"
        },
        {
            "name_short": "Robot Scientist (Adam)",
            "name_full": "Robot Scientist 'Adam' (King et al.)",
            "brief_description": "A domain-specific automated discovery system that generated functional genomics hypotheses and experimentally tested them in the laboratory using lab automation (applied to yeast Saccharomyces cerevisiae).",
            "citation_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist 'Adam'",
            "system_description": "A closed-loop automated discovery system that generates hypotheses (functional genomics), plans experiments, and executes those experiments via laboratory automation; integrates hypothesis generation, experiment design, and wet-lab execution to validate predictions.",
            "system_type": "Automated Discovery System / Automated Experimentation Platform",
            "problem_domain": "Biology — functional genomics (yeast Saccharomyces cerevisiae)",
            "problem_description": "Autonomously generate and test hypotheses about gene function and interactions in yeast using high-throughput laboratory protocols.",
            "problem_complexity": "Domain-specific biological experimentation involving wet-lab protocols, combinatorially large hypothesis spaces (genes and interactions), experimental stochasticity, and resource-constrained laboratory runs. Complexity is high due to laboratory procedures and biological variability.",
            "data_availability": "Relied on biological assay data generated by the system's experiments; data generation occurred via automated laboratory execution (not pre-existing datasets).",
            "computational_requirements": "Includes wet-lab execution time and laboratory resources; computational modeling/hypothesis generation components are less emphasized in the paper. The MLAgentBench paper references Adam as prior art but does not provide compute numbers.",
            "problem_structure": "Closed-loop experimental structure (open scientific discovery problem but with procedural steps and measurable experimental outcomes); stochastic biological outcomes and high cost per experiment.",
            "success_metric": "Scientific hypotheses validated experimentally (qualitative/quantitative lab assay results); original Adam papers reported ability to propose and test non-trivial hypotheses, but MLAgentBench does not restate quantitative success measures.",
            "success_rate": "Not reported in MLAgentBench paper (Adam cited as an example of domain-tailored automated scientific discovery).",
            "failure_modes": "Not detailed in this paper; general challenges for such systems include domain specificity, experimental cost, and limited generalization outside designed lab pipelines.",
            "success_factors": "Tight integration with laboratory automation, domain-specific instrumentation, and carefully engineered hypothesis-testing pipelines enabled closed-loop discovery.",
            "comparative_results": "Mentioned as contrast: Adam represents highly tailored automated discovery whereas MLAgentBench aims toward more general, flexible LM-based research assistants.",
            "human_baseline": "Not provided in the MLAgentBench paper.",
            "uuid": "e2625.3"
        },
        {
            "name_short": "AutoML (general)",
            "name_full": "AutoML (Automated Machine Learning)",
            "brief_description": "A family of methods that automate parts of the ML pipeline (model selection, hyperparameter tuning, architecture search) to reduce human engineering in building performant models.",
            "citation_title": "Automl: A survey of the state-of-the-art",
            "mention_or_use": "mention",
            "system_name": "AutoML / Neural Architecture Search (general)",
            "system_description": "Encompasses techniques such as hyperparameter optimization, neural architecture search, and pipeline automation; often uses search strategies (e.g., Bayesian optimization, evolutionary algorithms) and can be computationally intensive but domain-general for ML tasks.",
            "system_type": "AutoML / Neural Architecture Search",
            "problem_domain": "Machine learning model search and optimization (across vision, NLP, tabular, etc.)",
            "problem_description": "Automates selection of model architectures, hyperparameters, and training pipelines to optimize predefined evaluation metrics (accuracy, MAE, etc.).",
            "problem_complexity": "Search over typically large, mixed discrete-continuous spaces (architectures, hyperparameters), multi-objective in some settings (accuracy vs. latency), computationally expensive (many training runs), and stochastic (training noise).",
            "data_availability": "Relies on available training/validation data; data volumes vary by problem. Does not generally require new data generation beyond standard supervised learning datasets.",
            "computational_requirements": "Often high (many model trainings); survey referenced in paper documents various compute strategies; specific compute numbers not provided in MLAgentBench paper.",
            "problem_structure": "Well-defined optimization problems with clear evaluation metrics but large search spaces; can be automated because objective functions are measurable.",
            "success_metric": "Model performance metrics (accuracy, MAE, validation loss), often compared to human-designed baselines.",
            "success_rate": "Not quantitatively reported in this paper; AutoML cited as an established area demonstrating practical automation of ML workflows.",
            "failure_modes": "Computational cost, overfitting to validation sets, and limited ability to handle open-ended research decisions beyond measurable optimization objectives.",
            "success_factors": "Well-defined objective, availability of validation metric, and ability to parallelize evaluations.",
            "comparative_results": "AutoML contrasted with the LM-based open-ended agent approach of this paper; MLAgentBench emphasizes broader decision-making beyond pure AutoML optimization.",
            "human_baseline": "AutoML systems are typically compared to human-engineered models; no direct human baselines reported in this paper.",
            "uuid": "e2625.4"
        },
        {
            "name_short": "AutoML-GPT",
            "name_full": "AutoML-GPT",
            "brief_description": "A concurrent approach that repeatedly prompts LMs with dataset/model information and predicts training outcomes to accelerate hyperparameter tuning and model selection.",
            "citation_title": "Automl-gpt: Automatic machine learning with gpt",
            "mention_or_use": "mention",
            "system_name": "AutoML-GPT (concurrent work)",
            "system_description": "Uses LMs to model/predict training logs and guide efficient hyperparameter search by prompting with data and model cards and anticipating training behavior; focuses on AutoML-style hyperparameter tuning rather than open-ended code editing.",
            "system_type": "LM-assisted AutoML / Hyperparameter Tuning System",
            "problem_domain": "Hyperparameter tuning and AutoML within ML tasks",
            "problem_description": "Given data and model descriptions, predict training curves and recommend hyperparameter changes to improve efficiency of search.",
            "problem_complexity": "Still large hyperparameter spaces, but approach reduces search cost by using LM priors to predict likely outcomes; complexity depends on model size and hyperparameter dimensionality.",
            "data_availability": "Depends on historical training logs / model cards; MLAgentBench references it as concurrent work but does not provide datasets for it.",
            "computational_requirements": "Less than exhaustive search if LM predictions are accurate; paper cites it as complementary work but provides no compute numbers here.",
            "problem_structure": "Well-posed optimization/estimation problem with measurable metrics; not fully open-ended research discovery.",
            "success_metric": "Hyperparameter search efficiency (reduced number of expensive training runs) and final model performance.",
            "success_rate": "Not reported in MLAgentBench paper.",
            "failure_modes": "Not detailed here; general risk is LM prediction inaccuracies leading to missed optima.",
            "success_factors": "Leveraging LM priors and historic training patterns to reduce expensive evaluations.",
            "comparative_results": "Positioned by the authors as complementary to MLAgentBench's open-ended agent evaluation; AutoML-GPT focuses on efficient tuning rather than full experimentation autonomy.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2625.5"
        },
        {
            "name_short": "MLCopilot",
            "name_full": "MLcopilot",
            "brief_description": "A system that prompts LMs with past experiences and knowledge to predict categorized hyperparameter settings (e.g., high/low values) for ML tasks.",
            "citation_title": "Mlcopilot: Unleashing the power of large language models in solving machine learning tasks",
            "mention_or_use": "mention",
            "system_name": "MLCopilot (concurrent work)",
            "system_description": "An LM-oriented assistant that uses prior runs/experience to recommend coarse hyperparameter choices or settings to guide ML experimentation, rather than performing end-to-end code edits and experiment execution.",
            "system_type": "LM-assisted Hyperparameter Recommendation System",
            "problem_domain": "Machine learning hyperparameter recommendation and guidance",
            "problem_description": "Provides recommended hyperparameter categories (e.g., low/high weight decay) based on prior knowledge and task descriptions to speed manual or automated tuning.",
            "problem_complexity": "Less granular than full AutoML; reduces dimensionality by categorization but still relies on correct mapping from task characteristics to hyperparameter choices.",
            "data_availability": "Relies on prior experience / metadata; MLAgentBench mentions it as related work without numeric evaluation here.",
            "computational_requirements": "Low relative to full AutoML since it provides recommendations rather than running many trainings.",
            "problem_structure": "Semi-structured recommendation problem with actionable suggestions but not full optimization.",
            "success_metric": "Improved tuning efficiency or final model performance if recommendations are followed.",
            "success_rate": "Not reported in MLAgentBench paper.",
            "failure_modes": "Coarse recommendations may miss precise optima; effectiveness depends on quality of prior experience encoded in prompts.",
            "success_factors": "Effective when prior examples are representative of the current task.",
            "comparative_results": "Contrasted by the authors as narrower in scope than their open-ended agent which must implement code edits and run experiments.",
            "human_baseline": "Not provided.",
            "uuid": "e2625.6"
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager (LLM-driven embodied agent)",
            "brief_description": "An open-ended embodied agent that uses LLMs to play Minecraft by decomposing long-term goals into executable steps and learning affordances in the environment.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "system_name": "Voyager",
            "system_description": "An LLM-based agent designed for an embodied environment (Minecraft) that iteratively plans, executes, and learns environment affordances to accomplish complex tasks, illustrating LLM agent capabilities in an interactive domain.",
            "system_type": "Embodied LM Agent / Generative Agent",
            "problem_domain": "Interactive game environment (Minecraft) — sequential decision-making and skill acquisition",
            "problem_description": "Open-ended tasks in a simulated environment that require long-horizon planning, tool use, and environment interaction.",
            "problem_complexity": "High due to long-horizon planning, partial observability, and large discrete action spaces; success measured in task completion/skill acquisition in game environment.",
            "data_availability": "Environment-provided observations and affordance learning from interactions.",
            "computational_requirements": "Environment simulation and many LLM calls; MLAgentBench cites Voyager as related work but provides no performance data here.",
            "problem_structure": "Open-ended and sequential, with emergent behaviors from iterated planning and affordance learning.",
            "success_metric": "Task completion and learned behaviors in the environment (not quantified in MLAgentBench paper).",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Long-term planning and hallucination risks are common in such agents, similar to those observed by the MLAgentBench authors.",
            "success_factors": "Modularity for learning affordances and decomposing tasks into subgoals.",
            "comparative_results": "Cited as an example of LLM-based agents applied outside pure ML experimentation to emphasize breadth of agent research.",
            "human_baseline": "Not provided.",
            "uuid": "e2625.7"
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (language grounding for robotics)",
            "brief_description": "A system combining language model affordance priors with robot low-level controllers to ground instructions into physically executable actions.",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "mention_or_use": "mention",
            "system_name": "SayCan",
            "system_description": "Combines LLM-derived high-level intent scoring with robot affordances and low-level controllers to select feasible actions for real-world robotic tasks; an example of grounding LMs to physical systems.",
            "system_type": "LM-grounded Robotics Execution System",
            "problem_domain": "Robotics — grounding language instructions to robot actions",
            "problem_description": "Mapping high-level instructions to sequences of physically executable motor commands using affordance models and LLM guidance.",
            "problem_complexity": "High due to real-world uncertainty, safety constraints, continuous control spaces, and need for robust affordance estimation.",
            "data_availability": "Requires environment-specific affordance models and robot perception data; MLAgentBench only mentions SayCan as related work.",
            "computational_requirements": "Real-time perception and control pipelines; not quantified in this paper.",
            "problem_structure": "Sequential decision-making with real-world stochasticity and safety-critical constraints.",
            "success_metric": "Task completion and safety/feasibility in robotic execution; not reported in MLAgentBench.",
            "success_rate": "Not reported here.",
            "failure_modes": "Affordance misestimation, safety concerns, and mismatch between language priors and physical capabilities.",
            "success_factors": "Integration of LLM guidance with grounded affordance models and robust controllers.",
            "comparative_results": "Included as an example of successful grounding of LLM agents in the physical world; MLAgentBench contrasts domain-specific integrated systems like SayCan with their more general software-focused agent.",
            "human_baseline": "Not provided.",
            "uuid": "e2625.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist",
            "rating": 2
        },
        {
            "paper_title": "The automation of science",
            "rating": 2
        },
        {
            "paper_title": "Automl: A survey of the state-of-the-art",
            "rating": 2
        },
        {
            "paper_title": "Automl-gpt: Automatic machine learning with gpt",
            "rating": 2
        },
        {
            "paper_title": "Mlcopilot: Unleashing the power of large language models in solving machine learning tasks",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 1
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.02343325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation</h1>
<p>Qian Huang ${ }^{1}$ Jian Vora ${ }^{1}$ Percy Liang ${ }^{1}$ Jure Leskovec ${ }^{1}$</p>
<h4>Abstract</h4>
<p>A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with $37.5 \%$ average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from $100 \%$ on well-established older datasets to as low as $0 \%$ on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination. ${ }^{2}$</p>
<h2>1. Introduction</h2>
<p>Much of the progress in machine learning is driven by effective experimentation: Given a task (e.g., image classification), a researcher develops a method (e.g., choice of model architecture and learning algorithm), runs an experiment, and then evaluates the results. Based on the outcome of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the experiment (e.g., validation accuracy), they revise their method to improve performance on the task. This iterative process is challenging, as it requires the researcher to possess extensive prior knowledge about potential methods, to produce functional code, and to interpret experimental results for future improvements.</p>
<p>The complexity and expertise required for successful machine learning experimentation pose significant barriers to entry. In light of these challenges, there has been interest in the possibility of automating aspects of the machine learning workflow, such as Neural Architecture Search (Elsken et al., 2019) and AutoML (He et al., 2021). The emergence of advanced language models, with their ability to understand and generate human-like text, presents an promising opportunity to further automate ML experimentation end to end. Can we develop an agent capable of conducting machine learning experimentation autonomously?</p>
<p>In this paper, we propose MLAgentBench, the first benchmark for evaluating agents capable of machine learning experimentation (Figure 1). MLAgentBench is a general framework for specifying experimentation tasks with clear goals and automatically evaluates agents on these tasks. Concretely, each task is specified with a task description, a set of starter files (including starter code and data, e.g., Kaggle data package), and an evaluator that can assign a performance metric score to a final submission (such as test set accuracy of the submitted test set prediction). Given these, an agent can perform actions like reading/writing files and executing Python code in a workspace. During the agent's interaction with the environment, we collect its interaction trace for evaluation, which is the agent actions and intermediate snapshots of the workspace (i.e., the set of files and directories in the working directory). We evaluate the agent along two aspects: 1) competence in accomplishing the task, i.e., the fraction of time that the agent was able to improve the performance metric (e.g., test accuracy) by at least $10 \%$ over the baseline in the starter code; 2) efficiency, the amount of time and number of tokens LM queries spent by the agent. While our benchmark is framed in terms of automation for simplicity, we stress the importance of interpretability for building trust and also providing a hook for human augmentation: Indeed, a researcher could intervene</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of MLAgentBench. Each environment in MLAgentBench includes a task description, a set of starter files, and an evaluator. An agent can read/write files and execute Python code repeatedly, eventually producing a final file (e.g., test predictions in submission.csv). The agent is evaluated based on the quality of this file. At each time step, the language agent should produce a language output $r_{t}$, which contains reflection, research plan and status, etc, and action $a_{t}$, which is then executed by the environment to update state $s_{t}$, i.e. the set of files in the workspace and produce an observation $o_{t}$ as shown in Table 1.
and edit the workspace or plans of the agent.
MLAgentBench includes 13 ML tasks from diverse domains ranging in difficulty and recency (Table 2), where the code execution is relatively inexpensive-on the order of minutes. For example, one task is to increase the test accuracy of a baseline Convolution Neural Networks (CNN) model on the CIFAR-10 dataset (Krizhevsky, 2009) by more than 10\%. Beyond well-established datasets like CIFAR-10, we also include more recent Kaggle challenges launched between August 31, 2022 and May 11, 2023 and other research datasets launched in January 2023 to see whether the agent can extrapolate to newer datasets potentially unseen during (pre-)training.</p>
<p>We then create an agent for ML experimentation inspired by existing works (Yao et al., 2023; Shinn et al., 2023; Wang et al., 2023a; aut, 2023; Schick et al., 2023; Park et al., 2023). At each step, we automatically construct a prompt that summarizes all known information about the task and prior actions, and query the LM to produce a step-wise reflection (Shinn et al., 2023), a high-level plan (aut, 2023), a fact-checking section, a reasoning step before action (Yao et al., 2023), and the next action to take. The actions include basic actions in the environment as well as compound actions that involve several basic actions and modular LM calls, such as understanding a file and editing a file based on instructions. See more details in Section 2.2.1 and 3.</p>
<p>On MLAgentBench, we benchmark agents based on GPT-4 (0613), GPT-4-turbo (0125),(Nakano et al., 2021; OpenAI, 2023), Claude v1.0, Claude v2.1, Claude v3 Opus (opus-</p>
<p>20240229)(Anthropic, 2023), Gemini Pro (Anil et al., 2023), and Mixtral (Instruct-v0.1) (Jiang et al., 2024). We find that our agent performs the best in terms of success rate when based on Claude v3 Opus with $37.5 \%$ average success rate. Our agent is able to successfully solve many tasks and generate highly interpretable research plans along the way, though there are still many limitations. On well-established tasks like training a model over the house-price dataset, it is able to achieve $100 \%$ success rate over 8 runs. However, the agent struggles with Kaggle challenges and BabyLM (Warstadt et al., 2023), with only a $0-25 \%$ success rate. We then compare results against the adaptation of other existing agents such as ReAct and AutoGPT and find improvements upon them. We also identify several key challenges for LM-based agent designs, e.g. how to effectively plan and replan over long horizons and hallucination about the current progress, and show how our design handles them qualitatively. Overall, our agent demonstrates feasibility and success with LM-based agents for ML experimentation, but there is still some ways until they can succeed reliably.</p>
<h2>2. MLAgentBench: Benchmarking ML experimentation</h2>
<p>MLAgentBench introduces a general framework for specifying well-scoped executable tasks and automatically evaluating agents on these tasks. The benchmark provides a modular implementation of the environment and the agent, and captures the entire interaction trace for evaluation. We include 13 concrete and diverse machine learning tasks in</p>
<table>
<thead>
<tr>
<th>Action Name</th>
<th>Input</th>
<th>Observation</th>
<th>Side Effects</th>
</tr>
</thead>
<tbody>
<tr>
<td>List Files</td>
<td>directory (e.g. . )</td>
<td>list of files in the directory</td>
<td>None</td>
</tr>
<tr>
<td>Read File</td>
<td>file name (e.g. train.py)</td>
<td>contents of the file</td>
<td>None</td>
</tr>
<tr>
<td>Write File</td>
<td>file name, content</td>
<td>A success or error message</td>
<td>Content written to given file</td>
</tr>
<tr>
<td>Append File</td>
<td>file name, content</td>
<td>A success or error message</td>
<td>Content appended to given file</td>
</tr>
<tr>
<td>Copy File</td>
<td>Source (e.g. train.py), destination (e.g. train_copy.py)</td>
<td>A success or error message</td>
<td>Source file copied to destination</td>
</tr>
<tr>
<td>Inspect Script Lines</td>
<td>file name, start line number, end line number</td>
<td>the file content between start and end line numbers</td>
<td>None</td>
</tr>
<tr>
<td>Undo Edit Script</td>
<td>file name (e.g. train.py)</td>
<td>The content of the file after undo</td>
<td>The given file is restored to before an edit</td>
</tr>
<tr>
<td>Execute Script</td>
<td>file name (e.g. train.py)</td>
<td>Any output from the execution</td>
<td>Any side effect from code execution</td>
</tr>
<tr>
<td>Final Answer</td>
<td>None</td>
<td>None</td>
<td>The environment shuts down</td>
</tr>
<tr>
<td>Understand File</td>
<td>file name, a query (e.g. the model architecture)</td>
<td>retrieved content from the file relevant to the query</td>
<td>None</td>
</tr>
<tr>
<td>Edit Script</td>
<td>file name, edit instruction (e.g. change epoch to 20), save file name</td>
<td>The diff of the edited file based on the instruction</td>
<td>Edited file is saved to save path</td>
</tr>
<tr>
<td>Edit Script Segment</td>
<td>file name, start line number, end line number, edit instruction, save file name</td>
<td>The diff of the edited file based on the instruction</td>
<td>Edited file is saved to save path</td>
</tr>
</tbody>
</table>
<p>Table 1. Actions in MLAgentBench, where each action has a name, input, output, and side effects. Most of the actions are primitive actions that include file system operations and python script execution. The last three are compound actions that is composed of multiple primitive actions and LM calls.
the benchmark. Each task is specified by task description, starter files, and an evaluator, and instantiated in a general environment with a task-independent set of actions and states. In the subsequent subsections, we describe each of the key components of MLAgentBench: task specification (section 2.1), general environment (section 2.2), and evaluation (section 2.3).</p>
<h3>2.1. Task Specification</h3>
<p>Each task is specified by a textual task description, a set of starter files, and an evaluator.</p>
<p>Task description. In MLAgentBench, the task description describes the desired goal, e.g. "Given a training script on a dataset train.py, improve upon the current model accuracy" (as shown in Figure 1), and how the agent should submit the final answer for evaluation, e.g. "Save per class probabilities for test set examples to submission.csv". The description could also include constraints like limiting the model size and training epochs, or occasionally include specific directions to approach the problem like "by fine-tuning a pretrained BERT model".</p>
<p>Starter Files. The starter files include training and testing data (without test labels), detailed data descriptions, metric descriptions, and the starter code. The starter code is based on diverse ML frameworks, including PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2015), JAX (Bradbury et al., 2018), Keras (Chollet et al., 2015), etc. The starter code mostly implements a simple baseline model that we can compare with during evaluation, but some tasks do not have any baseline implementation, and the agent is responsible for coding up the model from scratch from the task description and dataset files.</p>
<p>Evaluator. Each environment has its own evaluator. The evaluator assigns a raw score to a final submission of the agent. A typical evaluator, for example, gives the test accuracy of the predictions recorded in submission.csv.</p>
<h3>2.2. General Environment</h3>
<p>Each task in MLAgentBench is instantiated in a taskagnostic environment. As shown in Figure 1, the agent operates over a sequence of time steps $t=1, \ldots, T$. Each time step is broken into three parts:</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Task Type</th>
<th>Modality</th>
<th>Dataset Name</th>
<th>Metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>Canonical</td>
<td>Classification</td>
<td>Image</td>
<td>CIFAR-10 (Krizhevsky, 2009)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td>Tasks</td>
<td>Classification</td>
<td>Text</td>
<td>imdb (Maas et al., 2011)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td></td>
<td>Node Classification</td>
<td>Graph</td>
<td>ogbn-arxiv (Hu et al., 2020)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td>Classic</td>
<td>Regression</td>
<td>Tabular</td>
<td>house-price (Anna Montoya, 2016)</td>
<td>Mean absolute error</td>
</tr>
<tr>
<td>Kaggle</td>
<td>Classification</td>
<td>Tabular</td>
<td>spaceship-titanic (Howard et al., 2022)</td>
<td>Classification accuracy</td>
</tr>
<tr>
<td></td>
<td>Regression</td>
<td>Time Series</td>
<td>parkinsons-disease (Kirsch et al., 2023)</td>
<td>SMAPE score</td>
</tr>
<tr>
<td>Kaggle</td>
<td>Classification</td>
<td>Image</td>
<td>fathomnet (Woodward et al., 2023)</td>
<td>MAP@20</td>
</tr>
<tr>
<td>Challenges</td>
<td>Regression</td>
<td>Text</td>
<td>feedback (Franklin et al., 2022)</td>
<td>MCRMSE</td>
</tr>
<tr>
<td></td>
<td>Segmentation</td>
<td>Images</td>
<td>identify-contrails (Sarna et al., 2023)</td>
<td>Dice coefficient</td>
</tr>
<tr>
<td>Recent</td>
<td>Node Regression</td>
<td>Graph</td>
<td>CLRS (Velivckovi’c et al., 2022)</td>
<td>Mean square error</td>
</tr>
<tr>
<td>Research</td>
<td>Language Modeling</td>
<td>Text</td>
<td>BabyLM (Warstadt et al., 2023)</td>
<td>Perplexity</td>
</tr>
<tr>
<td>Code</td>
<td>Improve speed</td>
<td>Text</td>
<td>llama-inference</td>
<td>Wall Clock Time</td>
</tr>
<tr>
<td>Improvement</td>
<td>Improve speed</td>
<td>Image</td>
<td>vectorization</td>
<td>Wall Clock Time</td>
</tr>
</tbody>
</table>
<p>Table 2. 13 MLAgentBench tasks. For each task, we show its task category, task type, modality and evaluator metric.</p>
<ol>
<li>Act: The agent takes its memory $m_{t}$ (see 3 for an example) and current workspace $s_{t-1}$ and produces a rationale $r_{t}$ (e.g., reflecting on previous actions and observations) and action $a_{t}$ (e.g., read a file).</li>
</ol>
<p>$$
r_{t}, a_{t}=\operatorname{Agent}\left(s_{t-1}, m_{t-1}\right)
$$</p>
<ol>
<li>Execution: The environment then executes the action $a_{t}$ on workspace $s_{t-1}$ to produce updated workspace $s_{t}$ and returns observation $o_{t}$ (See 2.2.1), based on descriptions in section 2.2.1:</li>
</ol>
<p>$$
s_{t}, o_{t}=\operatorname{Env}\left(s_{t-1}, a_{t}\right)
$$</p>
<ol>
<li>Update: Finally, agent updates its memory $m_{t-1}$ based on its action $a_{t}$, its rationale $r_{t}$, and observation $o_{t}$ :</li>
</ol>
<p>$$
m_{t}=\operatorname{Update}\left(m_{t-1}, a_{t}, r_{t}, o_{t}\right)
$$</p>
<p>The agent can take a variable number of actions many times until it decides to submit the final answer, or the environment shuts down itself due to exceeding a maximum number of actions or maximum time.</p>
<h3>2.2.1. Actions</h3>
<p>As listed in table 1, actions that are available in the environment include file system operations (read, write, append, copy, edit, undo edit), execution of any arbitrary Python script, and a final answer declaration action. Beyond these, we also manually designed a few commonly useful compound actions that perform several basic environment actions and separate modular LM calls together:</p>
<p>Understand File. This action takes a file name and a short query as input e.g. what is the model architecture, reads the file, and calls an LM to summarize it based on the short query. It then returns the retrieved and summarized information with detailed references to line numbers.</p>
<p>Edit Script. This action takes a file name, a string of edit instruction, e.g. change learning rate to $1 \mathrm{e}-3$, and a save file name as inputs. It first reads the file, calls an LM to perform an edit of a file given a short edit instruction from the main agent, then writes the modified version to the file with the given file name.</p>
<p>Edit Script Segment. Similar to Edit Script, but also takes start and end line numbers as inputs and only edits the segment in between. This is particularly helpful when the task involves manipulating a large codebase (i.e. CLRS and BabyLM).</p>
<p>Each action is specified with a name, description, usage, return value description, and a Python implementation. See Table 1 for complete descriptions.</p>
<h3>2.3. Evaluation</h3>
<p>After the agent submits the result or the environment shuts down, all actions $a_{1}, \ldots, a_{T}$, responses $r_{1}, \ldots, r_{T}$, observations $o_{1}, \ldots, o_{T}$, and snapshots of the workspace $s_{1}, \ldots, s_{T}$ after each action is executed are recorded as an interaction trace. Given the interaction traces collected, we then evaluate the agent from three aspects:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Overview of our LM-based agent. On the left we show the prompt and context of LM $p_{t}$ at each step, which includes past three steps of observations. On the upper right, we show the agent's response to the prompt $r_{t}$ and action $a_{t}$. On the lower right, the code execution results are returned as observations $o_{t}$ to the agent. Next step, this observation is incorporated into the prompt as the past history, and then the cycle repeats. Note that in this example, the agent starts with a baseline train.py and is now trying to execute this baseline.</p>
<p>Competence in accomplishing the objectives. We run the evaluator to obtain a single performance metric based on the final snapshot of the working directory. Then we define success as whether the performance metric is improved over baseline in the starter code by $10 \%$. We then compute aggregated metrics over the performence metric of multiple runs such as success rate and the average amount of improvement of the performance metric.</p>
<p>Efficiency. We evaluate efficiency in terms of the total amount of wall clock time spent and the total number of input and output tokens consumed by the agent.</p>
<h3>2.4. Tasks</h3>
<p>MLAgentBench includes 13 tasks from diverse domains including text, image, time series, graphs, and tabular data as shown in Table 2. Our tasks include both well-studied datasets like CIFAR-10 and open challenges like Parkinson's disease progression prediction from Kaggle, which is released after the language model (e.g. GPT-4) pre-training that therefore has not been pretrained on . The tasks are chosen such that they range in various difficulties and recency. In this way, we test the generalizability of the agent and mitigate data contamination. They are divided to the
following categories:</p>
<p>Canonical Tasks. We included CIFAR-10 (image classification) (Krizhevsky, 2009), imdb (sentiment classification) (Maas et al., 2011), and ogbn-arxiv (paper category classification over citation network) (Hu et al., 2020) as canonical tasks that are well-studied and easy to iterate on. For CIFAR10 and ogbn-arxiv, the task was to improve a baseline model, but for $i m d b$, the agent was expected to write the model from scratch which involved finetuning a BERT model as mentioned in the task description.</p>
<p>Classic Kaggle. House-price (Anna Montoya, 2016) and spaceship-titanic (Howard et al., 2022) are two introductory Kaggle challenges for tabular regression and classification. These tasks mainly involve feature engineering, writing, and training models from scratch (no baselines provided), and properly following the Kaggle submission format.</p>
<p>Kaggle Challenges. We select four recent open Kaggle Challenges launched between August 31, 2022 and May 11, 2023 to test agents' ability to generalize to more realistic and out-of-distribution tasks.</p>
<p>Recent Research. We include CLRS (Velivckovi'c et al., 2022) and BabyLM (Warstadt et al., 2023) as two datasets that are actively being researched and do not yet have a consensus on the best approaches. CLRS involves predicting the output of classic algorithms over graphs and lists. BabyLM requires training a language model over 10M words.</p>
<p>Code Improvement. We include llama-inference and vectorization as two datasets where the goal is to improve the runtime of code instead of optimizing its prediction performance. llama-inference is about improving the autoregressive generation speed of the LLaMA 7B model (Touvron et al., 2023), and vectorization is about speeding up the inference of a convolutional model with stacks of for loops in the forward pass.</p>
<p>More details on the benchmark tasks can be found in Appendix B.</p>
<h2>3. Our LM-based Agent</h2>
<p>To tackle MLAgentBench, we design an LM-based agent as shown in Figure 2. At a high level, we prompt the LM to provide the next step action and action arguments $a_{t}$ in a JSON format. The prompt $p_{t}$ starts with a description of all the actions available, the task description, a template to instruct the LM to produce text in parsable format, and the last 3 steps taken including $r_{t-3}, a_{t-3}, o_{t-3}, r_{t-2}, a_{t-2}, o_{t-2}, r_{t-1}, a_{t-1}, o_{t-1}$ (see Appendix F for a full example of what prompt the agent sees at each interaction step). Formally, our agent implements equation 1:</p>
<p>$$
r_{t}, a_{t}=\operatorname{Agent}\left(s_{t-1}, m_{t-1}\right)
$$</p>
<p>where $m_{t}=\left(o_{&lt;t}, r_{&lt;t}\right)$. At each time step, the agent constructs prompt $p_{t}$ and queries LM to get $r_{t}, a_{t}=L M\left(p_{t}\right)$, where $a_{t}$ is parsed from part of LLM response $r_{t}$ as detailed bellow.</p>
<h3>3.1. Thinking before Acting</h3>
<p>The most important component of our agent is specifying the response format, i.e., "Please respond in this format exactly:..." (see Figure 2), so that the LM can first generate plan and thought before proposing an action. Specifically, we ask the LM to generate the rationale $r_{t}$ before the action $a_{t}$, where the thought consists of a Reflection, Research Plan and Status, Fact Check, Thought, and then Action and Action Input.</p>
<p>As shown in Figure 2, Reflection is an entry for reflecting about the previous step as inspired by Reflexion (Shinn et al., 2023); Research Plan and Status is an entry for current planning and status designed to produce better planning and keep track of what has been done; Fact</p>
<p>Check double-checks whether a statement in Research Plan and Status has been confirmed or hallucinated; Thought is an entry for thought about what action to take similar to ReAct (Yao et al., 2023).</p>
<p>Specifically, the Research Plan and Status entries produced by our agent at each step are highly detailed and interpretable, so it is both useful for guiding the agent through the exploration process and for human understanding. It essentially enumerates the steps agent will take. The Fact Check entry allows the agent to double-check whether the update to Research Plan and Status is factual. One common failure mode during our preliminary experiments is that the model hallucinates improvement after modifying the file without ever executing it. For example, with the Fact Check entry, it will show the model that the performance of the updated model is still unknown. We discuss these entries more in Appendix D. 1 and D.2.</p>
<h2>4. Experiments</h2>
<p>We evaluate our designed agent with GPT-4 (0613), GPT-4-turbo (0125),(OpenAI, 2023), Claude v1.0, Claude v2.1, Claude v3 Opus (opus-20240229)(Anthropic, 2023), Gemini Pro (Anil et al., 2023), and Mixtral (Instruct-v0.1) (Jiang et al., 2024) on MLAgentBench. We also benchmark the adaptation of several existing generative agents: 1) AutoGPT, a popular open-source project for general-purpose autonomous AI agents (aut, 2023) which has much more complicated tools such as Google search, and 2) LangChain, another popular framework that implements various generative agents. Here we use "zero-shot-react-description" which implements ReAct (Yao et al., 2023) too similar to our agent, but just does not have research status and plan and fact checking entries. We evaluated GPT-4-turbo and Claude v3 Opus for both agents.</p>
<p>We conduct 8 runs for all agents. For most runs, we allow a maximum of 50 actions in the environment and a maximum time of 5 hours, whereas for GPT-4 runs we only allow 30 actions due to the cost associated with GPT-4 API calls.</p>
<h3>4.1. Competence in Accomplishing The Objectives</h3>
<p>As shown in Tables 3 and 4, the Claude v3 Opus agent achieves the best results over most tasks and a far better average success rate of $37.5 \%$, but with varying degrees of success from $100 \%$ over house-price to $0 \%$ over BabyLM. We also see a general positive progression of performance across different generations of models in the same family. However, GPT-4 obtains a much higher average improvement in performance metric, which means it is improving the performance metric more positively overall than Claude v3 Opus. Note that the simple averaging may exaggerate how much better GPT-4 is than Claude v3, since the gain</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-4- <br> turbo</th>
<th style="text-align: left;">Claude <br> v1.0</th>
<th style="text-align: left;">Claude <br> v2.1</th>
<th style="text-align: left;">Claude <br> v3 Opus</th>
<th style="text-align: left;">Gemini <br> Pro</th>
<th style="text-align: left;">Mixtral</th>
<th style="text-align: left;">Baseline</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">cifar10</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">imdb</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">ogbn-arxiv</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">house-price</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">spaceship-titanic</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">parkinsons-disease</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">fathomnet</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">feedback</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">identify-contrails</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">40.0</td>
</tr>
<tr>
<td style="text-align: left;">llama-inference</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">12.5</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">vectorization</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">CLRS</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">50.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">25.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">42.9</td>
</tr>
<tr>
<td style="text-align: left;">BabyLM</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">19.2</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">16.3</td>
<td style="text-align: left;">26.0</td>
<td style="text-align: left;">37.5</td>
<td style="text-align: left;">18.3</td>
<td style="text-align: left;">3.8</td>
<td style="text-align: left;">10.4</td>
</tr>
</tbody>
</table>
<p>Table 3. For each task and LM, we show the success rate, the percentage over 8 trials where the LM-based agent achieves an 10\% improvement on the performance metric over the baseline in the starter code.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. At each time step on the x-axis, we evaluate performance metric based on the workspace and take average across all runs to obtain the test accuracy shown. We can see that running longer generally degrades the performance except for Claude v3 Opus. is mainly dominated by the high improvement on identifycontrails.</p>
<p>Comparing our proposed agent with existing baseline agents based on GPT-4-turbo and Claude v3 Opus, our agent achieves a higher success rate on average, as shown in table 5. We note that LangChain with Claude v3 is very competitive to our method, partially because it is simpler so that the agent does not attempt to change the submission format incorrectly.</p>
<h3>4.2. Research Process</h3>
<p>We show a full example of agent responses on CIFAR-10 to demonstrate what our agent actually does qualitatively
in Appendix F. Several example actions trace on CIFAR10 are shown in Figure 4. As shown in the example, our agent generally follows the cycle of making/revising plans, editing scripts, performing experiments, interpreting results, etc. We also show a plot of average performance metric across different time steps, i.e. we evaluate not only the last step but all intermediate steps. As shown in figure 3, the agent can sometimes regress in performance as step goes later on and generally running longer steps tends to degrade the performance metric except for Claude v3 Opus. We show more analysis in Appendix C.</p>
<p>To more carefully evaluate the reasoning and research process of the agent, we analyze the traces of all runs for CIFAR-10 and categorize them as shown in Figure 5:</p>
<ol>
<li>Hallucination, where the agent claims to know something or fabricates some results such as claiming performance increase without even executing any edits in the training script.</li>
<li>Bad Plan, where the agent fails to make a correct plan that brings direct progress (such as dropping some features of the data before finding the utility of those in predicting that target). Most of these bad plans occur in the initial steps and recovery is difficult thereafter.</li>
<li>Response Format Error, where the agent produces invalid JSON and cannot be parsed with our best effort.</li>
<li>Submission Format Error, where the agent changes the submission.csv format incorrectly that our evaluator cannot recognize, even if the predictions are good.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-4- <br> turbo</th>
<th style="text-align: left;">Claude <br> v1.0</th>
<th style="text-align: left;">Claude <br> v2.1</th>
<th style="text-align: left;">Claude <br> v3 Opus</th>
<th style="text-align: left;">Gemini <br> Pro</th>
<th style="text-align: left;">Mixtral</th>
<th style="text-align: left;">Baseline</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">cifar10</td>
<td style="text-align: left;">9.2</td>
<td style="text-align: left;">5.3</td>
<td style="text-align: left;">-3.1</td>
<td style="text-align: left;">5.1</td>
<td style="text-align: left;">18.5</td>
<td style="text-align: left;">-36.4</td>
<td style="text-align: left;">6.5</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">imdb</td>
<td style="text-align: left;">86.4</td>
<td style="text-align: left;">86.2</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">82.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">ogbn-arxiv</td>
<td style="text-align: left;">48.9</td>
<td style="text-align: left;">38.6</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">19.8</td>
<td style="text-align: left;">49.5</td>
<td style="text-align: left;">7.3</td>
<td style="text-align: left;">-2.2</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">house-price</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">100.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">spaceship-titanic</td>
<td style="text-align: left;">45.8</td>
<td style="text-align: left;">45.0</td>
<td style="text-align: left;">48.4</td>
<td style="text-align: left;">40.5</td>
<td style="text-align: left;">44.8</td>
<td style="text-align: left;">45.4</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">parkinsons-disease</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">-13.3</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">-0.2</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">fathomnet</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">feedback</td>
<td style="text-align: left;">78.0</td>
<td style="text-align: left;">68.1</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">32.8</td>
<td style="text-align: left;">74.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">identify-contrails</td>
<td style="text-align: left;">143.3</td>
<td style="text-align: left;">114.9</td>
<td style="text-align: left;">-48.9</td>
<td style="text-align: left;">24.1</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-98.8</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">llama-inference</td>
<td style="text-align: left;">-1.3</td>
<td style="text-align: left;">-0.3</td>
<td style="text-align: left;">8.1</td>
<td style="text-align: left;">18.5</td>
<td style="text-align: left;">0.8</td>
<td style="text-align: left;">-23.0</td>
<td style="text-align: left;">10.7</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">vectorization</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-6.8</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-10.0</td>
<td style="text-align: left;">-18.7</td>
<td style="text-align: left;">-11.9</td>
<td style="text-align: left;">-3.9</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">CLRS</td>
<td style="text-align: left;">26.5</td>
<td style="text-align: left;">-24.2</td>
<td style="text-align: left;">0.6</td>
<td style="text-align: left;">-22.1</td>
<td style="text-align: left;">-11.6</td>
<td style="text-align: left;">-28.7</td>
<td style="text-align: left;">-6.6</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">BabyLM</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">-0.5</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">-0.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">41.3</td>
<td style="text-align: left;">32.8</td>
<td style="text-align: left;">8.9</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">26.1</td>
<td style="text-align: left;">-3.6</td>
<td style="text-align: left;">8.0</td>
<td style="text-align: left;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 4. For each task and each agent, we show the average percentage improvement of the performance metric over the baseline in starter code among the runs that made a valid submission at the last step. If the improvement is beyond $10 \%$ we count it as success in Table 3. for the tasks that don't have a baseline, how do you compute improvement? technically any non-zero improvement is infinite percent increase?
5. Small Improvement, where the agent successfully makes minor improvement but it does not reach $10 \%$.</p>
<p>Note that the GPT-4 based agent is more prone to hallucinations and poor planning compared to the Claude v3 Opus based agent. We show a more detailed qualitative analysis in Appendix D, which demonstrates the benefits of Research Plan and Status entries for long-term interpretable planning and Fact Check entries against hallucination.</p>
<h3>4.3. Efficiency</h3>
<p>We compare the average number of tokens and time spent by each agent for all tasks in Figure 6. We also break down the tokens and time spent for each task in Figure 8 and 9 in the Appendix. On average, the GPT-4-turbo based agent is the most efficient, spending $51.0 \%$ fewer tokens than an average agent due to its efficiency in finishing the task and submitting early, while having a high success rate. On the other hand, the best Claude v3 Opus model spends nearly the most tokens and wall clock time, potentially due to the slower API and longer time spent on running ML experiments. Overall, gpt-4 family models have improved performance to tokens ratio trend, while Claude models generally improve performance at the cost of more tokens. Converting with the current API prices, each run on each task only costs a few dollars. In total, running the entire benchmark with GPT-4-turbo once took 6 million tokens, which is around 60 dollars. However, with the low average success rate of $26 \%$, the expected cost to accomplish a task
becomes $\$ 231$, making reliability important for the usability of the agents.</p>
<h2>5. Related Work</h2>
<h3>5.1. Language Agents</h3>
<p>This combination of strong prior knowledge and action/reaction abilities of LMs gives rise to explorations of developing various LM-based agents, such as generative agents for simulating interactions between humans (Park et al., 2023), Voyager for playing Minecraft (Wang et al., 2023a), SayCan for physical robotics (Ahn et al., 2022), as well as open source projects like AutoGPT (aut, 2023) for everything and commercial product like Adept. However, it is hard to evaluate the performance and reliability of these agents, especially over a long horizon of complex interactions. Moreover, such under-studied experimental agents can become increasingly dangerous when allowed to interact directly with personal data, the internet, or even bank accounts and military devices.</p>
<p>There are several concurrent works that also benchmark agent abilities in different aspects: AgentBench (Liu et al., 2024) benchmarks large language models with fixed simple agents in diverse environment; WebArena (Zhou et al., 2023) benchmarks agents in web interactions; ARA (Kinniment et al., 2023) evaluates agents on realistic high stakes scenarios. From this general benchmarking perspective, our MLAgentBench offers a testbed for agents with the desired combination of containability, complexity, evaluability, and practical usefulness.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Example agent traces on CIFAR-10. As shown on the left, agents mostly alternate between editing and executing training script when performing good planing; sometimes, it strays off to random actions when having a bad plan as shown on the right.</p>
<h3>5.2. Language Models for AutoML</h3>
<p>Several concurrent works have explored using LMs for AutoML type of tasks: AutoML-GPT (Zhang et al., 2023c) repeatedly prompts LMs with data and model cards and predicts training logs to perform efficient hyperparameter tuning; MLcopilot (Zhang et al., 2023a) prompts LMs with past experiences and knowledge to predict one final categorized hyperparameter setting (e.g. low or high weight decay). In contrast, our work focuses on benchmarking and developing agents that can perform very open-ended decisions by interacting with the file system and executing code with full flexibility.</p>
<h3>5.3. AI for Automating Scientific Discovery</h3>
<p>Numerous research endeavors seek to enhance the pace of manual observations and experiments through automated ML predictions (Berens et al., 2023; Zhang et al., 2023b; Jumper et al., 2021; Adam-Bourdarios et al., 2016; Schwaller et al., 2017; Wang et al., 2023b). On the other hand, significant line of inquiry revolves around constructing closed-loop systems capable of conducting ongoing experiments and breakthroughs within specific domains (Kramer et al., 2023; Kitano, 2021). For example, Robot Scientist "Adam" is developed to autonomously generate</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Percentage of runs over CIFAR-10 task that falls into different error modes.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Comparing different agents in terms of efficiency, i.e. the number of tokens spent (on x axis and the smaller the better) and success rate (on y axis and the higher the better).
functional genomics hypotheses about the yeast Saccharomyces cerevisiae and experimentally test these hypotheses by using laboratory automation (King et al., 2009; 2004). Nevertheless, these existing systems are highly tailored to process specific types of data for designated tasks and domains. Our work aims to help push toward the ultimate goal of a general and versatile research assistant agent that can perform open-ended decision-making.</p>
<h2>6. Conclusion</h2>
<p>In this paper, we introduce MLAgentBench for benchmarking LM-based agents on performing machine learning experimentation end-to-end. We develop an LM-based agent based on prompting that can accomplish many tasks in MLAgentBench with varying success rates. In the future, we would like to pursue a more robust agent and expand MLAgentBench with more complex and creative tasks accordingly. We would also like to explore the usability of our agents from a human-AI collaboration perspective with real user studies.</p>
<h2>Impact Statement</h2>
<p>Our paper presents the development and evaluation of MLAgentBench for language model-based ML experimentation agents. It carries both significant potential benefits and risks that warrant careful consideration.</p>
<p>On the positive side, the advancement of language models as tools for ML experimentation can democratize access to sophisticated ML research. It can enable a broader range of researchers, including those without extensive coding or ML expertise, to engage in ML research and experimentation. This has the potential to accelerate innovation in various fields, foster interdisciplinary research, and potentially lead to breakthroughs in areas like healthcare, environmental science, and others. The use of autonomous agents in ML experimentation also helps the reproducibility of results.</p>
<p>However, there are notable risks. The ability of these agents to autonomously modify and run ML pipelines arbitrarily can lead to unpredictable outcomes, such as writing dangerous system code. This makes it important to be under close human supervision. Furthermore, by accelerating AI development, it could make it harder for people to adapt to the new technology and defend against the risks.</p>
<p>Finally, there's a societal impact to consider in terms of employment and skill displacement. As these agents become more capable, there is a potential for them to replace or diminish the role of human engineers or researchers in certain aspects of ML experimentation, which could have broader implications for the job market and required skill sets in the field. To mitigate these risks and transform potential challenges into opportunities, it is crucial to involve ML researchers and engineers in the development and implementation of these AI systems. By doing so, AI can be used to augment the work of professionals rather than replace it. This approach not only preserves jobs but also enhances the productivity and creativity of human workers.</p>
<h2>References</h2>
<p>Significant-gravitas/auto-gpt: An experimental open-source attempt to make gpt-4 fully autonomous. https://github.com/SignificantGravitas/Auto-GPT, 2023.</p>
<p>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. TensorFlow: Largescale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.</p>
<p>Adam-Bourdarios, C., Cowan, G., Germain, C., Guyon, I. M., Kégl, B., and Rousseau, D. How machine learning won the higgs boson challenge. In The European Symposium on Artificial Neural Networks, 2016.</p>
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R. C., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D. M., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., and Yan, M. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022.</p>
<p>Anil, G. T. G. R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T. P., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K. W., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L. N., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Brustemi, A., Clay, N., Crone, P., Kociský, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D. H., Barnes, M., Thakoor, S., Austin, J., Barth-Maron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R.,</p>
<p>Li, Y., Cogan, S., Chen, J., Jia, C., Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., de Castro Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki, R., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z. C., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J. C., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozi'nska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C. A., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn, D., Sachan, D. S., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Gim'enez, M., Yeung, L., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R., Luvci'c, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A., Tomasevic, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins,
R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V., Lan, C. L., Haridasan, K. S., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sjosund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V. V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., cCauglar Unlu, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M. L., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T. N., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J. R., Mandhane, A., Joshi, P. M., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., Raki'cevi'c, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., hsin Chi, E. H., Mahdieh, M., Tenney, I., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu, I.-R., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Stańczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Gorgolewski, C. F., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., Castano, A., Giannoumis, I., Kim, W., Rybi'nski, M., Sreevatsa, A., Prendki, J., Soergel, D. G., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A. Y., Cui, A., Lin, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Piper, J., Ie, N., Lui, M., Pasumarthi, R. K., Lintz, N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng,</p>
<p>D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q. V., Abellan, E. A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D. F., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R., Yadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D. S., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J., Polozov, O., Kushman, N., Krakovna, V., Brown, S. M., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor, J. F. J., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J. M., Korchemniy, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P., Zheng, Z., Liu, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Goyal, A., Krishna, K., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J. M., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., H’eliou, A., Niu, N., Gu, S., Pang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., Santamaria-Fernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D. N., Bachem, O., Yue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., Poder, S., Zheng, S., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P. D., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Riviere, M., Walton, A., Crepy, C., Parrish, A., Liu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A. Ø., Scellato, S., Latorre-Chimoto, E., Klimczak-Pluci’nska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio,</p>
<p>P., Walker, L., Morris, A., Penchev, I., Mauger, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb, L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S., Wang, L., Tekur, C., Kale, M., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Yu, A. W., Angermueller, C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M. E., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G., Hechtman, B. A., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J., Green, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly capable multimodal models. ArXiv, abs/2312.11805, 2023. URL https://api.semanticscholar.org/ CorpusID:266361876.</p>
<p>Anna Montoya, D. House prices - advanced regression techniques, 2016. URL https://kaggle.com/ competitions/house-prices-advancedregression-techniques.</p>
<p>Anthropic. Introducing claude, 2023. URL https://www.anthropic.com/index/ introducing-claude.</p>
<p>Berens, P., Cranmer, K., Lawrence, N. D., von Luxburg, U., and Montgomery, J. Ai for science: An emerging agenda. ArXiv, abs/2303.04217, 2023.</p>
<p>Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Chollet, F. et al. Keras, 2015. URL https:// github.com/fchollet/keras.</p>
<p>Elsken, T., Metzen, J. H., and Hutter, F. Neural architecture search: a survey. J. Mach. Learn. Res., 20(1):1997-2017, jan 2019. ISSN 1532-4435.</p>
<p>Franklin, A., Maggie, Benner, M., Rambis, N., Baffour, P., Holbrook, R., Crossley, S., and ulrichboser. Feedback prize - english language learning, 2022. URL https: //kaggle.com/competitions/feedback-prize-english-language-learning.</p>
<p>He, X., Zhao, K., and Chu, X. Automl: A survey of the state-of-the-art. Knowledge-Based Systems, 212:106622, 2021. ISSN 0950-7051. doi: https://doi.org/10.1016/j.knosys.2020.106622. URL https://www.sciencedirect.com/science/ article/pii/S0950705120307516.</p>
<p>Howard, A., Chow, A., and Holbrook, R. Spaceship titanic, 2022. URL https://kaggle.com/ competitions/spaceship-titanic.</p>
<p>Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. ArXiv, abs/2005.00687, 2020.</p>
<p>Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de Las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts. ArXiv, abs/2401.04088, 2024. URL https://api.semanticscholar.org/ CorpusID:266844877.</p>
<p>Jumper, J. M., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Zídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., Back, T., Petersen, S., Reiman, D. A., Clancy, E., Zielinski, M., Steinegger, M., Pacholska, M., Berghammer, T., Bodenstein, S., Silver, D., Vinyals, O., Senior, A. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D. Highly accurate protein structure prediction with alphafold. Nature, 596:583 - 589, 2021.</p>
<p>King, R. D., Whelan, K. E., Jones, F. M., Reiser, P. G. K., Bryant, C. H., Muggleton, S. H., Kell, D. B., and Oliver, S. G. Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427:247252, 2004.</p>
<p>King, R. D., Rowland, J. J., Oliver, S. G., Young, M., Aubrey, W., Byrne, E., Liakata, M., Markham, M., Pir, P., Soldatova, L. N., Sparkes, A., Whelan, K. E., and Clare, A. The automation of science. Science, 324:85 - 89, 2009.</p>
<p>Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk,
H., Burget, J., Ho, A., Barnes, E., and Christiano, P. F. Evaluating language-model agents on realistic autonomous tasks. ArXiv, abs/2312.11671, 2023. URL https://api.semanticscholar.org/ CorpusID:260472392.</p>
<p>Kirsch, L., Dane, S., Adam, S., and Dardov, V. Amp®parkinson's disease progression prediction, 2023. URL https://kaggle.com/competitions/ amp-parkinsons-disease-progressionprediction.</p>
<p>Kitano, H. Nobel turing challenge: creating the engine for scientific discovery. NPJ Systems Biology and Applications, 7, 2021.</p>
<p>Kramer, S., Cerrato, M., Dzeroski, S., and King, R. D. Automated scientific discovery: From equation discovery to autonomous discovery systems. ArXiv, abs/2305.02251, 2023.</p>
<p>Krizhevsky, A. Learning multiple layers of features from tiny images. 2009.</p>
<p>Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024. URL https : //openreview.net/forum?id=zAdUB0aCTQ.</p>
<p>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ P11-1015.</p>
<p>Nakano, R., Hilton, J., Balaji, S., Wu, J., Long, O., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021. URL https://api.semanticscholar.org/ CorpusID:245329531.</p>
<p>OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.</p>
<p>Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and</p>
<p>Technology, UIST '23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701320. doi: 10.1145/3586183.3606763. URL https:// doi.org/10.1145/3586183.3606763.</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.</p>
<p>Sarna, A., Elkin, C., inversion, Ng, J., Maggie, and Reade, W. Google research - identify contrails to reduce global warming, 2023. URL https://kaggle.com/competitions/ google-research-identify-contrails-reduce-global-warming.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=Yacmpz84TH.</p>
<p>Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C., and Laino, T. "found in translation": predicting outcomes of complex organic chemistry reactions using neural sequence-tosequence models? ?electronic supplementary information (esi) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. see do. Chemical Science, 9:6091 6098, 2017.</p>
<p>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=vAElhFcKW6.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.</p>
<p>Velivckovi'c, P., Badia, A. P., Budden, D., Pascanu, R., Banino, A., Dashevskiy, M., Hadsell, R., and Blundell, C. The clrs algorithmic reasoning benchmark. In International Conference on Machine Learning, 2022.</p>
<p>URL https://api.semanticscholar.org/ CorpusID:249210177.</p>
<p>Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L. J., and Anandkumar, A. Voyager: An openended embodied agent with large language models. ArXiv, abs/2305.16291, 2023a.</p>
<p>Wang, Q., Downey, D., Ji, H., and Hope, T. Scimon: Scientific inspiration machines optimized for novelty. arXiv preprint arXiv:2305.14259, 2023b.</p>
<p>Warstadt, A., Choshen, L., Mueller, A., Williams, A., Wilcox, E. G., and Zhuang, C. Call for papers - the babylm challenge: Sample-efficient pretraining on a developmentally plausible corpus. ArXiv, abs/2301.11796, 2023.</p>
<p>Woodward, B., eor123, GenevievePatterson, and Carlsen, L. Fathomnet 2023, 2023. URL https://kaggle.com/competitions/ fathomnet-out-of-sample-detection.</p>
<p>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Zhang, L., Zhang, Y., Ren, K., Li, D., and Yang, Y. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. ArXiv, abs/2304.14979, 2023a. URL https: //api.semanticscholar.org/CorpusID: 258418182.</p>
<p>Zhang, M., Qamar, M., Kang, T., Jung, Y., Zhang, C., Bae, S.-H., and Zhang, C. A survey on graph diffusion models: Generative ai in science for molecule, protein and material. ArXiv, abs/2304.01565, 2023b.</p>
<p>Zhang, S., Gong, C., Wu, L., Liu, X., and Zhou, M. Automl-gpt: Automatic machine learning with gpt. ArXiv, abs/2305.02499, 2023c. URL https: //api.semanticscholar.org/CorpusID: 258480269.</p>
<p>Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., and Neubig, G. Webarena: A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. URL https://api.semanticscholar.org/ CorpusID:260164780.</p>
<h1>A. Agent Framework Comparison</h1>
<p>In Table 5, we show the comparison against different agent frameworks such as LangChain and AutoGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: right;">GPT-4-turbo</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Claude v3 Opus</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Ours</td>
<td style="text-align: right;">AutoGPT</td>
<td style="text-align: right;">LangChain</td>
<td style="text-align: right;">Ours</td>
<td style="text-align: right;">AutoGPT</td>
<td style="text-align: right;">LangChain</td>
</tr>
<tr>
<td style="text-align: left;">cifar10</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">imdb</td>
<td style="text-align: right;">$\mathbf{1 2 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ogbn-arxiv</td>
<td style="text-align: right;">$\mathbf{6 2 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">62.5</td>
</tr>
<tr>
<td style="text-align: left;">house-price</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: right;">25.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: right;">62.5</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">spaceship-titanic</td>
<td style="text-align: right;">$\mathbf{5 0 . 0}$</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: right;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: right;">75.0</td>
</tr>
<tr>
<td style="text-align: left;">parkinsons-disease</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">fathomnet</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">feedback</td>
<td style="text-align: right;">$\mathbf{3 7 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">identify-contrails</td>
<td style="text-align: right;">$\mathbf{6 2 . 5}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">llama-inference</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">vectorization</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{1 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">CLRS</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">$\mathbf{2 5 . 0}$</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">BabyLM</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: right;">$\mathbf{2 6 . 0}$</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">$\mathbf{3 7 . 5}$</td>
<td style="text-align: right;">13.5</td>
<td style="text-align: right;">33.7</td>
</tr>
</tbody>
</table>
<p>Table 5. The comparison of success rates of different agent frameworks using GPT-4-turbo and Claude v3 Opus.</p>
<h2>B. Benchmark Details</h2>
<p>For Canonical Tasks, Classic Kaggle, Kaggle Challenges and Recent Research, we require the agent to generate a submission.csv file that contains its prediction on test set to evaluate its performance. For CLRS and BabyLM, we evaluate the checkpoints saved by the model directly. For these tasks, we provide a starter code train.py that can already generate the required submission files properly with a baseline model or dummy predictions. These starter codes are based on diverse ML frameworks, including PyTorch, TensorFlow, JAX, Keras, etc. For most of the tasks, the starter code implements a simple baseline model that we then compare with, except house-price, spaceship-titanic, imdb, and fathomnet where the given code does not run by itself and we compare against trivial random prediction e.g. 0.5 accuracy for imdb. For Code Improvement tasks, we simply time the produced code. For Tools tasks, we perform preliminary human evaluation.</p>
<h2>C. Quantitative Analysis</h2>
<p>In Figure 7, we show the percentage of time agents spent on using each action and the distribution of numbers of steps used by agents.</p>
<h2>D. Qualitative Examples</h2>
<p>Bellow, we show some examples to demonstrate the benefits of each component in our agent as well as the failure modes.</p>
<h2>D.1. Research Plan and Status</h2>
<p>The Research Plan and Status entries produced by our agent at each step are highly detailed and interpretable, so it is both useful for guiding the agent through the exploration process and for human understanding. Here we present one example from the agent with Claude v1.0 for CIFAR-10 training.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Left: the percentage of time agents spent on using each action; Right: the distribution of numbers of steps used by agents.</p>
<p>At step 0 , the agent comes up the following plan:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nl">Status:</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">Understand</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">train</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">dataset</span><span class="p">.</span><span class="w"> </span><span class="n">Get</span><span class="w"> </span><span class="n">familiar</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span>
<span class="w">    </span><span class="n">architecture</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">hyperparameters</span><span class="p">.</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Come</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">improve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="nl">performance:</span>
<span class="o">-</span><span class="w"> </span><span class="n">Try</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="n">learning</span><span class="w"> </span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="n">Add</span><span class="w"> </span><span class="n">regularization</span><span class="w"> </span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="n">decay</span><span class="p">,</span><span class="w"> </span><span class="n">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="n">Change</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">architecture</span><span class="w"> </span><span class="p">(</span><span class="n">more</span><span class="w"> </span><span class="n">layers</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">nodes</span><span class="p">)</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">Define</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">baseline</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">original</span><span class="w"> </span><span class="n">train</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="n">script</span><span class="p">.</span><span class="w"> </span><span class="n">Record</span><span class="w"> </span><span class="n">accuracy</span><span class="w"> </span><span class="n">on</span>
<span class="w">    </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="p">.</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">Try</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">planned</span><span class="w"> </span><span class="n">improvements</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">one</span><span class="p">,</span><span class="w"> </span><span class="n">evaluating</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">each</span>
<span class="w">    </span><span class="n">trial</span><span class="p">.</span><span class="w"> </span><span class="n">Compare</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">baseline</span><span class="p">.</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">Once</span><span class="w"> </span><span class="n">improved</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="mh">10</span><span class="o">%</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">achieved</span><span class="w"> </span><span class="n">within</span><span class="w"> </span><span class="mh">10</span><span class="w"> </span><span class="n">epochs</span><span class="p">,</span><span class="w"> </span><span class="n">save</span>
<span class="w">    </span><span class="n">per</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="n">probabilities</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">submission</span><span class="p">.</span><span class="n">csv</span><span class="p">.</span>
<span class="mf">6.</span><span class="w"> </span><span class="n">Submit</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">details</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">configuration</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">performance</span><span class="p">.</span>
</code></pre></div>

<p>At step 10 before the agent submit the final answer, the agent's plan and status is updated to below:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">Understand</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">dataset</span><span class="p">.</span><span class="w"> </span><span class="nx">Got</span><span class="w"> </span><span class="nx">familiar</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span>
<span class="w">    </span><span class="nx">architecture</span><span class="w"> </span><span class="p">(</span><span class="nx">simple</span><span class="w"> </span><span class="nx">CNN</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="nx">lr</span><span class="p">=</span><span class="m m-Double">0.1</span><span class="p">,</span><span class="w"> </span><span class="nx">momentum</span><span class="p">=</span><span class="m m-Double">0.9</span><span class="p">,</span><span class="w"> </span><span class="nx">batch</span>
<span class="w">    </span><span class="nx">size</span><span class="p">=</span><span class="mi">128</span><span class="p">).</span><span class="w"> </span><span class="nx">Dataset</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">CIFAR</span><span class="o">-</span><span class="mi">10</span><span class="p">.</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Come</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performance</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">different</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="nx">learning</span><span class="w"> </span><span class="nx">rate</span><span class="p">,</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span><span class="w"> </span><span class="nx">Increase</span>
<span class="w">    </span><span class="nx">learning</span><span class="w"> </span><span class="nx">rate</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="m m-Double">0.3</span><span class="p">,</span><span class="w"> </span><span class="nx">decrease</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="mi">64</span><span class="w"> </span><span class="nx">Tried</span><span class="w"> </span><span class="nx">increasing</span><span class="w"> </span><span class="nx">LR</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="m m-Double">0.3</span>
<span class="w">    </span><span class="k">in</span><span class="w"> </span><span class="nx">train_lr03</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">but</span><span class="w"> </span><span class="nx">resulted</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">lower</span><span class="w"> </span><span class="nx">accuracy</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">regularization</span><span class="w"> </span><span class="p">(</span><span class="nx">dropout</span><span class="p">,</span><span class="w"> </span><span class="nx">weight</span><span class="w"> </span><span class="nx">decay</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">dropout</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">p</span><span class="p">=</span><span class="m m-Double">0.5</span><span class="w"> </span><span class="nx">after</span>
<span class="w">    </span><span class="nx">conv</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">fc</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">train_dropout</span><span class="p">.</span><span class="nx">py</span><span class="p">.</span><span class="w"> </span><span class="nx">Improved</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">but</span><span class="w"> </span><span class="nx">lower</span>
<span class="w">        </span><span class="nx">accuracy</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Change</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">architecture</span><span class="w"> </span><span class="p">(</span><span class="nx">more</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">nodes</span><span class="p">)</span><span class="w"> </span><span class="nx">Increase</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">filters</span><span class="w"> </span><span class="k">in</span>
<span class="w">    </span><span class="nx">conv</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="mi">64</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">train_arch</span><span class="p">.</span><span class="nx">py</span><span class="p">.</span><span class="w"> </span><span class="nx">Test</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="m m-Double">64.31</span><span class="o">%</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="mi">5</span>
<span class="w">    </span><span class="nx">epochs</span><span class="p">.</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">Define</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">baseline</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="p">.</span><span class="w"> </span><span class="nx">Record</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="nx">on</span>
<span class="w">    </span><span class="nx">test</span><span class="w"> </span><span class="nx">set</span><span class="p">.</span><span class="w"> </span><span class="nx">Baseline</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="m m-Double">52.53</span><span class="o">%</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nx">epochs</span><span class="p">.</span>
<span class="mi">4</span><span class="p">.</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">planned</span><span class="w"> </span><span class="nx">improvements</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">one</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluating</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">each</span>
<span class="w">    </span><span class="nx">trial</span><span class="p">.</span><span class="w"> </span><span class="nx">Compare</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">baseline</span><span class="p">.</span><span class="w"> </span><span class="nx">Tried</span><span class="w"> </span><span class="nx">increasing</span><span class="w"> </span><span class="nx">LR</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="m m-Double">0.3</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluating</span>
<span class="w">    </span><span class="nx">performance</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">train_lr03</span><span class="p">.</span><span class="nx">py</span><span class="p">.</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">much</span><span class="w"> </span><span class="nx">worse</span><span class="p">.</span><span class="w"> </span><span class="nx">Added</span>
</code></pre></div>

<p>dropout to train_dropout.py, test accuracy 49.34\% after 5 epochs. Modified architecture in train_arch.py, test accuracy 64.31\% after 5 epochs.
5. Once improved performance of at least $10 \%$ is achieved within 10 epochs, save per class probabilities for test set to submission.csv. Saved test set probabilities to submission.csv after running train_arch_submission.py.
6. Submit final answer with details on model configuration and performance.</p>
<p>Between these two steps, the agent gradually updated the Research Plan and Status entry after editing the file and executing it as recorded. See the full example in the appendix.</p>
<p>However, one common failure mode that this entry fails to prevent is when the agent plans to carry out too complex an edit and becomes stuck with debugging, which occurs in $40 \%$ of the runs for Claude v1.0 as shown in 5. Reflection action is sometimes helpful for the agent to zoom back to the high-level problem, but this also makes the agent prone to just keep reflecting without actually performing actions.</p>
<h1>D.2. Fact Check</h1>
<p>The Fact Check entry allows the agent to double-check whether the update to Research Plan and Status is factual. One common failure mode during our prelimiary experiments is that the model hallucinates improvement after modifying the file without ever executing it. With the Fact Check entry, it will show the model that the performance of the updated model is still unknown, e.g.</p>
<div class="codehilite"><pre><span></span><code>Fact Check: Performance after running train_dropout.py still needs to be
    evaluated. Other facts unchanged.
</code></pre></div>

<p>Of course, this does not guard against hallucination completely. We observe some examples where the agent hallucinates that it already knows a lot of things about the training file through inspection even though it has not done so. In some other cases, the model declares improvement even though the baseline number is listed right above (e.g. $51.80 \%$ ) and clearly is higher: "Achieved test accuracy of $26.35 \%$ which improves over baseline by $10 \%$ ". As shown in 5 , this happens to $20 \%$ of the runs for Claude-1</p>
<div class="codehilite"><pre><span></span><code>Research Plan and Status:
1. Understand the existing train.py script and extract the baseline performance.
    Done. Baseline test accuracy is 51.80%.
2. Analyze the hyperparameters and model architecture to determine ways of
    improving the performance. In progress.
3. Run experiments with new hyperparameters and model architecture. Done. Tried
    increasing learning rate to 0.3 and 10 epochs. Achieved test accuracy of
    26.35% which improves over baseline by 10%.
...
Fact Check: The statement &quot;**Increased learning rate to 0.3 and trained for 10
    epochs. Achieved test accuracy of 26.35% which improves over baseline by 10%,
    meeting the performance goal.**&quot; is confirmed by running the train_lr03.py
    script and observing the output.
</code></pre></div>

<h2>D.3. Problem Misspecification</h2>
<p>One "failure mode" we observe during the development of this benchmark is that the problem specification can be critical to agent performance. The problem description needs to clearly specify what file and what metrics will be evaluated. In one of the extreme case, we actually observed that our agent tried to increase SMAPE score on amp-parkinsons-disease-progression-prediction dataset, since it does not know that SMAPE is the lower the better:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Assistant</span><span class="o">:</span>
<span class="n">Reflection</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">shows</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">executing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">updated</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="na">py</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="k">with</span>
<span class="w">    </span><span class="n">the</span><span class="w"> </span><span class="n">tuned</span><span class="w"> </span><span class="n">random</span><span class="w"> </span><span class="n">forest</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">achieved</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">cross</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mf">76.16</span><span class="o">,</span>
</code></pre></div>

<p>which is an improvement over the baseline score of 61.40. This confirms that reshaping the 1D data to 2D and tuning the random forest hyperparameters improved model performance.</p>
<p>We compare the average amount of tokens and time spent by different agents for each task in Figure 8 and 9. Note that the total tokens is the sum prompt and completion tokens. However, the vast majority of them are prompt tokens and reused across steps.</p>
<h1>E. Efficiency</h1>
<h2>F. Full Example</h2>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">helpful</span><span class="w"> </span><span class="n">research</span><span class="w"> </span><span class="n">assistant</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">access</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">tools</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="n">Files</span><span class="p">:</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">navigate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">system</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="n">Files</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;dir_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">directory</span><span class="p">,</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="s2">&quot;.&quot;</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="s2">&quot;</span>
<span class="w">            </span><span class="n">folder1</span><span class="o">/</span><span class="n">folder2</span><span class="s2">&quot;]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">folders</span><span class="w"> </span><span class="ow">in</span>
<span class="w">        </span><span class="n">dir_path</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">dir_path</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">empty</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span>
<span class="w">        </span><span class="n">message</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">dir_path</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">invalid</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="o">-</span><span class="w"> </span><span class="n">Copy</span><span class="w"> </span><span class="n">File</span><span class="p">:</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">copy</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">name</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">Copy</span><span class="w"> </span><span class="n">File</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;source&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;destination&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">current</span>
<span class="w">                </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">A</span><span class="w"> </span><span class="n">success</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">copied</span><span class="w"> </span><span class="n">successfully</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">an</span>
<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="n">message</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">copied</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="o">-</span><span class="w"> </span><span class="n">Undo</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="p">:</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">undo</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">edit</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">Undo</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;script_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">before</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">last</span><span class="w"> </span><span class="n">edit</span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    an error message.]
    ***
<span class="k">-</span> Execute Script:
    Use this to execute the python script. The script must already exist.
    Usage:
    ***
    Action: Execute Script
    Action Input: {
        &quot;script_name&quot;: [a valid python script name with relative path to
            current directory if needed]
    }
    Observation: [The observation will be output of the script or errors.]
    ***
<span class="k">-</span> Final Answer:
    Use this to provide the final answer to the current task.
    Usage:
    ***
    Action: Final Answer
    Action Input: {
        &quot;final_answer&quot;: [a detailed description on the final answer]
    }
    Observation: [The observation will be empty.]
    ***
<span class="k">-</span> Understand File:
    Use this to read the whole file and understand certain aspects. You
        should provide detailed description on what to look for and what
        should be returned. To get a better understanding of the file, you
        can use Inspect Script Lines action to inspect specific part of the
        file.
    Usage:
    ***
    Action: Understand File
    Action Input: {
        &quot;file_name&quot;: [a valid file name with relative path to current
            directory if needed],
            &quot;things_to_look_for&quot;: [a detailed description on what to look for and
                what should returned]
    }
    Observation: [The observation will be a description of relevant content
        and lines in the file. If the file does not exist, the observation
        will be an error message.]
    ***
<span class="k">-</span> Inspect Script Lines:
    Use this to inspect specific part of a python script precisely, or the
        full content of a short script. The number of lines to display is
        limited to 100 lines. This is especially helpful when debugging.
    Usage:
    ***
    Action: Inspect Script Lines
    Action Input: {
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">            </span><span class="s2">&quot;script_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;start_line_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">number</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;end_line_number&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">number</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">between</span>
<span class="w">        </span><span class="n">start_line_number</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">end_line_number</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">message</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="o">-</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="w"> </span><span class="p">(</span><span class="n">AI</span><span class="p">):</span>
<span class="w">    </span><span class="n">Use</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">relatively</span><span class="w"> </span><span class="n">large</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">cohesive</span><span class="w"> </span><span class="n">edit</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="o">.</span>
<span class="w">        </span><span class="n">Instead</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">editing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">directly</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">describe</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edit</span>
<span class="w">        </span><span class="n">instruction</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">another</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="n">this</span><span class="o">.</span>
<span class="w">    </span><span class="n">Usage</span><span class="p">:</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="n">Action</span><span class="p">:</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="w"> </span><span class="p">(</span><span class="n">AI</span><span class="p">)</span>
<span class="w">    </span><span class="n">Action</span><span class="w"> </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;script_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">current</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="o">.</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">sctipt</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="n">it</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="o">.</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;edit_instruction&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">detailed</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">to</span>
<span class="w">                </span><span class="n">edit</span><span class="w"> </span><span class="n">it</span><span class="o">.</span><span class="p">],</span>
<span class="w">            </span><span class="s2">&quot;save_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">a</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">relative</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">current</span>
<span class="w">                </span><span class="n">directory</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">needed</span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">Observation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edited</span><span class="w"> </span><span class="n">content</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="o">.</span>
<span class="w">        </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">exist</span><span class="p">,</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">observation</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span>
<span class="w">        </span><span class="n">message</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">always</span><span class="w"> </span><span class="n">double</span><span class="w"> </span><span class="n">check</span><span class="w"> </span><span class="n">whether</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edit</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">correct</span><span class="o">.</span>
<span class="w">        </span><span class="n">If</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">far</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">correct</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Undo</span><span class="w"> </span><span class="n">Edit</span><span class="w"> </span><span class="n">Script</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">to</span>
<span class="w">            </span><span class="n">undo</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">edit</span><span class="o">.</span><span class="p">]</span>
<span class="w">    </span><span class="o">***</span>
<span class="n">Research</span><span class="w"> </span><span class="n">Problem</span><span class="p">:</span><span class="w"> </span><span class="n">Given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">script</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="n">py</span><span class="p">,</span><span class="w"> </span><span class="n">improve</span><span class="w"> </span><span class="n">upon</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">current</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">performance</span><span class="w"> </span><span class="p">(</span><span class="n">trained</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">hyperparmeters</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="n">py</span><span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="n">The</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">epochs</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">within</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">save</span><span class="w"> </span><span class="n">time</span><span class="o">.</span><span class="w"> </span><span class="n">Save</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="k">class</span>
<span class="w">    </span><span class="n">probabilities</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">submission</span><span class="o">.</span><span class="n">csv</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">shown</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">train</span><span class="o">.</span><span class="n">py</span><span class="o">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">know</span><span class="w"> </span><span class="n">anything</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">far</span><span class="o">.</span>
<span class="n">Follow</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">instructions</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">forget</span><span class="w"> </span><span class="n">them</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="n">First</span><span class="p">,</span><span class="w"> </span><span class="n">come</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">high</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">understanding</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span>
<span class="n">problem</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">available</span><span class="w"> </span><span class="n">tools</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">record</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Status</span><span class="o">.</span>
<span class="n">You</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">revise</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">later</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Status</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">well</span><span class="w"> </span><span class="n">organized</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">succinctly</span><span class="w"> </span><span class="n">keep</span><span class="w"> </span><span class="n">track</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="n">high</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="p">(</span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">revised</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">steps</span>
<span class="n">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">progress</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="n">short</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">conclusions</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">after</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">has</span>
<span class="n">been</span><span class="w"> </span><span class="n">performed</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">Research</span><span class="w"> </span><span class="n">Plan</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Status</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">include</span><span class="w"> </span><span class="n">progress</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">made</span><span class="w"> </span><span class="n">by</span>
<span class="n">previous</span><span class="w"> </span><span class="n">steps</span><span class="o">.</span><span class="w"> </span><span class="n">It</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">include</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="n">confirmed</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span>
<span class="n">previous</span><span class="w"> </span><span class="n">observation</span><span class="o">.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">-</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">estimates</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">confirmed</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">included</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">status</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">observing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">come</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">good</span><span class="w"> </span><span class="nx">experiment</span><span class="w"> </span><span class="nx">design</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">addresses</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="p">,</span>
<span class="w">    </span><span class="k">and</span><span class="w"> </span><span class="nx">whenever</span><span class="w"> </span><span class="nx">applicable</span><span class="p">,</span><span class="w"> </span><span class="nx">define</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">measure</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">baseline</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">relevant</span><span class="w"> </span><span class="nx">system</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">attempting</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">improvements</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Follow</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">try</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">achieve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">straightforwardly</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">possible</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Highlight</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">supporting</span><span class="w"> </span><span class="nx">experiment</span><span class="w"> </span><span class="nx">results</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">reasoning</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">drawing</span><span class="w"> </span><span class="nx">any</span>
<span class="w">    </span><span class="nx">conclusions</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">try</span><span class="w"> </span><span class="nx">installing</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">packages</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">libraries</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">believe</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">solved</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="p">,</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Final</span><span class="w"> </span><span class="nx">Answer</span><span class="w"> </span><span class="nx">action</span>
<span class="w">        </span><span class="nx">to</span><span class="w"> </span><span class="nx">submit</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">answer</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">submit</span><span class="w"> </span><span class="nx">once</span><span class="p">,</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">check</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span>
<span class="w">        </span><span class="nx">have</span><span class="w"> </span><span class="nx">achieved</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="nx">before</span><span class="w"> </span><span class="nx">submitting</span><span class="p">.</span>
<span class="nx">Always</span><span class="w"> </span><span class="nx">respond</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="nx">exactly</span><span class="p">:</span>
<span class="nx">Reflection</span><span class="p">:</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="nx">does</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">observation</span><span class="w"> </span><span class="nx">mean</span><span class="p">?</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">error</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">caused</span><span class="w"> </span><span class="nx">the</span>
<span class="w">        </span><span class="nx">error</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">how</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">debug</span><span class="p">?</span>
<span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">:</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">full</span><span class="w"> </span><span class="nx">high</span><span class="w"> </span><span class="nx">level</span><span class="w"> </span><span class="nx">research</span><span class="w"> </span><span class="nx">plan</span><span class="p">,</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">status</span>
<span class="w">    </span><span class="k">and</span><span class="w"> </span><span class="nx">confirmed</span><span class="w"> </span><span class="nx">results</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">briefly</span><span class="w"> </span><span class="nx">annotated</span><span class="p">.</span><span class="w"> </span><span class="nx">It</span><span class="w"> </span><span class="nx">must</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">include</span>
<span class="w">    </span><span class="nx">progress</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">has</span><span class="w"> </span><span class="nx">been</span><span class="w"> </span><span class="nx">made</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">steps</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">update</span><span class="p">,</span>
<span class="w">    </span><span class="nx">enclose</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">update</span><span class="w"> </span><span class="nx">text</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">asterisks</span><span class="w"> </span><span class="o">**</span><span class="k">like</span><span class="w"> </span><span class="nx">this</span><span class="o">**</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">no</span>
<span class="w">        </span><span class="nx">update</span><span class="p">,</span><span class="w"> </span><span class="nx">just</span><span class="w"> </span><span class="nx">copy</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">high</span><span class="w"> </span><span class="nx">level</span>
<span class="w">        </span><span class="nx">plan</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">fully</span><span class="w"> </span><span class="nx">retained</span><span class="p">,</span><span class="w"> </span><span class="nx">unless</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span>
<span class="w">        </span><span class="nx">intentionally</span><span class="w"> </span><span class="nx">revised</span><span class="p">.</span>
<span class="nx">Fact</span><span class="w"> </span><span class="nx">Check</span><span class="p">:</span><span class="w"> </span><span class="nx">List</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">objective</span><span class="w"> </span><span class="nx">statements</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">updates</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span>
<span class="w">        </span><span class="nx">Status</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">point</span><span class="w"> </span><span class="nx">out</span><span class="w"> </span><span class="nx">whether</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">guessed</span><span class="w"> </span><span class="nx">versus</span><span class="w"> </span><span class="nx">directly</span>
<span class="w">        </span><span class="nx">confirmed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">previous</span><span class="w"> </span><span class="nx">observation</span><span class="w"> </span><span class="nx">directly</span><span class="w"> </span><span class="nx">above</span><span class="p">.</span><span class="w"> </span><span class="nx">Performance</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="nx">can</span>
<span class="w">        </span><span class="nx">only</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">confirmed</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">observing</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span><span class="p">.</span>
<span class="nx">Thought</span><span class="p">:</span><span class="w"> </span><span class="nx">What</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">currently</span><span class="w"> </span><span class="nx">doing</span><span class="p">,</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">actions</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">perform</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">why</span>
<span class="nx">Action</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">take</span><span class="p">,</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">names</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">tools</span>
<span class="nx">Action</span><span class="w"> </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">input</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">valid</span><span class="w"> </span><span class="nx">JSON</span><span class="w"> </span><span class="kt">string</span>
<span class="nx">Observation</span><span class="p">:</span>
<span class="w">    </span><span class="o">...</span>
<span class="nx">the</span><span class="w"> </span><span class="nx">result</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">action</span>
<span class="w">    </span><span class="o">...</span>
<span class="nx">Step</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span>
<span class="nx">Assistant</span><span class="p">:</span>
<span class="nx">Reflection</span><span class="p">:</span><span class="w"> </span><span class="nx">I</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">read</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="w"> </span><span class="nx">description</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">tools</span><span class="w"> </span><span class="nx">available</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">goal</span><span class="w"> </span><span class="k">is</span>
<span class="w">        </span><span class="nx">to</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">least</span><span class="w"> </span><span class="mi">10</span><span class="o">%</span><span class="w"> </span><span class="nx">within</span><span class="w"> </span><span class="mi">10</span>
<span class="w">        </span><span class="nx">epochs</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">training</span><span class="p">.</span>
<span class="nx">Research</span><span class="w"> </span><span class="nx">Plan</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Status</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">Understand</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">dataset</span><span class="p">.</span><span class="w"> </span><span class="nx">Get</span><span class="w"> </span><span class="nx">familiar</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span>
<span class="w">        </span><span class="nx">architecture</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="p">.</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Come</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">performance</span><span class="p">:</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">different</span><span class="w"> </span><span class="nx">hyperparameters</span><span class="w"> </span><span class="p">(</span><span class="nx">learning</span><span class="w"> </span><span class="nx">rate</span><span class="p">,</span><span class="w"> </span><span class="nx">batch</span><span class="w"> </span><span class="nx">size</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">regularization</span><span class="w"> </span><span class="p">(</span><span class="nx">dropout</span><span class="p">,</span><span class="w"> </span><span class="nx">weight</span><span class="w"> </span><span class="nx">decay</span><span class="p">,</span><span class="w"> </span><span class="nx">etc</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="nx">Change</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">architecture</span><span class="w"> </span><span class="p">(</span><span class="nx">more</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">nodes</span><span class="p">)</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">Define</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">baseline</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">running</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">original</span><span class="w"> </span><span class="nx">train</span><span class="p">.</span><span class="nx">py</span><span class="w"> </span><span class="nx">script</span><span class="p">.</span><span class="w"> </span><span class="nx">Record</span><span class="w"> </span><span class="nx">accuracy</span><span class="w"> </span><span class="nx">on</span>
<span class="w">    </span><span class="nx">test</span><span class="w"> </span><span class="nx">set</span><span class="p">.</span>
<span class="mi">4</span><span class="p">.</span><span class="w"> </span><span class="nx">Try</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">planned</span><span class="w"> </span><span class="nx">improvements</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">one</span><span class="p">,</span><span class="w"> </span><span class="nx">evaluating</span><span class="w"> </span><span class="nx">performance</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">each</span>
<span class="w">        </span><span class="nx">trial</span><span class="p">.</span><span class="w"> </span><span class="nx">Compare</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">baseline</span><span class="p">.</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Stanford University. Correspondence to: Qian Huang &lt;qhwang@cs.stanford.edu $&gt;$.</p>
<p>2 Our code is released at https://github.com/snapstanford/MLAgentBench/.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2235 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2235</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2235</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-280151696</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.07485v1.pdf" target="_blank">Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning</a></p>
                <p><strong>Paper Abstract:</strong> Multi-Task Learning (MTL) enables multiple tasks to be learned within a shared network, but differences in objectives across tasks can cause negative transfer, where the learning of one task degrades another task's performance. While pre-trained transformers significantly improve MTL performance, their fixed network capacity and rigid structure limit adaptability. Previous dynamic network architectures attempt to address this but are inefficient as they directly convert shared parameters into task-specific ones. We propose Dynamic Token Modulation and Expansion (DTME-MTL), a framework applicable to any transformer-based MTL architecture. DTME-MTL enhances adaptability and reduces overfitting by identifying gradient conflicts in token space and applying adaptive solutions based on conflict type. Unlike prior methods that mitigate negative transfer by duplicating network parameters, DTME-MTL operates entirely in token space, enabling efficient adaptation without excessive parameter growth. Extensive experiments demonstrate that DTME-MTL consistently improves multi-task performance with minimal computational overhead, offering a scalable and effective solution for enhancing transformer-based MTL models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2235.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2235.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DTME-MTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Token Modulation and Expansion for Multi-Task Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-space adaptive framework for transformer-based multi-task learning that (1) analyzes token covariance via SVD to separate range vs null subspaces, and (2) applies token modulation (affine modulators) for range-space conflicts and token expansion (task-specific tokens) for null-space conflicts to mitigate negative transfer with minimal parameter overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DTME-MTL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Operates on pretrained vision transformers by treating layer tokens as learnable objects; uses SVD on token uncentered covariance to split token space into range and null subspaces, detects gradient conflicts per-subspace, and adaptively (a) inserts per-task affine token modulators when conflicts lie in the range space, and (b) concatenates a small number of learnable task-specific tokens when conflicts lie in the null space. Integrates with existing transformer MTL architectures (e.g., InvPT, TaskPrompter).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>token-space partitioning (SVD-derived range vs null) with task-specific affine modulators and a small set of task-specific tokens concatenated to the shared token set</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision multi-task dense prediction (NYUD-v2, PASCAL-Context, Taskonomy) using pretrained ViT backbones</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>On Taskonomy (11 tasks, ViT-B) DTME-MTL reported a multi-task improvement △m = +4.67% (Table 3) and numerically improved many per-task metrics (see Table 3 row 'DTME-MTL' for task-specific numbers). On NYUD-v2 and PASCAL-Context, DTME-MTL (TM+TE) gives consistent gains over shared multi-task baseline and is often comparable to single-task fine-tuning baselines (examples: NYUD-v2, Semseg mIoU 38.27 with TM+TE vs lower baseline (MT) values reported in paper; integration increases InvPT from 53.56→54.38 mIoU and TaskPrompter 55.30→56.36 mIoU in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Baseline (shared transformer, 'Baseline (MT)') results are reported throughout; DTME-MTL improves over Baseline (MT) with small parameter increase (examples: paper reports 0.2%–0.3% param increase yields sizable gains; Taskonomy param ↑ reported at 0.118% with ViT-B). Single-task full fine-tuning ('Baseline (ST)') is shown as an upper bound; DTME-MTL is often comparable to ST in aggregate multi-task metric.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Small parameter overhead: reported increases range from ≈0.048% (when applied to InvPT) and 0.046% (TaskPrompter) to dataset/backbone-specific values (paper reports ~0.2–0.3% on some experiments; Supplementary: ViT-L +0.089%, ViT-S +0.23%, ViT-T +0.46%). Inference-time overhead: ≈13.4% increase per task with ViT-B. SVD and conflict computation costs reported in Table 6 (example ViT-B: token-space SVD ~11.54 minutes; gradient-conflict computation ~21.94 minutes on a single NVIDIA RTX A6000 for the reported setup).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Baseline shared transformer (MT) inference/time is the reference; DTME-MTL adds only a minor parameter and time overhead (see above). Exact baseline FLOPs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>No explicit numeric claim of reduced sample complexity; authors state they perform SVD once early in training and static small task-token allocation; no quantified 'less data' claim reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Authors report that token-space conflict resolution reduces overfitting compared to parameter-branching methods (e.g., Recon) and 'improves generalization' implicitly — DTME-MTL obtains higher multi-task metrics while showing less overfitting. Specific cross-dataset or OOD metrics not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Authors provide interpretability-style analysis by decomposing token gradients into range/null components and tracking conflict reductions per-space; show Token Modulation reduces range-space conflicts and Token Expansion reduces null-space conflicts (Table 14 / Appendix D). No human-interpretable explanations beyond subspace attribution are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Across NYUD-v2, PASCAL-Context, and Taskonomy, DTME-MTL consistently improves multi-task metrics over shared baselines and outperforms many multi-task optimization methods on Taskonomy (△m +4.67% over ST baseline and better than listed optimization baselines in Table 3). Selective application to high-conflict layers (≈25–50% of layers) gives best trade-off; indiscriminate expansion harms performance.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Authors emphasize minimal parameter overhead (fractions of a percent) and give wall-clock timings for SVD/conflict computation; claim relative computational cost is small compared to day(s)-scale MTL training (SVD+conflict ≈ 1 hour for ViT-L reported). No explicit FLOPs-memory tradeoff curve under tight budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adapting token representations (modulation in pretrained span; expansion for out-of-span directions) yields consistent multi-task gains with minimal parameter/time overhead and less overfitting than naive parameter-branching; resolving conflicts in token space is an effective, efficient alternative to converting shared parameters into task-specific branches.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results and analyses support the idea that task-aligned, adaptive representations (here token-space modulation/expansion) outperform uniform shared representations and naive parameter duplication, improving multi-task performance and reducing overfitting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2235.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2235.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recon (Reducing conflicting gradients from the root for multi-task learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic network architecture that identifies layers with high gradient conflict and converts those shared parameters into task-specific parameters (i.e., splits/branches layers) to eliminate conflicts by parameter duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reducing conflicting gradients from the root for multi-task learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Detects conflicting gradients per-layer and dynamically converts the most-conflicted shared layers into task-specific branches (duplicating parameters) to avoid gradient interference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>layer-wise parameter branching (duplicating shared parameters into task-specific copies)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task learning (prior work and compared on vision MTL benchmarks in this paper, e.g., NYUD-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>In experiments reported in this paper (NYUD-v2, ViT backbone), Recon performed worse than DTME-MTL and even worse than the joint baseline in reported metrics: Recon Semseg 31.92 (vs DTME-MTL 38.27), Recon depth RMSE 0.693 (vs DTME-MTL 0.637), Recon normal mErr 23.35 (vs DTME-MTL 21.64) as shown in Table 7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Recon was compared to a joint shared-parameter baseline and was observed to reduce parameter-space conflicts but lead to overfitting and degraded multi-task performance relative to DTME-MTL and sometimes to the joint baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Paper reports Recon increased parameter count substantially in the compared setting (Table 7 shows #Param ↑ = 3.34% for Recon on NYUD-v2), with corresponding computational/overfitting costs; authors state parameter inefficiency and excessive computational overhead make Recon less scalable for large transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Joint baseline has lower parameter count; Recon increases parameters by multiple percent (3.34% in Table 7) versus DTME-MTL's ~0.2–0.3% in related experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Authors report that converting parameters to task-specific copies (Recon) tends to overfit in transfer/fine-tuning regimes; no explicit numeric sample-efficiency curves provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Recon's aggressive parameter branching was reported to cause overfitting and poorer generalization in transformer-based MTL finetuning (empirically worse metrics on NYUD-v2 with ViT backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>In the paper's experiments Recon reduced parameter-space gradient conflicts but led to worse multi-task performance and more overfitting compared to DTME-MTL and the shared baseline on NYUD-v2.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Higher parameter growth (3.34% reported) and higher compute relative to DTME-MTL; authors call it parameter-inefficient and less scalable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Naively converting shared transformer parameters into task-specific branches (Recon-style) can reduce parameter-space conflicts but is parameter-inefficient and prone to overfitting in pretrained transformer fine-tuning, yielding worse multi-task performance than token-space adaptive methods.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>While Recon embodies task-aligned specialization (task-specific branches), its empirical failure in this setting challenges the simplistic notion that more task-specific parameters always improve multi-task results — careful, space-aware adaptation (token-space) can be superior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2235.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2235.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixture-of-Experts (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Experts (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures that partition model capacity into multiple expert modules and route inputs to subsets of experts, enabling adaptive specialization across tasks or inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixture-of-Experts (MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Predefined groups of expert parameters (layers or submodules) with a gating mechanism that adaptively selects or weighs experts per input or task, enabling selective allocation of capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>mixture-of-experts gating (expert routing) enabling adaptive per-task/exemplar specialization</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task learning and large-scale models (discussed as prior work in vision and NLP contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>MoE approaches can be compute-efficient in inference if only a subset of experts is activated, but the paper only references them as prior work and does not provide new measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Mentioned as an existing mechanism to separate shared and task-specific components; not evaluated directly in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>MoE-style adaptive capacity allocation is a relevant prior approach to task-aligned specialization; authors contrast MoE's predefined expert groups with their token-space adaptive approach which avoids duplicating large parameter sets.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MoE represents a class of task-aligned approaches, but this paper neither empirically validates nor refutes MoE; it contrasts MoE-style parameter-level specialization with token-space methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2235.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2235.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaskPrompter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TaskPrompter (spatial-channel multi-task prompting for dense scene understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based multi-task architecture that uses task-specific prompt tokens to encapsulate task-specific information for dense prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Taskprompter: Spatial-channel multi-task prompting for dense scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TaskPrompter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adds task-specific prompt tokens (task prompters) to a shared transformer to inject task-specific biases and interactions for dense prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>task-specific prompt tokens concatenated to shared tokens (prompting mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision multi-task dense prediction (PASCAL-Context, NYUD-v2 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Base TaskPrompter results reported in Tables 4–5 (e.g., NYUD-v2 TaskPrompter baseline Semseg mIoU 55.30 → with DTME-MTL 56.36; PASCAL-Context baseline mIoU 80.89 → with DTME-MTL 81.01).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to a shared Baseline (MT) and to the TaskPrompter base; DTME-MTL integration improved TaskPrompter performance with negligible parameter increase.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>When DTME-MTL is integrated, the paper reports tiny parameter increases for TaskPrompter (+0.046%) and small runtime overhead (DTME-MTL inference overhead ~13.4% for ViT-B; specific incremental cost for TaskPrompter+DTME not separately listed).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>TaskPrompter benefits from DTME-MTL integration (small mIoU gains reported); TaskPrompter itself is a task-aligned prompting architecture aimed at multi-task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Prompting-based per-task tokens provide task-aligned capacity that can be further improved (with negligible extra params) by token-space adaptive methods (DTME-MTL).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>TaskPrompter embodies a task-aligned representation approach; integrating DTME-MTL further improves performance, supporting the benefit of task-specific representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2235.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2235.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InvPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InvPT (Inverted Pyramid Multi-Task Transformer variant used in MTL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based multi-task architecture (InvPT) that can be equipped with task-specific tokens/prompters; used in the paper to demonstrate compatibility and gains from DTME-MTL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InvPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based multi-task architecture (inverted-pyramid style) that shares an encoder and uses task-specific decoding mechanisms; serves as a state-of-the-art baseline that DTME-MTL can augment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>shared transformer encoder with task-specific decoding/prompting mechanisms (task tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision multi-task dense prediction (NYUD-v2, PASCAL-Context in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>InvPT baseline (NYUD-v2) Semseg mIoU 53.56 → with DTME-MTL 54.38 (Table 4). On PASCAL-Context InvPT 79.03 → with DTME-MTL 81.91 (Table 5). Paper reports DTME-MTL integration adds ~0.048% parameters for InvPT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>InvPT as reported is a task-aware architecture; DTME-MTL further improves it slightly.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>DTME-MTL integration yields tiny additional parameter overhead (~0.048% reported) and modest runtime overhead as reported for DTME generally; exact per-model FLOPs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>DTME-MTL consistently raises InvPT multi-task performance modestly with negligible parameter growth.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>DTME-MTL can be plugged into existing task-aligned MTL architectures like InvPT to further improve multi-task performance with almost no parameter cost.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Integration gains indicate that task-aligned token mechanisms benefit from additional token-space adaptive operations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2235.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2235.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low-Rank Adaptation (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient adaptation method that injects low-rank weight updates for per-task or per-domain adaptation, producing compact task-specific modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Low-rank adaptation of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns low-rank additive updates to pre-trained weight matrices to provide task-specific adaptation with few extra parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>task-specific low-rank adapters attached to weight matrices (parameter-level low-rank factors)</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general transfer/adaptation; discussed in context of MTL inference efficiency in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Paper notes that per-task LoRA modules result in disjoint sets of task-specific weights which (even when merged) require separate forward passes per task, causing inference to scale linearly with number of tasks; DTME-MTL contrasts by enabling single batched forward computation across tasks with only ~13.4% overhead per task.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated in this work; authors discuss LoRA qualitatively and contrast its per-task inference scaling with DTME-MTL's joint inference capability.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>While LoRA is parameter-efficient per-task, its per-task modules require separate forward passes, making it less efficient for joint multi-task inference than shared token-space approaches like DTME-MTL.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>LoRA demonstrates that compact task-specific parameterization can be effective, but the inference scaling issue highlights limitations of certain task-aligned parameterizations for efficient multi-task deployment.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2235.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2235.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shared Transformer Baseline (MT / ST)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shared Transformer Multi-Task Baseline (MT) and Single-Task Full Fine-tuning Baseline (ST)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines used in experiments: (MT) shared pretrained transformer encoder with small task-specific decoders; (ST) same architecture trained per-task (upper bound reference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shared Transformer Baseline (MT) / Single-Task Fine-tune (ST)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MT: one shared pretrained transformer encoder with lightweight decoders per task; ST: same architecture trained for a single task (used as upper bound).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>uniform shared representations across tasks (shared encoder) with small task-specific decoders</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision multi-task dense prediction (NYUD-v2, PASCAL-Context, Taskonomy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>ST reported as an upper bound (Table 3 ST row lists per-task metrics on Taskonomy); MT baseline reported throughout; DTME-MTL improves over MT and approaches ST in aggregate metrics for some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Shared-MT is efficient for joint inference; ST requires separate models per task. DTME-MTL aims to retain joint efficiency while adding small extra params.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>MT suffers from negative transfer; ST is an upper bound often better per-task but inefficient; DTME-MTL closes gap between MT and ST.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Uniform shared representations (MT) can suffer negative transfer; small token-space adaptations can recover much of the gap to single-task fine-tuning (ST) without per-task models.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Empirical results show that having small task-aligned adaptations on top of a shared backbone improves multi-task outcomes relative to purely uniform representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reducing conflicting gradients from the root for multi-task learning <em>(Rating: 2)</em></li>
                <li>Taskprompter: Spatial-channel multi-task prompting for dense scene understanding <em>(Rating: 2)</em></li>
                <li>Low-rank adaptation of large language models <em>(Rating: 2)</em></li>
                <li>Mixture-of-experts vision transformer for efficient multitask learning with model-accelerator co-design <em>(Rating: 2)</em></li>
                <li>Scaling vision with sparse mixture of experts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2235",
    "paper_id": "paper-280151696",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "DTME-MTL",
            "name_full": "Dynamic Token Modulation and Expansion for Multi-Task Learning",
            "brief_description": "A token-space adaptive framework for transformer-based multi-task learning that (1) analyzes token covariance via SVD to separate range vs null subspaces, and (2) applies token modulation (affine modulators) for range-space conflicts and token expansion (task-specific tokens) for null-space conflicts to mitigate negative transfer with minimal parameter overhead.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DTME-MTL",
            "model_description": "Operates on pretrained vision transformers by treating layer tokens as learnable objects; uses SVD on token uncentered covariance to split token space into range and null subspaces, detects gradient conflicts per-subspace, and adaptively (a) inserts per-task affine token modulators when conflicts lie in the range space, and (b) concatenates a small number of learnable task-specific tokens when conflicts lie in the null space. Integrates with existing transformer MTL architectures (e.g., InvPT, TaskPrompter).",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "token-space partitioning (SVD-derived range vs null) with task-specific affine modulators and a small set of task-specific tokens concatenated to the shared token set",
            "is_dynamic_or_adaptive": true,
            "task_domain": "vision multi-task dense prediction (NYUD-v2, PASCAL-Context, Taskonomy) using pretrained ViT backbones",
            "performance_task_aligned": "On Taskonomy (11 tasks, ViT-B) DTME-MTL reported a multi-task improvement △m = +4.67% (Table 3) and numerically improved many per-task metrics (see Table 3 row 'DTME-MTL' for task-specific numbers). On NYUD-v2 and PASCAL-Context, DTME-MTL (TM+TE) gives consistent gains over shared multi-task baseline and is often comparable to single-task fine-tuning baselines (examples: NYUD-v2, Semseg mIoU 38.27 with TM+TE vs lower baseline (MT) values reported in paper; integration increases InvPT from 53.56→54.38 mIoU and TaskPrompter 55.30→56.36 mIoU in Table 4).",
            "performance_uniform_baseline": "Baseline (shared transformer, 'Baseline (MT)') results are reported throughout; DTME-MTL improves over Baseline (MT) with small parameter increase (examples: paper reports 0.2%–0.3% param increase yields sizable gains; Taskonomy param ↑ reported at 0.118% with ViT-B). Single-task full fine-tuning ('Baseline (ST)') is shown as an upper bound; DTME-MTL is often comparable to ST in aggregate multi-task metric.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Small parameter overhead: reported increases range from ≈0.048% (when applied to InvPT) and 0.046% (TaskPrompter) to dataset/backbone-specific values (paper reports ~0.2–0.3% on some experiments; Supplementary: ViT-L +0.089%, ViT-S +0.23%, ViT-T +0.46%). Inference-time overhead: ≈13.4% increase per task with ViT-B. SVD and conflict computation costs reported in Table 6 (example ViT-B: token-space SVD ~11.54 minutes; gradient-conflict computation ~21.94 minutes on a single NVIDIA RTX A6000 for the reported setup).",
            "computational_efficiency_baseline": "Baseline shared transformer (MT) inference/time is the reference; DTME-MTL adds only a minor parameter and time overhead (see above). Exact baseline FLOPs not reported.",
            "sample_efficiency_results": "No explicit numeric claim of reduced sample complexity; authors state they perform SVD once early in training and static small task-token allocation; no quantified 'less data' claim reported.",
            "transfer_generalization_results": "Authors report that token-space conflict resolution reduces overfitting compared to parameter-branching methods (e.g., Recon) and 'improves generalization' implicitly — DTME-MTL obtains higher multi-task metrics while showing less overfitting. Specific cross-dataset or OOD metrics not reported.",
            "interpretability_results": "Authors provide interpretability-style analysis by decomposing token gradients into range/null components and tracking conflict reductions per-space; show Token Modulation reduces range-space conflicts and Token Expansion reduces null-space conflicts (Table 14 / Appendix D). No human-interpretable explanations beyond subspace attribution are provided.",
            "multi_task_performance": "Across NYUD-v2, PASCAL-Context, and Taskonomy, DTME-MTL consistently improves multi-task metrics over shared baselines and outperforms many multi-task optimization methods on Taskonomy (△m +4.67% over ST baseline and better than listed optimization baselines in Table 3). Selective application to high-conflict layers (≈25–50% of layers) gives best trade-off; indiscriminate expansion harms performance.",
            "resource_constrained_results": "Authors emphasize minimal parameter overhead (fractions of a percent) and give wall-clock timings for SVD/conflict computation; claim relative computational cost is small compared to day(s)-scale MTL training (SVD+conflict ≈ 1 hour for ViT-L reported). No explicit FLOPs-memory tradeoff curve under tight budgets.",
            "key_finding_summary": "Adapting token representations (modulation in pretrained span; expansion for out-of-span directions) yields consistent multi-task gains with minimal parameter/time overhead and less overfitting than naive parameter-branching; resolving conflicts in token space is an effective, efficient alternative to converting shared parameters into task-specific branches.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results and analyses support the idea that task-aligned, adaptive representations (here token-space modulation/expansion) outperform uniform shared representations and naive parameter duplication, improving multi-task performance and reducing overfitting.",
            "uuid": "e2235.0"
        },
        {
            "name_short": "Recon",
            "name_full": "Recon (Reducing conflicting gradients from the root for multi-task learning)",
            "brief_description": "A dynamic network architecture that identifies layers with high gradient conflict and converts those shared parameters into task-specific parameters (i.e., splits/branches layers) to eliminate conflicts by parameter duplication.",
            "citation_title": "Reducing conflicting gradients from the root for multi-task learning",
            "mention_or_use": "use",
            "model_name": "Recon",
            "model_description": "Detects conflicting gradients per-layer and dynamically converts the most-conflicted shared layers into task-specific branches (duplicating parameters) to avoid gradient interference.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "layer-wise parameter branching (duplicating shared parameters into task-specific copies)",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task learning (prior work and compared on vision MTL benchmarks in this paper, e.g., NYUD-v2)",
            "performance_task_aligned": "In experiments reported in this paper (NYUD-v2, ViT backbone), Recon performed worse than DTME-MTL and even worse than the joint baseline in reported metrics: Recon Semseg 31.92 (vs DTME-MTL 38.27), Recon depth RMSE 0.693 (vs DTME-MTL 0.637), Recon normal mErr 23.35 (vs DTME-MTL 21.64) as shown in Table 7.",
            "performance_uniform_baseline": "Recon was compared to a joint shared-parameter baseline and was observed to reduce parameter-space conflicts but lead to overfitting and degraded multi-task performance relative to DTME-MTL and sometimes to the joint baseline.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Paper reports Recon increased parameter count substantially in the compared setting (Table 7 shows #Param ↑ = 3.34% for Recon on NYUD-v2), with corresponding computational/overfitting costs; authors state parameter inefficiency and excessive computational overhead make Recon less scalable for large transformers.",
            "computational_efficiency_baseline": "Joint baseline has lower parameter count; Recon increases parameters by multiple percent (3.34% in Table 7) versus DTME-MTL's ~0.2–0.3% in related experiments.",
            "sample_efficiency_results": "Authors report that converting parameters to task-specific copies (Recon) tends to overfit in transfer/fine-tuning regimes; no explicit numeric sample-efficiency curves provided.",
            "transfer_generalization_results": "Recon's aggressive parameter branching was reported to cause overfitting and poorer generalization in transformer-based MTL finetuning (empirically worse metrics on NYUD-v2 with ViT backbone).",
            "interpretability_results": null,
            "multi_task_performance": "In the paper's experiments Recon reduced parameter-space gradient conflicts but led to worse multi-task performance and more overfitting compared to DTME-MTL and the shared baseline on NYUD-v2.",
            "resource_constrained_results": "Higher parameter growth (3.34% reported) and higher compute relative to DTME-MTL; authors call it parameter-inefficient and less scalable.",
            "key_finding_summary": "Naively converting shared transformer parameters into task-specific branches (Recon-style) can reduce parameter-space conflicts but is parameter-inefficient and prone to overfitting in pretrained transformer fine-tuning, yielding worse multi-task performance than token-space adaptive methods.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "While Recon embodies task-aligned specialization (task-specific branches), its empirical failure in this setting challenges the simplistic notion that more task-specific parameters always improve multi-task results — careful, space-aware adaptation (token-space) can be superior.",
            "uuid": "e2235.1"
        },
        {
            "name_short": "Mixture-of-Experts (MoE)",
            "name_full": "Mixture of Experts (MoE)",
            "brief_description": "Architectures that partition model capacity into multiple expert modules and route inputs to subsets of experts, enabling adaptive specialization across tasks or inputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Mixture-of-Experts (MoE)",
            "model_description": "Predefined groups of expert parameters (layers or submodules) with a gating mechanism that adaptively selects or weighs experts per input or task, enabling selective allocation of capacity.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "mixture-of-experts gating (expert routing) enabling adaptive per-task/exemplar specialization",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task learning and large-scale models (discussed as prior work in vision and NLP contexts)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": "MoE approaches can be compute-efficient in inference if only a subset of experts is activated, but the paper only references them as prior work and does not provide new measurements.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Mentioned as an existing mechanism to separate shared and task-specific components; not evaluated directly in this paper.",
            "resource_constrained_results": null,
            "key_finding_summary": "MoE-style adaptive capacity allocation is a relevant prior approach to task-aligned specialization; authors contrast MoE's predefined expert groups with their token-space adaptive approach which avoids duplicating large parameter sets.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "MoE represents a class of task-aligned approaches, but this paper neither empirically validates nor refutes MoE; it contrasts MoE-style parameter-level specialization with token-space methods.",
            "uuid": "e2235.2"
        },
        {
            "name_short": "TaskPrompter",
            "name_full": "TaskPrompter (spatial-channel multi-task prompting for dense scene understanding)",
            "brief_description": "A transformer-based multi-task architecture that uses task-specific prompt tokens to encapsulate task-specific information for dense prediction tasks.",
            "citation_title": "Taskprompter: Spatial-channel multi-task prompting for dense scene understanding",
            "mention_or_use": "use",
            "model_name": "TaskPrompter",
            "model_description": "Adds task-specific prompt tokens (task prompters) to a shared transformer to inject task-specific biases and interactions for dense prediction tasks.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "task-specific prompt tokens concatenated to shared tokens (prompting mechanism)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "vision multi-task dense prediction (PASCAL-Context, NYUD-v2 in experiments)",
            "performance_task_aligned": "Base TaskPrompter results reported in Tables 4–5 (e.g., NYUD-v2 TaskPrompter baseline Semseg mIoU 55.30 → with DTME-MTL 56.36; PASCAL-Context baseline mIoU 80.89 → with DTME-MTL 81.01).",
            "performance_uniform_baseline": "Compared to a shared Baseline (MT) and to the TaskPrompter base; DTME-MTL integration improved TaskPrompter performance with negligible parameter increase.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "When DTME-MTL is integrated, the paper reports tiny parameter increases for TaskPrompter (+0.046%) and small runtime overhead (DTME-MTL inference overhead ~13.4% for ViT-B; specific incremental cost for TaskPrompter+DTME not separately listed).",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "TaskPrompter benefits from DTME-MTL integration (small mIoU gains reported); TaskPrompter itself is a task-aligned prompting architecture aimed at multi-task performance.",
            "resource_constrained_results": null,
            "key_finding_summary": "Prompting-based per-task tokens provide task-aligned capacity that can be further improved (with negligible extra params) by token-space adaptive methods (DTME-MTL).",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "TaskPrompter embodies a task-aligned representation approach; integrating DTME-MTL further improves performance, supporting the benefit of task-specific representations.",
            "uuid": "e2235.3"
        },
        {
            "name_short": "InvPT",
            "name_full": "InvPT (Inverted Pyramid Multi-Task Transformer variant used in MTL)",
            "brief_description": "A transformer-based multi-task architecture (InvPT) that can be equipped with task-specific tokens/prompters; used in the paper to demonstrate compatibility and gains from DTME-MTL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InvPT",
            "model_description": "A transformer-based multi-task architecture (inverted-pyramid style) that shares an encoder and uses task-specific decoding mechanisms; serves as a state-of-the-art baseline that DTME-MTL can augment.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "shared transformer encoder with task-specific decoding/prompting mechanisms (task tokens)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "vision multi-task dense prediction (NYUD-v2, PASCAL-Context in experiments)",
            "performance_task_aligned": "InvPT baseline (NYUD-v2) Semseg mIoU 53.56 → with DTME-MTL 54.38 (Table 4). On PASCAL-Context InvPT 79.03 → with DTME-MTL 81.91 (Table 5). Paper reports DTME-MTL integration adds ~0.048% parameters for InvPT.",
            "performance_uniform_baseline": "InvPT as reported is a task-aware architecture; DTME-MTL further improves it slightly.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "DTME-MTL integration yields tiny additional parameter overhead (~0.048% reported) and modest runtime overhead as reported for DTME generally; exact per-model FLOPs not provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "DTME-MTL consistently raises InvPT multi-task performance modestly with negligible parameter growth.",
            "resource_constrained_results": null,
            "key_finding_summary": "DTME-MTL can be plugged into existing task-aligned MTL architectures like InvPT to further improve multi-task performance with almost no parameter cost.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Integration gains indicate that task-aligned token mechanisms benefit from additional token-space adaptive operations.",
            "uuid": "e2235.4"
        },
        {
            "name_short": "LoRA",
            "name_full": "Low-Rank Adaptation (LoRA)",
            "brief_description": "A parameter-efficient adaptation method that injects low-rank weight updates for per-task or per-domain adaptation, producing compact task-specific modules.",
            "citation_title": "Low-rank adaptation of large language models",
            "mention_or_use": "mention",
            "model_name": "LoRA",
            "model_description": "Learns low-rank additive updates to pre-trained weight matrices to provide task-specific adaptation with few extra parameters.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "task-specific low-rank adapters attached to weight matrices (parameter-level low-rank factors)",
            "is_dynamic_or_adaptive": false,
            "task_domain": "general transfer/adaptation; discussed in context of MTL inference efficiency in the paper",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": "Paper notes that per-task LoRA modules result in disjoint sets of task-specific weights which (even when merged) require separate forward passes per task, causing inference to scale linearly with number of tasks; DTME-MTL contrasts by enabling single batched forward computation across tasks with only ~13.4% overhead per task.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Not evaluated in this work; authors discuss LoRA qualitatively and contrast its per-task inference scaling with DTME-MTL's joint inference capability.",
            "resource_constrained_results": null,
            "key_finding_summary": "While LoRA is parameter-efficient per-task, its per-task modules require separate forward passes, making it less efficient for joint multi-task inference than shared token-space approaches like DTME-MTL.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "LoRA demonstrates that compact task-specific parameterization can be effective, but the inference scaling issue highlights limitations of certain task-aligned parameterizations for efficient multi-task deployment.",
            "uuid": "e2235.5"
        },
        {
            "name_short": "Shared Transformer Baseline (MT / ST)",
            "name_full": "Shared Transformer Multi-Task Baseline (MT) and Single-Task Full Fine-tuning Baseline (ST)",
            "brief_description": "Baselines used in experiments: (MT) shared pretrained transformer encoder with small task-specific decoders; (ST) same architecture trained per-task (upper bound reference).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Shared Transformer Baseline (MT) / Single-Task Fine-tune (ST)",
            "model_description": "MT: one shared pretrained transformer encoder with lightweight decoders per task; ST: same architecture trained for a single task (used as upper bound).",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "uniform shared representations across tasks (shared encoder) with small task-specific decoders",
            "is_dynamic_or_adaptive": false,
            "task_domain": "vision multi-task dense prediction (NYUD-v2, PASCAL-Context, Taskonomy)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "ST reported as an upper bound (Table 3 ST row lists per-task metrics on Taskonomy); MT baseline reported throughout; DTME-MTL improves over MT and approaches ST in aggregate metrics for some settings.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Shared-MT is efficient for joint inference; ST requires separate models per task. DTME-MTL aims to retain joint efficiency while adding small extra params.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "MT suffers from negative transfer; ST is an upper bound often better per-task but inefficient; DTME-MTL closes gap between MT and ST.",
            "resource_constrained_results": null,
            "key_finding_summary": "Uniform shared representations (MT) can suffer negative transfer; small token-space adaptations can recover much of the gap to single-task fine-tuning (ST) without per-task models.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Empirical results show that having small task-aligned adaptations on top of a shared backbone improves multi-task outcomes relative to purely uniform representations.",
            "uuid": "e2235.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reducing conflicting gradients from the root for multi-task learning",
            "rating": 2
        },
        {
            "paper_title": "Taskprompter: Spatial-channel multi-task prompting for dense scene understanding",
            "rating": 2
        },
        {
            "paper_title": "Low-rank adaptation of large language models",
            "rating": 2
        },
        {
            "paper_title": "Mixture-of-experts vision transformer for efficient multitask learning with model-accelerator co-design",
            "rating": 2
        },
        {
            "paper_title": "Scaling vision with sparse mixture of experts",
            "rating": 1
        }
    ],
    "cost": 0.019709499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning
10 Jul 2025</p>
<p>Wooseong Jeong 
Kuk-Jin Yoon kjyoon@kaist.ac.kr 
Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning
10 Jul 20256AD8763B60703611A9B830D3F3C15207arXiv:2507.07485v1[cs.LG]
Multi-Task Learning (MTL) enables multiple tasks to be learned within a shared network, but differences in objectives across tasks can cause negative transfer, where the learning of one task degrades another task's performance.While pre-trained transformers significantly improve MTL performance, their fixed network capacity and rigid structure limit adaptability.Previous dynamic network architectures attempt to address this but are inefficient as they directly convert shared parameters into task-specific ones.We propose Dynamic Token Modulation and Expansion (DTME-MTL), a framework applicable to any transformerbased MTL architecture.DTME-MTL enhances adaptability and reduces overfitting by identifying gradient conflicts in token space and applying adaptive solutions based on conflict type.Unlike prior methods that mitigate negative transfer by duplicating network parameters, DTME-MTL operates entirely in token space, enabling efficient adaptation without excessive parameter growth.Extensive experiments demonstrate that DTME-MTL consistently improves multi-task performance with minimal computational overhead, offering a scalable and effective solution for enhancing transformer-based MTL models.</p>
<p>Introduction</p>
<p>Multi-Task Learning (MTL) enables multiple tasks to be learned simultaneously within a shared network, improving generalization, efficiency, and convergence speed compared to training separate models [6].However, conflicting objectives among tasks can lead to negative transfer, where learning one task degrades the performance of another [10].The key challenge lies in designing architectures that effectively balance shared and task-specific representations to mitigate negative transfer.</p>
<p>Transformer-based MTL architectures [62,63,66] lever-age the strong generalization capabilities of large-scale pretrained networks such as Vision Transformers (ViTs) [14].By utilizing pre-trained transformers trained on large opensource datasets, these architectures demonstrate improved generalization compared to conventional CNN-based MTL methods [11,16,40,52,56,60,70,71]. However, they typically rely on predefined modules, such as Task Prompter mechanisms [62,63,66] and Mixture of Experts (MoE) [9,18,43,47,69], to separate shared and task-specific components.These rigid structures struggle to adapt to dynamic task relationships, leading to inefficient information sharing and suboptimal performance.The degree of task specialization required varies across different network depths [15]: high-level tasks such as semantic segmentation demand greater capacity in deeper layers, while low-level tasks like surface normal estimation rely more on shallower layers.Ideally, MTL architectures should dynamically adjust the allocation of shared and task-specific representations to accommodate these variations.However, existing transformer-based MTL frameworks are inherently constrained by their fixed network capacity, limiting their ability to adapt to evolving task dependencies and effectively mitigate negative transfer.</p>
<p>A straightforward approach to addressing these limitations is to increase the size of the transformer backbone.While this expands network capacity, it has a major drawback: it prevents the use of open-source pre-trained networks, which provide strong initialization and generalization capabilities across multiple tasks.Training a larger network from scratch requires massive computational resources and large-scale datasets, making this approach impractical for many applications.Instead, an effective MTL framework should refine existing architectures to retain the advantages of pre-trained transformers while improving adaptability to task-specific needs.</p>
<p>To achieve this, efficient adaptation methods for pretrained transformer-based MTL architectures are needed.Unlike approaches that build MTL frameworks from scratch or rely on task optimization within a fixed network capacity, we focus on adapting and enhancing predefined architec-tures while preserving their core design.This allows existing MTL models to be improved efficiently while dynamically adjusting task-specific representations.Despite its potential, the challenge of how to adaptively expand existing multi-task networks remains an underexplored problem.</p>
<p>One possible approach for adapting models during finetuning is the use of multi-task optimization techniques [13, 23, 32, 34-37, 44, 48, 49, 67], which mitigate negative transfer by adjusting task loss weights or modifying gradients.While these methods help balance task performance, they remain limited by a fixed network design and cannot expand model capacity.</p>
<p>A more direct approach involves dynamic network architectures, such as Recon [22], which directly expand network capacity to mitigate negative transfer.Recon measures conflicting gradients [67] in each layer-where gradients from different tasks point in opposing directions-and transforms the most conflicting layers into task-specific ones.While this increases flexibility by expanding the capacity of predefined architectures, directly converting shared parameters into task-specific ones in transformers leads to parameter inefficiency, excessive computational overhead, and a higher risk of overfitting.Consequently, its scalability to large transformer-based architectures is limited.</p>
<p>To address these challenges, we propose Dynamic Token Modulation and Expansion (DTME-MTL), a novel framework designed to improve pre-trained transformerbased MTL architectures.Unlike previous methods that directly manipulate network parameters, our approach mitigates negative transfer by restructuring the token space of multi-task networks.We treat transformer tokens as learnable parameters and analyze their structure using singular value decomposition (SVD) to identify gradient conflicts in token space.These conflicts are categorized into two types: range space conflicts, which are addressed through modulation via affine transformation of existing tokens, and null space conflicts, which are resolved by introducing new taskspecific tokens through expansion.</p>
<p>In our experiments, we demonstrate that DTME-MTL effectively enhances multi-task performance with minimal parameter overhead.Additionally, our results highlight that resolving task conflicts in the token space improves adaptability while mitigating overfitting.</p>
<p>Our main contributions are summarized as follows: • We propose DTME-MTL which dynamically modulates and expands token spaces to mitigate negative transfer in transformer-based multi-task architectures.• We introduce a structured approach to resolving gradient conflicts in token space by categorizing them into range and null space conflicts, demonstrating how this improves multi-task performance.• DTME-MTL is an off-the-shelf solution that seamlessly integrates with existing state-of-the-art transformer-based MTL architectures, enhancing performance with minimal computational overhead.</p>
<p>Related Works</p>
<p>Multi-Task Learning in Vision Transformers.Originally designed for NLP tasks, transformers have outperformed existing CNN models in various computer vision tasks.Attempts have been made to incorporate Vision Transformer [14,38,[57][58][59]64] in MTL.MTFormer [61] employs a shared transformer encoder and decoder with a cross-task attention mechanism.MulT [1] utilizes a shared attention mechanism to model task dependencies based on the Swin transformer.InvPT [65] focuses on global spatial position and multi-task context for dense prediction tasks through multi-scale feature aggregation.Mixture of Experts (MoE), inspired by the NLP domain, divides the model into predefined expert groups, adaptively shared or devoted to specific tasks during the learning phase [9,18,27,43,47,69].</p>
<p>Task prompter [62,63,66] uses task-specific tokens to encapsulate task-specific information and employs cross-task interactions to enhance multi-task performance.</p>
<p>Dynamic Network Architectures for MTL.Dynamic networks adapt their structure during training to improve efficiency and task specialization.Several methods have explored dynamic architectures for MTL.Channel-wise dynamic allocation [2] assigns different convolutional channels to different tasks, but this method is not directly applicable to transformer-based architectures.Neural Architecture Search (NAS) for MTL [3,5,21,24,33,46,51] explores optimal network configurations but is computationally expensive and incompatible with large pre-trained backbone models such as ViTs [14].Recon [22] transforms shared parameters directly into task-specific ones to handle conflicting gradients.Unlike most dynamic network architectures, our approach focuses on a dynamic system that can be directly applied to transformer-based multi-task architectures, leveraging pre-trained backbones while maintaining a reasonable computational cost.</p>
<p>Multi-Task Optimization.Optimizing the MTL aims to address negative transfer by adjusting the relative weighting of task losses or directly manipulating gradients.Taskdependent uncertainty [32] is utilized to weigh the loss of multiple tasks.Liu et al. [37] considers the rate of loss descent, while [23] prioritizes tasks based on difficulty.Recently, Liu et al. [35] proposed updating task weights based on the loss history.In contrast, approaches like [13,34,36,44,48,49,67] directly modify task gradients to achieve the desired balance.PCGrad [67] analyzes negative transfer by identifying conflicting gradients in the shared parameters of the network.Jiang et al. [30] suggests a positive link between negative transfer and conflicting gradients in auxiliary task learning.However, the conventional view in MTL considers conflicting gradients a key factor contributing to negative transfer in joint multi-task learning optimization [13,29,34,36,44,48,49,67], where tasks are learned together rather than serving as auxiliary tasks.Therefore, we adopt a similar perspective.Normalized gradients are employed to prevent spillover between tasks [7], whereas Chen et al. [8] introduce stochasticity to the network's parameters based on the consistency in the sign of gradients.RotoGrad [28] rotates the feature space of the network to narrow the gap between tasks.</p>
<p>Preliminaries</p>
<p>In multi-task learning, the network learns a set of tasks {τ i } K i=1 jointly, where K is the number of tasks.Each task τ i has its own loss function L i .The network parameter Θ can be classified into Θ = {Θ s , Θ 1 , ..., Θ K } where Θ s is shared parameter across all tasks and Θ i is task-specific parameters devoted to task τ i .Then, the objective function of multi-task learning is to minimize the weighted sum of all tasks' losses: Θ * = arg min Θ K i=1 w i L i (Θ s , Θ i ) where w i represents the scale of the task-specific loss L i .A phenomenon called conflicting gradients [67], where the gradients of each objective point in different directions, has been identified as a main cause of negative transfer.</p>
<p>Definition 1 (Conflicting gradients).Define g i as the gradient of task τ i with respect to the shared parameters Θ s as
g i = ∇ Θs L i (Θ s , Θ i ).
Let g i and g j represent the gradients for a pair of tasks τ i and τ j where i ̸ = j.If g i • g j ≤ 0, these two gradients are termed conflicting gradients.</p>
<p>However, the role of conflicting gradients remains a topic of debate.While conventional MTL optimization studies [13,29,34,36,44,48,49,67] consider conflicting gradients as a main cause of negative transfer, Jiang et al. [30] argue that they can serve as a form of regularization that improves generalization when present in network parameters.Our findings align with Jiang et al. [30] in that directly resolving conflicting gradients by converting shared parameters into task-specific ones [22] leads to overfitting when applied to transformers.In contrast, we propose a tokenbased network expansion approach that categorizes gradient conflicts within token space and adapts accordingly, mitigating negative transfer while maintaining generalization.</p>
<p>Method</p>
<p>In order to mitigate negative transfer by ensuring sufficient space for tasks, we adopt token-based network expansion.Initially, we define the token space as the output of each layer in the transformer block through singular value decomposition (SVD).Subsequently, we categorize conflicts in task-specific gradients into two types: conflicts in the range space of tokens and conflicts in the null space of Figure 2. The process approximates the range and null spaces of T d s based on the proportion of total variance, r.These eigenvalues are arranged in descending order, satisfying λi ≥ λj if i &lt; j.If r is greater than the sum up to λm and smaller than the sum up to λm+1, then we select the set {λi} m i=1 as ΛR, and the remaining set {λi} p i=m+1 as ΛN .</p>
<p>tokens.Finally, based on the type of conflict, we introduce efficient token modulation and expansion techniques for transformer-based multi-task architectures.</p>
<p>Defining Token Space using SVD</p>
<p>In this section, we create a vector space consisting of shared tokens in a transformer, aiming to classify the types of conflicting gradients.More specifically, we approximate the range space and null space of the uncentered covariance of the tokens before applying our methods.Let's consider a dataset {X l , Y l } n l=1 , where X l represents the input, Y l denotes the label, and n is the number of samples.Denote input shared token for a layer d as
T l,d s = [T l,d s,1 , T l,d s,2 , ..., T l,d s,N ]
where N is the total number of shared tokens in that layer.Every token T l,d s,k ∈ R p represents the output of the transformer layer d − 1 for the corresponding input data X l , where p is the hidden dimension of the token embedding.Let's consider a total of D transformer layers.Next, the uncentered covariance of the token in layer d (where 1 ≤ d ≤ D) is as follows:
T d s = 1 n n l=1 (T l,d s )(T l,d s ) T(1)
T d s is a square matrix of dimensions p × p.To define the token space, we apply SVD to T d s .Following this, we can divide vector space formed by T d s into its range space R( T d s ) and null space N ( T d s ) depending on the magnitude of eigenvalue Λ.The process is illustrated below:
T d s = UΛV T , Λ = Λ R 0 0 Λ N (2)
where Λ is a diagonal matrix.Each Λ R and Λ N represent submatrices of Λ containing the eigenvalues of the range space and null space, respectively.Both U and V are square matrices, each with dimensions p × p. From Eq. ( 2), we obtain a mathematical tool to define the range and null space of the covariance of the token, T d s .To approximate the range space, we choose the eigenvalue Λ R along with their corresponding eigenvectors from U R .</p>
<p>On the other hand, when approximating the null space, we should select the eigenvalues Λ N and their corresponding eigenvectors from U N .Ideally, we should choose eigenvalues that are exactly zero to form the null space.However, in practice, Λ can not be precisely zero.Therefore, it's essential to establish a criterion for selecting the eigenvalue to distinguish between these two spaces.</p>
<p>Instead of introducing a new manually designed rule for approximating each range and null space of T d s , we opt to directly employ the evaluation tool for the SVD [31] as criteria for determining the range and null space of tokens.In assessing the accuracy of the SVD approximation, the proportion of total variance, denoted as r, has been employed:
r = λ∈diag(Λ N ) λ λ∈diag(Λ R ) λ(3)
The diag function serves as an operator, returning a set containing the diagonal entries of the input matrix.In our approach, we employ Eq. ( 3) to directly divide the range and null space of T d s .As depicted in Fig. 2, the diagonal elements of the matrix Λ, obtained through the SVD of T d s , are arranged in descending order based on their magnitudes.We can select the index of the eigenvalue m such that the sum of eigenvalues up to order m is smaller than r, and the sum up to m + 1 is larger than r.This index serves as a boundary to divide the range space and null space of T d s .</p>
<p>Types of Gradient Conflicts</p>
<p>In Section 4.1, we create a p-dimensional vector space using the uncentered covariance of the shared token T d s , linked to the input data set {X } n l=1 .This vector space is divided into the range and null space, with each space spanned by eigenvectors corresponding to singular values selected based on a specified ratio r.In the upcoming sections, we pinpoint the types of gradient conflict within the vector space we've constructed.We then address these conflicts adaptively by introducing token modulation and expansion techniques.</p>
<p>Using Eq. ( 2) and Eq. ( 3), we can partition the eigenvectors of the p-dimensional vector space into its range and null space.Now, let's consider the shared tokens T l,d s = [T l,d s,1 , . . ., T l,d s,N ], omitting the explicit notation of l, d for simplicity.For example, we write T l,d s → T s , T l,d s,k → T s,k , and T d s → Ts .We treat T s as network parameters, for which gradients can be computed during the backpropagation process.Then, for each loss L i , the task-specific gradient for T s,k is denoted as g i = ∇ T s,k L i .Consequently, we obtain task-specific gradients {g i } K i=1 corresponding to a set of losses {L i } K i=1 for T s as shown in Fig. 1-(a).Each task-specific gradient g i can be decomposed into two components, g R,i and g N ,i , through projection onto the range and null space of T d s , respectively.This breakdown is expressed as follows:
g R,i = (U R U T R )∇ T s,k L i g N ,i = (U N U T N )∇ T s,k L i (4)
U R and U N are orthogonal matrices that consist of eigenvectors of the range space and null space, respectively, with each column representing one eigenvector.Then, the matrices (U R U T R ) and (U N U T N ) function as projection operators onto the range and null spaces, respectively.</p>
<p>Building upon the concept of conflicting gradients outlined in Definition 1, we classify conflicts into two types based on the space in which they occur: range space conflicts and null space conflicts.Specifically, conflicts in the range space of tokens occur when g R,i • g R,j ≤ 0 for any pair of i and j where i ̸ = j.Likewise, conflicts in the null space of tokens emerge when g N ,i • g N ,j ≤ 0.</p>
<p>Token Modulation and Expansion</p>
<p>Building on the gradient conflict types defined in Sec.4.2, we propose adaptive strategies to mitigate task interference.Specifically, if gradient conflicts occur in the range space, we apply an affine transformation to modulate tokens, while conflicts in the null space are addressed by introducing additional tokens to expand the feature space.This distinction is particularly relevant in the transfer learning setting, where a pretrained transformer backbone is used, and task interference must be handled during fine-tuning.According to [45], training from pretrained weights constrains the model within the same basin of the loss landscape, preserving a feature space similar to that of the pretrained network.This insight guides our separation of token space into range and null spaces: conflicts in the row space indicate that the network already has relevant interpretative capabilities and can be resolved through rotation or scaling, whereas conflicts in the null space suggest the need for additional features, requiring token expansion to enhance the model's capacity.Token Modulation.In situations where task-specific gradients conflict within the range space of T s , such as g R,i • g R,j ≤ 0, modulators M i and M j are added after the shared token T s as shown in Fig. 1-(b).The token modulator M is a straightforward affine transformation that modulates the shared token T s along the channel dimension.To elaborate, considering the embedding dimension of the Calculate uncentered covariance.
Algorithm 1: DTME-MTL Data: Task {τ i } K i=1 , Loss function {L i } K i=1 , Dataset {X l , Y l } n l=1 , Shared tokens T l,d s = {T l,d s,i } N i=1 ,T d s = 1 n n l=1 (T l,d s )(T l,d s ) T 4 Singular value decomposition. U, Λ, V = SV D( T d s )
5</p>
<p>Divide range and null space.
U = [U R , U N ] 6 Projection to range space. {g R,i } K i=1 = {(U R U T R )∇ T l,d s,k L i } K i=1 7 Projection to null space. {g N ,i } K i=1 = {(U N U T N )∇ T l,d s,k L i } K i=1 8 if g R,i • g R,j ≤ 0 then 9 Insert token modulators M i and M j 10 if g N ,i • g N ,j ≤ 0 then 11
Insert task-specific tokens T i and T j transformer p and the number of shared tokens is N , we can arrange T s in the form [T s,1 , . . ., T s,N ].This arrangement turns T s into a p × N matrix.The modulator M, which incorporates weight and bias W, b ∈ R p , performs the transformation W ⊙ T s,i + b, where ⊙ denotes elementwise multiplication.When the gradient lies in the row space of Ts , Proposition 1 demonstrates that applying token modulation can effectively resolve gradient conflicts, lowering the multi-task loss.</p>
<p>Proposition 1.When the input token T in for input sample X l spans the range space of Ts , optimizing the token modulators {M i } K i=1 reduces gradient conflicts in the row space of Ts and leads to a reduction in the multi-task loss.</p>
<p>Token Expansion.Similarly, in cases where task-specific gradients conflict within the null space of T s , such as g N ,i • g N ,j ≤ 0, task-specific tokens T i and T j are added alongside shared tokens T s as shown in Fig. 1-(c).The task-specific tokens {T i } K i=1 are concatenated with shared tokens before entering the transformer block.Consequently, each task-specific token acquires task-specific information within that layer.Specifically, in a standard transformer block, self-attention is performed for each pair of tokens in the form of [T s,1 , . . ., T s,N ] × [T s,1 , . . ., T s,N ].With token expansion, attention is extended to include [T s,1 , . . ., T s,N ] × [T 1 , . . ., T K ] on the output.Proposition 2 explains how expanding the token space to address gradi-Table 1.We conduct an ablation study on dynamic token modulation and expansion, evaluating the multi-task performance of our method on NYUD-v2 and PASCAL-Context.The results of TE, TM, and their combination, TE+TM are presented.We employ a shared encoder and multiple decoders, using ViT-T as the backbone network.ent conflicts in the null space of Ts leads to a reduction in multi-task loss when the gradient lies in this null space.All proofs are provided in Supple E.</p>
<p>Proposition 2. When the input token T in for input sample X l spans the null space of Ts , token expansion using {T i } K i=1 alleviates the increase in multi-task loss caused by gradient conflicts in the null space of Ts .</p>
<p>The complete procedure for DTME-MTL is outlined in Alg.1.Handling gradient conflicts in token space improves adaptability and reduces overfitting, which we demonstrate in experiments.By adjusting token representations instead of modifying network parameters, our approach enables more efficient and flexible multi-task fine-tuning.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Datasets and Evaluation.Our method is evaluated on multi-task datasets: NYUD-v2 [50], PASCAL-Context [42] and Taskonomy [68].Each of them with 4, 5, 11 tasks.To evaluate the performance of tasks, we employed widely used metrics.To evaluate the multi-task performance, we utilize the metric proposed by Maninis et al. [41].It measures the per-task performance M m,i by averaging it with respect to the single-task baseline M b,i , as shown in To show how effectively our approach reduces negative transfer, we also compare it with previous multi-task optimization, though our methods can be used alongside them.We include simple gradient descent (GD), GradDrop [8], MGDA [48], PCGrad [67], CAGrad [34], IMTL [36], Nash-MTL [44], and Aligned-MTL [49], as well as loss balancing methods such as UW [32], DWA [37], and FAMO [35].We also compare our results with dynamic network architecture such as Recon [22].Further experimental details are summarized in Supple B.
△ m = (1/K) K i=1 (−1) li (M m,i − M b,i )/M b,i . l i = 1 if</p>
<p>Experimental Results</p>
<p>Effectiveness of Token Modulation and Expansion.We assess the effectiveness of the proposed methods on the NYUD-v2 and PASCAL-Context datasets, with results detailed in Tab. 1.In the last three rows of the table, we depict the performance gains compared to the two baselines and  the increased number of parameters in "#P aram ↑ (%)".Compared to the Baseline (MT), our methods demonstrate significant performance improvements across all tasks in both datasets.Particularly noteworthy is the substantial increase in multi-task performance achieved with just a 0.2% to 0.3% increase in the total network parameters.Additionally, our approach exhibits comparable performance to Baseline (ST) in a multi-task scenario.This implies that reducing negative transfer between tasks can be effectively accomplished by merely integrating introduced token modulators and task-specific tokens, without the need for intricately designed modules.</p>
<p>Analysis of the Timing of Network Expansion.In Fig. 3, we analyze the performance of each task according to the timing of network expansion using the proposed DTME-MTL.Specifically, the timing for expansion refers to the point at which token modulation and expansion are performed based on calculations of the token space using Singular Value Decomposition and measurement of gradient conflicts.The figure illustrates the performance results when network expansion is conducted at the beginning of training (0 th ) and after each quarter of the entire training process (i th 25% Iter).To ensure fair comparisons, we trained the network using the same number of iterations after the expansion.The results indicate that the optimal timing for expansion may vary across tasks.However, overall, early-stage expansion during network training tends to yield better performance.This aligns with the fact that our approach builds on pre-trained backbone networks.</p>
<p>Analysis of Gradient Conflicts in Parameters.We focus on resolving gradient conflicts in token space.While our primary method operates in token-level representations, we also monitor gradient conflicts in parameter space to better understand training dynamics.In Fig. 4, we visualize the distribution of angles between task-specific gradients of network parameters, categorizing them into different ranges and tracking their frequency over the course of training.When applied to the baseline model, both Token Modulation (TM) and Token Expansion (TE) reduce gradient conflicts in parameter space to some extent.However, as shown in Tab. 7, where our methods are compared with Recon [22], we observe important differences.Recon explicitly suppresses conflicts by modifying network parameters whenever the cosine similarity between task gradients becomes negative.Although this reduces gradient conflicts in parameter space, it often results in severe overfitting and degraded multi-task performance.These findings suggest that conflicts in parameter space are not always reliable indicators of negative transfer.Instead, resolving conflicts in token space offers an alternative strategy that avoids overfitting while still mitigating interference.Additional analysis of token-level conflicts is provided in Appendix D.</p>
<p>Computational Cost of DTME-MTL.In Tab. 6, we report the time consumption for each process of DTME-MTL on PASCAL-Context using a single NVIDIA RTX A6000.We measure the time required for calculating the token space with SVD and for computing gradient conflicts in the token space of the transformer.The time required for each process increases with the size of the transformer.However, the proposed methods are computationally efficient, requiring approximately 1 hour with ViT-L.Considering that typical multi-task architectures require at least one day of training on a single GPU, the computational cost of DTME-MTL is relatively low.Proposed DTME-MTL increases inference time of each task about 13.4% with ViT-B.Comparing Performance based on Layer Selection Criteria.In Tab. 2, we applied token modulation and expansion (TM+TE) to layers with the highest gradient conflicts between tasks.Results are also shown for randomly chosen layers (Random) or layers with the lowest gradient conflicts (Reverse).The network expansion system, using conflict detection, outperforms random selection across all tasks.Particularly, applying TM+TE to layers with severe conflict levels consistently outperforms its application in layers with lower conflict levels, validating the effectiveness of the proposed expansion strategy.</p>
<p>Comparison with Multi-Task Optimization.In Tab. 3, we compare DTME-MTL with previous multi-task optimization approaches to demonstrate its effectiveness in reducing negative transfer between tasks on the Taskonomy benchmark using ViT-B.DTME-MTL achieves the best multitask performance, improving each task by an average of 4.67% with only a 0.118% increase in the number of parameters.Although DTME-MTL introduces additional parameters to address negative transfer, making direct comparisons with optimization methods less straightforward, it consistently improves multi-task performance.However, using more task-specific parameters does not always lead to better performance.As as shown in Tab. 7, Recon shows poor results with the vision transformer on NYUD-v2.This comparison highlights that previous multi-task expansion approaches, which naively duplicate network branches, are not only parameter-inefficient but also prone to overfitting due to the increased complexity of transformers.</p>
<p>Adapting to Multi-Task Architectures.In Tabs. 4 and 5, we compare DTME-MTL with leading multi-task architectures on the NYUD-v2 and PASCAL-Context datasets.We evaluate its multi-task performance against transformerbased approaches.Our method is compatible with any transformer-based multi-task architecture, enabling us to assess its effectiveness by integrating it into two leading models: InvPT and TaskPrompter.DTME-MTL seamlessly enhances these architectures, significantly boosting performance with only a minimal increase in parameters -just 0.048% for InvPT and 0.046% for TaskPrompter.</p>
<p>Conclusion</p>
<p>We introduced Dynamic Token Modulation and Expansion for Multi-Task Learning, an efficient approach for improving transformer-based MTL architectures.By categorizing gradient conflicts into range space and null space, it adaptively applies token modulation and expansion to mitigate negative transfer and reduce overfitting.DTME-MTL seamlessly integrates with existing transformer-based MTL frameworks, requiring minimal additional parameters.By refining encoded token space instead of modifying network parameters, it provides a lightweight and scalable solution for enhancing multi-task performance.</p>
<p>Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning</p>
<p>Supplementary Material</p>
<p>A. Additional Related Works</p>
<p>Multi-Task Architectures.Various multi-task architectures can be categorized based on how the parameters or features of the sharing network are distributed among tasks.</p>
<p>The widely used shared trunk structure comprises a common encoder shared by multiple tasks and a dedicated decoder for each task [11,40,52,70].A tree-like architecture, with multiple division points for each task group, offers a more generalized structure [4,25,39,55].The cross-talk architecture employs separate symmetrical networks for each task, utilizing feature exchange between layers at the same depth for information sharing between tasks [20,60].The prediction distillation model [16,56,60,71] incorporates cross-task interactions at the end of the shared encoder, while the task switching network [19,41,53,54] changes network parameters depending on the task.</p>
<p>B. Experimental Settings B.1. Datasets</p>
<p>We evaluate our method on three benchmarks: NYUD-v2, PASCAL-Context, and Taskonomy.NYUD-v2 contains 4 vision tasks: Our evaluation is based on depth estimation, semantic segmentation, surface normal prediction, and edge detection.PASCAL-Context contains 5 tasks: We evaluate semantic segmentation, human parts estimation, saliency estimation, surface normal prediction, and edge detection.We used 11 tasks for Taskonomy: We evaluate Depth Euclidean (DE), Depth Zbuffer (DZ), Edge Texture (ET), Keypoints 2D (Key2D), Keypoints 3D (Key3D), Normal (N), Principal Curvature (PC), Reshading (R), Segment Unsup 2d (S2D), and Segment Unsup 2.5D (S25D).</p>
<p>B.2. Implementation Details</p>
<p>For experiments, we adopt ViT [14] pre-trained on ImageNet-22K [12] as the transformer encoder.The models are trained for 60,000 iterations on both NYUD [50] and PASCAL [17] datasets with batch size 6.We use Adam optimizer with learning rate 2×10 −5 and 1×10 −6 of a weight decay with a polynomial learning rate schedule.Following the previous works [65,66], the cross-entropy loss is used for semantic segmentation, human parts estimation, and saliency, edge detection.Surface normal prediction and depth estimation use L1 loss.</p>
<p>B.3. Design and Implementation Strategy</p>
<p>To improve efficiency, we perform SVD only once early in training to estimate the feature space for conflict analysis.Gradient conflicts are measured in a pairwise manner across tasks, and the average number of conflicts in each space is used to guide token expansion.Based on this, we statically allocate a small number of task-specific tokens (six in our setup) as learnable parameters, independently applied at each layer.These tokens are fixed during training and do not adapt dynamically.For NYUD-v2 and PASCAL-Context, we use the full training sets to compute gradient statistics, while for Taskonomy, covariance is estimated using 100 randomly sampled mini-batches.The assignment of Token Modulation (TM) and Token Expansion (TE) is determined by a manually chosen activation ratio, which we analyze in Fig. 6.Rather than activating all components uniformly, TM and TE are selectively applied to layers with the highest conflict levels, either individually or jointly, based on their effectiveness in reducing task interference.</p>
<p>B.4. Evaluation</p>
<p>For semantic segmentation, we utilize mean Intersection over Union (mIoU).Surface normal prediction performance is measured by the mean angular distance between the predicted output and ground truth.Depth estimation is evaluated using Root Mean Squared Error (RMSE).For saliency estimation and human part segmentation, we employ mIoU.Edge detection is assessed using the optimal-dataset-scale F-measure (odsF).For Taskonomy, we adopt RMSE for principal curvature and L1 distance for the remaining tasks.</p>
<p>C. Additional Experiments</p>
<p>Comparison with Multi-Task Optimization.In Tabs.8 to 10, we further evaluate the proposed DTME-MTL against previous multi-task optimization approaches using different backbone sizes.Our method demonstrates significant improvements in multi-task performance with minimal increases in parameters.Specifically, DTME-MTL results in a parameter increase of 0.089% for ViT-L, 0.23% for ViT-S, and 0.46% for ViT-T.Analysis on the Modulator Configuration.In Tab.11, we show the performance difference based on the configuration of the token modulators.Specifically, we compared the outcomes obtained when employing affine transformation and batch normalization, which could be considered as the most common and straightforward approaches.Through experi-    ments, we find that affine transformations consistently exhibit better performance across all tasks compared to batch normalization layers used as modulators for both datasets.</p>
<p>Analyzing Performance Differences with Backbone Network Freezing.In Tab. 12, we examine the performance variation based on whether we freeze the existing backbone network components when training the expanded network after implementing the proposed dynamic token modulation and expansion.The results indicate that training networks without freezing the existing backbone network com-ponents leads to significantly better performance compared to training networks with freezing.We guess that allowing modifications to the learned token space after expansion helps the network to dynamically partition the token space for each task.</p>
<p>Influence of r on SVD Approximation.In Fig. 5, we illustrate how the proportion of total variance r impacts the approximation of a token's range and null space.We assess the performance of tasks across five values of r (1, 10, 100, 500, 1000).Our results suggest that the value of r has minimal impact on task performance, implying that there is less need for extensive tuning of the r parameter to optimize performance.In our other experiments, we chose r as 100 for training.</p>
<p>Impact of the Number of Layers Expanded by DTME-MTL.DTME-MTL expands a subset of transformer layers selected based on the severity of gradient conflicts.In Fig. 6, we analyze how varying the number of expanded layers affects task performance.The x-axis denotes the ratio of expanded layers to the total number of layers.We observe that applying TM+TE to approximately 25%-50% of the layers yields consistent performance gains across tasks while maintaining parameter efficiency.Performance improves as more high-conflict layers are expanded, but begins to degrade when expansion exceeds 50%, especially when low-conflict layers are included.This suggests that overexpansion can be detrimental.Table 2 further confirms that using a moderate expansion ratio (50%) avoids overfitting, whereas Fig. 6 highlights that indiscriminate expansion into less conflicting layers harms performance.These findings underscore the importance of both the extent and location of TM+TE application.Effect of Swapping Conflict Types.In Tab. 13, we present the results of an experiment on NYUD-v2 where we intentionally swap the conflict types targeted by each method.Specifically, Token Expansion (TE) is applied to layers with severe range space conflict, and Token Modulation (TM) is applied to layers with severe null space conflict-opposite to our standard configuration.This reversal leads to a clear performance drop, confirming that each method is most effective when applied to the type of conflict it is designed to resolve.These results support our design choice of assigning TM to range space conflict and TE to null space conflict.</p>
<p>D. Additional Analysis</p>
<p>Further Justification for Targeted TM/TE Assignment.</p>
<p>Prior work [45] suggests that fine-tuning from a pretrained model tends to remain in the same loss basin, preserving the structure of the pretrained feature space.Accordingly, we view the token space during fine-tuning as constrained by the span of the pretrained features.If the conflict lies within this span (i.e., the range space), it can be resolved by rotating the token space-achievable via a modulator, since affine transformations include rotation.However, if the conflict resides in the null space, it lies outside the span and cannot be sufficiently addressed by modulation alone.In such cases, expanding the token space with task-specific tokens helps relax this constraint.We theoretically support this view in Propositions 1 and 2 (with proofs in Appendix E), which analyze how each method addresses conflict in its respective subspace.This is further validated empirically: we measure the reduction in gradient conflicts by comparing the start and end of training in each space (Tab.14).The results show that Token Modulation (TM) is more effective in reducing conflicts in the range space, while Token Expansion (TE) is more effective in the null space.This consistency between theoretical analysis and empirical behavior supports our design choice to selectively apply TM and TE based on the dominant type of conflict in each layer.</p>
<p>Token-Level vs. Parameter-Level Conflict Handling.</p>
<p>Parameter-space conflicts reflect an aggregate gradient across all tokens, which makes it difficult to localize or disentangle the source of interference.In contrast, token-level conflicts can be measured for each individual token, allowing more localized and fine-grained analysis.This granularity enables our method to selectively modulate or expand tokens based on where the conflict occurs.Furthermore, by decomposing the token space into range and null components-depending on whether the pretrained model already spans those directions-we adaptively apply Token Modulation (TM) or Token Expansion (TE) to address conflicts.Such space-aware conflict resolution is fundamentally infeasible in parameter space, where task interference is entangled across layers and tokens.</p>
<p>Comparison with LoRA in Multi-Task Inference.As shown in Tab. 1, the baseline (ST) corresponds to full fine-tuning and serves as an upper bound on performance.</p>
<p>While LoRA [26] is a parameter-efficient method, assigning a separate LoRA module for each task leads to disjoint sets of task-specific weights.Even when merged into the base model, these configurations require separate forward passes per task, negating the efficiency benefits of multi-task learning (MTL).In contrast, our method maintains shared weights across tasks and allows all outputs to be computed jointly in a single batched tensor operation on GPU.This enables highly parallel inference with only a 13.4% overhead per task, whereas the inference time in LoRA scales linearly with the number of tasks.When the input token T in for input sample X l spans the range space of Ts , optimizing the token modulators {M i } K i=1 reduces gradient conflicts in the row space of Ts and leads to a reduction in the multi-task loss.Proof.Let the loss function L i be a function of the shared parameters Θ s , the token modulator M i , and the input data X l .Since transformers convert input data into tokens, we consider the loss to be a function of one of the input tokens, T in , rather than X l .To represent the updating step during optimization, we use the superscript t for current variables, such as Θ t s , and M t i , and t + 1 for the next-step variables, such as Θ t+1 s , and M t+1 i .In cases where the input token T in spans the row space of Ts , this can be expressed as follows:
U N U T N ∇ Tin L i (Θ t s , M t i , T in ) ≃ 0(5)
Since the row space and null space are perpendicular to each other, with their dimensions summing to the entire space, the following holds according to Eq. ( 5):
K i=1 ∇ Tin L i = K i=1 (U R U T R + U N U T N )∇ Tin L i ≃ K i=1 (U R U T R )∇ Tin L i(6)
Let the token modulator M i be a p × p matrix that manipulates the input token T in .
K i=1 ∇ Tin L i = K i=1 (U R M t i )(U R M t i ) T • ∇ M t i L i • ∇ Tin M t i(7)
The total multi-task loss can be represented using a Taylor expansion.Assuming η ≪ 1, we can ignore the second-order terms of η:
K i=1 L i (Θ t+1 s , M t+1 i , T s ) = K i=1 L i (Θ t s , M t i , T s ) + K i=1 ∇ Θ t s L i (Θ t s , M t i , T s )(Θ t+1 s − Θ t s )(8)+ K i=1 ∇ M t i L i (Θ t s , M t i , T s )(M t+1 i − M t i )(9)= K i=1 L i (Θ t s , M t i , T s ) − η| K i=1 ∇ Θs L i (Θ t s , M t i , T s )| 2(10)−η K i=1 |∇ M t i L i (Θ t s , M t i , T s )| 2(11)
By optimizing the modulator M t i so that |∇ M t i L i (Θ t s , M t i , T in )| approaches zero for each task i = 1, 2, . . ., K, we can alleviate gradient conflicts in the row space of Ts (as Eq. ( 7) also approaches zero) and reduce the overall multi-task loss, since Eq. ( 11) is always greater than or equal to zero.</p>
<p>E.2. Proof of Proposition 2</p>
<p>Proposition 2. When the input token T in for input sample X l spans the null space of Ts , token expansion using {T i } K i=1 alleviates the increase in multi-task loss caused by gradient conflicts in the null space of Ts .</p>
<p>Proof.Let the loss function L i be a function of the shared parameters Θ t s , the task-specific token T t i , and the input data X t .Similarly, since transformers convert input data into tokens, we consider the loss to be a function of one of the input tokens, T t in , rather than X t .To represent the updating step during optimization, we use the superscript t for current variables, such as Θ t s , T t in and M t i , and t + 1 for the next-step variables, such as Θ t+1 s , T t+1 in and M t+1 i .In the case where the input token T t in spans the null space of Ts , this can be expressed as follows:
K i=1 U R U T R ∇ T t in L i (Θ t s , T t in , T t i ) ≃ 0(12)
The derivative of the task-specific loss L i with respect to the expanded token, including the input token T t in and the learnable task-specific tokens T t i , is given as follows:
K i=1 ∇ [T t in ,T t i ] L i (13) = K i=1 U R 0 d×K 0 K×d U R,i U R 0 d×K 0 K×d U R,i T + U N 0 d×K 0 K×d 0 K×K U N 0 d×K 0 K×d 0 K×K T ∇ T t in L i ∇ T t i L i (14) = K i=1 U R U T R + U N U T N 0 d×K 0 K×d U R,i U T R,i ∇ T t in L i ∇ T t i L i(15)≃ K i=1 U N U T N 0 d×K 0 K×d U R,i U T R,i ∇ T t in L i ∇ T t i L i(16)= K i=1 (U N U T N )∇ T t in L i (U R,i U T R,i )∇ T t i L i(17)
The total multi-task loss can be expressed as follows:
L i (Θ t+1 s , T t+1 in , T t+1 i ) = L i (Θ t in , T t s , T t i ) + ∇ Θ t s L i (Θ t s , T t s , T t i )(Θ t+1 s − Θ t s )(18)+∇ T t in L i (Θ t s , T t s , T t i )(T t+1 in − T t in )(19)+∇ T t i L i (Θ t s , T t s , T t i )(T t+1 i − T t i )(20)= L i (Θ t s , T t in , T t i ) − η∇ Θ t s L i (Θ t s , T t s , T t i ) • K i=1 ∇ Θ t s L i (Θ t s , T t in , T t i )(21)
−η(U N U T N )∇ T t in L i (Θ t s , T t in , T t i )
• K i=1 (U N U T N )∇ T t in L i (Θ t s , T t in , T t i )(22)
−η(U R,i U T R,i )∇ T t i L i (Θ t s , T t in ,
T t i ) • (U R,i U T R,i )∇ T t i L i (Θ t s , T t in , T t i )(23)
The increase in multi-task loss caused by gradient conflicts in the null space (as described in Eq. ( 22)) cannot be reduced since the shared token T t in is not a learnable parameter.Instead, task-specific tokens T t i can be added to mitigate the increase in multi-task loss due to null space gradient conflicts by optimizing the learnable parameters {T i } K i=1 as described in Eq. ( 23).</p>
<p>Figure 1 .
1
Figure 1.Framework overview of the proposed DTME-MTL.(a) At each network layer, we compute the input token's range space R( T d s ) and their task-specific gradients, determining principal vectors from the uncentered covariance of Ts.(b) In cases where task-specific gradients conflict in the range space of T d s (e.g.gR,i • gR,j ≤ 0), modulation is applied to Ts by introducing Mi and Mj.(c) When task-specific gradients conflict within the null space of T d s (e.g.gN ,i • gN ,j ≤ 0), task-specific tokens Ti and Tj are added.</p>
<p>Depth of the Network D 1 for each layer of the network (d ← 1 to D) do 2 Get tokens {T l,d s } n l=1 for the layer d corresponding to input {X l } n l=1 3</p>
<p>Figure 3 .
3
Figure 3. Task performance varies based on when we expand the network.To determine the optimal timing, we assess expansions at the beginning of training and at the end of each quarter iteration, monitoring the corresponding changes in performance.</p>
<p>a lower value of the measure M i indicates better performance for task i, and 0 otherwise.Baselines and Model Variants.For a comprehensive analysis of the proposed DTME-MTL framework, we adopt a typical experimental setup for MTL in our experiments.In Tab. 1, 'Baseline (MT)' refers to a simple multi-task architecture consisting of a shared transformer backbone and basic task-specific decoders.Each decoder comprises one 3 × 3 Conv-BN-ReLU block.'Baseline (ST)' has the same structure as 'Baseline (MT)' but is trained with only a single task.We assess the proposed DTME-MTL framework by expanding the network from 'Baseline (MT)' and measure the performance gains achieved by the proposed methods.'TM' (Token Modulation) signifies the addition of the proposed token modulator to 'baseline (MT)', while 'TE' (Token Expansion) indicates the incorporation of task-specific tokens onto 'Baseline (MT)'.Finally, 'TM+TE' combines both proposed methods.</p>
<p>Figure 4 .
4
Figure 4. We evaluate the distribution of gradient conflicts by measuring the cosine similarity between task-specific gradients across all shared parameters throughout the optimization process.This is represented as cosϕij in (a) for NYUD-v2 and in (b) for PASCAL-Context.</p>
<p>(a) Results on NYUD-v2.(b) Results on PASCAL-Context.</p>
<p>Figure 5 .
5
Figure 5.We assess the performance of tasks based on the proportion of total variance r.The results are displayed for both (a) NYUD-v2 and (b) PASCAL-Context.</p>
<p>Figure 6 .
6
Figure 6.The performance of tasks based on the ratio of the number of expanded layers to the total number of layers.The results are displayed for both (a) NYUD-v2 and (b) PASCAL-Context.</p>
<p>Table 2 .
2
Performance comparison based on the degree of conflicts in reversed order (Reversed) and randomly selected layers (Random).
NYUD-v2PASCAL-ContextModelSemsegDepthNormalEdgeSemsegParsingSaliencyNormalEdgemIoU ↑RMSE ↓mErr ↓odsF ↑mIoU ↑mIoU ↑maxF ↑mErr ↓odsF ↑TM+TE38.270.637021.6457.9066.1856.2983.2115.2647.00TM+TE (Random)36.880.656722.2756.3062.1254.4382.9515.5545.80TM+TE (Reverse)34.710.689822.5955.8057.8452.8282.7515.7443.20(a) NYUD-v2(b) PASCAL-Context</p>
<p>Table 3 .
3
Comparison of multi-task optimization methods on Taskonomy across 11 tasks.Non-converged results are indicated with a dash.L1 Dist.↓ L1 Dist.↓ L1 Dist.↓ L1 Dist.↓ L1 Dist.↓ L1 Dist.RMSE ↓ L1 Dist.↓ L1 Dist.↓ L1 Dist.↓ △ m ↑ (%)
TaskDEDZEOETKey2DKey3DNPCRS2DS25DMetric L1 Dist. ↓ ST 0.01990.01950.10850.17140.16330.08720.27150.75860.15030.17420.15040.00GD0.01870.01880.13010.17570.17330.09420.30760.79910.18260.19020.1652-7.83GradDrop [8]0.03150.02420.13900.17760.17780.09760.45640.86440.20880.19950.1752-26.11MGDA [48]------------UW [32]0.01900.01900.13080.17580.17340.09450.31090.80090.18400.19060.1657-8.43DWA [37]0.01860.01870.12940.17590.17350.09380.27880.79430.18050.19020.1640-6.45PCGrad [67]0.02170.01920.12980.17750.17140.09390.28560.79850.18170.19270.1595-8.29CAGrad [34]0.02190.02030.13140.18000.16650.09320.30390.81210.18740.19530.1673-10.57IMTL [36]0.02100.01920.12820.17720.17190.09360.24680.77840.17340.19430.1647-6.17Align-MTL [49]0.01890.01930.12540.17280.16640.09140.35240.86400.19380.18890.1582-9.41Nash-MTL [44]0.02010.01840.12480.17640.17010.09210.26580.77930.17060.19140.1624-5.01FAMO [35]0.01880.01880.13000.17580.17330.09420.30580.79860.18260.19040.1654-7.87DTME-MTL0.01500.01540.11930.17330.16680.08910.20380.73730.15670.17730.1517+ 4.67</p>
<p>Table 4 .
4
Adaptation of DTME-MTL to other state-of-the-art MTL methods on NYUD-v2.
TaskSemsegDepthNormalEdgeMetricmIoU ↑ RMSE ↓ mErr ↓ odsF ↑MTformer [61]50.040.490--InvPT [65]53.560.518318.8178.10+ DTME-MTL54.380.502018.5178.20Taskprompter [66]55.300.515218.4778.20+ DTME-MTL56.360.512218.3878.40</p>
<p>Table 5 .
5
Adaptation of DTME-MTL to other state-of-the-art MTL methods on PASCAL-Context.
TaskSemseg Parsing Saliency NormalEdgeMetricmIoU ↑ mIoU ↑ maxF ↑mErr ↓ odsF ↑MTformer [61]73.5164.2667.24--InvPT [65]79.0367.6184.8114.1573.00+ DTME-MTL81.9171.1384.9613.7373.80Taskprompter [66]80.8968.8984.8313.7273.50+ DTME-MTL81.0169.0884.7513.6573.60</p>
<p>Table 6 .
6
Time consumption of each process in DTME-MTL across different backbone sizes, recorded in minutes.
Process (min)ViT-TViT-SViT-BViT-LCalculate Token Space (SVD)3.613.7411.5411.96Calculate Gradient Conflict8.2516.4321.9458.66</p>
<p>Table 7 .
7
Comparison with Recon on NYUD-v2
MethodSemseg mIoU ↑Depth RMSE ↓Normal mErr ↓Edge odsF ↑#Param ↑ (%)Joint34.130.67322.5156.380.0Recon [22]31.920.69323.3552.8023.34Ours38.270.637021.6457.900.24</p>
<p>Table 8 .
8
Comparison with multi-task optimization approaches on Taskonomy across 11 different tasks with ViT-L.Non-converged results are indicated with a dash.</p>
<p>Table 12 .
12
We assess task performance by comparing scenarios where we freeze the backbone network after expansion (w/ Freeze) and where we don't (w/o Freeze).
NYUD-v2PASCAL-ContextModelSemsegDepthNormalEdgeSemsegParsingSaliencyNormalEdgemIoU ↑RMSE ↓mErr ↓odsF ↑mIoU ↑mIoU ↑maxF ↑mErr ↓odsF ↑TM+TE (w/ Freeze)34.800.673022.4856.0058.3452.9682.8615.6343.20TM+TE (w/o Freeze)38.270.637021.6457.9066.1856.2983.2115.2647.00</p>
<p>Table 13 .
13
Performance comparison across selection strategies.
MethodRandom ReverseSwapStandard△m ↑ (%)-2.966-6.167-2.608+0.044</p>
<p>Table 14
14TM11.60 % ↓8.92 % ↓TE4.64% ↓15.44 % ↓
. Reduction in gradient conflict numbers (NYUD-v2).Method Num(gR,i • gR,i ≤ 0) Num(gN ,i • gN ,i ≤ 0)</p>
<p>Table11.We compare task performance based on the configuration of the modulator.Specifically, we compare the performance of tasks using an affine transformation against those using a batch normalization layer as configurations for the modulator.
Mult: an end-to-end multitask learning transformer. Deblina Bhattacharjee, Tong Zhang, Sabine Süsstrunk, Mathieu Salzmann, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution kernels. Ryutaro Felix Js Bragman, Sebastien Tanno, Daniel C Ourselin, Jorge Alexander, Cardoso, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Automated search for resourceefficient branched multi-task networks. David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, Luc Van Gool, arXiv:2008.102922020arXiv preprint</p>
<p>Automated search for resourceefficient branched multi-task networks. David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, Luc Van Gool, arXiv:2008.10292202012arXiv preprint</p>
<p>Multi-task learning for multi-objective evolutionary neural architecture search. Ronghong Cai, Jianping Luo, IEEE Congress on Evolutionary Computation (CEC). 22021. 2021IEEE</p>
<p>Multitask learning. Rich Caruana, Machine learning. 2811997</p>
<p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Vijay Zhao Chen, Chen-Yu Badrinarayanan, Andrew Lee, Rabinovich, International conference on machine learning. PMLR2018</p>
<p>Just pick a sign: Optimizing deep multitask models with gradient sign dropout. Jiquan Zhao Chen, Yanping Ngiam, Thang Huang, Henrik Luong, Yuning Kretzschmar, Dragomir Chai, Anguelov, Advances in Neural Information Processing Systems. 2039-2050, 2020. 3, 6, 833</p>
<p>Mod-squad: Designing mixtures of experts as modular multi-task learners. Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik G Learned-Miller, Chuang Gan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20231</p>
<p>Michael Crawshaw, arXiv:2009.09796Multi-task learning with deep neural networks: A survey. 2020arXiv preprint</p>
<p>Instance-aware semantic segmentation via multi-task network cascades. Jifeng Dai, Kaiming He, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016112</p>
<p>Imagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee200912</p>
<p>Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Jean-Antoine Désidéri, Comptes Rendus Mathematique. 3505-632012</p>
<p>Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.119292020112arXiv preprint</p>
<p>Representation similarity analysis for efficient task taxonomy &amp; transfer learning. Kshitij Dwivedi, Gemma Roig, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015112</p>
<p>The pascal visual object classes challenge 2012 (voc2012) development kit. Pattern Anal. Mark Everingham, John Winn, Stat. Model. Comput. Learn., Tech. Rep. 122007. 2012</p>
<p>M 3 vit: Mixture-of-experts vision transformer for efficient multitask learning with model-accelerator co-design. Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, Zhangyang Wang, Advances in Neural Information Processing Systems. 202235</p>
<p>Pathnet: Evolution channels gradient descent in super neural networks. Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, Daan Wierstra, arXiv:1701.08734201712arXiv preprint</p>
<p>Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction. Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, Alan L Yuille, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition201912</p>
<p>Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning. Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, Wei Liu, Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. the IEEE/CVF Conference on computer vision and pattern recognition2020</p>
<p>Recon: Reducing conflicting gradients from the root for multi-task learning. Qimai Shi Guangyuan, Wenlong Li, Jiaxin Zhang, Xiao-Ming Chen, Wu, The Eleventh International Conference on Learning Representations. 2022. 2, 3, 6, 7, 8</p>
<p>Dynamic task prioritization for multitask learning. Michelle Guo, Albert Haque, De-An, Serena Huang, Li Yeung, Fei-Fei, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Learning to branch for multi-task learning. Pengsheng Guo, Chen-Yu Lee, Daniel Ulbricht, International conference on machine learning. 2020</p>
<p>Learning to branch for multi-task learning. Pengsheng Guo, Chen-Yu Lee, Daniel Ulbricht, International Conference on Machine Learning. PMLR202012</p>
<p>Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 12152022</p>
<p>Going beyond multi-task dense prediction with synergy embedding models. Huimin Huang, Yawen Huang, Lanfen Lin, Ruofeng Tong, Yen-Wei Chen, Hao Zheng, Yuexiang Li, Yefeng Zheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Rotograd: Gradient homogenization in multitask learning. Adrián Javaloy, Isabel Valera, arXiv:2103.026312021arXiv preprint</p>
<p>Quantifying task priority for multi-task optimization. Wooseong Jeong, Kuk-Jin Yoon, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Forkmerge: Mitigating negative transfer in auxiliary-task learning. Junguang Jiang, Baixu Chen, Junwei Pan, Ximei Wang, Dapeng Liu, Jie Jiang, Mingsheng Long, Advances in Neural Information Processing Systems. 2024363</p>
<p>Principal component analysis: A review and recent developments. Ian T Jollife , Jorge Cadima, Philos. Trans. R. Soc. A Math. Phys. Eng. Sci. 37442065. 20150202. 2016</p>
<p>Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. Alex Kendall, Yarin Gal, Roberto Cipolla, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition20186</p>
<p>Evolutionary architecture search for deep multitask networks. Jason Liang, Elliot Meyerson, Risto Miikkulainen, Proceedings of the genetic and evolutionary computation conference. the genetic and evolutionary computation conference2018</p>
<p>Conflict-averse gradient descent for multi-task learning. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, Qiang Liu, Advances in Neural Information Processing Systems. 18878-18890, 2021. 2, 3, 6, 834</p>
<p>Famo: Fast adaptive multitask optimization. Bo Liu, Yihao Feng, Peter Stone, Qiang Liu, Advances in Neural Information Processing Systems. 2024. 2, 6, 836</p>
<p>Towards impartial multi-task learning. iclr. Liyang Liu, Yi Li, Zhanghui Kuang, Yimin Xue, Wenming Chen, Qingmin Yang, Wayne Liao, Zhang, 2021. 2, 3, 6, 8</p>
<p>Endto-end multi-task learning with attention. Shikun Liu, Edward Johns, Andrew J Davison, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019. 2, 6, 8</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition201712</p>
<p>Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, Ed H Chi, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018112</p>
<p>Attentive single-tasking of multiple tasks. Kevis-Kokitsi Maninis, Ilija Radosavovic, Iasonas Kokkinos, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019612</p>
<p>The role of context for object detection and semantic segmentation in the wild. Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, Alan Yuille, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2014</p>
<p>Multimodal contrastive learning with limoe: the language-image mixture of experts. Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, Neil Houlsby, Advances in Neural Information Processing Systems. 202235</p>
<p>Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, Ethan Fetaya, arXiv:2202.01017Multitask learning as a bargaining game. 2022. 2, 3, 6, 8arXiv preprint</p>
<p>What is being transferred in transfer learning? Advances in neural information processing systems. Hanie Behnam Neyshabur, Chiyuan Sedghi, Zhang, 20203315</p>
<p>Continual and multi-task architecture search. Ramakanth Pasunuru, Mohit Bansal, arXiv:1906.052262019arXiv preprint</p>
<p>Scaling vision with sparse mixture of experts. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Susano André, Daniel Pinto, Neil Keysers, Houlsby, Advances in Neural Information Processing Systems. 3422021</p>
<p>Multi-task learning as multi-objective optimization. Ozan Sener, Vladlen Koltun, 2018. 2, 3, 6, 831Advances in neural information processing systems</p>
<p>Independent component alignment for multi-task learning. Dmitry Senushkin, Nikolay Patakin, Arseny Kuznetsov, Anton Konushin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20083-20093, 2023. 2, 3, 6, 8</p>
<p>Indoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, Computer Vision-ECCV 2012: 12th European Conference on Computer Vision. Florence, ItalySpringerOctober 7-13, 2012. 2012612Proceedings, Part V 12</p>
<p>One-shot neural architecture search for deep multi-task learning in computer vision. Gianluigi Silvestri, 2020</p>
<p>Very deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.15562014112arXiv preprint</p>
<p>Gradient adversarial training of neural networks. Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, Andrew Rabinovich, 201812</p>
<p>Task switching network for multi-task learning. Guolei Sun, Thomas Probst, Pani Danda, Nikola Paudel, Menelaos Popović, Jagruti Kanakis, Dengxin Patel, Luc Dai, Van Gool, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision202112</p>
<p>Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere, Luc Van Gool, arXiv:1904.02920Branched multi-task networks: deciding what layers to share. 201912arXiv preprint</p>
<p>Mti-net: Multi-scale task interaction networks for multi-task learning. Simon Vandenhende, Stamatios Georgoulis, Luc Van Gool, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020112Proceedings, Part IV 16</p>
<p>Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Crossformer: A versatile vision transformer hinging on cross-scale attention. Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, Wei Liu, arXiv:2108.001542021arXiv preprint</p>
<p>Segformer: Simple and efficient design for semantic segmentation with transformers. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, Advances in Neural Information Processing Systems. 202134</p>
<p>Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing. Dan Xu, Wanli Ouyang, Xiaogang Wang, Nicu Sebe, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018112</p>
<p>Mtformer: Multi-task learning via transformer and cross-task reasoning. Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam Lim, Antonio Torralba, Computer Vision-ECCV 2022: 17th European Conference. Tel AvivSpringerOctober 23-27, 2022. 20222Proceedings, Part XXVII</p>
<p>Multi-task learning with multi-query transformer for dense prediction. Yangyang Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, Lefei Zhang, IEEE Transactions on Circuits and Systems for Video Technology. 20231</p>
<p>Demt: Deformable mixer transformer for multi-task learning of dense prediction. Yangyang Xu, Yibo Yang, Lefei Zhang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence20231</p>
<p>Focal self-attention for local-global interactions in vision transformers. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao, arXiv:2107.006412021arXiv preprint</p>
<p>Inverted pyramid multi-task transformer for dense scene understanding. Hanrong Ye, Dan Xu, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022812Proceedings, Part XXVII</p>
<p>Taskprompter: Spatial-channel multi-task prompting for dense scene understanding. Hanrong Ye, Dan Xu, The Eleventh International Conference on Learning Representations. 2022. 1, 2, 812</p>
<p>Gradient surgery for multi-task learning. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, Advances in Neural Information Processing Systems. 2020. 2, 3, 6, 833</p>
<p>Taskonomy: Disentangling task transfer learning. Alexander Amir R Zamir, William Sax, Leonidas J Shen, Jitendra Guibas, Silvio Malik, Savarese, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Mixture of attention heads: Selecting attention heads per token. Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, Zhang Xiong, arXiv:2210.0514420221arXiv preprint</p>
<p>Facial landmark detection by deep multi-task learning. Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014112Proceedings, Part VI 13</p>
<p>Pattern-affinitive propagation across depth, surface normal and semantic segmentation. Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, Jian Yang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019112</p>            </div>
        </div>

    </div>
</body>
</html>
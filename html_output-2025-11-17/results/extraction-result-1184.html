<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1184 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1184</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1184</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-219687812</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2006.07409v1.pdf" target="_blank">How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games are long puzzles or quests, characterized by a sequence of sparse and potentially deceptive rewards. They provide an ideal platform to develop agents that perceive and act upon the world using a combinatorially sized natural language state-action space. Standard Reinforcement Learning agents are poorly equipped to effectively explore such spaces and often struggle to overcome bottlenecks---states that agents are unable to pass through simply because they do not see the right action sequence enough times to be sufficiently reinforced. We introduce Q*BERT, an agent that learns to build a knowledge graph of the world by answering questions, which leads to greater sample efficiency. To overcome bottlenecks, we further introduce MC!Q*BERT an agent that uses an knowledge-graph-based intrinsic motivation to detect bottlenecks and a novel exploration strategy to efficiently learn a chain of policy modules to overcome them. We present an ablation study and results demonstrating how our method outperforms the current state-of-the-art on nine text games, including the popular game, Zork, where, for the first time, a learning agent gets past the bottleneck where the player is eaten by a Grue.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1184.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1184.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zork I (classic parser-based text-adventure game)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonical fantasy parser-based interactive fiction used as a hard benchmark: partially observable, deterministic text descriptions of rooms, puzzles requiring long-range dependencies (e.g., an unlit Cellar with a Grue) and large combinatorial action space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Zork1</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fantasy parser-based interactive fiction; agent receives textual room descriptions and issues multi-word commands; gameplay requires visiting locations, collecting and using inventory items, and satisfying long-range puzzle dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Multiple conditional-access constraints: boarded/locked doors and windows, location-dependent hazards (e.g., Cellar contains a Grue that kills the player unless they have and have lit a lantern), and other conditional access that depends on prior, often unrewarded, actions (e.g., draining reservoirs before entering submerged rooms).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Modeled as a directed acyclic dependency graph (DAG) over rewards and dependencies; topology described qualitatively as 'relatively linear plot' with local regions of high branching factor separated by bottleneck nodes (single-node topological levels).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Not specified in node-count; action-space sizes stated: raw parser action space O(697^5) ≈ 1.64×10^14; template-based action space O(237×697^2) ≈ 1.15×10^8 (used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT / MC!Q*BERT (compared with KG-A2C, GO!Q*BERT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Q*BERT: builds a dynamic knowledge graph from observations using an ALBERT QA model and R-GCN embeddings; uses graph masks to constrain template action decoding and an A2C training loop. MC!Q*BERT: extends Q*BERT with graph-based intrinsic motivation (reward for adding novel KG edges) to detect stagnation/bottlenecks and a modular policy-chaining/backtracking procedure that learns sub-policies (options) to reach successive bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Sample efficiency (speed to asymptotic performance), max score achieved (across runs), asymptotic score (average final episodes), and KG-growth-based intrinsic-reward; training budget also reported in steps (e.g., up to 10^6 steps per parallel A2C agent).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Qualitative: Q*BERT reaches asymptotic performance faster than KG-A2C (no numeric speedup reported); MC!Q*BERT achieves higher max scores and passes key bottlenecks (e.g., Cellar/Grue) that other agents fail to pass; no single numeric step-to-goal provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not given as a percentage; reported as higher max scores and for the first time a learning agent (MC!Q*BERT) consistently gets past the Grue bottleneck in Zork1 across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Modular, memory/graph-informed, bottleneck-focused policies (options) that can be chained and that use backtracking to fulfill long-range dependencies (i.e., modular sub-policies that each reach a bottleneck).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Explicitly links topology to performance: bottlenecks are defined as nodes that occupy singleton topological levels in the dependency DAG and partition the state space — agents that do not detect or plan for these bottlenecks (i.e., that are greedy or lack modular policies) become stuck and fail to reach later high-reward states. High branching factor regions increase exploration difficulty before/after bottlenecks. Unrewarded dependencies (inventory/location preconditions) make bottlenecks deceptive; using KG growth as intrinsic motivation helps detect stagnation (no new KG edges) and thus identifies likely bottlenecks. Determinism of transitions enables backtracking and modular chaining to overcome topological partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No explicit, controlled comparison across different graph topologies; the paper evaluates methods across multiple distinct human-designed games (including Zork1) but does not vary or systematically compare topology parameters (diameter, clustering coefficient) in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that perform best are decomposed into modular sub-policies (options) that reach sequential bottlenecks and can be chained; backtracking to prior states (leveraging environment determinism) to learn alternative sub-policies is critical. Graph-informed state (KG) and graph masks reduce action-space and enable effective selection of entities; intrinsic rewards based on KG expansion prevent premature convergence to locally high-reward trajectories and encourage satisfying unrewarded dependencies (e.g., acquiring and lighting a lamp to traverse an otherwise deadly Cellar).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1184.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1184.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-adventure games (parser-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parser-based text-adventure / text-world games (general class)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of partially-observable, mostly-deterministic sequential decision problems where agents receive textual observations (room descriptions, inventory, simulator feedback) and must issue natural-language commands (large combinatorial action space); tasks commonly exhibit sparse, delayed rewards and long-range dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>parser-based text-adventure games (general)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Domains are often fantasy or puzzle-adventure themed; require visiting locations, interacting with objects, and satisfying item/location dependencies to unlock new areas or rewards; used as benchmarks for language-conditioned RL.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>Frequent conditional access rules: locked/boarded doors, single-use or conditional passages, environmental preconditions (e.g., draining, lighting) and inventory-dependent access that are not explicitly rewarded by the game engine.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Conceptualized as sparse DAG-like dependency graphs over tasks/rewards with local regions of high branching factor separated by narrow connectors (bottlenecks); connectivity is not fully connected but organized into topological levels of dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Varies by game; action-space combinatorics can be enormous (example: Zork1 template-action space ≈1.15×10^8), number of rooms/nodes unspecified and game-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT, MC!Q*BERT, KG-A2C, GO!Q*BERT, DRRN, TDQN, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Range from graph-constrained A2C agents (KG-A2C) to QA-driven graph builders (Q*BERT) to exploration-focused algorithms (Go-Explore adaptations). Key navigation-relevant features include knowledge-graph state representations, graph masks to constrain entities, intrinsic KG-growth rewards, and modular option-based policy chaining.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Sample efficiency (learning speed to asymptote), coverage of new states (KG edge growth), max achieved score across runs, and steps-to-discover (implicit via training-step budgets).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Qualitative: knowledge-graph-based agents show faster sample efficiency (Q*BERT faster than KG-A2C); intrinsic KG-based motivation improves bottleneck detection and final max scores (MC!Q*BERT > baselines); no universal numeric rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Graph-informed, memory-based modular policies (options) with explicit subgoal/bottleneck detection and backtracking capability.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper argues generally that sparse reward placement combined with DAG-like dependency structure produces bottlenecks (singleton topological levels) that critically constrain progress; agents unaware of these partitions (no subgoal detection or backtracking) tend to prematurely converge to locally high-reward trajectories and fail to explore other branches, reducing final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No parametric study across synthetic topologies; insights are qualitative and derived from different human-authored games rather than controlled topology manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Agents benefit from explicit subtask decomposition into policies that reach bottlenecks (options). Intrinsic rewards tied to knowledge-graph expansion help detect when exploration has stalled in a topological partition; backtracking and deterministic replay of trajectories are effective when the environment is deterministic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1184.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1184.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jericho / TextWorld (frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jericho (interaction framework) and TextWorld (procedural text-game generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Jericho is a framework for interacting with parser-based text games and extracting ground-truth state for dataset creation; TextWorld is a procedural generator for parser-based games enabling control over difficulty and environment generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho / TextWorld frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Tooling/frameworks used to run and collect data from text-adventure games (Jericho) and to procedurally generate text-game environments under controlled parameters (TextWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>TextWorld allows generation of varying connectivity/difficulty; Jericho exposes game engine states allowing extraction of ground-truth location/inventory dependencies used to build dependency graphs for analysis (paper uses Jericho to build Jericho-QA).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Framework-level; individual game sizes vary. Jericho-QA dataset contains 221,453 QA training pairs and 56,667 held-out test pairs constructed from Jericho games.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Used as testbeds for the agents (Q*BERT / MC!Q*BERT / KG-A2C / GO!Q*BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Frameworks are not agents but provide simulator determinism and state access for oracle data collection; Jericho enabled creation of the Jericho-QA dataset and ground-truth KG edges for QA fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Frameworks support reporting of game score, number of steps, and access to ground-truth state for measuring KG expansion; paper uses those metrics for agent evaluation (e.g., max score, asymptotic score over last 100 episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper uses Jericho to extract dependency graphs and reason about bottlenecks; TextWorld referenced as prior work that enables procedural control of difficulty/topology but the present paper does not present controlled topology experiments using it.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>None presented in this work using TextWorld; TextWorld mentioned as a tool that could enable such studies but not used for parametric topology comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>The determinism of Jericho-style simulators is leveraged by MC!Q*BERT and GO!Q*BERT to enable backtracking and deterministic replay of promising trajectories; this property is part of the reason policy-chaining and Go-Explore style methods can be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Go-explore: a new approach for hard-exploration problems <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Exploration based language learning for text-based games <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1184",
    "paper_id": "paper-219687812",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Zork1",
            "name_full": "Zork I (classic parser-based text-adventure game)",
            "brief_description": "A canonical fantasy parser-based interactive fiction used as a hard benchmark: partially observable, deterministic text descriptions of rooms, puzzles requiring long-range dependencies (e.g., an unlit Cellar with a Grue) and large combinatorial action space.",
            "citation_title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds",
            "mention_or_use": "use",
            "environment_name": "Zork1",
            "environment_description": "Fantasy parser-based interactive fiction; agent receives textual room descriptions and issues multi-word commands; gameplay requires visiting locations, collecting and using inventory items, and satisfying long-range puzzle dependencies.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Multiple conditional-access constraints: boarded/locked doors and windows, location-dependent hazards (e.g., Cellar contains a Grue that kills the player unless they have and have lit a lantern), and other conditional access that depends on prior, often unrewarded, actions (e.g., draining reservoirs before entering submerged rooms).",
            "graph_connectivity": "Modeled as a directed acyclic dependency graph (DAG) over rewards and dependencies; topology described qualitatively as 'relatively linear plot' with local regions of high branching factor separated by bottleneck nodes (single-node topological levels).",
            "environment_size": "Not specified in node-count; action-space sizes stated: raw parser action space O(697^5) ≈ 1.64×10^14; template-based action space O(237×697^2) ≈ 1.15×10^8 (used in experiments).",
            "agent_name": "Q*BERT / MC!Q*BERT (compared with KG-A2C, GO!Q*BERT, etc.)",
            "agent_description": "Q*BERT: builds a dynamic knowledge graph from observations using an ALBERT QA model and R-GCN embeddings; uses graph masks to constrain template action decoding and an A2C training loop. MC!Q*BERT: extends Q*BERT with graph-based intrinsic motivation (reward for adding novel KG edges) to detect stagnation/bottlenecks and a modular policy-chaining/backtracking procedure that learns sub-policies (options) to reach successive bottlenecks.",
            "exploration_efficiency_metric": "Sample efficiency (speed to asymptotic performance), max score achieved (across runs), asymptotic score (average final episodes), and KG-growth-based intrinsic-reward; training budget also reported in steps (e.g., up to 10^6 steps per parallel A2C agent).",
            "exploration_efficiency_value": "Qualitative: Q*BERT reaches asymptotic performance faster than KG-A2C (no numeric speedup reported); MC!Q*BERT achieves higher max scores and passes key bottlenecks (e.g., Cellar/Grue) that other agents fail to pass; no single numeric step-to-goal provided.",
            "success_rate": "Not given as a percentage; reported as higher max scores and for the first time a learning agent (MC!Q*BERT) consistently gets past the Grue bottleneck in Zork1 across runs.",
            "optimal_policy_type": "Modular, memory/graph-informed, bottleneck-focused policies (options) that can be chained and that use backtracking to fulfill long-range dependencies (i.e., modular sub-policies that each reach a bottleneck).",
            "topology_performance_relationship": "Explicitly links topology to performance: bottlenecks are defined as nodes that occupy singleton topological levels in the dependency DAG and partition the state space — agents that do not detect or plan for these bottlenecks (i.e., that are greedy or lack modular policies) become stuck and fail to reach later high-reward states. High branching factor regions increase exploration difficulty before/after bottlenecks. Unrewarded dependencies (inventory/location preconditions) make bottlenecks deceptive; using KG growth as intrinsic motivation helps detect stagnation (no new KG edges) and thus identifies likely bottlenecks. Determinism of transitions enables backtracking and modular chaining to overcome topological partitions.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No explicit, controlled comparison across different graph topologies; the paper evaluates methods across multiple distinct human-designed games (including Zork1) but does not vary or systematically compare topology parameters (diameter, clustering coefficient) in isolation.",
            "policy_structure_findings": "Policies that perform best are decomposed into modular sub-policies (options) that reach sequential bottlenecks and can be chained; backtracking to prior states (leveraging environment determinism) to learn alternative sub-policies is critical. Graph-informed state (KG) and graph masks reduce action-space and enable effective selection of entities; intrinsic rewards based on KG expansion prevent premature convergence to locally high-reward trajectories and encourage satisfying unrewarded dependencies (e.g., acquiring and lighting a lamp to traverse an otherwise deadly Cellar).",
            "uuid": "e1184.0",
            "source_info": {
                "paper_title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Text-adventure games (parser-based)",
            "name_full": "Parser-based text-adventure / text-world games (general class)",
            "brief_description": "A class of partially-observable, mostly-deterministic sequential decision problems where agents receive textual observations (room descriptions, inventory, simulator feedback) and must issue natural-language commands (large combinatorial action space); tasks commonly exhibit sparse, delayed rewards and long-range dependencies.",
            "citation_title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds",
            "mention_or_use": "use",
            "environment_name": "parser-based text-adventure games (general)",
            "environment_description": "Domains are often fantasy or puzzle-adventure themed; require visiting locations, interacting with objects, and satisfying item/location dependencies to unlock new areas or rewards; used as benchmarks for language-conditioned RL.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": true,
            "door_constraints_description": "Frequent conditional access rules: locked/boarded doors, single-use or conditional passages, environmental preconditions (e.g., draining, lighting) and inventory-dependent access that are not explicitly rewarded by the game engine.",
            "graph_connectivity": "Conceptualized as sparse DAG-like dependency graphs over tasks/rewards with local regions of high branching factor separated by narrow connectors (bottlenecks); connectivity is not fully connected but organized into topological levels of dependencies.",
            "environment_size": "Varies by game; action-space combinatorics can be enormous (example: Zork1 template-action space ≈1.15×10^8), number of rooms/nodes unspecified and game-dependent.",
            "agent_name": "Q*BERT, MC!Q*BERT, KG-A2C, GO!Q*BERT, DRRN, TDQN, etc.",
            "agent_description": "Range from graph-constrained A2C agents (KG-A2C) to QA-driven graph builders (Q*BERT) to exploration-focused algorithms (Go-Explore adaptations). Key navigation-relevant features include knowledge-graph state representations, graph masks to constrain entities, intrinsic KG-growth rewards, and modular option-based policy chaining.",
            "exploration_efficiency_metric": "Sample efficiency (learning speed to asymptote), coverage of new states (KG edge growth), max achieved score across runs, and steps-to-discover (implicit via training-step budgets).",
            "exploration_efficiency_value": "Qualitative: knowledge-graph-based agents show faster sample efficiency (Q*BERT faster than KG-A2C); intrinsic KG-based motivation improves bottleneck detection and final max scores (MC!Q*BERT &gt; baselines); no universal numeric rates reported.",
            "success_rate": null,
            "optimal_policy_type": "Graph-informed, memory-based modular policies (options) with explicit subgoal/bottleneck detection and backtracking capability.",
            "topology_performance_relationship": "Paper argues generally that sparse reward placement combined with DAG-like dependency structure produces bottlenecks (singleton topological levels) that critically constrain progress; agents unaware of these partitions (no subgoal detection or backtracking) tend to prematurely converge to locally high-reward trajectories and fail to explore other branches, reducing final performance.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No parametric study across synthetic topologies; insights are qualitative and derived from different human-authored games rather than controlled topology manipulation.",
            "policy_structure_findings": "Agents benefit from explicit subtask decomposition into policies that reach bottlenecks (options). Intrinsic rewards tied to knowledge-graph expansion help detect when exploration has stalled in a topological partition; backtracking and deterministic replay of trajectories are effective when the environment is deterministic.",
            "uuid": "e1184.1",
            "source_info": {
                "paper_title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Jericho / TextWorld (frameworks)",
            "name_full": "Jericho (interaction framework) and TextWorld (procedural text-game generator)",
            "brief_description": "Jericho is a framework for interacting with parser-based text games and extracting ground-truth state for dataset creation; TextWorld is a procedural generator for parser-based games enabling control over difficulty and environment generation.",
            "citation_title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds",
            "mention_or_use": "use",
            "environment_name": "Jericho / TextWorld frameworks",
            "environment_description": "Tooling/frameworks used to run and collect data from text-adventure games (Jericho) and to procedurally generate text-game environments under controlled parameters (TextWorld).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "TextWorld allows generation of varying connectivity/difficulty; Jericho exposes game engine states allowing extraction of ground-truth location/inventory dependencies used to build dependency graphs for analysis (paper uses Jericho to build Jericho-QA).",
            "environment_size": "Framework-level; individual game sizes vary. Jericho-QA dataset contains 221,453 QA training pairs and 56,667 held-out test pairs constructed from Jericho games.",
            "agent_name": "Used as testbeds for the agents (Q*BERT / MC!Q*BERT / KG-A2C / GO!Q*BERT)",
            "agent_description": "Frameworks are not agents but provide simulator determinism and state access for oracle data collection; Jericho enabled creation of the Jericho-QA dataset and ground-truth KG edges for QA fine-tuning.",
            "exploration_efficiency_metric": "Frameworks support reporting of game score, number of steps, and access to ground-truth state for measuring KG expansion; paper uses those metrics for agent evaluation (e.g., max score, asymptotic score over last 100 episodes).",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": null,
            "topology_performance_relationship": "Paper uses Jericho to extract dependency graphs and reason about bottlenecks; TextWorld referenced as prior work that enables procedural control of difficulty/topology but the present paper does not present controlled topology experiments using it.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "None presented in this work using TextWorld; TextWorld mentioned as a tool that could enable such studies but not used for parametric topology comparisons here.",
            "policy_structure_findings": "The determinism of Jericho-style simulators is leveraged by MC!Q*BERT and GO!Q*BERT to enable backtracking and deterministic replay of promising trajectories; this property is part of the reason policy-chaining and Go-Explore style methods can be effective.",
            "uuid": "e1184.2",
            "source_info": {
                "paper_title": "How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Go-explore: a new approach for hard-exploration problems",
            "rating": 2,
            "sanitized_title": "goexplore_a_new_approach_for_hardexploration_problems"
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 2,
            "sanitized_title": "counting_to_explore_and_generalize_in_textbased_games"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Exploration based language learning for text-based games",
            "rating": 2,
            "sanitized_title": "exploration_based_language_learning_for_textbased_games"
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        }
    ],
    "cost": 0.01515325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</p>
<p>Prithviraj Ammanabrolu raj.ammanabrolu@gatech.edu 
Georgia Institute of Technology ‡ Microsoft Research</p>
<p>Ethan Tien 
Georgia Institute of Technology ‡ Microsoft Research</p>
<p>Matthew Hausknecht 
Georgia Institute of Technology ‡ Microsoft Research</p>
<p>Mark O Riedl 
Georgia Institute of Technology ‡ Microsoft Research</p>
<p>How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</p>
<p>Text-based games are long puzzles or quests, characterized by a sequence of sparse and potentially deceptive rewards. They provide an ideal platform to develop agents that perceive and act upon the world using a combinatorially sized natural language state-action space. Standard Reinforcement Learning agents are poorly equipped to effectively explore such spaces and often struggle to overcome bottlenecks-states that agents are unable to pass through simply because they do not see the right action sequence enough times to be sufficiently reinforced. We introduce Q<em>BERT 1 , an agent that learns to build a knowledge graph of the world by answering questions, which leads to greater sample efficiency. To overcome bottlenecks, we further introduce MC!Q</em>BERT an agent that uses an knowledge-graph-based intrinsic motivation to detect bottlenecks and a novel exploration strategy to efficiently learn a chain of policy modules to overcome them. We present an ablation study and results demonstrating how our method outperforms the current state-of-the-art on nine text games, including the popular game, Zork, where, for the first time, a learning agent gets past the bottleneck where the player is eaten by a Grue.</p>
<p>Introduction</p>
<p>Text-adventure-or interaction fiction-games are simulations featuring language-based state and action spaces. An example of a one turn agent interaction in the popular text-game Zork1 [7] can be seen in Fig. 1. Prior works have focused on a few challenges that are inherent to this medium: (1) Partial observability the agent must reason about the world solely through incomplete textual descriptions [22,10,5]. (2) Commonsense reasoning to enable the agent to more intelligently interact with objects in its surroundings [14,27,2,4]. (3) A combinatorial state-action space wherein most games have action spaces exceeding a billion possible actions per step; for example the game Zork1 has 1.64 × 10 14 possible actions at every step [15,3]. Despite these challenges, modern text-adventure agents such as KG-A2C [3], TDQN [15], and DRRN [16] have relied on surprisingly simple exploration strategies such as -greedy or sampling from the distribution of possible actions.</p>
<p>In this paper, we focus on a particular type of exploration problem: that of detecting and overcoming bottleneck states. Most text-adventure games have relatively linear plots in which players must solve a sequence of puzzles to advance the story and gain score. To solve these puzzles, players have freedom to a explore previously unlocked areas of the game, collect clues, and acquire tools needed to solve the next puzzle and unlock the next portion of the game. From a Reinforcement Learning perspective, These puzzles can be viewed as bottlenecks that act as partitions between different regions of the state space. We contend that existing Reinforcement Learning agents are unaware of such latent structure and are thus poorly equipped for solving these types of problems. We present techniques for automatically detecting bottlenecks and efficiently learning policies that take advantage of the natural partitions in the state space.</p>
<p>Observation: West of House You are standing in an open field west of a white house, with a boarded front door. There is a small mailbox here.</p>
<p>Action: Open mailbox</p>
<p>Observation: Opening the small mailbox reveals a leaflet.</p>
<p>Action: Read leaflet</p>
<p>Observation: (Taken) "WELCOME TO ZORK! ZORK is a game of adventure, danger, and low cunning. In it you will explore some of the most amazing territory ever seen by mortals. No computer should be without one!"  Overcoming bottlenecks is not as simple as selecting the correct action from the bottleneck state. Most bottlenecks have long-range dependencies that must first be satisfied: Zork1 for instance features a bottleneck in which the agent must pass through the unlit Cellar where a monster known as a Grue lurks, ready to eat unsuspecting players who enter without a light source. To pass this bottleneck the player must have previously acquired and lit the latern. Other bottlenecks don't rely on inventory items and instead require the player to have satisfied an external condition such as visiting the reservoir control to drain water from a submerged room before being able to visit it. In both cases, the actions that fulfill dependencies of the bottleneck, e.g. acquiring the lantern or draining the room, are not rewarded by the game. Thus agents must correctly satisfy all latent dependencies, most of which are unrewarded, then take the right action from the correct location to overcome such bottlenecks. Consequently, most existing agentsregardless of whether they use a reduced action space [31,27] or the full space [15,3]-have failed to consistently clear these bottlenecks.</p>
<p>To better understand how to design algorithms that pass these bottlenecks, we first need to gain a sense for what they are. We observe that quests in text games-and any such sequential decision making problem requiring long term dependencies-can be modeled in the form of a dependency graph. These dependency graphs are directed acyclic graphs (DAGs) where the vertices indicate either rewards that can be collected or dependencies that must be met to progress. In text-adventure games the dependencies are of two types: items that must be collected for future use, and locations that must be visited. An example of such a graph for the game of Zork1 can found in Fig. 2. More formally, bottleneck states are vertices in the dependency graph that, when the graph is laid out topographically, are (a) the only state on a level, and (b) there is another state at a higher level with non-zero reward. Bottlenecks can be mathematically expressed as follows: let D = V, E be the directed acyclic dependency graph for a particular game where each vertex is tuple v = s l , s i , r(s) containing information on some state s such that s l are location dependencies, s i are inventory dependencies, and r(s) is the reward associated with the state. There is a directed edge e ∈ E between any two vertices such that the originating state meets the requirements s l and s i of the terminating vertex. D can be topologically sorted into levels L = {l 1 , ..., l n } where each level represents a set of game states that are not dependant on each other. We formulate the set of all bottleneck states in the game:
B = {b : (|l i | = 1, b ∈ l i , V ) ∧ (∃s ∈ l j s.t. (j &gt; i ∧ r(s) = 0))}(1)
This reads as the set of all states that that belong to a level with only one vertex and that there exists some state with a non-zero reward that depends on it. Intuitively, regardless of the path taken to get to a bottleneck state, any agent must pass it in order to continue collecting future rewards. Behind House is an example of a bottleneck state as seen in Fig. 2. The branching factor before and after this state is high but it is the only state through which one can enter the Kitchen through the window.</p>
<p>In this paper, we introduce Q<em>BERT, a deep reinforcement learning agent that plays text games by building a knowledge graph of the world and answering questions about it. Knowledge graph state representations have been shown to alleviate other challenges associated with text games such as partial-observability [5,28,3,6,1,21]. We introduce the Jericho-QA dataset, for questionanswering in text-game-like environments, and show that our novel question-answering-based graph construction procedure improves sample efficiency but not asymptotic performance. In order to improve performance and pass through bottlenecks, we extend Q</em>BERT with a novel exploration strategy that uses intrinsic motivation based on the knowledge graph to alleviate the sparse, deceptive reward problem. Our exploration strategy first detects bottlenecks and then modularly chains policies that go from one bottleneck to another. We call this combined system MC!Q<em>BERT. These two  Figure 2: Portion of the Zork1 quest structure visualized as a directed acyclic graph. Each node represents a state; clouds represent areas of high branching factor with labels indicating some of the actions that must be performed to progress enhancements form the two core contributions of this paper. We evaluate Q</em>BERT, MC!Q*BERT, and ablations of both on a set of nine text games. We further compare our technique to alternative exploration methods such as Go Explore [12]; our full technique achieves state-of-the-art performance on eight out of nine games.</p>
<p>Related Work and Background</p>
<p>We use the definition of text-adventure games as seen in Côté et al. [10] and Hausknecht et al. [15]. These games are partially observable Markov decision processes (POMDPs), represented as a 7-tuple of S, T, A, Ω, O, R, γ representing the set of environment states, mostly deterministic conditional transition probabilities between states, the vocabulary or words used to compose text commands, observations returned by the game, observation conditional probabilities, reward function, and the discount factor respectively. LSTM-DQN [22] and Action Elimination DQN [31] operate on a reduced action space of the order of 10 2 actions per step by considering either verb-noun pairs or by using a walkthrough of the game respectively. The agents learn how to produce Q-value estimates that maximize long term expected reward. The DRRN algorithm for choice-based games [16,32] estimates Q-values for a particular action from a particular state. Fulda et al. [14] try to use word embeddings specifically in an attempt to model affordances for items in these games, learning how to interact with them.</p>
<p>There have been a couple of works detailing potential methods of exploration in this domain. Jain et al. [17] extend consistent Q-learning [9] to text-games, focusing on taking into account historical context. In terms of exploration strategies, Yuan et al. [29] detail how counting the number of unique states visited improves generalization in unseen games. Côté et al. [10] introduce TextWorld, a framework for procedurally generating parser-based games via a grammar, allowing a user to control the difficulty of a generated game.Urbanek et al. [26] introduce LIGHT, a dataset of crowdsourced text-adventure game dialogs focusing on giving collaborative agents the ability to generate contextually relevant dialog and emotes. Hausknecht et al. [15] introduce Jericho, a framework for interacting with textgames, in addition to a series of baseline reinforcement learning agents. Yuan et al. [30] introduce the concept of interactive question-answering in the form of QAit-modeling QA tasks in TextWorld.</p>
<p>Ammanabrolu and Riedl [5] introduce the KG-DQN, using knowledge graphs as states spaces for text-game agents and Ammanabrolu and Riedl [4] extend it to enable transfer of knowledge between games. Ammanabrolu and Hausknecht [3] showcase the KG-A2C, for the first time tackling the fully combinatorial action space and presenting state-of-the-art results on many man-made text games. In a similar vein, Adhikari et al. [1] present the Graph-Aided Transformer Agent (GATA) which learns to construct a knowledge graph during game play and improves zero-shot generalization on procedurally generated TextWorld games.</p>
<p>Q*BERT</p>
<p>This section presents the base reinforcement learning algorithm we introduce, which we call Q<em>BERT. Q</em>BERT is based on KG-A2C [3]; it uses a knowledge-graph to represent it's understanding of the world state. A knowledge graph is a set of relations s, r, o such that s is a subject, r is a relation, and o is an object. See Figure 3 (left) for an example fragment of a knowledge graph for a text-adventure game. Instead of using relation extraction rules, Q*BERT uses a variant of the BERT [11]   language transformer to answer questions about the current state text description and populate the knowledge graph from the answers.</p>
<p>Knowledge Graph State Representation Ammanabrolu and Riedl [5] are the first to use question answering (QA) in text-game playing to pre-train a network to answer the question of "What action best next to take?" using game traces from an oracle agent capable of playing a game perfectly. They pre-train an LSTM to predict the action based on a environment text description. We build on this idea but instead treat the problem of constructing the knowledge graph as a question-answering task. The method first extracts a set of graph vertices V by asking a question-answering system relevant questions and then linking them together using a set of relations R to form a knowledge graph representing information the agent has learned about the world. Examples of questions include: "What is my current location?", "What objects are around me?", and "What am I carrying?" to respectively extract information regarding the agent's current location, surrounding objects, inventory objects. Further, we predict attributes for each object by asking the question "What attributes does x object have?". An example of the knowledge graph that can be extracted from description text and the overall architecture are shown in Fig. 3.</p>
<p>For question-answering, we use the pre-trained language model ALBERT [18], a variant of BERT [11] that is fine-tuned for question answering on the SQuAD [23] question-answering dataset. We further fine-tune the ALBERT model on a dataset specific to the text-game domain. This dataset, dubbed Jericho-QA, was created by making question answering pairs about text-games. Jericho [15] 2 is a framework for reinforcement learning in text-games. Using Jericho we construct a question-answering corpus for fine-tuning ALBERT as follows. For each game in Jericho, we use an oracle-an agent capable of playing the game perfectly-and a random exploration agent to gather ground truth state information about locations, objects, and attributes. These agents are designed to extract this information directly from the game engine, which is otherwise off-limits when Q<em>BERT is trained. From this ground truth, we construct pairs of questions in the form that Q</em>BERT will ask as it encounters environment description text, and the corresponding answers. These question-answer pairs are used to fine-tune the Q/A model and the ground truth data is discarded. No data from games we test on is used during ALBERT fine-tuning. Additional details regarding Jericho-QA, graph update rules, and Q*BERT can be found in Appendix A.1.</p>
<p>In a text-game the observation is a textual description of the environment. For every observation received, Q<em>BERT produces a fixed set of questions. The questions and the observation text are sent to the question-answering system. Predicted answers are converted into s, r, o triples and added to the knowledge graph. The complete knowledge graph is the input into Q</em>BERT's neural architecture (training described below), which makes a prediction of the next action to take.</p>
<p>Action Space Solving Zork1, the cannonical text-adventure game, requires the generation of actions consisting of up to five-words from a relatively modest vocabulary of 697 words recognized by the game's parser. This results in O(697 5 ) = 1.64 × 10 14 possible actions at every step. Hausknecht et al. [15] propose a template-based action space in which the agent first selects a template, consisting of an action verb and preposition, and then filling that in with relevant entities (e.g.</p>
<p>[get] [f rom] ). Zork1 has 237 templates, each with up to two blanks, yielding a template-action space of size O(237 × 697 2 ) = 1.15 × 10 8 . This space is still far larger than most used by previous approaches applying reinforcement learning to text-based games. We use this template action space for all games.</p>
<p>Training At every step an observation consisting of several components is received: o t = (o t desc , o tgame , o tinv , a t−1 ) corresponding to the room description, game feedback, inventory, and previous action, and total score R t . The room description o t desc is a textual description of the agent's location, obtained by executing the command "look". The game feedback o tgame is the simulators response to the agent's previous action and consists of narrative and flavor text. The inventory o tinv and previous action a t−1 components inform the agent about the contents of its inventory and the last action taken respectively.</p>
<p>Each of these components is processed using a GRU based encoder utilizing the hidden state from the previous step and combined to have a single observation embedding o t . At each step, we update our knowledge graph G t using o t as described in earlier in Section 3 and it is then embedded into a single vector g t . This encoding is based on the R-GCN and is calculated as:
g t = f   W g σ   r∈R j∈Ni r 1 c i,r W r (l) h j (l) + W 0 (l) h i (l)   + b g   (2)
Where R is the set of relations, N i r is the 1-step neighborhood of a vertex i with respect to relation r, W r (l) and h j (l) are the learnable convolutional filter weights with respect to relation r and hidden state of a vertex j in the last layer l of the R-GCN respectively, c i,r is a normalization constant, and W g and b g the weights and biases of the output linear layer. The full architecture can be found in Fig. 3. The state representation consists only of the textual observations and knowledge graph.</p>
<p>Another key use of the knowledge graph, introduced as part of KG-A2C, is the graph mask, which restricts the possible set of entities that can be predicted to fill into the action templates at every step to those found in the agent's knowledge graph. The rest of the training methodology is unchanged from Ammanabrolu and Hausknecht [3], more details can be found in Appendix A.1.</p>
<p>Structured Exploration</p>
<p>This section describes an exploration method built on top of Q*BERT that first detects bottlenecks and then searches for ways to pass them, learning policies that take it from bottleneck to bottleneck. This method of chaining policies and backtracking can be thought of in terms of options [24,25], where the agent decomposes the task of solving the text game into the sub-tasks, each of which has it's own policy. In our case, each sub-task delivers the agent to a bottleneck state.</p>
<p>Bottleneck Detection using Intrinsic Motivation</p>
<p>Examples of some bottlenecks can be seen in Figure 2 based on our definition of a bottleneck in Eq. 1. Inspired by McGovern and Barto [20], we present an intuitive way of detecting these bottleneck states-or sub-tasks-in terms of whether or not the agent's ability to collect reward stagnates. If the agent does not collect a new reward for a number of environment interactions-defined in terms of a patience parameter-then it is possible that it is stuck due to a bottleneck state. An issue with this method, however, is that the placement of rewards does not always correspond to an agent being stuck. Complicating matters, rewards are sparse and often delayed; the agent not collecting a reward for a while might simply indicate that further exploration is required instead of truly being stuck.</p>
<p>To alleviate these issues, we define an intrinsic motivation for the agent that leverages the knowledge graph being built during exploration. The motivation is for the agent to learn more information regarding the world and expand the size of its knowledge graph. This provides us with a better indication of whether an agent is stuck or not-a stuck agent does not visit any new states, learns no new information about the world, and therefore does not expand its knowledge graph-leading to more effective bottleneck detection overall. To prevent the agent from discovering reward loops based on knowledge graph changes, we formally define this reward in terms of new information learned.
r IMt = ∆(KG global − KG t ) where KG global = t−1 i=1 KG i (3)
Here KG global is the set of all edges that the agent has ever had in its knowledge graph and the subtraction operator is a set difference. When the agent adds new edges to the graph perhaps as a the result of finding a new room KG global changes and a positive reward is generated-this does not happen when that room is rediscovered in subsequent episodes. This is then scaled by the game score so the intrinsic motivation does not drown out the actual quest rewards, the overall reward the agent receives at time step t looks like this:
r t = r gt + αr IMt r gt + r max (4) Algorithm 1 Structured Exploration {πchain, π b , π} ← φ Chained, backtrack, current policy {S b , S} ← φ Backtrack, current state buffers s0, rinit ← ENV.RESET() Jmax ← rinit, p ← 0 for timestep t in 0...M do Train for M Steps st+1, rt, π ← Q<em>BERTUPDATE(st, π) S ← S + st+1
Append current state to state buffer
p ← p + 1 Lose patience if J (π) ≤ Jmax then if p &gt;= patience then Stuck at a bottleneck st, rmax, π ← BACKTRACK(π b , S b )
Bottleneck passed; Add π to the chained policy πchain ← πchain + π if J (π) &gt; Jmax then New highscore found Jmax ← J (π); π b ← π; S b ← S; p ← 0 return πchain Chained policy that reaches max score
function Q</em>BERTUPDATE(st, π) One-step update st+1, rg t ← ENV.STEP(st, π) Section 3 rt ← CALCULATEREWARD(st+1, rg t ) Eq. 4 π ← A2C.UPDATE(π, rt) Appendix A.1 return st+1, rt, π function BACKTRACK(π b , S b ) Try to overcome bottleneck for b in REVERSE(S b ) do States leading to highscore s0 ← b; π ← φ for timestep t in 0...N do Train for N steps st+1, rt, π ← Q*BERTUPDATE(st, π) if J (π) &gt; J (π b ) then return st, rt, π Terminate
Can't find better score; Give up.</p>
<p>where is a small smoothing factor, α is a scaling factor, r gt is the game reward, r max is the maximum score possible for that game, and r t is the reward received by the agent on time step t.</p>
<p>Modular Policy Chaining</p>
<p>A primary reason that agents fail to pass bottlenecks is not satisfying all the required dependencies. To solve this problem, we introduce a method of policy chaining, where the agent utilizes the determinism of the simulator to backtrack to previously visited states in order to fulfill dependencies required to overcome a bottleneck.</p>
<p>Specifically, Algorithm 1 optimizes the policy π as usual, but also keeps track of a buffer S of the distinct states and knowledge graphs that led up to each state (we use state s t to colloquially refer to the combination of an observation o t and knowledge graph KG t ). Similarly, a bottleneck buffer S b and policy π b reflect the sequence of states and policy with the maximal return J max . A bottleneck is identified when the agents fails to improve upon J max after patience number of steps, i.e. no improvement in raw game score or knowledge-graphbased intrinsic motivation reward. The agent then backtracks by searching backwards through the state sequence S b , restarting from each of the previous states-and training for N steps in search of a more optimal policy to overcome the bottleneck. When such a policy is found, it is appended to modular policy chain π chain . Conversely, if no such policy is found, then we have failed to pass the current bottleneck and the training terminates.</p>
<p>Evaluation</p>
<p>We first evaluate the quality of the knowledge graph construction in a supervised setting. Next we perform and end-to-end evaluation in which knowledge graph construction is used by Q<em>BERT.     Table 1 left shows QA performance on the Jericho-QA dataset. Exact match (EM) refers to the percentage of times the model was able to predict the exact answer string, while F1 measures token overlap between prediction and ground truth. We observe a direct correlation between the quality of the extracted graph and Q</em>BERT's performance on the games. On games where Q<em>BERT performed comparatively better than KG-A2C in terms of asymptotic scores, e.g. detective, the QA model had relatively high EM and F1, and vice versa as seen with ztuu. In general, however, Q</em>BERT reaches comparable asymptotic performance to KG-A2C on 7 out of 9 games. However, as shown in Figure 4a, Q<em>BERT reaches asymptotic performance faster than KG-A2C, indicating that the QA model leads to faster learning. Appendix B contains more plots illustrating this trend. Both agents rely on the graph to constrain the action space and provide a richer input state representation. Q</em>BERT uses a QA model fine-tuned on regularities of a text-game producing more relevant knowledge graphs than those extracted by OpenIE [8] in KG-A2C for this purpose.</p>
<p>Graph Extraction Evaluation</p>
<p>Intrinsic Motivation and Exploration Strategy Evaluation</p>
<p>We evaluate intrinsic motivation through policy chaining, dubbed MC!Q<em>BERT (Modularly Chained Q</em>BERT) by first testing policy chaining with only game reward and with both game reward and intrinsic motivation. We provide a qualitative analysis of the bottlenecks detected with both methods with respect to those found in Fig. 2 on Zork1. Just as KG-A2C provided us with a direct comparison for assessing the graph extraction abilities of Q<em>BERT, we test MC!Q</em>BERT's structured exploration against an alternative exploration method in the form of Go-Explore [12], an algorithm with similar properties to our policy module chaining and has been show to work well for large, discrete game state spaces. Further, MC!Q<em>BERT using both game reward and intrinsic motivation matches or outperforms all other methods on 8 out of 9 games, with MC!Q</em>BERT using only game reward received the highest score on the 9 th game, deephome.</p>
<p>GO!Q<em>BERT Go-Explore [12] is an algorithm designed to keep track of sub-optimal and underexplored states in order to allow the agent to explore upon more optimal states that may be a result of sparse rewards. The Go-Explore algorithm consists of two phases, the first to continuously explore until a set of promising states and corresponding trajectories are found on the basis of total score, and the second to robustify this found policy against potential stochasticity in the game. Promising states are defined as those states when explored from will likely result in higher reward trajectories. Madotto et al. [19] look at applying Go-Explore to text-games on a set of simpler games generated using the game generation framework TextWorld [10]. They use a small set of "admissible actions"-actions guaranteed to change the world state at any given step during Phase 1-to explore and find high reward trajectories. We adapt this, instead training Q</em>BERT in parallel to generate actions from the full action space used for exploration to maintain a constant action space size across all models. Implementation details are found in Appendix A.3. Table 1 shows that across all the games MC!Q<em>BERT matches or outperforms the current state-of-theart when compared across the metric of the max score consistently received across runs. There are two main trends: First, MC!Q</em>BERT greatly benefits from the inclusion of intrinsic motivation rewards. Qualitative analysis of bottlenecks detected by each agent on the game of Zork1 reveals differences in the overall accuracy of the bottleneck detection between MC!Q<em>BERT with and without intrinsic motivation. Figure 4b shows exactly when each of these agents detects and subsequently overcomes the bottlenecks outlined in Figure 2. What we see here is that when the intrinsic motivation is not used, the agent discovers that it can get to the Kitchen with a score of +10 and then Cellar with a score of +25 immediately after. It forgets how to get the Egg with a smaller score of +5 and never makes it past the Grue in the Cellar. Intrinsic motivation prevents this in two ways: (1) it makes it less focused on a locally high-reward trajectory-making it less greedy and helping it chain together rewards for the Egg and Cellar, and (2) provides rewards for fulfilling dependencies that would otherwise not be rewarded by the game-this is seen by the fact that it learns that picking up the lamp is the right way to surpass the Cellar bottleneck and reach the Painting. A similar behavior is observed with GO!Q</em>BERT, the agent settles pre-maturely on a locally high-reward trajectory and thus never has incentive to find more globally optimal trajectories by fulfilling the underlying dependency graph. Here, the likely cause is due to GO!Q*BERT's inability to backtrack and rethink discovered high reward trajectories.</p>
<p>Analysis</p>
<p>The second point is that using both the improvements to graph construction in addition to intrinsic motivation and structured exploration consistently yields higher max scores across a majority of the games when compared to the rest of the methods. Having just the improvements to graph building or structured exploration by themselves is not enough. Thus we infer that the full MC!Q<em>BERT agent is fundamentally exploring this combinatorially-sized space more effectively by virtue of being able to more consistently detect and clear bottlenecks. The improvement over systems using default exploration such as KG-A2C or Q</em>BERT by itself indicates that structured exploration is necessary when dealing with sparse and ill-placed reward functions.</p>
<p>Conclusions</p>
<p>Modern deep reinforcement learning agents using default exploration strategies such as -greedy are ill-equipped to deal with the challenge of sparse and delayed rewards, especially when placed in a combinatorially-sized state-action space. Building on top of Q<em>BERT, an agent that constructs a knowledge graph of the world by asking questions about it, we introduce MC!Q</em>BERT, an agent that uses this graph as an intrinsic motivation to help detect bottlenecks arising from delayed rewards and chains policies that go from bottleneck to bottleneck. A key insight from an ablation study is that the graph-based intrinsic motivation is crucial for bottleneck detection, preventing the agent from falling into locally optimal high reward trajectories due to ill-placed rewards. Policy chaining used in tandem with intrinsic motivation results in agents that explore further in the game by clearing bottlenecks more consistently.</p>
<p>Broader Impacts</p>
<p>The ability to plan for long-term state dependencies in partially-observable environment has downstream applications beyond playing games. We see text games as simplified analogues for systems capable of long-term dialogue with humans, such as in assistance with planning complex tasks, and also discrete planning domains such as logistics. Broadly speaking, reinforcement learning is applicable to many sequential tasks, some of which cannot be anticipated. Reinforcement learning for text environments are more suited for domains in which change in the world is affected via language, which mitigates physical risks-our line of work is not directly relevant to robotics-but not cognitive and emotional risks, as any system capable of generating natural language is capable of accidental or intentional non-normative language use [13].</p>
<p>A Implementation Details</p>
<p>We would like to preface the appendix with a discussion on the relative differences in the assumptions that Q<em>BERT and MC!Q</em>BERT make regarding the underlying environment. Although both are framed as POMDPs, MC!Q<em>BERT makes stronger assumptions regarding the determinism of the game as compared to Q</em>BERT. MC!Q<em>BERT (and GO!Q</em>BERT) rely on the fact that the set of transition probabilities in a text-game are mostly deterministic. Using this, they are able to assume that frozen policies can be executed deterministically, i.e. with no significant deviations from the original trajectory. It is possible to robustify such policies by extending our method of structured exploration to perhaps perform imitation learning on the found highest score trajectories as seen in Phase 2 of the original GoExplore algorithm [12]. Stochasticity is not among set of challenges tackled in this work, however-we focus on learning how to better explore combinatorially-sized spaces with underlying long-term dependencies. For future works in this space, we believe that agents should be compared based on the set of assumptions made: agents like KG-A2C and Q<em>BERT when operating under standard reinforcement learning assumptions, and MC!Q</em>BERT and GO!Q*BERT when under the stronger assumption of having a deterministic simulator.</p>
<p>A.1 Q*BERT</p>
<p>This section outlines how Q*BERT is trained, including details of the Jericho-QA dataset, the overall architecture, A2C training and hyperparameter details.</p>
<p>A.1.1 Jericho-QA Dataset</p>
<p>Jericho-QA contains 221453 Question-Answer pairs in the training set and 56667 pairs in the held out test set. The test set consists of all the games that we test on in this paper. It is collected by randomly exploring games using a set of admissible actions in addition to using the walkthroughs for each game as found in the Jericho framework [15]. The set of attributes for a game is taken directly from the game engine and is defined by the game developer.</p>
<p>A single sample looks like this:</p>
<p>Context:</p>
<p>[loc] Chief's Office You are standing in the chief's office. He is telling you, "The mayor was murdered yeaterday night at 12:03 am. I want you to solve it before we get any bad publicity or the FBI has to come in." "Yessir!" you reply. He hands you a sheet of paper. once you have read it, go north or west. You can see a piece of white paper here. </p>
<p>A.1.2 Knowledge Graph Update Rules</p>
<p>Every step, given the current state and possible attributes as context-the QA network predicts the current room location, the set of all inventory objects, the set of all surrounding objects, and all attributes for each object.</p>
<p>• Linking the current room type (e.g. "Kitchen", "Cellar") to the items found in the room with the relation "has", e.g. kitchen, has, lamp • All attribute information for each object is linked to the object with the relation "is". e.g. egg, is, treasure</p>
<p>• Linking all inventory objects with relation "have" to the "you" node, e.g. you, have, sword</p>
<p>• Linking rooms with directions based on the action taken to move between the rooms, e.g.</p>
<p>Behind House, east of, F orest after the action "go east" is taken to go from behind the house to the forest Below is an excerpt from Zork1 showing the exact observations given to the Q*BERT,the knowledge graph, and the corresponding action taken by the agent after the graph extraction and update process has occurred as described above for a trajectory consisting of 5 timesteps. These timesteps begin at the start of the game in West of House and continue till the agent has entered the Kitchen as seen in Fig. 2 and Fig. 7. The set of s, r, o triples that make up the graph are in the text and the figure shows a partial visualization of the graph at that particular step in the trajectory. Further details of what is found in Figure 3. The sequential action decoder consists two GRUs that are linked together as seen in Ammanabrolu and Hausknecht [3]. The first GRU decodes an action template and the second decodes objects that can be filled into the template. These objects are constrained by a graph mask, i.e. the decoder is only allowed to select entities that are already present in the knowledge graph.</p>
<p>The question answering network based on ALBERT [18] has the following hyperparameters, taken from the original paper and known to work well on the SQuAD [23] dataset. No further hyperparameter tuning was conducted.</p>
<p>Parameters</p>
<p>Value batch size 8 learning rate 3 × 10 −5 max seq len 512 doc stride 128 warmup steps 814 max steps 8144 gradient accumulation steps 24</p>
<p>A.1.4 A2C Training</p>
<p>The rest of the A2C training is unchanged from Ammanabrolu and Hausknecht [3]. A2C training starts with calculating the advantage of taking an action in a state A(s t , a t ), defined as the value of taking an action Q(s t , a t ) compared to the average value of taking all possible admissible actions in that state V (s t ):
A(s t , a t ) = Q(s t , a t ) − V (s t ) (5) Q(s t , a t ) = E<a href="6">r t + γV (s t+1 )</a>
The value is predicted by the critic as shown in Fig. 3 and r t is the reward received at step t.</p>
<p>The action decoder or actor is then updated according to the gradient:</p>
<p>−∇ θ (logπ T (τ |s t ; θ t ) + n i=1 logπ Oi (o i |s t , τ, ..., o i−1 ; θ t ))A(s t , a t )</p>
<p>updating the template policy π T and object policies π Oi based on the fact that each step in the action decoding process is conditioned on all the previously decoded portions. The critic is updated with respect to the gradient: 1 2 ∇ θ (Q(s t , a t ; θ t ) − V (s t ; θ t )) 2</p>
<p>bringing the critic's prediction of the value of being in a state closer to its true underlying value. An entropy loss is also added:</p>
<p>L E (s t , a t ; θ t ) = a∈V (st) P (a|s t )logP (a|s t )</p>
<p>Hyperparameters are taken from KG-A2C as detailed by Ammanabrolu and Hausknecht [3] and not tuned any further.</p>
<p>A.2 MC!Q*BERT</p>
<p>The additional hyperparamters used for modular policy chaining are detailed below. Patience batch factor is the proportion of the batch that must have stagnated at a particular score for patience number of episodes of unchanging score before a bottleneck is detected. Patience within a range of 1000 − 6000 in increments of 500 and buffer size within a range of 10 − 60 in increments of 10 were the only additional parameters tuned for, on Zork1. The resulting best hyperparameter set was used on the rest of the games. </p>
<p>B.2 Intrinsic Motivation and Structured Exploration Results</p>
<p>C Zork1</p>
<p>Start here Kitchen +10</p>
<p>Egg +5 Cellar +25</p>
<p>Painting +4 Figure 7: Map of Zork1 annotated with rewards taken from Ammanabrolu and Hausknecht [3] and corresponding to the states and rewards found in Figure 2.</p>
<p>North of House You are facing the north side of a white house. There is no door here, and all the windows are boarded up. To the north a narrow path winds through the trees.</p>
<p>Figure 1 :
1Excerpt from Zork1.</p>
<p>Figure 3 :
3One-step knowledge graph extraction in the Jericho-QA format, and overall Q*BERT architecture at time step t. At each step the ALBERT-QA model extracts a relevant highlighted entity set V t by answering questions based on the observation, which is used to update the knowledge graph.</p>
<p>Max reward curves for exploration strategies.</p>
<p>Figure 4 :
4Select ablation results on Zork1 conducted across 5 independent runs per experiment. We see where the agents using structured exploration pass each bottleneck seen inFig. 2. Q*BERT without IM is unable to detect nor surpass bottlenecks beyond the Cellar.</p>
<p>Figure 6 :
6Best initial reward curves for the exploration strategies.</p>
<p>natural
Behind HouseYou are behind the white house. A path leads into the forest to the east. In one corner of the house there is a small window which is slightly ajar. You are carrying: A jewel-encrusted egg Attributes: talkable, openable, animate, treasure ...You </p>
<p>egg </p>
<p>Behind </p>
<p>House 
Forest </p>
<p>North of </p>
<p>House </p>
<p>South of </p>
<p>House </p>
<p>house </p>
<p>window </p>
<p>path </p>
<p>treasure </p>
<p>open-</p>
<p>able </p>
<p>open-</p>
<p>able </p>
<p>Key: 
Locations 
Surr. Obj.s 
Inv. Obj.s 
Attributes </p>
<p>. . . </p>
<p>. 
. 
. </p>
<p>. 
. 
. </p>
<p>. . . </p>
<p>in 
east </p>
<p>is 
is </p>
<p>is </p>
<p>north </p>
<p>south </p>
<p>h a s </p>
<p>h a s 
h a s 
have </p>
<p>Observation </p>
<p>ALBERT-QA </p>
<p>R-GCN </p>
<p>KG t-1 </p>
<p>KG t </p>
<p>Update </p>
<p>V t </p>
<p>Recurrent 
Text Encoder </p>
<p>Sequential 
Action 
Decoder </p>
<p>Value 
Predictor </p>
<p>Actor </p>
<p>graph mask </p>
<p>g t </p>
<p>Value </p>
<p>Action </p>
<p>Critic </p>
<p>Q*BERT </p>
<p>Questions: 
Where am I located? 
What is here? 
What do I have? ... </p>
<p>Table 1 :
1QA results on Jericho-QA test set and averaged asymptotic scores on games by different methods across 5 independent runs. For KG-A2C and Q*BERT, we present scores averaged across the final 100 episodes as well as max scores. Methods using exploration strategies show only max scores given their workings. Agents are allowed 10 6 steps for each parallel A2C agent with a batch size of 16.</p>
<p>North of House You are facing the north side of a white house. There is no door here, and all the windows are boarded up. To the north a narrow path winds through the trees. Behind House You are behind the white house. A path leads into the forest to the east. In one corner of the house there is a small window which is slightly ajar.[inv] You are empty handed.[obs] Behind House You are behind the white house. A path leads into the forest to the east. In one corner of the house[loc] Behind House You are behind the white house. A path leads into the forest to the east. In one corner of the house there is a small window which is open. [inv] You are empty handed. [obs] With great effort, you open the window far enough to allow entry. [atr] talkable, seen, lieable, enterable, nodwarf, indoors, visited, handed, lockable, surface, thing, water_room, unlock, lost, afflicted, is_treasure, converse, mentioned, male, npcworn, no_article, relevant, scored, queryable, town, pluggable, happy, is_followable, legible, multitude, burning, room, clothing, underneath, ward_area, little, intact, animate, bled_in, supporter, readable, openable, near, nonlocal, door, plugged, sittable, toolbit, vehicle, light, lens_searchable, open, familiar, is_scroll, aimable, takeable, static, unique, concealed, vowelstart, alcoholic, bodypart, general, is_spell, full, dry_land, pushable, known, proper, inside, clean, ambiguously_plural, container, edible, treasure, can_plug, weapon, is_arrow, insubstantial, pluralname, transparent, is_coin, air_room, scenery, on, is_spell_book, burnt, burnable, auto_searched, locked, switchable, absent, rockable, beenunlocked, progressing, severed, worn, windy, stone, random, neuter, legible, female, asleep, wiped [graph] [(north_of_house, north, west), (behind_house, east, north_of_house), (you, in, behind_house), (door, is, animate), (door, in, west), (west, is, animate), (west, in, west), (mailbox, in, west), (mailbox, is, animate), (windows, in, north_of_house), (windows, is, animate), (windows, is, open), (north, is, animate), (north, in, north_of_house), (path , is, animate), (path, in, north_of_house), (trees, in, north_of_house), (trees, is, animate), (window, in, behind_house), (window, is, animate), (forest, in, behind_house), (forest, is, animate), (east, in, behind_house), ( east, is, animate)] [loc] Kitchen You are in the kitchen of the white house. A table seems to have been used recently for the preparation of food. A passage leads to the west and a dark staircase can be seen leading upward. A dark chimney leads down and to the east is a small window which is open. On the table is an elongated brown sack, smelling of hot peppers. A bottle is sitting on the table. The glass bottle contains: A quantity of water [inv] You are empty handed. [obs] Kitchen You are in the kitchen of the white house. A table seems to have been used recently for the preparation of food. A passage leads to the west and a dark staircase can be seen leading upward. A dark chimney leads down and to the east is a small window which is open. On the table is an elongated brown sack, smelling of hot peppers. A bottle is sitting on the table. The glass bottle contains: A quantity of water [atr] talkable, seen, lieable, enterable, nodwarf, indoors, visited, handed, lockable, surface, thing, water_room, unlock, lost, afflicted, is_treasure, converse, mentioned, male, npcworn, no_article, relevant, scored, queryable, town, pluggable, happy, is_followable, legible, multitude, burning, room, clothing, underneath, ward_area, little, intact, animate, bled_in, supporter, readable, openable, near, nonlocal, door, plugged, sittable, toolbit, vehicle, light, lens_searchable, open, familiar, is_scroll, aimable, takeable, static, unique, concealed, vowelstart, alcoholic, bodypart, general, is_spell, full, dry_land, pushable, known, proper, inside, clean, ambiguously_plural, container, edible, treasure, can_plug, weapon, is_arrow, insubstantial, pluralname, transparent, is_coin, air_room, scenery, on, is_spell_book, burnt, burnable, auto_searched, locked, switchable, absent, rockable, beenunlocked, progressing, severed, worn, windy, stone, random, neuter, legible, female, asleep, wiped [graph] [(north_of_house, north, west), (behind_house, east, north_of_house), (behind_house, in, kitchen), (you, in, kitchen ), (door, is, animate), (door, in, west), (west, is, animate), (west, in, west), (west, in, kitchen), (mailbox, in, west), (mailbox, is, animate), (windows, in, north_of_house), (windows, is, animate), (north, is, animate), (north, in , north_of_house), (path, is, animate), (path, in, north_of_house), (trees, in, north_of_house), (trees, is, animate), (window, in, behind_house), (window, is, animate), (forest, in, behind_house), (forest, is, animate), (east, in, behind_house), (east, is, animate), (table, in, kitchen), (table, is, animate)]] [next act] go in A.1.3 Architectureis </p>
<p>in </p>
<p>is </p>
<p>in 
in </p>
<p>is </p>
<p>door </p>
<p>animate </p>
<p>west </p>
<p>you </p>
<p>mailbox </p>
<p>[loc] West of House You are standing in an open field west of a white house, with a boarded front door. There is a small 
mailbox here. [inv] You are empty handed. 
[obs] Copyright c 1981, 1982, 1983 Infocom, Inc. All rights reserved. ZORK is a registered trademark of Infocom, Inc. 
Revision 88 / Serial number 840726 West of House You are standing in an open field west of a white house, with a 
boarded front door. There is a small mailbox here. 
[atr] talkable, seen, lieable, enterable, nodwarf, indoors, visited, handed, lockable, surface, thing, water_room, unlock, 
lost, afflicted, is_treasure, converse, mentioned, male, npcworn, no_article, relevant, scored, queryable, town, 
pluggable, happy, is_followable, legible, multitude, burning, room, clothing, underneath, ward_area, little, intact, 
animate, bled_in, supporter, readable, openable, near, nonlocal, door, plugged, sittable, toolbit, vehicle, light, 
lens_searchable, open, familiar, is_scroll, aimable, takeable, static, unique, concealed, vowelstart, alcoholic, 
bodypart, general, is_spell, full, dry_land, pushable, known, proper, inside, clean, ambiguously_plural, container, 
edible, treasure, can_plug, weapon, is_arrow, insubstantial, pluralname, transparent, is_coin, air_room, scenery, on, 
is_spell_book, burnt, burnable, auto_searched, locked, switchable, absent, rockable, beenunlocked, progressing, 
severed, worn, windy, stone, random, neuter, legible, female, asleep, wiped 
[graph] [(you, in, west), (door, is, animate), (door, in, west), (west, is, animate), (mailbox, in, west), (mailbox, is, 
animate)] 
[next act] go north 
n o rt h </p>
<p>in 
in </p>
<p>is </p>
<p>is 
in </p>
<p>in 
is </p>
<p>north_of_house </p>
<p>west </p>
<p>you 
windows </p>
<p>window </p>
<p>animate </p>
<p>north 
path </p>
<p>trees </p>
<p>[loc] North of House You are facing the north side of a white house. There is no door here, and all the windows are boarded 
up. To the north a narrow path winds through the trees. 
[inv] You are empty handed. 
[obs] [atr] talkable, seen, lieable, enterable, nodwarf, indoors, visited, handed, lockable, surface, thing, water_room, unlock, 
lost, afflicted, is_treasure, converse, mentioned, male, npcworn, no_article, relevant, scored, queryable, town, 
pluggable, happy, is_followable, legible, multitude, burning, room, clothing, underneath, ward_area, little, intact, 
animate, bled_in, supporter, readable, openable, near, nonlocal, door, plugged, sittable, toolbit, vehicle, light, 
lens_searchable, open, familiar, is_scroll, aimable, takeable, static, unique, concealed, vowelstart, alcoholic, 
bodypart, general, is_spell, full, dry_land, pushable, known, proper, inside, clean, ambiguously_plural, container, 
edible, treasure, can_plug, weapon, is_arrow, insubstantial, pluralname, transparent, is_coin, air_room, scenery, on, 
is_spell_book, burnt, burnable, auto_searched, locked, switchable, absent, rockable, beenunlocked, progressing, 
severed, worn, windy, stone, random, neuter, legible, female, asleep, wiped 
[graph] [(north_of_house, north, west), (you, in, north_of_house), (door, is, animate), (door, in, west), (west, is, animate 
), (west, in, west), (mailbox, in, west), (mailbox, is, animate), (windows, in, north_of_house), (windows, is, animate 
), (north, is, animate), (north, in, north_of_house), (path, is, animate), (path, in, north_of_house), (trees, in, 
north_of_house), (trees, is, animate)] 
[next act] go east </p>
<p>n 
o 
rt 
h </p>
<p>e a st </p>
<p>in </p>
<p>in </p>
<p>is </p>
<p>is </p>
<p>north_of_house </p>
<p>west </p>
<p>behind_house </p>
<p>you </p>
<p>window </p>
<p>animate </p>
<p>forest </p>
<p>[loc] there is a small window which is slightly ajar. 
[atr] talkable, seen, lieable, enterable, nodwarf, indoors, visited, handed, lockable, surface, thing, water_room, unlock, 
lost, afflicted, is_treasure, converse, mentioned, male, npcworn, no_article, relevant, scored, queryable, town, 
pluggable, happy, is_followable, legible, multitude, burning, room, clothing, underneath, ward_area, little, intact, 
animate, bled_in, supporter, readable, openable, near, nonlocal, door, plugged, sittable, toolbit, vehicle, light, 
lens_searchable, open, familiar, is_scroll, aimable, takeable, static, unique, concealed, vowelstart, alcoholic, 
bodypart, general, is_spell, full, dry_land, pushable, known, proper, inside, clean, ambiguously_plural, container, 
edible, treasure, can_plug, weapon, is_arrow, insubstantial, pluralname, transparent, is_coin, air_room, scenery, on, 
is_spell_book, burnt, burnable, auto_searched, locked, switchable, absent, rockable, beenunlocked, progressing, 
severed, worn, windy, stone, random, neuter, legible, female, asleep, wiped 
[graph] [(north_of_house, north, west), (behind_house, east, north_of_house), (you, in, behind_house), (door, is, animate), 
(door, in, west), (west, is, animate), (west, in, west), (you, in, behind_house), (mailbox, in, west), (mailbox, is, 
animate), (windows, in, north_of_house), (windows, is, animate), (north, is, animate), (north, in, north_of_house), ( 
path, is, animate), (path, in, north_of_house), (trees, in, north_of_house), (trees, is, animate), (window, in, 
behind_house), (window, is, animate), (forest, in, behind_house), (forest, is, animate), (east, in, behind_house), ( 
east, is, animate)] 
[next act] open window 
n o rt h </p>
<p>e a st </p>
<p>in </p>
<p>is </p>
<p>in </p>
<p>in </p>
<p>north_of_house </p>
<p>west </p>
<p>behind_house </p>
<p>you </p>
<p>window </p>
<p>open </p>
<p>path </p>
<p>n 
o 
rt 
h </p>
<p>e a s t </p>
<p>in </p>
<p>in </p>
<p>in </p>
<p>in </p>
<p>in </p>
<p>north_of_house </p>
<p>west </p>
<p>behind_house </p>
<p>kitchen </p>
<p>you </p>
<p>window </p>
<p>table </p>
<p>bottle </p>
<p>Code can be found here https://github.com/rajammanabrolu/Q-BERT Preprint. Under review. arXiv:2006.07409v1 [cs.AI] 12 Jun 2020
https://github.com/microsoft/jericho
ParametersValue patience 3000 buffer size 40 batch size 16 patience batch factor .75A.3 GO!Q<em>BERTSince the text games we are dealing with are mostly deterministic, with the exception of Zork1 in later stages, we only focus on using Phase 1 of the Go-Explore algorithm to find an optimal policy. Go-Explore maintains an archive of cells-defined as a set of states that map to a single representation-to keep track of promising states. Ecoffet et al.[12]simply encodes each cell by keeping track of the agent's position and Madotto et al.[19]use the textual observations encoded by recurrent neural network as a cell representation. We improve on this implementation by training the Q</em>BERT network in parallel, using the snapshot of the knowledge graph in conjunction with the game state to further encode the current state and use this as a cell representation. At each step, Go-Explore chooses a cell to explore at random (weighted by score to prefer more advanced cells). Q<em>BERT will run for a number of steps in each cell, for all our experiments we use a cell step size of 32, starting with the knowledge graph state and the last seen state of the game from the cell. This will generate a trajectory for the agent while further training Q</em>BERT at each iteration, creating a new representation for the knowledge graph as well as a new game state for the cell. After expanding a cell, Go-Explore will continue to sample cells by weight to continue expanding its known states. At the same time, Q*BERT will benefit from the heuristics of selecting preferred cells and be trained on promising states more often.B ResultsB.1 Graph Evaluation Results
A Adhikari, X Yuan, M.-A Côté, M Zelinka, M.-A Rondeau, R Laroche, P Poupart, J Tang, A Trischler, W L Hamilton, arXiv:2002.09127Learning dynamic knowledge graphs to generalize on text-based games. arXiv preprintA. Adhikari, X. Yuan, M.-A. Côté, M. Zelinka, M.-A. Rondeau, R. Laroche, P. Poupart, J. Tang, A. Trischler, and W. L. Hamilton. Learning dynamic knowledge graphs to generalize on text-based games. arXiv preprint arXiv:2002.09127, 2020.</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. L Adolphs, T Hofmann, arXiv:1909.01646arXiv preprintL. Adolphs and T. Hofmann. Ledeepchef: Deep reinforcement learning agent for families of text-based games. arXiv preprint arXiv:1909.01646, 2019.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. P Ammanabrolu, M Hausknecht, International Conference on Learning Representations. P. Ammanabrolu and M. Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1x6w0EtwH.</p>
<p>Transfer in deep reinforcement learning using knowledge graphs. P Ammanabrolu, M Riedl, Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13) at EMNLP. the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13) at EMNLPP. Ammanabrolu and M. Riedl. Transfer in deep reinforcement learning using knowledge graphs. In Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13) at EMNLP, 2019. URL https://www.aclweb.org/ anthology/D19-5301.</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. P Ammanabrolu, M O , Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019P. Ammanabrolu and M. O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, 2019.</p>
<p>Bringing stories alive: Generating interactive fiction worlds. P Ammanabrolu, W Cheung, D Tu, W Broniec, M O , 1st Joint Workshop on Narrative Understanding, Storylines, and Events (NUSE) at ACL. P. Ammanabrolu, W. Cheung, D. Tu, W. Broniec, and M. O. Riedl. Bringing stories alive: Generating interactive fiction worlds. In 1st Joint Workshop on Narrative Understanding, Storylines, and Events (NUSE) at ACL, 2020. URL https://arxiv.org/abs/2001.10161.</p>
<p>. T Anderson, M Blank, B Daniels, D Lebling, Zork, T. Anderson, M. Blank, B. Daniels, and D. Lebling. Zork. http://ifdb.tads.org/ viewgame?id=4gxk83ja4twckm6j, 1979.</p>
<p>Leveraging Linguistic Structure For Open Domain Information Extraction. G Angeli, J Premkumar, M Jose, C D Manning, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing1G. Angeli, J. Premkumar, M. Jose, and C. D. Manning. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2015.</p>
<p>Increasing the action gap: New operators for reinforcement learning. M G Bellemare, G Ostrovski, A Guez, P Thomas, R Munos, AAAI Conference on Artificial Intelligence. M. G. Bellemare, G. Ostrovski, A. Guez, P. Thomas, and R. Munos. Increasing the ac- tion gap: New operators for reinforcement learning. In AAAI Conference on Artificial In- telligence, 2016. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/ view/12428/11761.</p>
<p>Textworld: A learning environment for text-based games. M.-A Côté, A Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, abs/1806.11532CoRRM.-A. Côté, A. Kádár, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, abs/1810.04805CoRRJ. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.</p>
<p>Go-explore: a new approach for hard-exploration problems. A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, abs/1901.10995CoRRA. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for hard-exploration problems. CoRR, abs/1901.10995, 2019.</p>
<p>Learning norms from stories: A prior for value aligned agents. S Frazier, M O Al Nahian, Md Sultan Riedl, B Harrison, abs/1912.03553CoRRS. Frazier, M. O. Al Nahian, Md Sultan Riedl, and B. Harrison. Learning norms from stories: A prior for value aligned agents. CoRR, abs/1912.03553, 2019.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. N Fulda, D Ricks, B Murdoch, D Wingate, 10.24963/ijcai.2017/144IJCAI. N. Fulda, D. Ricks, B. Murdoch, and D. Wingate. What can you do with a rock? affordance extraction via word embeddings. In IJCAI, pages 1039-1045, 2017. doi: 10.24963/ijcai.2017/ 144.</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI). M. Hausknecht, P. Ammanabrolu, M.-A. Côté, and X. Yuan. Interactive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020. URL https://arxiv.org/abs/1909.05398.</p>
<p>Deep reinforcement learning with a natural language action space. J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, ACL. J. He, J. Chen, X. He, J. Gao, L. Li, L. Deng, and M. Ostendorf. Deep reinforcement learning with a natural language action space. In ACL, 2016.</p>
<p>Algorithmic improvements for deep reinforcement learning applied to interactive fiction. CoRR, abs. V Jain, W Fedus, H Larochelle, D Precup, M G Bellemare, V. Jain, W. Fedus, H. Larochelle, D. Precup, and M. G. Bellemare. Algorithmic improvements for deep reinforcement learning applied to interactive fiction. CoRR, abs/1911.12511, 2019.</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, International Conference on Learning Representations. Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS.</p>
<p>Exploration based language learning for text-based games. A Madotto, M Namazifar, J Huizinga, P Molino, A Ecoffet, H Zheng, A Papangelis, D Yu, C Khatri, G Tur, abs/2001.08868CoRRA. Madotto, M. Namazifar, J. Huizinga, P. Molino, A. Ecoffet, H. Zheng, A. Papangelis, D. Yu, C. Khatri, and G. Tur. Exploration based language learning for text-based games. CoRR, abs/2001.08868, 2020.</p>
<p>Automatic discovery of subgoals in reinforcement learning using diverse density. A Mcgovern, A G Barto, A. McGovern and A. G. Barto. Automatic discovery of subgoals in reinforcement learning using diverse density. 2001.</p>
<p>K Murugesan, M Atzeni, P Shukla, M Sachan, P Kapanipathi, K Talamadupula, arXiv:2005.00811Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprintK. Murugesan, M. Atzeni, P. Shukla, M. Sachan, P. Kapanipathi, and K. Talamadupula. Enhanc- ing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811, 2020.</p>
<p>Language understanding for text-based games using deep reinforcement learning. K Narasimhan, T D Kulkarni, R Barzilay, EMNLP. K. Narasimhan, T. D. Kulkarni, and R. Barzilay. Language understanding for text-based games using deep reinforcement learning. In EMNLP, pages 1-11, 2015.</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas, Nov. 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/ anthology/D16-1264.</p>
<p>Learning options in reinforcement learning. M Stolle, D Precup, Proceedings of the 5th International Symposium on Abstraction, Reformulation and Approximation. the 5th International Symposium on Abstraction, Reformulation and ApproximationBerlin, HeidelbergSpringer-VerlagISBN 3540439412M. Stolle and D. Precup. Learning options in reinforcement learning. In Proceedings of the 5th International Symposium on Abstraction, Reformulation and Approximation, page 212-223, Berlin, Heidelberg, 2002. Springer-Verlag. ISBN 3540439412.</p>
<p>Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. R S Sutton, D Precup, S Singh, Artificial intelligence. 1121-2R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999.</p>
<p>Learning to speak and act in a fantasy text adventure game. J Urbanek, A Fan, S Karamcheti, S Jain, S Humeau, E Dinan, T Rocktäschel, D Kiela, A Szlam, J Weston, abs/1903.03094CoRRJ. Urbanek, A. Fan, S. Karamcheti, S. Jain, S. Humeau, E. Dinan, T. Rocktäschel, D. Kiela, A. Szlam, and J. Weston. Learning to speak and act in a fantasy text adventure game. CoRR, abs/1903.03094, 2019.</p>
<p>Comprehensible context-driven text game playing. CoRR, abs. X Yin, J May, X. Yin and J. May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.</p>
<p>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games. X Yin, J May, arXiv:1908.04777arXiv preprintX. Yin and J. May. Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games. arXiv preprint arXiv:1908.04777, 2019.</p>
<p>Counting to explore and generalize in text-based games. X Yuan, M Côté, A Sordoni, R Laroche, R T Des Combes, M J Hausknecht, A Trischler, abs/1806.11525CoRRX. Yuan, M. Côté, A. Sordoni, R. Laroche, R. T. des Combes, M. J. Hausknecht, and A. Trischler. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525, 2018.</p>
<p>Interactive language learning by question answering. X Yuan, M.-A Côté, J Fu, Z Lin, C Pal, Y Bengio, A Trischler, EMNLP. X. Yuan, M.-A. Côté, J. Fu, Z. Lin, C. Pal, Y. Bengio, and A. Trischler. Interactive language learning by question answering. In EMNLP, 2019.</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. T Zahavy, M Haroush, N Merlis, D J Mankowitz, S Mannor, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Pro- cessing Systems 31, pages 3562-3573. Curran Associates, Inc., 2018.</p>
<p>Using reinforcement learning to learn how to play text-based games. M Zelinka, abs/1801.01999CoRRM. Zelinka. Using reinforcement learning to learn how to play text-based games. CoRR, abs/1801.01999, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
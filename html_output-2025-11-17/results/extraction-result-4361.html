<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4361 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4361</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4361</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-267627030</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.07770v1.pdf" target="_blank">Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been extensively studied for their ability to generate convincing natural language sequences; however, their utility for quantitative information retrieval is less well understood. Here, we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid two data analysis tasks: elicitation of prior distributions for Bayesian models and imputation of missing data. We introduce a framework that leverages LLMs to enhance Bayesian workflows by eliciting expert‐like prior knowledge and imputing missing data. Tested on diverse datasets, this approach can improve predictive accuracy and reduce data requirements, offering significant potential in healthcare, environmental science and engineering applications. We discuss the implications and challenges of treating LLMs as ‘experts’.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4361.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4361.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Bayesian Elicitation & Imputation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Prior Elicitation and Zero-shot Missing Data Imputation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting and serialization pipeline that treats pretrained LLMs as domain 'experts' to (a) elicit parametric Bayesian prior distributions and (b) perform zero-shot imputation of missing tabular values by serializing rows to natural language and instructing the model to return parseable numeric or distributional outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Expert Prompt Initialization (EPI) + Task Specification + Data Serialization zero-shot LLM imputer</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline components: (1) Expert Prompt Initialization (EPI) — a system prompt (generated once per task using the LLM) that instructs the model to roleplay a detailed domain persona ('You are a ...') for elicitation; (2) Task Specification — a strict task prompt that enforces an elicitation protocol and requires the LLM to return programmatically parseable outputs (e.g., parametric priors like Beta(a,b) or single numeric imputed values) and to avoid generic advice; (3) Data Serialization — deterministic conversion of tabular row context into short natural-language sentences of the form 'The {variable} is {value}', omitting missing features; (4) Zero-shot operation — for prior elicitation no observed data are provided, for imputation only same-row observed features are provided; (5) Inference settings: deterministic decoding (temperature=0) to reduce stochasticity. The framework enforces output formats and evaluates priors using prior ESS, lppd/CRPS and imputation with NRMSE / macro-F1 and downstream classifier performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama 2 13B Chat, Llama 2 70B Chat, Mistral 7B Instruct, Mixtral 8x7B Instruct (evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multiple domains (healthcare, environmental science, engineering, social sciences, education, biology, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>parametric prior distributions for model parameters (e.g., Beta, Normal, Student-t) and imputed numeric/categorical feature values (not discovery of new physical laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>parametric distribution expressions (e.g., Beta(a,b), Normal(mu,sigma)) and scalar imputed values in a parseable textual format</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical evaluation on real-world datasets: prior informativeness/capacity measured with prior ESS and prior predictive metrics (lppd/CRPS/MSE) against historical/weather data; imputation quality measured by upstream NRMSE / RMSE and macro-F1 and downstream performance measured via a random forest classifier on imputed datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported summary: LLM-elicited priors showed varying informativeness (GPT-family generally more informative; Mistral sometimes produced ESS >= 1000); on imputation tasks LLM methods often underperformed mean/mode baselines upstream and were outperformed by KNN and random-forest imputers on RMSE and macro-F1, while downstream classification performance narrowed the gap (e.g., Llama 2 70B outperformed mean baseline in 4/5 domains); precise numeric results are shown in the paper's Figures 2–7.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared priors against human-elicited priors (Stefan et al. 2022) and simple conjugate statistical models (normal-inverse-gamma for temperature, gamma-exponential for precipitation) for ESS/predictive comparisons; imputation compared to mean/mode, k-NN and random-forest imputers — KNN/RF consistently outperformed LLMs on upstream metrics, while some LLMs beat mean imputation on downstream metrics in limited domains.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Large inter-model variability; underestimation of uncertainty and mode collapse in outputs; sensitivity to metadata and feature descriptions (poor metadata caused large errors); potential data leakage/task contamination for publicly-known datasets; limited ability to aggregate across dataset rows (imputation uses only same-row features), inconsistent calibration, and domain-dependent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4361.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMs-as-KB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm showing that pretrained language models can serve as unsupervised knowledge stores, answering factual queries about entities encoded in their weights; cited as evidence that LLMs may retrieve latent information from their training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Models as Knowledge Bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompt-based factual retrieval from pretrained LMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use cloze/prompt queries on a pretrained language model to recover facts encoded in model parameters (i.e., read-off factual triples or attribute values without an external knowledge graph or retrieval augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / NLP (applies across domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>factual numeric facts or attributes (e.g., dates, numeric properties), not explicit discovery of mathematical laws in the cited usage</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual/cloze answers (slot-filling), single-value responses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LMs may encode incorrect or biased facts, struggle with calibrated uncertainty and with generating distributions over values; limitations in numeracy and reliability noted in the manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4361.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLP-for-Materials-IE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of NLP and information extraction pipelines to mine large scientific text corpora for quantitative materials data and measurements, to populate structured datasets used in materials research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NLP information-extraction pipelines for literature</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply information-extraction and NLP methods to scientific articles to extract quantitative measurements (e.g., material properties), experimental conditions and structured metadata, producing structured tables for downstream quantitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / applied physics / chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>quantitative measurements and empirical relationships (material properties, numeric experimental results) extracted from text</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>structured data tables and extracted numeric fields</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed in this paper; in general IE pipelines need rich metadata, disambiguation and units handling and can suffer from extraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4361.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Structural Priors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses LLMs to curate structural information (priors over causal links/graph structure) from text to inform causal model construction and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-generated structural priors for causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use language models to read/parse literature and propose structural priors (e.g., likely edges, variable relationships) to be used as prior information in causal inference workflows or structural causal models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>causal inference / multiple applied domains where causal structure is needed</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>causal relationships / structural priors (graph edges, causal links)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>graph-structured priors / statements of conditional relationships</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper cites general concerns about domain mismatch, contamination and reliability of priors generated from large pretraining corpora; details not provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4361.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>scite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>scite: A Smart Citation Index That Displays the Context of Citations and Classifies Their Intent Using Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning based citation-indexing system that classifies citation intent and surfaces citation contexts to help users assess relationships among papers and thereby distill scientific claims and relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>scite: A Smart Citation Index That Displays the Context of Citations and Classifies Their Intent Using Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Citation-context classification via deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply deep learning to citation sentences in scholarly texts to classify citation intent (e.g., supporting, contrasting, background) and present context to users, aiding distillation of relationships and claims across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scholarly communication / bibliometrics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>indirectly supports discovery of relationships by classifying citation intents and contexts (not direct extraction of mathematical laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>classified citation contexts and metadata (textual labels)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Helps contextualise citations but does not itself produce symbolic quantitative laws; dependent on quality of classification and context extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4361.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated Prior Elicitation (others)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited that uses LLMs to automate elicitation of priors for Bayesian logistic regression models (references in manuscript indicate initial explorations in this direction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted automated prior elicitation for Bayesian models</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use LLMs to produce prior distributions or generate pseudodata to inform priors for Bayesian logistic regression; specific pipeline details are not reported in the present manuscript beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian statistics / applied domains using logistic regression</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>parametric priors for model coefficients (informative priors)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>parametric prior expressions or pseudodata</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as related work; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4361.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pseudodata Prior Elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pseudodata generation for indirect prior elicitation (Gouk and Gao 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach (cited) that generates pseudodata from LLMs as an indirect mechanism to form priors for Bayesian models, instead of directly eliciting parametric priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-generated pseudodata for prior construction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Generate synthetic/pseudodata with an LLM trained on relevant literature or prompts, then fit conventional statistical priors from the pseudodata for use in Bayesian inference; the present manuscript cites this approach as alternative to direct parametric elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Bayesian modelling / general applied domains</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>empirically-derived prior distributions obtained from pseudodata</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>simulated dataset (tabular) and derived parametric priors</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed here; potential issues include fidelity of pseudodata to real-world distributions and contamination from training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4361.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4361.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-driven elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eliciting Human Preferences With Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of language models to assist or automate aspects of elicitation workflows (e.g., generating questions, aggregating expert opinions or simulating experts) to produce priors or preference models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Eliciting Human Preferences With Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-assisted elicitation of human judgements/preferences</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use LLMs as facilitators or simulators in elicitation exercises to generate candidate priors, questionnaires, or to assist human experts in articulating distributions; cited as interactive work where LLMs support human elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>human preference elicitation / decision analysis / Bayesian elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>probability distributions representing human beliefs or preferences (priors)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>elicited probability distributions, questionnaires, or aggregated judgements</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Works may involve human-in-the-loop and are sensitive to prompt wording and model alignment; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction <em>(Rating: 2)</em></li>
                <li>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes <em>(Rating: 2)</em></li>
                <li>Language Models as Knowledge Bases? <em>(Rating: 2)</em></li>
                <li>Eliciting Human Preferences With Language Models <em>(Rating: 2)</em></li>
                <li>Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression <em>(Rating: 2)</em></li>
                <li>Table-GPT: Table-Tuned GPT for Diverse Table Tasks <em>(Rating: 1)</em></li>
                <li>Jellyfish: A Large Language Model for Data Preprocessing <em>(Rating: 1)</em></li>
                <li>GATGPT: A Pre-Trained Large Language Model With Graph Attention Network for Spatiotemporal Imputation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4361",
    "paper_id": "paper-267627030",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "LLM Bayesian Elicitation & Imputation Framework",
            "name_full": "LLM-based Prior Elicitation and Zero-shot Missing Data Imputation Framework",
            "brief_description": "A prompting and serialization pipeline that treats pretrained LLMs as domain 'experts' to (a) elicit parametric Bayesian prior distributions and (b) perform zero-shot imputation of missing tabular values by serializing rows to natural language and instructing the model to return parseable numeric or distributional outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Expert Prompt Initialization (EPI) + Task Specification + Data Serialization zero-shot LLM imputer",
            "method_description": "Pipeline components: (1) Expert Prompt Initialization (EPI) — a system prompt (generated once per task using the LLM) that instructs the model to roleplay a detailed domain persona ('You are a ...') for elicitation; (2) Task Specification — a strict task prompt that enforces an elicitation protocol and requires the LLM to return programmatically parseable outputs (e.g., parametric priors like Beta(a,b) or single numeric imputed values) and to avoid generic advice; (3) Data Serialization — deterministic conversion of tabular row context into short natural-language sentences of the form 'The {variable} is {value}', omitting missing features; (4) Zero-shot operation — for prior elicitation no observed data are provided, for imputation only same-row observed features are provided; (5) Inference settings: deterministic decoding (temperature=0) to reduce stochasticity. The framework enforces output formats and evaluates priors using prior ESS, lppd/CRPS and imputation with NRMSE / macro-F1 and downstream classifier performance.",
            "llm_model_used": "Llama 2 13B Chat, Llama 2 70B Chat, Mistral 7B Instruct, Mixtral 8x7B Instruct (evaluated separately)",
            "scientific_domain": "multiple domains (healthcare, environmental science, engineering, social sciences, education, biology, etc.)",
            "number_of_papers": null,
            "type_of_quantitative_law": "parametric prior distributions for model parameters (e.g., Beta, Normal, Student-t) and imputed numeric/categorical feature values (not discovery of new physical laws)",
            "extraction_output_format": "parametric distribution expressions (e.g., Beta(a,b), Normal(mu,sigma)) and scalar imputed values in a parseable textual format",
            "validation_method": "Empirical evaluation on real-world datasets: prior informativeness/capacity measured with prior ESS and prior predictive metrics (lppd/CRPS/MSE) against historical/weather data; imputation quality measured by upstream NRMSE / RMSE and macro-F1 and downstream performance measured via a random forest classifier on imputed datasets.",
            "performance_metrics": "Reported summary: LLM-elicited priors showed varying informativeness (GPT-family generally more informative; Mistral sometimes produced ESS &gt;= 1000); on imputation tasks LLM methods often underperformed mean/mode baselines upstream and were outperformed by KNN and random-forest imputers on RMSE and macro-F1, while downstream classification performance narrowed the gap (e.g., Llama 2 70B outperformed mean baseline in 4/5 domains); precise numeric results are shown in the paper's Figures 2–7.",
            "baseline_comparison": "Compared priors against human-elicited priors (Stefan et al. 2022) and simple conjugate statistical models (normal-inverse-gamma for temperature, gamma-exponential for precipitation) for ESS/predictive comparisons; imputation compared to mean/mode, k-NN and random-forest imputers — KNN/RF consistently outperformed LLMs on upstream metrics, while some LLMs beat mean imputation on downstream metrics in limited domains.",
            "challenges_limitations": "Large inter-model variability; underestimation of uncertainty and mode collapse in outputs; sensitivity to metadata and feature descriptions (poor metadata caused large errors); potential data leakage/task contamination for publicly-known datasets; limited ability to aggregate across dataset rows (imputation uses only same-row features), inconsistent calibration, and domain-dependent performance.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4361.0",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LMs-as-KB",
            "name_full": "Language Models as Knowledge Bases?",
            "brief_description": "A paradigm showing that pretrained language models can serve as unsupervised knowledge stores, answering factual queries about entities encoded in their weights; cited as evidence that LLMs may retrieve latent information from their training corpora.",
            "citation_title": "Language Models as Knowledge Bases?",
            "mention_or_use": "mention",
            "method_name": "Prompt-based factual retrieval from pretrained LMs",
            "method_description": "Use cloze/prompt queries on a pretrained language model to recover facts encoded in model parameters (i.e., read-off factual triples or attribute values without an external knowledge graph or retrieval augmentation).",
            "llm_model_used": null,
            "scientific_domain": "general / NLP (applies across domains)",
            "number_of_papers": null,
            "type_of_quantitative_law": "factual numeric facts or attributes (e.g., dates, numeric properties), not explicit discovery of mathematical laws in the cited usage",
            "extraction_output_format": "textual/cloze answers (slot-filling), single-value responses",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "LMs may encode incorrect or biased facts, struggle with calibrated uncertainty and with generating distributions over values; limitations in numeracy and reliability noted in the manuscript.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.1",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "NLP-for-Materials-IE",
            "name_full": "Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction",
            "brief_description": "Use of NLP and information extraction pipelines to mine large scientific text corpora for quantitative materials data and measurements, to populate structured datasets used in materials research.",
            "citation_title": "Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction",
            "mention_or_use": "mention",
            "method_name": "NLP information-extraction pipelines for literature",
            "method_description": "Apply information-extraction and NLP methods to scientific articles to extract quantitative measurements (e.g., material properties), experimental conditions and structured metadata, producing structured tables for downstream quantitative analysis.",
            "llm_model_used": null,
            "scientific_domain": "materials science / applied physics / chemistry",
            "number_of_papers": null,
            "type_of_quantitative_law": "quantitative measurements and empirical relationships (material properties, numeric experimental results) extracted from text",
            "extraction_output_format": "structured data tables and extracted numeric fields",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not detailed in this paper; in general IE pipelines need rich metadata, disambiguation and units handling and can suffer from extraction errors.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.2",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLM Structural Priors",
            "name_full": "Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes",
            "brief_description": "An approach that uses LLMs to curate structural information (priors over causal links/graph structure) from text to inform causal model construction and inference.",
            "citation_title": "Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes",
            "mention_or_use": "mention",
            "method_name": "LLM-generated structural priors for causal discovery",
            "method_description": "Use language models to read/parse literature and propose structural priors (e.g., likely edges, variable relationships) to be used as prior information in causal inference workflows or structural causal models.",
            "llm_model_used": null,
            "scientific_domain": "causal inference / multiple applied domains where causal structure is needed",
            "number_of_papers": null,
            "type_of_quantitative_law": "causal relationships / structural priors (graph edges, causal links)",
            "extraction_output_format": "graph-structured priors / statements of conditional relationships",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Paper cites general concerns about domain mismatch, contamination and reliability of priors generated from large pretraining corpora; details not provided in this manuscript.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.3",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "scite",
            "name_full": "scite: A Smart Citation Index That Displays the Context of Citations and Classifies Their Intent Using Deep Learning",
            "brief_description": "A deep-learning based citation-indexing system that classifies citation intent and surfaces citation contexts to help users assess relationships among papers and thereby distill scientific claims and relationships.",
            "citation_title": "scite: A Smart Citation Index That Displays the Context of Citations and Classifies Their Intent Using Deep Learning",
            "mention_or_use": "mention",
            "method_name": "Citation-context classification via deep learning",
            "method_description": "Apply deep learning to citation sentences in scholarly texts to classify citation intent (e.g., supporting, contrasting, background) and present context to users, aiding distillation of relationships and claims across the literature.",
            "llm_model_used": null,
            "scientific_domain": "scholarly communication / bibliometrics",
            "number_of_papers": null,
            "type_of_quantitative_law": "indirectly supports discovery of relationships by classifying citation intents and contexts (not direct extraction of mathematical laws)",
            "extraction_output_format": "classified citation contexts and metadata (textual labels)",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Helps contextualise citations but does not itself produce symbolic quantitative laws; dependent on quality of classification and context extraction.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.4",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Automated Prior Elicitation (others)",
            "name_full": "Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression",
            "brief_description": "Prior work cited that uses LLMs to automate elicitation of priors for Bayesian logistic regression models (references in manuscript indicate initial explorations in this direction).",
            "citation_title": "Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression",
            "mention_or_use": "mention",
            "method_name": "LLM-assisted automated prior elicitation for Bayesian models",
            "method_description": "Use LLMs to produce prior distributions or generate pseudodata to inform priors for Bayesian logistic regression; specific pipeline details are not reported in the present manuscript beyond the citation.",
            "llm_model_used": null,
            "scientific_domain": "Bayesian statistics / applied domains using logistic regression",
            "number_of_papers": null,
            "type_of_quantitative_law": "parametric priors for model coefficients (informative priors)",
            "extraction_output_format": "parametric prior expressions or pseudodata",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Mentioned as related work; specifics not detailed in this paper.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.5",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Pseudodata Prior Elicitation",
            "name_full": "Pseudodata generation for indirect prior elicitation (Gouk and Gao 2024)",
            "brief_description": "An approach (cited) that generates pseudodata from LLMs as an indirect mechanism to form priors for Bayesian models, instead of directly eliciting parametric priors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "method_name": "LLM-generated pseudodata for prior construction",
            "method_description": "Generate synthetic/pseudodata with an LLM trained on relevant literature or prompts, then fit conventional statistical priors from the pseudodata for use in Bayesian inference; the present manuscript cites this approach as alternative to direct parametric elicitation.",
            "llm_model_used": null,
            "scientific_domain": "Bayesian modelling / general applied domains",
            "number_of_papers": null,
            "type_of_quantitative_law": "empirically-derived prior distributions obtained from pseudodata",
            "extraction_output_format": "simulated dataset (tabular) and derived parametric priors",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Not detailed here; potential issues include fidelity of pseudodata to real-world distributions and contamination from training corpora.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.6",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLM-driven elicitation",
            "name_full": "Eliciting Human Preferences With Language Models",
            "brief_description": "Use of language models to assist or automate aspects of elicitation workflows (e.g., generating questions, aggregating expert opinions or simulating experts) to produce priors or preference models.",
            "citation_title": "Eliciting Human Preferences With Language Models",
            "mention_or_use": "mention",
            "method_name": "LLM-assisted elicitation of human judgements/preferences",
            "method_description": "Use LLMs as facilitators or simulators in elicitation exercises to generate candidate priors, questionnaires, or to assist human experts in articulating distributions; cited as interactive work where LLMs support human elicitation.",
            "llm_model_used": null,
            "scientific_domain": "human preference elicitation / decision analysis / Bayesian elicitation",
            "number_of_papers": null,
            "type_of_quantitative_law": "probability distributions representing human beliefs or preferences (priors)",
            "extraction_output_format": "elicited probability distributions, questionnaires, or aggregated judgements",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Works may involve human-in-the-loop and are sensitive to prompt wording and model alignment; specifics not provided here.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4361.7",
            "source_info": {
                "paper_title": "Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction",
            "rating": 2,
            "sanitized_title": "datadriven_materials_research_enabled_by_natural_language_processing_and_information_extraction"
        },
        {
            "paper_title": "Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes",
            "rating": 2,
            "sanitized_title": "leveraging_llmgenerated_structural_prior_for_causal_inference_with_concurrent_causes"
        },
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 2,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "Eliciting Human Preferences With Language Models",
            "rating": 2,
            "sanitized_title": "eliciting_human_preferences_with_language_models"
        },
        {
            "paper_title": "Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression",
            "rating": 2,
            "sanitized_title": "automated_prior_elicitation_from_large_language_models_for_bayesian_logistic_regression"
        },
        {
            "paper_title": "Table-GPT: Table-Tuned GPT for Diverse Table Tasks",
            "rating": 1,
            "sanitized_title": "tablegpt_tabletuned_gpt_for_diverse_table_tasks"
        },
        {
            "paper_title": "Jellyfish: A Large Language Model for Data Preprocessing",
            "rating": 1,
            "sanitized_title": "jellyfish_a_large_language_model_for_data_preprocessing"
        },
        {
            "paper_title": "GATGPT: A Pre-Trained Large Language Model With Graph Attention Network for Spatiotemporal Imputation",
            "rating": 1,
            "sanitized_title": "gatgpt_a_pretrained_large_language_model_with_graph_attention_network_for_spatiotemporal_imputation"
        }
    ],
    "cost": 0.0199015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models</p>
<p>David Selby david.selby@dfki.de 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Yuichiro Iwashita 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Osaka Metropolitan University
OsakaJapan |</p>
<p>Kai Spriestersbach 
DFKI GmbH
KaiserslauternGermany |</p>
<p>| Mohammad Saad 
DFKI GmbH
KaiserslauternGermany |</p>
<p>| Dennis Bappert 
Amazon Web Services
SeattleWashingtonUSA |</p>
<p>Archana Warrier 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Sumantrak Mukherjee 
DFKI GmbH
KaiserslauternGermany |</p>
<p>| Koichi Kise 
DFKI GmbH
KaiserslauternGermany |</p>
<p>Osaka Metropolitan University
OsakaJapan |</p>
<p>Sebastian Vollmer 
DFKI GmbH
KaiserslauternGermany |</p>
<p>University of Kaiserslautern-Landau
KaiserslauternGermany</p>
<p>Had Enough of Experts? Quantitative Knowledge Retrieval From Large Language Models
96EB11E40FA6E5E4E483D3595DEE10D310.1002/sta4.70054Received: 31 December 2024 | Revised: 6 February 2025 | Accepted: 17 February 2025Bayesian modelsexpert systemslarge language modelsmissing data imputationprior elicitation
Large language models (LLMs) have been extensively studied for their ability to generate convincing natural language sequences; however, their utility for quantitative information retrieval is less well understood.Here, we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid two data analysis tasks: elicitation of prior distributions for Bayesian models and imputation of missing data.We introduce a framework that leverages LLMs to enhance Bayesian workflows by eliciting expert-like prior knowledge and imputing missing data.Tested on diverse datasets, this approach can improve predictive accuracy and reduce data requirements, offering significant potential in healthcare, environmental science and engineering applications.We discuss the implications and challenges of treating LLMs as 'experts'.</p>
<p>| Introduction</p>
<p>Automated solutions for life sciences, industrial and governmental processes demand large amounts of data, which are not always available or complete.Small datasets are vulnerable to overfitting, weakening the validity, reliability and generalizability of statistical insights.To overcome these limitations, analysts employ two approaches.Data-based or empirical methods maximize information extraction, through imputation models, data augmentation and transfer learning; however, this is limited by the size, availability and representativeness of the training set.Alternatively, one can exploit prior information, via knowledge graphs or expert-elicited Bayesian priors, allowing for sparser models and handling of missing values.This approach is constrained by the difficulty, cost and myriad different methods of obtaining and eliciting subjective and heterogeneous opinions from experts, then translating them into a form amenable to quantitative analysis (Falconer et al. 2022).</p>
<p>Large language models (LLMs) are generative models capable of producing natural language texts based on a given prompt or context.LLMs such as GPT-4 have been used in various applications, such as chatbots, summarization and content creation.In the quantitative sciences, LLMs have been applied to mostly qualitative tasks such as code completion, teaching of mathematical concepts (Wardat et al. 2023) and offering advice on modelling workflows or explaining data preparation pipelines (Barberio 2023;Hassani and Silva 2023).Some work has also applied LLMs to mathematical reasoning and symbolic logic (He-Yueya et al. 2023;Orrù et al. 2023).When linked with certain application programming interfaces (APIs), or incorporated into a retrieval-augmented generation (RAG) tool, some LLM frameworks (e.g.Ge et al. 2023) are also capable of evaluating code, connecting to other data analysis tools or looking up supporting information (Nicholson et al. 2021;Kamalloo et al. 2023).However, the capabilities of LLMs to retrieve accurate and reliable quantitative information are less well explored.</p>
<p>Stat, 2025</p>
<p>Here, we explore the use of LLMs for missing value imputation and for prior elicitation.Data imputation is the process of replacing missing entries of a variable with substituted values based on empirical or informed estimates of the distribution (Donders et al. 2006).Prior elicitation is eliciting knowledge from a domain expert to be converted into a prior distribution for use in a probabilistic Bayesian model (Mikkola et al. 2023).</p>
<p>Can LLMs be considered 'experts', having read a large sample of the scientific literature in their training corpora, and thus treated as an accessible interface to this knowledge?We develop a prompting methodology to elicit prior distributions from LLMs, emulating real-world elicitation protocols.LLM-elicited priors are compared with those from human experts, and the LLM 'expertise' is quantitatively evaluated for several tasks.We also present a zero-shot missing data imputation framework, based on LLMs playing 'expert' roles derived from metadata such as the dataset description.An empirical evaluation of imputation quality and impact on downstream tasks compares LLMs with baseline approaches on a diverse set of 50 real-world datasets.Analysis code is available on GitHub.</p>
<p>| Related Work</p>
<p>Language models have been noted for their remarkable ability to act as unsupervised knowledge bases (Petroni et al. 2019).Noever and McKee (2023) and Cheng and Zhang (2023) discuss the 'emergent' numeracy skills of LLMs, from early models unable to perform simple addition to later versions able to compute correlations.Hopkins et al. (2023) showed that repeated sampling from LLMs does not yield reasonable distributions of random numbers, making them poor data generators.Xiong et al. (2023) also suggested LLMs tend to underestimate uncertainty.It has been hypothesized that mode collapse inhibits the diversity of outputs (Anonymous 2023).The design, adaptation and use of LLMs to assist data analysis is a broad topic.</p>
<p>Many LLM-based data science tools focus on tasks such as code generation for analysis scripts (Megahed et al. 2023) or connection with external APIs (Ge et al. 2023).A conversation with a chatbot can also offer generic advice on data science practices.Ahmad et al. (2023) proposed a data cleaning model that combines a fine-tuned foundation model augmented with retrieval from a user-supplied data lake.Here, however, we are interested in evaluating the intrinsic ability of an LLM to retrieve latent quantitative information directly, that is, not to perform mathematical operations on an input dataset nor to offer code or advice on how to do so, rather to offer educated numerical suggestions based on its large training corpus containing specialist technical knowledge.</p>
<p>There is some promise in converting data into natural language inputs for an LLM to perform preprocessing: Narayan et al. (2022) tested GPT-3 on entity matching, error detection and data imputation tasks, in zero-shot and few-shot settings.Their approach serialized tabular data and tasks into natural language using manually tuned prompt templates.Vos et al. (2022) explored prefix tuning as an alternative to full fine-tuning of an LLM for such tasks, whereas H. Zhang et al. (2023b) compared GPT-3.5, GPT-4 and Vicuna-13B in a data preprocessing framework, later developing Jellyfish-13B, an open-source LLM fine-tuned specifically for data preprocessing (H.Zhang et al. 2023a).P. Li et al.'s (2023) 2023) utilized fine-tuning in tandem with a graph attention mechanism to impute spatiotemporal data.Nazir et al. (2023) further explored the capability of ChatGPT in missing value imputation, focussing on imputation quality (see Section 3.5) in psychological and biological data.Conversely, Hayat and Hasan (2024) investigated the role of contextual information in ChatGPT's handling of missing values, but only for downstream LLM-based tasks.An alternative approach to LLM-assisted data analysis involves using only the model's encoder to project natural language representations into a latent space, then performing anomaly detection on this embedding (Lopatecki et al. 2023a(Lopatecki et al. , 2023b)).Tabular representation learning is an active area of research (Hollmann et al. 2023).</p>
<p>However, the level of 'expertise' offered by pretrained LLMs on quantitative tasks across different domains has not yet been extensively studied, nor the effect of LLM imputations on performance in downstream supervised learning tasks.</p>
<p>Prior distributions are just one form of knowledge elicited from domain experts; others include feature engineering, model explanations and labelling heuristics, but in each case, the process of elicitation typically involves interviews, written correspondence or interaction with a custom app (Kerrigan et al. 2021).A good expert-elicited prior distribution can help a statistical model effectively represent the data-generating process, although due to various practical, technical and societal factors, prior elicitation is not yet widespread practice.A lack of standardized software means there is no way for an analyst using, for example, Stan, to initiate an elicitation exercise for a specific model (Mikkola et al. 2023).</p>
<p>LLM-driven elicitation (B.Z. Li et al. 2023) uses an LLM to assist elicitation from human experts, making the process interactive.In engineering, LLMs have been employed in generating (and responding to) requirements elicitation surveys (White et al. 2023;Ronanki et al. 2023;Görer and Aydemir 2023).Natural language processing is already extensively used to extract quantitative information from large texts to aid quantitative research (see, e.g.Olivetti et al. 2020) and to curate structural information for causal models (X.Zhang et al. 2024).Prior distributions can be elicited from literature via systematic reviews (Rietbergen et al. 2011;van de Schoot et al. 2018;Linde et al. 2023).A meta-analytic-predictive prior uses historical data to reduce the required sample size in clinical trials (Weber et al. 2021).To our knowledge, direct elicitation of parametric priors from a 'domain expert' LLM has not yet been explored.Gouk and Gao (2024) generated pseudodata as an indirect prior elicitation approach for Bayesian logistic regression; by contrast, in this paper, we attempt to elicit the distributional parameters directly.Our work has already been extended by Capstick et al. (2024), who elicit multiple Gaussian distributions as priors to obtain a mixture model.Several elicitation protocols have been developed to mitigate cognitive biases and combine the judgements of multiple experts (O'Hagan 2019).The Sheffield Elicitation Framework (shelf Gosling 2018) describes a collection of methods for eliciting a distribution based on aggregated opinions of multiple experts, through group discussion guided by a facilitator.Following a primer in probability and statistics, the protocol includes various ways of eliciting a univariate distribution, such as the 'roulette method', where participants assign chips to bins to form a histogram.Alternatively, the quartile method (or 'Sheffield method' European Food Safety Authority 2014) uses a series of questions to elicit quantiles of a distribution.Cooke's method (Cooke 1991) pools the distributions of multiple experts, weighted according to their respective 'calibration' (accuracy) and 'information' (uncertainty).The Delphi method uses the quartile method, iteratively refined over successive rounds using anonymized feedback from other participants.In this paper, however, we consider only single-agent LLMs with a zero-shot approach.</p>
<p>| Methods</p>
<p>| Overview</p>
<p>We describe our framework for generating and assessing 'expert' prior distributions and imputed missing values for tabular datasets.Evaluation criteria include prior informativeness, calibration and imputation quality, benchmarked against statistical baselines.Because an empirical evaluation of an LLM's 'real-world' knowledge precludes purely abstract simulation-based studies, we carefully sample real datasets spanning a diversity of scientific and technical disciplines.Prior elicitation is tested on domainspecific knowledge tasks, while imputation is evaluated using classification benchmark datasets.These experiments assess both prior elicitation and imputation across multiple domains.</p>
<p>| Eliciting Priors and Imputed Values From LLMs</p>
<p>Impersonating a human domain expert can improve an LLM's performance at related tasks (Salewski et al. 2023).Nevertheless, in response to scientific questions, especially on potentially sensitive topics, such as healthcare advice, language models often prevaricate (Lautrup et al. 2023).An LLM elicitation system should therefore not only prompt the model to roleplay an expert but also carefully specify the task to ensure contextually relevant information is returned in the appropriate format.</p>
<p>Our expert prompt initialization module (Algorithm 1), is a system prompt defining a suitable expert role for the model to imitate.For efficiency, the LLM itself is used to generate these descriptions, once per task, of the form 'You are a …'.To avoid the model offering verbose, generic or prevaricating advice about prior elicitation (such as suggesting R or Python code or advising the user to consult a real expert), the task specification module (Algorithm 2) insists that the model follows a particular elicitation protocol followed by returning a parametric prior distribution, for example, 'Beta(1, 1)', or an imputed number in a format that can be parsed programmatically (Algorithm 3).</p>
<p>Once an appropriate expert role is defined, we structure the input data in a format suitable for LLM processing.Whereas some authors (e.g.H. Zhang et al. 2023a;P. Li et al. 2023) pass data to the LLM in a tabular format, we serialize it to a natural language form using our data serialization module (as in Nazir et al. 2023), described in Algorithm 4, allowing the system to emulate interaction with a human expert.Note, however, that in the case of prior elicitation, no observed data are passed to the model at all.During imputation, the only data fed to the model are the values of other features from the same row-thus, the LLM may not perform symbolic reasoning or otherwise average over other samples in the dataset.In line with earlier work (Narayan et al. 2022;Vos et al. 2022;Nazir et al. 2023), we serialize tabular data using a simple template structure 'the {variable} is {value}': for example, a row vector of data (37, M, 120) with column names 'Age', 'Sex' and 'Blood Pressure' would become the sentence 'The Age is 37.The Sex is M. The Blood Pressure is 120'.Though one might be tempted to add units or expand abbreviations, this conversion is necessarily deterministic to avoid data corruption.Missing values (that are not to be imputed) are simply omitted from the prompt.</p>
<p>To mitigate stochasticity and redundant computation, our framework assumes a temperature setting of zero.Further details are given in Appendices A-E and our code is available on GitHub.</p>
<p>To ensure robustness in evaluation, we conduct empirical tests using datasets spanning multiple fields.We compare LLM priors with those from human experts and simple statistical models fitted to real-world data.The effectiveness of priors is further assessed by examining their predictive consistency across domains.</p>
<p>| Evaluating Expert Priors</p>
<p>What makes a good prior?Bayesian statistics involves decisionmaking based on a posterior distribution,
p( |D) ∝ ( ) n ∏ i=1 p(x i | ),
where ( ) denotes the prior distribution and D the observed data.</p>
<p>The definition of a 'good' prior distribution-like Bayesian statistics itself-is subjective, depending on the analyst's understanding of the purpose of expert-elicited information.No standard benchmark exists for expert-elicited prior distributions; a prior is a function of the expert and the elicitation method, as well as of the predictive task (Gelman et al. 2017).One purpose of prior information is to reduce amount of data needed.Another is to treat expert knowledge and observed data as complementary sources of information about a natural process.Any statistical model is at least slightly misspecified, but a prior can still be informative, well-calibrated and useful (see Williams et al. 2021).An informative prior is different from a noninformative or default prior, that is, it is not too vague.Well-calibrated or 'realistic' priors should align with those from human experts or be otherwise externally verifiable.'Useful' means superior posterior predictive performance on a downstream task, improving expected utility over reference priors.Here, we consider informativeness and calibration.</p>
<p>A measurement of the informativeness of a prior distribution is the prior effective sample size (ESS) (Morita et al. 2008;Neuenschwander et al. 2020).This is not data dependent and does not measure improvement on downstream tasks, but rather how many data points one would need to get similar peakiness/curvature around the posterior mode.The heuristic prior ESS for Beta( , ) is ESS = + (Morita et al. 2008), which measures the concentration of the prior and the amount of data needed to shift the posterior if the prior were misspecified.</p>
<p>For measuring data-prior calibration, we can use the Bayesian log posterior predictive density, or lppd (McElreath 2016)-also called log loss-or the continuous ranked probability score (CRPS), a proper scoring rule used in weather forecasting (Gneiting and Raftery 2007).We can estimate either metric using the posterior predictive distribution p(x � |D) = E p( |D) [p(x � | )] on held-out data.Wilde et al. (2021) describe a similar approach quantifying utility of synthetic data.</p>
<p>| Prior Elicitation Experiments</p>
<p>Our exploration of LLM-elicited priors includes three experiments: (1) a qualitative comparison with human-elicited priors, (2) estimating expert confidence or informativeness across different tasks and (3) a quantitative evaluation of calibration on real-world datasets.A fourth experiment is given in Appendices E.</p>
<p>Absent an open benchmark or database of expert-elicited priors, we select a recent work from the literature that describes an elicitation procedure and reported the resulting distributions.Stefan et al. (2022) interviewed six psychology researchers about typical small-to-medium effect sizes and Pearson correlations in their respective specialisms, using the histogram method.In our first experiment, using similar question wording, we elicited prior distributions from LLMs prompted to simulate an expert, conference of experts (Phillips 1991) or non-expert, with and without mentioning the shelf protocol.This experiment is a qualitative comparison of how LLMs behave when emulating a published example of a prior elicitation exercise with published question wording and results.</p>
<p>For our second experiment, we prompted GPT 3.5 to formulate 25 tasks that might call for expert elicitation in the fields of healthcare, economics, technology, environmental science, marketing and education.Tasks correspond to proportions or probabilities following a Beta( , ) distribution.These scenarios were then used to gauge general levels of confidence of elicited distributions from different LLMs, using the prior ESS metric for this distribution: ESS = + (Morita et al. 2008).Thirdly, we attempted to quantify the realism of priors and any prior-data conflict, by estimating how many samples the LLM prior offers for an analyst who has not yet collected any data.Specifically, we compared LLM priors with simple models fitted to readily available meteorological data.Priors were elicited from LLMs for the typical daily temperature (°C) and precipitation (mm) in December for 25 small and large cities around the world.These distributions were then compared with historical weather data from the openmeteo API.By investigating different continents and varying sizes of settlements, the goal was to identify any systematic biases that might emerge from LLMs' respective training corpora.Exploring both temperature and precipitation allow us to observe behaviour with symmetrical and skewed distributions, respectively.</p>
<p>We compared the prior predictive distribution to a supervised learning model with conjugate posterior (Gressmann et al. 2019): a normal-inverse-gamma model for temperature and a gamma exponential for precipitation.We ask: How many samples on average would a frequentist model need to achieve the same or better log loss (or CRPS or MSE) than the prior predictive distribution?We split the data in half for testing and repeatedly sample up to 1 3 for training from the remaining half.An alternative comparison would be of a posterior predictive based on data and a baseline prior; however, choosing such a baseline is difficult.Unlike the ( + ) ESS heuristic, this data-dependent approach quantifies prior-data conflict.</p>
<p>| Evaluating Missing Data Imputation</p>
<p>What makes a good imputed value?Jäger et al. ( 2021) describe two principles of benchmarking imputation methods: imputation quality and downstream evaluation.Imputation qualityor upstream performance-measures the extent to which an imputation method can accurately recover artificially missing values.Downstream evaluation measures the predictive performance of a supervised learning model on the imputed dataset.Imputation quality for continuous features can be calculated using the root mean square error,
RMSE = � 1 n ∑ n i=1 (x i −x i ) 2
, where x i represents the original discarded value and xi the output of imputation, whereas for categorical features, imputation quality can be calculated via the F 1 score, F 1 = 2(recall −1 + precision −1 ) −1 , the harmonic mean of precision and recall.As RMSE is unbounded, interdataset comparison is made possible by using a normalized version: NRMSE = RMSE∕(maxx − minx).Downstream performance is then determined from the relative improvement in predictive performance of models trained on imputed (vs.complete-case) data.</p>
<p>| Imputation Experiments</p>
<p>Our imputation experiments expand on previous work by exploring a wider range of applications.The OpenML-CC18 Curated Classification benchmark (Bischl et al. 2017)  ranging from 5 to 3073.Though all ostensibly provided in dense tabular format, some are actually drawn from other modes; for example, the MNIST and CIFAR-10 imaging datasets are represented as wide tables, with columns corresponding to individual pixels.It is fair to assume that any human-like expert is unlikely to make particularly informed imputations about such features.Further dataset details are given in Appendix D.</p>
<p>To evaluate the imputation quality, we used all 64 of the datasets that did not already contain missing values.We then split each of the datasets into training and test sets respectively comprising 80% and 20% of the samples.For each dataset, we artificially generated missing values based on the missing at random (MAR) missingness pattern, where the probability of a value being missing depends only on the observed values, using the Python package Jenga (Schelter et al. 2021).The number of features affected by missingness was set to min(p, 3), and the number of missing values was set to 40 for the training set and 10 for the test set.</p>
<p>Building on Jäger et al. (2021) and Nazir et al. (2023), our LLM data imputer is compared with three data-driven approaches: mean and mode imputation (for continuous and categorical features, respectively), k-nearest neighbours (KNN) imputation (the respective mean/mode of the k nearest samples) and random forest imputation.Mean/mode imputation served as the primary baseline.The LLM-based data imputer was powered by Llama 2 13B Chat, Llama 2 70B Chat (Touvron et al. 2023), Mistral 7B Instruct (Jiang et al. 2023) and Mixtral 8x7B Instruct (Jiang et al. 2024), each evaluated separately.</p>
<p>Upstream performance was calculated as the average imputation quality across selected features.Downstream evaluation used a random forest classifier (a RandomForestClassifier from scikit-learn 1.3.2 with default hyperparameters) trained on the imputed training data and evaluated on the held-out test sets.</p>
<p>| Results</p>
<p>| Prior Elicitation</p>
<p>While variations in prompting methodology-asking LLMs to roleplay as experts or non-experts-had some effect on the elicited priors, greater differences were observed between models than between tasks.Variation between human experts is mirrored by variation between LLMs, but there was no obvious pattern to explain performance at particular categories of tasks.Repeated queries with the same prompts offered mostly consistent responses, which can be attributed to the low temperature setting.</p>
<p>Figure 1 compares the priors elicited by (Stefan et al. 2022) from human experts with those we elicited from LLM counterparts in the fields of social and developmental psychology and cognitive neuroscience.Roleplaying as experts in different subfields did not have a noticeable effect on the priors.LLM priors for Cohen's were mostly centred around small effect sizes of 0.2-0.25,except GPT-4, which offered distributions around = 0.5.Mistral-7B-Instruct invariably gave t distributions with = 30 (Llama-70B-Chat-Q5: = 5); other models appeared to grow more conservative (smaller ; more leptokurtic distributions) if asked to roleplay as an expert, simulate a decision conference or employ the shelf protocol.Pearson correlation beta priors from LLMs apparently had little in common with those from real experts: GPT-4 provides a symmetric unimodal distribution whereas other models offer a right-skewed 'bathtub' distribution.Like real experts, different LLMs offered different opinions to each other.</p>
<p>From the second experiment, Figure 2 shows the prior ESScorresponding to model confidence or informativeness-for priors on different tasks.Llama-based models appear to give more conservative priors, whereas GPT is consistently more informative.Mistral 7B Instruct occasionally offered extremely high values ≥ 1000, but, generally speaking, there was no clear and systematic difference between domains.</p>
<p>Our third task was a data-driven evaluation on LLM meteorological estimates.Figure 3 shows the data-dependent ESSs from the elicited prior predictive distribution.We measure the effective increase in observations (starting from zero samples) for a frequentist model to obtain better mean squared error (MSE) than the elicited prior predictive distribution.The ESS is the number of samples needed by the frequentist model to outperform the prior predictive model.In many cases, the prior is in conflict with the data and so ESS → 0 (or, strictly speaking, 2, the minimum data points needed to compute an empirical standard deviation).As might be expected, there was a slight (though not necessarily overwhelming) bias in favour of larger, more populous cities, which would be better represented in the LLMs' training corpora.However, a strong trend of better predictions for English-speaking countries (irrespective of size) was not apparent.</p>
<p>| Missing Value Imputation</p>
<p>Data imputation tasks revealed greater variation between domains; however, LLMs did not consistently outperform more traditional imputation methods.</p>
<p>Figures 4 and 5 compare the performance of LLM-based imputation methods to traditional approaches.For upstream macro-F 1 , most LLM-based methods underperform compared to the mean/ mode baseline, while KNN and random forest imputation consistently outperform it.Upstream RMSE results highlight significant challenges for LLMs, with some cases showing extreme deviations for continuous variables.In contrast, KNN and random forest methods exhibit robust performance across all datasets.</p>
<p>Interestingly, downstream evaluations (Figure 6) show a narrowed performance gap between LLMs and baseline.This improvement may result from feature correlations in imputed rows that enhance downstream classification performance, reducing the sensitivity to choice of imputation method.Nonetheless, KNN and random forest imputations continue to outperform both LLM-based methods and mean/mode imputation.Performance differences across domains are evident, as shown in Figure 7.For instance, the Llama LLM family shows slightly better downstream macro-F 1 scores than Mistral in several datasets.Notably, Llama 2 70B outperforms the mean imputation baseline in four out of five domains, while Llama 2 13B does so in three.In contrast, Mixtral 8x7B and Mistral 7B outperform baseline in only two domains.These results may reflect differences in alignment between training data and the domain.</p>
<p>Further exploration of domain-level variations could provide insight into the capabilities and limitations of LLM imputation.</p>
<p>| Investigating Data Leakage</p>
<p>To investigate potential data leakage in LLM-based imputation, we conducted an experiment designed to assess whether the models had prior exposure to specific datasets.Specifically, we tested whether LLMs could identify datasets based on a single data row, stripped of column headers or feature descriptions, suggesting prior exposure during training.</p>
<p>We evaluated 11 datasets across three categories: well-known datasets (e.g.Titanic), random OpenML datasets and 'suspicious' datasets where LLMs exhibited anomalously high imputation accuracy.The LLMs tested were GPT-3.5, Gemini 1.5 Pro and Llama 2 70B.</p>
<p>Results showed significant variability.While GPT-3.5 and Gemini 1.5 Pro identified six and five of the 11 datasets, respectively, Llama 2 70B identified only two.All models recognized the Titanic dataset and at least one 'suspicious' dataset, supporting the hypothesis of data leakage.However, recognition rates were low for datasets where imputation performance was also poor, likely due to limited representation in training data.The utility of prior knowledge depends heavily on the application and the quality of prior elicitation.While Bayesian methods allow quantifying this utility in decision problems, establishing a benchmark class of problems remains an open challenge.Our findings suggest that, in practice, the value of priors is domain specific and may vary depending on the complexity and quality of data descriptions provided.</p>
<p>Our experiments reveal substantial limitations in LLM-based imputation (in contrast to Nazir et al. 2023, which enjoyed access to a detailed data dictionary), particularly for datasets with incomplete or ambiguous metadata.Poor upstream imputation quality for some datasets was potentially attributable to inadequate feature descriptions or unclear units, hindering the LLM's ability to infer context.For example, datasets like those describing steel plate faults or breast cancer detection lacked sufficient detail, leading to large errors.This underscores the need for careful dataset curation when evaluating LLM-based methods.</p>
<p>Additionally, variability in LLM performance across datasets raises concerns about potential data leakage.Some models demonstrated unexpectedly high performance on datasets that may have been included in their training data, highlighting the importance of designing evaluations robust to task contamination.The results of our data recognition experiment highlight the challenges of using widely available datasets for evaluation of LLMs, which may inadvertently include direct knowledge of their contents.This complicates the assessment of genuine model capabilities and raises concerns about task contamination (C.Li and Flanigan 2023).Perhaps paradoxically, future evaluations may need to prioritize curated or proprietary datasets to minimize the risk of contamination and ensure unbiased benchmarking.</p>
<p>While LLMs struggle with cross-domain imputation, our results suggest several promising avenues for improvement.</p>
<p>Fine-tuning pretrained models on domain-specific tasks and providing richer contextual prompts could enhance their imputation accuracy.Exploring hybrid approaches, such as combining LLMs with traditional imputation methods, may also yield better results.</p>
<p>Our findings also highlight the limitations of commonly used datasets, such as OpenML collections, for evaluating LLMs.These datasets, while suitable for traditional ML benchmarks, often lack the descriptive metadata required by LLMs and are widely known, increasing the risk of leakage.Evaluating LLMs on less accessible, curated datasets with richer descriptions could provide a more accurate measure of their capabilities.</p>
<p>A natural extension of our prior elicitation framework is to (Bayesian) experimental design (see Ryan et al. 2016), which can illustrate the utility of 'good' priors.Suppose a consultant is contracted with a fixed budget of 1100€to obtain an estimate with a target level of precision and, with an 'uninformed approach', can attain this goal with n samples at a cost of 1000€.</p>
<p>If prior knowledge allows similar precision with n∕2 samples (500€), the expected increase in utility (profit) would be 500%.</p>
<p>To conclude, in this paper, we have demonstrated the feasibility of extracting informative Bayesian prior distributions from generic LLMs with a simple expert prompting framework.Methods for the qualitative and quantitative evaluation of informativeness and realism of elicited priors allow assessment without specifying downstream tasks.LLMs potentially promise a more efficient interface to scientific knowledge than recruiting and interviewing domain experts, while enriching analyses with more information than purely data-driven approaches.</p>
<p>However, like human experts, the models vary considerably in their level of confidence around different   where x ij is a covariate, 0j is a random intercept for school j, 1j is a random slope and ij is a normally distributed residual term.The random coefficients are themselves normally distributed with noninformative or weakly informative priors.</p>
<p>We prompt different LLMs with 'Based on your own expert knowledge, suggest informative priors for this model in rstanarm format' and resulting summary statistics from models fitted in rstanarm (Brilleman et al. 2018) are given in Table E1.</p>
<p>(E1)
y ij = 0j + 1j x ij + ij ,
FIGURE 1 |
1
FIGURE 1 | Priors for Cohen's (top) and Pearson correlations (bottom) elicited from LLM and human experts in psychology.Dashed lines denote a shelf-like elicitation protocol.</p>
<p>FIGURE 2 |
2
FIGURE 2 | Distribution of prior effective sample size ( + ) for beta priors on various tasks.Outliers are omitted.</p>
<p>FIGURE 4 | Upstream RMSE performance across LLMs, KNN and random forest.</p>
<p>FIGURE 5 |
5
FIGURE 5 | Upstream macro-F 1 imputation performance across LLM, KNN and random forest.</p>
<p>FIGURE 6 |
6
FIGURE 6 | Downstream macro-F 1 change of LLMs.</p>
<p>FIGURE 7 |
7
FIGURE 7 | Downstream performance of different models, plotted by domain category.</p>
<p>Table-GPT describes a framework for fine-tuning language models on 'table tasks', including finding and predicting missing values.Separately, Chen et al. (</p>
<p>TABLE D1
D1| (Continued)IDNameDomainnp1487ozone-level-8hrEnvironment2534731489phonemeNatural language processing540461494qsar-biodegBiology1055421497wall-robot-navigationEngineering5456251501semeionComputer vision15932571510wdbcMedicine569311590adultSocial sciences48,842154134BioresponseBiology375117774534PhishingWebsitesNatural language processing11,055314538GesturePhaseSegmentationProcessedComputer vision9873336332cylinder-bandsPhysics and chemistry5404023381dresses-salesBusiness5001323517numerai28.6Economics96,3202240499textureComputer vision55004140668connect-4Board game67,5574340670dnaBiology318618140701churnBusiness50002140923Devnagari-ScriptComputer vision92,000102540927CIFAR_10Computer vision60,000307340966MiceProteinMedicine10808240975carBusiness1728740978Internet-AdvertisementsNatural language processing3279155940979mfeat-pixelComputer vision200024140982steel-plates-faultEngineering19412840983wiltEnvironment4839640984segmentComputer vision23102040994climate-model-simulation-crashesEnvironment5402140996Fashion-MNISTComputer vision70,00078541027jungle_chess_2pcs_raw_endgame_completeBoard game44,8197375JapaneseVowelsNatural language processing996115
Abbreviations: n = sample size, p = number of features.</p>
<p>TABLE D1 |
D1
Kaplan and Harra (2024)omputer vision, environment, natural language processing, board game and computer science.For a more complex example,Kaplan and Harra (2024)describe a hierarchical Bayesian model for analysis of the OECD Teaching and Learning International Survey (talis, data available to download online)(OECD 2018).They propose a mixed effects model for the reported job satisfaction of teacher i in school j,
Appendix ETeaching and Learning International Survey</p>
<p>TABLE E1 |
E1
Summary statistics for hierarchical models fitted to talis data, with 95% posterior interval for 'induction' coefficient.
ModelPriorRMSECoef5%95%BaselineDefault2.3370.7380.4011.066priorsKaplanWeakly2.3360.7330.4071.061informa-tiveGPT-4oLLM2.3270.6180.3800.850informa-tiveMistralLLM2.3360.6700.3380.990Chatinforma-tive
Data Availability StatementThe data that support the findings of this study are available in OpenML at https:// openml.org/ .These data were derived from the following resources available in the public domain: OpenML-CC18, https:// openml.org/ search?type= bench mark&amp; sort= tasks_ inclu ded&amp; study_ type= task&amp; id= 99; TALIS 2018, https:// www.oecd.org/ en/ data/ datas ets/ talis -2018-datab ase.html.Author ContributionsS.V. conceived the original idea.Y.I., K.S. and D.B. designed and performed the imputation study.D.S., A.W. and S.M. performed the elicitation studies.Additional analysis was performed by M.S.The main manuscript was written by D.S. with contributions from all authors.All authors reviewed and contributed to the final manuscript.Appendix A Prompting for Prior ElicitationA.1 | GuardrailsSafeguards built into ChatGPT forbid the agent from providing quantitative information about certain sensitive topics, for example, health conditions.However, these restrictions are circumvented when similar information is requested in the form of prior distributions.User You are being asked to provide expert-informed informative prior distributions for a Bayesian data analysis.You give results in pseudocode Stan distributions, for example, 'y ∼ normal(0, 1)'.Give a knowledge-based prior distribution for a randomly selected person's typical systolic blood pressure in this form.Surround your answer with 'backticks'.Do not give an explanation, just give the distribution ChatGPT 'y ∼ normal(120, 10)'Appendix B Expert Prompt Initialization (EPI)The template of our EPI module has the following format.{description} is replaced with the description of the dataset.System I am going to give you a description of a dataset.Please read it and then tell me which hypothetical persona would be the best domain expert on the content of the dataset if I had questions about specific variables, attributes or properties.I don't need a data scientist or machine learning expert, and I don't have questions about the analysis of the data but about specific attributes and values.Please do not give me a list.Just give me a detailed description of a (single) person who really knows a lot about the field in which the dataset was generated.Formulate this as an instruction like 'You are an …'.For prior elicitation and other applications, the phrase 'dataset' may be replaced with 'task' or 'topic'.As a control, we alternate with a 'non-expert' prompt of the form:You are an individual with no academic or professional background related to the dataset's field.Your interests and expertise lie completely outside of the dataset's domain, such as a chef specializing in Italian cuisine when the dataset is about astrophysics.You lack familiarity with the technical jargon, concepts and methodologies pertinent to the dataset.Your approach to questions about specific variables, attributes or properties is based on general knowledge or common sense, without any specialized understanding of the dataset's context or significance.You are more inclined to provide answers based on personal opinions or unrelated experiences rather than data-driven insights.Appendix C Task SpecificationWe used the following prompt template for task specification in data imputation.{expert prompt} is replaced with the output of the EPI module, and {data} is replaced with the serialized data.System {expert prompt}###User THE PROBLEM: We would like to analyse a dataset, but unfortunately this dataset has some missing values.OpenML-CC18A list of OpenML-CC18 datasets used in the experiment is given in TableD1.The domains were selected from medicine, biology, economics, engineering, social sciences, business, psychology, physics and
RetClean: Retrieval-Based Data Cleaning Using Foundation Models and Data Lakes. M S Ahmad, Z A Naeem, M Eltabakh, M Ouzzani, N Tang, 2023PreprintUnderstanding the Effects of RLHF on LLM Generalisation and Diversity</p>
<p>Large Language Models in Data Preparation: Opportunities and Challenges. A Barberio, 2023Master's Thesis, Politecnico di Milano</p>
<p>OpenML Benchmarking Suites. B Bischl, G Casalicchio, M Feurer, 2017Preprint</p>
<p>Joint Longitudinal and Time-to-Event Models via Stan. S L Brilleman, M J Crowther, M Moreno-Betancur, J Buros Novik, R Wolfe, 2018. 2018. January 10-12. 2018Pacific Grove, CA, USA</p>
<p>GATGPT: A Pre-Trained Large Language Model With Graph Attention Network for Spatiotemporal Imputation. A Capstick, R G Krishnan, P Barnaghi ; Chen, Y , X Wang, G Xu ; Cheng, V , Y Zhang, ID: 5nDmCwAAQBAJProceedings of the 35th Conference on Computational Linguistics and Speech Processing. J.-L Wu, M.-H Su, the 35th Conference on Computational Linguistics and Speech ProcessingGoogle-Books2024. 2023. 2023PreprintExperts in Uncertainty: Opinion and Subjective Probability in Science</p>
<p>Guidance on Expert Knowledge Elicitation in Food and Feed Safety Risk Assessment. A R T Donders, G J M G Van Der Heijden, T Stijnen, K G M Moons, 10.2903/j.efsa.2014.3734Journal of Clinical Epidemiology. 591037342006. 2014EFSA Journal</p>
<p>Methods for Eliciting Informative Prior Distributions: A Critical Review. J R Falconer, E Frank, D L L Polaschek, C Joshi, 10.1287/deca.2022.0451Decision Analysis. 1932022</p>
<p>The Prior Can Often Only Be Understood in the Context of the Likelihood. Y Ge, W Hua, K Mei, 10.1198/016214506000001437Journal of the American Statistical Association. 191014372023. 2017. 2007PreprintEntropy</p>
<p>Automated Prior Elicitation From Large Language Models for Bayesian Logistic Regression. B Görer, F B Aydemir, J P Gosling, H Gouk, B Gao, 10.1007/978-3-319-65052-4_4Elicitation: The Science and Art of Structuring Judgement, International Series in Operations Research &amp; Management Science. L C Dias, A Morton, J Quigley, Springer International Publishing2023. 2018. 2024Generating Requirements Elicitation Interview Scripts With Large Language Models</p>
<p>Probabilistic Supervised Learning. F Gressmann, F J Király, B Mateen, H Oberhauser, 2019Preprint</p>
<p>The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field. H Hassani, E S Silva, Big Data and Cognitive Computing. 2023762</p>
<p>CLAIM Your Data: Enhancing Imputation Accuracy With Contextual Large Language Models. A Hayat, M R Hasan, J , G Poesia, R E Wang, N D Goodman, Preprint. Hollmann, N., S. Müller, K. Eggensperger, and F. Hutter2024. 2023. 2023PreprintTabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</p>
<p>Can LLMs Generate Random Numbers? Evaluating LLM Sampling in Controlled Domains. A K Hopkins, A Renda, M Carbin, ICML 2023 Workshop: Sampling and Optimization in Discrete Space. International Conference on Machine Learning (ICML). 2023</p>
<p>A Benchmark for Data Imputation Methods. S Jäger, A Allhorn, F Bießmann, 10.3389/fdata.2021.693674Frontiers in Big Data. 46936742021</p>
<p>Mistral 7B. A Q Jiang, A Sablayrolles, A Mensch, 2023Preprint</p>
<p>Mixtral of Experts. A Q Jiang, A Sablayrolles, A Roux, 2024Preprint</p>
<p>HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking With Attribution. E Kamalloo, A Jafari, X Zhang, N Thakur, J Lin, 2023Preprint</p>
<p>A Bayesian Workflow for the Analysis and Reporting of International Large-Scale Assessments: A Case Study Using the OECD Teaching and Learning International Survey. D Kaplan, K Harra, 10.1186/s40536-023-00189-1Large-Scale Assessments in Education. 121: 22024</p>
<p>A Survey of Domain Knowledge Elicitation in Applied Machine Learning. D Kerrigan, J Hullman, E Bertini, Multimodal Technologies and Interaction. 512732021</p>
<p>Heart-to-Heart With ChatGPT: The Impact of Patients Consulting AI for Cardiovascular Health Advice. A D Lautrup, T Hyrup, A Schneider-Kamp, M Dahl, J S Lindholt, P Schneider-Kamp, Open heart. 102e0024552023</p>
<p>Task Contamination: Language Models May Not Be Few-Shot Anymore. C Li, J Flanigan, 2023Preprint</p>
<p>Table-GPT: Table-Tuned GPT for Diverse Table Tasks. P Li, Y He, D Yashar, 2023Preprint</p>
<p>Eliciting Human Preferences With Language Models. B Z Li, A Tamkin, N Goodman, J Andreas, 2023Preprint</p>
<p>Data-Driven Prior Elicitation for Bayes Factors in Cox Regression for Nine Subfields in Biomedicine. M Linde, L Jochim, J N Tendeiro, D Ravenzwaaij, 10.1101/2023.09.04.23295029v1Preprint, medRxiv. 2023</p>
<p>Applying Large Language Models to Tabular Data: A New Approach. J Lopatecki, A Dhinakaran, C Brown, 2023</p>
<p>A Novel Approach for Anomaly Detection Using Large Language Models. J Lopatecki, A Dhinakaran, C Brown, 2023</p>
<p>Statistical Rethinking: A Bayesian Course With Examples in R and Stan. R Mcelreath, Texts in Statistical Science Series. 2016CRC Press</p>
<p>How Generative AI Models Such as ChatGPT Can Be (Mis)used in SPC Practice, Education, and Research? An Exploratory Study. F M Megahed, Y.-J Chen, J A Ferris, S Knoth, L A Jones-Farmer, 10.1080/08982112.2023.2206479Quality Engineering. 3622023</p>
<p>Prior Knowledge Elicitation: The Past, Present, and Future. P Mikkola, O A Martin, S Chandramouli, 2023Preprint</p>
<p>Determining the Effective Sample Size of a Parametric Prior. S Morita, P F Thall, P Müller ; Narayan, A , I Chami, L Orr, S Arora, C Ré, 10.1111/j.1541-0420.2007.00888.xBiometrics. 6422008. 2022PreprintCan Foundation Models Wrangle Your Data?</p>
<p>ChatGPT-Based Biological and Psychological Data Imputation. A Nazir, M N Cheeema, Z Wang Neuenschwander, B , S Weber, H Schmidli, A O'hagan, 10.1111/biom.13252Meta-Radiology. 132023. 2020Biometrics</p>
<p>scite: A Smart Citation Index That Displays the Context of Citations and Classifies Their Intent Using Deep Learning. J M Nicholson, M Mordaunt, P Lopez, 10.1162/qss_a_00146Quantitative Science Studies. 232021. 2023PreprintNumeracy From Literacy: Data Science as an Emergent Skill From Large Language Models</p>
<p>Expert Knowledge Elicitation: Subjective but Scientific. A O'hagan, 10.1080/00031305.2018.1518265American Statistician. 73sup12019. 2018Talis 2018 Database</p>
<p>Data-Driven Materials Research Enabled by Natural Language Processing and Information Extraction. E A Olivetti, J M Cole, E Kim, Applied Physics Reviews. 74413172020</p>
<p>Human-Like Problem-Solving Abilities in Large Language Models Using ChatGPT. G Orrù, A Piarulli, C Conversano, A Gemignani, 10.3389/frai.2023.1199350Frontiers in Artificial Intelligence. 611993502023</p>
<p>Language Models as Knowledge Bases?. F Petroni, T Rocktäschel, S Riedel, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). K Inui, J Jiang, V Ng, X Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational Linguistics2019</p>
<p>Decision Conferencing. L D Phillips, IEE Colloquium on CSCW: Some Fundamental Issues. 1991</p>
<p>Incorporation of Historical Data in the Analysis of Randomized Therapeutic Trials. C Rietbergen, I Klugkist, K J M Janssen, K G M Moons, H J A Hoijtink, Contemporary Clinical Trials. 3262011</p>
<p>Investigating ChatGPT's Potential to Assist in Requirements Elicitation Processes. K Ronanki, C Berger, J Horkoff, 2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). IEEE2023</p>
<p>A Review of Modern Computational Algorithms for Bayesian Optimal Design. E G Ryan, C C Drovandi, J M Mcgree, A N Pettitt, 10.1111/insr.12107International Statistical Review. 8412016</p>
<p>JENGA-A Framework to Study the Impact of Data Errors on the Predictions of Machine Learning Models. L Salewski, S Alaniz, I Rio-Torto, E Schulz, Z Akata, S Schelter, T Rukat, F Biessmann, EDBT 2021 Industrial and Application Track. EDBT Association. 2023. 2021PreprintIn-Context Impersonation Reveals Large Language Models' Strengths and Biases</p>
<p>. A M Stefan, D Katsimpokis, Q F Gronau, E.-J Wagenmakers, </p>
<p>Expert Agreement in Prior Elicitation and Its Effects on Bayesian Inference. Psychonomic Bulletin &amp; Review. 295</p>
<p>Bayesian PTSD-Trajectory Analysis With Informed Priors Based on a Systematic Literature Search and Expert Elicitation. H Touvron, L Martin, K Stone, 10.1080/00273171.2017.1412293Multivariate Behavioral Research. 5322023. 2018PreprintLlama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Towards Parameter-Efficient Automation of Data Wrangling Tasks With Prefix-Tuning. D Vos, T Döhmen, S Schelter, Y Wardat, M A Tashtoush, R Alali, A M Jarrah, NeurIPS 2022 Table Representation Workshop. Neural Information Processing Systems (NeurIPS). 2022. 2023192286Eurasia Journal of Mathematics</p>
<p>Applying Meta-Analytic-Predictive Priors With the R Bayesian Evidence Synthesis Tools. S Weber, Y Li, J W S Iii, T Kakizume, H Schmidli, 10.18637/jss.v100.i19Journal of Statistical Software. 1002021</p>
<p>ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design. J White, S Hays, Q Fu, J Spencer-Smith, D C Schmidt, 2023Preprint</p>
<p>Foundations of Bayesian Learning From Synthetic Data. H Wilde, J Jewson, S Vollmer, C Holmes, Proceedings of the 24th International Conference on Artificial Intelligence and Statistics. the 24th International Conference on Artificial Intelligence and Statistics2021</p>
<p>A Comparison of Prior Elicitation Aggregation Using the Classical Method and SHELF. C J Williams, K J Wilson, N Wilson, 10.1111/rssa.12691Journal of the Royal Statistical Society Series A: Statistics in Society. 18432021</p>
<p>Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. M Xiong, Z Hu, X Lu, 2023Preprint</p>
<p>Jellyfish: A Large Language Model for Data Preprocessing. H Zhang, Y Dong, C Xiao, M Oyamada, 2023Preprint</p>
<p>Large Language Models as Data Preprocessors. H Zhang, Y Dong, C Xiao, M Oyamada, 2023Preprint</p>
<p>Leveraging LLM-Generated Structural Prior for Causal Inference With Concurrent Causes. X Zhang, S Liu, Y Wang, Q Mei, Causality and Large Models @NeurIPS 2024. Neural Information Processing Systems (NeurIPS). 2024</p>            </div>
        </div>

    </div>
</body>
</html>
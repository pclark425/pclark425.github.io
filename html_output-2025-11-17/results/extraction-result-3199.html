<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3199 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3199</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3199</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-0476750c048abe336b9a24dbfa60b975bf0834c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0476750c048abe336b9a24dbfa60b975bf0834c1" target="_blank">SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> SimplyRetrieve is an open-source tool that provides a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community, and explores the potential of Retrieval-Centric Generation for improving generative AI performance while maintaining privacy standards.</p>
                <p><strong>Paper Abstract:</strong> Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI performance while maintaining privacy standards. The tool is available at https://github.com/RCGAI/SimplyRetrieve with an MIT license.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3199.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3199.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimplyRetrieve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimplyRetrieve (Retrieval-Centric Generation Tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, local GUI/API tool implementing a Retrieval-Centric Generation (RCG) pipeline that pairs an off-the-shelf LLM with a dense ANNS retriever and private knowledge-base constructor to enable retrieval-based memory for QA and generation over private data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SimplyRetrieve RCG platform</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-centric system that composes an off-the-shelf LLM (examples used: Wizard-Vicuna-13B and Llama-2-13B-chat) with a dense embedding encoder (Multilingual-E5-base) and an Approximate Nearest Neighbor Search (ANNS) retriever (Faiss HNSW by default). It exposes prompt slots (AI Prefix, Retriever Prefix/Suffix, Model Prefix/Suffix) and supports Mixture-of-Knowledge-Bases and Explicit Prompt-Weighting (EPW).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external knowledge base / ANNS index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Documents are chunked into passages, encoded into dense embeddings via Multilingual-E5-base, and indexed with Faiss (HNSW by default). At inference the retriever returns top-K (K=5 in experiments) passages which are concatenated into the prompt between Retriever Prefix and Retriever Suffix. Explicit Prompt-Weighting can adjust the fraction of retrieved tokens used; selection among multiple knowledge bases can be performed via semantic similarity of functional descriptions (MoKB). Storage/update details beyond initial index construction are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive question answering on private organizational website + synthetic QA dataset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering factual questions grounded in a private knowledge base constructed from an organization's website (Kioxia), plus an internally generated QA dataset (queries/labels synthesized by Llama-2-13B-chat) used for Rouge-L evaluation; main challenge is factual recall of never-seen-before data and avoiding hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / knowledge-intensive generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Rouge-L 0.413; time/query 11.67 s (RCG configuration using retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using retrieval-centric integration (SimplyRetrieve RCG) substantially improves Rouge-L (0.413) over a retrieval-off baseline and slightly outperforms a less-constrained RAG prompt (0.359); RCG also produced shorter responses (≈36% fewer tokens than ROG) and faster per-query latency (11.67 s) despite longer prompts. EPW ablation shows best performance at 50% weight (Rouge-L 0.414).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper notes the system is not foolproof for safe/responsible responses; retrieved information quality and prompt sensitivity can affect outputs. Update/maintenance procedures for the knowledge base are not detailed. Potential retrieval errors and reliance on prompt-engineering are discussed as limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3199.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3199.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Centric Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed architectural concept that emphasizes LLMs as context interpreters and delegates knowledge memorization to retrievers (dense retrieval from external KB), implemented and evaluated in SimplyRetrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RCG (Retrieval-Centric Generation agent/configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A configuration/approach where an off-the-shelf LLM is instructed (via prompt engineering) to only use the provided retrieved knowledge to answer queries; integration performed by concatenating retrieved passages into the prompt with a constraining retriever suffix (e.g., 'answer the following question with the provided knowledge').</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external KB indexed via ANNS)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Dense embeddings created from document chunks (Multilingual-E5-base) are indexed (Faiss HNSW). Top-5 passages are retrieved and inserted into the prompt; the LLM is prompted to rely solely on the provided retrieved knowledge. Explicit Prompt-Weighting (EPW) optionally controls how many/top fraction of retrieved items influence the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Organization-specific factual QA and synthetic QA evaluation (Rouge-L)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grounded factual question answering requiring recall of organization-specific facts from a constructed private knowledge base and an automatically generated QA evaluation set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / knowledge-grounded generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Rouge-L 0.413; time/query 11.67 s (RCG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RCG (constraining the LLM to use retrieved knowledge) yielded highest Rouge-L and better factual accuracy in qualitative examples compared to RAG and ROG, reduced hallucinations, produced briefer responses, and resulted in lower latency in experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RCG relies on quality of retriever and retrieved passages; prompt-engineering is required to elicit desired behavior; broader generalization and robustness across datasets remain to be studied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3199.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3199.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard retrieval-augmented approach where retrieved knowledge is provided to the LLM with permissive instructions allowing mixing of retrieved content and model parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (permissive prompt configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach/configuration in which retrieved passages are provided to the LLM but the LLM is allowed (via a less constraining prompt like 'You may use the provided knowledge') to mix retrieved knowledge with its internal parametric knowledge when generating answers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external KB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Same retriever pipeline as RCG: dense embeddings (Multilingual-E5-base), indexed by Faiss (HNSW), top-5 passages retrieved and concatenated into the prompt; prompt instructs LLM that it 'may use' this knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Organization-specific factual QA and synthetic QA evaluation (Rouge-L)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-grounded QA where the model may use both retrieved material and its own learned knowledge to answer factual questions about a private knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / knowledge-grounded generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Rouge-L 0.359; time/query 18.41 s</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG improves over retrieval-off baseline (ROG) but underperforms the stricter RCG prompt in Rouge-L and produced partially erroneous outputs in some qualitative examples due to blending retrieved facts with incorrect parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Permissive integration allows the LLM to mix parametric (possibly outdated or incorrect) knowledge with retrieved facts, leading to partially erroneous or hallucinated outputs; longer latency observed vs RCG in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3199.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3199.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-OFF Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline setting where the retriever is disabled and the LLM answers using only its parametric knowledge (no external memory).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ROG (Retrieval-OFF baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A baseline configuration in which no retrieval is performed and the LLM must answer queries from its pre-trained parameters alone.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>none (parametric-only)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>No external memory or retrieval; responses are generated solely from the LLM's internal weights and context window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Organization-specific factual QA and synthetic QA evaluation (Rouge-L)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same knowledge-grounded QA tasks but without access to the private KB, testing the LLM's parametric recall and tendency to hallucinate on never-seen facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / knowledge-grounded generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Rouge-L 0.186; time/query 17.22 s</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The retrieval-off baseline performed poorly on factual QA about private/unseen data (low Rouge-L and qualitative hallucinations), demonstrating the need for external memory for never-seen facts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Parametric-only models can hallucinate or recall incorrect facts for out-of-training data; accuracy is low on private knowledge without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3199.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3199.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative chat AI library/framework mentioned for providing agent orchestration, data augmentation, and memory capabilities for LLM-based systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LangChain agents (framework-level)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework that enables construction of agent-based pipelines, tool use, data augmentation, and memory modules to extend LLM behavior; cited as related work and contrasted with SimplyRetrieve's lightweight, transparent approach.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>framework-provided agent memory (various memory modules supported)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Mentioned only at a high level: LangChain 'offers agents, data augmentation, and memory capabilities' but the paper does not detail which memory types or implementations are used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LangChain is noted as a comprehensive library that supports agents and memory, but its complexity may reduce explainability compared to SimplyRetrieve's design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper mentions that agent-based pipelining complexity (as in LangChain) may hinder explainability in retrieval-augmented settings; no experimental details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3199.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3199.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Haystack agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Haystack (end-to-end NLP framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end NLP framework supporting question answering, answer generation, semantic search, and retrieval-augmentation, using agent-based pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Haystack: the end-to-end NLP framework for pragmatic builders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Haystack pipeline agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework that supports retrieval-augmented QA and agent-style pipelining for complex queries; cited as related work and contrasted for its complexity versus SimplyRetrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented / pipeline-managed memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Paper references Haystack's agent-based pipelining and retrieval-augmentation capabilities but does not provide implementation specifics; Haystack typically composes retrievers, readers, and pipelines to provide memory via external indices.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Haystack is acknowledged as a comprehensive framework for retrieval-augmented tasks but may introduce complexity that impedes interpretability compared to the lightweight and transparent SimplyRetrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No experimental details in this paper; complexity and reduced explainability of agent pipelines are noted as concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for opendomain question answering <em>(Rating: 2)</em></li>
                <li>When not to trust language models: Investigating effectiveness of parametric and non-parametric memories <em>(Rating: 2)</em></li>
                <li>LangChain <em>(Rating: 1)</em></li>
                <li>Haystack: the end-to-end NLP framework for pragmatic builders <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3199",
    "paper_id": "paper-0476750c048abe336b9a24dbfa60b975bf0834c1",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "SimplyRetrieve",
            "name_full": "SimplyRetrieve (Retrieval-Centric Generation Tool)",
            "brief_description": "An open-source, local GUI/API tool implementing a Retrieval-Centric Generation (RCG) pipeline that pairs an off-the-shelf LLM with a dense ANNS retriever and private knowledge-base constructor to enable retrieval-based memory for QA and generation over private data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SimplyRetrieve RCG platform",
            "agent_description": "A retrieval-centric system that composes an off-the-shelf LLM (examples used: Wizard-Vicuna-13B and Llama-2-13B-chat) with a dense embedding encoder (Multilingual-E5-base) and an Approximate Nearest Neighbor Search (ANNS) retriever (Faiss HNSW by default). It exposes prompt slots (AI Prefix, Retriever Prefix/Suffix, Model Prefix/Suffix) and supports Mixture-of-Knowledge-Bases and Explicit Prompt-Weighting (EPW).",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external knowledge base / ANNS index)",
            "memory_mechanism_description": "Documents are chunked into passages, encoded into dense embeddings via Multilingual-E5-base, and indexed with Faiss (HNSW by default). At inference the retriever returns top-K (K=5 in experiments) passages which are concatenated into the prompt between Retriever Prefix and Retriever Suffix. Explicit Prompt-Weighting can adjust the fraction of retrieved tokens used; selection among multiple knowledge bases can be performed via semantic similarity of functional descriptions (MoKB). Storage/update details beyond initial index construction are not specified.",
            "task_name": "Knowledge-intensive question answering on private organizational website + synthetic QA dataset",
            "task_description": "Answering factual questions grounded in a private knowledge base constructed from an organization's website (Kioxia), plus an internally generated QA dataset (queries/labels synthesized by Llama-2-13B-chat) used for Rouge-L evaluation; main challenge is factual recall of never-seen-before data and avoiding hallucinations.",
            "task_type": "question answering / knowledge-intensive generation",
            "performance_with_memory": "Rouge-L 0.413; time/query 11.67 s (RCG configuration using retrieval)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Using retrieval-centric integration (SimplyRetrieve RCG) substantially improves Rouge-L (0.413) over a retrieval-off baseline and slightly outperforms a less-constrained RAG prompt (0.359); RCG also produced shorter responses (≈36% fewer tokens than ROG) and faster per-query latency (11.67 s) despite longer prompts. EPW ablation shows best performance at 50% weight (Rouge-L 0.414).",
            "limitations_or_challenges": "Paper notes the system is not foolproof for safe/responsible responses; retrieved information quality and prompt sensitivity can affect outputs. Update/maintenance procedures for the knowledge base are not detailed. Potential retrieval errors and reliance on prompt-engineering are discussed as limitations.",
            "uuid": "e3199.0",
            "source_info": {
                "paper_title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RCG",
            "name_full": "Retrieval-Centric Generation",
            "brief_description": "A proposed architectural concept that emphasizes LLMs as context interpreters and delegates knowledge memorization to retrievers (dense retrieval from external KB), implemented and evaluated in SimplyRetrieve.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RCG (Retrieval-Centric Generation agent/configuration)",
            "agent_description": "A configuration/approach where an off-the-shelf LLM is instructed (via prompt engineering) to only use the provided retrieved knowledge to answer queries; integration performed by concatenating retrieved passages into the prompt with a constraining retriever suffix (e.g., 'answer the following question with the provided knowledge').",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external KB indexed via ANNS)",
            "memory_mechanism_description": "Dense embeddings created from document chunks (Multilingual-E5-base) are indexed (Faiss HNSW). Top-5 passages are retrieved and inserted into the prompt; the LLM is prompted to rely solely on the provided retrieved knowledge. Explicit Prompt-Weighting (EPW) optionally controls how many/top fraction of retrieved items influence the prompt.",
            "task_name": "Organization-specific factual QA and synthetic QA evaluation (Rouge-L)",
            "task_description": "Grounded factual question answering requiring recall of organization-specific facts from a constructed private knowledge base and an automatically generated QA evaluation set.",
            "task_type": "question answering / knowledge-grounded generation",
            "performance_with_memory": "Rouge-L 0.413; time/query 11.67 s (RCG)",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "RCG (constraining the LLM to use retrieved knowledge) yielded highest Rouge-L and better factual accuracy in qualitative examples compared to RAG and ROG, reduced hallucinations, produced briefer responses, and resulted in lower latency in experiments reported.",
            "limitations_or_challenges": "RCG relies on quality of retriever and retrieved passages; prompt-engineering is required to elicit desired behavior; broader generalization and robustness across datasets remain to be studied.",
            "uuid": "e3199.1",
            "source_info": {
                "paper_title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A standard retrieval-augmented approach where retrieved knowledge is provided to the LLM with permissive instructions allowing mixing of retrieved content and model parametric knowledge.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "use",
            "agent_name": "RAG (permissive prompt configuration)",
            "agent_description": "An approach/configuration in which retrieved passages are provided to the LLM but the LLM is allowed (via a less constraining prompt like 'You may use the provided knowledge') to mix retrieved knowledge with its internal parametric knowledge when generating answers.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external KB)",
            "memory_mechanism_description": "Same retriever pipeline as RCG: dense embeddings (Multilingual-E5-base), indexed by Faiss (HNSW), top-5 passages retrieved and concatenated into the prompt; prompt instructs LLM that it 'may use' this knowledge.",
            "task_name": "Organization-specific factual QA and synthetic QA evaluation (Rouge-L)",
            "task_description": "Knowledge-grounded QA where the model may use both retrieved material and its own learned knowledge to answer factual questions about a private knowledge base.",
            "task_type": "question answering / knowledge-grounded generation",
            "performance_with_memory": "Rouge-L 0.359; time/query 18.41 s",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "RAG improves over retrieval-off baseline (ROG) but underperforms the stricter RCG prompt in Rouge-L and produced partially erroneous outputs in some qualitative examples due to blending retrieved facts with incorrect parametric knowledge.",
            "limitations_or_challenges": "Permissive integration allows the LLM to mix parametric (possibly outdated or incorrect) knowledge with retrieved facts, leading to partially erroneous or hallucinated outputs; longer latency observed vs RCG in reported experiments.",
            "uuid": "e3199.2",
            "source_info": {
                "paper_title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ROG",
            "name_full": "Retrieval-OFF Generation",
            "brief_description": "Baseline setting where the retriever is disabled and the LLM answers using only its parametric knowledge (no external memory).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ROG (Retrieval-OFF baseline)",
            "agent_description": "A baseline configuration in which no retrieval is performed and the LLM must answer queries from its pre-trained parameters alone.",
            "memory_used": false,
            "memory_type": "none (parametric-only)",
            "memory_mechanism_description": "No external memory or retrieval; responses are generated solely from the LLM's internal weights and context window.",
            "task_name": "Organization-specific factual QA and synthetic QA evaluation (Rouge-L)",
            "task_description": "Same knowledge-grounded QA tasks but without access to the private KB, testing the LLM's parametric recall and tendency to hallucinate on never-seen facts.",
            "task_type": "question answering / knowledge-grounded generation",
            "performance_with_memory": null,
            "performance_without_memory": "Rouge-L 0.186; time/query 17.22 s",
            "has_performance_comparison": true,
            "key_findings": "The retrieval-off baseline performed poorly on factual QA about private/unseen data (low Rouge-L and qualitative hallucinations), demonstrating the need for external memory for never-seen facts.",
            "limitations_or_challenges": "Parametric-only models can hallucinate or recall incorrect facts for out-of-training data; accuracy is low on private knowledge without retrieval.",
            "uuid": "e3199.3",
            "source_info": {
                "paper_title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LangChain agents",
            "name_full": "LangChain agent framework",
            "brief_description": "A generative chat AI library/framework mentioned for providing agent orchestration, data augmentation, and memory capabilities for LLM-based systems.",
            "citation_title": "LangChain",
            "mention_or_use": "mention",
            "agent_name": "LangChain agents (framework-level)",
            "agent_description": "Framework that enables construction of agent-based pipelines, tool use, data augmentation, and memory modules to extend LLM behavior; cited as related work and contrasted with SimplyRetrieve's lightweight, transparent approach.",
            "memory_used": true,
            "memory_type": "framework-provided agent memory (various memory modules supported)",
            "memory_mechanism_description": "Mentioned only at a high level: LangChain 'offers agents, data augmentation, and memory capabilities' but the paper does not detail which memory types or implementations are used.",
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "LangChain is noted as a comprehensive library that supports agents and memory, but its complexity may reduce explainability compared to SimplyRetrieve's design.",
            "limitations_or_challenges": "Paper mentions that agent-based pipelining complexity (as in LangChain) may hinder explainability in retrieval-augmented settings; no experimental details are provided in this paper.",
            "uuid": "e3199.4",
            "source_info": {
                "paper_title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Haystack agents",
            "name_full": "Haystack (end-to-end NLP framework)",
            "brief_description": "An end-to-end NLP framework supporting question answering, answer generation, semantic search, and retrieval-augmentation, using agent-based pipelines.",
            "citation_title": "Haystack: the end-to-end NLP framework for pragmatic builders",
            "mention_or_use": "mention",
            "agent_name": "Haystack pipeline agents",
            "agent_description": "Framework that supports retrieval-augmented QA and agent-style pipelining for complex queries; cited as related work and contrasted for its complexity versus SimplyRetrieve.",
            "memory_used": true,
            "memory_type": "retrieval-augmented / pipeline-managed memory",
            "memory_mechanism_description": "Paper references Haystack's agent-based pipelining and retrieval-augmentation capabilities but does not provide implementation specifics; Haystack typically composes retrievers, readers, and pipelines to provide memory via external indices.",
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "key_findings": "Haystack is acknowledged as a comprehensive framework for retrieval-augmented tasks but may introduce complexity that impedes interpretability compared to the lightweight and transparent SimplyRetrieve.",
            "limitations_or_challenges": "No experimental details in this paper; complexity and reduced explainability of agent pipelines are noted as concerns.",
            "uuid": "e3199.5",
            "source_info": {
                "paper_title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for opendomain question answering",
            "rating": 2
        },
        {
            "paper_title": "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
            "rating": 2
        },
        {
            "paper_title": "LangChain",
            "rating": 1
        },
        {
            "paper_title": "Haystack: the end-to-end NLP framework for pragmatic builders",
            "rating": 1
        }
    ],
    "cost": 0.013081249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool</h1>
<p>Youyang Ng, Daisuke Miyashita, Yasuto Hoshi, Yasuhiro Morioka,<br>Osamu Torii, Tomoya Kodama, Jun Deguchi<br>Kioxia Corporation, Japan<br>youyang.ng@kioxia.com</p>
<h4>Abstract</h4>
<p>Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI performance while maintaining privacy standards. The tool is available at https: //github.com/RCGAI/SimplyRetrieve with an MIT license.</p>
<h2>1 Introduction</h2>
<p>Generative-based Natural Language Processing (NLP) has witnessed significant progress (Brown et al., 2020) in recent years. With the introduction of Transformer (Vaswani et al., 2017) architecture, the possibility of developing high-accuracy language models that can perform tasks such as text generation, text summarization and language translation has become a reality. These models (Brown et al., 2020; Chowdhery et al., 2022), when scaled up to billions of parameters (Wei et al., 2022a), have shown remarkable improvements in text generation tasks such as zero-shot inference, popularized the term Generative AI. Instead of model fine-tuning, careful design of prompts has proven
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Retrieval-Centric Generation (RCG) approach presents an innovative concept that leverages the mutually beneficial interaction between LLMs and retrievers for more efficient context interpretation and knowledge memorization. Increased clarity in role-separation between context interpretation and knowledge memorization can potentially boost the performance of generative AI systems.
effective in adapting these models to specific domains for various tasks (Brown et al., 2020). This has given rise to the field of prompt-engineering. Additionally, Chain-of-Thought (Wei et al., 2022b; Kojima et al., 2022) decomposes a complex task assigned into manageable steps, thereby expanding the capabilities of generative-based language models even further.</p>
<p>Training large language models (LLMs) requires immense computational resources, often involving thousands of high-end GPUs. Fine-tuning these models can also be challenging. Although prompt-engineering helped to reduce the need for fine-tuning, there was still noticeable instruction misalignment when interacting with a human user. To address this issue, techniques such as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) have been explored to align the behavior of LLMs with human values (Ouyang et al., 2022; OpenAI, 2023). Additionally, QLoRA (Dettmers et al., 2023), combining low-rank adap-</p>
<p>tation technique (Hu et al., 2022) and quantization technique, has made it possible to fine-tune these models on individual developer's hardware, making them more accessible to a wider range of users. Despite these advances, there are still limitations to the capacity of LLMs, and they do not inherently recognize information that was not present during training and fine-tuning. Memorization of factual knowledge in the long tail is also a challenge (Mallen et al., 2023).</p>
<p>Most recently, there has been growing interest in integrating external knowledge sources into LLMs for generating text (Borgeaud et al., 2022; Guu et al., 2020; Lewis et al., 2020). Similar approaches have also been proposed in solving computer vision tasks (Nakata et al., 2022; Iscen et al., 2023). Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) architecture is an approach that enhances the capabilities of LLMs by incorporating external data sources using a sparse or dense retriever (Karpukhin et al., 2020), enabling the use of privately owned data without requiring retraining or fine-tuning the LLM (Chase, 2022). However, developing retrieval-augmented LLM-based generative models is still in its early stages. Our proposed tool can help facilitate these developments.</p>
<p>Additionally, we introduce a new architectural concept called Retrieval-Centric Generation (RCG), which builds upon the RetrievalAugmented Generation approach by emphasizing the crucial role of the LLM in interpreting context and entrusting knowledge memorization to the retriever component, putting greater importance on retriever, as depicted in Figure 1. By separating context interpretation from knowledge memorization, this approach has the potential to reduce the scale (Carlini et al., 2023) of the LLM required for generative tasks, leading to more efficient and interpretable results. Moreover, this approach may help mitigate hallucinations (Maynez et al., 2020) by limiting the scope of the LLM's generation. Once we define RCG as above, we can re-define RAG that enables more permissible usage of LLM's inherent knowledge, whereas RCG prioritizes clear demarcations between context interpretation and knowledge memorization.</p>
<p>SimplyRetrieve is an open-source tool aimed at providing a localized, lightweight, and userfriendly interface to Retrieval-Centric Generation approach to the machine learning community. This tool encompasses a GUI and API based RCG plat-
form, assisted by a Private Knowledge Base Constructors and a Retrieval Tuning Module. SimplyRetrieve is designed to be simple and accessible to the community, as well as end-users. Our retrieval-centric platform incorporates multiple selectable knowledge bases featuring Mixtures-of-Knowledge-Bases (MoKB) mode and Explicit Prompt-Weighting (EPW) of retrieved knowledge base. By designing SimplyRetrieve with these features, we enable the machine learning community to explore and develop with a lightweight, private data interface to LLM-based generative AI systems, with a focus on retrieval-centric generation. Potential developments that can be explored using this tool include: (1) examining the effectiveness of retrieval-centric generation in developing safer, more interpretable, and responsible AI systems; (2) optimizing the efficiency of separating context interpretation and knowledge memorization within retrieval-centric generation approach; and (3) improving prompt-engineering techniques for retrieval-centric generation. SimplyRetrieve is available at https://github.com/ RCGAI/SimplyRetrieve.</p>
<p>Our contributions can be summarized as follows:</p>
<ul>
<li>We propose SimplyRetrieve, an innovative and user-friendly tool that leverages GUI and API platform to facilitate a Retrieval-Centric Generation approach. This platform is further strengthened by two key components: Private Knowledge Base Constructor and Retrieval Tuning Module.</li>
<li>We open sourced our tool to the machine learning community and identify potential development directions of Retrieval-Centric Generation.</li>
</ul>
<h2>2 Related Works</h2>
<p>The emergence of Retrieval-Augmented Generation architecture has spurred the development of numerous open-source tools. The ChatGPT Retrieval Plugin ${ }^{1}$, for instance, integrates the ability to retrieve and enhance personal or organizational documents into the widely used ChatGPT model (OpenAI, 2023). Similarly, fastRAG (Izsak et al., 2023) provides a streamlined platform for constructing efficient retrieval-augmented generation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: SimplyRetrieve is an open-source tool that provides a localized, lightweight, and user-friendly interface to the Retrieval-Centric Generation approach for the machine learning community. This tool features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module.</p>
<p>pipelines. Additionally, LangChain <em>Chase (2022)</em> offers a comprehensive generative chat AI library featuring agents, data augmentation, and memory capabilities. Finally, Haystack <em>Pietsch et al. (2019)</em> presents an all-encompassing NLP framework supporting question answering, answer generation, semantic document search, and retrieval-augmentation. Both LangChain and Haystack employ agent-based pipelining techniques and can process complex queries. However, this complexity may hinder the explainability of LLMs, making it challenging to interpret their performance in retrieval-augmented settings.</p>
<p>On the other hand, our work offers a lightweight and transparent approach to implementing sophisticated retrieval-centric, as well as retrieval-augmented architecture, while maintaining a strong emphasis on response interpretability and wider accessibility to the community. Unlike previous works such as PrivateGPT <em>PrivateGPT</em>, which provides a privacy-preserving chat AI tool but lacks customization options and analytical capabilities, our tool offers a comprehensive set of features for tailoring and analyzing retrieval-centric generation.</p>
<p>Furthermore, to the best of our knowledge, we are the first to introduce RCG concept and show initial experiments of it using our tool.</p>
<h2>3 Tool Design</h2>
<p>SimplyRetrieve is designed to deploy RCG pipeline: construct knowledge base, tune architecture, make predictions. In this paper, we focus on describing the core specifications of the tool. For details about the setup procedures, refer to the repository of https://github.com/RCGAI/SimplyRetrieve.</p>
<h3>3.1 GUI and API based Retrieval-Centric Generation Platform</h3>
<p>As shown in Figure 2, there are two dense models in our tool: an LLM and an Approximate Nearest Neighbor Search (ANNS) based Knowledge Retriever. The LLM can be any one of the off-the-shelf open-source LLM models available in Hugging Face <em>Wolf et al. (2020)</em>, ranging from 1B to more than 100B-scale in parameters such as <em>Touvron et al. (2023a, b)</em>. The Knowledge Retriever employs a dense retriever that is compatible with various embedding models available in Hugging Face. Additionally, our tool allows integration of multiple knowledge bases simultaneously, enabling user-selectable knowledge bases depending on the specific use case.</p>
<p>In terms of the GUI, we have designed a simple yet intuitive layout using Gradio <em>Abid et al. (2019)</em>, which provides a familiar streaming chatbot interface with user control for managing the running modes of the retriever, engineering prompts, and configuring the tool. As depicted in Figure 3, our GUI features a comprehensive <em>retrieval-centric tuning panel</em> for functions including <em>manual knowledge base selection</em> from multiple sources and <em>Mixture-of-Knowledge-Base</em> modes. Moreover, we employ <em>Explicit Prompt-Weighting</em> of retrieval to adjust the level of influence exerted by the retriever. To ensure seamless integration, we also developed a comprehensive API access function using the Gradio Client Interface, and we allow multi-user</p>
<table>
<thead>
<tr>
<th>Chat</th>
<th>Prompt</th>
<th>Config</th>
<th>Analysis</th>
</tr>
</thead>
<tbody>
<tr>
<td>In- Chatbot</td>
<td>Functional Tabs</td>
<td>Streaming Chatbot Interface</td>
<td></td>
</tr>
<tr>
<td></td>
<td>What is the purpose of establishing KIOKIA Iwate Corporation?</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>The purpose of establishing KIOKIA Iwate Corporation is to meet the growing demand for flash memory through advanced manufacturing processes utilizing AI.</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Enter text and press enter</td>
<td>Retrieval-Centric Tuning Panel</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Use KnowledgeBase</td>
<td>KnowledgeBase Mode</td>
<td>KnowledgeBase</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Selectable KnowledgeBase</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Kiosia Expert</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Kiosia Expert</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>KnowledgeBase Weightage</td>
</tr>
</tbody>
</table>
<p>Figure 3: The GUI design of SimplyRetrieve features four primary tabs. The Chat tab serves as the central query and response interface with retrieval-centric tuning panel. The Prompt tab provides an intuitive editor for modifying, updating, and saving prompts used by the AI. The Config tab enables users to customize various tool settings and save their preferences. Finally, the Analysis tab offers a comprehensive analytics platform for analyzing and logging data related to SimplyRetrieve's performance and usage.
concurrent access to both UIs, leveraging Gradio's queue functionality to manage requests efficiently.</p>
<p>The retrieval-centric tuning panel enables lightweight and simplistic access to RCG. By using the manual knowledge base selection mode, users can construct and import multiple private knowledge bases simultaneously into this tool. The ability to select the most relevant knowledge base for each task allows users to maintain control over the selection process while avoiding any unexpected outcomes. Our MoKB mode enables automatic selection of the most suitable knowledge base based on the similarity between the query and knowledge base functional descriptions. We use semantic cosine similarity of embedding space to calculate these scores, providing an efficient and lightweight approach to knowledge base auto-selection. By updating the functional descriptions in the configuration file, users can further enhance the accuracy of the selection algorithm.</p>
<p>Additionally, our Explicit Prompt-Weighting feature allows manual adjustment of the degree of influence of retrievers on the language model, enabling customized control over the balance between retriever and LLM. Through prompt-engineering or token weight adjustment, users can adapt the tool to their specific needs, ensuring optimal performance. SimplyRetrieve has incorporated Explicit PromptWeighting through prompt-engineering, where the weightage can be adjusted to fine-tune the percentage of knowledge tokens to be used in the prompt out of retrieved tokens. However, we have not implemented token weight adjustment in this study and leave it for future work.</p>
<h3>3.2 Private Knowledge Base Constructor</h3>
<p>Our Retrieval-Centric Generation Platform is assisted by a Private Knowledge Base Constructor that creates a local and personalized knowledge base using the user's documents. This constructor employs a scalable documents loader that can handle large volumes of documents by chunking and streaming the loading, splitting and knowledge base creation processes, allowing for efficient document processing. The constructor supports various document formats such as PDF, TXT, DOC, DOCX, PPT, PPTX, HTML, MD, CSV, among others, and can be easily expanded by editing configuration file. Additionally, the length of passages in the documents splitting function is easily configurable to meet specific requirements.</p>
<p>After generating the sources for the knowledge base, we use a dense encoder to convert the text into numerical embeddings that can be used for semantic search and retrieve. To accommodate large-scale knowledge bases, we utilize ANNS for efficient semantic retrieval. By default, our tool employs the Hierarchical Navigable Small Worlds (HNSW) [malkov2020hnsw] algorithm, but we also provide support for flat indexing and the IVFPQ-HNSW method, which combines inverted file indexing with product quantization and HNSW course quantizers. The Index Constructor function automatically creates the required index files for semantic searching. We implement our indexing function by using Faiss library [johnson</p>
<p>et al., 2019).</p>
<h3>3.3 Retrieval Tuning Module</h3>
<p>The Retrieval Tuning Module of our tool includes three key functionalities: prompt-engineering, tool configuration, and analysis and data logging. The prompt-engineering functionality allows users to easily edit, update, and save retrieval-related prompts using a user-friendly Prompt Tab within our GUI. Available prompts are AI Prefix, Retriever Prefix, Retriever Suffix, Model Prefix and Model Suffix. The configuration functionality enables users to modify and save all configurable settings via the Config Tab within our GUI. Finally, the analysis and data logging functionality collects and displays retrieval-related analysis data, including retrieved knowledge base, query, response, sentencelevel and token-level similarity scores, in the Analysis Tab of our GUI. Similarity scores are calculated based on both semantic cosine similarity of sentence-to-sentence embeddings and all-token-totoken embeddings. This approach allows us to capture both local and global similarities between sentences, leading to more accurate assessments of their comparability. Additionally, users can save all logged data to a log file for further analysis. GUI designs are depicted in Figure 4, 5 and 6 of Appendix A.2. To deploy an end-user mode, users can simply disable the update functions in the Retrieval Tuning Module through command-line options.</p>
<h2>4 Evaluations</h2>
<p>In this section, we perform several qualitative evaluations to demonstrate the usability and behavior of our tool. We construct our knowledge base using the most recent information available on the website of an organization ${ }^{2}$. We utilize the models publicly available on Hugging Face, Wizard-Vicuna$13 B^{3}$ (Xu et al., 2023; Chiang et al., 2023) as the LLM and Multilingual-E5-base ${ }^{4}$ (Wang et al., 2022) as the encoder for our evaluations, unless specified otherwise. We load both models into a single Nvidia A100 GPU in 8-bit INT8 mode for lower memory usage and higher inference speed. We set temperature of LLM to 0 . We utilize HNSW for indexing of knowledge bases and set the number of passages retrieved to 5 .</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1 Qualitative Evaluation</h3>
<p>We compare the results of three approaches: Retrieval-Centric Generation (RCG), RetrievalAugmented Generation (RAG), and Retrieval-OFF Generation (ROG). Note that in this work, we define RAG as allowing more permissible integration of LLM's inherent and externally retrieved knowledge, whereas RCG prioritizes clear demarcations between context interpretation and knowledge memorization. Investigating advanced methods in extracting RCG behavior is a promising research topic. In this work, we conduct simple experiments using prompt-engineering technique to reveal the potential of RCG approach. Specifically, for RCG, we employ a retrieval suffix prompt that reads "answer the following question with the provided knowledge." For RAG, we use a less constraining prompt that reads "answer the following question. You may use the provided knowledge." Finally, for ROG, which is the baseline in our evaluation, we disable the retriever altogether. By using the prompt-engineering function in SimplyRetrieve, we can easily perform these experiments. Queries are human annotated. In Table 1 and 2, blue-colored texts indicates factually correct responses while red-colored texts indicates hallucinations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Query: where is Kioxia's factory?</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ApproachResponse</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RCG</td>
<td style="text-align: center;">Kioxia has factories located in both Yokkaichi city, Mie prefecture and Kitakami city, Iwate prefecture in Japan.</td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;">Kioxia has factories located in various regions across the globe including Japan, Thailand, Malaysia, China, and the United States. However, its main production site is located in Yokkaichi City, Mie Prefecture, Japan.</td>
</tr>
<tr>
<td style="text-align: center;">ROG</td>
<td style="text-align: center;">Our main production site for HDD and SSD products is located in Yamagata, Japan. We also have manufacturing sites in Thailand and the Philippines that produce optical disc media such as Bluray Discs and DVDs.</td>
</tr>
</tbody>
</table>
<p>Table 1: Qualitative comparison between RetrievalCentric, Retrieval-Augmented and Retrieval-OFF approaches using prompt-engineering in SimplyRetrieve.</p>
<p>Based on the results in Table 1, we observed that RCG successfully answered our query accurately, whereas RAG attempted to integrate the retrieved knowledge base with LLM's own knowledge of the specific organization, resulting in partially erroneous information. Meanwhile, ROG responded to our query with LLM's preexisting mistaken knowledge, generating falsehoods. Our find-</p>
<table>
<thead>
<tr>
<th>Query: what did Kioxia do in Image Classification</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Approach</td>
<td>Response</td>
<td></td>
</tr>
<tr>
<td>RCG</td>
<td>Kioxia developed a high capacity storage device which could store large amounts of image data required by AI algorithms. This allowed them to deploy memory centric AI systems capable of handling multiple classification tasks simultaneously without compromising performance or accuracy.</td>
<td></td>
</tr>
<tr>
<td>RCG-EPW</td>
<td>Deployment of Memory-centric AI with high capacity storage</td>
<td></td>
</tr>
<tr>
<td>ROG</td>
<td>The company developed a new image classification algorithm that achieved top-1 accuracy of 82.7% on the ImageNet validation set, which is an improvement over previous state-of-the-art results by more than one percentage point. This was made possible through advancements in hardware and software technologies such as Tensor Core GPUs and our own deep learning framework, ONNX-MXNET.</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: Effects of Retrieval-Centric Generation in SimplyRetrieve, based on the knowledge base about an organization.</p>
<h3>4.2 Accuracy &amp; Speed Evaluations</h3>
<p>In addition to evaluating the effectiveness of RCG using human annotations, we also conduct an internal evaluation of our tool’s performance using a self-generated dataset. To create this dataset, we pass relevant passages through the language model Llama-2-13B-chat [touvron2023llama] to generate 10 query and label pairs. For details on how we generated this dataset, refer to Appendix A.4. We employ Rouge-L score [lin2004rouge] as our performance metric. We perform this evaluation by using the API function of SimplyRetrieve. Our results in Table 3 show that RCG significantly improves the Rouge-L score compared to the baseline approach of ROG, while also slightly more competitive than RAG. Moreover, despite the fact that RCG processes longer prompts than ROG due to the addition of knowledge tokens, we observe a decrease in processing time owing to the increased precision and brevity of the generated responses. Specifically, number of response tokens generated in RCG are in average 36% less than those generated in ROG. This efficient performance may facilitate broader adoption within the community, as users can expect quicker response generation without sacrificing accuracy.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Rouge-L Score</th>
<th>time/query(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ROG</td>
<td>0.186</td>
<td>17.22</td>
</tr>
<tr>
<td>RAG</td>
<td>0.359</td>
<td>18.41</td>
</tr>
<tr>
<td>RCG</td>
<td>0.413</td>
<td>11.67</td>
</tr>
</tbody>
</table>
<p>Table 3: Response accuracy &amp; speed evaluation of SimplyRetrieve.</p>
<p>Finally, our findings suggest that even a modestly sized LLM of 13B parameters can demonstrate satisfactory performance in RCG approach towards never-seen-before factual knowledge without any model fine-tuning, potentially facilitates the deployment of Generative AI systems in real-world scenarios. See Appendix A.2 for further discussions and A.5 for ablation studies.</p>
<h2>5 Conclusion</h2>
<p>We introduced SimplyRetrieve, an open-source tool that aims to provide a localizable, lightweight, and user-friendly GUI and API platform for a Retrieval-Centric Generation approach based on LLMs. Our tool enables developers and end-users to easily interact and develop with a privacy-preserving and locally implemented LLM-based RCG system, which we believe will contribute to the democratization of these technologies within the machine learning community. Increased clarity in role-separation between context interpretation and knowledge memorization can potentially boost the performance and interpretability of generative AI systems, facilitating deployments.</p>
<h2>Limitations</h2>
<p>It is important to note that this tool does not provide a foolproof solution for ensuring a completely safe and responsible response from generative AI models, even within a retrieval-centric approach. The</p>
<p>development of safer, interpretable, and responsible AI systems remains an active area of research and ongoing effort.</p>
<p>Generated texts from this tool may exhibit variations, even when only slightly modifying prompts or queries, due to the next token prediction behavior of current-generation LLMs. This means users may need to carefully fine-tune both the prompts and queries to obtain optimal responses.</p>
<h2>References</h2>
<p>Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019. Gradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206-2240. PMLR.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Harrison Chase. 2022. LangChain.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML'20. JMLR.org.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Ahmet Iscen, Alireza Fathi, and Cordelia Schmid. 2023. Improving image recognition by retrieving from web-scale image-text data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19295-19304.</p>
<p>Peter Izsak, Moshe Berchansky, Daniel Fleischer, and Ronen Laperdon. 2023. fastRAG: Efficient Retrieval Augmentation and Generation Framework.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the</p>
<p>2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yu A. Malkov and D. A. Yashunin. 2020. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Trans. Pattern Anal. Mach. Intell., 42(4):824-836.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802-9822, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Kengo Nakata, Youyang Ng, Daisuke Miyashita, Asuka Maki, Yu-Chieh Lin, and Jun Deguchi. 2022. Revisiting a knn-based image classification system with high-capacity storage. In Computer Vision ECCV 2022, pages 457-474, Cham. Springer Nature Switzerland.</p>
<p>OpenAI. 2023. Chatgpt. https://openai.com/blog/ chatgpt.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc.</p>
<p>Malte Pietsch, Timo Möller, Bogdan Kostic, Julian Risch, Massimiliano Pippi, Mayank Jobanputra, Sara Zanzottera, Silvano Cerza, Vladimir Blagojevic, Thomas Stadelmann, Tanay Soni, and Sebastian Lee. 2019. Haystack: the end-to-end NLP framework for pragmatic builders.</p>
<p>PrivateGPT. PrivateGPT. Accessed: 2023-07-04.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,</p>
<p>and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.</p>
<h2>A Appendix</h2>
<h2>A. 1 GUI Design of Retrieval Tuning Module</h2>
<p>Figure 4 shows the GUI design of promptengineering interface. Figure 5 shows the GUI design of tool configuration interface. Figure 6 shows the GUI design of analysis and data logging interface.</p>
<h2>A. 2 Applications</h2>
<p>SimplyRetrieve has vast potential for various practical applications. For instance, it can serve as the foundation for building private, personalized, and lightweight generative AI systems. Sensitive and personal information can be securely stored and processed within the retrieval-centric platform. This approach enables organizations to develop interpretable and locally tailored generative AI systems for critical infrastructure. Additionally, the use of a relatively smaller language model as a contextual interpreter in this approach facilitates seamless integration into edge computing environments. The decreasing costs of data storage devices also make it feasible to establish large-scale knowledge bases. Furthermore, SimplyRetrieve paves the way for the development of LLM-based personalized AI assistants. Lastly, an in-depth exploration of LLM-based retrieval-centric generation using SimplyRetrieve may offer valuable insights and opportunities for future research.</p>
<h2>A. 3 Prompt Catalogs</h2>
<p>Table 5 shows the prompts used in the evaluation results of Section 4 while Table 6 shows sample prompts that may exhibit retrieval-centric behaviors. Prompts are passed to LLM in the following format: AI Prefix + Retriever Prefix + Retrieved Knowledge Base + Retriever Suffix + Model Prefix + Query + Model Suffix.</p>
<h2>A. 4 Evaluation Data</h2>
<p>Table 7 presents the data used for evaluating the performance of our proposed tool in Section 4.2. We employed the Llama-2-13B-chat model (Touvron et al., 2023b) with a customized prompt ("relevant information." Please create a query and answer from the paragraph above) to generate query and label pairs automatically from relevant information on the website of an organization.</p>
<h2>A. 5 Ablation Study</h2>
<p>As shown in Table 4, our ablation study reveals that adjusting Explicit Prompt-Weighting in SimplyRetrieve leads to significant improvements in RougeL scores. Interestingly, increasing the weightage to $50 \%$ yields the highest improvement, beyond which the performance remains relatively stable. This suggests that the top $50 \%$ of retrieved knowledge bases are crucial for achieving high accuracy. However, it is important to note that these findings may not generalize to all datasets or knowledge bases, and further investigation may be necessary to determine optimal weightages for specific use cases. In comparing the response times for each query across different settings, we observe that the response times remain relatively consistent for all cases of RCG, while they increase significantly in the baseline (ROG) setting. Despite the fact that RCG processes longer prompts than the baseline, we observe a decrease in processing time owing to the increased precision and brevity of the generated responses.</p>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Rouge-L</th>
<th>time/query(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ROG</td>
<td>0.186</td>
<td>17.22</td>
</tr>
<tr>
<td>RCG-EPW-10</td>
<td>0.275</td>
<td>12.72</td>
</tr>
<tr>
<td>RCG-EPW-20</td>
<td>0.313</td>
<td>13.00</td>
</tr>
<tr>
<td>RCG-EPW-30</td>
<td>0.403</td>
<td>13.06</td>
</tr>
<tr>
<td>RCG-EPW-40</td>
<td>0.354</td>
<td>11.98</td>
</tr>
<tr>
<td>RCG-EPW-50</td>
<td>0.414</td>
<td>12.46</td>
</tr>
<tr>
<td>RCG-EPW-60</td>
<td>0.331</td>
<td>11.36</td>
</tr>
<tr>
<td>RCG-EPW-70</td>
<td>0.392</td>
<td>13.56</td>
</tr>
<tr>
<td>RCG-EPW-80</td>
<td>0.306</td>
<td>16.32</td>
</tr>
<tr>
<td>RCG-EPW-90</td>
<td>0.378</td>
<td>13.13</td>
</tr>
<tr>
<td>RCG</td>
<td>0.413</td>
<td>11.67</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study of Explicit Prompt-Weighting in SimplyRetrieve.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">AI Prefix</th>
<th style="text-align: left;">Retriever Prefix</th>
<th style="text-align: left;">Retriever Suffix</th>
<th style="text-align: left;">Model Prefix</th>
<th style="text-align: left;">Model Suffix</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">"</td>
<td style="text-align: left;">" <br> answer the following question with <br> the provided knowledge.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">AI:</td>
</tr>
</tbody>
</table>
<p>Table 5: Prompts used in the evaluation results of Section 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">AI Prefix</th>
<th style="text-align: center;">Retriever Prefix</th>
<th style="text-align: center;">Retriever Suffix</th>
<th style="text-align: center;">Model Prefix</th>
<th style="text-align: center;">Model Suffix</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">you are a Retrieval-Centric <br> AI. Knowledge below are <br> provided.</td>
<td style="text-align: center;">"</td>
<td style="text-align: center;">" <br> only use the provided <br> knowledge to answer the <br> following question.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"</td>
<td style="text-align: center;">" <br> answer the following ques- <br> tion with the provided <br> knowledge.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AI:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"</td>
<td style="text-align: center;">" <br> only use the provided <br> knowledge to answer the <br> following question.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AI:</td>
</tr>
<tr>
<td style="text-align: center;">you are a Retrieval-Centric <br> AI. Knowledge below are <br> provided.</td>
<td style="text-align: center;">"</td>
<td style="text-align: center;">" <br> only use the provided <br> knowledge to answer the <br> following question.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AI:</td>
</tr>
</tbody>
</table>
<p>Table 6: Sample Prompts Catalog of Retrieval-Centric Generation in SimplyRetrieve.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The Prompt-Engineering interface of SimplyRetrieve. The Tab is for editing, updating and saving of model-related and retrieval-related prompts. Available prompts are AI Prefix, Retriever Prefix, Retriever Suffix, Model Prefix and Model Suffix.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The Tool Configuration interface of SimplyRetrieve. The Tab is for modifying, updating and saving all configurable settings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Chat Prompt Config Analysis</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Config File</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Config Editing</td>
</tr>
<tr>
<td style="text-align: center;">I <br> "itm_config": <br> "model_args": <br> "model_type": \'red\Livser\youyang/\M_models\Wizard-Vicuna-LSB-Umzensored", <br> "device_map": <br> "":0 <br> L <br> "quantization_config": <br> Config Updating and Saving</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Update Config</td>
<td style="text-align: center;">Save Config</td>
<td style="text-align: center;">Save Path <br> configs/default_chat1_paper_new.json</td>
</tr>
</tbody>
</table>
<p>Figure 5: The Tool Configuration interface of SimplyRetrieve. The Tab is for modifying, updating and saving all configurable settings.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Chat Prompt Config Analysis</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">- Save Logging</td>
<td style="text-align: center;">Sentence \&amp; Token Levels Retrieval Similarity Analysis</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Query \&amp; KnowledgeBase Sentence Level Similarity Score</td>
<td style="text-align: center;">Query \&amp; KnowledgeBase Tokens Level Similarity Score</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0.8461886476450748</td>
<td style="text-align: center;">0.7471086010310315</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Response \&amp; KnowledgeBase Sentence Level Similarity Score</td>
<td style="text-align: center;">Response \&amp; KnowledgeBase Tokens Level Similarity Score</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0.876272000649239</td>
<td style="text-align: center;">0.7219502029734694</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Query</td>
<td style="text-align: center;">Prompts Analysis</td>
<td style="text-align: center;">Prompt</td>
</tr>
<tr>
<td style="text-align: center;">Tell me about the cutting edge deep learning research in 2021</td>
<td style="text-align: center;">"As of March 31, 2021" What is NAND Flash Memory? 2023/06/21 20:48 System technology Development of Image Classification System Deploying Memory Centric AI with High capacity Storage (. . . https://www.kluxus.com/en- <br> gz/rd technology/topics/topics-20.html 1/2 Development of Image Class at when System Deploying Memory Centric AI with High capacity Storage January 12, 2021 In deep learning, neural networks can use Big Data Automated Classrooms of infrastructure and Big Data Sophisticated in based Analysis Image Class at when using Deep Learning Digital Tools In and Tr mated neural Networks (https://www.kluxus.com/en-gz/rd-technology/topics/topics-20.html 2/279 g z Accuracy with platform when increasing</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The cutting edge deep learning research in 2023 focuses on developing memory-centric artificial intelligence that can be used in digital tools in and Tr mated neural networks (https://www.kluxus.com/en-gz/rd-technology/topics/topics-20.html 1/2 Development of Image Class at when using Deep Learning Digital Tools In and Tr mated Neural Networks (https://www.kluxus.com/en-gz/rd-technology/topics/topics-20.html 2/279 g z Accuracy with platform when increasing</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Logs Display \&amp; Store</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">https://www.kluxus.com/en-gz/rd-technology/topics/topics-20.html 2/279 g z Accuracy with platform when increasing</td>
</tr>
<tr>
<td style="text-align: center;">Load Logs</td>
<td style="text-align: center;">Save Logs</td>
<td style="text-align: center;">Save Log Path <br> analysis/logs.cio</td>
</tr>
</tbody>
</table>
<p>Figure 6: The Analysis and Data Logging interface of SimplyRetrieve. The Tab is for analysis and logging of retrieved knowledge base, query, response, retrieval sentence level and tokens level similarity scores. Users can save all the logged data for further analysis.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">What is KIOXIA known for in terms of innovation?</td>
<td style="text-align: center;">Based on the text, KIOXIA is known for inventing NAND flash memory in 1987 and advancing the information society on a global scale through its R\&amp;D efforts. The company has also been recognized for presenting its research and development achievements through various channels, including major academic societies and research conferences.</td>
</tr>
<tr>
<td style="text-align: center;">What is the purpose of establishing KIOXIA Iwate Corporation?</td>
<td style="text-align: center;">The purpose of establishing KIOXIA Iwate Corporation is to meet the growing demand for flash memory by leveraging advanced manufacturing processes that utilize AI.</td>
</tr>
<tr>
<td style="text-align: center;">When did Kioxia start operating its new fabrication facility (Fab7)?</td>
<td style="text-align: center;">According to the text, Kioxia started operating its new fabrication facility (Fab7) in the fall of 2022.</td>
</tr>
<tr>
<td style="text-align: center;">What are some ways in which the Yokkaichi Plant contributes to society?</td>
<td style="text-align: center;">The Yokkaichi Plant contributes to society by manufacturing high-quality flash memory and SSDs that support the digitalization of various industries, as well as through its research and development (R\&amp;D) activities. Additionally, the plant prioritizes legal compliance, environmental protection, and social initiatives, making it a responsible corporate citizen.</td>
</tr>
<tr>
<td style="text-align: center;">What is KIOXIA's approach to corporate social responsibility?</td>
<td style="text-align: center;">According to the passage, KIOXIA's approach to corporate social responsibility involves making their Yokkaichi Plant a factory that is admired and trusted by everyone in the community. They aim to contribute to local development and meet the expectations of local communities by being a responsible member of society.</td>
</tr>
<tr>
<td style="text-align: center;">What kind of environmental initiatives has KIOXIA taken at its Yokkaichi Plant?</td>
<td style="text-align: center;">At KIOXIA's Yokkaichi Plant, all employees, including resident suppliers, actively participate in various initiatives to contribute to society by improving the environment. These include measures to mitigate the environmental burden of the company's semiconductor manufacturing facilities.</td>
</tr>
<tr>
<td style="text-align: center;">What was the total revenue for the Kioxia group in fiscal year 21 ?</td>
<td style="text-align: center;">According to the passage, the total revenue for the Kioxia group in fiscal year 21 was 1,526.5 billion yen.</td>
</tr>
<tr>
<td style="text-align: center;">What initiatives has the Yokkaichi Plant taken to protect the environment?</td>
<td style="text-align: center;">The Yokkaichi Plant has taken various initiatives such as reducing greenhouse gas emissions, conserving water resources, and implementing recycling programs to protect the environment. These efforts aim to not only reduce the plant's ecological footprint but also raise awareness among local communities about the importance of environmental conservation.</td>
</tr>
<tr>
<td style="text-align: center;">What is KIOXIA's mission statement?</td>
<td style="text-align: center;">According to the passage, KIOXIA's mission statement is to uplift the world with 'memory', which means they aim to provide high-quality memory and SSD products that offer fast speeds, large capacities, and reliable performance. Additionally, the company seeks to achieve this goal through collaboration and knowledge sharing across different departments and organizations.</td>
</tr>
<tr>
<td style="text-align: center;">What is the purpose of collecting over two billion data items daily?</td>
<td style="text-align: center;">To maintain high-quality products by analyzing the vast amounts of data generated during the manufacturing process using advanced technologies like deep learning and AI.</td>
</tr>
</tbody>
</table>
<p>Table 7: Dataset used in the evaluation results of Section 4.2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.kioxia.com/en-jp/top.html
${ }^{3}$ https://huggingface.co/ehartford/
Wizard-Vicuna-13B-Uncensored
${ }^{4}$ https://huggingface.co/intfloat/
multilingual-e5-base&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
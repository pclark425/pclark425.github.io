<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2127 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2127</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2127</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-55.html">extraction-schema-55</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-278739825</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12039v1.pdf" target="_blank">AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</a></p>
                <p><strong>Paper Abstract:</strong> The Science of Science (SoS) explores the mechanisms underlying scientific discovery, and offers valuable insights for enhancing scientific efficiency and fostering innovation. Traditional approaches often rely on simplistic assumptions and basic statistical tools, such as linear regression and rule-based simulations, which struggle to capture the complexity and scale of modern research ecosystems. The advent of artificial intelligence (AI) presents a transformative opportunity for the next generation of SoS, enabling the automation of large-scale pattern discovery and uncovering insights previously unattainable. This paper offers a forward-looking perspective on the integration of Science of Science with AI for automated research pattern discovery and highlights key open challenges that could greatly benefit from AI. We outline the advantages of AI over traditional methods, discuss potential limitations, and propose pathways to overcome them. Additionally, we present a preliminary multi-agent system as an illustrative example to simulate research societies, showcasing AI's ability to replicate real-world research patterns and accelerate progress in Science of Science research.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2127.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2127.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Citation counts (proxy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation counts as a proxy for scientific impact</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses citation counts as the primary quantitative proxy for paper impact in experiments and discusses limitations of citations as a proxy for true scientific value, especially for novel or disruptive work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>citation counts</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>In simulations: ethnicity diversity (Shannon entropy), affiliation diversity, affiliation ranking; more generally the paper cites literature measures such as atypical combinations / Z-score of referenced journal pairings as candidate novelty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>The paper reports qualitative/empirical correlations from simulation: higher ethnicity diversity correlates with higher citation counts; affiliation ranking negatively correlates with citation counts. No numeric effect sizes or correlation coefficients are reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>The authors validate simulated citation patterns against real-world data from 2010 and 2011 and report minor year-to-year variations; no specific temporal gap (e.g., undervaluation years) is quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Multidisciplinary (Open Academic Graph spanning physics, chemistry, computer science, social sciences, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Paper intends cross-disciplinary analysis but does not report explicit quantified differences between fields in proxy vs ground truth behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>The multi-agent LLM simulation partially reproduces real-world citation correlations but with weaker strength; affiliation diversity correlation in the simulation was not statistically significant (p > 0.05). No accuracy metrics for identifying transformational vs incremental work are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Suggested mitigation (conceptual): reduce probability of citing highly cited papers in agent citation selection and increase probability of citing less-cited, more novel work; not experimentally evaluated in reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Yes — the paper explicitly discusses training-data bias and data imbalance across disciplines and how models trained on historically skewed data can propagate biases affecting citation-based predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>LLM-based multi-agent simulation using OAG 3.11 (35M authors, 131M papers) as initial database; simulated a society of 1,000,000 agents over 40 epochs, used citation counts updated during idea-generation retrievals, and compared simulated citation patterns to real-world data from 2010–2011.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2127.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2127.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Peer-review score (simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerical peer-review scores used in the multi-agent simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The simulation assigns peer-review scores (1–10) to agent-authored papers, and accepts papers exceeding threshold (>5) into the reference database; peer review is also discussed as an evaluation axis and as a candidate automated evaluation measure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>peer review scores (1-10 acceptance threshold >5)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Not operationalized quantitatively in the experiment; the paper references model-based peer-review scoring and Z-score of referenced journal pairs as literature approaches to assess novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Cross-disciplinary (peer review criteria adapted from NeurIPS-style guidelines for multidisciplinary papers)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not reported; peer review is implemented mechanistically in the simulation (3 reviewers per paper) but no performance metrics (e.g., precision/recall for novel work) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Paper notes that review/indexing systems and reviewer heuristics can embed cross-disciplinary bias; simulation uses NeurIPS-derived criteria which may carry domain-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Simulation implements a review-and-indexing pipeline: each submitted paper is reviewed by 3 reviewers, scored 1–10, and accepted if score >5; accepted papers are added to the reference database and then can accrue citations in subsequent epochs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2127.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2127.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rich-get-richer effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rich-get-richer (preferential attachment) in citations, prestige, and funding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses the well-documented rich-get-richer phenomenon (preferential attachment) whereby historical prominence (high citations, prestige, funding) begets future visibility, and warns that AI systems trained on historical data can amplify this effect, disadvantaging novel or unconventional work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>highly cited paper status / journal prestige / historical funding trends</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td>Described qualitatively; the paper references prior literature documenting the effect but does not provide new numeric quantification in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>General / cross-disciplinary (discussed as a systemic property across fields)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td>The paper warns that multiple biases (citation, funding, prestige) can interact and be amplified by AI, but does not quantify whether effects are multiplicative or additive.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Proposed mitigation (conceptual): in agent citation-selection, lower the probability of citing already highly cited papers and increase probability of citing less-cited or novel papers to promote diversity; not empirically tested in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Yes — the authors emphasize that AI trained on historical SoS data may perpetuate and amplify existing inequalities and mainstreaming of topics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Conceptual discussion grounded in literature; the simulation incorporates citation updating mechanics that can reproduce preferential-attachment-like patterns, but no controlled experiment isolating the rich-get-richer amplification is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2127.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2127.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how proxy metrics (like citations, journal prestige, peer review scores) compare to ground truth measures of scientific value, especially for novel or transformational versus incremental work, including quantitative relationships, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alternative novelty metrics (literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alternative novelty/impact measures discussed (Z-score for journal pairings, model-based peer-review scoring, atypical combination metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper surveys candidate methods from the literature for assessing novelty and transformational value (e.g., Z-score for pairs of referenced journals, atypical combinations, and large-model peer-review scoring) and suggests these could be integrated into AI4SoS evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>atypical combination scores / Z-score of referenced journal pairings / model-based peer-review scores</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_transformation_measure</strong></td>
                            <td>Z-score for each pairing of referenced journals (cited from prior literature), atypical combination measures (cited literature), and model-based peer-review scoring (suggested as novelty indicator).</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_relationship</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Referenced literature spans multiple fields; the paper does not present field-specific evaluations of these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiplicative_vs_additive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Not evaluated in this paper; suggested as prospective methods for automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>correction_mechanism</strong></td>
                            <td>Presented as recommended evaluation augmentations (e.g., use Z-scores and model-based review to better capture novelty) but not experimentally validated here.</td>
                        </tr>
                        <tr>
                            <td><strong>training_distribution_bias</strong></td>
                            <td>Paper notes that automated novelty detectors and LLM-based reviewers could inherit biases from their training corpora; recommends diverse datasets and fairness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>study_design</strong></td>
                            <td>Survey / methodological recommendation — the paper cites prior work that uses these metrics but does not itself compute them in the reported simulation results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large teams develop and small teams disrupt science and technology <em>(Rating: 2)</em></li>
                <li>The diversity-innovation paradox in science <em>(Rating: 2)</em></li>
                <li>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines <em>(Rating: 2)</em></li>
                <li>On modeling and predicting individual paper citation count over time <em>(Rating: 2)</em></li>
                <li>Which type of citation analysis generates the most accurate taxonomy of scientific and technical knowledge <em>(Rating: 1)</em></li>
                <li>Leading countries in global science increasingly receive more citations than other countries doing similar research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2127",
    "paper_id": "paper-278739825",
    "extraction_schema_id": "extraction-schema-55",
    "extracted_data": [
        {
            "name_short": "Citation counts (proxy)",
            "name_full": "Citation counts as a proxy for scientific impact",
            "brief_description": "The paper uses citation counts as the primary quantitative proxy for paper impact in experiments and discusses limitations of citations as a proxy for true scientific value, especially for novel or disruptive work.",
            "citation_title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research",
            "mention_or_use": "use",
            "proxy_metric_type": "citation counts",
            "ground_truth_measure": null,
            "novelty_transformation_measure": "In simulations: ethnicity diversity (Shannon entropy), affiliation diversity, affiliation ranking; more generally the paper cites literature measures such as atypical combinations / Z-score of referenced journal pairings as candidate novelty measures.",
            "quantitative_relationship": "The paper reports qualitative/empirical correlations from simulation: higher ethnicity diversity correlates with higher citation counts; affiliation ranking negatively correlates with citation counts. No numeric effect sizes or correlation coefficients are reported in the text.",
            "gap_magnitude": null,
            "temporal_pattern": "The authors validate simulated citation patterns against real-world data from 2010 and 2011 and report minor year-to-year variations; no specific temporal gap (e.g., undervaluation years) is quantified.",
            "field_studied": "Multidisciplinary (Open Academic Graph spanning physics, chemistry, computer science, social sciences, etc.)",
            "field_differences": "Paper intends cross-disciplinary analysis but does not report explicit quantified differences between fields in proxy vs ground truth behavior.",
            "multiplicative_vs_additive": null,
            "automated_system_performance": "The multi-agent LLM simulation partially reproduces real-world citation correlations but with weaker strength; affiliation diversity correlation in the simulation was not statistically significant (p &gt; 0.05). No accuracy metrics for identifying transformational vs incremental work are reported.",
            "correction_mechanism": "Suggested mitigation (conceptual): reduce probability of citing highly cited papers in agent citation selection and increase probability of citing less-cited, more novel work; not experimentally evaluated in reported results.",
            "training_distribution_bias": "Yes — the paper explicitly discusses training-data bias and data imbalance across disciplines and how models trained on historically skewed data can propagate biases affecting citation-based predictions.",
            "counterexamples": null,
            "study_design": "LLM-based multi-agent simulation using OAG 3.11 (35M authors, 131M papers) as initial database; simulated a society of 1,000,000 agents over 40 epochs, used citation counts updated during idea-generation retrievals, and compared simulated citation patterns to real-world data from 2010–2011.",
            "uuid": "e2127.0"
        },
        {
            "name_short": "Peer-review score (simulation)",
            "name_full": "Numerical peer-review scores used in the multi-agent simulation",
            "brief_description": "The simulation assigns peer-review scores (1–10) to agent-authored papers, and accepts papers exceeding threshold (&gt;5) into the reference database; peer review is also discussed as an evaluation axis and as a candidate automated evaluation measure.",
            "citation_title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research",
            "mention_or_use": "use",
            "proxy_metric_type": "peer review scores (1-10 acceptance threshold &gt;5)",
            "ground_truth_measure": null,
            "novelty_transformation_measure": "Not operationalized quantitatively in the experiment; the paper references model-based peer-review scoring and Z-score of referenced journal pairs as literature approaches to assess novelty.",
            "quantitative_relationship": null,
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Cross-disciplinary (peer review criteria adapted from NeurIPS-style guidelines for multidisciplinary papers)",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Not reported; peer review is implemented mechanistically in the simulation (3 reviewers per paper) but no performance metrics (e.g., precision/recall for novel work) are provided.",
            "correction_mechanism": null,
            "training_distribution_bias": "Paper notes that review/indexing systems and reviewer heuristics can embed cross-disciplinary bias; simulation uses NeurIPS-derived criteria which may carry domain-specific biases.",
            "counterexamples": null,
            "study_design": "Simulation implements a review-and-indexing pipeline: each submitted paper is reviewed by 3 reviewers, scored 1–10, and accepted if score &gt;5; accepted papers are added to the reference database and then can accrue citations in subsequent epochs.",
            "uuid": "e2127.1"
        },
        {
            "name_short": "Rich-get-richer effect",
            "name_full": "Rich-get-richer (preferential attachment) in citations, prestige, and funding",
            "brief_description": "The paper discusses the well-documented rich-get-richer phenomenon (preferential attachment) whereby historical prominence (high citations, prestige, funding) begets future visibility, and warns that AI systems trained on historical data can amplify this effect, disadvantaging novel or unconventional work.",
            "citation_title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research",
            "mention_or_use": "mention",
            "proxy_metric_type": "highly cited paper status / journal prestige / historical funding trends",
            "ground_truth_measure": null,
            "novelty_transformation_measure": null,
            "quantitative_relationship": "Described qualitatively; the paper references prior literature documenting the effect but does not provide new numeric quantification in this manuscript.",
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "General / cross-disciplinary (discussed as a systemic property across fields)",
            "field_differences": null,
            "multiplicative_vs_additive": "The paper warns that multiple biases (citation, funding, prestige) can interact and be amplified by AI, but does not quantify whether effects are multiplicative or additive.",
            "automated_system_performance": null,
            "correction_mechanism": "Proposed mitigation (conceptual): in agent citation-selection, lower the probability of citing already highly cited papers and increase probability of citing less-cited or novel papers to promote diversity; not empirically tested in the paper.",
            "training_distribution_bias": "Yes — the authors emphasize that AI trained on historical SoS data may perpetuate and amplify existing inequalities and mainstreaming of topics.",
            "counterexamples": null,
            "study_design": "Conceptual discussion grounded in literature; the simulation incorporates citation updating mechanics that can reproduce preferential-attachment-like patterns, but no controlled experiment isolating the rich-get-richer amplification is reported.",
            "uuid": "e2127.2"
        },
        {
            "name_short": "Alternative novelty metrics (literature)",
            "name_full": "Alternative novelty/impact measures discussed (Z-score for journal pairings, model-based peer-review scoring, atypical combination metrics)",
            "brief_description": "The paper surveys candidate methods from the literature for assessing novelty and transformational value (e.g., Z-score for pairs of referenced journals, atypical combinations, and large-model peer-review scoring) and suggests these could be integrated into AI4SoS evaluation pipelines.",
            "citation_title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research",
            "mention_or_use": "mention",
            "proxy_metric_type": "atypical combination scores / Z-score of referenced journal pairings / model-based peer-review scores",
            "ground_truth_measure": null,
            "novelty_transformation_measure": "Z-score for each pairing of referenced journals (cited from prior literature), atypical combination measures (cited literature), and model-based peer-review scoring (suggested as novelty indicator).",
            "quantitative_relationship": null,
            "gap_magnitude": null,
            "temporal_pattern": null,
            "field_studied": "Referenced literature spans multiple fields; the paper does not present field-specific evaluations of these metrics.",
            "field_differences": null,
            "multiplicative_vs_additive": null,
            "automated_system_performance": "Not evaluated in this paper; suggested as prospective methods for automated evaluation.",
            "correction_mechanism": "Presented as recommended evaluation augmentations (e.g., use Z-scores and model-based review to better capture novelty) but not experimentally validated here.",
            "training_distribution_bias": "Paper notes that automated novelty detectors and LLM-based reviewers could inherit biases from their training corpora; recommends diverse datasets and fairness checks.",
            "counterexamples": null,
            "study_design": "Survey / methodological recommendation — the paper cites prior work that uses these metrics but does not itself compute them in the reported simulation results.",
            "uuid": "e2127.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large teams develop and small teams disrupt science and technology",
            "rating": 2
        },
        {
            "paper_title": "The diversity-innovation paradox in science",
            "rating": 2
        },
        {
            "paper_title": "Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines",
            "rating": 2
        },
        {
            "paper_title": "On modeling and predicting individual paper citation count over time",
            "rating": 2
        },
        {
            "paper_title": "Which type of citation analysis generates the most accurate taxonomy of scientific and technical knowledge",
            "rating": 1
        },
        {
            "paper_title": "Leading countries in global science increasingly receive more citations than other countries doing similar research",
            "rating": 1
        }
    ],
    "cost": 0.012452499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research
17 May 2025</p>
<p>Renqi Chen 
Shanghai Artificial Intelligence Laboratory</p>
<p>Haoyang Su 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shixiang Tang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Department of Information Engineering
Chinese University of Hong Kong</p>
<p>Zhenfei Yin 
Department of Engineering Science
University of Oxford</p>
<p>Qi Wu 
Shanghai Institute for Science of Science</p>
<p>Hui Li 
Shanghai Institute for Science of Science</p>
<p>Ye Sun 
School of Mathematics
Southeast University</p>
<p>Nanqing Dong dongnanqing@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Wanli Ouyang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Department of Information Engineering
Chinese University of Hong Kong</p>
<p>Philip Torr 
Department of Engineering Science
University of Oxford</p>
<p>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research
17 May 2025918871C0DF3B560F1FFE6D5A59EFAEDBarXiv:2505.12039v1[cs.AI]
The Science of Science (SoS) explores the mechanisms underlying scientific discovery, and offers valuable insights for enhancing scientific efficiency and fostering innovation.Traditional approaches often rely on simplistic assumptions and basic statistical tools, such as linear regression and rule-based simulations, which struggle to capture the complexity and scale of modern research ecosystems.The advent of artificial intelligence (AI) presents a transformative opportunity for the next generation of SoS, enabling the automation of large-scale pattern discovery and uncovering insights previously unattainable.This paper offers a forward-looking perspective on the integration of Science of Science with AI for automated research pattern discovery and highlights key open challenges that could greatly benefit from AI.We outline the advantages of AI over traditional methods, discuss potential limitations, and propose pathways to overcome them.Additionally, we present * Equal contributions.</p>
<p>Introduction</p>
<p>Science of Science (SoS), a pivotal and rapidly evolving field, serves as a strategic compass for guiding the trajectory of scientific and technological progress.By analyzing the complex dynamics of research collaboration and scientific output across geographic and temporal scales, it sheds light on the factors that drive creativity and the emergence of scientific discoveries, with the goal of developing tools and policies to accelerate scientific advancement [1].Unlike broader social sciences that examine societal structures, SoS delves deep into the mechanisms that fuel scientific breakthroughs [2][3][4]-illuminating the hidden forces that propel discovery and transformation.Ultimately, SoS underscores that groundbreaking advancements are not solely the result of talented minds and quality data, but are profoundly shaped by effective resource allocation, supportive policies and well-designed organizational structures [5,6].</p>
<p>In recent years, the deep fusion of AI and SoS has become more feasible and promising than ever before.First, the increasing availability of large-scale scholarly data-publications, funding records, and collaboration networks-provides unprecedented opportunities to gain deeper insights into the evolution of scientific progress.Second, rapid advancements in AI technologies, such as large language models (LLMs), along with improvements in computational power, have greatly enhanced our ability to analyze and interpret complex scientific information with unprecedented accuracy and scale.These technological breakthroughs mark a critical moment for integrating AI into SoS, paving the way for a more data-driven approach to understanding and guiding research pattern discovery.While some recent works have begun exploring autonomous scientific discovery, the field remains in its infancy, and there is still much progress to be made before realizing its full potential.</p>
<p>In this paper, we take a step forward by providing the first glimpse into the integration of AI and SoS for automated research pattern discovery.We take the position that AI has the potential to revolutionize SoS, enabling the next generation of research by not only automating traditional research processes but also providing a sandbox for SoS research, allowing scientists to observe research processes in action and validate their hypotheses.As illustrated in Fig. 1, traditional SoS methods have primarily relied on manual data processing, bibliometric-based data analysis, rule-based system simulations, and real-world pattern validation.In contrast, AI-driven SoS leverages automated techniques to assist scientists  in processing and analyzing data while offering more advanced and comprehensive systems for simulation and validation.This shift from human-driven to AI-driven methodologies unlocks the potential for more efficient, scalable, and data-driven analysis, ultimately providing deeper and more actionable insights into the mechanisms that shape scientific progress.Thus, we define AI for SoS (AI4SoS) as a cross-disciplinary field that not only focuses on facilitating each step within the research process but also aims to achieve fully automated SoS research to uncover the hidden forces driving scientific innovation.This distinguishes AI4SoS from existing AI for Science approaches, which focus on using AI tools to solve domain-specific scientific problems [7][8][9].</p>
<p>To consolidate our insights, we propose a forward-looking hierarchy of different levels of AI4SoS automation in Sec.2.3, which outlines a possible step-by-step approach to achieving the goal of fully automated SoS discovery.Within each level, we describe the key differences compared to previous levels and provide related examples.In Sec. 3, we highlight critical open problems in SoS where AI offers advantages.Despite its promise, we discuss challenges such as data imbalance across disciplines in Sec. 4, overwhelming parameters in the simulation system of scientific societies, and the need for a reasonable evaluation system to validate the reliability of the simulation.We also propose possible pathways to overcome these challenges.Last but not least, we introduce a preliminary multi-agent system to simulate research societies in Sec. 5, illustrating AI's capability to enable fully automated pattern discovery.</p>
<p>AI4SoS</p>
<p>Comparison between AI for Science and AI4SoS</p>
<p>Both AI for Science (AI4S) and AI4SoS aim to leverage AI to solve scientific problems.However, they differ in research goals.AI4S focuses on solving a particular scientific problem directly, such as weather prediction, drug discovery, and materials science.AI4SoS takes a meta-level approach, focusing on understanding the mechanisms of scientific progress to facilitate and accelerate research.More specifically, AI4SoS focuses on factors such as innovation drivers, research collaboration patterns, and the evolution of scientific knowledge, which are not bound to a single scientific problem.</p>
<p>Hierarchy of Automation Degree in AI4SoS</p>
<p>The integration of AI techniques into scientific research follows a progressive hierarchy, reflecting the increasing autonomy and sophistication of AI systems in advancing the SoS field.As illustrated in Fig. 2, we define five levels of autonomy, ranging from no AI involvement in pattern recognition and analysis to full autonomy in uncovering new scientific insights and guiding research strategies.Level 0: Non-automated SoS Discovery At this level, scientific pattern discovery is entirely human-driven and relies on traditional statistical methods.Researchers apply fundamental techniques such as probabilistic models, linear regression, and hypothesis testing to analyze scientific data and uncover patterns.AI is not involved in the process, and all tasks are conducted manually using well-established statistical procedures.Notable studies in this domain include the application of regression analysis to identify research trends [10], correlation analysis to examine relationships between variables [11], and statistical estimation methods to explain observed scientific phenomena [12,13].</p>
<p>Level 1: AI-Assisted SoS Discovery In Level 1, AI only supports scientific data processing.Specifically, AI methods are able to transform real-world scientific data into a more comprehensible form, including tasks such as completing and structuring bibliometric data, extracting key features such as author networks and institutional collaborations, and converting text information (e.g., papers, scientists) into embedding representations, thereby enhancing the efficiency and accuracy of data handling.However, AI's role remains supplementary, with human researchers still conducting data analysis, understanding and prediction.From the perspective of AI4SoS, some related works include: utilizing text-to-embedding methods for mapping papers to vector space [14], extracting key information from papers using named entity recognition [15], and constructing networks for faculty mobility [16].</p>
<p>Level 2: Partially Automated SoS Discovery In Level 2, AI techniques (e.g., supervised learning), play a central role in analyzing scientific data, enabling tasks such as predicting emerging trends, research hotspots and collaboration opportunities, based on historical patterns.This marks a shift from AI-assisted data processing to AI-driven data analysis.However, in this level, AI struggles to design and implement experiments automatically.For instance, a simulation environment that can automatically conduct scientific experiments is not available, therefore it is difficult to model hidden dynamic</p>
<p>AI4SoS</p>
<p>processes within the scientific ecosystem.Related works include the use of machine learning models to predict individual paper citation counts [17], neural networks for forecasting research trends and generating novel ideas [18], clustering publications based on citation relationships [19], and applying structural topic models to extract topics from scientific texts [20].Level 3: Highly Automated SoS Discovery In Level 3, AI not only drives the analysis but also designs and implements experiments to simulate scientific patterns in the real world.In this case, researchers can compare results generated by simulation systems and those in the real world to explore strategies in SoS for potential real-world applications.While AI can support automatic experiment conduction, human supervision is required to define the specific application scenarios and corresponding experimental parameters (e.g., scientist information, boundary conditions) based on system feedback.Consequently, the authenticity and rationality of the system depends on whether the researchers have considered all relevant factors, making the automatic pattern validation difficult.Research at this level is still in its early stages, including systems simulating specific research scenarios to propose hypotheses [21], AI predicting outcomes under different simulation conditions to provide insights into collaboration patterns [22].and systems reproducing historical events based on specific environmental settings [23].Currently, most research remains at Level 2 or below, with only limited progress observed at Level 3, while fully automated SoS discovery is still in the exploratory stage.Looking ahead, several potential tasks are envisioned, including automated discovery of new collaboration patterns within the simulated scientific community [22], systems capable of simulating and conducting experiments in real-world settings [24], and AI that continuously refines research directions based on emerging data [25].</p>
<p>Advantages of Automatic SoS Discovery</p>
<p>In this section, we delve into critical open problems within the SoS that stand to benefit substantially from AI-driven automation.These problems are categorized into two primary areas: Forecasting Trends in Technology and Innovation and Understanding the Dynamics of Research Society.For each subproblem, we provide a brief background and outline key opportunities where AI offers advantages.</p>
<p>Forecasting Trends in Technology and Innovation</p>
<p>Background of Problem</p>
<p>Accurately forecasting the trajectory of science and technology is a crucial aspect of SoS, as it informs decisions related to funding, policy-making, and research prioritization.Two major challenges are predicting technological trends and identifying interdisciplinary opportunities.The Trend in Technological Development Technological development follows intricate and often non-linear trajectories, making prediction difficult.To predict these trends, it is essential to understand which technologies are gaining momentum, identify emerging breakthroughs, and anticipate when they will transition from research to real-world applications [26].Traditional methods, such as historical data analysis, often fall short in scalability and struggle to keep pace with rapid advancements.The Interdisciplinary Future of Innovation Interdisciplinary research, which often serves as the pivotal role for major breakthroughs, presents another significant challenge.With the rapid growth of scientific literature across diverse fields, manual identification of promising cross-disciplinary opportunities has become increasingly unfeasible [27].The complexity and scale of this task call for automated solutions capable of discovering novel connections across fields.</p>
<p>Advantages of AI4SoS</p>
<p>AI offers an opportunity for tackling challenges in the SoS by leveraging its capacity to process vast datasets and identify complex patterns beyond human discernment.In the context of forecasting technological development, AI models can analyze citation networks, research metadata, and publication trends to detect emerging technological trajectories with enhanced precision [28].</p>
<p>Moreover, AI-driven methods excel in uncovering interdisciplinary opportunities by representing scientific knowledge as graph structures and employing advanced similarity metrics.Graph neural networks, for instance, have demonstrated the ability to model intricate relationships across scientific literature, facilitating the discovery of latent connections and novel collaborations across disparate domains [29].This capability empowers researchers to target high-potential interdisciplinary collaborations, fostering innovation at the convergence of fields.</p>
<p>AI4SoS</p>
<p>Understanding the Dynamics of Research Society</p>
<p>Background of Problem</p>
<p>The dynamics of research societies play a fundamental role in shaping scientific progress, which encompass how scientist research patterns evolve, how different team constructions influence the impact of research output, and how current research society influences scientists.The Dynamics and Mechanics of Scientist Career The role of studying scientific careers is to provide personalized support to the academic community, thereby enhancing individual innovation capabilities, optimize team collaboration efficiency, and improving the allocation of research resources [1].However, challenges include the highly individualized nature of career development paths, data scarcity and bias, and the complexity of external environmental factors [30].The Dynamics and Mechanics of Research Team The composition and dynamics of scientific teams play a crucial role in improving research outcomes, with elements such as size, diversity, and collaboration patterns influencing team creativity and productivity [11,31].Over time, shifts in team structures and researcher mobility have reflected broader changes in the research landscape.Understanding these evolving dynamics presents challenges, as the relationships between team composition and research impact are multifaceted [32,33].The Dynamics and Mechanics of Research Society The organization and dynamics of research societies play a crucial role in shaping the progression and fairness of scientific endeavors.Studies have highlighted persistent inequalities in academic representation, participation, and recognition, both within and across nations [6,34].These disparities, influenced by systemic and structural factors, hinder the equitable generation and dissemination of knowledge.On a broader scale, imbalances in citation patterns and collaboration networks often reflect biases rooted in reputation and resources rather than research quality [35].</p>
<p>Advantages of AI4SoS</p>
<p>AI offers potential for understanding and improving the dynamics of research societies.By analyzing large-scale historical datasets-such as collaboration patterns, research trajectories, and external influences-AI can uncover critical factors driving individual career development.This enables personalized researcher support and helps institutions optimize talent management.Techniques such as predictive modeling have proven effective in tracking and forecasting team member mobility patterns [36].</p>
<p>Moreover, AI-driven agents can simulate complex team dynamics, providing insights into how various factors, such as diversity and team size, influence research productivity and innovation.Taking this a step further, AI can simulate entire scientific societies, not only uncovering hidden patterns and problems but also guiding the policymaking process by validating potential policies within the simulated environment.For instance, multi-agent systems have been employed to model team formation processes and predict collaboration outcomes under varying settings [22].</p>
<p>Challenges and Pathways</p>
<p>Achieving fully automated SoS discovery centers on effectively utilizing AI techniques to process scientific data.This endeavor involves addressing four key challenges: data-related issues, comprehensive system construction, robust system evaluation, and system explainability.For each of these challenges, we provide a detailed analysis along with potential pathways for resolution.</p>
<p>Data Issues</p>
<p>Challenges Data issues mainly include data imbalance across disciplines and training data bias.For the first issue, many disciplines, such as computer science and engineering, produce large volumes of well-structured data readily used by AI systems [37,38].However, other fields, such as social sciences or humanities, often suffer from smaller datasets, less structured data, or incomplete information, which makes it difficult for AI models to provide accurate predictions [39,40].This imbalance can lead to skewed results where AI predictions are disproportionately driven by well-represented fields, neglecting potentially valuable insights from underrepresented areas of research.Another issue is training data bias.When predicting reproducible patterns from data, machine learning models inevitably incorporate and perpetuate biases present in the data, often in opaque ways [41].For example, the training data and alignment methods of LLMs (whether open-source or closed-source) are not fully disclosed [42][43][44], making it impossible to objectively assess their bias and fairness.Therefore, the fairness of machine learning becomes a heavily debated issue in applications ranging from the criminal justice system to hiring processes [45].Pathway To address issues of data imbalance and biases in training data, constructing a large and diverse dataset is essential to improve data representativeness, ensuring coverage across various domains, groups, and contexts.Several large-scale, cross-disciplinary academic datasets are currently available for SoS research, including the Microsoft Academic Graph (MAG) [46], Open Academic Graph (OAG) [47], and SciSciNet [48], where the statistical information of each dataset is summarized in Table 2.In the process of data auditing and filtering, it is crucial to examine data sources and mitigate any potential historical or socio-cultural biases to ensure the dataset is free from implicit biases [49].Additionally, employing multi-annotator strategies, conducting group balance checks, and performing fairness evaluations can further ensure the fairness and diversity of the dataset [50].These measures not only enhance the model's generalization ability but also reduce unfairness stemming from data biases.</p>
<p>Comprehensive System Construction</p>
<p>Challenges Simulating a research society using AI for fully automated SoS discovery, particularly through an agent-based system, presents numerous challenges.Each scientist-agent requires detailed modeling of their research expertise, career trajectory, and collaborative networks, which are often too complex to be fully captured in the simulation system [51,52].Critical but unobservable factors, such as internal cognitive processes and informal discussions that drive real-world decision-making, remain challenging to replicate accurately.These limitations inevitably make simulations discrete and less representative of actual societal dynamics.Moreover, the simulation process itself introduces complexities.Aligning the simulated timeline with real-world events necessitates careful calibration; for instance, determining how many simulation epochs correspond to a year in reality [53].Determining the appropriate size of the simulated society is also crucial; an overly small-scale model risks failing to capture the emergent behaviors of a real research ecosystem, while an overly large model may become impractical to manage and analyze [54,55].Another pressing challenge lies in bias amplification when designing AI systems-a concern that builds on the broader implications of how AI interacts with societal structures.Since AI systems are often designed to optimize based on historical data of SoS, they risk perpetuating existing paradigms, funding trends, and citation networks.This aligns with the well-documented "rich get richer" effect in citation and funding dynamics [56][57][58].If an AI system prioritizes high-impact metrics, it may inadvertently favor mainstream topics and established researchers, further marginalizing unconventional or disruptive ideas.Without explicit mechanisms to value novelty and diversity, such systems could unintentionally confine the scientific community to existing trends, hindering pathways to groundbreaking innovation.Lastly, the system must account for unexpected exceptions to ensure the simulation operates smoothly and continuously for fully automated scientific discovery.Striking a balance between realism and feasibility remains a persistent and fundamental challenge in these simulations.</p>
<p>Pathway Several potential pathways can help address these complexities.</p>
<p>With the continuous advancement of LLMs' comprehensive capabilities, handling complex multi-level modeling is becoming increasingly feasible.By defining agent models with distinct roles and appropriately assigning tasks, the behaviors of scientists at various levels can be more accurately simulated [59].</p>
<p>Fine-tuning LLMs on extensive academic datasets can further optimize the behavioral patterns of agents [60], enhancing their adaptability to reflect realworld research dynamics.One solution for timeline alignment is to build flexible, dynamic calibration techniques that adjust the simulation's temporal parameters based on context and event-driven data [23].In determining the appropriate scale for the simulated society, agent-based sampling methods (random or rule-based) or dynamic population expansion techniques can be utilized [22].When addressing bias in AI systems, it is crucial to consider the nature of SoS, a discipline dedicated to analyzing historical data and uncovering biases or patterns within the scientific community.To ensure alignment between simulations and real-world dynamics, it is essential to incorporate these biases into SoS studies, as AI designed for this field seeks to enhance and advance SoS research.At the same time, such biases can be mitigated through targeted adjustments to system parameters.For instance, to counteract the "rich get richer" effect in citations, one effective approach could involve reducing the likelihood of citing highly cited papers when an agent selects a reference.Instead, assigning higher probabilities to less-cited, more novel papers can help promote diversity in citation practices and encourage the exploration of unconventional ideas.Moreover, the system can integrate robust anomaly detection and recovery mechanisms to handle unexpected situations.Using unsupervised learning techniques (such as clustering), the model can identify deviations from expected behaviors and adjust simulation parameters accordingly to ensure stability and continuity [61].These potential solutions try to strike a balance between realism and operational feasibility, providing a technological foundation for research society simulations.</p>
<p>AI4SoS</p>
<p>Comprehensive System Evaluation</p>
<p>Challenges Evaluating the validity of outputs generated by AI systems in the field of SoS is a complex and multifaceted challenge.SoS research addresses a broad range of problems and lacks unified evaluation standards, with different tasks often necessitating tailored metrics [41].Moreover, innovation-a key attribute of AI outputs-is inherently subjective and context-dependent, making it difficult to quantify accurately using traditional methods [22,62].Validity assessments also heavily rely on specific domain contexts.However, the interdisciplinary nature of SoS compounds the complexity, requiring the integration of knowledge and evaluation standards from diverse fields.Additionally, the dynamic nature and long-term implications of AI-generated outputs present further challenges, as their true impact on scientific progress often cannot be evaluated in the short term [63].Addressing this requires advanced tools, such as time-series analysis and virtual scientist simulations, to facilitate longitudinal tracking.Furthermore, AI-generated scientific recommendations may raise ethical issues and have far-reaching consequences for scientific communities and research practices [64].Therefore, a comprehensive and adaptable evaluation framework is necessary, integrating scientometric methodologies, multidisciplinary expert reviews, dynamic analytical approaches, and stringent ethical guidelines.</p>
<p>Pathway To address these challenges, appropriate solutions can be implemented.First, collaborating with domain experts to define task-specific evaluation metrics is essential, and then quantitative evaluation methods based on scientometrics should be developed.For instance, citation counts can be used as a measure of influence when evaluating the impact of system outputs, and they can also track knowledge flow [41].In simulating a scientist's career, individual impact metrics such as the h-index, which reflects both productivity and impact, can be applied.Additionally, to assess output novelty, feasible approaches include large model-based peer-review scoring [22,65] or calculating the Z-score for each pairing of referenced journals [62].With the ongoing expansion of LLMs' expertise and improved reasoning capabilities, interdisciplinary testing and long-term large-scale simulations have become increasingly feasible.Moreover, LLMs are now being employed in social simulations [23], assuming role-based agents.In terms of ethical and social impacts, aligning model preferences and improving transparency can partially address ethical concerns and enhance user trust, while ethical benchmarks [66,67] can be used to test the validity of system outputs.By integrating these strategies, a multidimensional evaluation framework can be established.</p>
<p>Explainability and Causal Inference</p>
<p>Challenges While the AI framework emphasizes automated discovery and evaluation, it lacks mechanisms to explain the causal pathways behind AIgenerated outputs [68,69].This limitation makes it difficult for researchers and policymakers to trust and adopt AI-driven insights, as they may not fully understand the underlying logic or relationships.Moreover, the complex and interdisciplinary nature of SoS often involves interactions between numerous variables, such as collaborations, funding patterns, and citation networks [1,70], which cannot be adequately captured through correlationbased approaches.Without explicit causal explanations, it is challenging to ensure the auditability, accountability, and interpretability of the system, undermining its credibility and ethical alignment.Pathway To address these challenges, it is crucial to introduce causal modeling [71,72] and explainable AI (XAI) [73,74] techniques to assist in interpreting and validating simulation results.Approaches such as Counterfactual Analysis can clarify the logical origins of AI-driven recommendations or discoveries, making the reasoning process more transparent.Relevant methods in the SoS domain include causal inference techniques like Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM), which are useful for identifying causal relationships in complex systems [75,76].Additionally, causal graphical models and structural equation modeling (SEM) can be applied to analyze scientific impact by modeling the flow of influence across variables such as collaboration networks or funding distributions [77][78][79].These tools provide a robust foundation for explaining AI-generated outputs.</p>
<p>Proof-of-Concept Studies</p>
<p>In this section, we present case studies to illustrate a practical application scenarios in AI4SoS.Specifically, by constructing a simplified preliminary multi-agent system to replicate phenomena observed in real-world scientific societies and uncover underlying patterns in SoS, we aim to demonstrate the possibility of automated pattern discovery.</p>
<p>Environment Construction</p>
<p>We construct a preliminary multi-agent system to simulate a society-level scientific collaboration through an end-to-end pipeline, including collaborator selection, topic discussion, idea generation, novelty assessment, abstract generation, and peer review, inspired by [22,65,80].Existing studies primarily focus on simulating individual scientists or small research teams within specific fields (e.g., computer science) and are often constrained to isolated settings that do not capture the broader research ecosystem.In contrast, our work enhances the system's complexity by incorporating realistic factors such as multidisciplinary data, a review and indexing system, and scalable simulation across multiple research teams.The overview of our system is shown in Fig. 3. Multidisciplinary Data We use the OAG 3.11 as the initial database for our system, which developed from the Open Academic Graph [47].This data set includes 35,774,510 authors and 130,710,733 papers as of 2023, spanning diverse domains such as physics, chemistry, and computer science.In Table 3, we present the disciplines and fields of paper in the Open Academic Graph,  The overview of our preliminary multi-agent system for scientific collaboration simulation.We place the simulation within a community of scientists.After a scientist leads his/her team in submitting a paper, it undergoes peer review.If accepted, it is added to the reference database and can be cited by other scientists in subsequent epochs.Due to varying author information, the citation count of the final research output differs, then we can analyze the correlation between them-understanding the dynamics of research organizations, which is important in the field of SoS.Review and Indexing System To better simulate and reveal the patterns of scientific collaboration mechanisms, we introduce a review and indexing system.Papers written by scientist teams are peer-reviewed and scored (ranging from 1 to 10), and those that exceed the acceptance threshold (with score larger than 5) are added to the reference paper database as newly published papers.The peer review criteria are discussed in Appx.B, considering that the outcomes are cross-disciplinary.Besides, the indexing system allows agents to retrieve published papers as references, and the citation count of referenced papers is updated accordingly, which is later used for metric evaluation.</p>
<p>AI4SoS</p>
<p>Table 5: Different strategies are adopted for various pieces of information regarding papers.The papers in the initial database have None for this information due to its absence, while the papers published by the agent contain the names of the cited papers</p>
<p>None</p>
<p>Discipline</p>
<p>Use GPT-4 to classify the papers into disciplines based on their keywords and titles.Refer to Table 3</p>
<p>for all the disciplines used Environmental Science</p>
<p>Scalable Simulation To better replicate the phenomenon of free collaboration in real scientific cooperation, we implement an adaptive concurrent distributed system based on the OASIS [23].The system's asynchronous mechanism achieves concurrent processing by queuing multiple requests from agents in an inference channel and then distributing them to different ports for sending and receiving, where each port has deployed an LLM responsible for chatting or embedding.Furthermore, to reduce CPU load, we set the channel allocation wait time based on the number of pending requests in the channel, thereby enabling long-term large-scale asynchronous simulation.This mechanism serves the two purposes: 1. Enabling scientist agents from different teams to communicate simultaneously, including both intra-team and cross-team collaboration, and 2. Accelerating the simulation process to enable large-scale simulations at the million-agent level.We test the time cost of our simulation system under different number of agents, illustrated in Fig. 4. It could be found that we realize a fast large-scale agent system, where a simulation of a million agent society takes only one week.</p>
<p>Experiments</p>
<p>Implementation Details We implement our system on 32 NVIDIA A100 GPUs, with 4 ports deployed on each GPU, and each port running the LLaMA3.1-8bmodel.We allow each agent to create up to 3 teams simultaneously, with team sizes following an exponential distribution.This is because we analyze the team sizes of papers published between 2002 and 2009 in the OAG (over 1,000,000 papers), as shown in Fig. 5.The red fitting line indicates that the team sizes in the real data follow an exponential distribution.Therefore, in our simulation, the team size of each agent is also modeled using an exponential distribution.</p>
<p>In idea generation and novelty assessment, each agent can cite up to 9 references per speech, where the retrieval results are obtained based on the similarity between the embeddings of the query terms and the embeddings of the papers in the database.The model used for embedding is mxbai-embedlarge.To avoid storage issues, each agent's memory retains a maximum of 5 entries.Each paper undergoes peer review by 3 reviewers.In terms of the  timeline, each epoch allows for 1 action, meaning a complete scientific collaboration can be completed in 6 epochs if the team progresses without any delays or interruptions.In our final experiment, the size of our society is maintained at 1 million agents, with a total of 40 epochs.</p>
<p>Involved Metrics Following the settings of [11,30,82], we measure the impact of scientific output by the number of citations a paper receives.In the simulation, the citation counts are updated each time a paper is retrieved during the idea generation phase.For validation, we analyze the citation counts of agent-generated papers to assess whether the system can replicate patterns observed in real-world data from the years 2010 to 2011.To evaluate AI's potential in pattern discovery, we examine the influence of three key factors on citation counts: ethnicity diversity, affiliation diversity, and average university ranking.Specifically, we measure diversity using Shannon entropy.For instance, the ethnicity diversity d eth of paper s is calculated as:
d eth = − k i=1 p i (s) ln p i (s),(1)
where k represents the total number of ethnicity categories, and p i (s) is the proportion of authors from the i-th ethnicity category in paper s.</p>
<p>Simulation Results</p>
<p>The experimental results presented in Fig. 6 compare real-world data in 2010 with the outcomes generated by our preliminary LLM-based multi-agent system.Both the real-world and simulated data show that higher citation counts are positively correlated with greater ethnicity diversity, which aligns with  existing findings in SoS literature [11], although the correlations are slightly weaker in the simulation.Additionally, the negative correlation between affiliation ranking and citation counts is also reproduced in the simulated data, suggesting that institutions with higher rankings may achieve higher citation counts per research output.A similar comparison using real-world data from 2011 and the simulated result is provided in Fig. 7.The statistical analysis of the 2011 data exhibits similar trends to those observed in Fig. 6, which presents the comparison using 2010 data.The positive correlation between citation counts and ethnicity AI4SoS diversity, as well as the negative correlation between affiliation ranking and citation counts, are consistently reflected in both years.However, minor variations in correlation strength are observed, highlighting the dynamic nature of scientific collaboration trends over time.</p>
<p>However, while both real-world and simulated data indicate a positive correlation between citation counts and affiliation diversity, the pattern observed in the simulation is not statistically significant, with a p-value greater than 0.05.These results suggest that the preliminary AI-driven simulations have the potential to replicate and uncover key patterns in scientific research, but there remains significant room for improvement.For instance, the current system lacks several critical components, such as comprehensive modeling of individual research trajectories and realistic funding and policy influences.These limitations contribute to the preliminary nature of our approach, as the absence of such factors restricts the system's ability to fully capture the complexity of real-world scientific ecosystems.Developing a more comprehensive and sophisticated simulation framework will enhance the system's capability to automatically model complex scientific dynamics with greater accuracy and reliability.</p>
<p>Alternative Views</p>
<p>The application of AI in SoS is often seen as transformative, promising to accelerate discovery.However, critics highlight significant limitations and risks, questioning its unqualified benefits.These concerns focus on systemic issues and unintended consequences [83][84][85].Key counterarguments include: (1) Reinforcement of Existing Inequalities: AI systems rely heavily on historical data, which often mirror long-standing inequities within the scientific community.For instance, datasets may disproportionately represent well-established disciplines, regions, or researchers, thereby perpetuating an imbalanced view of scientific contributions.Critics argue that this could stifle innovation by overlooking emerging fields and underrepresented groups, ultimately reinforcing the leading trend rather than fostering diversity.(2) Overreliance on Traditional Metrics: Academic evaluation metrics, such as citation counts and journal impact factors, are central to many AI applications in SoS.These metrics have been criticized for prioritizing mainstream research while marginalizing unconventional or nascent ideas.Opponents caution that AIdriven analyses might amplify this bias, narrowing the scope of scientific discovery and undervaluing novel contributions.</p>
<p>While these critiques highlight significant challenges, they underscore the importance of addressing fairness, and inclusivity in AI applications for SoS [86][87][88].To mitigate these concerns, the following strategies can be adopted: (1) Promoting Diversity in Data and Metrics: Expanding data curation efforts to include a wider range of disciplines, regions, and research communities is critical for minimizing biases.Additionally, developing diversified scientific impact metrics beyond citation counts can ensure a more equitable evaluation of research contributions.(2) Incorporating Bias Mitigation Techniques: Embedding bias detection and correction mechanisms in AI systems can help identify and address inequities in the data and algorithms.These techniques should be complemented by rigorous validation to ensure fairness and reliability.</p>
<p>Outlook</p>
<p>As AI4SoS progresses toward full autonomy, we envision a future where scientific discovery itself becomes a more self-reflective, adaptive, and strategically guided process.In this envisioned landscape, AI agents are trained on vast corpora of scholarly data and historical innovation patterns, which will not only map the contours of scientific fields but also anticipate emerging disciplines and recommend actionable research agendas.</p>
<p>Automated SoS systems will continuously monitor the evolving structure of scientific collaboration, offering dynamic guidance to policymakers, institutions, and individual researchers.Research teams may be formed or optimized based on predicted synergy and complementary expertise, while funding strategies could adapt in real time to maximize long-term innovation impact.Moreover, AI4SoS could democratize scientific foresight, making sophisticated analyses accessible to a broader range of stakeholders, from early-career researchers to global research organizations.The resulting ecosystem would be one where science is not only accelerated but also made more transparent, inclusive, and responsive to societal needs.</p>
<p>To enhance real-world applicability, we also envision deployment scenarios in which AI4SoS integrates directly with existing scientific ecosystems.For instance, it could serve as a sandbox environment for evaluating national research policies, allowing simulated assessments before implementation.Within academic institutions, AI4SoS could support internal research strategy formulation, identifying growth areas and optimizing resource allocation.Additionally, it could assist governmental and funding bodies in planning emerging discipline layouts and national innovation agendas.These integration pathways would significantly boost the practical value, societal impact, and credibility of AI4SoS.</p>
<p>Achieving this vision will demand sustained interdisciplinary collaboration, ethical oversight, and robust infrastructure, but the potential payoff is immense: a future in which the SoS is not just studied, but actively shaped by intelligent systems.</p>
<p>Conclusion</p>
<p>This paper presents a forward-looking perspective on the future of AI4SoS, proposing a five-level autonomy framework for understanding the progression toward automated SoS discovery.We emphasize the importance of AI4SoS by demonstrating its potential in two critical domains: forecasting trends in AI4SoS technology and innovation, and analyzing the evolution of research communities.Furthermore, we discuss key challenges and future directions, supporting our vision with literature reviews and proof-of-concept studies that showcase early applications.Ultimately, AI4SoS holds the promise of enabling automated SoS discovery, thereby enhancing scientific efficiency and promoting interdisciplinary innovation.</p>
<p>Impact Statement</p>
<p>We believe that sustained collaboration between AI researchers and SoS scholars is essential for advancing our understanding of complex scientific processes.This study leverages the complementary expertise of both fields to address key SoS challenges, improving scientific efficiency and fostering interdisciplinary innovation.</p>
<p>However, from an ethical perspective, the integration of AI with SoS research may present several concerns.First, accountability: When AI participates in scientific decision-making, it is crucial to clarify responsibility.For instance, if an AI-generated prediction leads to errors, should developers bear full responsibility?We suggest enhancing AI system transparency (e.g., recording decision-making pathways) and explainability (e.g., providing reasoning behind decisions) to help researchers and regulators delineate accountability more clearly.Second, fairness and bias: AI systems rely on training data, which may contain inherent biases related to gender, geography, or economic disparities.These biases can lead to unjust scientific conclusions.Therefore, AI development and application should include rigorous data preprocessing and incorporate fairness constraints within algorithms to mitigate the risk of bias propagation.Finally, public trust: AI-driven automation tools, due to their complexity, may create a sense of detachment among the public.When AI decision-making processes are opaque, concerns about the credibility of scientific findings may arise.To foster trust, it is essential to develop more interpretable AI models and ensure human oversight in scientific processes.</p>
<p>From a societal perspective, the complexity of SoS demands innovative approaches.Conventional statistical studies, which depend largely on historical data, frequently struggle to uncover causal mechanisms.In contrast, agentbased AI provides a dynamic, causality-driven alternative.By elucidating the mechanisms behind the evolution of scientific knowledge, these methods can clarify how government policies influence research funding, academic publishing, and interdisciplinary collaboration.As AI4SoS advances, it will foster more effective knowledge exchange among academia, industry, and government, accelerating technological and theoretical innovation.Through intelligent analysis and predictive modeling, researchers can more precisely identify scientific challenges, significantly enhancing the efficiency of discovery.</p>
<p>AI4SoS</p>
<p>A Related Work</p>
<p>A.1 AI for Science</p>
<p>In recent years, AI has become increasingly common in science and is expected to become the center of research practice [89].AI has demonstrated great potential to accelerate experimental design, data analysis, optimization problem solving, and discovery of new theories [90][91][92].Specifically, deep neural networks are used to predict the relationship between molecular structures and biological activity [93,94], reinforcement learning is used to discover unknown materials with superior properties [95,96], and agent-based systems are introduced to simulate social science scenarios [97,98].In addition, as a subfield of science, AI has undergone some preliminary explorations in the SoS [11,14,22], revealing promising results.</p>
<p>A.2 Large Language Models</p>
<p>The role of large language models (LLMs) can be articulated from two perspectives: chat (T5 [99], GPT-4 [100], and LLaMA3.1 [101]) and embedding (BERT [102] and DNABERT [103]) generation.First, the capability of dialogue generation enables LLMs to understand user input in natural language and generate contextually relevant responses in various conversational contexts such as knowledge testing, game play, and software programming [98,[104][105][106].Additionally, embedding generation allows LLMs to convert input text into fixed-dimensional vector representations, which effectively capture the semantic information of the text and can be used for tasks such as text similarity computation, information retrieval, and sentiment analysis [107][108][109][110]. Therefore, the capabilities of LLMs in both text generation and embedding generation make them applications spanning from natural language processing tasks to more complex domains such as SoS, where they can assist in understanding research dynamics, scientific discovery, and scientific collaboration.</p>
<p>B Review and Indexing System</p>
<p>In Table 6 and 7, we present the peer review criteria used in our simulation system, which is based on the modified Neural Information Processing Systems review guidelines2 considering that the papers produced by cross-discipline agents are not all in the field of computer science.Although this criteria comes from a computer science conference, the basic evaluation metrics can be applied in multiple areas.You are a researcher from a multidisciplinary background reviewing a paper that has been submitted to a venue that involves multiple scientific disciplines.Be critical and cautious in your decision-making.If the paper has significant weaknesses or you are uncertain about its quality, provide lower scores and recommend rejection.Below are the questions you will be asked on the review form for each paper and some guidelines on what to consider when answering these questions.Reviewer Guidelines for Multidisciplinary Paper Review: 1. Summary: Provide a brief summary of the paper and its contributions.This is not the place to critique the paper.The authors should generally agree with a well-written summary, which reflects an accurate understanding of their work from a multidisciplinary perspective.2. Strengths and Weaknesses: Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions:</p>
<p>-Originality: Are the tasks or methods novel within each of the relevant disciplines?Does the work represent an innovative combination of techniques or concepts from different fields?Is it clear how this work distinguishes itself from previous contributions in each discipline involved?-Quality: Is the submission technically sound in each of the relevant fields?Are claims well-supported by evidence (e.g., theoretical analysis or experimental results)?Are the methods used appropriately for each discipline involved?Is this a complete piece of work, or still a work in progress?Are the authors transparent and honest in evaluating both the strengths and weaknesses of their work?-Clarity: Is the paper written in a way that is accessible to readers from multiple disciplines?Is it well-organized, with clear explanations of concepts across different fields?If not, please suggest improvements for clarity.Does it provide sufficient detail for an expert in each relevant field to understand the methodology and reproduce results?-Significance: Are the results important?Are others (researchers or practitioners) likely to use the ideas or build on them?Does the submission address a difficult task in a better way than previous work?Does it advance the state of the art in a demonstrable way?Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?3. Questions: Please list any questions or suggestions that could help clarify the paper's limitations or improve its quality.Responses from the authors could change your opinion or address areas of confusion.This feedback can be critical for the rebuttal and discussion phase with the authors.</p>
<p>Figure 1 :
1
Figure 1: An illustration comparing human-driven and AI-driven research processes in the SoS, highlighting step-by-step differences across four key stages in order: data processing, data analysis, system simulation, and pattern validation.</p>
<p>Figure 3 :
3
Figure3: The overview of our preliminary multi-agent system for scientific collaboration simulation.We place the simulation within a community of scientists.After a scientist leads his/her team in submitting a paper, it undergoes peer review.If accepted, it is added to the reference database and can be cited by other scientists in subsequent epochs.Due to varying author information, the citation count of the final research output differs, then we can analyze the correlation between them-understanding the dynamics of research organizations, which is important in the field of SoS.</p>
<p>Figure 4 :
4
Figure 4: The time taken for a complete scientific collaboration with agents of different scales.A simulation of a million-agent society takes only one week.</p>
<p>AI4SoS</p>
<p>Figure 5 :
5
Figure 5: The statistics of team sizes for papers published between 2002 and 2009 in the OAG, with the red fitting line revealing that the distribution follows an exponential pattern.</p>
<p>Figure 6 :
6
Figure 6: Comparison of real-world (2010) and AI-simulated scientific research patterns.The scatter plots illustrate the relationships between Ethnicity Diversity, Affiliation Diversity, and Affiliation Ranking with Citation Count in both real-world (top row) and simulated (bottom row) data.Strong correlations observed in real data are partially reproduced by the AI-driven multi-agent system, demonstrating its potential to uncover meaningful patterns in scientific research and support automated SoS studies.</p>
<p>Figure 7 :
7
Figure 7: Comparison of real-world (2011) and AI-simulated scientific research patterns.</p>
<p>Table 1 :
1
Comparison between AI for Science and AI for Science of Science.
FeatureAI for ScienceAI for Science of ScienceFocusSolving domain-specificUnderstanding mechanismsscientific problems.of scientific progress.Approach Direct application of AI toMeta-level analysis toaddress scientific challenges.enhance the research process.Examples Predicting weather,Studying researchdesigning new drugs,collaboration trends,optimizing materials.analyzing innovation triggers,mapping knowledge growth.</p>
<p>An overview of the five progressively advancing levels of autonomy in AI4SoS, with more green areas indicating that higher levels correspond to greater degrees of autonomy.Current research is primarily at Level 2 or below, with very limited work at Level 3, while fully automated SoS discovery remains in the prospective stage.
Research ProcessPattern Validatione.g., Social experimentse.g., A simulated society for collaboration pattern discovery, …System Simulatione.g., Rule-based simulatione.g., Multi-agent systems for simulating specific research scenarios to propose hypotheses or reproduce historical events, …e.g., Correlatione.g., Machine learning models for predicting individual paper citationData Analysisanalysis for variablecounts, neural networks for forecasting research trends and generatingrelationshipsunexpected ideas, …Data Processinge.g., Manual data cleaninge.g., Text-to-Embedding methods for mapping papers to vector space, named entity recognition for extracting key information from papers, …Level 0:Level 1:Level 2:Level 3:Level 4:Autonomy LevelsNon-automatedAssistedPartially AutomatedHighly AutomatedFully AutomatedFigure 2:</p>
<p>Table 2 :
2
Summary table of large-scale cross-discipline academic datasets.
DatasetsMAGOAGSciSciNetDue202020232021DomainArt, Biology,Art, Biology,Art, Biology,Business,Business,Business,Chemistry,Chemistry,Chemistry,Computer Science,Computer Science,Computer Science,Economics,Economics,Economics,Engineering,Engineering,Engineering,EnvironmentalEnvironmentalEnvironmentalScience,Science,Science,Geography,Geography,Geography,Geology, History,Geology, History,Geology, History,Materials Science,Materials Science,Materials Science,Mathematics,Mathematics,Mathematics,Philosophy,Philosophy,Medicine,Physics, PoliticalPhysics, PoliticalPhilosophy,Science,Science,Physics, PoliticalPsychology,Psychology,Science,SociologySociologyPsychology,SociologyAuthor261,445,82535,774,510134,197,162Paper247,389,875130,710,733134,129,188Affiliation25,811143,74926,998</p>
<p>Goal: Analysis of Science of Science Goal: Analysis of Science of Science Scientist Society
Collaborator SelectionAuthor InformationIncludeName/Affiliation/Ethnicity/Citation…Topic DiscussionLarge Fail?Idea GenerationReference DatabaseIncludeTitle/Abstract/Year/ Author/Citation…Novelty Assessment1. Record new published paperAbstract GenerationPeer Review2. Update the citation of cited papersSmall Fail?Multi-Agent System</p>
<p>Table 3 :
3
[11]ary table of disciplines and fields[11].
FieldDisciplineHumanities, Literature &amp; Arts [Art, History, Philosophy, Psychology]Life Science &amp; Earth Sciences[Biology, Environmental Science, Geogra-phy, Geology]Business, Economics &amp;[Business, Economics]ManagementEngineering &amp; Computer[Computer Science, Engineering]ScienceChemical &amp; Material Sciences[Chemistry, Materials Science]Physics &amp; Mathematics[Mathematics, Physics]Health &amp; Medical Sciences[Medicine]Social Sciences[Political Science, Sociology]which is used to analyze the potential different patterns in various areas. Weuse papers from 2002 to 2009 as the reference database and papers from 2010 to2011 as the validation database. To address missing author ethnicity and paper
field information-key elements for validating SoS findings-we employ several data completion strategies.Specifically, we adopt corresponding approaches for the various pieces of author information and paper information in this dataset for our simulation, shown in Table4 and 5.</p>
<p>Table 4 :
4
Different strategies are adopted for various pieces of information regarding authors.
FieldStrategyExampleNameAuthor InformationNameUse the anonymization techniqueScientist 1EthnicityUse the name ethnicity classifier [81]BritishAffiliationRetain the original content[King's CollegeLondon]AffiliationUse THE World University Rankings36Ranking2025 1CitationExtract the author's published papers be-1800tween 2010 to 2020 and calculate the totalnumber of citations for the papers; In thesimulation, it will be updated if his/herpaper is citedCo-author Extract the author's published papers be-[Scientist 10,tween 2010 to 2020 and record the collab-Scientist 201,orators in the papers; In the simulation,Scientist 1002,it will be updated if there are new collab-. . . ]oratorsDisciplineExtract the author's published papers be-Psychologytween 2010 to 2020 and assign the au-thor's discipline as the one that appearsmost frequentlyResearchExtract the author's published papers[Neuropsychology,topicbetween 2010 to 2020 and record theCognitivekeywords in the papers; Use GPT-4 toflexibility, Atten-summarize these keywords into researchtional bias, . . . ]topics</p>
<p>Table 6 :
6
Prompt Tailored for Multidisciplinary Reviewers Prompt Tailored for Multidisciplinary Reviewers (1/2)</p>
<p>https://open.aminer.cn/open/article?id=65bf053091c938e5025a31e2
https://www.timeshighereducation.com/world-university-rankings/latest /world-ranking
https://neurips.cc/Conferences/2024/ReviewerGuidelines
AcknowledgementsThis work is supported by Shanghai Artificial Intelligence Laboratory.AI4SoSAI4SoSTable7: Prompt Tailored for Multidisciplinary Reviewers Prompt Tailored for Multidisciplinary Reviewers (2/2) 4. Ethical Concerns: Flag any ethical concerns, particularly those that may arise from interdisciplinary collaboration.Ensure any ethical issues related to research design, data usage, or broader implications are addressed.5. Overall Score: Provide a final score based on the paper's strengths and weaknesses.Use the following scale:-10: Award Quality: A technically flawless paper with groundbreaking impact across one or more disciplines, with exceptionally strong evaluation, reproducibility, and resources, and no unaddressed ethical concerns.-9: Very Strong Accept: A technically flawless paper with groundbreaking impact in at least one area and strong impact on multiple areas, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical concerns.-8: Strong Accept: A technically strong paper with novel ideas, significant impact on at least one discipline or moderate-to-high impact on multiple areas, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical concerns.-7: Accept: A technically solid paper with moderate-to-high impact in one or more subfields, good-to-excellent evaluation, reproducibility, and resources, and no unaddressed ethical concerns.-6: Weak Accept: A solid paper with moderate impact, no major concerns in terms of evaluation, reproducibility, and ethical considerations.-5: Borderline Accept: A technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation.Use sparingly.-4: Borderline Reject: A technically solid paper where reasons to reject outweigh reasons to accept, e.g., limited evaluation.Use sparingly.-3: Reject: A paper with technical flaws, weak evaluation, inadequate reproducibility, or incompletely addressed ethical concerns.-2: Strong Reject: A paper with major technical flaws, poor evaluation, limited impact, poor reproducibility, or mostly unaddressed ethical considerations.-1: Very Strong Reject: A paper with trivial results, poor evaluation, or unaddressed ethical issues.
. S Fortunato, C T Bergstrom, K Börner, J A Evans, D Helbing, S Milojević, A M Petersen, F Radicchi, R Sinatra, B Uzzi, Science of science. 35963791852018Science</p>
<p>Scientific discovery and topological transitions in collaboration networks. L M Bettencourt, D I Kaiser, J Kaur, Journal of Informetrics. 332009</p>
<p>Weaving the fabric of science: Dynamic network models of science's unfolding structure. F Shi, J G Foster, J A Evans, Social Networks. 432015</p>
<p>Which type of citation analysis generates the most accurate taxonomy of scientific and technical knowledge. R Klavans, K W Boyack, Journal of the Association for Information Science and Technology. 6842017</p>
<p>The science of science. D Wang, L Liu, Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020. the ACM/IEEE Joint Conference on Digital Libraries in 20202020</p>
<p>Quantifying hierarchy and dynamics in us faculty hiring and retention. K H Wapman, S Zhang, A Clauset, D B Larremore, Nature. 61079302022</p>
<p>Drugclip: Contrasive protein-molecule representation learning for virtual screening. B Gao, B Qiang, H Tan, Y Jia, M Ren, M Lu, J Liu, W.-Y Ma, Y Lan, Advances in Neural Information Processing Systems. 362024</p>
<p>Accurate structure prediction of biomolecular interactions with alphafold 3. J Abramson, J Adler, J Dunger, R Evans, T Green, A Pritzel, O Ronneberger, L Willmore, A J Ballard, J Bambrick, Nature. 2024</p>
<p>Bidirectional generation of structure and properties through a single molecular foundation model. J Chang, J C Ye, Nature Communications. 15123232024</p>
<p>Choosing experiments to accelerate collective discovery. A Rzhetsky, J G Foster, I T Foster, J A Evans, Proceedings of the National Academy of Sciences. 112472015</p>
<p>The preeminence of ethnic diversity in scientific collaboration. B K Alshebli, T Rahwan, W L Woon, Nature communications. 915163</p>
<p>. Ai4sos, 2018</p>
<p>Hot streaks in artistic, cultural, and scientific careers. L Liu, Y Wang, R Sinatra, C L Giles, C Song, D Wang, Nature. 55977142018</p>
<p>Quantifying the dynamics of failure across science, startups and security. Y Yin, Y Wang, J A Evans, D Wang, Nature. 57577812019</p>
<p>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. F Shi, J Evans, Nature Communications. 1416412023</p>
<p>Named entity recognition and normalization applied to large-scale information extraction from the materials science literature. L Weston, V Tshitoyan, J Dagdelen, O Kononova, A Trewartha, K A Persson, G Ceder, A Jain, Journal of chemical information and modeling. 5992019</p>
<p>Systematic inequality and hierarchy in faculty hiring networks. A Clauset, S Arbesman, D B Larremore, Science advances. 1114000052015</p>
<p>On modeling and predicting individual paper citation count over time. S Xiao, J Yan, C Li, B Jin, X Wang, X Yang, S M Chu, H Zha, Ijcai. 2016</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. M Krenn, A Zeilinger, Proceedings of the National Academy of Sciences. 11742020</p>
<p>Citation-based clustering of publications using citnetexplorer and vosviewer. N J Van Eck, L Waltman, Scientometrics. 1112017</p>
<p>The diversity-innovation paradox in science. B Hofstra, V V Kulkarni, Munoz-Najar, S Galvez, B He, D Jurafsky, D A Mcfarland, Proceedings of the National Academy of Sciences. 117172020</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, arXiv:2409.055562024arXiv preprint</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. H Su, R Chen, S Tang, X Zheng, J Li, Z Yin, W Ouyang, N Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Z Yang, Z Zhang, Z Zheng, Y Jiang, Z Gan, Z Wang, Z Ling, J Chen, M Ma, B Dong, arXiv:2411.11581Oasis: Open agents social interaction simulations on one million agents. 2024arXiv preprint</p>
<p>Ai energized hydrogel design, optimization and application in biomedicine. Z Li, P Song, G Li, Y Han, X Ren, L Bai, J Su, Materials Today Bio. 1010142024</p>
<p>Artificial intelligence research: A review on dominant themes, methods, frameworks and future research directions. K Ofosu-Ampong, Telematics and Informatics Reports. 1001272024</p>
<p>Network dynamics of innovation processes. I Iacopini, S Milojević, V Latora, Physical review letters. 1204483012018</p>
<p>Educating the future generation of researchers: A cross-disciplinary survey of trends in analysis methods. T Bolt, J S Nomi, D Bzdok, L Q Uddin, PLoS biology. 19730013132021</p>
<p>Forecasting innovations in science, technology, and education. K Börner, W B Rouse, P Trunfio, H E Stanley, Proceedings of the National Academy of Sciences. 115502018</p>
<p>J Zhou, G Cui, S Hu, Z Zhang, C Yang, Z Liu, L Wang, C Li, M Sun, Graph neural networks: A review of methods and applications. AI open 1. 2020</p>
<p>Early-career setback and future career impact. Y Wang, B F Jones, D Wang, Nature communications. 10143312019</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 5662019</p>
<p>The increasing dominance of teams in production of knowledge. S Wuchty, B F Jones, B Uzzi, Science. 31658272007</p>
<p>Genderdiverse teams produce more novel and higher-impact scientific ideas. Y Yang, T Y Tian, T K Woodruff, B F Jones, B Uzzi, Proceedings of the National Academy of Sciences. 1193622008411192022</p>
<p>Non-white scientists appear on fewer AI4SoS editorial boards, spend more time under review, and receive fewer citations. F Liu, T Rahwan, B Alshebli, Proceedings of the National Academy of Sciences. 1201322153241202023</p>
<p>Leading countries in global science increasingly receive more citations than other countries doing similar research. C J Gomez, A C Herman, P Parigi, Nature Human Behaviour. 672022</p>
<p>Team assembly mechanisms determine collaboration network structure and team performance. R Guimera, B Uzzi, J Spiro, L A N Amaral, Science. 30857222005</p>
<p>An insight into imbalanced big data classification: outcomes and challenges. A Fernández, S Del Río, N V Chawla, F Herrera, Complex &amp; Intelligent Systems. 32017</p>
<p>A systematic review on imbalanced data challenges in machine learning: Applications and solutions. H Kaur, H S Pannu, A K Malhi, ACM computing surveys (CSUR). 5242019</p>
<p>A survey on addressing high-class imbalance in big data. J L Leevy, T M Khoshgoftaar, R A Bauder, N Seliya, Journal of Big Data. 512018</p>
<p>Survey on deep learning with class imbalance. J M Johnson, T M Khoshgoftaar, Journal of big data. 612019</p>
<p>Data, measurement and empirical methods in the science of science. L Liu, B F Jones, B Uzzi, D Wang, Nature human behaviour. 772023</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024arXiv preprint</p>
<p>A survey on bias and fairness in machine learning. N Mehrabi, F Morstatter, N Saxena, K Lerman, A Galstyan, ACM computing surveys (CSUR). 5462021</p>
<p>An overview of microsoft academic service (mas) and applications. A Sinha, Z Shen, Y Song, H Ma, D Eide, B.-J Hsu, K Wang, Proceedings of the 24th International Conference on World Wide Web. the 24th International Conference on World Wide Web2015</p>
<p>Oag: Linking entities across largescale heterogeneous knowledge graphs. F Zhang, X Liu, J Tang, Y Dong, P Yao, J Zhang, X Gu, Y Wang, E Kharlamov, B Shao, IEEE Transactions on Knowledge and Data Engineering. 3592022</p>
<p>Sciscinet: A large-scale open data lake for the science of science research. Z Lin, Y Yin, L Liu, D Wang, Scientific Data. 1013152023</p>
<p>Tackling the issue of bias in artificial intelligence to design ai-driven fair and inclusive service systems. how human biases are breaching into ai algorithms, with severe impacts on individuals and societies, and what designers can do to face this phenomenon and change for the better. V Scatiggio, 2020</p>
<p>V Prabhakaran, A M Davani, M Diaz, arXiv:2110.05699On releasing annotator-level labels and information in datasets. 2021arXiv preprint</p>
<p>Informal approaches to developing simulation models. E Norling, B Edmonds, R Meyer, Simulating Social Complexity: A Handbook. 2017</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. C Gao, X Lan, N Li, Y Yuan, J Ding, Z Zhou, F Xu, Y Li, Humanities and Social Sciences Communications. 1112024</p>
<p>Calibrating real-world city traffic simulation model using vehicle speed data. S Khaleghian, H Neema, M Sartipi, T Tran, R Sen, A Dubey, 2023 IEEE International Conference on Smart Computing (SMARTCOMP). IEEE2023</p>
<p>Agent-based modelling of social-ecological systems: achievements, challenges, and a way forward. J Schulze, B Müller, J Groeneveld, V Grimm, Journal of Artificial Societies and Social Simulation. 2022017</p>
<p>Challenges, tasks, and opportunities in modeling agent-based complex systems. L An, V Grimm, A Sullivan, B Turner Ii, N Malleson, A Heppenstall, C Vincenot, D Robinson, X Ye, J Liu, Ecological Modelling. 457I4S2021</p>
<p>How to receive more funding for your research? get connected to the right people. A Ebadi, A Schiffauerova, PloS one. 1071330612015</p>
<p>The evolutions of the rich get richer and the fit get richer phenomena in scholarly networks: The case of the strategic management journal. G A Ronda-Pupo, T Pham, Scientometrics. 11612018</p>
<p>Metrics of inequality: The concentration of resources in the us biomedical elite. Y Katz, U Matter, Science as Culture. 2942020</p>
<p>Chatdev: Communicative agents for software development. C Qian, W Liu, H Liu, N Chen, Y Dang, J Li, C Yang, W Chen, Y Su, X Cong, J Xu, D Li, Z Liu, M Sun, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>Large language model based multi-agents: A survey of progress and challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>C Aldrich, L Auret, Unsupervised Process Monitoring and Fault Diagnosis with Machine Learning Methods. Springer201316</p>
<p>The philosopher's stone for science-the catalyst change of ai for scientific creativity. Pin and Wang, Dashun, The Philosopher's Stone for Science-The Catalyst Change of AI for Scientific Creativity. Q Chen, Y.-J I Ho, P Sun, D Wang, March 5, 2024. 2024</p>
<p>The road ahead: Emerging trends, unresolved issues, and concluding remarks in generative ai-a comprehensive review. S Balasubramaniam, V Chirchi, S Kadry, M Agoramoorthy, S P Gururama, K K Satheesh, T Sivakumar, International Journal of Intelligent Systems. 20242024</p>
<p>Navigating the future of large language models in scientific research: Opportunities, challenges, and ethical considerations. Challenges, and Ethical Considerations. M Lissack, B Meagher, September 02, 2024. 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Localvaluebench: A collaboratively built and extensible benchmark for evaluating localized value alignment and ethical safety in large language models. G I Meadows, N W L Lau, E A Susanto, C L Yu, A Paul, arXiv:2408.014602024arXiv preprint</p>
<p>J Ji, Y Chen, M Jin, W Xu, W Hua, Y Zhang, arXiv:2406.04428Moralbench: Moral evaluation of llms. 2024arXiv preprint</p>
<p>Interpreting black-box models: a review on explainable artificial intelligence. V Hassija, V Chamola, A Mahapatra, A Singal, D Goel, K Huang, S Scardapane, I Spinelli, M Mahmud, A Hussain, Cognitive Computation. 1612024</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. C K Reddy, P Shojaee, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Scientific collaboration. D H Sonnenwald, Annu. Rev. Inf. Sci. Technol. 4112007</p>
<p>Causal models and learning from data: integrating causal modeling and statistical estimation. M L Petersen, M J Van Der Laan, Epidemiology. 2532014</p>
<p>Causal machine learning for predicting treatment outcomes. S Feuerriegel, D Frauen, V Melnychuk, J Schweisthal, K Hess, A Curth, S Bauer, N Kilbertus, I S Kohane, M Van Der Schaar, Nature Medicine. 3042024</p>
<p>Explainable ai (xai): Core ideas, techniques, and solutions. R Dwivedi, D Dave, H Naik, S Singhal, R Omer, P Patel, B Qian, Z Wen, T Shah, G Morgan, ACM Computing Surveys. 5592023</p>
<p>Explainable artificial intelligence (xai) 2.0: A manifesto of open challenges and interdisciplinary research directions. L Longo, M Brcic, F Cabitza, J Choi, R Confalonieri, J Del Ser, R Guidotti, Y Hayashi, F Herrera, A Holzinger, Information Fusion. 1061023012024</p>
<p>Comparative effectiveness of matching methods for causal inference. Unpublished manuscript. G King, R Nielsen, C Coberley, J E Pope, A Wells, 2011Cambridge, MAInstitute for Quantitative Social Science, Harvard University</p>
<p>G W Imbens, D B Rubin, Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge university press2015I4S</p>
<p>Applications of structural equation modeling in social sciences research. J De Carvalho, F O Chima, American International Journal of Contemporary Research. 412014</p>
<p>Methodological research on partial least squares structural equation modeling (pls-sem) an analysis based on social network approaches. G F Khan, M Sarstedt, W.-L Shiau, J F Hair, C M Ringle, M P Fritze, Internet Research. 2932019</p>
<p>Mapping of machine learning approaches for description, prediction, and causal inference in the social and health sciences. A K Leist, M Klee, J H Kim, D H Rehkopf, S P Bordas, G Muniz-Terrera, S Wade, Science Advances. 84219422022</p>
<p>B Qi, K Zhang, K Tian, H Li, Z.-R Chen, S Zeng, E Hua, H Jinfang, B Zhou, arXiv:2407.08940Large language models as biomedical hypothesis generators: A comprehensive evaluation. 2024arXiv preprint</p>
<p>Nameethnicity classification from open sources. A Ambekar, C Ward, J Mohammed, S Male, S Skiena, Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2009</p>
<p>Early coauthorship with top scientists predicts success in academic careers. W Li, T Aste, F Caccioli, G Livan, Nature communications. 10151702019</p>
<p>Fairness in machine learning: Lessons from political philosophy. R Binns, Conference on Fairness, Accountability and Transparency. PMLR2018</p>
<p>Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. I D Raji, J Buolamwini, Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. the 2019 AAAI/ACM Conference on AI, Ethics, and Society2019</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M Crockett, Nature. 6272024</p>
<p>Improving fairness in machine learning systems: What do industry practitioners need?. K Holstein, J Wortman Vaughan, Iii Daumé, H Dudik, M Wallach, H , Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. the 2019 CHI Conference on Human Factors in Computing Systems2019</p>
<p>A proposal for identifying and managing bias in artificial intelligence. R Schwartz, L Down, A Jonas, E Tabassi, Draft NIST Special Publication. 12702021</p>
<p>R Schwartz, R Schwartz, A Vassilev, K Greene, L Perine, A Burt, P Hall, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. 20223US Department of Commerce, National Institute of Standards and Technology</p>
<p>Ai and science: what 1,600 researchers think. R Van Noorden, J M Perkel, Nature. 62179802023</p>
<p>Artificial intelligence within the interplay between natural and artificial computation: Advances in data science, trends and applications. J M Górriz, J Ramírez, A Ortiz, F J Martinez-Murcia, F Segovia, J Suckling, M Leming, Y.-D Zhang, J R Álvarez-Sánchez, G Bologna, Neurocomputing. 4102020</p>
<p>Innovation and design in the age of artificial intelligence. R Verganti, L Vendraminelli, M Iansiti, Journal of product innovation management. 3732020</p>
<p>Ai-powered innovations in high-tech research and development: From theory to practice. M Madanchian, H Taherdoost, Computers, Materials &amp; Continua. 8122024</p>
<p>Atomnet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery. I Wallach, M Dzamba, A Heifets, arXiv:1510.028552015arXiv preprint</p>
<p>Machine learning in drug design: Use of artificial intelligence to explore the chemical structure-biological activity relationship. M Staszak, K Staszak, K Wieszczycka, A Bajek, K Roszkowski, B Tylkowski, Wiley Interdisciplinary Reviews: Computational Molecular Science. 12215682022</p>
<p>Evolving the materials genome: How machine learning is fueling the next generation of materials discovery. C Suh, C Fare, J A Warren, E O Pyzer-Knapp, Annual Review of Materials Research. 5012020</p>
<p>Materials discovery with extreme properties via reinforcement learning-guided combinatorial chemistry. H Kim, H Choi, D Kang, W B Lee, J Na, Chemical Science. 2024</p>
<p>Avalonbench: Evaluating llms playing the game of avalon. J Light, M Cai, S Shen, Z Hu, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023I4S</p>
<p>Z Du, C Qian, W Liu, Z Xie, Y Wang, Y Dang, W Chen, C Yang, arXiv:2406.08979Multi-agent software development through cross-team collaboration. 2024arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>OpenAI: GPT-4 technical report. CoRR2023</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D Kenton, M.-W C Toutanova, L K , Proceedings of naacL-HLT. naacL-HLTMinneapolis, Minnesota201912</p>
<p>Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Y Ji, Z Zhou, H Liu, R V Davuluri, Bioinformatics. 37152021</p>
<p>D Zhang, W Liu, Q Tan, J Chen, H Yan, Y Yan, J Li, W Huang, X Yue, D Zhou, arXiv:2402.06852Chemllm: A chemical large language model. 2024arXiv preprint</p>
<p>Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset. J Liu, P Zhou, Y Hua, D Chong, Z Tian, A Liu, H Wang, C You, Z Guo, L Zhu, Advances in Neural Information Processing Systems. 362024</p>
<p>Exploring collaboration mechanisms for LLM agents: A social psychology view. J Zhang, X Xu, N Zhang, R Liu, B Hooi, S Deng, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024</p>
<p>R Nogueira, K Cho, arXiv:1901.04085Passage re-ranking with bert. 2019arXiv preprint</p>
<p>From word embeddings to pre-trained language models: A state-of-the-art walkthrough. M Mars, Applied Sciences. 121788052022</p>
<p>Improving sentiment analysis for social media applications using an ensemble deep learning language model. A Alsayat, Arabian Journal for Science and Engineering. 4722022</p>
<p>A Petukhova, J P Matos-Carvalho, N Fachada, arXiv:2403.15112Text clustering with llm embeddings. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3693 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3693</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3693</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-259108337</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.04926v1.pdf" target="_blank">covLLM: Large Language Models for COVID-19 Biomedical Literature</a></p>
                <p><strong>Paper Abstract:</strong> The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs. covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets. These models were evaluated by two human evaluators and ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3693.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3693.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>covLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>covLLM: Large Language Models for COVID-19 Biomedical Literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A COVID-19–focused instruction-following LLM built by fine-tuning LLaMA-7B on a combination of synthetic instruction-abstract pairs (synCovid), mined real abstract-title pairs, and/or the Alpaca instruction dataset to enable query-driven summarization and information extraction from COVID-19 literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>covLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Fine-tuned LLaMA-7B instruction-following model designed to take an input research article (abstract) and a natural-language query/instruction and return a concise, query-specific answer; fine-tuning used synthetic instruction-input-output triplets (synCovid), real abstract->title pairs, and/or Alpaca 52K tasks via the Alpaca-LoRA fine-tuning framework.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Training and data generation used the BREATHE biomedical literature dataset, specifically sampling CORD-19 articles; three training regimens: (1) Alpaca 52K + synCovid (53,097 instructions), (2) synCovid only (1,097 synthetic instruction-input-output triplets), and (3) synCovid + real abstract-title pairs (2,194 instructions). Evaluation used 26 instruction-input test prompts sampled from COVID-19 literature.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language instruction prompts paired with an input (typically a ~250–300 word abstract); example instructions include 'Summarize this abstract', 'Extract key findings', 'Identify study type', and other user-specified queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Instruction-tuning via self-instruction synthetic data generation and fine-tuning of a pre-trained LLaMA-7B using Alpaca-LoRA (LoRA adaptation). Synthetic data (synCovid) was generated using OpenAI's text-davinci-003 following the Self-Instruct / Alpaca self-instruction pipeline; some seed outputs were generated with gpt-turbo-3.5 and manually audited.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Short, human-readable answers: summaries, extracted facts, study-type labels, key findings, or titles (for abstract->title training). Outputs were generated as a few-sentence responses constrained by decoding parameters (temperature 0.1, top-p 0.75, top-k 40, beams 4, max tokens 128).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Blind pairwise/quadruple comparison of model responses across 26 prompts where two human evaluators and GPT-3.5 (used as an automated evaluator) ranked responses by helpfulness, relevance, accuracy, and level of detail; responses were graded as Fail / Pass / Excellent following LIMA-style criteria and models were ranked from 1–4.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The synCovid+abstracts variant performed best: in aggregated evaluator preferences (humans + GPT-3.5) it was preferred or tied with ChatGPT on ~65% of prompts. Adding Alpaca 52K to synCovid did not yield substantial improvement; synCovid-only and synCovid+abstracts achieved competitive performance with ChatGPT on the evaluated prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training and inference hardware requirements (powerful machine/GPU) limit practical deployment; synthetic training data contained hallucinated or partially incomplete abstracts but still provided useful style/syntax; outputs can be less reliable on broad or philosophical questions, and hallucination risk remains; small curated training sets raise overfitting concerns (though training curves showed no overfit in reported runs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared directly against ChatGPT in blinded human/GPT-3.5 evaluations: synCovid+abstracts matched or tied ChatGPT on 65% of prompts; Alpaca+synCovid did not significantly outperform other variants; overall covLLM variants were competitive with ChatGPT in the limited evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3693.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>synCovid</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>synCovid synthetic COVID-19 instruction dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic instruction-input-output dataset of 1,097 instruction-abstract-answer triplets generated for fine-tuning LLMs on biomedical literature tasks using the Alpaca self-instruction pipeline and OpenAI models, manually audited for quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>synCovid (synthetic data generation via Self-Instruct / Alpaca pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Synthetic instruction dataset created by pairing 18 handwritten seed instructions with 175 CORD-19 abstracts to generate outputs using gpt-turbo-3.5, then using a directed prompt and text-davinci-003 to expand into 1,097 instruction-input-output triplets guided toward biomedical research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Seed inputs sampled from CORD-19 (a COVID-19 subset of the BREATHE biomedical literature dataset); resulting synCovid contains 1,097 triplets composed of ~250–300 word abstract-style inputs (1,035 unique instructions and 865 unique inputs according to the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Instruction templates and diverse, domain-focused prompts (example tasks: summarization, key finding extraction, identifying study design, extracting biological pathways) supplied as natural-language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Self-instruction / directed prompt generation: seed seed-handwritten tasks + CORD-19 abstracts -> gpt-turbo-3.5 and text-davinci-003 used to synthesize outputs; outputs manually audited and edited for comprehensibility and concision; used to instruction-tune LLaMA via LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Instruction-following example pairs: each entry is (instruction, input-abstract, output-answer) where outputs are concise natural-language summaries, extractions, or labels appropriate to the instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Manual quality checks: sampled 120 synCovid examples for completeness (background, methods, results, conclusions) and study-design diversity; manual auditing and editing of generated outputs prior to inclusion in training.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>synCovid provided diverse, biomedical-styled instruction examples that improved model performance when combined with real abstract->title pairs; inclusion of synCovid enabled strong performance with relatively small dataset sizes (1,097 examples) when properly fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some synthetic inputs were incomplete or contained hallucinated content; not all generated abstracts had full background/conclusion sections; synthetic nature may introduce artifacts but authors retained partial/incomplete examples for prompt diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>When used for fine-tuning, synCovid combined with real abstracts outperformed a model primarily trained with the larger Alpaca 52K dataset in the authors' evaluations; direct comparison of synCovid-only versus human-generated instruction data not fully explored beyond reported evaluations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3693.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (7-billion-parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation language model from Meta (LLaMA family) used as the base model for covLLM fine-tuning; chosen for its efficiency and suitability for research fine-tuning under a non-commercial license.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaMA: Open and Efficient Foundation Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLaMA-7B (foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Pretrained transformer foundation model used as the base checkpoint for instruction-tuning via LoRA; provides the base language capability that is specialized by fine-tuning on synthetic and real biomedical instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>The pretraining corpus for LLaMA itself is not detailed in this paper; within this study LLaMA-7B was fine-tuned on synCovid, real abstract-title pairs, and/or Alpaca instruction sets as described above.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Receives instruction-input pairs during fine-tuning and natural-language prompts at inference; standard instruction-following interface.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Fine-tuning via LoRA (Alpaca-LoRA framework) using instruction-tuning datasets; no novel distillation algorithm beyond instruction tuning is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language responses to instructions (summaries, extracted facts, labels).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluated as part of covLLM variants against ChatGPT and across human/GPT-3.5 evaluators as described for covLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLaMA-7B served as an effective, efficient base model that—when instruction-tuned with domain-focused synthetic and mined data—yielded a model competitive with ChatGPT on the paper's 26-prompt evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLaMA is under a non-commercial license; fine-tuning and inference require GPU resources; performance depends on quality and domain-specificity of fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>When fine-tuned appropriately, LLaMA-7B-based covLLM variants matched or approached ChatGPT performance on the narrow evaluation tasks used in this study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3693.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stanford Alpaca (instruction-following LLaMA fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-following model derived by fine-tuning LLaMA on a 52K self-instruction dataset (Alpaca) that emulates OpenAI instruction-following behavior; used both as a baseline dataset and as part of one covLLM training regimen.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stanford Alpaca: An Instruction-following LLaMA Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Alpaca instruction dataset / model</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Provides 52K instruction-response pairs generated via self-instruction methods and used commonly for instruction-tuning; in this paper one covLLM variant was trained on Alpaca 52K + synCovid to test the effect of mixing large generic instruction data with domain-specific synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Alpaca dataset: ~52,000 synthetic instruction-response pairs (general-domain), details per original Alpaca release; in this paper combined with synCovid for a 53,097-instruction training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language instructions in diverse general-purpose tasks; used here as additional instruction-tuning examples rather than domain-specific prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Alpaca's self-instruction generation approach (external to this paper) provided instruction-response pairs; used during LoRA fine-tuning of LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Instruction-following outputs (natural-language responses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Included as part of covLLM training variants and evaluated in the same human/GPT-3.5 comparison protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Including the large Alpaca 52K dataset in addition to synCovid did not provide significant performance gains compared to synCovid+abstracts in the authors' evaluations; suggests domain-specific curated/synthetic data can be more effective than large generic instruction sets for domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generic instruction data may dilute domain-specific signals; mixing large general instruction datasets with small domain-specific sets requires careful balancing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>The Alpaca+synCovid model performed worse than the synCovid+abstracts variant and did not significantly outperform ChatGPT in the reported comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3693.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-LoRA / LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca-LoRA (Low-Rank Adaptation fine-tuning framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient fine-tuning approach (LoRA) and an Alpaca-LoRA training framework used to fine-tune LLaMA-7B quickly on the synCovid / abstract datasets using limited GPU resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Low-Rank Adaptation of Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Alpaca-LoRA / LoRA fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>LoRA (Low-Rank Adaptation) injects low-rank updates into transformer weights enabling efficient fine-tuning; the Alpaca-LoRA framework implements LoRA-based instruction-tuning workflows used here to fine-tune LLaMA-7B variants on the synthetic and mined datasets in a few hours on a single A100 GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Fine-tuning corpora described above (Alpaca 52K, synCovid 1,097, real abstract-title pairs 1,097); LoRA enables using these datasets efficiently without full-weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Standard instruction-pairs used for supervised fine-tuning; queries are natural-language instructions paired with inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Parameter-efficient supervised fine-tuning (LoRA) combined with instruction-tuning; no additional distillation (e.g., teacher-student) reported beyond this fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Instruction-following model capable of producing short natural-language answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Fine-tuned models evaluated via the blinded human + GPT-3.5 ranking and grading protocol described for covLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LoRA-based fine-tuning allowed rapid experimentation and produced high-performing models (especially synCovid+abstracts) with modest compute (single A100), demonstrating feasibility of efficient instruction-tuning for domain-specific literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>While computationally efficient, LoRA fine-tuning still requires GPU resources and careful hyperparameter tuning; potential sensitivity to small dataset overfitting was a concern but not observed in training curves reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Used to produce covLLM variants that were competitive with ChatGPT on the evaluated prompts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3693.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A powerful OpenAI GPT-3.5-era model used here as a generator in the self-instruction pipeline to synthesize outputs (and to expand seed tasks into the synCovid dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>text-davinci-003 (synthetic data generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Used with a directed prompt adapted from the Alpaca synthesis pipeline to generate synthetic instruction-input-output triplets (synCovid) from seed instructions and abstracts sampled from CORD-19; outputs manually audited before inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Seed seed tasks: 18 handwritten instructions paired with 175 CORD-19 abstracts; text-davinci-003 generated expanded synthetic examples targeting biomedical research topics.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Directed prompts instructing the model to produce instruction-input-output triplets in biomedical task formats; inputs were abstract texts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Synthetic data generation (self-instruction) rather than model distillation; leveraged a strong LLM as a data generator for supervised instruction-tuning of a smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generated instruction-input-output triplets (natural-language answers/summaries/extractions).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Manual auditing and editing of generated outputs for comprehensibility, correctness, and conciseness; random sample quality checks were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Enabled creation of a modest-size domain-focused instruction dataset (1,097 examples) that, when used for fine-tuning, contributed to covLLM variants that matched or approached ChatGPT performance on the tested prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Synthetic outputs sometimes contained incompleteness or hallucinated content; manual curation required; cost of generation limited dataset size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Synthetic examples generated by text-davinci-003 combined well with real abstracts to produce a strong fine-tuned model; no direct quantitative comparison between synthetic-generator choices was reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3693.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioMedLM: a Domain-Specific Large Language Model for Biomedical Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific LLM (from Stanford CRFM and MosaicML) trained on PubMed biomedical text, cited as evidence that field-specific LLMs can outperform general-purpose models for biomedical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioMedLM: a Domain-Specific Large Language Model for Biomedical Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A biomedical-specific language model trained on PubMed data (reported by CRFM and MosaicML) intended to provide stronger performance on biomedical NLP tasks compared to general-purpose LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Reportedly trained on biomedical data from PubMed (details not provided in this paper beyond that statement).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not detailed in this paper; implied to accept biomedical natural-language prompts for domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain-focused pretraining/fine-tuning on biomedical corpora (specific methods not described here beyond the high-level statement).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Biomedical-domain natural-language outputs (summaries, answers, etc.) as demonstrated by MosaicML/CRFM in their release (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Paper reports (in this manuscript) that BioMedLM demonstrated improved performance over general-purpose models, but specific evaluation protocols are not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example that training LLMs on field-specific biomedical data can outperform general-purpose models; used as motivation for building covLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No limitations or experimental details about BioMedLM are provided in this paper beyond the brief mention.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Reported by the original sources (cited) to outperform general-purpose models on biomedical tasks; this paper does not present direct comparative metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3693.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3693.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoronaCentral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoronaCentral</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature analysis resource that used a BERT-based multilabel document classifier to categorize ~130,000 coronavirus papers by topic and article type, facilitating literature triage and discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Analyzing the vast coronavirus literature with CoronaCentral</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>CoronaCentral (BERT-based literature categorization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Uses a BERT-based multilabel document classification system to tag large collections of COVID-19 literature by topic and article type, supporting users in identifying relevant papers quickly; cited as prior work on computational processing of COVID-19 literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to nearly 130,000 papers relevant to COVID-19 (sources not exhaustively listed here), providing topic and article-type labels.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not a query-driven summarizer; provides classification labels for documents to enable filtering by topic or article type.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning of BERT-like models for multilabel document classification (no LLM instruction-tuning described here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Multilabel topic and article-type tags for individual papers; not designed to produce synthesized theory statements or summaries in the way covLLM does.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not detailed in this paper beyond the citation; original CoronaCentral work includes classification evaluation (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of large-scale automated literature categorization that aids discovery and filtering across a large COVID-19 corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As a multilabel classifier, it provides categorization rather than open-ended synthesis; no LLM-style instruction-following synthesis described.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not compared directly in this paper to LLM-based syntheses; presented as complementary prior work enabling literature triage.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Instruct: Aligning Language Model with Self Generated Instructions <em>(Rating: 2)</em></li>
                <li>Stanford Alpaca: An Instruction-following LLaMA Model <em>(Rating: 2)</em></li>
                <li>BioMedLM: a Domain-Specific Large Language Model for Biomedical Text <em>(Rating: 2)</em></li>
                <li>Analyzing the vast coronavirus literature with CoronaCentral <em>(Rating: 2)</em></li>
                <li>CORD-19: The COVID-19 Open Research Dataset <em>(Rating: 2)</em></li>
                <li>LLaMA: Open and Efficient Foundation Language Models <em>(Rating: 2)</em></li>
                <li>Low-Rank Adaptation of Large Language Models <em>(Rating: 2)</em></li>
                <li>Less Is More for Alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3693",
    "paper_id": "paper-259108337",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "covLLM",
            "name_full": "covLLM: Large Language Models for COVID-19 Biomedical Literature",
            "brief_description": "A COVID-19–focused instruction-following LLM built by fine-tuning LLaMA-7B on a combination of synthetic instruction-abstract pairs (synCovid), mined real abstract-title pairs, and/or the Alpaca instruction dataset to enable query-driven summarization and information extraction from COVID-19 literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "covLLM",
            "system_or_method_description": "Fine-tuned LLaMA-7B instruction-following model designed to take an input research article (abstract) and a natural-language query/instruction and return a concise, query-specific answer; fine-tuning used synthetic instruction-input-output triplets (synCovid), real abstract-&gt;title pairs, and/or Alpaca 52K tasks via the Alpaca-LoRA fine-tuning framework.",
            "input_corpus_description": "Training and data generation used the BREATHE biomedical literature dataset, specifically sampling CORD-19 articles; three training regimens: (1) Alpaca 52K + synCovid (53,097 instructions), (2) synCovid only (1,097 synthetic instruction-input-output triplets), and (3) synCovid + real abstract-title pairs (2,194 instructions). Evaluation used 26 instruction-input test prompts sampled from COVID-19 literature.",
            "topic_or_query_specification": "Natural-language instruction prompts paired with an input (typically a ~250–300 word abstract); example instructions include 'Summarize this abstract', 'Extract key findings', 'Identify study type', and other user-specified queries.",
            "distillation_method": "Instruction-tuning via self-instruction synthetic data generation and fine-tuning of a pre-trained LLaMA-7B using Alpaca-LoRA (LoRA adaptation). Synthetic data (synCovid) was generated using OpenAI's text-davinci-003 following the Self-Instruct / Alpaca self-instruction pipeline; some seed outputs were generated with gpt-turbo-3.5 and manually audited.",
            "output_type_and_format": "Short, human-readable answers: summaries, extracted facts, study-type labels, key findings, or titles (for abstract-&gt;title training). Outputs were generated as a few-sentence responses constrained by decoding parameters (temperature 0.1, top-p 0.75, top-k 40, beams 4, max tokens 128).",
            "evaluation_or_validation_method": "Blind pairwise/quadruple comparison of model responses across 26 prompts where two human evaluators and GPT-3.5 (used as an automated evaluator) ranked responses by helpfulness, relevance, accuracy, and level of detail; responses were graded as Fail / Pass / Excellent following LIMA-style criteria and models were ranked from 1–4.",
            "results_summary": "The synCovid+abstracts variant performed best: in aggregated evaluator preferences (humans + GPT-3.5) it was preferred or tied with ChatGPT on ~65% of prompts. Adding Alpaca 52K to synCovid did not yield substantial improvement; synCovid-only and synCovid+abstracts achieved competitive performance with ChatGPT on the evaluated prompts.",
            "limitations_or_challenges": "Training and inference hardware requirements (powerful machine/GPU) limit practical deployment; synthetic training data contained hallucinated or partially incomplete abstracts but still provided useful style/syntax; outputs can be less reliable on broad or philosophical questions, and hallucination risk remains; small curated training sets raise overfitting concerns (though training curves showed no overfit in reported runs).",
            "comparison_to_baselines_or_humans": "Compared directly against ChatGPT in blinded human/GPT-3.5 evaluations: synCovid+abstracts matched or tied ChatGPT on 65% of prompts; Alpaca+synCovid did not significantly outperform other variants; overall covLLM variants were competitive with ChatGPT in the limited evaluation.",
            "uuid": "e3693.0"
        },
        {
            "name_short": "synCovid",
            "name_full": "synCovid synthetic COVID-19 instruction dataset",
            "brief_description": "A synthetic instruction-input-output dataset of 1,097 instruction-abstract-answer triplets generated for fine-tuning LLMs on biomedical literature tasks using the Alpaca self-instruction pipeline and OpenAI models, manually audited for quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "synCovid (synthetic data generation via Self-Instruct / Alpaca pipeline)",
            "system_or_method_description": "Synthetic instruction dataset created by pairing 18 handwritten seed instructions with 175 CORD-19 abstracts to generate outputs using gpt-turbo-3.5, then using a directed prompt and text-davinci-003 to expand into 1,097 instruction-input-output triplets guided toward biomedical research tasks.",
            "input_corpus_description": "Seed inputs sampled from CORD-19 (a COVID-19 subset of the BREATHE biomedical literature dataset); resulting synCovid contains 1,097 triplets composed of ~250–300 word abstract-style inputs (1,035 unique instructions and 865 unique inputs according to the paper).",
            "topic_or_query_specification": "Instruction templates and diverse, domain-focused prompts (example tasks: summarization, key finding extraction, identifying study design, extracting biological pathways) supplied as natural-language instructions.",
            "distillation_method": "Self-instruction / directed prompt generation: seed seed-handwritten tasks + CORD-19 abstracts -&gt; gpt-turbo-3.5 and text-davinci-003 used to synthesize outputs; outputs manually audited and edited for comprehensibility and concision; used to instruction-tune LLaMA via LoRA.",
            "output_type_and_format": "Instruction-following example pairs: each entry is (instruction, input-abstract, output-answer) where outputs are concise natural-language summaries, extractions, or labels appropriate to the instruction.",
            "evaluation_or_validation_method": "Manual quality checks: sampled 120 synCovid examples for completeness (background, methods, results, conclusions) and study-design diversity; manual auditing and editing of generated outputs prior to inclusion in training.",
            "results_summary": "synCovid provided diverse, biomedical-styled instruction examples that improved model performance when combined with real abstract-&gt;title pairs; inclusion of synCovid enabled strong performance with relatively small dataset sizes (1,097 examples) when properly fine-tuned.",
            "limitations_or_challenges": "Some synthetic inputs were incomplete or contained hallucinated content; not all generated abstracts had full background/conclusion sections; synthetic nature may introduce artifacts but authors retained partial/incomplete examples for prompt diversity.",
            "comparison_to_baselines_or_humans": "When used for fine-tuning, synCovid combined with real abstracts outperformed a model primarily trained with the larger Alpaca 52K dataset in the authors' evaluations; direct comparison of synCovid-only versus human-generated instruction data not fully explored beyond reported evaluations.",
            "uuid": "e3693.1"
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA (7-billion-parameter variant)",
            "brief_description": "An open foundation language model from Meta (LLaMA family) used as the base model for covLLM fine-tuning; chosen for its efficiency and suitability for research fine-tuning under a non-commercial license.",
            "citation_title": "LLaMA: Open and Efficient Foundation Language Models.",
            "mention_or_use": "use",
            "system_or_method_name": "LLaMA-7B (foundation model)",
            "system_or_method_description": "Pretrained transformer foundation model used as the base checkpoint for instruction-tuning via LoRA; provides the base language capability that is specialized by fine-tuning on synthetic and real biomedical instruction data.",
            "input_corpus_description": "The pretraining corpus for LLaMA itself is not detailed in this paper; within this study LLaMA-7B was fine-tuned on synCovid, real abstract-title pairs, and/or Alpaca instruction sets as described above.",
            "topic_or_query_specification": "Receives instruction-input pairs during fine-tuning and natural-language prompts at inference; standard instruction-following interface.",
            "distillation_method": "Fine-tuning via LoRA (Alpaca-LoRA framework) using instruction-tuning datasets; no novel distillation algorithm beyond instruction tuning is reported.",
            "output_type_and_format": "Natural-language responses to instructions (summaries, extracted facts, labels).",
            "evaluation_or_validation_method": "Evaluated as part of covLLM variants against ChatGPT and across human/GPT-3.5 evaluators as described for covLLM.",
            "results_summary": "LLaMA-7B served as an effective, efficient base model that—when instruction-tuned with domain-focused synthetic and mined data—yielded a model competitive with ChatGPT on the paper's 26-prompt evaluation.",
            "limitations_or_challenges": "LLaMA is under a non-commercial license; fine-tuning and inference require GPU resources; performance depends on quality and domain-specificity of fine-tuning data.",
            "comparison_to_baselines_or_humans": "When fine-tuned appropriately, LLaMA-7B-based covLLM variants matched or approached ChatGPT performance on the narrow evaluation tasks used in this study.",
            "uuid": "e3693.2"
        },
        {
            "name_short": "Alpaca",
            "name_full": "Stanford Alpaca (instruction-following LLaMA fine-tune)",
            "brief_description": "An instruction-following model derived by fine-tuning LLaMA on a 52K self-instruction dataset (Alpaca) that emulates OpenAI instruction-following behavior; used both as a baseline dataset and as part of one covLLM training regimen.",
            "citation_title": "Stanford Alpaca: An Instruction-following LLaMA Model.",
            "mention_or_use": "use",
            "system_or_method_name": "Alpaca instruction dataset / model",
            "system_or_method_description": "Provides 52K instruction-response pairs generated via self-instruction methods and used commonly for instruction-tuning; in this paper one covLLM variant was trained on Alpaca 52K + synCovid to test the effect of mixing large generic instruction data with domain-specific synthetic data.",
            "input_corpus_description": "Alpaca dataset: ~52,000 synthetic instruction-response pairs (general-domain), details per original Alpaca release; in this paper combined with synCovid for a 53,097-instruction training regime.",
            "topic_or_query_specification": "Natural-language instructions in diverse general-purpose tasks; used here as additional instruction-tuning examples rather than domain-specific prompts.",
            "distillation_method": "Alpaca's self-instruction generation approach (external to this paper) provided instruction-response pairs; used during LoRA fine-tuning of LLaMA.",
            "output_type_and_format": "Instruction-following outputs (natural-language responses).",
            "evaluation_or_validation_method": "Included as part of covLLM training variants and evaluated in the same human/GPT-3.5 comparison protocol.",
            "results_summary": "Including the large Alpaca 52K dataset in addition to synCovid did not provide significant performance gains compared to synCovid+abstracts in the authors' evaluations; suggests domain-specific curated/synthetic data can be more effective than large generic instruction sets for domain tasks.",
            "limitations_or_challenges": "Generic instruction data may dilute domain-specific signals; mixing large general instruction datasets with small domain-specific sets requires careful balancing.",
            "comparison_to_baselines_or_humans": "The Alpaca+synCovid model performed worse than the synCovid+abstracts variant and did not significantly outperform ChatGPT in the reported comparisons.",
            "uuid": "e3693.3"
        },
        {
            "name_short": "Alpaca-LoRA / LoRA",
            "name_full": "Alpaca-LoRA (Low-Rank Adaptation fine-tuning framework)",
            "brief_description": "A parameter-efficient fine-tuning approach (LoRA) and an Alpaca-LoRA training framework used to fine-tune LLaMA-7B quickly on the synCovid / abstract datasets using limited GPU resources.",
            "citation_title": "Low-Rank Adaptation of Large Language Models.",
            "mention_or_use": "use",
            "system_or_method_name": "Alpaca-LoRA / LoRA fine-tuning",
            "system_or_method_description": "LoRA (Low-Rank Adaptation) injects low-rank updates into transformer weights enabling efficient fine-tuning; the Alpaca-LoRA framework implements LoRA-based instruction-tuning workflows used here to fine-tune LLaMA-7B variants on the synthetic and mined datasets in a few hours on a single A100 GPU.",
            "input_corpus_description": "Fine-tuning corpora described above (Alpaca 52K, synCovid 1,097, real abstract-title pairs 1,097); LoRA enables using these datasets efficiently without full-weight updates.",
            "topic_or_query_specification": "Standard instruction-pairs used for supervised fine-tuning; queries are natural-language instructions paired with inputs.",
            "distillation_method": "Parameter-efficient supervised fine-tuning (LoRA) combined with instruction-tuning; no additional distillation (e.g., teacher-student) reported beyond this fine-tuning.",
            "output_type_and_format": "Instruction-following model capable of producing short natural-language answers.",
            "evaluation_or_validation_method": "Fine-tuned models evaluated via the blinded human + GPT-3.5 ranking and grading protocol described for covLLM.",
            "results_summary": "LoRA-based fine-tuning allowed rapid experimentation and produced high-performing models (especially synCovid+abstracts) with modest compute (single A100), demonstrating feasibility of efficient instruction-tuning for domain-specific literature synthesis.",
            "limitations_or_challenges": "While computationally efficient, LoRA fine-tuning still requires GPU resources and careful hyperparameter tuning; potential sensitivity to small dataset overfitting was a concern but not observed in training curves reported.",
            "comparison_to_baselines_or_humans": "Used to produce covLLM variants that were competitive with ChatGPT on the evaluated prompts.",
            "uuid": "e3693.4"
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (OpenAI)",
            "brief_description": "A powerful OpenAI GPT-3.5-era model used here as a generator in the self-instruction pipeline to synthesize outputs (and to expand seed tasks into the synCovid dataset).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "text-davinci-003 (synthetic data generator)",
            "system_or_method_description": "Used with a directed prompt adapted from the Alpaca synthesis pipeline to generate synthetic instruction-input-output triplets (synCovid) from seed instructions and abstracts sampled from CORD-19; outputs manually audited before inclusion.",
            "input_corpus_description": "Seed seed tasks: 18 handwritten instructions paired with 175 CORD-19 abstracts; text-davinci-003 generated expanded synthetic examples targeting biomedical research topics.",
            "topic_or_query_specification": "Directed prompts instructing the model to produce instruction-input-output triplets in biomedical task formats; inputs were abstract texts.",
            "distillation_method": "Synthetic data generation (self-instruction) rather than model distillation; leveraged a strong LLM as a data generator for supervised instruction-tuning of a smaller model.",
            "output_type_and_format": "Generated instruction-input-output triplets (natural-language answers/summaries/extractions).",
            "evaluation_or_validation_method": "Manual auditing and editing of generated outputs for comprehensibility, correctness, and conciseness; random sample quality checks were performed.",
            "results_summary": "Enabled creation of a modest-size domain-focused instruction dataset (1,097 examples) that, when used for fine-tuning, contributed to covLLM variants that matched or approached ChatGPT performance on the tested prompts.",
            "limitations_or_challenges": "Synthetic outputs sometimes contained incompleteness or hallucinated content; manual curation required; cost of generation limited dataset size.",
            "comparison_to_baselines_or_humans": "Synthetic examples generated by text-davinci-003 combined well with real abstracts to produce a strong fine-tuned model; no direct quantitative comparison between synthetic-generator choices was reported.",
            "uuid": "e3693.5"
        },
        {
            "name_short": "BioMedLM",
            "name_full": "BioMedLM: a Domain-Specific Large Language Model for Biomedical Text",
            "brief_description": "A domain-specific LLM (from Stanford CRFM and MosaicML) trained on PubMed biomedical text, cited as evidence that field-specific LLMs can outperform general-purpose models for biomedical tasks.",
            "citation_title": "BioMedLM: a Domain-Specific Large Language Model for Biomedical Text",
            "mention_or_use": "mention",
            "system_or_method_name": "BioMedLM",
            "system_or_method_description": "A biomedical-specific language model trained on PubMed data (reported by CRFM and MosaicML) intended to provide stronger performance on biomedical NLP tasks compared to general-purpose LLMs.",
            "input_corpus_description": "Reportedly trained on biomedical data from PubMed (details not provided in this paper beyond that statement).",
            "topic_or_query_specification": "Not detailed in this paper; implied to accept biomedical natural-language prompts for domain tasks.",
            "distillation_method": "Domain-focused pretraining/fine-tuning on biomedical corpora (specific methods not described here beyond the high-level statement).",
            "output_type_and_format": "Biomedical-domain natural-language outputs (summaries, answers, etc.) as demonstrated by MosaicML/CRFM in their release (not detailed here).",
            "evaluation_or_validation_method": "Paper reports (in this manuscript) that BioMedLM demonstrated improved performance over general-purpose models, but specific evaluation protocols are not described in this paper.",
            "results_summary": "Cited as an example that training LLMs on field-specific biomedical data can outperform general-purpose models; used as motivation for building covLLM.",
            "limitations_or_challenges": "No limitations or experimental details about BioMedLM are provided in this paper beyond the brief mention.",
            "comparison_to_baselines_or_humans": "Reported by the original sources (cited) to outperform general-purpose models on biomedical tasks; this paper does not present direct comparative metrics.",
            "uuid": "e3693.6"
        },
        {
            "name_short": "CoronaCentral",
            "name_full": "CoronaCentral",
            "brief_description": "A literature analysis resource that used a BERT-based multilabel document classifier to categorize ~130,000 coronavirus papers by topic and article type, facilitating literature triage and discovery.",
            "citation_title": "Analyzing the vast coronavirus literature with CoronaCentral",
            "mention_or_use": "mention",
            "system_or_method_name": "CoronaCentral (BERT-based literature categorization)",
            "system_or_method_description": "Uses a BERT-based multilabel document classification system to tag large collections of COVID-19 literature by topic and article type, supporting users in identifying relevant papers quickly; cited as prior work on computational processing of COVID-19 literature.",
            "input_corpus_description": "Applied to nearly 130,000 papers relevant to COVID-19 (sources not exhaustively listed here), providing topic and article-type labels.",
            "topic_or_query_specification": "Not a query-driven summarizer; provides classification labels for documents to enable filtering by topic or article type.",
            "distillation_method": "Supervised fine-tuning of BERT-like models for multilabel document classification (no LLM instruction-tuning described here).",
            "output_type_and_format": "Multilabel topic and article-type tags for individual papers; not designed to produce synthesized theory statements or summaries in the way covLLM does.",
            "evaluation_or_validation_method": "Not detailed in this paper beyond the citation; original CoronaCentral work includes classification evaluation (not reproduced here).",
            "results_summary": "Cited as an example of large-scale automated literature categorization that aids discovery and filtering across a large COVID-19 corpus.",
            "limitations_or_challenges": "As a multilabel classifier, it provides categorization rather than open-ended synthesis; no LLM-style instruction-following synthesis described.",
            "comparison_to_baselines_or_humans": "Not compared directly in this paper to LLM-based syntheses; presented as complementary prior work enabling literature triage.",
            "uuid": "e3693.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Instruct: Aligning Language Model with Self Generated Instructions",
            "rating": 2,
            "sanitized_title": "selfinstruct_aligning_language_model_with_self_generated_instructions"
        },
        {
            "paper_title": "Stanford Alpaca: An Instruction-following LLaMA Model",
            "rating": 2,
            "sanitized_title": "stanford_alpaca_an_instructionfollowing_llama_model"
        },
        {
            "paper_title": "BioMedLM: a Domain-Specific Large Language Model for Biomedical Text",
            "rating": 2,
            "sanitized_title": "biomedlm_a_domainspecific_large_language_model_for_biomedical_text"
        },
        {
            "paper_title": "Analyzing the vast coronavirus literature with CoronaCentral",
            "rating": 2,
            "sanitized_title": "analyzing_the_vast_coronavirus_literature_with_coronacentral"
        },
        {
            "paper_title": "CORD-19: The COVID-19 Open Research Dataset",
            "rating": 2,
            "sanitized_title": "cord19_the_covid19_open_research_dataset"
        },
        {
            "paper_title": "LLaMA: Open and Efficient Foundation Language Models",
            "rating": 2,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "Low-Rank Adaptation of Large Language Models",
            "rating": 2,
            "sanitized_title": "lowrank_adaptation_of_large_language_models"
        },
        {
            "paper_title": "Less Is More for Alignment",
            "rating": 1,
            "sanitized_title": "less_is_more_for_alignment"
        }
    ],
    "cost": 0.017128499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>covLLM: Large Language Models for COVID-19 Biomedical Literature</p>
<p>Yousuf A Khan 
Department of Biomedical Data Science
Stanford University
CAUSA</p>
<p>Department of Molecular and Cellular Physiology
Stanford University
Stanford University
CAUSA</p>
<p>Department of Neurology and Neurological Sciences
Stanford University
StanfordCAUSA</p>
<p>Department of Structural Biology
Stanford University
StanfordCAUSA</p>
<p>Department of Photon Science
Stanford University
StanfordCAUSA</p>
<p>Clarisse Hokia chokia@stanford.edu 
Department of Biomedical Data Science
Stanford University
CAUSA</p>
<p>Department of Computer Science
Stanford University
StanfordCAUSA</p>
<p>Jennifer Xu 
Department of Computer Science
Stanford University
StanfordCAUSA</p>
<p>Department of Bioengineering
Stanford University
StanfordCAUSA</p>
<p>Ben Ehlert behlert@stanford.edu 
Department of Biomedical Data Science
Stanford University
CAUSA</p>
<p>covLLM: Large Language Models for COVID-19 Biomedical Literature
* All authors contributed equally to this work
The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs)neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and preprocessed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD-19), we produced two datasets: (1) synCovid, which uses a combination of handwritten prompts and synthetic prompts generated using OpenAI, and (2) real abstracts, which contains abstract and title pairs. covLLM was trained with LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real abstract datasets. These models were evaluated by two human evaluators and ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract pairs datasets performs competitively with ChatGPT and outperforms covLLM trained primarily using the Alpaca dataset.</p>
<p>Introduction</p>
<p>Covid-19</p>
<p>In just three years, over 103 million people in the United States tested positive for COVID-19 and over 1.1 million people in the United States died due to COVID-19 complications 1 . COVID-19 is a highly infectious viral disease caused by SARS-CoV-2. It can cause a wide range of symptoms, most commonly fever, chills, and sore throat. Depending on the severity of the symptoms, several patients require immediate medical attention for severe difficulty in breathing, confusion, chest pain, or other symptoms of severe illness. Additionally, certain populations with pre-existing health conditions, those over age 60, and unvaccinated individuals are at increased risk for severe illness, hospitalization, and death, though anyone can become sick with COVID-19. People infected with COVID-19 are also at risk of long COVID, which occurs when they experience prolonged fatigue, respiratory, and neurological symptoms 2 .</p>
<p>Over the past 3.5 years, few COVID-19 treatments have been developed and refined. In addition to the COVID-19 vaccines 3,4 , these include over-the-counter medication, prescription medication, and in-patient treatments. Healthcare providers may prescribe Paxlovid or Lagevrio for high risk individuals infected with COVID-19. Evusheld monoclonal antibodies are prescribed to immunocompromised individuals exposed to COVID-19. Patients with severe illness due to COVID-19 who require hospitalization may be treated with the antiviral medication remdesivir and medications to counteract overactive immune systems or to treat complications. Through clinical trials and other research, these treatments were eventually developed and made available to both adult and pediatric patients who qualify. The NIH's Accelerating COVID-19 Therapeutic Interventions and Vaccines (ACTIV) initiative has promoted additional research on treatments such as immune modulators, monoclonal and polyclonal antibodies, and blood thinners and on the uses of medications used to treat other conditions 5 . However, this research has been slow to translate to clinical treatments.</p>
<p>Vaccine and treatment developments for COVID-19 were developed through an acceleration in research and treatments. Between January 1, 2020 and June 30, 2020, researchers published over 23,500 coronavirus articles, letters, reviews, notes, and editorials to major databases 6 . By August 1, 2021, the number of publications increased to 210,183, with 720,801 unique authors from all scientific subfields 7 . This vast involvement of the scientific research community was unlike trends from other infectious diseases, including HIV/AIDS, Zika, and tuberculosis. The United States, China, and Italy were the countries that published the most papers by volume, while BMJ, Journal of Medical Virology, and The Lancet were the journals that published the most papers by volume. Of the articles published on Scopus and Web of Science, 48% and 37%, respectively, were research papers. Findings have involved topics such as data reporting quality, mental health impacts of the pandemic, conflicts of interest, quality of research publications and studies, impacts of the pandemic on academia, and the uses of technology to learn more about COVID-19 6 . Additionally, at least one author from each of the 21 major scientific fields and 174 scientific subfields published on COVID-19 7 .</p>
<p>Through the race to publish on the COVID-19 pandemic, scientists have highlighted the volume of articles and questioned the quality of clinical trials. Ioannidis, Salholz-Hillel, et. al underscore the number of researchers and breadth of disciplines that published on COVID-19, stating that 28% of COVID-19 publication authors published in a subfield that was different from their subfield of expertise. They express concern that some COVID-19 authors' fields of expertise were "remote" from COVID-19, including "fisheries, ornithology, entomology, or architecture". They also cite that some scientists had participated in "epistemic trespassing," where scientists publish on health and medical questions, despite being experts in other fields. Moreover, surveys on the quality of COVID-19 research consistently found a high prevalence of low-quality studies 7 . Park, Mogg, et. al argue that clinical trials focused primarily on treatments for severe disease, rather than pre-exposure, post-exposure, or outpatient treatments and identified shortcomings including overlaps in proposed trials, having sample sizes smaller than 100 participants, and not identifying dose ranges. The translation of relevant findings to clinical practice has been slow and inconsistent, resulting in poorer quality of care to patients 8 . This exponential rise in coronavirus research creates the opportunity for computational methods that enable clinicians to efficiently filter through papers and rapidly translate these findings into treatments.</p>
<p>Large Language Models</p>
<p>Large language models (LLMs) are deep learning algorithms that can engage with linguistic and language components, such as text, for natural language processing and other artificial intelligence applications. LLMs learn from large datasets that typically include almost everything available on the internet, where algorithms define metrics of similarity and use those to group inputs. Following training, LLMs are then able to use the given knowledge to generate desired outputs. They can also be trained or fine-tuned with smaller batches of data for specific applications, such as biomedical research. One commonly known example of an LLM application is ChatGPT, which can perform natural language processing functions. Despite the potential applications of LLMs, some challenges of using LLMs for specific fields include domain constraints, dataset availability, and technical skillset of the developers 9 . Branching from current technologies, emerging areas of LLMs include developing models that can check their own outputs. A current shortcoming of existing generative language models is models' tendencies to "hallucinate", which occurs when LLMs present false or inaccurate information as facts. Potential solutions to this problem include having a model provide citations, allowing the model to access external information sources, or asking the model to identify aspects of the output that it feels are the weakest 10 .</p>
<p>In addition to the development of powerful large language models, several LLMs have also been made available to researchers. For example, in February 2023, Meta publicly released Large Language Model Meta AI (LLaMA), an LLM that works with less compute and resources and provides researchers access to studying and fine-tuning LLMs. This allows researchers to further understand how LLMs work, to improve LLMs, and to reduce issues like bias and misinformation 11 . One application of LLaMA is Stanford Alpaca, a fine-tuned model that can behave similarly to OpenAI's text-davinci-003 and follow instructions. However, due to ethical issues, safety concerns, and companies' policies, both are only available for academic research, and LLaMA is released under a non-commercial license 12 .</p>
<p>Opportunities that make the creation of a COVID-19-specific LLM are the availability of biomedical literature datasets and machine learning applications that process COVID-19 literature. For instance, in June 2020, the Machine Learning Google Developer Experts group (ML GDEs) released the first version of the Biomedical Research Extensive Archive To Help Everyone (BREATHE), which is a large-scale database with over 16 million biomedical articles from different repositories and hosted on Google BigQuery. This publicly accessible database contains titles, abstracts, and some full body texts and allows biomedical researchers to glean new insights from research publications 13 . Additionally, the CoronaCentral resource used a BERT-based document multilabel classification method to categorize nearly 130,000 papers based on topic, article type, and preprint server type so that users can identify papers that are most pertinent to their research or clinical needs 14 . A biomedical LLM, known as BioMedLM, has also been released by the Stanford Center for Research on Foundation Models (CRFM) and MosaicML. This model was trained on biomedical data from PubMed and demonstrated that training LLMs on data from specific fields can outperform general-purpose models. Other LLMs created by the CRFM team include DRAGON and BioLinkBERT. Work by the CRFM team has shown that LLMs are applicable to specific fields and that focusing the model on a specific field allows models to perform well with less data and compute 15 .</p>
<p>With the availability of foundational and fine-tuned LLMs, biomedical literature databases, and prior work on COVID-19 literature classification, we fine-tuned a large language model to interact with COVID-19 literature inputs and queries called covLLM.</p>
<p>Methods</p>
<p>Description of Data</p>
<p>We generated two types of training data: a) synthetic training data generated through OpenAI's text-davinci-003 model with diverse prompts and content and b) actual abstracts where the only provided prompt was to summarize the abstract and the output was the actual title of the article.</p>
<p>The BREATHE dataset was used for generating both types of training data, serving as the basis of synthetic data generation (described below) or for mining of real abstracts. BREATHE is a large biomedical literature database containing papers from 10 major repositories of biomedical research. We specifically sample from CORD-19, a subset of BREATHE that contains curated articles deemed relevant to COVID-19 research 16,17 .</p>
<p>Synthetic Data Generation</p>
<p>To generate our training data, we followed the self-instruction protocol 18. . In line with the self-instruction input format, our training dataset is a list of instruction-input-output triplets (Figure 1). To create the initial seed tasks, we paired 18 handwritten instructions with 175 randomly selected abstracts from the CORD-19 dataset. Examples of possible instructions include summarizing a provided abstract, extracting the key findings, identifying any mentioned biological or chemical pathways, determining the study type, and evaluating the quality of a study's findings. Using OpenAI's gpt-turbo-3.5 model, we utilized the instruction-abstract pairs to generate the corresponding outputs. Each output was manually evaluated and edited for comprehensibility, correctness, and conciseness.</p>
<p>We then employed Alpaca's self-instruction-based data synthesis pipeline 19 to generate a total of 1097 instruction-input-output triplets. The pipeline utilizes a directed prompt and OpenAI's text-davinci-003 to generate synthetic instruction-input-output triplets from a given set of seed tasks. We modified Alpaca's directed prompt to guide synthetic tasks towards biomedical research-related topics, ensuring each task included an input formatted as a 250-300 word abstract.  20 . This collection of synthetic COVID19 instructions, synCovid, consists of 1097 instruction-input-output triplets.</p>
<p>Real abstract mining</p>
<p>In addition to the synthetic training set, we also created a simple dataset of instruction-input-output triplets in which the inputs were real, COVID19 specific abstracts. For each entry in this dataset, the instruction is "Summarize this abstract", the input is an abstract sampled from CORD-19, and the output is the actual title associated with the selected abstract. We sampled 1097 examples for this training data, equal to the number of synthetically generated instructions.</p>
<p>2.2.</p>
<p>Developing our models</p>
<p>Training covLLM</p>
<p>We trained covLLM using the LLaMa 7B model as our baseline model 21 . We ultimately trained three different models (the classic Alpaca 52K self-instruction dataset supplemented with 1097 synthetic scientific literature specific tasks, 1097 synthetically generated tasks, and 1097 input, real abstract paired prompts). We fine-tuned our models using the Alpaca-Lora framework 22,23 , which only required several hours on a single NVIDIA A100 per model. The relevant training parameters for the following datasets were the following: 1) Alpaca 52K + synCovid dataset -53097 total instructions, 3 epochs, learning rate of 3e-4, batch size of 128, and eval size of 2,000 2) synCovid dataset only -1097 total instructions, 30 epochs, learning rate of 1e-5, batch size of 16, eval size of 100 3) synCovid and real abstract paired prompts -2194 instructions, 30 epochs, learning rate of 1e-5, batch , with examples of a seed task and a generated task. Handwritten prompts and abstracts sampled from CORD-19 were fed into gpt-turbo-3.5 to generate expected outputs, which were manually audited. Then, the seed tasks and a directed prompt were fed to text-davinci-003 to generate synCovid. size of 16, eval size of 100. These parameters were determined by a parameter sweep and by assessing the training and evaluation loss curves (data not shown). Otherwise, all other parameters were kept identical to the Alpaca-Lora framework.</p>
<p>2.3.</p>
<p>Evaluating our models</p>
<p>Experiment</p>
<p>We conducted an experiment to assess the performance of our three models, namely synCovid, synCovid+abstracts and synCovid+Alpaca against ChatGPT in generating satisfactory outputs. The purpose was to evaluate how well these models can respond to various test prompts.</p>
<p>We devised an experimental setup where we generate a single response to each test prompt from each model. Responses were blinded to the human evaluators and ordering was randomized. Two Human evaluators compared the responses and indicated their preference for each prompt. We also repeated the experiment using GPT-3.5 as the evaluator.</p>
<p>Model Output Generation</p>
<p>For generating outputs from our model for test set evaluation (i.e. inference), we used the following parameters for all three major models that we trained: Temperature: 0.1, Top p: 0.75, Top k: 40, Beams: 4, and Max Tokens: 128. For generating ChatGPT outputs to compare these models against, we simply prompted it with the following input: "Please respond to these instructions with a given input in a few sentences; assume that each question is independent of each other and answer each one individually."</p>
<p>Methodology</p>
<p>During each iteration, the evaluators received the instruction (e.g. "Summarize this abstract"), input (e.g. text of the abstract) and 4 responses from the models (Figure 2). The evaluators were requested to rank each model considering helpfulness, relevance, accuracy, and level of detail, and ties between models were allowed. Furthermore, the evaluators scored each model as either Fail: the response did not meet the requirements of the prompt, Pass: the response met the requirements of the prompt, or Excellent: the model provided an excellent response to the prompt. This follows the same grading system as described in the LIMA study 20 . The specific prompt for GPT3.5 evaluation is in the appendix.</p>
<p>To evaluate each model, we counted the number of Excellent, Pass, and Fail grades then averaged the results from the three sets of evaluations. This was repeated for the models' rankings from 1 to 4. Results from the two human evaluators and GPT3.5 were weighted equally, and the models' training dataset(s) remained blinded until evaluations were completed.</p>
<p>Results</p>
<p>3.1.</p>
<p>Data Generation</p>
<p>Synthetic Data Generation Quality Control</p>
<p>We devised a synCovid, a synthetic data generated by OpenAI's text-davinci-003 model, as one of our sources for training data. synCovid is a dataset of instruction-input-output triplets consisting of 1035 unique instructions and 865 unique inputs for a total of 1097 aggregate instructions. To evaluate the diversity and quality of synCovid prior to its inclusion into training, we examined the generated instructions in two ways.</p>
<p>First, we manually extracted verb-subject pairs from each of the synCovid instructions, resulting in 581 unique verb-subject pairs (Figure 3). The majority of the instructions were related to extracting specific information from the given input, such as identifying the sample population or describing the study methodology. The diversity of the synCovid instructions can be seen through the subjects, which are more equally represented in the verb-subject pairs.</p>
<p>To assess the quality of the synCovid generated inputs, we randomly sampled 120 synCovid examples. Ideally, these inputs mimic an abstract of a biomedical research paper. Therefore, an input was considered complete if it discussed background information, methodology, results, and conclusions. We classified each sample input as complete or incomplete. We also determined the study design described by input. Our sampled generated inputs are representative of a variety of study designs. The most common study designs generated were literature reviews, cross-sectional studies, and method development studies (Figure 4). While all the sampled generated inputs were  comprehensible, a minority were incomplete. Some generated abstracts consisted solely of a methodology and description of results ( Figure 5). Despite this, we decided to include both fully complete and partially complete in our training data due to the prompt diversity they provided.</p>
<p>Synthetic Data Generation Quality Control</p>
<p>We manually double checked the results of our real abstract mining to ensure that our abstract instructions were of good quality.</p>
<p>3.2.</p>
<p>Developing well-trained models</p>
<p>covLLM Training Results</p>
<p>Here we present our training and evaluation curves for our three major models after training. As expected, the Alpaca + synCovid model showed both a decrease in training and evaluation loss over the course of our training, demonstrating that the model was not overfit (Figure 6). Overfitting was a major concern we had using such small training sets for our synCovid only (1097 unique instructions) and synCovid + real abstract prompts (2194 instructions). However, our training and evaluation curves demonstrate that, despite cycling through these limited datasets for 30 epochs, we did not overfit our model (Figure 7). Figure 5. Example of a complete generated input (left) and partially complete generated input (right). Key parts of the abstract are bolded. Note that the complete input has a background, objective, methods, results, and conclusion. Note also that the incomplete input is missing background information and conclusions. </p>
<p>3.3.</p>
<p>How the models performed</p>
<p>-3.3.4. Evaluation summary</p>
<p>In this section, we present the findings from our evaluation, where the evaluators assessed a total of 26 instruction-input pairs. Figure 8 illustrates the average grade assigned to the models across all evaluators. Several key observations emerged from the evaluation process. First, we observed that the inclusion of abstractsummarization pairs proved to be beneficial. The model that incorporated these pairs (synCovid+abstracts) showed great performance. Second, we  discovered that the addition of the 52k Alpaca tasks to the synCovid model did not lead to any significant enhancement in performance.</p>
<p>In Figure 9, we present a head-to-head comparison between our models and ChatGPT. Notably, we found that the synCovid+abstracts model emerged as the best performing model, exhibiting promising results. In 65% of the prompts, this model was either preferred by the evaluators or tied with ChatGPT in terms of performance.</p>
<p>Discussion</p>
<p>The COVID-19 pandemic simultaneously led to millions of deaths globally and a need to rapidly translate research to treatment. covLLM, a machine learning-based tool that we successfully developed, will enable scientists and clinicians to rapidly incorporate knowledge from the growing body of literature into their decisions, research, and clinical care that will impact patient outcomes. covLLM will provide an architecture to tackle current diseases and future pandemics. Additionally, pilot studies into other research fields (data not shown) demonstrates that covLLM's base architecture and training strategy can be generalized to additional scientific field.</p>
<p>From an LLM perspective, we demonstrate how a general, small pre-trained language model can be guided and fine-tuned to accomplish a highly specific task given a handful of synthetically generated and mined tasks. This was especially surprising, given that the synthetically generated instructions and abstracts contained hallucinated information but were still matched the style and syntax of biomedical research. Additionally, covLLM performance was comparable or exceeded ChatGPT's performance in our evaluation. This emphasizes the importance of prompt diversity, not necessarily prompt accuracy, in the fine-tuning stage and shows how limited real-world data can still lead to robust performance.</p>
<p>The limitations of covLLM to practical usage by clinicians and researchers is twofold. First, to quickly process entire papers in a reasonable time frame, a powerful and dedicated machine must be setup to take such requests. This is currently impractical given our current limitations as students, but could be solved if given additional resources and time to optimize the performance of these models on more consumer-based hardware. Another limitation is that its answers to broader, more philosophical questions such as "What are the public health implications of this basic science research?" can range from highly accurate to inaccurate. Thus, some degree of user filtering and prior knowledge is still required, as with many other computational tools and assistants.</p>
<p>Appendix</p>
<p>See https://github.com/clarisseh47/bioLLM.</p>
<p>Figure 1 :
1Diagram of synthetic data generation (synCovid)</p>
<p>Figure 2 .
2User interface for human evaluators. Instruction, input and the response were response were provided. Evaluators ranked and graded each model response.</p>
<p>Figure 3 .
3Distribution of verb-subject combinations from synCovid instructions. For readability, only the top 5% of subject-verb combinations are shown.</p>
<p>Figure 4 :
4Distribution of study types of generated inputs (left) and comparison of complete versus incomplete generated inputs (right).</p>
<p>Figure 6 .
6Alpaca 52K and synthetic covid combined dataset training and evaluation loss curves run over three epochs.</p>
<p>Figure 7 .
7The two left panels are from the synthetic covid19 dataset only (1097 instructions) and the right two panels are from the synthetic covid19 dataset and the real abstract pairs database (2194 instructions). Both regimens show a decrease in training and evaluation loss over 30 epochs of data.</p>
<p>Figure 8 .
8All four models average grade across 26 test prompts.</p>
<p>Figure 9 .
9Preference for all evaluators (human and chatGPT), comparing our models to chatGPT across 26 prompts.</p>
<p>on the results of another study that demonstrated training on as little as 1000 instructions can yield robust performance if properly fine-tunedWe chose to generate a 
small training set size of 
1097 examples, as 
compared to Alpacas' 
52,000 training set, due to 
the monetary cost of 
generating these examples 
and based </p>
<p>United States of America: WHO Coronavirus Disease (COVID-19) Dashboard With Vaccination Data. United States of America: WHO Coronavirus Disease (COVID-19) Dashboard With Vaccination Data. Accessed May 2, 2023. https://covid19.who.int</p>
<p>Coronavirus disease (COVID-19). Accessed. Coronavirus disease (COVID-19). Accessed May 2, 2023. https://www.who.int/news-room/questions-and- answers/item/coronavirus-disease-covid-19</p>
<p>Understanding How COVID-19 Vaccines Work. Centers for Disease Control and Prevention. Understanding How COVID-19 Vaccines Work. Centers for Disease Control and Prevention. Published February 3, 2023. Accessed May 2, 2023. https://www.cdc.gov/coronavirus/2019-ncov/vaccines/different- vaccines/how-they-work.html</p>
<p>Affairs (ASPA) AS for P. COVID-19 Vaccines. HHS.gov. 12Published DecemberAffairs (ASPA) AS for P. COVID-19 Vaccines. HHS.gov. Published December 12, 2020. Accessed May 2, 2023. https://www.hhs.gov/coronavirus/covid-19-vaccines/index.html</p>
<p>COVID-19 Treatments. NIH COVID-19 Research. Accessed. COVID-19 Treatments. NIH COVID-19 Research. Accessed May 2, 2023. https://covid19.nih.gov/covid- 19-treatments</p>
<p>COVID-19 research update: How many pandemic papers have been published? Nature Index. PublishedCOVID-19 research update: How many pandemic papers have been published? Nature Index. Published August 28, 2020. Accessed May 2, 2023. https://www.nature.com/nature-index/news-blog/how- coronavirus-is-changing-research-practices-and-publishing</p>
<p>The rapid, massive growth of COVID-19 authors in the scientific literature. Jpa Ioannidis, M Salholz-Hillel, K W Boyack, J Baas, 10.1098/rsos.210389R Soc Open Sci. 89210389Ioannidis JPA, Salholz-Hillel M, Boyack KW, Baas J. The rapid, massive growth of COVID-19 authors in the scientific literature. R Soc Open Sci. 2021;8(9):210389. doi:10.1098/rsos.210389</p>
<p>How COVID-19 has fundamentally changed clinical research in global health. Jjh Park, R Mogg, G E Smith, 10.1016/S2214-109X(20)30542-8Lancet Glob Health. 95Park JJH, Mogg R, Smith GE, et al. How COVID-19 has fundamentally changed clinical research in global health. Lancet Glob Health. 2021;9(5):e711-e720. doi:10.1016/S2214-109X(20)30542-8</p>
<p>What Are Large Language Models and Why Are They Important? NVIDIA Blog. A Lee, PublishedLee A. What Are Large Language Models and Why Are They Important? NVIDIA Blog. Published January 26, 2023. Accessed May 2, 2023. https://blogs.nvidia.com/blog/2023/01/26/what-are-large- language-models-used-for/</p>
<p>The Next Generation Of Large Language Models. The Next Generation Of Large Language Models. Accessed May 2, 2023. https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language- models/?sh=6e32ff9d18db</p>
<p>Introducing LLaMA: A foundational, 65-billion-parameter language model. Introducing LLaMA: A foundational, 65-billion-parameter language model. Accessed May 2, 2023. https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</p>
<p>How the Google AI Community Used Cloud to Help Biomedical Researchers. Crfm Stanford, Accessed, Google Cloud Blog. Accessed. Stanford CRFM. Accessed May 2, 2023. https://crfm.stanford.edu/2023/03/13/alpaca.html 13. How the Google AI Community Used Cloud to Help Biomedical Researchers. Google Cloud Blog. Accessed May 2, 2023. https://cloud.google.com/blog/products/ai-machine-learning/google-ai-community- used-cloud-to-help-biomedical-researchers</p>
<p>Analyzing the vast coronavirus literature with CoronaCentral. J Lever, R B Altman, 10.1073/pnas.2100766118Proc Natl Acad Sci. 118232100766118Lever J, Altman RB. Analyzing the vast coronavirus literature with CoronaCentral. Proc Natl Acad Sci. 2021;118(23):e2100766118. doi:10.1073/pnas.2100766118</p>
<p>BioMedLM: a Domain-Specific Large Language Model for Biomedical Text. BioMedLM: a Domain-Specific Large Language Model for Biomedical Text. Accessed May 2, 2023. https://www.mosaicml.com/blog/introducing-pubmed-gpt</p>
<p>BREATHE BioMedical Literature Dataset -Marketplace -Google Cloud console. Accessed. BREATHE BioMedical Literature Dataset -Marketplace -Google Cloud console. Accessed May 2, 2023. https://console.cloud.google.com/marketplace/product/breathe-gcp-public-data/breathe?hl=da- GL&amp;project=bmi-212</p>
<p>CORD-19: The COVID-19 Open Research Dataset. L L Wang, K Lo, Y Chandrasekhar, Published onlineWang LL, Lo K, Chandrasekhar Y, et al. CORD-19: The COVID-19 Open Research Dataset. Published online July 10, 2020. Accessed May 2, 2023. http://arxiv.org/abs/2004.10706</p>
<p>Self-Instruct: Aligning Language Model with Self Generated Instructions. Y Wang, Y Kordi, S Mishra, Published onlineWang Y, Kordi Y, Mishra S, et al. Self-Instruct: Aligning Language Model with Self Generated Instructions. Published online December 20, 2022. Accessed May 2, 2023. http://arxiv.org/abs/2212.10560</p>
<p>An Instruction-following LLaMA Model. Stanford Alpaca, Published onlineStanford Alpaca: An Instruction-following LLaMA Model. Published online May 2, 2023. Accessed May 2, 2023. https://github.com/tatsu-lab/stanford_alpaca</p>
<p>Less Is More for Alignment. C Zhou, P Liu, P Xu, 10.48550/arXiv.2305.11206Published onlineZhou C, Liu P, Xu P, et al. LIMA: Less Is More for Alignment. Published online May 18, 2023. doi:10.48550/arXiv.2305.11206</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, Published onlineTouvron H, Lavril T, Izacard G, et al. LLaMA: Open and Efficient Foundation Language Models. Published online February 27, 2023. Accessed May 2, 2023. http://arxiv.org/abs/2302.13971</p>
<p>Low-Rank Adaptation of Large Language Models. E J Hu, Y Shen, P Wallis, 10.48550/arXiv.2106.09685162021Published online OctoberHu EJ, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models. Published online October 16, 2021. doi:10.48550/arXiv.2106.09685</p>
<p>. E J Wang, Alpaca-Lora, Published onlineWang EJ. Alpaca-LoRA. Published online June 7, 2023. Accessed June 7, 2023. https://github.com/tloen/alpaca-lora</p>            </div>
        </div>

    </div>
</body>
</html>
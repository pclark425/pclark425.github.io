<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1244 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1244</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1244</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-270045288</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15083v1.pdf" target="_blank">MuDreamer: Learning Predictive World Models without Reconstruction</a></p>
                <p><strong>Paper Abstract:</strong> The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using a pixel reconstruction loss. However, while the reconstruction loss is essential to Dreamer's performance, it also necessitates modeling unnecessary information. Consequently, Dreamer sometimes fails to perceive crucial elements which are necessary for task-solving when visual distractions are present in the observation, significantly limiting its potential. In this paper, we present MuDreamer, a robust reinforcement learning agent that builds upon the DreamerV3 algorithm by learning a predictive world model without the need for reconstructing input signals. Rather than relying on pixel reconstruction, hidden representations are instead learned by predicting the environment value function and previously selected actions. Similar to predictive self-supervised methods for images, we find that the use of batch normalization is crucial to prevent learning collapse. We also study the effect of KL balancing between model posterior and prior losses on convergence speed and learning stability. We evaluate MuDreamer on the commonly used DeepMind Visual Control Suite and demonstrate stronger robustness to visual distractions compared to DreamerV3 and other reconstruction-free approaches, replacing the environment background with task-irrelevant real-world videos. Our method also achieves comparable performance on the Atari100k benchmark while benefiting from faster training.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1244.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1244.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuDreamer (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free latent world model built on DreamerV3 that learns predictive representations by predicting rewards, continuation flags, the value function, and previous actions (with an optional auxiliary decoder stop‑grads). It replaces pixel reconstruction with task-focused predictive losses and uses batch normalization to avoid representational collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuDreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space Recurrent State Space Model (RSSM) with a CNN encoder; a GRU-based sequential network produces recurrent hidden state h_t; a representation network produces stochastic categorical latents z_t (one-hot categorical sampling) conditioned on h_t and encoder features x_t; a dynamics predictor predicts prior ẑ_t from h_t. From concatenated {h_t, z_t} the model predicts reward, continuation flag, Symlog-discretized λ-returns (value), and the previously selected action. An auxiliary decoder reconstruction head may be trained with a stop-gradient (so decoder gradients do not propagate into representation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM with stochastic categorical latents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous visual control (DeepMind Control / Visual Control Suite) and Atari100k (discrete-action Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction losses (negative log-likelihood) on reward, continuation, discretized Symlog λ-returns (value), and action-prediction loss; KL divergence between posterior q(z_t|h_t,x_t) and prior p(ẑ_t|h_t) (dynamics loss) and between q and stop-gradient prior (representation loss). Auxiliary reconstruction loss (log p(o_t|sg(s_t))) is optional and trained with stop-gradient so it does not guide representation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No low-level MSE or pixel-wise fidelity metrics reported; fidelity is evaluated implicitly via predictive losses and task performance. Task-level results: Visual Control Suite (1M steps) mean score 784.7 and median 849.6 (Table 1); Natural background (background replaced by real-world videos) mean 517.0 (Table 2). Atari100k: reported comparable performance to DreamerV3 (no single aggregated numeric fidelity metric reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural latent model, but interpretable aspects via visualizations: decoder reconstructions (auxiliary decoder with stop-gradient), imagined latent trajectories visualized as decoded frames, and per-channel standard-deviation diagnostics over encoder/model outputs to detect collapse. Latents are not reported to map to semantically disentangled factors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of decoder reconstructions, visualization of imagined trajectories decoded from latent rollouts, monitoring per-channel std of l2-normalized encoder outputs and recurrent state; qualitative inspection (examples) of whether reconstructions keep agent vs background.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model parameters: MuDreamer total ~15.3M trainable parameters (paper). FLOPs for a forward world-model training pass (64 time frames, 64x64 input) ~2.5 billion (Table 11). Training speed reported: ~14 hours to reach 1M environment steps on a single NVIDIA RTX 3090 + 16 CPU cores (Walker-run task); Atari Breakout: 4h40 to reach 400K steps (single RTX 3090, 16 cores).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Fewer FLOPs and slightly faster training than DreamerV3 and substantially less compute than reconstruction-free methods that encode two augmented views (e.g., DreamerPro). Table 11 compares forward FLOPs: MuDreamer 2.5B vs DreamerV3 4.3B and DreamerPro ~7.9B (64x64). Benefit arises because MuDreamer removes decoder gradients and avoids encoding multiple augmented views.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Visual Control Suite (1M env steps): mean 784.7, median 849.6 (Table 1). Natural background (1M env steps): mean 517.0 (Table 2), outperforming DreamerPro and TPC under visual distractions. Atari100k: comparable to DreamerV3 (no exact consolidated score given here but reported as comparable in text and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuDreamer trades pixel-level reconstruction fidelity for task-focused predictive fidelity: by predicting rewards, continuation, value, and actions it learns representations prioritizing task-relevant elements (e.g., agent body, small objects) and is more robust to task-irrelevant visual distractions (natural video backgrounds). The paper shows that higher pixel reconstruction fidelity (DreamerV3) can lead to modeling unnecessary background detail and failure to represent small but task-relevant objects.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Removing reconstruction reduces modeling of unnecessary visual detail and improves robustness and compute-efficiency, but requires architectural and objective changes to prevent collapse (batch normalization in representation network, action & value predictors). KL balancing is delicate: too strong posterior regularization (β_rep high) limits latent information and slows learning; β_rep=0 leads to instabilities; paper settles on β_pred=1.0, β_dyn=0.95, β_rep=0.05. Action/value heads increase representational usefulness but add prediction losses.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predictive-only objective: reward/continue/value/action prediction plus dynamics KL losses; stop-gradient auxiliary decoder optional (does not affect training); categorical stochastic latents with 32 latents × 32 classes; batch normalization inside representation network to prevent feature collapse; KL balancing (β_dyn=0.95, β_rep=0.05); imagination horizon H=15; Symlog-discretized λ-returns for value training; actor-critic learned in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DreamerV3 (reconstruction-based): MuDreamer has fewer parameters (15.3M vs 17.9M), substantially lower FLOPs, slightly faster training, and better robustness to visual distractions while achieving comparable or better control performance on many tasks. Compared to DreamerPro (reconstruction-free prototypical two-view method): MuDreamer attains similar or better performance without encoding two augmented views (hence lower FLOPs and memory). Compared to TPC (contrastive temporal predictive coding): TPC is faster but has lower task performance. Compared to MuZero-inspired approaches, MuDreamer borrows value-prediction ideas but operates without MCTS and works in continuous action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations/findings: include both action and value prediction heads (removing them degrades performance); use batch normalization in the representation network to avoid collapse; set KL balancing to small but nonzero β_rep (paper uses 0.05) and β_dyn near 0.95 (default DreamerV3 settings slowed convergence); optional stop-gradient on reconstruction avoids decoder influencing representations. These settings were empirically identified as best trade-offs for stability, fidelity to task-relevant features, and computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1244.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1244.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-space model-based RL agent that learns a Recurrent State Space Model with a pixel reconstruction loss and trains actor-critic purely in imagination; uses normalization and symlog outputs to scale across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent RSSM with CNN encoder and decoder autoencoder; sequential GRU-based prior/posterior dynamics with stochastic latents (originally Gaussian/categorical variants across versions); trained with pixel reconstruction loss + predictions for reward, continuation, and next latent; actor and critic trained on imagined latent rollouts. Uses layer normalization, symlog for reward/value scaling, and KL-balancing between posterior and prior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM with pixel reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Diverse domains: DeepMind Control Suite (visual control), Atari, Minecraft and others (as claimed by DreamerV3)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel reconstruction loss (autoencoder reconstruction), predictive negative log-likelihood for reward/continue/value, KL divergence between posterior and prior (KL-balancing). Value trained on discretized Symlog λ-returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No reported low-level numeric fidelity multipliers like MSE; reported task performance on Visual Control Suite (1M steps) mean score 739.6, median 808.5 (Table 1). Under natural background distractions DreamerV3 often reconstructs background details and may lose small task elements (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box latent model but decoder reconstruction provides a way to inspect what the latent encodes; paper reports that DreamerV3 reconstructions often preserve background detail at expense of small task-critical items (object vanishing phenomenon).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Decoder reconstructions, qualitative comparison of reconstructions under natural background setting, imagined trajectory decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Parameters reported ~17.9M for the DreamerV3 configuration used in comparisons. Forward FLOPs (world-model training forward) ~4.3B for 64x64 example. Training time reported ~15 hours to 1M env steps on same hardware (RTX 3090 + 16 CPUs) for Walker-Run.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Higher FLOPs and slightly slower training than MuDreamer due to decoder/backprop to encoder and possibly other architectural choices. Reconstruction and use of autoencoder increases compute and memory compared to reconstruction-free alternatives that encode single view only.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Visual Control Suite (1M steps): mean 739.6, median 808.5 (Table 1). On natural-background tasks DreamerV3 performs worse than MuDreamer and DreamerPro in many cases due to focus on background details.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High pixel reconstruction fidelity does not necessarily translate to task utility: DreamerV3 sometimes models background detail and neglects small but critical task objects, leading to degraded policy performance under visual distractions. Reconstruction enforces encoding of all visual information which can be detrimental when many features are irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel-reconstruction enforces full-image fidelity (useful in clean settings) at the cost of modeling irrelevant detail and increased compute; KL-balancing hyperparameters can slow convergence if set suboptimally (authors note default DreamerV3 KL balancing slowed learning in MuDreamer experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pixel reconstruction loss with autoencoder, layer normalization, symlog outputs for reward/value, KL balancing of prior/posterior (DreamerV3 defaults differ from MuDreamer tuned values), training actor-critic in latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Against MuDreamer, DreamerV3 achieves strong baseline performance in many tasks but is less robust to visual distractions and uses more compute. Against reconstruction-free methods (DreamerPro, TPC), DreamerV3's reconstruction objective can be a liability in settings with task-irrelevant visual variability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests DreamerV3 hyperparameters work well across diverse domains but warns that KL-balancing parameters should be tuned per objective; MuDreamer experiments indicate reducing posterior regularization (different β values) can improve learning speed/quality when not using reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1244.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1244.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL algorithm that learns a compact model predicting rewards, value and policy and uses MCTS (Monte-Carlo Tree Search) over the learned model for planning, enabling strong performance in discrete precision planning domains without reconstructing observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns representation, dynamics and prediction networks that predict value, reward and policy logits for hypothetical steps; used with MCTS to perform planning using learned model predictions rather than reconstructing observations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model combined with tree search (MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Discrete planning domains (Chess, Shogi, Go) and was extended to Atari; excels in discrete action spaces with precision planning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Matches TD targets on predicted value and predicted rewards across unrolled steps; policy targets derived from MCTS visit distributions. Fidelity measured as error in predicted rewards/values and alignment with MCTS outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported in this paper beyond being cited as inspiration; MuZero is known to achieve superhuman performance in planning benchmarks (original MuZero results reported in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is a learned neural predictor plus MCTS; interpretability is limited; planning traces (MCTS rollouts) provide some insight into decision reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Analysis typically via MCTS tree visualization and inspecting predicted reward/value estimates; not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MCTS is computationally expensive and scales poorly to high-dimensional continuous action spaces (noted in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Highly effective in discrete domains but not readily applicable to high-dimensional continuous action problems without discretization; compared qualitatively in the paper as inspiration for predictive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not evaluated here; referenced as state-of-the-art in discrete planning domains in original work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's predictive losses (value/reward/policy) inspire MuDreamer's choice to predict task-relevant quantities rather than pixel reconstructions; MuZero's planning utility does not directly transfer to continuous-action visual control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Strong planning performance vs high compute and poor scaling to continuous actions; motivates MuDreamer's different design choices (no MCTS, predictive objectives that work with continuous actions).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predict value, reward and policy logits; use MCTS on learned model; train on TD value/r reward/policy targets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>MuZero excels in discrete planning but is less applicable to continuous high-dimensional visual control, whereas Dreamer-family methods operate directly in latent space and handle continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1244.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1244.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerPro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerPro (reconstruction-free prototypical representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free variant of Dreamer that uses prototypical/self-supervised representation learning (SwAV-like) requiring two augmented views of images to compute a clustering loss, aiming to avoid pixel reconstruction while learning useful latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerPro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reconstruction-free Dreamer variant that encodes two augmented views of the same image and enforces consistent prototypical assignments (SwAV-style) across views to learn latent representations, combined with RSSM latent dynamics for imagination-based actor-critic training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with prototypical/self-supervised representation objective</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual control (DeepMind Control), similar benchmarks as Dreamer family</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Self-supervised prototypical clustering consistency (SwAV objective) across augmented views, plus predictive losses for reward/value; fidelity judged via downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported in this paper as a strong reconstruction-free baseline: in natural background experiments DreamerPro mean 445.2 (Table 2) on the Visual Control Suite; in some configurations DreamerPro competitive with MuDreamer but requires encoding two views and thus higher compute.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations learned via prototypical clustering; interpretability limited but representations aim for discriminative clusters rather than pixel-perfect reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not deeply detailed in this paper; referenced methods typically inspect cluster assignments or downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher compute and memory due to encoding two augmented views per observation: Table 11 shows forward FLOPs ~7.9B (64x64) and larger memory usage leading to possible OOM for larger image sizes; training slower than MuDreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient than MuDreamer (encodes two views => ~3x FLOPs compared to MuDreamer in reported example) but can achieve competitive performance on clean tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On Visual Control Suite under natural background (1M steps) DreamerPro mean reported 445.2 (Table 2); on the six-task natural background subset MuDreamer outperforms DreamerPro (Figure 15).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Prototypical objectives can avoid reconstructing irrelevant pixels, but the two-view requirement increases compute and memory; DreamerPro performs well but may be less compute-efficient and generalize differently under unseen distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Avoids pixel-reconstruction but increases FLOPs and memory due to multi-view encoding; trade-off between representation quality and computational resource use.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>SwAV-like prototypical clustering across two augmented views, combined with RSSM latent dynamics and imagination-based actor-critic. Requires encoding two views of each observation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with MuDreamer: similar reconstruction-free objective but higher compute (two views) and somewhat lower robustness to natural backgrounds in some tasks; compared with TPC: DreamerPro generally achieves higher task performance but at higher compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1244.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1244.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Predictive Coding (TPC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reconstruction-free approach applying contrastive or predictive objectives to align past and future latent states to learn dynamics-aware representations for model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Temporal predictive coding for model-based planning in latent space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TPC (Temporal Predictive Coding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns representations by maximizing mutual information / contrastive alignment between past and future latent states (InfoNCE-style or similar), combined with RSSM dynamics for latent imagination and actor-critic training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with contrastive/predictive representation objective</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual control (DeepMind Control Suite) and related model-based RL tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive/predictive loss (InfoNCE or similar) between encoded past and future latent representations; predictive accuracy in latent space measured indirectly via downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In this paper TPC is reported to be faster (lower FLOPs) but with noticeably lower task performance on Visual Control Suite with natural backgrounds: mean 372.8 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Representations are learned to maximize mutual information across time; interpretability not emphasized—mostly black-box latent features with contrastive training.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed in this paper; typically uses embedding-space inspection and downstream performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported lower FLOPs vs prototypical multi-view methods and DreamerV3 because it does not require a decoder or multi-view encoding; Table 11: example FLOPs ~2.2B for 64x64 forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More compute-efficient than DreamerPro and DreamerV3 but underperforms them on several tasks; trade-off between compute savings and task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Visual Control Suite under natural background: mean 372.8 (Table 2), generally lower than MuDreamer and DreamerPro.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Predictive contrastive objectives can produce compact representations quickly, but may not capture sufficient task-relevant detail in presence of heavy visual distractions, leading to lower downstream control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Lower computational cost but at expense of task-level performance in environment with distracting visual backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Contrastive/predictive (InfoNCE-like) objective across time; no decoder and no two-view encoding; simpler and cheaper but requires careful design to capture task-relevant dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MuDreamer: TPC cheaper but less robust/accurate on natural-background tasks; compared to DreamerPro: less compute but lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1244.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1244.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early model-based RL approach that trains a pixel-space world model (autoencoder + autoregressive LSTM over discrete latent bits) and uses planning to train policies, demonstrated on Atari games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-based reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pixel-space world model using convolutional autoencoder to get discrete latent tokens and LSTM-based recurrent network predicting latent bits autoregressively; used to generate imagined trajectories to train a policy (e.g., PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pixel-space world model (autoregressive latent bits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (low-data regime)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Pixel reconstruction / next-frame prediction likelihood in pixel space; fidelity judged by ability to plan/train policies that generalize in Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically reported in this MuDreamer paper beyond being referenced as an early pixel-space world model for Atari planning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Pixel-space dynamics are interpretable visually via predicted frames; internal latent bits are discrete and somewhat interpretable but not strongly so.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Decoded frame visualization; autoregressive latent inspection in original SimPLe work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Pixel-space models are typically computationally expensive at the frame-prediction level; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned historically as effective but less sample-efficient than later latent-space world models for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrated that planning with pixel-space models can train agents in Atari, but such models require modeling high-dimensional visual detail which later latent approaches avoid.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity pixel modeling vs inefficiency and modeling unnecessary visual details.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Autoregressive prediction of discrete latent bits, pixel reconstruction objective, planning with PPO inside learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Precursor to latent-space RSSM methods; later methods (PlaNet/Dreamer) moved to learned latent representations for efficiency and better planning in high-dimensional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1244.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1244.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent approach proposing discrete autoencoders for imagining trajectories and using a transformer to predict discrete latent tokens for planning and imagination-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discrete autoencoder producing discrete latent tokens for frames, with an autoregressive transformer used to imagine latent-token sequences; used for planning/imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with discrete tokens and transformer predictor</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General model-based RL tasks including Atari (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Token-prediction accuracy and downstream policy performance; fidelity primarily assessed by task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper (only referenced among related work).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete tokens lend some interpretability to predicted sequences; decoder visualizations possible in original work but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformers over discrete tokens can be compute-intensive; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as an alternative family of discrete-token imagination models; no direct comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Listed as part of the spectrum of latent-world-model approaches, particularly for discrete-token imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete tokenization can simplify modeling but may lose fine-grained visual detail; tradeoffs are context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use discrete autoencoder + transformer predictor for imagining long-horizon token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Alternative to continuous or categorical RSSM latents; offers a different trade-off in expressivity vs structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuDreamer: Learning Predictive World Models without Reconstruction', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations <em>(Rating: 2)</em></li>
                <li>Temporal predictive coding for model-based planning in latent space <em>(Rating: 2)</em></li>
                <li>Model-based reinforcement learning for atari <em>(Rating: 1)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1244",
    "paper_id": "paper-270045288",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "MuDreamer",
            "name_full": "MuDreamer (this paper)",
            "brief_description": "A reconstruction-free latent world model built on DreamerV3 that learns predictive representations by predicting rewards, continuation flags, the value function, and previous actions (with an optional auxiliary decoder stop‑grads). It replaces pixel reconstruction with task-focused predictive losses and uses batch normalization to avoid representational collapse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MuDreamer",
            "model_description": "Latent-space Recurrent State Space Model (RSSM) with a CNN encoder; a GRU-based sequential network produces recurrent hidden state h_t; a representation network produces stochastic categorical latents z_t (one-hot categorical sampling) conditioned on h_t and encoder features x_t; a dynamics predictor predicts prior ẑ_t from h_t. From concatenated {h_t, z_t} the model predicts reward, continuation flag, Symlog-discretized λ-returns (value), and the previously selected action. An auxiliary decoder reconstruction head may be trained with a stop-gradient (so decoder gradients do not propagate into representation).",
            "model_type": "latent world model (RSSM with stochastic categorical latents)",
            "task_domain": "Continuous visual control (DeepMind Control / Visual Control Suite) and Atari100k (discrete-action Atari)",
            "fidelity_metric": "Prediction losses (negative log-likelihood) on reward, continuation, discretized Symlog λ-returns (value), and action-prediction loss; KL divergence between posterior q(z_t|h_t,x_t) and prior p(ẑ_t|h_t) (dynamics loss) and between q and stop-gradient prior (representation loss). Auxiliary reconstruction loss (log p(o_t|sg(s_t))) is optional and trained with stop-gradient so it does not guide representation.",
            "fidelity_performance": "No low-level MSE or pixel-wise fidelity metrics reported; fidelity is evaluated implicitly via predictive losses and task performance. Task-level results: Visual Control Suite (1M steps) mean score 784.7 and median 849.6 (Table 1); Natural background (background replaced by real-world videos) mean 517.0 (Table 2). Atari100k: reported comparable performance to DreamerV3 (no single aggregated numeric fidelity metric reported).",
            "interpretability_assessment": "Primarily a black-box neural latent model, but interpretable aspects via visualizations: decoder reconstructions (auxiliary decoder with stop-gradient), imagined latent trajectories visualized as decoded frames, and per-channel standard-deviation diagnostics over encoder/model outputs to detect collapse. Latents are not reported to map to semantically disentangled factors.",
            "interpretability_method": "Visualization of decoder reconstructions, visualization of imagined trajectories decoded from latent rollouts, monitoring per-channel std of l2-normalized encoder outputs and recurrent state; qualitative inspection (examples) of whether reconstructions keep agent vs background.",
            "computational_cost": "Model parameters: MuDreamer total ~15.3M trainable parameters (paper). FLOPs for a forward world-model training pass (64 time frames, 64x64 input) ~2.5 billion (Table 11). Training speed reported: ~14 hours to reach 1M environment steps on a single NVIDIA RTX 3090 + 16 CPU cores (Walker-run task); Atari Breakout: 4h40 to reach 400K steps (single RTX 3090, 16 cores).",
            "efficiency_comparison": "Fewer FLOPs and slightly faster training than DreamerV3 and substantially less compute than reconstruction-free methods that encode two augmented views (e.g., DreamerPro). Table 11 compares forward FLOPs: MuDreamer 2.5B vs DreamerV3 4.3B and DreamerPro ~7.9B (64x64). Benefit arises because MuDreamer removes decoder gradients and avoids encoding multiple augmented views.",
            "task_performance": "Visual Control Suite (1M env steps): mean 784.7, median 849.6 (Table 1). Natural background (1M env steps): mean 517.0 (Table 2), outperforming DreamerPro and TPC under visual distractions. Atari100k: comparable to DreamerV3 (no exact consolidated score given here but reported as comparable in text and Table 4).",
            "task_utility_analysis": "MuDreamer trades pixel-level reconstruction fidelity for task-focused predictive fidelity: by predicting rewards, continuation, value, and actions it learns representations prioritizing task-relevant elements (e.g., agent body, small objects) and is more robust to task-irrelevant visual distractions (natural video backgrounds). The paper shows that higher pixel reconstruction fidelity (DreamerV3) can lead to modeling unnecessary background detail and failure to represent small but task-relevant objects.",
            "tradeoffs_observed": "Removing reconstruction reduces modeling of unnecessary visual detail and improves robustness and compute-efficiency, but requires architectural and objective changes to prevent collapse (batch normalization in representation network, action & value predictors). KL balancing is delicate: too strong posterior regularization (β_rep high) limits latent information and slows learning; β_rep=0 leads to instabilities; paper settles on β_pred=1.0, β_dyn=0.95, β_rep=0.05. Action/value heads increase representational usefulness but add prediction losses.",
            "design_choices": "Predictive-only objective: reward/continue/value/action prediction plus dynamics KL losses; stop-gradient auxiliary decoder optional (does not affect training); categorical stochastic latents with 32 latents × 32 classes; batch normalization inside representation network to prevent feature collapse; KL balancing (β_dyn=0.95, β_rep=0.05); imagination horizon H=15; Symlog-discretized λ-returns for value training; actor-critic learned in latent space.",
            "comparison_to_alternatives": "Compared to DreamerV3 (reconstruction-based): MuDreamer has fewer parameters (15.3M vs 17.9M), substantially lower FLOPs, slightly faster training, and better robustness to visual distractions while achieving comparable or better control performance on many tasks. Compared to DreamerPro (reconstruction-free prototypical two-view method): MuDreamer attains similar or better performance without encoding two augmented views (hence lower FLOPs and memory). Compared to TPC (contrastive temporal predictive coding): TPC is faster but has lower task performance. Compared to MuZero-inspired approaches, MuDreamer borrows value-prediction ideas but operates without MCTS and works in continuous action spaces.",
            "optimal_configuration": "Paper recommendations/findings: include both action and value prediction heads (removing them degrades performance); use batch normalization in the representation network to avoid collapse; set KL balancing to small but nonzero β_rep (paper uses 0.05) and β_dyn near 0.95 (default DreamerV3 settings slowed convergence); optional stop-gradient on reconstruction avoids decoder influencing representations. These settings were empirically identified as best trade-offs for stability, fidelity to task-relevant features, and computational efficiency.",
            "uuid": "e1244.0",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3",
            "brief_description": "A latent-space model-based RL agent that learns a Recurrent State Space Model with a pixel reconstruction loss and trains actor-critic purely in imagination; uses normalization and symlog outputs to scale across domains.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "use",
            "model_name": "DreamerV3",
            "model_description": "Latent RSSM with CNN encoder and decoder autoencoder; sequential GRU-based prior/posterior dynamics with stochastic latents (originally Gaussian/categorical variants across versions); trained with pixel reconstruction loss + predictions for reward, continuation, and next latent; actor and critic trained on imagined latent rollouts. Uses layer normalization, symlog for reward/value scaling, and KL-balancing between posterior and prior.",
            "model_type": "latent world model (RSSM with pixel reconstruction)",
            "task_domain": "Diverse domains: DeepMind Control Suite (visual control), Atari, Minecraft and others (as claimed by DreamerV3)",
            "fidelity_metric": "Pixel reconstruction loss (autoencoder reconstruction), predictive negative log-likelihood for reward/continue/value, KL divergence between posterior and prior (KL-balancing). Value trained on discretized Symlog λ-returns.",
            "fidelity_performance": "No reported low-level numeric fidelity multipliers like MSE; reported task performance on Visual Control Suite (1M steps) mean score 739.6, median 808.5 (Table 1). Under natural background distractions DreamerV3 often reconstructs background details and may lose small task elements (qualitative).",
            "interpretability_assessment": "Black-box latent model but decoder reconstruction provides a way to inspect what the latent encodes; paper reports that DreamerV3 reconstructions often preserve background detail at expense of small task-critical items (object vanishing phenomenon).",
            "interpretability_method": "Decoder reconstructions, qualitative comparison of reconstructions under natural background setting, imagined trajectory decoding.",
            "computational_cost": "Parameters reported ~17.9M for the DreamerV3 configuration used in comparisons. Forward FLOPs (world-model training forward) ~4.3B for 64x64 example. Training time reported ~15 hours to 1M env steps on same hardware (RTX 3090 + 16 CPUs) for Walker-Run.",
            "efficiency_comparison": "Higher FLOPs and slightly slower training than MuDreamer due to decoder/backprop to encoder and possibly other architectural choices. Reconstruction and use of autoencoder increases compute and memory compared to reconstruction-free alternatives that encode single view only.",
            "task_performance": "Visual Control Suite (1M steps): mean 739.6, median 808.5 (Table 1). On natural-background tasks DreamerV3 performs worse than MuDreamer and DreamerPro in many cases due to focus on background details.",
            "task_utility_analysis": "High pixel reconstruction fidelity does not necessarily translate to task utility: DreamerV3 sometimes models background detail and neglects small but critical task objects, leading to degraded policy performance under visual distractions. Reconstruction enforces encoding of all visual information which can be detrimental when many features are irrelevant.",
            "tradeoffs_observed": "Pixel-reconstruction enforces full-image fidelity (useful in clean settings) at the cost of modeling irrelevant detail and increased compute; KL-balancing hyperparameters can slow convergence if set suboptimally (authors note default DreamerV3 KL balancing slowed learning in MuDreamer experiments).",
            "design_choices": "Pixel reconstruction loss with autoencoder, layer normalization, symlog outputs for reward/value, KL balancing of prior/posterior (DreamerV3 defaults differ from MuDreamer tuned values), training actor-critic in latent imagination.",
            "comparison_to_alternatives": "Against MuDreamer, DreamerV3 achieves strong baseline performance in many tasks but is less robust to visual distractions and uses more compute. Against reconstruction-free methods (DreamerPro, TPC), DreamerV3's reconstruction objective can be a liability in settings with task-irrelevant visual variability.",
            "optimal_configuration": "Paper suggests DreamerV3 hyperparameters work well across diverse domains but warns that KL-balancing parameters should be tuned per objective; MuDreamer experiments indicate reducing posterior regularization (different β values) can improve learning speed/quality when not using reconstruction.",
            "uuid": "e1244.1",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero",
            "brief_description": "A model-based RL algorithm that learns a compact model predicting rewards, value and policy and uses MCTS (Monte-Carlo Tree Search) over the learned model for planning, enabling strong performance in discrete precision planning domains without reconstructing observations.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero",
            "model_description": "Learns representation, dynamics and prediction networks that predict value, reward and policy logits for hypothetical steps; used with MCTS to perform planning using learned model predictions rather than reconstructing observations.",
            "model_type": "latent world model combined with tree search (MCTS)",
            "task_domain": "Discrete planning domains (Chess, Shogi, Go) and was extended to Atari; excels in discrete action spaces with precision planning.",
            "fidelity_metric": "Matches TD targets on predicted value and predicted rewards across unrolled steps; policy targets derived from MCTS visit distributions. Fidelity measured as error in predicted rewards/values and alignment with MCTS outcomes.",
            "fidelity_performance": "Not reported in this paper beyond being cited as inspiration; MuZero is known to achieve superhuman performance in planning benchmarks (original MuZero results reported in the cited paper).",
            "interpretability_assessment": "Model is a learned neural predictor plus MCTS; interpretability is limited; planning traces (MCTS rollouts) provide some insight into decision reasoning.",
            "interpretability_method": "Analysis typically via MCTS tree visualization and inspecting predicted reward/value estimates; not elaborated in this paper.",
            "computational_cost": "MCTS is computationally expensive and scales poorly to high-dimensional continuous action spaces (noted in this paper).",
            "efficiency_comparison": "Highly effective in discrete domains but not readily applicable to high-dimensional continuous action problems without discretization; compared qualitatively in the paper as inspiration for predictive objectives.",
            "task_performance": "Not evaluated here; referenced as state-of-the-art in discrete planning domains in original work.",
            "task_utility_analysis": "MuZero's predictive losses (value/reward/policy) inspire MuDreamer's choice to predict task-relevant quantities rather than pixel reconstructions; MuZero's planning utility does not directly transfer to continuous-action visual control tasks.",
            "tradeoffs_observed": "Strong planning performance vs high compute and poor scaling to continuous actions; motivates MuDreamer's different design choices (no MCTS, predictive objectives that work with continuous actions).",
            "design_choices": "Predict value, reward and policy logits; use MCTS on learned model; train on TD value/r reward/policy targets.",
            "comparison_to_alternatives": "MuZero excels in discrete planning but is less applicable to continuous high-dimensional visual control, whereas Dreamer-family methods operate directly in latent space and handle continuous actions.",
            "uuid": "e1244.2",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DreamerPro",
            "name_full": "DreamerPro (reconstruction-free prototypical representations)",
            "brief_description": "A reconstruction-free variant of Dreamer that uses prototypical/self-supervised representation learning (SwAV-like) requiring two augmented views of images to compute a clustering loss, aiming to avoid pixel reconstruction while learning useful latent dynamics.",
            "citation_title": "Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations",
            "mention_or_use": "use",
            "model_name": "DreamerPro",
            "model_description": "Reconstruction-free Dreamer variant that encodes two augmented views of the same image and enforces consistent prototypical assignments (SwAV-style) across views to learn latent representations, combined with RSSM latent dynamics for imagination-based actor-critic training.",
            "model_type": "latent world model with prototypical/self-supervised representation objective",
            "task_domain": "Visual control (DeepMind Control), similar benchmarks as Dreamer family",
            "fidelity_metric": "Self-supervised prototypical clustering consistency (SwAV objective) across augmented views, plus predictive losses for reward/value; fidelity judged via downstream task performance.",
            "fidelity_performance": "Reported in this paper as a strong reconstruction-free baseline: in natural background experiments DreamerPro mean 445.2 (Table 2) on the Visual Control Suite; in some configurations DreamerPro competitive with MuDreamer but requires encoding two views and thus higher compute.",
            "interpretability_assessment": "Latent representations learned via prototypical clustering; interpretability limited but representations aim for discriminative clusters rather than pixel-perfect reconstructions.",
            "interpretability_method": "Not deeply detailed in this paper; referenced methods typically inspect cluster assignments or downstream task performance.",
            "computational_cost": "Higher compute and memory due to encoding two augmented views per observation: Table 11 shows forward FLOPs ~7.9B (64x64) and larger memory usage leading to possible OOM for larger image sizes; training slower than MuDreamer.",
            "efficiency_comparison": "Less efficient than MuDreamer (encodes two views =&gt; ~3x FLOPs compared to MuDreamer in reported example) but can achieve competitive performance on clean tasks.",
            "task_performance": "On Visual Control Suite under natural background (1M steps) DreamerPro mean reported 445.2 (Table 2); on the six-task natural background subset MuDreamer outperforms DreamerPro (Figure 15).",
            "task_utility_analysis": "Prototypical objectives can avoid reconstructing irrelevant pixels, but the two-view requirement increases compute and memory; DreamerPro performs well but may be less compute-efficient and generalize differently under unseen distractions.",
            "tradeoffs_observed": "Avoids pixel-reconstruction but increases FLOPs and memory due to multi-view encoding; trade-off between representation quality and computational resource use.",
            "design_choices": "SwAV-like prototypical clustering across two augmented views, combined with RSSM latent dynamics and imagination-based actor-critic. Requires encoding two views of each observation.",
            "comparison_to_alternatives": "Compared with MuDreamer: similar reconstruction-free objective but higher compute (two views) and somewhat lower robustness to natural backgrounds in some tasks; compared with TPC: DreamerPro generally achieves higher task performance but at higher compute cost.",
            "uuid": "e1244.3",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "TPC",
            "name_full": "Temporal Predictive Coding (TPC)",
            "brief_description": "A reconstruction-free approach applying contrastive or predictive objectives to align past and future latent states to learn dynamics-aware representations for model-based RL.",
            "citation_title": "Temporal predictive coding for model-based planning in latent space",
            "mention_or_use": "use",
            "model_name": "TPC (Temporal Predictive Coding)",
            "model_description": "Learns representations by maximizing mutual information / contrastive alignment between past and future latent states (InfoNCE-style or similar), combined with RSSM dynamics for latent imagination and actor-critic training.",
            "model_type": "latent world model with contrastive/predictive representation objective",
            "task_domain": "Visual control (DeepMind Control Suite) and related model-based RL tasks",
            "fidelity_metric": "Contrastive/predictive loss (InfoNCE or similar) between encoded past and future latent representations; predictive accuracy in latent space measured indirectly via downstream task performance.",
            "fidelity_performance": "In this paper TPC is reported to be faster (lower FLOPs) but with noticeably lower task performance on Visual Control Suite with natural backgrounds: mean 372.8 (Table 2).",
            "interpretability_assessment": "Representations are learned to maximize mutual information across time; interpretability not emphasized—mostly black-box latent features with contrastive training.",
            "interpretability_method": "Not detailed in this paper; typically uses embedding-space inspection and downstream performance.",
            "computational_cost": "Reported lower FLOPs vs prototypical multi-view methods and DreamerV3 because it does not require a decoder or multi-view encoding; Table 11: example FLOPs ~2.2B for 64x64 forward pass.",
            "efficiency_comparison": "More compute-efficient than DreamerPro and DreamerV3 but underperforms them on several tasks; trade-off between compute savings and task performance.",
            "task_performance": "Visual Control Suite under natural background: mean 372.8 (Table 2), generally lower than MuDreamer and DreamerPro.",
            "task_utility_analysis": "Predictive contrastive objectives can produce compact representations quickly, but may not capture sufficient task-relevant detail in presence of heavy visual distractions, leading to lower downstream control performance.",
            "tradeoffs_observed": "Lower computational cost but at expense of task-level performance in environment with distracting visual backgrounds.",
            "design_choices": "Contrastive/predictive (InfoNCE-like) objective across time; no decoder and no two-view encoding; simpler and cheaper but requires careful design to capture task-relevant dynamics.",
            "comparison_to_alternatives": "Compared to MuDreamer: TPC cheaper but less robust/accurate on natural-background tasks; compared to DreamerPro: less compute but lower performance.",
            "uuid": "e1244.4",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SimPLe",
            "name_full": "SimPLe",
            "brief_description": "An early model-based RL approach that trains a pixel-space world model (autoencoder + autoregressive LSTM over discrete latent bits) and uses planning to train policies, demonstrated on Atari games.",
            "citation_title": "Model-based reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe",
            "model_description": "Pixel-space world model using convolutional autoencoder to get discrete latent tokens and LSTM-based recurrent network predicting latent bits autoregressively; used to generate imagined trajectories to train a policy (e.g., PPO).",
            "model_type": "pixel-space world model (autoregressive latent bits)",
            "task_domain": "Atari games (low-data regime)",
            "fidelity_metric": "Pixel reconstruction / next-frame prediction likelihood in pixel space; fidelity judged by ability to plan/train policies that generalize in Atari.",
            "fidelity_performance": "Not numerically reported in this MuDreamer paper beyond being referenced as an early pixel-space world model for Atari planning.",
            "interpretability_assessment": "Pixel-space dynamics are interpretable visually via predicted frames; internal latent bits are discrete and somewhat interpretable but not strongly so.",
            "interpretability_method": "Decoded frame visualization; autoregressive latent inspection in original SimPLe work (not detailed here).",
            "computational_cost": "Pixel-space models are typically computationally expensive at the frame-prediction level; specifics not provided in this paper.",
            "efficiency_comparison": "Mentioned historically as effective but less sample-efficient than later latent-space world models for many tasks.",
            "task_performance": "Not provided here.",
            "task_utility_analysis": "Demonstrated that planning with pixel-space models can train agents in Atari, but such models require modeling high-dimensional visual detail which later latent approaches avoid.",
            "tradeoffs_observed": "High-fidelity pixel modeling vs inefficiency and modeling unnecessary visual details.",
            "design_choices": "Autoregressive prediction of discrete latent bits, pixel reconstruction objective, planning with PPO inside learned model.",
            "comparison_to_alternatives": "Precursor to latent-space RSSM methods; later methods (PlaNet/Dreamer) moved to learned latent representations for efficiency and better planning in high-dimensional tasks.",
            "uuid": "e1244.5",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "IRIS",
            "name_full": "IRIS",
            "brief_description": "A recent approach proposing discrete autoencoders for imagining trajectories and using a transformer to predict discrete latent tokens for planning and imagination-based RL.",
            "citation_title": "IRIS",
            "mention_or_use": "mention",
            "model_name": "IRIS",
            "model_description": "Discrete autoencoder producing discrete latent tokens for frames, with an autoregressive transformer used to imagine latent-token sequences; used for planning/imagined rollouts.",
            "model_type": "latent world model with discrete tokens and transformer predictor",
            "task_domain": "General model-based RL tasks including Atari (as cited)",
            "fidelity_metric": "Token-prediction accuracy and downstream policy performance; fidelity primarily assessed by task performance.",
            "fidelity_performance": "Not quantified in this paper (only referenced among related work).",
            "interpretability_assessment": "Discrete tokens lend some interpretability to predicted sequences; decoder visualizations possible in original work but not detailed here.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Transformers over discrete tokens can be compute-intensive; specifics not provided here.",
            "efficiency_comparison": "Mentioned as an alternative family of discrete-token imagination models; no direct comparison provided in this paper.",
            "task_performance": "Not provided here.",
            "task_utility_analysis": "Listed as part of the spectrum of latent-world-model approaches, particularly for discrete-token imagination.",
            "tradeoffs_observed": "Discrete tokenization can simplify modeling but may lose fine-grained visual detail; tradeoffs are context-dependent.",
            "design_choices": "Use discrete autoencoder + transformer predictor for imagining long-horizon token sequences.",
            "comparison_to_alternatives": "Alternative to continuous or categorical RSSM latents; offers a different trade-off in expressivity vs structure.",
            "uuid": "e1244.6",
            "source_info": {
                "paper_title": "MuDreamer: Learning Predictive World Models without Reconstruction",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations",
            "rating": 2,
            "sanitized_title": "dreamerpro_reconstructionfree_modelbased_reinforcement_learning_with_prototypical_representations"
        },
        {
            "paper_title": "Temporal predictive coding for model-based planning in latent space",
            "rating": 2,
            "sanitized_title": "temporal_predictive_coding_for_modelbased_planning_in_latent_space"
        },
        {
            "paper_title": "Model-based reinforcement learning for atari",
            "rating": 1,
            "sanitized_title": "modelbased_reinforcement_learning_for_atari"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 1,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        }
    ],
    "cost": 0.021360749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MuDreamer: Learning Predictive World Models without Reconstruction
23 May 2024</p>
<p>Maxime Burchi maxime.burchi@uni-wuerzburg.de 
Computer Vision Lab
CAIDAS &amp; IFI
University of Würzburg
Germany</p>
<p>Radu Timofte 
Computer Vision Lab
CAIDAS &amp; IFI
University of Würzburg
Germany</p>
<p>MuDreamer: Learning Predictive World Models without Reconstruction
23 May 20240561808F46CADF599668F5CDC9D20E17arXiv:2405.15083v1[cs.AI]
The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using a pixel reconstruction loss.However, while the reconstruction loss is essential to Dreamer's performance, it also necessitates modeling unnecessary information.Consequently, Dreamer sometimes fails to perceive crucial elements which are necessary for task-solving when visual distractions are present in the observation, significantly limiting its potential.In this paper, we present MuDreamer, a robust reinforcement learning agent that builds upon the DreamerV3 algorithm by learning a predictive world model without the need for reconstructing input signals.Rather than relying on pixel reconstruction, hidden representations are instead learned by predicting the environment value function and previously selected actions.Similar to predictive self-supervised methods for images, we find that the use of batch normalization is crucial to prevent learning collapse.We also study the effect of KL balancing between model posterior and prior losses on convergence speed and learning stability.We evaluate MuDreamer on the commonly used DeepMind Visual Control Suite and demonstrate stronger robustness to visual distractions compared to DreamerV3 and other reconstruction-free approaches, replacing the environment background with task-irrelevant real-world videos.Our method also achieves comparable performance on the Atari100k benchmark while benefiting from faster training.</p>
<p>Introduction</p>
<p>Deep reinforcement learning has achieved great success in recent years, solving complex tasks in diverse domains.Researchers made significant progress applying advances in deep learning for learning feature representations [41].The use of deep neural networks as function approximations made it possible to train powerful agents directly from high-dimensional observations like images, achieving human to superhuman performance in challenging and visually complex domains like Atari games [42,29], visual control [38,6], the game of Go [53,49], StarCraft II [58] and even Minecraft [4,25].However, these approaches generally require a large amount of environment interactions [56] or behavior cloning pretraining [51] to achieve strong performance.</p>
<p>To address this issue, concurrent works have chosen to focus on model-based approaches [52,61], aiming to enhance the agent performance while reducing the number of necessary interactions with the environment.Reinforcement learning algorithms are typically categorized into two main groups: model-free algorithms, which directly learn value and/or policy functions through interaction with the environment, and model-based algorithms, which learn a model of the world.World models [55,20] summarize an agent's experience into a predictive model that can be used in place of the real environment to learn complex behaviors.This enables the agent to simulate multiple plausible trajectories in parallel, which not only enhances generalization but also improves sample efficiency.</p>
<p>Recent works have shown that model-based agents can effectively be trained from images, leading to enhanced performance and sample efficiency compared to model-free approaches [22,23,33,49,24,66,40].The DreamerV3 agent [25] recently demonstrated state-of-the-art performance across diverse domains, learning powerful world models in latent space using a pixel reconstruction loss.The agent solves long-horizon tasks from image inputs with both continuous and discrete action spaces.However, while the reconstruction loss is essential for Dreamer's performance, it also necessitates modeling unnecessary information [44,39,43,47,15,8].Consequently, Dreamer sometimes fails to perceive crucial elements which are necessary for task-solving, significantly limiting its potential.</p>
<p>In this paper, we present MuDreamer, a robust reinforcement learning agent that builds upon the DreamerV3 [25] algorithm by learning a predictive world model without the necessity of reconstructing input signals.Taking inspiration from the MuZero [49] agent, MuDreamer learns a world model in latent space by predicting the environment rewards, continuation flags and value function, focusing on information relevant to the task.We also propose to incorporate an action prediction branch to predict the sequence of selected actions from the observed data.This additional task trains the world model to associate actions with environment changes and proves particularly beneficial for learning hidden representations in scenarios where environment rewards are extremely sparse.Similar to predictive self-supervised methods used for image data, we find that the use of batch normalization is crucial to prevent learning collapse in which the model produces constant or non-informative hidden states.Following Paster et al. [47], we solve this issue by introducing batch normalization inside the model representation network.We also study the effect of KL balancing between model posterior and prior losses on convergence speed and learning stability.We evaluate MuDreamer on the commonly used DeepMind Visual Control Suite [56] and demonstrate stronger robustness to visual distractions compared to DreamerV3 and other reconstruction-free approaches [43,15].MuDreamer learns to distinguish relevant details from unnecessary information when replacing the environment background with task-irrelevant real-world videos while DreamerV3 focuses on background details irrelevant to the task.Our method also achieves comparable performance on the Atari100k benchmark while benefiting from faster training.</p>
<p>2 Related Works</p>
<p>Model-based Reinforcement Learning</p>
<p>In recent years, there has been a growing interest in using neural networks as world models to simulate environments and train reinforcement learning agents from hypothetical trajectories.Initial research primarily focused on proprioceptive tasks [18,52,28,60,59], involving simple, lowdimensional environments.However, more recent efforts have shifted towards learning world models for environments with high-dimensional observations like images [33,22,23,49,66,40].For example, SimPLe [33] successfully demonstrated planning in Atari games by training a world model in pixel space and utilizing it to train a Proximal Policy Optimization (PPO) agent [50].This approach involves a convolutional autoencoder for generating discrete latent variables and an LSTM-based recurrent network predicting latent bits autoregressively.PlaNet [22] proposed to learn a Recurrent State-Space Model (RSSM) in latent space using a pixel reconstruction loss, planning using model predictive control.Dreamer [23,24,25] extended PlaNet by incorporating actor and critic networks trained from simulated trajectories and imaginary rewards.MuZero [49] took a different approach, focusing on learning a model of the environment by predicting quantities crucial for planning, such as reward, action-selection policy, and value function.This approach allowed MuZero to excel in reinforcement learning tasks without relying on the reconstruction of input observations.EfficientZero [66] improved upon MuZero's sample efficiency by incorporating a representation learning objective [12] to achieve better performance with limited data.Lastly, IRIS [40] proposed a discrete autoencoder [57] model for imagining trajectories, predicting discrete latent tokens in an autoregressive manner, using a transformer model.</p>
<p>Self-Supervised Representation Learning for Images</p>
<p>Self-Supervised Learning (SSL) of image representations has attracted significant research attention in recent years for its ability to learn hidden representations from large scale unlabelled datasets.Reconstruction-based approaches proposed to learn hidden representations by reconstructing a corrupted version of the input image [27,64,17].Many works focused on pixel space reconstruction while other proposed to predict hand-designed features like Histograms of Oriented Gradients (HOG) [62].Contrastive approaches proposed to learn hidden representations using joint embedding architectures where output features of a sample and its distorted version are bought close to each other, while negative samples and their distortions are pushed away [31,46,26,11].These methods are commonly applied to Siamese architectures, where two identical networks are trained together, sharing parameters.In contrast, SwAV [9] proposed a different approach by ensuring consistency between cluster assignments produced for different augmentations of the same image, rather than directly comparing features.Predictive approaches proposed to predict the hidden representation of a similar view of the input signal without relying on negative samples [19,10,16,12,67,3,1].These methods prevent learning collapse using various architectural tricks such as knowledge distillation [30], normalizing output representations, or the application of additional constraints to output representations like VICReg [5].</p>
<p>Reconstruction-Free Dreamer</p>
<p>Following advances in the area of self-supervised representation learning for image data, several works proposed to apply reconstruction-free representation learning techniques to Dreamer.The original Dreamer paper [23] initially experimented with contrastive learning [46] to learn representations having maximal mutual information with the encoded observation but found that it did not match the performance of reconstruction-based representations.Subsequently, several works proposed Dreamer variants using contrastive learning [44,39,43,45,8], successfully competing with Dreamer on several tasks of the Visual Control Suite.Dreaming [44] proposed to use a multi-step InfoNCE loss to sequentially predict future time steps representations of augmented views.Temporal Predictive Coding (TPC) [43] followed a similar approach using contrastive learning to maximize the mutual information between past and the future latent states.DreamerPro [15] proposed to encourage uniform cluster assignment across batches of samples, implicitly pushing apart embeddings of different observations.Concurrently, BLAST [47] proposed to learn hidden representation using a slow-moving teacher network to generate target embeddings [19].BLAST also demonstrated that batch normalization was critical to the agent performance.In this paper, we present a reconstructionfree variant of DreamerV3 achieving comparable performance without using negatives samples, separate augmented views of images, or an additional slow-moving teacher encoder network.</p>
<p>Background</p>
<p>Dreamer</p>
<p>Our method is built on the DreamerV3 [25] algorithm which we refer to as Dreamer throughout the paper.Dreamer [23] is an actor-critic model-based reinforcement learning algorithm learning a powerful predictive world model from past experience in latent space using a replay buffer.The world model is learned from self-supervised learning by predicting the environment reward, episode continuation and next latent state given previously selected actions.The algorithm also uses a pixel reconstruction loss using an autoencoder architecture such that all information about the observations must pass through the model hidden state.The actor and critic neural networks learn behaviors purely from abstract sequences predicted by the world model.The model generates simulated trajectories from replayed experience states using the actor network to sample actions.The value network is trained to predict the sum of future reward while the actor network is trained to maximize the expected sum of future reward from the value network.</p>
<p>DreamerV2 [24] applied Dreamer to Atari games, utilizing categorical latent states with straightthrough gradients [7] in the world model to improve performance, instead of Gaussian latents with reparameterized gradients [35].It also introduced KL balancing, separately scaling the prior cross entropy and the posterior entropy in the KL loss to encourage learning an accurate temporal prior.</p>
<p>DreamerV3 [25] mastered diverse domains using the same hyper-parameters with a set of architectural changes to stabilize learning across tasks.The agent uses Symlog predictions for the reward and value function to address the scale variance across domains.The networks also employ layer normalization [2] to improve robustness and performance while scaling up to larger model sizes.It regularizes the policy by normalizing the returns and value function using an Exponential Moving Average (EMA) of the returns percentiles.Using these modifications, the agent solves Atari games and DeepMind Control tasks while collecting diamonds in Minecraft.</p>
<p>MuZero</p>
<p>MuZero [49] is a model-based algorithm combining Monte-Carlo Tree Search (MCTS) [14] with a world model to achieve superhuman performance in precision planning tasks such as Chess, Shogi and Go.The model is learned by being unrolled recurrently for K steps and predicting environment quantities relevant to planning.All parameters of the model are trained jointly to accurately match the TD value [54] and reward, for every hypothetical step k.The MCTS algorithm uses the learned model to simulate environment trajectories and output an action visit distribution over the root node.This potentially better policy compared to the neural network one is used to train the policy network.MuZero excels in discrete action domains but struggles with high-dimensional continuous action spaces, where a discretization of possible actions is required to apply MCTS [66].Our proposed method, MuDreamer, draws inspiration from MuZero to learn a world model by predicting the expected sum of future rewards.MuDreamer solves tasks from pixels in both continuous and discrete action space, without the need for a reconstruction loss.</p>
<p>MuDreamer</p>
<p>We present MuDreamer, a reconstruction-free variant of the Dreamer algorithm, which learns a world model in latent space by predicting not only rewards and continuation flags but also the environment value function and previously selected actions.Figure 1 illustrates the learning process of the MuDreamer world model.Similar to Dreamer, MuDreamer comprises three neural networks: a world model, a critic network, and an actor network.These three networks are trained concurrently using an experience replay buffer that collects past experiences.This section provides an overview of the world model and the modifications applied to the Dreamer agent.We also detail the learning process of the critic and actor networks.T is sampled from the replay buffer.The sequence is mapped to hidden representations x 1:T using a CNN encoder.At each step, the RSSM computes a posterior state z t representing the current observation o t and a prior state ẑt that predict the posterior without having access to the current observation.Unlike Dreamer, the decoder gradients are not back-propagated to the rest of the model.The hidden representations are learned solely using value, reward, episode continuation and action prediction heads.</p>
<p>World Model Learning</p>
<p>Following DreamerV3, we learn a world model in latent space, using a Convolutional Neural Network (CNN) [37] encoder to map high-dimensional visual observations o t to hidden representations x t .The world model is implemented as a Recurrent State-Space Model (RSSM) [22] composed of three sub networks: A sequential network using a Gated Recurrent Unit (GRU) [13] to predict the next hidden state h t given past action a t−1 .A representation network predicting the current stochastic state z t using both h t and encoded features x t .And a dynamics network predicting the stochastic state z t given the current recurrent state h t .The concatenation of h t and z t forms the model hidden state s t = {h t , z t } from which we predict environment rewards r t , episode continuation c t ∈ {0, 1} and value function v t .We also learn an action predictor network using encoded features x t and preceding model hidden state s t−1 to predict the action which led to the observed environment change.</p>
<p>The trainable world model components are the following:</p>
<p>Encoder:
x t = enc ϕ (o t ) RSSM Sequential Network: h t = f ϕ (h t−1 , z t−1 , a t−1 ) Representation Network: z t ∼ q ϕ (z t | h t , x t ) Dynamics Predictor: ẑt ∼ p ϕ (ẑ t | h t ) Reward Predictor: rt ∼ p ϕ (r t | s t ) Continue Predictor: ĉt ∼ p ϕ (ĉ t | s t ) Value Predictor: vt ∼ p ϕ (v t | s t ) Action Predictor: ât−1 ∼ p ϕ (â t−1 | x t , s t−1 ) Decoder (auxiliary): ôt ∼ p ϕ (ô t | sg(s t ))(1)
Given a sequence batch of inputs x 1:T , actions a 1:T , rewards r 1:T , and continuation flags c 1:T , the world model parameters (ϕ) are optimized end-to-end to minimize a prediction loss L pred , dynamics loss L dyn , and representation loss L rep with corresponding loss weights β pred = 1.0, β dyn = 0.95 and β rep = 0.05.The loss function for learning the world model is:
L model (ϕ) = E q ϕ T t=1 (β pred L pred (ϕ) + β dyn L dyn (ϕ) + β rep L rep (ϕ))(2)
The prediction loss trains the reward, continue, value and action predictors to learn hidden representations.We optionally learn an auxiliary decoder network to reconstruct the sequence of observations using the stop gradient operator sg(.) to prevent the gradients from being back-propagated to other network parameters.The auxiliary decoder reconstruction loss has no effect on MuDreamer training and the decoder is excluded from total number of parameters.The dynamics loss trains the dynamics predictor to predict the next representation by minimizing the KL divergence between the predictor p ϕ (ẑ t | h t ) and the next stochastic representation q ϕ (z t | h t , x t ).While the representation loss trains the representations to become more predictable if the dynamics cannot predict their distribution:
L pred (ϕ) = − log p ϕ (r t | s t ) reward predictor loss − log p ϕ (c t | s t ) continue predictor loss − log p ϕ (R λ t | s t ) value predictor loss − log p ϕ (a t−1 | x t , s t−1 )
action predictor loss
− log p ϕ (o t | sg(s t ))
reconstruction loss (auxiliary)</p>
<p>(3)
L dyn (ϕ) = max(1, KL[ sg(q ϕ (z t | h t , x t )) || p ϕ (ẑ t | h t ) ]) L rep (ϕ) = max(1, KL[ q ϕ (z t | h t , x t ) || sg(p ϕ (ẑ t | h t )) ])(4)
Value Predictor We use a value network to predict the environment value function from the model hidden state.The value function aims to represent the expected λ-returns [54] with λ set 0.95, using a slow moving target value predictor v ϕ ′ with EMA momentum τ = 0.01:
R λ t = r t+1 + γc t+1 (1 − λ)v ϕ ′ (s t+1 ) + λR λ t+1 R λ T = v ϕ ′ (s T )(5)
Following previous works [49,25], we set the discount factor γ to 0.997 and use a discrete regression of λ-returns to let the critic maintain and refine a distribution over potential returns.The λ-return targets are first transformed using the Symlog function and discretized using a twohot encoding.Given twohot-encoded target returns y t = sg(twohot(symlog(R λ t ))) the value predictor minimizes the categorical cross entropy loss with the predicted value logits.</p>
<p>Action Predictor</p>
<p>The action predictor network learns to identify the previously selected actions for each time step of the sampled trajectory.It shares the same architecture as the actor network but takes the current encoded features x t and preceding model hidden state s t−1 as inputs.Since the current model hidden state s t already depends of the action target a t .</p>
<p>Batch Normalization</p>
<p>In order to prevent learning collapse to constant or non-informative model states, like observed in predictive self-supervised learning for image data [19,11], we apply batch normalization [32] to MuDreamer.Following BLAST [47], we replace the hidden normalization layer inside the representation network by a batch normalization layer.We find that it is sufficient to prevent collapse on all tasks and stabilize learning.</p>
<p>Behavior Learning</p>
<p>Following DreamerV3 [25], MuDreamer policy and value functions are learned by imagining trajectories using the world model (Figure 2).Actor-critic learning takes place entirely in latent space, allowing the agent to use a large batch size of imagined trajectories.To do so, the model hidden states of the sampled sequence are flattened along batch and time dimensions.The world model imagines H = 15 steps into the future using the sequential and dynamics networks, selecting actions from the actor.The actor and critic use parameter vectors (θ) and (ψ), respectively:</p>
<p>Actor:
a t ∼ π θ (a t |ŝ t ) Critic: v ψ (ŝ t ) ≈ E p ϕ π θ <a href="6"> Rλ t </a>
Critic Learning Similarly to the value predictor branch, the critic is trained by predicting the discretized λ-returns, but using rewards predictions imagined by the world model:
Rλ t = rt+1 + γĉ t+1 (1 − λ)v ψ (ŝ t+1 ) + λ Rλ t+1 Rλ H+1 = v ψ (ŝ H+1 )(7)
The critic also does not use a target network but relies on its own predictions for estimating rewards beyond the prediction horizon.This requires stabilizing the critic by adding a regularizing term of the predicted values toward the outputs of its own EMA network:
L critic (ψ) = H t=1 − log p ψ ( Rλ t | ŝt ) discrete returns regression − log p ψ (v ψ ′ (ŝ t ) | ŝt ) critic EMA regularizer (8)
Actor Learning The actor network learns to select actions that maximize advantages A λ t = Rλ t − v ψ (ŝ t ) / max(1, S) while regularizing the policy entropy to ensure exploration both in imagination and during data collection.In order to use a single regularization scale for all domains, DreamerV3 stabilizes the scale of returns using exponentially moving average statistics of their percentiles: S = EMA(Per( Rλ t , 95) − Per( Rλ t , 5), 0.99).The actor loss computes policy gradients using stochastic back-propagation thought the model sequential and dynamics networks for continuous actions (ρ = 0) and using reinforce [63] for discrete actions (ρ = 1):
L actor (θ) = H t=1 − ρ log π θ (a t | ŝt )sg(A λ t ) reinforce − (1 − ρ)A λ t dynamics backprop − ηH[π θ (a t | ŝt )]
entropy regularizer (9) 5 Experiments</p>
<p>In this section, we aim to evaluate the performance of our MuDreamer algorithm compared to its reconstruction-based version and other published model-based and model-free methods.We evaluate MuDreamer on the Visual Control Suite from DeepMind (Table 1) and under the natural background setting (Table 2) where tasks background is replaced by real-world videos.We also proceed to a detailed ablation study, studying the effect of batch normalization, action-value predictors and KL balancing on performance and learning stability (Table 3).The performance curves showing comparison for each individual task, Atari100k benchmark comparison with other model-based methods, as well as additional visualizations can be found in the appendix.</p>
<p>Visual Control Suite</p>
<p>The DeepMind Control Suite was introduced by Tassa et al. [56] as a set of continuous control tasks with a standardised structure and interpretable rewards.The suite is intended to serve as performance benchmarks for reinforcement learning agents in continuous action space.The tasks can be solved from low-dimensional inputs and/or pixel observations.In this work, we evaluate our method on the Visual Control Suite benchmark which contains 20 tasks where the agent receives only highdimensional images as inputs and a budget of 1M environment steps.Similarly to DreamerV3 [25], we use 4 environment instances during training with a training ratio of 512 replayed steps per policy step.Our PyTorch [48] [21], CURL [36], DrQ-v2 [65] and DreamerV3.As shown in Figure 2, although the reconstruction gradients are not propagated to the whole network, the decoder easily reconstruct the input image, meaning that the model hidden state contains all necessary information about the environment.We find that MuDreamer achieves better performance and learning stability on tasks like Finger Turn Hard and Reacher Hard where crucial task-solving elements are small and harder to model using reconstruction.</p>
<p>Natural Background Setting</p>
<p>While the reconstruction loss is essential for Dreamer's performance, it also necessitates modeling unnecessary information.Consequently, Dreamer sometimes fails to perceives crucial elements which are necessary for task-solving when visual distractions are present in the observation, significantly limiting its potential.This object vanishing phenomenon arises particularly when crucial elements are small like the ball in Cup Catch.In order to further study the effect of visual distractions on DreamerV3 and MuDreamer performance, we experiment with the natural background setting [39,43,15,8], where the DeepMind Visual Control tasks background is replaced by real-world videos.</p>
<p>Following TPC [43] and DreamerPro [15], we replace the background by task-irrelevant videos randomly sampled from the driving car class of the Kinetics 400 dataset [34].We also use two separate sets of background videos for training and evaluation in order to test generalization to unseen distractions.Table 2 shows the comparison of MuDreamer with DreamerV3, DreamerPro and TPC on the Visual Control Suite under the natural background setting.MuDreamer successfully learns a policy on most tasks and achieves state-of-the-art performance with a mean-score of 517.0 compared to 445.2 and 372.8 for DreamerPro and TPC. Figure 3 shows the decoder reconstruction of observations by DreamerV3 and MuDreamer on Walker Run, Finger Spin and Quadruped Run tasks.MuDreamer correctly reconstructs the agent body with a monochrome or blurry background while DreamerV3 focuses on the background details, discarding the agent body and necessary information.</p>
<p>Ablation Study</p>
<p>In order to understand the necessary components of MuDreamer, we conduct ablation studies applying one modification at a time.We study the impact of using value and action prediction branches, removing one or both branches during training.We study the effect of using batch normalization in the representation network to stabilize learning and avoid collapse without the reconstruction loss.We also study the effect of KL balancing hyper-parameters on learning speed and stability.We perform all these ablation studies on the Visual Control Suite, observing the effect of each modification for diverse tasks.The mean and median score results of these studies are summarized in Table 3 and Figure 4. Please refer to the appendix for the evaluation curves of individual tasks.</p>
<p>Action-Value Predictors</p>
<p>We study the necessity of using action and value prediction branches to learn hidden representations and successfully solve the tasks without reconstruction loss.We observed that removing the action or value prediction heads led to a degradation of MuDreamer performance and decoder reconstruction quality on most of the Visual Control tasks.The action and value prediction losses require the model to learn environment dynamics which leads to more accurate model predictions and better performance.Batch Normalization We study the effect of using batch normalization instead of layer normalization inside the representation network to stabilize representation learning.We find that batch normalization prevents collapse in which the model produces constant or non-informative hidden states.Without batch normalization, we observe that MuDreamer fails to learn representations for some of the tasks.The standard deviation of the model encoded features x t converges to zero, which prevent learning hidden representations.Batch normalization solves this problem by stabilizing dynamics and representation losses.</p>
<p>KL Balancing</p>
<p>We find that applying the default KL balancing parameters of DreamerV3 (β dyn = 0.5, β rep = 0.1) slows down convergence for some of the tasks, restraining the model from learning representations.A larger regularization of the posterior toward the prior with β rep = 0.2 limits the amount of information encoded in the model stochastic state.We observe that unnecessary information such as the environment floor and the agent shadow are sometimes reconstructed as monochrome surfaces without the original details.Similar to BLAST and predictive SSL approaches [19,11], we experiment using the stop gradient operation with β rep = 0.0.We find that this solves the issue but generates learning instabilities with spikes for the dynamics and prediction losses.We suppose this create difficulties for the prior to predict posterior representations since we do not use a slow moving teacher encoder network.Using a slight regularization of the representations with β rep = 0.05 solved both of these issues.</p>
<p>Conclusion</p>
<p>We presented MuDreamer, a robust reinforcement learning agent solving control tasks from image inputs with continuous and discrete action spaces, all without the need to reconstruct input signals.</p>
<p>MuDreamer learns a world model by predicting environment rewards, value function, and continuation flags, focusing on information relevant to the task.We also proposed to incorporate an action prediction branch to predict the sequence of selected actions.MuDreamer demonstrates stronger robustness to visual distractions on the DeepMind Visual Control Suite compared to DreamerV3 and other methods when the environment background is replaced by task-irrelevant real-world videos.</p>
<p>Our approach also benefits from faster training, as it does not require training an additional decoder network or encoding separate augmented views of the observation to learn hidden representations.</p>
<p>A Appendix</p>
<p>A.1 Atari100k Benchmark</p>
<p>The Atari100k benchmark was proposed in Kaiser et al. [33] to evaluate reinforcement learning agents on Atari games in low data regime.The benchmark includes 26 Atari games with discrete actions and a budget of 400K environment steps, amounting to 100K policy steps using the default action repeat setting.This represents 2 hours of real-time play.The current state-of-the-art is held by EfficientZero (EffZero) [66], which uses look-ahead search to select the best action, with a human mean score of 190% and median of 116%.In this work, we compare our method with DreamerV3 and other model-based approaches which do not use look-ahead Table 4 compares MuDreamer with DreamerV3 [25] and other model-based approaches [49,11,36,66,40] on the Atari100k benchmark.Following preceding works, we compare the mean and median returns across all 26 games using human-normalized scores calculated as follow: norm_score = (agent_score − random_score) / (human_score − random_score) (10) MuDreamer achieves results comparable to DreamerV3, without using a decoder reconstruction loss during training.Our implementation takes 4 hours and 40 minutes to reach 400K steps on the Breakout Atari game using a single NVIDIA RTX 3090 GPU and 16 CPU cores, while DreamerV3 takes 5 hours and 20 minutes.</p>
<p>A.2 Limitations</p>
<p>Certain limitations of our work arise from the fact that trajectories sampled from the replay buffer are off-policy, which requires the value predictor to fit the value of older policies that are no longer accurate.The resulting value function differs from the one learned by the critic network which fits the value of the current neural network policy thought the world model.While we found this to have no effect on performance, experimenting with off-policy correction [66], this may be the case when scaling up to larger model sizes and more complex tasks.</p>
<p>A.3 Ethics Statement</p>
<p>The development of autonomous agents for real-world applications introduces various safety and environmental concerns.In real-world scenarios, an agent might cause harm to individuals and damage to its surroundings during both training and deployment.Although using world models during training can mitigate these risks by allowing policies to be learned through simulation, some risks may still persist.This statement aims to inform users of these potential risks and emphasize the importance of AI safety in the application of autonomous agents to real-world scenarios.</p>
<p>A.4 MuDreamer Hyper-Parameters     11 compares the training speed on the Walker-run visual control task using a RTX 3090 GPU.We show the number of FLOPs required for a world model training forward using a sequence of 64 time frames and same architecture hyper-parameters as MuDreamer (Table 5).We also experiment with larger image sizes, adding one strided convolution layer to the encoder and decoder networks when the image size is doubled.DreamerPro achieves competitive performance compared to MuDreamer but relies on the encoding of two augmented views of the same image to compute the SwAV loss.This results in a significant increase of the number of FLOPs and GPU memory, leading to slower training and memory overflow for larger image sizes.TPC benefits from good training speed since it does not requires a decoder or action-value predictors but it also has a noticeable negative impact on performance.The default regularization of the posterior representations toward the prior tends to limit the amount of information in the latent space in order to minimize the KL divergence between the two state distributions.In this examples, we observe that unnecessary information such as the environment floor are reconstructed as monochrome surfaces without the original details.</p>
<p>A.15 Natural Background Setting 6 tasks Comparison</p>
<p>We compare MuDreamer with DreamerPro and TPC on the six visual control tasks considered in Deng et al. [15].Following DreamerPro, we scale the reward prediction loss by a factor of 100 to further encourage extraction of task-relevant information.Figure 15 shows that MuDreamer outperforms DreamerPro and TPC over the six tasks without requiring encoding separate augmented views of input images during training.MuDreamer achieves a mean score of 661.3 over the six tasks against 562.4 and 453.3 for DreamerPro and TPC.Table 12: Visual Control Suite scores on the six tasks considered in Deng et al. [15] under the natural background setting (1M environment steps).† denotes our tested reimplementation of DreamerV3.‡ results were taken from DreamerPro.We average the evaluation score over 10 episodes and use 3 seeds per experiment.</p>
<p>Figure 1 :
1
Figure 1: MuDreamer world model training.A sequence of image observations o 1:T is sampled from the replay buffer.The sequence is mapped to hidden representations x 1:T using a CNN encoder.At each step, the RSSM computes a posterior state z t representing the current observation o t and a prior state ẑt that predict the posterior without having access to the current observation.Unlike Dreamer, the decoder gradients are not back-propagated to the rest of the model.The hidden representations are learned solely using value, reward, episode continuation and action prediction heads.</p>
<p>Figure 2 :
2
Figure 2: Reconstruction of MuDreamer model predictions over 64 time steps.We take 5 context frames and generate trajectories of 59 steps into the future using the model sequential and dynamics networks.Actions are predicted using the policy network given generated latent states.MuDreamer generates accurate long-term predictions similar to Dreamer without requiring reconstruction loss gradients during training to compress the observation information into the model hidden state.</p>
<p>Figure 3 :
3
Figure 3: Agents reconstruction of observations using natural backgrounds for Walker Run, Finger Spin and Quadruped Run tasks.First row shows original sequence of observation, second row shows DreamerV3 reconstruction and third row MuDreamer decoder reconstruction.We observe that DreamerV3 reconstructs general details while MuDreamer learns to filter unnecessary information.</p>
<p>Figure 4 :
4
Figure 4: Ablations mean scores on the Visual Control Suite using 1M environment steps.</p>
<p>Figure 6 :
6
Figure 6: Trajectories imagined by DreamerV3 and MuDreamer world models over 64 time steps using 5 context frames under the natural background setting.MuDreamer models the environment dynamics in latent space, filtering the unnecessary information while DreamerV3 focuses on modeling the environment background.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: Comparison of MuDreamer with DreamerV3, DreamerPro and TPC under the natural background setting (1M environment steps).TPC and DreamerPro results were obtained using the official implementation of DreamerPro.We average the evaluation score over 10 episodes and use 3 seeds per experiment.</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: DreamerV3 and MuDreamer comparison with or without using batch normalization in the representation network instead of layer normalization (train ratio = 128).Left plots: agent evaluation score.Middle: the per-channel std of the l2-normalized encoder outputs x t , plotted as the averaged std over all channels.Right plots: per-channel std of the l2-normalized model recurrent state h t .Batch normalization improves learning stability and prevents collapses to constant feature vectors for all images.</p>
<p>Figure 15 :
15
Figure 15: Comparison of MuDreamer with DreamerV3, DreamerPro and TPC on the six visual control tasks considered in Deng et al. [15] under the natural background setting (1M env.steps).</p>
<p>Table 1 :
1
[25]ementation of MuDreamer takes a little less than 14 hours to reach 1M environment steps on the Walker Run visual control task using a single NVIDIA RTX 3090 GPU and 16 CPU cores, while DreamerV3 takes 15 hours.The total amount of trainable parameters for MuDreamer and DreamerV3 are 15.3M and 17.9M, respectively.Visual Control Suite scores (1M environment steps).†resultswere taken from Hafner et al.[25].We average the evaluation score over 10 episodes and use 3 different seeds per experiment.
TaskSAC  †CURL  †DrQ-v2  †DreamerV3  †MuDreamerAcrobot Swingup5.15.1128.4210.0304.6Cartpole Balance963.1979.0991.5996.4990.4Cartpole Balance Sparse950.8981.0996.21000.01000.0Cartpole Swingup692.1762.7858.9819.1823.0Cartpole Swingup Sparse154.6236.2706.9792.9582.0Cheetah Run27.2474.3691.0728.7872.5Cup Catch163.9965.5931.8957.1930.8Finger Spin312.2877.1846.7818.5603.6Finger Turn Easy176.7338.0448.4787.7915.4Finger Turn Hard70.5215.6220.0810.8886.5Hopper Hop3.1152.5189.9369.6311.8Hopper Stand5.2786.8893.0900.6883.9Pendulum Swingup560.1376.4839.7806.3806.7Quadruped Run50.5141.5407.0352.3627.8Quadruped Walk49.7123.7660.3352.6860.0Reacher Easy86.5609.3910.2898.9907.0Reacher Hard9.1400.2572.9499.2733.0Walker Run26.9376.2517.1757.8740.9Walker Stand159.3463.5974.1976.7964.3Walker Walk38.9828.8762.9955.8949.8Mean225.3504.7677.4739.6784.7Median78.5431.8734.9808.5849.6</p>
<p>Table 1 compares
1
MuDreamer with DreamerV3 and other recent methods on the Visual Control Suite using 1M environment steps.MuDreamer achieves state-of-the-art mean-score without reconstructing the input signal, outperforming SAC</p>
<p>Table 2 :
2
Visual Control Suite scores under the natural background setting (1M env.steps).† denotes our tested reimplementation of DreamerV3.‡ results were obtained using the official implementation of DreamerPro.We average the evaluation score over 10 episodes and use 3 seeds per experiment.
TaskRandomTPC  ‡DreamerPro  ‡DreamerV3  †MuDreamerAcrobot Swingup0.35.113.19.141.9Cartpole Balance329.3792.9870.1198.7974.8Cartpole Balance Sparse53.926.9198.418.4898.7Cartpole Swingup67.4574.8689.2145.7794.4Cartpole Swingup Sparse0.00.217.80.30.0Cheetah Run6.7440.8380.794.3318.1Cup Catch31.5451.5437.527.9904.5Finger Spin0.9696.8724.296.5644.2Finger Turn Easy48.8479.5232.4197.8229.4Finger Turn Hard35.0198.3228.339.8226.7Hopper Hop0.00.21.40.60.2Hopper Stand1.914.5296.53.05.4Pendulum Swingup2.0778.7777.68.0606.8Quadruped Run8.8162.9470.8108.9735.0Quadruped Walk110.0681.4784.561.2872.8Reacher Easy52.6642.4692.7154.2914.4Reacher Hard7.47.09.410.613.5Walker Run25.9137.9402.978.7432.8Walker Stand139.4935.4940.6254.4966.7Walker Walk36.8428.3736.1164.7759.0Mean47.9372.8445.283.6517.0Median28.7409.8416.257.1620.0OriginalDreamerV3MuDreamer</p>
<p>Table 3 :
3
Ablations evaluated on 20 tasks of the Visual Control Suite using 1M environment steps.Each ablation applies only one modification to the MuDreamer agent.
AgentMeanMedianTask Score ≥ MuDreamerMuDreamer784.7849.6No Value Predictor628.8745.65 / 20No Action Predictor648.0810.86 / 20No Action and Value Predictors505.4493.24 / 20No Batch Normalization689.5820.78 / 20βrep = 0.0, β dyn = 1.0713.5824.36 / 20βrep = 0.1, β dyn = 0.9673.7845.78 / 20βrep = 0.1, β dyn = 0.5601.1790.210 / 20βrep = 0.2, β dyn = 0.8603.9694.55 / 20</p>
<p>Table 4 :
4
Atari100k scores (400K environment steps).We average the evaluation score over 10 episodes and use 5 different seeds per experiment.
Lookahead searchNo lookahead searchGameRandomHumanMuZeroEffZeroSimPLeIRISDreamerV3MuDreamerAlien22871285301140617420959951Amidar617203910274143139153Assault22274250014075271524706891Asterix210850317341684411288549321411Bank Heist147531933623453649156Battle Zone2360371887688179384031130741225012080Boxing01215448707896Breakout2304840616843134Chopper Com.8117388135017949791565420808Crazy Climber1078035829569378012562584593249719096128Demon Attack15219713527132982082034303553Freeway0302222173105Frostbite6543352553142372599091652Gopher258241212563518597223637301500Hero1027308263095853026577037111618272James Bond2930388459100463445409Kangaroo523035639625183840984380Krull15982666489160472205661677829644Kung Fu Mas.25822736188133111214862217602142026832Ms Pacman307695212661387148099913272311Pong-2115-72113151818Private Eye256957156100351008821042Qbert16413455395215458128974634054061Road Runner12784525001851256419615155658460Seaquest68420552081020683661618428Up N Down5331169328971609633503546760026494#Superhuman0N/A513110911Human Mean0%100%56%190%33%105%112%126%Human Median0%100%23%116%13%29%49%43%</p>
<p>Table 5 :
5
MuDreamer hyper-parameters applied to the DeepMind Visual Control Suite (DMC) and Atari100k benchmark.We apply the same hyper-parameters to all tasks except for the number of environment instances and the training ratio (number of replayed steps per policy step).Discount and Return Lambda values are kept the same for World Model and Actor Critic learning.
ParameterSymbolSettingGeneralReplay Buffer Capacity (FIFO)-10 6Batch SizeB16Batch LengthT64Optimizer-AdamActivation-LayerNorm + SiLUModel Size-DreamerV3 SmallInput Image Resolution-64 × 64 RGBReplayed Steps per Policy Step-512 (DMC) / 1024 (Atari100k)Environment Instances-4 (DMC) / 1 (Atari100k)World ModelNumber of Latents-32Classes per Latent-32Prediction Loss Scaleβ pred1.0Dynamics Loss Scaleβ dyn0.95Representation Loss Scaleβrep0.05Learning Rateα10 −4Adam Betasβ1, β20.9, 0.999Adam Epsilonϵ10 −8Gradient Clipping-1000Slow Value Momentumτ0.99Model Discountγ0.997Model Return Lambdaλ0.95Activation Representation Network-BatchNorm + SiLUBatchNorm Momentum-0.9Actor CriticImagination HorizonH15Discountγ0.997Return Lambdaλ0.95Critic EMA Decay-0.98Critic EMA regularizer-1.0Return Normalization Percentiles-5 th and 95 thReturn Normalization Decay-0.99Actor Entropy Scaleη3 • 10 −4Learning Rateα3 • 10 −5Adam Betasβ1, β20.9, 0.999Adam Epsilonϵ10 −5Gradient Clipping-100</p>
<p>Table 6 :
6
[2]oder network architecture.LN refers to Layer Normalization[2].The encoder downsamples images with strided convolutions of kernel size = 4, stride = 2 and zero padding = 1.
ModuleOutput shapeInput image (ot)3 × 64 × 64Conv2d + LN + SiLU32 × 32 × 32Conv2d + LN + SiLU64 × 16 × 16Conv2d + LN + SiLU128 × 8 × 8Conv2d + LN + SiLU256 × 4 × 4Flatten and Output features (xt)4096</p>
<p>Table 7 :
7
[13]Sequential Network process previous stochastic state z t−1 and action a t−1 to compute the next model hidden state h t using a Gated Recurrent Unit (GRU)[13].The GRU conditions the model hidden state on past context h t−1 which helps the world model to make accurate predictions.The initial GRU hidden state vector h 0 is learned via gradient descent among other network parameters and processed by the dynamics predictor to generate the initial stochastic state z 0 .
ModuleOutput shapeInput stochastic state (zt−1), Input action (at−1)32 × 32, AFlatten and Concatenate1024 + ALinear + LN + SiLU512GRU and Output hidden state (ht)512</p>
<p>Table 8 :
8
[32]Representation Network process encoded features x t and hidden state h t to generate the model stochastic state z t which is sampled from a one hot categorical distribution.We use batch normalization (BN)[32]inside the representation network to ensure stable training.
ModuleOutput shapeInput features (xt), Input hidden state (ht)4096, 512Concatenate4608Linear + BN + SiLU512Linear and Reshape32 × 32Sample one hot and Output stochastic state (zt)32 × 32</p>
<p>Table 9 :
9
The Dynamics Predictor learns to predict current stochastic state z t without being conditioned on encoded features x t .The sequential network and dynamics predictor are used during the behavior learning phase to generate imaginary trajectories and train the actor-critic networks.
ModuleOutput shapeInput hidden state (ht)512Linear + LN + SiLU512Linear and Reshape32 × 32Sample one hot and Output stochastic state (ẑt)32 × 32</p>
<p>Table 10 :
10
Networks with Multi Layer Perceptron (MLP) structure.Inputs are first flattened and concatenated along the feature dimension.Each MLP layer is followed by a layer normalization and SiLU activation except for the last layer which outputs distribution logits.
NetworkMLP layersInputsHidden dimension Output dimensionOutput DistributionReward predictor3st512255Symlog discreteContinue predictor3st5121BernoulliValue predictor3st512255Symlog discreteAction predictor3xt, st−1512ANormal (continuous) / One hot (discrete)Critic network3st512255Symlog discreteAction network3st512ANormal (continuous) / One hot (discrete)</p>
<p>Table 11 :
11
Training speed comparison on Walker Run task.
MethodReconstruction Free ModelSingle View Training#FLOPs (Billion) 64x64 128x128 256x256 64x64 128x128 256x256 Training step per secBackground Setting mean scoreDreamer [25]✗✓4.37.018.04.62.8OOM86.7TPC [43]✓✓2.23.69.15.33.81.8372.8DreamerPro [15]✓✗7.912.934.93.22.0OOM445.2MuDreamer✓✓2.53.89.35.03.71.8517.0
A.6 World Model PredictionsModel
Self-supervised learning from images with a jointembedding predictive architecture. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann Lecun, Nicolas Ballas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>Data2vec: A general framework for self-supervised learning in speech, vision and language. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli, International Conference on Machine Learning. PMLR2022</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems. 202235</p>
<p>Vicreg: Variance-invariance-covariance regularization for self-supervised learning. Adrien Bardes, Jean Ponce, Yann Lecun, arXiv:2105.049062021arXiv preprint</p>
<p>Distributional policy gradients. Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, T B Dhruva, Alistair Muldal, Nicolas Heess, Timothy Lillicrap, International Conference on Learning Representations. 2018</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas Léonard, Aaron Courville, arXiv:1308.34322013arXiv preprint</p>
<p>Information prioritization through empowerment in visual model-based rl. Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, Sergey Levine, International Conference on Learning Representations. 2022</p>
<p>Unsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, Advances in neural information processing systems. 202033</p>
<p>Emerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLR2020</p>
<p>Exploring simple siamese representation learning. Xinlei Chen, Kaiming He, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Learning phrase representations using rnn encoderdecoder for statistical machine translation. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, arXiv:1406.10782014arXiv preprint</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. Rémi Coulom, International conference on computers and games. Springer2006</p>
<p>Dreamerpro: Reconstruction-free model-based reinforcement learning with prototypical representations. Fei Deng, Ingook Jang, Sungjin Ahn, International Conference on Machine Learning. PMLR2022</p>
<p>Whitening for self-supervised representation learning. Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, Nicu Sebe, International Conference on Machine Learning. PMLR2021</p>
<p>Masked autoencoders as spatiotemporal learners. Christoph Feichtenhofer, Yanghao Li, Kaiming He, Advances in neural information processing systems. 202235</p>
<p>Improving pilco with bayesian neural network dynamics models. Yarin Gal, Rowan Mcallister, Carl Edward Rasmussen, Data-efficient machine learning workshop, ICML. 2016425</p>
<p>Bootstrap your own latent-a new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Advances in neural information processing systems. 202033</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLR2018</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, International conference on machine learning. PMLR2019</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, International Conference on Learning Representations. 2020</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, International Conference on Learning Representations. 2021</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Model-based planning with discrete and continuous actions. Mikael Henaff, William F Whitney, Yann Lecun, arXiv:1705.071772017arXiv preprint</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2015arXiv preprint</p>
<p>Learning deep representations by mutual information estimation and maximization. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, Yoshua Bengio, International Conference on Learning Representations. 2019</p>
<p>Batch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International conference on machine learning. 2015</p>
<p>Modelbased reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, International Conference on Learning Representations. 2020</p>
<p>Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, arXiv:1705.06950The kinetics human action video dataset. 2017arXiv preprint</p>
<p>. P Diederik, Max Kingma, Welling, arXiv:1312.61142013Auto-encoding variational bayes. arXiv preprint</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, International Conference on Machine Learning. PMLR2020</p>
<p>Backpropagation applied to handwritten zip code recognition. Yann Lecun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, Lawrence D Jackel, Neural computation. 141989</p>
<p>Continuous control with deep reinforcement learning. Jonathan J Timothy P Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, International Conference on Learning Representations. 2016</p>
<p>Contrastive variational reinforcement learning for complex observations. Xiao Ma, Siwei Chen, David Hsu, Wee Sun, Lee , Conference on Robot Learning. PMLR2021</p>
<p>Transformers are sample efficient world models. Vincent Micheli, Eloi Alonso, François Fleuret, International Conference on Learning Representations. 2023</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Temporal predictive coding for model-based planning in latent space. Rui Tung D Nguyen, Tuan Shu, Hung Pham, Stefano Bui, Ermon, International Conference on Machine Learning. PMLR2021</p>
<p>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. Masashi Okada, Tadahiro Taniguchi, 2021 ieee international conference on robotics and automation (icra). IEEE2021</p>
<p>Dreamingv2: Reinforcement learning with discrete world models without reconstruction. Masashi Okada, Tadahiro Taniguchi, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Representation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.037482018arXiv preprint</p>
<p>Blast: Latent dynamics models from bootstrapping. Keiran Paster, Lev E Mckinney, Sheila A Mcilraith, Jimmy Ba, Deep RL Workshop NeurIPS 2021. 2021</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>The predictron: End-toend learning and planning. David Silver, Hado Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, International Conference on Machine Learning. PMLR2017</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Science. 36264192018</p>
<p>Learning to predict by the methods of temporal differences. Richard S Sutton, Machine learning. 31988</p>
<p>an integrated architecture for learning, planning, and reacting. Richard S Sutton, Dyna, ACM Sigart Bulletin. 241991</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego De Las, David Casas, Budden, arXiv:1801.00690Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. 2018arXiv preprint</p>
<p>Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 201730Neural discrete representation learning</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 57577822019</p>
<p>Exploring model-based planning with policy networks. Tingwu Wang, Jimmy Ba, arXiv:1906.086492019arXiv preprint</p>
<p>Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, arXiv:1907.02057Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. 2019arXiv preprint</p>
<p>Embed to control: A locally linear latent dynamics model for control from raw images. Manuel Watter, Jost Springenberg, Joschka Boedecker, Martin Riedmiller, Advances in neural information processing systems. 282015</p>
<p>Masked feature prediction for self-supervised visual pre-training. Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, Christoph Feichtenhofer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 81992</p>
<p>Simmim: A simple framework for masked image modeling. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, Han Hu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Mastering visual continuous control: Improved data-augmented reinforcement learning. Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto, arXiv:2107.096452021arXiv preprint</p>
<p>Mastering atari games with limited data. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao, Advances in Neural Information Processing Systems. 202134</p>
<p>Barlow twins: Selfsupervised learning via redundancy reduction. Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, Stéphane Deny, International Conference on Machine Learning. PMLR2021</p>            </div>
        </div>

    </div>
</body>
</html>
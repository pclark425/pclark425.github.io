<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3330 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3330</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3330</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-56cafbac34f2bb3f6a9828cd228ff281b810d6bb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/56cafbac34f2bb3f6a9828cd228ff281b810d6bb" target="_blank">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs is proposed.</p>
                <p><strong>Paper Abstract:</strong> Abstract Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3330.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3330.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KEPLER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified PLM that jointly optimizes a masked language modeling (MLM) objective and a knowledge embedding (KE) objective by encoding entity descriptions with the same Transformer encoder (RoBERTa_BASE) to produce text-enhanced entity embeddings and knowledge-aware language representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KEPLER-Wiki (KEPLER)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Built on RoBERTa_BASE (Transformer encoder, L=12, d=768) initialized from RoBERTa_BASE; encodes entity descriptions (first 512 tokens) as entity embeddings and jointly trains MLM + KE losses. KE variants include entity-as-embedding, entity+relation-descriptions, and entity-conditioned-on-relation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BASE (RoBERTa_BASE; L=12, d=768)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Masked Language Modeling (MLM)', 'Knowledge Embedding objective (KE) via text-encoded entity embeddings (TransE scoring)', 'Entity-and-relation description encoding', 'Entity embeddings conditioned on relations']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MLM: standard masked-token prediction (15% masking, 80/10/10 scheme) to preserve language understanding; KE: negative-sampling margin ranking loss with TransE scoring d_r(h,t) = ||h + r - t||_1 where entity embeddings are obtained by encoding entity descriptions with the PLM (variants encode relations or concatenate relation descriptions to condition entity embeddings). The joint training combines both losses (L = L_KE + L_MLM) so the model both 'extracts' facts from text and 'stores' knowledge into entity/name representations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses a multi-objective training setup (two distinct methods: MLM and KE) â€” not multiple prompting styles but complementary learning signals. KEPLER also studies multiple KE variants (entity descriptions, relation descriptions, relation-conditioned entity embeddings) representing somewhat diverse KE styles within a single architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Relation classification (TACRED, FewRel), entity typing (OpenEntity), general NLU (GLUE), link prediction on Wikidata5M (transductive & inductive), knowledge probing (LAMA / LAMA-UHN)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks include relation classification (TACRED: 42 relations, sentence-level relation labels), few-shot relation classification (FewRel N-way K-shot), entity typing, GLUE suite for general language understanding, link prediction on large KG Wikidata5M (transductive and inductive splits), and cloze-style knowledge probing (LAMA, LAMA-UHN).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key reported results: TACRED F1: KEPLER-Wiki = 72.0% (Our RoBERTa baseline = 70.2%, KEPLER-KE [only KE] = 62.0%). FewRel (examples): Proto(KEPLER-Wiki) and PAIR(KEPLER-Wiki) reach top accuracy among BASE models across many N-K settings (e.g., PAIR(KEPLER-Wiki) 5-way-1-shot = 90.31% on FewRel 1.0). Link prediction on Wikidata5M transductive: KEPLER-Wiki MR=14454, MRR=15.4, H@10=24.4; KEPLER-Cond (relation-conditioned) transductive MRR=21.0, H@10=27.7. Inductive link prediction: KEPLER-Wiki MRR=35.1, H@10=71.9; KEPLER-Cond MRR=40.2, H@10=73.0. LAMA (P@1): KEPLER-Wiki Google-RE=7.3, T-REx=24.6 (Our RoBERTa: 7.0 / 23.2; BERT_BASE: 9.8 / 31.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper directly compares joint KE+MLM (KEPLER) to: (1) MLM-only RoBERTa (Our RoBERTa), (2) KE-only (KEPLER-KE), and (3) other knowledge-enhanced PLMs (ERNIE, KnowBert) and specialized pretraining (MTB). Ablation shows KEPLER > Our RoBERTa on entity-centric NLP tasks and KEPLER >> KEPLER-KE, demonstrating the necessity of both objectives. For KE tasks, KEPLER-Cond (conditioning entity embeddings on relation descriptions) improves link prediction most. Compared to baselines, KEPLER yields higher TACRED and many FewRel settings; KEPLER shows strong inductive KE compared to DKRL and standard KE baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Jointly optimizing MLM and KE aligns textual representations with KG symbol space, improving both knowledge extraction from text and KE performance; KEPLER preserves language understanding (GLUE close to RoBERTa) while gaining factual knowledge benefits; KE objective enables inductive embeddings for unseen entities (good inductive link prediction), and relation-conditioned entity encoding yields best KE metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>KEPLER-OnlyDesc (training MLM only on entity descriptions) degrades general language understanding. KEPLER-Rel (encoding relation descriptions as relation embeddings) performed worse on some tasks because Wikidata relation descriptions are short/homogeneous. KEPLER-KE (only KE objective) gives poor NLP task performance (e.g., TACRED F1 62.0), showing that a single KE style alone is insufficient. On LAMA, RoBERTa-based models (including KEPLER variants) underperform BERT_BASE on some probes, attributed to vocabulary mismatches rather than method superiority. Also, ERNIE sometimes outperforms KEPLER in 1-shot FewRel scenarios (benefit from injected entity embeddings), indicating single-purpose injection can help in very low-data cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3330.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3330.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERNIE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERNIE: Enhanced language representation with informative entities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-enhanced PLM that injects external entity embeddings into the PLM by identifying and linking entity mentions in text and fusing fixed pre-trained entity embeddings at corresponding token positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ERNIE: Enhanced language representation with informative entities</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ERNIE (reproduced variants: ERNIE_BERT, ERNIE_RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identifies entity mentions in input via entity linking and inserts pre-trained entity embeddings (learned separately) into the PLM representations; original ERNIE used BERT-based architectures; this paper reproduces ERNIE on both BERT_BASE and RoBERTa_BASE for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BASE (reproductions used BERT_BASE / RoBERTa_BASE)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Entity-injection via entity linking (external entity embeddings)', 'Use of fixed KE embeddings combined with the PLM']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>A pipeline does entity mention detection and linking to a KG, retrieves pre-computed entity embeddings and injects or fuses them into PLM token representations to provide explicit factual signals (no joint end-to-end KE training in original ERNIE implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Relies on a single style (external entity embedding injection via entity linking) rather than a multi-objective internal learning approach; this is a similar-style, surgery-on-top-of-PLM approach rather than multi-objective training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TACRED, FewRel, OpenEntity, Few-shot relation tasks, LAMA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Applied as a knowledge-injected encoder for relation classification, entity typing, and knowledge probing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>TACRED: ERNIE_BERT F1 = 68.0; ERNIE_RoBERTa (reproduction) F1 = 70.7 (P=73.5, R=68.0). On FewRel, ERNIE performed particularly well in 1-shot settings of FewRel 1.0 (text reports ERNIE performs best on some 1-shot settings), but ERNIE degrades in 5-shot and domain-shift FewRel 2.0 settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to KEPLER's joint KE+MLM approach, ERNIE's entity-injection gives advantages in extreme low-shot (1-shot) cases by directly providing entity knowledge, but is less robust in domain-shift or when richer textual extraction is needed; ERNIE does not require the PLM to learn to encode entity descriptions end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Entity-injection via linking can bring immediate factual benefits, especially in very low-data scenarios, but may be brittle across domains and requires entity linking (runtime overhead) and pre-trained KE models (alignment challenges).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>ERNIE underperforms KEPLER on many multi-shot and cross-domain FewRel settings and on TACRED where joint training helps; ERNIE requires entity linkers and brings inference overhead and potential error propagation; ERNIE may degenerate on domains lacking KG coverage (FewRel 2.0 biomedical domain).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3330.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3330.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowBert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KnowBert: Knowledge enhanced contextual word representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-enhanced PLM that integrates an entity linker and an explicit knowledge-fusion component to augment PLM representations with structured KG embeddings in an end-to-end manner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge enhanced contextual word representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KnowBert (reproductions: KnowBert_BERT, KnowBert_RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Incorporates an integrated entity linker into the PLM, retrieves and fuses external knowledge embeddings (from KE methods like TuckER) into contextual representations during training; original KnowBert uses BERT as base; paper reproduces with RoBERTa as well.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BASE (BERT_BASE / RoBERTa_BASE reproductions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Entity linking + injected KG embeddings', 'End-to-end fusion of entity knowledge into PLM layers']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model detects entity mentions, links them to candidate KG entities, and fuses pre-trained entity embeddings (from a KE model such as TuckER) into internal PLM representations during fine-tuning/training to provide explicit factual cues.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses the single-style strategy of entity-linking plus embedding fusion (similar to ERNIE), not a multi-objective internal KE+MLM training as KEPLER does.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>TACRED, FewRel, OpenEntity, GLUE (as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as a knowledge-aware encoder for relation classification, entity typing, and other tasks requiring entity knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>TACRED: KnowBert_BERT (reproduction) F1 = 68.5; KnowBert_RoBERTa F1 = 70.9 (P=71.9, R=69.9). On FewRel results vary; KnowBert sometimes strong but not consistently above KEPLER.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>KnowBert performs competitively on some tasks; the paper notes KnowBert's performance depends on its KE model (it uses TuckER) and linking pipeline; compared to KEPLER's joint training, KnowBert can underperform in domain-shift or when linking fails.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding fusion with an entity linker can improve entity-centric tasks, but depends on quality of linker and KE embeddings; integration can add runtime overhead (entity linking) and is less straightforwardly aligned with PLM semantic space than joint training.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>KnowBert underperforms ERNIE in some FewRel settings and is outperformed by KEPLER in many TACRED/FewRel experiments; entity linking introduces runtime cost and potential error propagation (paper reports longer running time than KEPLER).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3330.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3330.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MTB: Matching the blanks (distributional similarity for relation learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining scheme ('matching the blank') that uses entity co-occurrence contexts: the model learns to match mentions to the same entity by seeing pairs of contexts with the entity blanked, providing relation/distributional supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Matching the blanks: Distributional similarity for relation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MTB (originally BERT_LARGE; reproduced BERT_BASE in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pretraining objective that constructs positive pairs by removing (blanking) shared entities in two contexts and training the model to identify matching entity contexts; originally implemented on BERT_LARGE, capturing distributional relation signals for relation classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Originally LARGE (BERT_LARGE); this paper reimplements a BASE variant for fair-size comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Matching-the-blank pretraining (contrastive/context-matching style)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Creates training pairs by blanking the same entity in two passages and trains the model to judge context-match or to retrieve matching contexts, thereby learning contextual signals useful for relation classification.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single specialized pretraining style focused on distributional matching rather than multi-objective knowledge+language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Relation classification (FewRel, TACRED comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated especially on few-shot relation classification (FewRel); original MTB with BERT_LARGE achieves state-of-the-art on FewRel.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>TACRED: MTB (BERT_BASE reproduction) F1 = 68.8; MTB (BERT_LARGE reported) F1 = 71.5. FewRel: MTB (BERT_LARGE) very strong (e.g. 5-1, 93.86% reported in paper's table for some settings), but MTB_BERT_BASE reproduction does not outperform other knowledge-enhanced PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>MTB with large-scale model (BERT_LARGE) is strong on FewRel but its BASE reimplementation underperforms KEPLER and other knowledge-enhanced PLMs. The paper notes MTB's pretraining objective may not pair well with sentence-pair frameworks (PAIR) leading to performance drops under PAIR.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Matching-the-blank is powerful when coupled with large models and large pretraining resources, but gains may rely heavily on model scale; objective can be incompatible with some downstream few-shot frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MTB_BERT_BASE does not outperform knowledge-enhanced baselines; when combined with PAIR framework MTB suffers an obvious performance drop, indicating method sensitivity to downstream architecture and pretraining scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3330.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3330.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Proto</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototypical Networks for Few-shot Learning (Proto framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot classification framework that represents each class by the prototype (mean) of support example embeddings and classifies queries by nearest prototype in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Proto (few-shot framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a language model but a few-shot classification method used in conjunction with text encoders (BERT, RoBERTa, KEPLER, ERNIE, KnowBert, MTB) where the encoder produces embeddings and Proto averages support embeddings per class to form prototypes for nearest-neighbor classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (framework used with various encoders, typically BASE models in these experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Prototype-based nearest-centroid classification (metric-based few-shot reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>In each episode, form class prototypes by averaging K support embeddings per relation; assign query to nearest prototype (typically cosine or Euclidean distance). The 'reasoning' is similarity-based retrieval in embedding space rather than symbolic stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style metric-based few-shot method; comparison performed by swapping different encoders (diverse encoders yield different performances).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FewRel (N-way K-shot few-shot relation classification)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Episodes sample N relations and K supports each; models must classify query sentences into N relations using only supports.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Proto with KEPLER-Wiki substantially outperforms Proto with Our RoBERTa and other BASE encoders in many settings (example: Proto(KEPLER-Wiki) 5-way-1-shot = 88.30% on FewRel 1.0; Proto(Our RoBERTa) = 84.42%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Replacing encoders shows KEPLER provides better embeddings for Proto classification compared to RoBERTa/BERT/ERNIE/KnowBert in many settings, demonstrating that encoder approach (joint KE+MLM) matters for metric-based few-shot 'reasoning'.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoder choice (knowledge-infused KEPLER vs vanilla RoBERTa) materially affects prototype-based few-shot performance; KEPLER embeddings yield stronger prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some oracle models that had seen FewRel facts during pretraining (indicated in paper) can outperform; performance differences shrink in some multi-shot settings where support text provides more information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3330.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3330.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAIR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAIR few-shot framework (PAIR: pairwise matching framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot relation classification framework that uses pairwise comparison between a query and each candidate support instance (often with a classifier over concatenated or paired representations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PAIR (few-shot framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sentence-pair style approach where each query is compared (scored/classified) against support sentences for candidate relations; used with a variety of encoders in the experiments to test few-shot relation classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (framework used with various encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Pairwise sentence comparison / matching (pairwise classification)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>For each query and candidate relation, compute a score by comparing the query to support sentences (e.g., concatenation inputs to the encoder) and aggregate to choose relation; this is effectively pairwise matching-based reasoning rather than prototype averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single-style pairwise matching; diversity arises from choice of encoder and pretraining objective.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>FewRel (N-way K-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same few-shot classification episodes as Proto, but uses pairwise matching methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>PAIR with KEPLER-Wiki achieves strong scores (e.g., PAIR(KEPLER-Wiki) 5-way-1-shot = 90.31% on FewRel 1.0), often outperforming PAIR with RoBERTa or other BASE encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PAIR combined with certain pretraining objectives (e.g., MTB) can degrade if pretraining objective conflicts with sentence-pair downstream format; KEPLER's pretraining generalizes well to PAIR and often yields top performance among BASE encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Choice of encoder and its pretraining objective interacts with few-shot method; KEPLER provides robust improvements for pairwise few-shot reasoning across numerous FewRel settings.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>MTB's pretraining objective, while strong in isolation, suffers when combined with PAIR, indicating method-objective mismatch; also model initializations and oracle data leakage can affect conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3330.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3330.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAMA / LAMA-UHN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LAMA: Language Models as Knowledge Bases (and LAMA-UHN filtered variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cloze-style probing benchmarks that measure a language model's ability to recall factual knowledge by predicting masked tokens in natural-language templates; LAMA-UHN filters out examples with superficial cues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as knowledge bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LAMA / LAMA-UHN (evaluation benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LAMA constructs cloze-style queries from existing KB facts (Google-RE, T-REx, ConceptNet, SQuAD subsets) and measures P@1 of masked-token prediction; LAMA-UHN filters templates to remove 'unhelpful' (name-primed) cases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>n/a (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Cloze-style knowledge probing (masked token prediction)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Given a templated sentence with a masked token (e.g., 'Paris is the capital of <mask>'), the PLM must predict the correct token in the top-1 predictions; measures how much factual knowledge is directly stored in model parameters or can be extracted from context.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Benchmark aggregates different factual probes; the models tested use their own internal training methods (MLM, KE injection, etc.) to perform the cloze task; LAMA itself is a single evaluation style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge probing: Google-RE, T-REx, ConceptNet, SQuAD subsets; and filtered LAMA-UHN variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Probe factual recall and robustness by asking masked-token questions derived from KG facts; LAMA-UHN reduces spurious easily-answered cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported P@1: BERT_BASE (Google-RE / T-REx) = 9.8 / 31.1; Our RoBERTa = 7.0 / 23.2; KEPLER-Wiki = 7.3 / 24.6; KEPLER-W+W shows gains on some LAMA-UHN splits (e.g., Google-RE UHN=4.1, T-REx UHN=17.1). Overall, KEPLER improves over RoBERTa but often remains below BERT_BASE on this probe (attributed to tokenization/vocabulary differences).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>LAMA highlights differences between models that store knowledge vs. those that can extract it from descriptions; KEPLER improves P@1 over RoBERTa indicating better factual integration, but vocabulary/tokenization differences confound direct comparisons (BERT vs RoBERTa).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Joint KE+MLM training increases factual recall in PLMs compared to vanilla RoBERTa, but evaluation via LAMA is sensitive to probe/template design and model vocabularies; LAMA-UHN shows that careful filtering changes relative rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>RoBERTa-based models (including KEPLER) often underperform BERT_BASE on LAMA probes due to vocabulary/tokenization mismatch rather than clear inferiority of knowledge integration; some LAMA items can be answered via superficial clues so raw LAMA scores can be misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>ERNIE: Enhanced language representation with informative entities <em>(Rating: 2)</em></li>
                <li>Knowledge enhanced contextual word representations <em>(Rating: 2)</em></li>
                <li>Matching the blanks: Distributional similarity for relation learning <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Representation learning of knowledge graphs with entity descriptions <em>(Rating: 2)</em></li>
                <li>Translating embeddings for modeling multi-relational data <em>(Rating: 2)</em></li>
                <li>RotatE: Knowledge graph embedding by relational rotation in complex space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3330",
    "paper_id": "paper-56cafbac34f2bb3f6a9828cd228ff281b810d6bb",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "KEPLER",
            "name_full": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
            "brief_description": "A unified PLM that jointly optimizes a masked language modeling (MLM) objective and a knowledge embedding (KE) objective by encoding entity descriptions with the same Transformer encoder (RoBERTa_BASE) to produce text-enhanced entity embeddings and knowledge-aware language representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "KEPLER-Wiki (KEPLER)",
            "model_description": "Built on RoBERTa_BASE (Transformer encoder, L=12, d=768) initialized from RoBERTa_BASE; encodes entity descriptions (first 512 tokens) as entity embeddings and jointly trains MLM + KE losses. KE variants include entity-as-embedding, entity+relation-descriptions, and entity-conditioned-on-relation.",
            "model_size": "BASE (RoBERTa_BASE; L=12, d=768)",
            "reasoning_methods": [
                "Masked Language Modeling (MLM)",
                "Knowledge Embedding objective (KE) via text-encoded entity embeddings (TransE scoring)",
                "Entity-and-relation description encoding",
                "Entity embeddings conditioned on relations"
            ],
            "reasoning_methods_description": "MLM: standard masked-token prediction (15% masking, 80/10/10 scheme) to preserve language understanding; KE: negative-sampling margin ranking loss with TransE scoring d_r(h,t) = ||h + r - t||_1 where entity embeddings are obtained by encoding entity descriptions with the PLM (variants encode relations or concatenate relation descriptions to condition entity embeddings). The joint training combines both losses (L = L_KE + L_MLM) so the model both 'extracts' facts from text and 'stores' knowledge into entity/name representations.",
            "diversity_of_methods": "Uses a multi-objective training setup (two distinct methods: MLM and KE) â€” not multiple prompting styles but complementary learning signals. KEPLER also studies multiple KE variants (entity descriptions, relation descriptions, relation-conditioned entity embeddings) representing somewhat diverse KE styles within a single architecture.",
            "reasoning_task_name": "Relation classification (TACRED, FewRel), entity typing (OpenEntity), general NLU (GLUE), link prediction on Wikidata5M (transductive & inductive), knowledge probing (LAMA / LAMA-UHN)",
            "reasoning_task_description": "Benchmarks include relation classification (TACRED: 42 relations, sentence-level relation labels), few-shot relation classification (FewRel N-way K-shot), entity typing, GLUE suite for general language understanding, link prediction on large KG Wikidata5M (transductive and inductive splits), and cloze-style knowledge probing (LAMA, LAMA-UHN).",
            "performance_by_method": "Key reported results: TACRED F1: KEPLER-Wiki = 72.0% (Our RoBERTa baseline = 70.2%, KEPLER-KE [only KE] = 62.0%). FewRel (examples): Proto(KEPLER-Wiki) and PAIR(KEPLER-Wiki) reach top accuracy among BASE models across many N-K settings (e.g., PAIR(KEPLER-Wiki) 5-way-1-shot = 90.31% on FewRel 1.0). Link prediction on Wikidata5M transductive: KEPLER-Wiki MR=14454, MRR=15.4, H@10=24.4; KEPLER-Cond (relation-conditioned) transductive MRR=21.0, H@10=27.7. Inductive link prediction: KEPLER-Wiki MRR=35.1, H@10=71.9; KEPLER-Cond MRR=40.2, H@10=73.0. LAMA (P@1): KEPLER-Wiki Google-RE=7.3, T-REx=24.6 (Our RoBERTa: 7.0 / 23.2; BERT_BASE: 9.8 / 31.1).",
            "comparison_of_methods": "The paper directly compares joint KE+MLM (KEPLER) to: (1) MLM-only RoBERTa (Our RoBERTa), (2) KE-only (KEPLER-KE), and (3) other knowledge-enhanced PLMs (ERNIE, KnowBert) and specialized pretraining (MTB). Ablation shows KEPLER &gt; Our RoBERTa on entity-centric NLP tasks and KEPLER &gt;&gt; KEPLER-KE, demonstrating the necessity of both objectives. For KE tasks, KEPLER-Cond (conditioning entity embeddings on relation descriptions) improves link prediction most. Compared to baselines, KEPLER yields higher TACRED and many FewRel settings; KEPLER shows strong inductive KE compared to DKRL and standard KE baselines.",
            "key_findings": "Jointly optimizing MLM and KE aligns textual representations with KG symbol space, improving both knowledge extraction from text and KE performance; KEPLER preserves language understanding (GLUE close to RoBERTa) while gaining factual knowledge benefits; KE objective enables inductive embeddings for unseen entities (good inductive link prediction), and relation-conditioned entity encoding yields best KE metrics.",
            "counter_examples_or_negative_results": "KEPLER-OnlyDesc (training MLM only on entity descriptions) degrades general language understanding. KEPLER-Rel (encoding relation descriptions as relation embeddings) performed worse on some tasks because Wikidata relation descriptions are short/homogeneous. KEPLER-KE (only KE objective) gives poor NLP task performance (e.g., TACRED F1 62.0), showing that a single KE style alone is insufficient. On LAMA, RoBERTa-based models (including KEPLER variants) underperform BERT_BASE on some probes, attributed to vocabulary mismatches rather than method superiority. Also, ERNIE sometimes outperforms KEPLER in 1-shot FewRel scenarios (benefit from injected entity embeddings), indicating single-purpose injection can help in very low-data cases.",
            "uuid": "e3330.0",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "ERNIE",
            "name_full": "ERNIE: Enhanced language representation with informative entities",
            "brief_description": "A knowledge-enhanced PLM that injects external entity embeddings into the PLM by identifying and linking entity mentions in text and fusing fixed pre-trained entity embeddings at corresponding token positions.",
            "citation_title": "ERNIE: Enhanced language representation with informative entities",
            "mention_or_use": "use",
            "model_name": "ERNIE (reproduced variants: ERNIE_BERT, ERNIE_RoBERTa)",
            "model_description": "Identifies entity mentions in input via entity linking and inserts pre-trained entity embeddings (learned separately) into the PLM representations; original ERNIE used BERT-based architectures; this paper reproduces ERNIE on both BERT_BASE and RoBERTa_BASE for comparison.",
            "model_size": "BASE (reproductions used BERT_BASE / RoBERTa_BASE)",
            "reasoning_methods": [
                "Entity-injection via entity linking (external entity embeddings)",
                "Use of fixed KE embeddings combined with the PLM"
            ],
            "reasoning_methods_description": "A pipeline does entity mention detection and linking to a KG, retrieves pre-computed entity embeddings and injects or fuses them into PLM token representations to provide explicit factual signals (no joint end-to-end KE training in original ERNIE implementation).",
            "diversity_of_methods": "Relies on a single style (external entity embedding injection via entity linking) rather than a multi-objective internal learning approach; this is a similar-style, surgery-on-top-of-PLM approach rather than multi-objective training.",
            "reasoning_task_name": "TACRED, FewRel, OpenEntity, Few-shot relation tasks, LAMA",
            "reasoning_task_description": "Applied as a knowledge-injected encoder for relation classification, entity typing, and knowledge probing.",
            "performance_by_method": "TACRED: ERNIE_BERT F1 = 68.0; ERNIE_RoBERTa (reproduction) F1 = 70.7 (P=73.5, R=68.0). On FewRel, ERNIE performed particularly well in 1-shot settings of FewRel 1.0 (text reports ERNIE performs best on some 1-shot settings), but ERNIE degrades in 5-shot and domain-shift FewRel 2.0 settings.",
            "comparison_of_methods": "Compared to KEPLER's joint KE+MLM approach, ERNIE's entity-injection gives advantages in extreme low-shot (1-shot) cases by directly providing entity knowledge, but is less robust in domain-shift or when richer textual extraction is needed; ERNIE does not require the PLM to learn to encode entity descriptions end-to-end.",
            "key_findings": "Entity-injection via linking can bring immediate factual benefits, especially in very low-data scenarios, but may be brittle across domains and requires entity linking (runtime overhead) and pre-trained KE models (alignment challenges).",
            "counter_examples_or_negative_results": "ERNIE underperforms KEPLER on many multi-shot and cross-domain FewRel settings and on TACRED where joint training helps; ERNIE requires entity linkers and brings inference overhead and potential error propagation; ERNIE may degenerate on domains lacking KG coverage (FewRel 2.0 biomedical domain).",
            "uuid": "e3330.1",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "KnowBert",
            "name_full": "KnowBert: Knowledge enhanced contextual word representations",
            "brief_description": "A knowledge-enhanced PLM that integrates an entity linker and an explicit knowledge-fusion component to augment PLM representations with structured KG embeddings in an end-to-end manner.",
            "citation_title": "Knowledge enhanced contextual word representations",
            "mention_or_use": "use",
            "model_name": "KnowBert (reproductions: KnowBert_BERT, KnowBert_RoBERTa)",
            "model_description": "Incorporates an integrated entity linker into the PLM, retrieves and fuses external knowledge embeddings (from KE methods like TuckER) into contextual representations during training; original KnowBert uses BERT as base; paper reproduces with RoBERTa as well.",
            "model_size": "BASE (BERT_BASE / RoBERTa_BASE reproductions)",
            "reasoning_methods": [
                "Entity linking + injected KG embeddings",
                "End-to-end fusion of entity knowledge into PLM layers"
            ],
            "reasoning_methods_description": "The model detects entity mentions, links them to candidate KG entities, and fuses pre-trained entity embeddings (from a KE model such as TuckER) into internal PLM representations during fine-tuning/training to provide explicit factual cues.",
            "diversity_of_methods": "Uses the single-style strategy of entity-linking plus embedding fusion (similar to ERNIE), not a multi-objective internal KE+MLM training as KEPLER does.",
            "reasoning_task_name": "TACRED, FewRel, OpenEntity, GLUE (as baseline comparisons)",
            "reasoning_task_description": "Used as a knowledge-aware encoder for relation classification, entity typing, and other tasks requiring entity knowledge.",
            "performance_by_method": "TACRED: KnowBert_BERT (reproduction) F1 = 68.5; KnowBert_RoBERTa F1 = 70.9 (P=71.9, R=69.9). On FewRel results vary; KnowBert sometimes strong but not consistently above KEPLER.",
            "comparison_of_methods": "KnowBert performs competitively on some tasks; the paper notes KnowBert's performance depends on its KE model (it uses TuckER) and linking pipeline; compared to KEPLER's joint training, KnowBert can underperform in domain-shift or when linking fails.",
            "key_findings": "Embedding fusion with an entity linker can improve entity-centric tasks, but depends on quality of linker and KE embeddings; integration can add runtime overhead (entity linking) and is less straightforwardly aligned with PLM semantic space than joint training.",
            "counter_examples_or_negative_results": "KnowBert underperforms ERNIE in some FewRel settings and is outperformed by KEPLER in many TACRED/FewRel experiments; entity linking introduces runtime cost and potential error propagation (paper reports longer running time than KEPLER).",
            "uuid": "e3330.2",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "MTB",
            "name_full": "MTB: Matching the blanks (distributional similarity for relation learning)",
            "brief_description": "A pretraining scheme ('matching the blank') that uses entity co-occurrence contexts: the model learns to match mentions to the same entity by seeing pairs of contexts with the entity blanked, providing relation/distributional supervision.",
            "citation_title": "Matching the blanks: Distributional similarity for relation learning",
            "mention_or_use": "use",
            "model_name": "MTB (originally BERT_LARGE; reproduced BERT_BASE in this paper)",
            "model_description": "A pretraining objective that constructs positive pairs by removing (blanking) shared entities in two contexts and training the model to identify matching entity contexts; originally implemented on BERT_LARGE, capturing distributional relation signals for relation classification.",
            "model_size": "Originally LARGE (BERT_LARGE); this paper reimplements a BASE variant for fair-size comparisons.",
            "reasoning_methods": [
                "Matching-the-blank pretraining (contrastive/context-matching style)"
            ],
            "reasoning_methods_description": "Creates training pairs by blanking the same entity in two passages and trains the model to judge context-match or to retrieve matching contexts, thereby learning contextual signals useful for relation classification.",
            "diversity_of_methods": "Single specialized pretraining style focused on distributional matching rather than multi-objective knowledge+language modeling.",
            "reasoning_task_name": "Relation classification (FewRel, TACRED comparisons)",
            "reasoning_task_description": "Evaluated especially on few-shot relation classification (FewRel); original MTB with BERT_LARGE achieves state-of-the-art on FewRel.",
            "performance_by_method": "TACRED: MTB (BERT_BASE reproduction) F1 = 68.8; MTB (BERT_LARGE reported) F1 = 71.5. FewRel: MTB (BERT_LARGE) very strong (e.g. 5-1, 93.86% reported in paper's table for some settings), but MTB_BERT_BASE reproduction does not outperform other knowledge-enhanced PLMs.",
            "comparison_of_methods": "MTB with large-scale model (BERT_LARGE) is strong on FewRel but its BASE reimplementation underperforms KEPLER and other knowledge-enhanced PLMs. The paper notes MTB's pretraining objective may not pair well with sentence-pair frameworks (PAIR) leading to performance drops under PAIR.",
            "key_findings": "Matching-the-blank is powerful when coupled with large models and large pretraining resources, but gains may rely heavily on model scale; objective can be incompatible with some downstream few-shot frameworks.",
            "counter_examples_or_negative_results": "MTB_BERT_BASE does not outperform knowledge-enhanced baselines; when combined with PAIR framework MTB suffers an obvious performance drop, indicating method sensitivity to downstream architecture and pretraining scale.",
            "uuid": "e3330.3",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Proto",
            "name_full": "Prototypical Networks for Few-shot Learning (Proto framework)",
            "brief_description": "A few-shot classification framework that represents each class by the prototype (mean) of support example embeddings and classifies queries by nearest prototype in embedding space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Proto (few-shot framework)",
            "model_description": "Not a language model but a few-shot classification method used in conjunction with text encoders (BERT, RoBERTa, KEPLER, ERNIE, KnowBert, MTB) where the encoder produces embeddings and Proto averages support embeddings per class to form prototypes for nearest-neighbor classification.",
            "model_size": "n/a (framework used with various encoders, typically BASE models in these experiments)",
            "reasoning_methods": [
                "Prototype-based nearest-centroid classification (metric-based few-shot reasoning)"
            ],
            "reasoning_methods_description": "In each episode, form class prototypes by averaging K support embeddings per relation; assign query to nearest prototype (typically cosine or Euclidean distance). The 'reasoning' is similarity-based retrieval in embedding space rather than symbolic stepwise reasoning.",
            "diversity_of_methods": "Single-style metric-based few-shot method; comparison performed by swapping different encoders (diverse encoders yield different performances).",
            "reasoning_task_name": "FewRel (N-way K-shot few-shot relation classification)",
            "reasoning_task_description": "Episodes sample N relations and K supports each; models must classify query sentences into N relations using only supports.",
            "performance_by_method": "Proto with KEPLER-Wiki substantially outperforms Proto with Our RoBERTa and other BASE encoders in many settings (example: Proto(KEPLER-Wiki) 5-way-1-shot = 88.30% on FewRel 1.0; Proto(Our RoBERTa) = 84.42%).",
            "comparison_of_methods": "Replacing encoders shows KEPLER provides better embeddings for Proto classification compared to RoBERTa/BERT/ERNIE/KnowBert in many settings, demonstrating that encoder approach (joint KE+MLM) matters for metric-based few-shot 'reasoning'.",
            "key_findings": "Encoder choice (knowledge-infused KEPLER vs vanilla RoBERTa) materially affects prototype-based few-shot performance; KEPLER embeddings yield stronger prototypes.",
            "counter_examples_or_negative_results": "Some oracle models that had seen FewRel facts during pretraining (indicated in paper) can outperform; performance differences shrink in some multi-shot settings where support text provides more information.",
            "uuid": "e3330.4",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "PAIR",
            "name_full": "PAIR few-shot framework (PAIR: pairwise matching framework)",
            "brief_description": "A few-shot relation classification framework that uses pairwise comparison between a query and each candidate support instance (often with a classifier over concatenated or paired representations).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PAIR (few-shot framework)",
            "model_description": "Sentence-pair style approach where each query is compared (scored/classified) against support sentences for candidate relations; used with a variety of encoders in the experiments to test few-shot relation classification.",
            "model_size": "n/a (framework used with various encoders)",
            "reasoning_methods": [
                "Pairwise sentence comparison / matching (pairwise classification)"
            ],
            "reasoning_methods_description": "For each query and candidate relation, compute a score by comparing the query to support sentences (e.g., concatenation inputs to the encoder) and aggregate to choose relation; this is effectively pairwise matching-based reasoning rather than prototype averaging.",
            "diversity_of_methods": "Single-style pairwise matching; diversity arises from choice of encoder and pretraining objective.",
            "reasoning_task_name": "FewRel (N-way K-shot)",
            "reasoning_task_description": "Same few-shot classification episodes as Proto, but uses pairwise matching methodology.",
            "performance_by_method": "PAIR with KEPLER-Wiki achieves strong scores (e.g., PAIR(KEPLER-Wiki) 5-way-1-shot = 90.31% on FewRel 1.0), often outperforming PAIR with RoBERTa or other BASE encoders.",
            "comparison_of_methods": "PAIR combined with certain pretraining objectives (e.g., MTB) can degrade if pretraining objective conflicts with sentence-pair downstream format; KEPLER's pretraining generalizes well to PAIR and often yields top performance among BASE encoders.",
            "key_findings": "Choice of encoder and its pretraining objective interacts with few-shot method; KEPLER provides robust improvements for pairwise few-shot reasoning across numerous FewRel settings.",
            "counter_examples_or_negative_results": "MTB's pretraining objective, while strong in isolation, suffers when combined with PAIR, indicating method-objective mismatch; also model initializations and oracle data leakage can affect conclusions.",
            "uuid": "e3330.5",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "LAMA / LAMA-UHN",
            "name_full": "LAMA: Language Models as Knowledge Bases (and LAMA-UHN filtered variant)",
            "brief_description": "Cloze-style probing benchmarks that measure a language model's ability to recall factual knowledge by predicting masked tokens in natural-language templates; LAMA-UHN filters out examples with superficial cues.",
            "citation_title": "Language models as knowledge bases?",
            "mention_or_use": "use",
            "model_name": "LAMA / LAMA-UHN (evaluation benchmarks)",
            "model_description": "LAMA constructs cloze-style queries from existing KB facts (Google-RE, T-REx, ConceptNet, SQuAD subsets) and measures P@1 of masked-token prediction; LAMA-UHN filters templates to remove 'unhelpful' (name-primed) cases.",
            "model_size": "n/a (benchmark)",
            "reasoning_methods": [
                "Cloze-style knowledge probing (masked token prediction)"
            ],
            "reasoning_methods_description": "Given a templated sentence with a masked token (e.g., 'Paris is the capital of &lt;mask&gt;'), the PLM must predict the correct token in the top-1 predictions; measures how much factual knowledge is directly stored in model parameters or can be extracted from context.",
            "diversity_of_methods": "Benchmark aggregates different factual probes; the models tested use their own internal training methods (MLM, KE injection, etc.) to perform the cloze task; LAMA itself is a single evaluation style.",
            "reasoning_task_name": "Knowledge probing: Google-RE, T-REx, ConceptNet, SQuAD subsets; and filtered LAMA-UHN variants",
            "reasoning_task_description": "Probe factual recall and robustness by asking masked-token questions derived from KG facts; LAMA-UHN reduces spurious easily-answered cases.",
            "performance_by_method": "Reported P@1: BERT_BASE (Google-RE / T-REx) = 9.8 / 31.1; Our RoBERTa = 7.0 / 23.2; KEPLER-Wiki = 7.3 / 24.6; KEPLER-W+W shows gains on some LAMA-UHN splits (e.g., Google-RE UHN=4.1, T-REx UHN=17.1). Overall, KEPLER improves over RoBERTa but often remains below BERT_BASE on this probe (attributed to tokenization/vocabulary differences).",
            "comparison_of_methods": "LAMA highlights differences between models that store knowledge vs. those that can extract it from descriptions; KEPLER improves P@1 over RoBERTa indicating better factual integration, but vocabulary/tokenization differences confound direct comparisons (BERT vs RoBERTa).",
            "key_findings": "Joint KE+MLM training increases factual recall in PLMs compared to vanilla RoBERTa, but evaluation via LAMA is sensitive to probe/template design and model vocabularies; LAMA-UHN shows that careful filtering changes relative rankings.",
            "counter_examples_or_negative_results": "RoBERTa-based models (including KEPLER) often underperform BERT_BASE on LAMA probes due to vocabulary/tokenization mismatch rather than clear inferiority of knowledge integration; some LAMA items can be answered via superficial clues so raw LAMA scores can be misleading.",
            "uuid": "e3330.6",
            "source_info": {
                "paper_title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "ERNIE: Enhanced language representation with informative entities",
            "rating": 2
        },
        {
            "paper_title": "Knowledge enhanced contextual word representations",
            "rating": 2
        },
        {
            "paper_title": "Matching the blanks: Distributional similarity for relation learning",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Representation learning of knowledge graphs with entity descriptions",
            "rating": 2
        },
        {
            "paper_title": "Translating embeddings for modeling multi-relational data",
            "rating": 2
        },
        {
            "paper_title": "RotatE: Knowledge graph embedding by relational rotation in complex space",
            "rating": 1
        }
    ],
    "cost": 0.022407999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</h1>
<p>Xiaozhi Wang ${ }^{1}$, Tianyu Gao ${ }^{3}$, Zhaocheng Zhu ${ }^{4,5}$, Zhengyan Zhang ${ }^{1}$<br>Zhiyuan Liu ${ }^{1,2 <em>}$, Juanzi Li ${ }^{1,2}$, and Jian Tang ${ }^{4,6,7 </em>}$<br>${ }^{1}$ Department of CST, BNRist; ${ }^{2}$ KIRC, Institute for AI, Tsinghua University, Beijing, China<br>{wangxz20,zy-z19}@mails.tsinghua.edu.cn<br>{liuzy,lijuanzi}@tsinghua.edu.cn<br>${ }^{3}$ Department of Computer Science, Princeton University, Princeton, NJ, USA<br>tianyug@princeton.edu<br>${ }^{4}$ Mila - QuÃ©bec AI Institute; ${ }^{5}$ UnivesitÃ© de MontrÃ©al; ${ }^{6}$ HEC, MontrÃ©al, Canada<br>zhaocheng.zhu@umontreal.ca, jian.tang@hec.ca<br>${ }^{7}$ CIFAR AI Research Chair</p>
<h4>Abstract</h4>
<p>Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata $5 \mathrm{M}^{1}$, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG /KEPLER.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 Introduction</h2>
<p>Recent pre-trained language representation models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c) learn effective language representation from large-scale unstructured corpora with language modeling objectives and have achieved superior performances on various natural language processing (NLP) tasks. Existing PLMs learn useful linguistic knowledge from unlabeled text (Liu et al., 2019a), but they generally cannot capture the world facts well, which are typically sparse and have complex forms in text (Petroni et al., 2019; Logan et al., 2019).</p>
<p>By contrast, knowledge graphs (KGs) contain extensive structural facts, and knowledge embedding (KE) methods (Bordes et al., 2013; Yang et al., 2015; Sun et al., 2019) can effectively embed them into continuous entity and relation embeddings. These embeddings can not only help with the KG completion but also benefit various NLP applications (Yang and Mitchell, 2017; Zaremoodi et al., 2018; Han et al., 2018a). As shown in Figure 1, textual entity descriptions contain abundant information. Intuitively, KE methods can provide factual knowledge for PLMs, while the informative text data can also benefit KE.</p>
<p>Inspired by Xie et al. (2016), we use entity descriptions to bridge the gap between KE and PLM, and align the semantic space of text to the symbol space of KGs (Logeswaran et al., 2019). We propose KEPLER, a unified model for Knowledge Embedding and Pre-trained LanguagE Representation. We encode the texts</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a KG with entity descriptions. The figure suggests that descriptions contain abundant information about entities and can help to represent the relational facts between them.
and entities into a unified semantic space with the same PLM as the encoder, and jointly optimize the KE and the masked language modeling (MLM) objectives. For the KE objective, we encode the entity descriptions as entity embeddings and then train them in the same way as conventional KE methods. For the MLM objective, we follow the approach of existing PLMs (Devlin et al., 2019; Liu et al., 2019c). KEPLER has the following strengths:</p>
<p>As a PLM, (1) KEPLER is able to integrate factual knowledge into language representation with the supervision from KG by the KE objective. (2) KEPLER inherits the strong ability of language understanding from PLMs by the MLM objective. (3) The KE objective enhances the ability of KEPLER to extract knowledge from text since it requires the model to encode the entities from their corresponding descriptions. (4) KEPLER can be directly adopted in a wide range of NLP tasks without additional inference overhead compared to conventional PLMs since we just add new training objectives without modifying model structures.</p>
<p>There are also some recent works (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020) directly integrating fixed entity embeddings into PLMs to provide external knowledge. However, (1) their entity embeddings are learned by a separate KE model, and thus cannot be easily aligned with the language representation space. (2) They require an entity linker to link the text to the corresponding entities, making them suffer from the error propagation problem. (3) Compared to vanilla PLMs, their sophisticated mechanisms to link and use entity embeddings lead to additional inference overhead.</p>
<p>As a KE model, (1) KEPLER can take full advantage of the abundant information from entity descriptions with the help of the MLM objective. (2) KEPLER is capable of performing KE in the inductive setting, that is, it can produce embeddings for unseen entities from their descriptions, while conventional KE methods are inherently transductive and they can only learn representations for the shown entities during training. Inductive KE is essential for many real-world applications, such as updating KGs with emerging entities and KG construction, and thus is worth more investigation.</p>
<p>For pre-training and evaluating KEPLER, we need a KG with (1) large amounts of knowledge facts, (2) aligned entity descriptions, and (3) reasonable inductive-setting data split, which cannot be satisfied by existing KE benchmarks. Therefore, we construct Wikidata5M, containing about 5M entities, 20M triplets, and aligned entity descriptions from Wikipedia. To the best of our knowledge, it is the largest general-domain KG dataset. We also benchmark several classical KE methods and give data splits for both the transductive and the inductive settings to facilitate future research.</p>
<p>To summarize, our contribution is three-fold: (1) We propose KEPLER, a knowledge-enhanced PLM by jointly optimizing the KE and MLM objectives, which brings great improvements on a wide range of NLP tasks. (2) By encoding text descriptions as entity embeddings, KEPLER shows its effectiveness as a KE model, especially in the inductive setting. (3) We also introduce Wikidata5M, a new large-scale KG dataset, which shall promote the research on large-scale KG, inductive KE, and the interactions between KG and NLP.</p>
<h2>2 KEPLER</h2>
<p>As shown in Figure 2, KEPLER implicitly incorporates factual knowledge into language representations by jointly training with two objectives. In this section, we detailedly introduce the encoder structure, the KE and MLM objectives, and how we combine the two as a unified model.</p>
<h3>2.1 Encoder</h3>
<p>For the text encoder, we use Transformer architecture (Vaswani et al., 2017) in the same way</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The KEPLER framework. We encode entity descriptions as entity embeddings and jointly train the knowledge embedding (KE) and masked language modeling (MLM) objectives on the same PLM.</p>
<p>as Devlin et al. (2019) and Liu et al. (2019c). The encoder takes a sequence of <em>N</em> tokens (<em>x</em><sub>1</sub>, ..., <em>x</em><sub>N</sub>) as inputs, and computes <em>L</em> layers of <em>d</em>-dimensional contextualized representations <strong>H</strong><sub><em>i</em></sub> âˆˆ â„<sup><em>N</em>Ã—<em>d</em></sup>, 1 â‰¤ <em>i</em> â‰¤ <em>L</em>. Each layer of the encoder <strong>E</strong><sub><em>i</em></sub> is a combination of a multihead self-attention network and a multilayer perceptron, and the encoder gets the representation of each layer by <strong>H</strong><sub><em>i</em></sub> = <strong>E</strong><sub><em>i</em></sub>(<strong>H</strong><sub><em>i</em>âˆ’1</sub>). Eventually, we get a contextualized representation for each position, which could be further used in downstream tasks. Usually, there is a special token <s> added to the beginning of the text, and the output at <s> is regarded sentence representation. We denote the representation function as <strong>E</strong><sub><s></sub>.</p>
<p>The encoder requires a tokenizer to convert plain texts into sequences of tokens. Here we use the same tokenization as RoBERTa: the Byte-Pair Encoding (BPE) (Sennrich et al., 2016).</p>
<p>Unlike previous knowledge-enhanced PLM works (Zhang et al., 2019; Peters et al., 2019), we do not modify the Transformer encoder structure to add external entity linkers or knowledge-integration layers. It means that our model has no additional inference overhead compared to vanilla PLMs, and it makes applying KEPLER in downstream tasks as easy as RoBERTa.</p>
<h3>2.2 Knowledge Embedding</h3>
<p>To integrate factual knowledge into KEPLER, we adopt the knowledge embedding (KE) objective in our pre-training. KE encodes entities and relations in knowledge graphs (KGs) as distributed representations, which benefits lots of downstream tasks, such as link prediction and relation extraction.</p>
<p>We first define KGs: A KG is a graph with entities as its nodes and relations between entities as its edges. We use a triplet (<em>h</em>, <em>r</em>, <em>t</em>) to describe a relational fact, where <em>h</em>, <em>t</em> are the head entity and the tail entity, and <em>r</em> is the relation type within a pre-defined relation set <strong>R</strong>. In conventional KE models, each entity and relation is assigned a <em>d</em>-dimensional vector, and a scoring function is defined for training the embeddings and predicting links.</p>
<p>In KEPLER, instead of using stored embeddings, we encode entities into vectors by using their corresponding text. By choosing different textual data and different KE scoring functions, we have multiple variants for the KE objective of KEPLER. In this paper, we explore three simple but effective ways: entity descriptions as embeddings, entity and relation descriptions as embeddings, and entity embeddings conditioned on relations. We leave exploring advanced KE methods as our future work.</p>
<p><strong>Entity Descriptions as Embeddings</strong> For a relational triplet (<em>h</em>, <em>r</em>, <em>t</em>), we have:</p>
<p>$$
\begin{aligned}
\mathbf{h} &amp;= \mathbf{E}<em h="h">{<s>}(\text{text}</em>), \
\mathbf{t} &amp;= \mathbf{E}<em t="t">{<s>}(\text{text}</em>), \
\mathbf{r} &amp;= \mathbf{T}_{r},
\end{aligned}
\tag{1}
$$</p>
<p>where text<sub><em>h</em></sub> and text<sub><em>t</em></sub> are the descriptions for <em>h</em> and <em>t</em>, with a special token <s> at the beginning. <strong>T</strong> âˆˆ â„<sup><em>|R</em>|Ã—<em>d</em></sup> is the relation embeddings and <strong>h</strong>, <strong>t</strong>, <strong>r</strong> are the embeddings for <em>h</em>, <em>t</em>, and <em>r</em>.</p>
<p>We use the loss from Sun et al. (2019) as our KE objective, which adopts negative sampling (Mikolov et al., 2013) for efficient optimization:</p>
<p>$$
\begin{array}{r}
\mathcal{L}<em r="r">{\mathrm{KE}}=-\log \sigma\left(\gamma-d</em>)\right) \
-\sum_{i=1}^{n} \frac{1}{n} \log \sigma\left(d_{r}\left(\mathbf{h}}(\mathbf{h}, \mathbf{t<em _mathbf_i="\mathbf{i">{\mathbf{i}}^{\prime}, \mathbf{t}</em>\right)-\gamma\right)
\end{array}
$$}}^{\prime</p>
<p>where $\left(h_{i}^{\prime}, r, t_{i}^{\prime}\right)$ are negative samples, $\gamma$ is the margin, $\sigma$ is the sigmoid function, and $d_{r}$ is the scoring function, for which we choose to follow TransE (Bordes et al., 2013) for its simplicity,</p>
<p>$$
d_{r}(\mathbf{h}, \mathbf{t})=|\mathbf{h}+\mathbf{r}-\mathbf{t}|_{p}
$$</p>
<p>where we take the norm $p$ as 1 . The negative sampling policy is to fix the head entity and randomly sample a tail entity, and vice versa.</p>
<p>Entity and Relation Descriptions as Embeddings A natural extension for the last method is to encode the relation descriptions as relation embeddings as well. Formally, we have,</p>
<p>$$
\hat{\mathbf{r}}=\mathrm{E}<em r="r">{<s>}\left(\text { text }</em>\right)
$$</p>
<p>where text $_{r}$ is the description for the relation $r$. Then we use $\hat{\mathbf{r}}$ to replace $\mathbf{r}$ in Equations 2 and 3.</p>
<p>Entity Embeddings Conditioned on Relations In this manner, we use entity embeddings conditioned on $r$ for better KE performances. The intuition is that semantics of an entity may have multiple aspects, and different relations focus on different ones (Lin et al., 2015). So we have,</p>
<p>$$
\mathbf{h}<em _wzxhzdk:40_="<s>">{r}=\mathrm{E}</em>\right)
$$}\left(\text { text }_{h, r</p>
<p>where text $<em r="r">{h, r}$ is the concatenation of the description for the entity $h$ and the description for the relation $r$, with the special token $<s>$ at the beginning and $&lt;/ s&gt;$ in between. Correspondingly, we use $\mathbf{h}</em>$ for Equations 2 and 3.}$ instead of $\mathbf{h</p>
<h3>2.3 Masked Language Modeling</h3>
<p>The masked language modeling (MLM) objective is inherited from BERT and RoBERTa. During pre-training, MLM randomly selects some of the input positions, and the objective is to predict the tokens at these selected positions within a fixed dictionary.</p>
<p>To be more specific, MLM randomly selects $15 \%$ of input positions, among which $80 \%$ are
masked with the special token <mask>, $10 \%$ are replaced by other random tokens, and the rest remain unchanged. For each selected position $j$, the last layer of the contextualized representation $\mathbf{H}<em _MLM="{MLM" _text="\text">{L, j}$ is used for a $W$-way classification, where $W$ is the size of the dictionary. At last, a crossentropy loss $\mathcal{L}</em>$ is calculated over these selected positions.}</p>
<p>We initialize our model with the pre-trained checkpoint of RoBERTa ${ }_{\text {BASE }}$. However, we still keep MLM as one of our objectives to avoid catastrophic forgetting (McCloskey and Cohen, 1989) while training towards the KE objective. Actually, as demonstrated in Section 5.1, only using the KE objective leads to poor results in NLP tasks.</p>
<h3>2.4 Training Objectives</h3>
<p>To incorporate factual knowledge and language understanding into one PLM, we design a multi-task loss as shown in Figure 2 and Equation 6,</p>
<p>$$
\mathcal{L}=\mathcal{L}<em _mathrm_MLM="\mathrm{MLM">{\mathrm{KE}}+\mathcal{L}</em>
$$}</p>
<p>where $\mathcal{L}<em _mathrm_MLM="\mathrm{MLM">{\mathrm{KE}}$ and $\mathcal{L}</em>$ are the losses for KE and MLM correspondingly. Jointly optimizing the two objectives can implicitly integrate knowledge from external KGs into the text encoder, while preserving the strong abilities of PLMs for syntactic and semantic understanding. Note that those two tasks only share the text encoder, and for each mini-batch, text data sampled for KE and MLM are not (necessarily) the same. This is because seeing a variety of text (instead of just entity descriptions) in MLM can help the model to have better language understanding ability.}</p>
<h3>2.5 Variants and Implementations</h3>
<p>We introduce the variants of KEPLER and the pretraining implementations here. The fine-tuning details will be introduced in Section 4.</p>
<h2>KEPLER Variants</h2>
<p>We implement multiple versions of KEPLER in experiments to explore the effectiveness of our pre-training framework. We use the same denotations in Section 4 as below.</p>
<p>KEPLER-Wiki is the principal model in our experiments, which adopts Wikidata5M (Section 3) as the KG and the entity-description-as-embedding method (Equation 1). All other variants, if not specified, use the same settings. KEPLER-Wiki achieves the best performances on most tasks.</p>
<p>KEPLER-WordNet uses the WordNet (Miller, 1995) as its KG source. WordNet is an English lexical graph, where nodes are lemmas and synsets, and edges are their relations. Intuitively, incorporating WordNet can bring lexical knowledge and thus benefits NLP tasks. We use the same WordNet 3.0 as in KnowBert (Peters et al., 2019), which is extracted from the nltk ${ }^{2}$ package.</p>
<p>KEPLER-W+W takes both Wikidata5M and WordNet as its KGs. To jointly train with two KG datasets, we modify the objective in Equation 6 as</p>
<p>$$
\mathcal{L}=\mathcal{L}<em _WordNet="{WordNet" _text="\text">{\text {Wiki }}+\mathcal{L}</em>
$$}}+\mathcal{L}_{\text {MLM }</p>
<p>where $\mathcal{L}<em _WordNet="{WordNet" _text="\text">{\text {Wiki }}$ and $\mathcal{L}</em>$ are losses from Wikidata5M and WordNet respectively.}</p>
<p>KEPLER-Rel uses the entity and relation descriptions as embeddings method (Equation 4). As the relation descriptions in Wikidata are short ( 11.7 words on average) and homogeneous, encoding relation descriptions as relation embeddings results in worse performance as shown in Section 4.</p>
<p>KEPLER-Cond uses the entity-embedding-conditioned-on-relation method (Equation 5). This model achieves superior results in link prediction tasks, both transductive and inductive (Section 4.3).</p>
<p>KEPLER-OnlyDesc trains the MLM objective directly on the entity descriptions from the KE objective rather than uses the English Wikipedia and BookCorpus as other versions of KEPLER. However, as the entity description data are smaller ( 2.3 GB vs 13 GB ) and homogeneous, it harms the general language understanding ability and thus performs worse (Section 4.2).</p>
<p>KEPLER-KE only adopts the KE objective in pre-training, which is an ablated version of KEPLER-Wiki. It is used to show the necessity of the MLM objective for language understanding.</p>
<h2>Pre-training Implementation</h2>
<p>In practice, we choose RoBERTa (Liu et al., 2019c) as our base model and implement KEPLER</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in the fairseq framework (Ott et al., 2019) for pretraining. Due to the computing resource limit, we choose the BASE size $(L=12, d=768)$ and use the released roberta. base parameters for initialization, which is a common practice to save pre-training time (Zhang et al., 2019; Peters et al., 2019). For the MLM objective, we use the English Wikipedia (2,500M words) and BookCorpus (800M words) (Zhu et al., 2015) as our pre-training corpora (except KEPLEROnlyDesc). We extract text from these two corpora in the same way as Devlin et al. (2019). For the KE objective, we encode the first 512 tokens of entity descriptions from the English Wikipedia as entity embeddings.</p>
<p>We set the $\gamma$ in Equation 2 as 4 and 9 for NLP and KE tasks respectively, and we use the models pre-trained with 10 and 30 epochs for NLP and KE. Specially, the $\gamma$ is 1 for KEPLER-WordNet. The two hyperparameters are tuned by multiple trials for $\gamma$ in ${1,2,4,6,9}$ and the number of epochs in ${5,10,20,30,40}$, and we select the model by performances on TACRED (F-1) and inductive link prediction (HITS@10). We use gradient accumulation to achieve a batch size of 12,288 .</p>
<h2>3 Wikidata5M</h2>
<p>As shown in Section 2, to train KEPLER, the KG dataset should (1) be large enough, (2) contain high-quality textual descriptions for its entities and relations, and (3) have a reasonable inductive setting, which most existing KG datasets do not provide. Thus, based on Wikidata ${ }^{3}$ and English Wikipedia, ${ }^{4}$ we construct Wikidata5M, a largescale KG dataset with aligned text descriptions from corresponding Wikipedia pages, and also an inductive test set. In the following sections, we first introduce the data collection (Section 3.1) and the data split (Section 3.2), and then provide the results of representative KE methods on the dataset (Section 3.3).</p>
<h3>3.1 Data Collection</h3>
<p>We use the July 2019 dump of Wikidata and Wikipedia. For each entity in Wikidata, we align it to its Wikipedia page and extract the first section as its description. Entities with no pages or with descriptions fewer than 5 words are discarded.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">#entity</th>
<th style="text-align: right;">#relation</th>
<th style="text-align: right;">#training</th>
<th style="text-align: right;">#validation</th>
<th style="text-align: right;">#test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FB15K</td>
<td style="text-align: right;">14,951</td>
<td style="text-align: right;">1,345</td>
<td style="text-align: right;">483,142</td>
<td style="text-align: right;">50,000</td>
<td style="text-align: right;">59,07</td>
</tr>
<tr>
<td style="text-align: left;">WN18</td>
<td style="text-align: right;">40,943</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">141,442</td>
<td style="text-align: right;">5,000</td>
<td style="text-align: right;">5,00</td>
</tr>
<tr>
<td style="text-align: left;">FB15K-237</td>
<td style="text-align: right;">14,541</td>
<td style="text-align: right;">237</td>
<td style="text-align: right;">272,115</td>
<td style="text-align: right;">17,535</td>
<td style="text-align: right;">20,466</td>
</tr>
<tr>
<td style="text-align: left;">WN18RR</td>
<td style="text-align: right;">40,943</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">86,835</td>
<td style="text-align: right;">3,034</td>
<td style="text-align: right;">3,134</td>
</tr>
<tr>
<td style="text-align: left;">Wikidata5M</td>
<td style="text-align: right;">$4,594,485$</td>
<td style="text-align: right;">822</td>
<td style="text-align: right;">$20,614,279$</td>
<td style="text-align: right;">5,163</td>
<td style="text-align: right;">5,133</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of Wikidata5M (transductive setting) compared with existing KE benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity Type</th>
<th style="text-align: right;">Occurrence</th>
<th style="text-align: right;">Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: right;">$1,517,591$</td>
<td style="text-align: right;">$33.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Taxon</td>
<td style="text-align: right;">363,882</td>
<td style="text-align: right;">$7.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Wikimedia list</td>
<td style="text-align: right;">118,823</td>
<td style="text-align: right;">$2.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Film</td>
<td style="text-align: right;">114,266</td>
<td style="text-align: right;">$2.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Human Settlement</td>
<td style="text-align: right;">110,939</td>
<td style="text-align: right;">$2.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">$2,225,501$</td>
<td style="text-align: right;">$48.4 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Top-5 entity categories in Wikidata5M.</p>
<p>We retrieve all the relational facts in Wikidata. A fact is considered to be valid when both of its entities are not discarded, and its relation has a non-empty page in Wikidata. The final KG contains $4,594,485$ entities, 822 relations and $20,624,575$ triplets. Statistics of Wikidata5M along with four other widely used benchmarks are shown in Table 1. Top-5 entity categories are listed in Table 2. We can see that Wikidata5M is much larger than other KG datasets, covering various domains.</p>
<h3>3.2 Data Split</h3>
<p>For Wikidata5M, we take two different settings: the transductive setting and the inductive setting.</p>
<p>The transductive setting (shown in Table 1) is adopted in most KG datasets, where the entities are shared and the triplet sets are disjoint across training, validation and test. In this case, KE models are expected to learn effective entity embeddings only for the entities in the training set. In the inductive setting (shown in Table 3), the entities and triplets are mutually disjoint across training, validation and test. We randomly sample some connected subgraphs as the validation and test set. In the inductive setting, the KE models should produce embeddings for the unseen entities given side features like descriptions, neighbors, etc. The inductive setting is more challenging and also</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">#entity</th>
<th style="text-align: center;">#relation</th>
<th style="text-align: center;">#triplet</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training</td>
<td style="text-align: center;">4,579,609</td>
<td style="text-align: center;">822</td>
<td style="text-align: center;">20,496,514</td>
</tr>
<tr>
<td style="text-align: center;">Validation</td>
<td style="text-align: center;">7,374</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">6,699</td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">7,475</td>
<td style="text-align: center;">201</td>
<td style="text-align: center;">6,894</td>
</tr>
</tbody>
</table>
<p>Table 3: Statistics of Wikidata5M inductive setting.
meaningful in real-world applications, where entities in KGs experience open-ended growth, and the inductive ability is crucial for online KE methods.</p>
<p>Although Wikidata5M contains massive entities and triplets, our validation and test set are not large, which is limited by the standard evaluation method of link prediction (Section 3.3). Each episode of evaluation requires $|\mathcal{E}| \times|\mathcal{T}| \times 2$ times of KE score calculation, where $|\mathcal{E}|$ and $|\mathcal{T}|$ are the total number of entities and the number of triplets in test set respectively. As Wikidata5M contains massive entities, the evaluation is very time-consuming, hence we have to limit the test set to thousands of triplets to ensure tractable evaluations. This indicates that large-scale KE urges a more efficient evaluation protocol. We will leave exploring it to future work.</p>
<h3>3.3 Benchmark</h3>
<p>To assess the challenges of Wikidata5M, we benchmark several popular KE models on our dataset in the transductive setting (as they inherently do not support the inductive setting). Because their original implementations do not scale to Wikidata5M, we benchmark these methods with GraphVite (Zhu et al., 2019), a multi-GPU KE toolkit.</p>
<p>In the transductive setting, for each test triplet $(h, r, t)$, the model ranks all the entities by scoring $\left(h, r, t^{\prime}\right), t^{\prime} \in \mathcal{E}$, where $\mathcal{E}$ is the entity set excluding other correct $t$. The evaluation metrics, MRR (mean reciprocal rank), MR (mean rank), and HITS@ ${1,3,10}$, are based on the rank of the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">MR</th>
<th style="text-align: center;">MRR</th>
<th style="text-align: center;">HITS@1</th>
<th style="text-align: center;">HITS@3</th>
<th style="text-align: center;">HITS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TransE (Bordes et al., 2013)</td>
<td style="text-align: right;">109370</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">39.2</td>
</tr>
<tr>
<td style="text-align: left;">DistMult (Yang et al., 2015)</td>
<td style="text-align: right;">211030</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: left;">ComplEx (Trouillon et al., 2016)</td>
<td style="text-align: right;">244540</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">37.3</td>
</tr>
<tr>
<td style="text-align: left;">SimplE (Kazemi and Poole, 2018)</td>
<td style="text-align: right;">115263</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: left;">RotatE (Sun et al., 2019)</td>
<td style="text-align: right;">89459</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">39.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of different KE models on Wikidata5M (\% except MR).
correct tail entity $t$ among all the entities in $\mathcal{E}$. Then we do the same thing for the head entities. We report the average results over all test triplets and over both head and tail entity predictions.</p>
<p>Table 4 shows the results of popular KE methods on Wikidata5M, which are all significantly lower than on existing KG datasets like FB15K237, WN18RR, and so forth. It demonstrates that Wikidata5M is more challenging due to its large scale and high coverage. The results advocate for more efforts towards large-scale KE.</p>
<h2>4 Experiments</h2>
<p>In this section, we introduce the experiment settings and results of our model on various NLP and KE tasks, along with some analyses on KEPLER.</p>
<h3>4.1 Experimental Setting</h3>
<p>Baselines In our experiments, RoBERTa is an important baseline since KEPLER is based on it (all mentioned models are of BASE size if not specified). As we cannot afford the full RoBERTa corpora ( 126 GB , and we only use 13 GB ) in KEPLER pre-training, we implement Our RoBERTa for direct comparisons to KEPLER. It is initialized by RoBERTa ${ }_{\text {BASE }}$ and is further trained with the MLM objective on the same corpora as KEPLER.</p>
<p>We also evaluate recent knowledge-enhanced PLMs, including ERNIE $<em _BERT="{BERT" _text="\text">{\text {BERT }}$ (Zhang et al., 2019) and KnowBert ${ }</em>$ (Peters et al., 2019). As ERNIE and our principal model KEPLER-Wiki only use Wikidata, we take KnowBert-Wiki in the experiments to ensure fair comparisons with the same knowledge source. Considering KEPLER is based on RoBERTa, we reproduce the two models with RoBERTa too (ERNIE $}<em _RoBERTa="{RoBERTa" _text="\text">{\text {RoBERTa }}$ and KnowBert $</em>$}}$ ). The reproduction of KnowBert is based on its original implementation. ${ }^{5</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>On relation classification, we also compare with MTB (Baldini Soares et al., 2019), which adopts "matching the blank" pre-training. Different from other baselines, the original MTB is based on BERT ${ }<em _LARGE="{LARGE" _text="\text">{\text {LARGE }}$ (denoted by MTB (BERT ${ }</em>$ (MTB).}}$ )). For a fair comparison under the same model size, we reimplement MTB with BERT ${ }_{\text {BASE }</p>
<p>Hyperparameter The pre-training settings are in Section 2.5. For fine-tuning on downstream tasks, we set KEPLER hyperparameters the same as reported in KnowBert on TACRED and OpenEntity. On FewRel, we set the learning rate as $2 \mathrm{e}-5$ and batch size as 20 and 4 for the Proto and PAIR frameworks respectively. For GLUE, we follow the hyperparameters reported in RoBERTa. For baselines, we keep their original hyperparameters unchanged or use the best trial in KEPLER searching space if no original settings are available.</p>
<h3>4.2 NLP Tasks</h3>
<p>In this section, we demonstrate the performance of KEPLER and its baselines on various NLP tasks.</p>
<h2>Relation Classification</h2>
<p>Relation classification requires models to classify relation types between two given entities from text. We evaluate KEPLER and other baselines on two widely used benchmarks: TACRED and FewRel.</p>
<p>TACRED (Zhang et al., 2017) has 42 relations and 106,264 sentences. Here we follow the settings of Baldini Soares et al. (2019), where we add four special tokens before and after the two entity mentions, and concatenate the representations at the beginnings of the two entities for classification. Note that the original KnowBert also takes entity types as inputs, which is different from Zhang et al. (2019); Baldini Soares et al. (2019). To ensure fair comparisons, we re-evaluate KnowBert with the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F - 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: left;">BERT $_{\text {LARGE }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: left;">MTB</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: left;">MTB (BERT $_{\text {LARGE }}$ )</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.5</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE $_{\text {BERT }}$</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: left;">KnowBert $_{\text {BERT }}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">68.5</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">KnowBert $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">$\mathbf{7 2 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-WordNet</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">71.3</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-W+W</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">71.5</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Rel</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">71.1</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Cond</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-OnlyDesc</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">70.7</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-KE</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">62.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Precision, recall, and F-1 on TACRED (\%). KnowBert results are different from the original paper since different task settings are used.
same setting as other baselines, thus the reported results are different from the original paper.</p>
<p>From the TACRED results in Table 5, we can observe that: (1) KEPLER-Wiki is the best one among KEPLER variants and significantly outperforms all the baselines, while other versions of KEPLER also achieve good results. It demonstrates the effectiveness of KEPLER on integrating factual knowledge into PLMs. Based on the result, we use KEPLER-Wiki as the principal model in the following experiments. (2) KEPLERWordNet shows a marginal improvement over Our RoBERTa, while KEPLER-W+W underperforms KEPLER-Wiki. It suggests that pre-training with WordNet only has limited benefits in the KEPLER framework. We will explore how to better combine different KGs in our future work.</p>
<p>FewRel (Han et al., 2018b) is a few-shot relation classification dataset with 100 relations and 70,000 instances, which is constructed with Wikipedia text and Wikidata facts. Furthermore, Gao et al. (2019) propose FewRel 2.0, adding a domain adaptation challenge with a new medical-domain test set.</p>
<p>FewRel takes the $N$-way $K$-shot setting. Relations in the training and test sets are disjoint.</p>
<p>For every evaluation episode, $N$ relations, $K$ supporting samples for each relation, and several query sentences are sampled from the test set. The models are required to classify queries into one of the $N$ relations only given the sampled $N \times K$ instances.</p>
<p>We use two state-of-the-art few-shot frameworks: Proto (Snell et al., 2017) and PAIR (Gao et al., 2019). We replace the text encoders with our baselines and KEPLER and compare the performance. Because FewRel 1.0 is constructed with Wikidata, we remove all the triplets in its test set from Wikidata5M to avoid information leakage for KEPLER. However, we cannot control the KGs used in our baselines. We mark the models utilizing Wikidata and have information leakage risk with ${ }^{1}$ in Table 6.</p>
<p>As Table 6 shows, KEPLER-Wiki achieves the best performance over the BASE-size PLMs in most settings. From the results, we also have some interesting observations: (1) RoBERTa consistently outperforms BERT on various NLP tasks (Liu et al., 2019c), yet the RoBERTa-based models here are comparable or even worse than BERT-based models in the PAIR framework. Because PAIR uses sentence concatenation, this result may be credited to the next sentence prediction (NSP) objective of BERT. (2) KEPLER brings improvements on FewRel 2.0, while ERNIE and KnowBert even degenerate in most of the settings. It indicates that the paradigms of ERNIE and KnowBert cannot well generalize to new domains which may require much different entity linkers and entity embeddings. On the other hand, KEPLER not only learns better entity representations but also acquires a general ability to extract factual knowledge from the context across different domains. We further verify this in Section 5.5. (3) KnowBert underperforms ERNIE in FewRel while it typically achieves better results on other tasks. This may be because it uses the TuckER (Balazevic et al., 2019) KE model while ERNIE and KEPLER follow TransE (Bordes et al., 2013). We will explore the effects of different KE methods in the future.</p>
<p>We also have another two observations with regard to ERNIE and MTB: (1) ERNIE performs the best on 1-shot settings of FewRel 1.0. We believe this is because that the knowledge embedding injection of ERNIE has particular advantages in this case, since it directly brings</p>
<p>knowledge about entities. When using 5-shot (supporting text provides more information) and FewRel 2.0 (ERNIE does not have knowledge for biomedical entities), KEPLER outperforms ERNIE. (2) Though MTB (BERT ${ }<em _BASE="{BASE" _text="\text">{\text {LARGE }}$ ) is the state-of-the-art model on FewRel, its BERT ${ }</em>$ version does not outperform other knowledgeenhanced PLMs, which suggests that using large models contributes much to its gain. We also notice that when combined with PAIR, MTB suffers an obvious performance drop, which may be because its pre-training objective degenerates sentence-pair tasks.}</p>
<h2>Entity Typing</h2>
<p>Entity typing requires to classify given entity mentions into pre-defined types. For this task, we carry out evaluations on OpenEntity (Choi et al., 2018) following the settings in Zhang et al. (2019). OpenEntity has 6 entity types and 2,000 instances for training, validation and test each.</p>
<p>To identify the entity mentions of interest, we add two special tokens before and after the entity spans, and use the representations of the first special tokens for classification. As shown in Table 7, KEPLER-Wiki achieves state-of-theart results. Note that the KnowBert results are different from the original paper since we use KnowBert-Wiki here rather than KnowBert-W+W to ensure the same knowledge resource and fair comparisons. KEPLER does not perform linking or entity embedding pre-training like ERNIE and KnowBert, which bring them special advantages in entity span tasks. However, KEPLER still outperforms these baselines, which proves its effectiveness.</p>
<h2>GLUE</h2>
<p>The General Language Understanding Evaluation (GLUE) (Wang et al., 2019b) collects several natural language understanding tasks and is widely used for evaluating PLMs. In general, solving GLUE does not require factual knowledge (Zhang et al., 2019) and we use it to examine whether KEPLER harms the general language understanding ability.</p>
<p>Table 8 shows the GLUE results. We can observe that KEPLER-Wiki is close to Our RoBERTa, suggesting that while incorporating factual knowledge, KEPLER maintains a strong language understanding ability. However, there
are significant performance drops of KEPLEROnlyDesc, which indicates that the small-scale entity description data are not sufficient for training KEPLER with MLM.</p>
<p>For the small datasets STS-B, MRPC and RTE, directly fine-tuning models on them typically result in unstable performance. Hence we finetune models on a large-scale dataset (here we use MNLI) first and then further fine-tune them on the small datasets. The method has been shown to be effective (Wang et al., 2019a) and is also used in the original RoBERTa paper (Liu et al., 2019c).</p>
<h3>4.3 KE Tasks</h3>
<p>We show how KEPLER works as a KE model, and evaluate it on Wikidata5M in both the transductive link prediction setting and the inductive setting.</p>
<h2>Experimental Settings</h2>
<p>In link prediction, the entity and relation embeddings of KEPLER are obtained as described in Section 2.2 and 2.5. The evaluation method is described in Section 3.3. We also add RoBERTa and Our RoBERTa as baselines. They adopt Equations 1 and 4 to acquire entity and relation embeddings, and use Equation 3 as their scoring function.</p>
<p>In the transductive setting, we compare our models with TransE (Bordes et al., 2013). We set its dimension as 512 , negative sampling size as 64 , batch size as 2048 , and learning rate as 0.001 after hyperparameter searching. The negative sampling size is crucial for the performance on KE tasks, but limited by the model complexity, KEPLER can only take a negative size of 1 . For a direct comparison to intuitively show the benefits of pretraining, we set a baseline TransE ${ }^{1}$, which also uses 1 as the negative sampling size and keeps the other hyperparameters unchanged.</p>
<p>Because conventional KE methods like TransE inherently cannot provide embeddings for unseen entities, we take DKRL (Xie et al., 2016) as our baseline in the KE experiments, which utilizes convolutional neural networks to encode entity descriptions as embeddings. We set its dimension as 768 , negative sampling size as 64 , batch size as 1024, and learning rate as 0.0005 .</p>
<h2>Transductive Setting</h2>
<p>Table 9a shows the results of the transductive setting. We observe that:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">FewRel 1.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FewRel 2.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">5-5</td>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">10-5</td>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">5-5</td>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">10-5</td>
</tr>
<tr>
<td style="text-align: center;">MTB $\left(\right.$ BERT $\left._{\text {LARGE }}\right)^{\dagger}$</td>
<td style="text-align: center;">93.86</td>
<td style="text-align: center;">97.06</td>
<td style="text-align: center;">89.20</td>
<td style="text-align: center;">94.27</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">Proto (BERT)</td>
<td style="text-align: center;">80.68</td>
<td style="text-align: center;">89.60</td>
<td style="text-align: center;">71.48</td>
<td style="text-align: center;">82.89</td>
<td style="text-align: center;">40.12</td>
<td style="text-align: center;">51.50</td>
<td style="text-align: center;">26.45</td>
<td style="text-align: center;">36.93</td>
</tr>
<tr>
<td style="text-align: center;">Proto (MTB)</td>
<td style="text-align: center;">81.39</td>
<td style="text-align: center;">91.05</td>
<td style="text-align: center;">71.55</td>
<td style="text-align: center;">83.47</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">76.67</td>
<td style="text-align: center;">48.28</td>
<td style="text-align: center;">69.75</td>
</tr>
<tr>
<td style="text-align: center;">Proto $\left(\right.$ ERNIE $_{\text {BERT }}$ ) ${ }^{\dagger}$</td>
<td style="text-align: center;">89.43</td>
<td style="text-align: center;">94.66</td>
<td style="text-align: center;">84.23</td>
<td style="text-align: center;">90.83</td>
<td style="text-align: center;">49.40</td>
<td style="text-align: center;">65.55</td>
<td style="text-align: center;">34.99</td>
<td style="text-align: center;">49.68</td>
</tr>
<tr>
<td style="text-align: center;">Proto (KnowBert ${ }_{\text {BERT }}$ ) ${ }^{\dagger}$</td>
<td style="text-align: center;">86.64</td>
<td style="text-align: center;">93.22</td>
<td style="text-align: center;">79.52</td>
<td style="text-align: center;">88.35</td>
<td style="text-align: center;">64.40</td>
<td style="text-align: center;">79.87</td>
<td style="text-align: center;">51.66</td>
<td style="text-align: center;">69.71</td>
</tr>
<tr>
<td style="text-align: center;">Proto (RoBERTa)</td>
<td style="text-align: center;">85.78</td>
<td style="text-align: center;">95.78</td>
<td style="text-align: center;">77.65</td>
<td style="text-align: center;">92.26</td>
<td style="text-align: center;">64.65</td>
<td style="text-align: center;">82.76</td>
<td style="text-align: center;">50.80</td>
<td style="text-align: center;">71.84</td>
</tr>
<tr>
<td style="text-align: center;">Proto (Our RoBERTa)</td>
<td style="text-align: center;">84.42</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">76.43</td>
<td style="text-align: center;">91.74</td>
<td style="text-align: center;">61.98</td>
<td style="text-align: center;">83.11</td>
<td style="text-align: center;">48.56</td>
<td style="text-align: center;">72.19</td>
</tr>
<tr>
<td style="text-align: center;">Proto $\left(\right.$ ERNIE $_{\text {RoBERTa }}$ ) ${ }^{\dagger}$</td>
<td style="text-align: center;">87.76</td>
<td style="text-align: center;">95.62</td>
<td style="text-align: center;">80.14</td>
<td style="text-align: center;">91.47</td>
<td style="text-align: center;">54.43</td>
<td style="text-align: center;">80.48</td>
<td style="text-align: center;">37.97</td>
<td style="text-align: center;">66.26</td>
</tr>
<tr>
<td style="text-align: center;">Proto (KnowBert ${ }_{\text {RoBERTa }}{ }^{\dagger}$</td>
<td style="text-align: center;">82.39</td>
<td style="text-align: center;">93.62</td>
<td style="text-align: center;">76.21</td>
<td style="text-align: center;">88.57</td>
<td style="text-align: center;">55.68</td>
<td style="text-align: center;">71.82</td>
<td style="text-align: center;">41.90</td>
<td style="text-align: center;">58.55</td>
</tr>
<tr>
<td style="text-align: center;">Proto (KEPLER-Wiki)</td>
<td style="text-align: center;">88.30</td>
<td style="text-align: center;">95.94</td>
<td style="text-align: center;">81.10</td>
<td style="text-align: center;">92.67</td>
<td style="text-align: center;">66.41</td>
<td style="text-align: center;">84.02</td>
<td style="text-align: center;">51.85</td>
<td style="text-align: center;">73.60</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (BERT)</td>
<td style="text-align: center;">88.32</td>
<td style="text-align: center;">93.22</td>
<td style="text-align: center;">80.63</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">67.41</td>
<td style="text-align: center;">78.57</td>
<td style="text-align: center;">54.89</td>
<td style="text-align: center;">66.85</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (MTB)</td>
<td style="text-align: center;">83.01</td>
<td style="text-align: center;">87.64</td>
<td style="text-align: center;">73.42</td>
<td style="text-align: center;">78.47</td>
<td style="text-align: center;">46.18</td>
<td style="text-align: center;">70.50</td>
<td style="text-align: center;">36.92</td>
<td style="text-align: center;">55.17</td>
</tr>
<tr>
<td style="text-align: center;">PAIR $\left(\right.$ ERNIE $_{\text {BERT }}$ ) ${ }^{\dagger}$</td>
<td style="text-align: center;">92.53</td>
<td style="text-align: center;">94.27</td>
<td style="text-align: center;">87.08</td>
<td style="text-align: center;">89.13</td>
<td style="text-align: center;">56.18</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">43.40</td>
<td style="text-align: center;">54.35</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (KnowBert ${ }_{\text {BERT }}$ ) ${ }^{\dagger}$</td>
<td style="text-align: center;">88.48</td>
<td style="text-align: center;">92.75</td>
<td style="text-align: center;">82.57</td>
<td style="text-align: center;">86.18</td>
<td style="text-align: center;">66.05</td>
<td style="text-align: center;">77.88</td>
<td style="text-align: center;">50.86</td>
<td style="text-align: center;">67.19</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (RoBERTa)</td>
<td style="text-align: center;">89.32</td>
<td style="text-align: center;">93.70</td>
<td style="text-align: center;">82.49</td>
<td style="text-align: center;">88.43</td>
<td style="text-align: center;">66.78</td>
<td style="text-align: center;">81.84</td>
<td style="text-align: center;">53.99</td>
<td style="text-align: center;">70.85</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (Our RoBERTa)</td>
<td style="text-align: center;">89.26</td>
<td style="text-align: center;">93.71</td>
<td style="text-align: center;">83.32</td>
<td style="text-align: center;">89.02</td>
<td style="text-align: center;">63.22</td>
<td style="text-align: center;">77.66</td>
<td style="text-align: center;">49.28</td>
<td style="text-align: center;">65.97</td>
</tr>
<tr>
<td style="text-align: center;">PAIR $\left(\right.$ ERNIE $_{\text {RoBERTa }}{ }^{\dagger}$</td>
<td style="text-align: center;">87.46</td>
<td style="text-align: center;">94.11</td>
<td style="text-align: center;">81.68</td>
<td style="text-align: center;">87.83</td>
<td style="text-align: center;">59.29</td>
<td style="text-align: center;">72.91</td>
<td style="text-align: center;">48.51</td>
<td style="text-align: center;">60.26</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (KnowBert ${ }_{\text {RoBERTa }}$ ) ${ }^{\dagger}$</td>
<td style="text-align: center;">85.05</td>
<td style="text-align: center;">91.34</td>
<td style="text-align: center;">76.04</td>
<td style="text-align: center;">85.25</td>
<td style="text-align: center;">50.68</td>
<td style="text-align: center;">66.04</td>
<td style="text-align: center;">37.10</td>
<td style="text-align: center;">51.13</td>
</tr>
<tr>
<td style="text-align: center;">PAIR (KEPLER-Wiki)</td>
<td style="text-align: center;">90.31</td>
<td style="text-align: center;">94.28</td>
<td style="text-align: center;">85.48</td>
<td style="text-align: center;">90.51</td>
<td style="text-align: center;">67.23</td>
<td style="text-align: center;">82.09</td>
<td style="text-align: center;">54.32</td>
<td style="text-align: center;">71.01</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracies (\%) on the FewRel dataset. $N-K$ indicates the $N$-way $K$-shot setting. MTB uses the LARGE size and all the other models use the BASE size. ${ }^{\dagger}$ indicates oracle models which may have seen facts in the FewRel 1.0 test set during pre-training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F - 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UFET (Choi et al., 2018)</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">68.0</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE $_{\text {BERT }}$</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">KnowBert $_{\text {BERT }}$</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">74.4</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">75.4</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: left;">KnowBert $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Entity typing results on OpenEntity (\%).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MNLI (m/mm) <br> $\mathbf{3 9 2 K}$</th>
<th style="text-align: center;">QQP <br> $\mathbf{3 6 3 K}$</th>
<th style="text-align: center;">$\mathbf{Q N L I}$ <br> $\mathbf{1 0 4 K}$</th>
<th style="text-align: center;">$\mathbf{S S T - 2}$ <br> $\mathbf{6 7 K}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">$87.5 / 87.2$</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">94.8</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">$87.1 / 86.8$</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">94.7</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">$87.2 / 86.5$</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-OnlyDesc</td>
<td style="text-align: center;">$85.9 / 85.6$</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.4</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$\mathbf{C o L A}$ <br> $\mathbf{8 . 5 K}$</td>
<td style="text-align: center;">$\mathbf{S T S - B}$ <br> $\mathbf{5 . 7 K}$</td>
<td style="text-align: center;">$\mathbf{M R P C}$ <br> $\mathbf{3 . 5 K}$</td>
<td style="text-align: center;">$\mathbf{R T E}$ <br> $\mathbf{2 . 5 K}$</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">80.9</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">82.3</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-OnlyDesc</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">78.3</td>
</tr>
</tbody>
</table>
<p>Table 8: GLUE results on the dev set (\%). All the results are medians over 5 runs. We report F-1 scores for QQP and MRPC, Spearman correlations for STS-B, and accuracy scores for the other tasks. The ' $\mathrm{m} / \mathrm{mm}$ '' stands for matched/mismatched evaluation sets for MNLI (Williams et al., 2018).
representations and textual entity descriptions so that outperform TransE. In the future, we will explore reducing the model size of KEPLER to take advantage of both large negative sampling size and pre-training.
(2) The vanilla RoBERTa perform poorly in KE while KEPLER achieves favorable performances,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">MR</th>
<th style="text-align: right;">MRR</th>
<th style="text-align: right;">HITS@1</th>
<th style="text-align: right;">HITS@3</th>
<th style="text-align: right;">HITS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TransE (Bordes et al., 2013)</td>
<td style="text-align: right;">109370</td>
<td style="text-align: right;">$\mathbf{2 5 . 3}$</td>
<td style="text-align: right;">17.0</td>
<td style="text-align: right;">$\mathbf{3 1 . 1}$</td>
<td style="text-align: right;">$\mathbf{3 9 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">TransE ${ }^{\dagger}$</td>
<td style="text-align: right;">406957</td>
<td style="text-align: right;">6.0</td>
<td style="text-align: right;">1.8</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">13.6</td>
</tr>
<tr>
<td style="text-align: left;">DKRL (Xie et al., 2016)</td>
<td style="text-align: right;">31566</td>
<td style="text-align: right;">16.0</td>
<td style="text-align: right;">12.0</td>
<td style="text-align: right;">18.1</td>
<td style="text-align: right;">22.9</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: right;">1381597</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: right;">1756130</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-KE</td>
<td style="text-align: right;">76735</td>
<td style="text-align: right;">8.2</td>
<td style="text-align: right;">4.9</td>
<td style="text-align: right;">8.9</td>
<td style="text-align: right;">15.1</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Rel</td>
<td style="text-align: right;">15820</td>
<td style="text-align: right;">6.6</td>
<td style="text-align: right;">3.7</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">11.7</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: right;">$\mathbf{1 4 4 5 4}$</td>
<td style="text-align: right;">15.4</td>
<td style="text-align: right;">10.5</td>
<td style="text-align: right;">17.4</td>
<td style="text-align: right;">24.4</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Cond</td>
<td style="text-align: right;">20267</td>
<td style="text-align: right;">21.0</td>
<td style="text-align: right;">$\mathbf{1 7 . 3}$</td>
<td style="text-align: right;">22.4</td>
<td style="text-align: right;">27.7</td>
</tr>
</tbody>
</table>
<p>(a) Transductive results on Wikidata5M (\% except MR). TransE ${ }^{\dagger}$ denotes a TransE modeled trained with the same negative sampling size (1) as KEPLER.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">MR</th>
<th style="text-align: right;">MRR</th>
<th style="text-align: right;">HITS@1</th>
<th style="text-align: right;">HITS@3</th>
<th style="text-align: right;">HITS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DKRL (Xie et al., 2016)</td>
<td style="text-align: right;">78</td>
<td style="text-align: right;">23.1</td>
<td style="text-align: right;">5.9</td>
<td style="text-align: right;">32.0</td>
<td style="text-align: right;">54.6</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: right;">723</td>
<td style="text-align: right;">7.4</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">19.6</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: right;">1070</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">1.9</td>
<td style="text-align: right;">6.3</td>
<td style="text-align: right;">13.0</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-KE</td>
<td style="text-align: right;">138</td>
<td style="text-align: right;">17.8</td>
<td style="text-align: right;">5.7</td>
<td style="text-align: right;">22.9</td>
<td style="text-align: right;">40.7</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Rel</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">33.4</td>
<td style="text-align: right;">15.9</td>
<td style="text-align: right;">43.5</td>
<td style="text-align: right;">66.1</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">35.1</td>
<td style="text-align: right;">15.4</td>
<td style="text-align: right;">46.9</td>
<td style="text-align: right;">71.9</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Cond</td>
<td style="text-align: right;">$\mathbf{2 8}$</td>
<td style="text-align: right;">$\mathbf{4 0 . 2}$</td>
<td style="text-align: right;">$\mathbf{2 2 . 2}$</td>
<td style="text-align: right;">$\mathbf{5 1 . 4}$</td>
<td style="text-align: right;">$\mathbf{7 3 . 0}$</td>
</tr>
</tbody>
</table>
<p>(b) Inductive results on Wikidata5M (\% except MR).</p>
<p>Table 9: Link prediction results on Wikidata5M transductive and inductive settings.
which demonstrates the effectiveness of our multitask pre-training to infuse factual knowledge.
(3) Among the KEPLER variants, KEPLERCond has superior results, which substantiates the intuition in Section 2.2. KEPLER-Rel performs worst, which we believe is due to the short and homogeneous relation descriptions of Wikidata. KEPLER-KE significantly underperforms KEPLER-Wiki, which suggests that the MLM objective is necessary as well for the KE tasks to build effective language representation.
(4) We also notice that DKRL performs well on the transductive setting and the result is close to KEPLER. We believe this is because DKRL takes a much smaller encoder (CNN) and thus is easier to train. In the more difficult inductive setting, the gap between DKRL and KEPLER is larger, which better shows the language understanding ability of KEPLER to utilize textual entity descriptions.</p>
<h2>Inductive Setting</h2>
<p>Table 9b shows the Wikidata5M inductive results. KEPLER outperforms DKRL and RoBERTa by a large margin, demonstrating the effectiveness of our joint training method. But KEPLER results are
still far from ideal performances required by practical applications (constructing KG from scratch, etc.), which urges further efforts on inductive KE. Comparisons among KEPLER variants are consistent with in the transductive setting.</p>
<p>In addition, we clarify why results in the inductive setting are much higher than the transductive setting, while the inductive setting is more difficult: As shown in Tables 1 and 3, the entities involved in the inductive evaluation is much less than the transductive setting ( 7,475 vs. $4,594,485$ ). Considering the KE evaluation metrics are based on entity ranking, it is reasonable to see higher values in the inductive setting. The performance in different settings should not be directly compared.</p>
<h2>5 Analysis</h2>
<p>In this section, we analyze the effectiveness and efficiency of KEPLER with experiments. All the hyperparameters are the same as reported in Section 4.1, including models in the ablation study.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{F - 1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-KE</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">72.0</td>
</tr>
</tbody>
</table>
<p>Table 10: Ablation study results on TACRED (\%).</p>
<h3>5.1 Ablation Study</h3>
<p>As shown in Equation 6, KEPLER takes a multitask loss. To demonstrate the effectiveness of the joint objective, we compare full KEPLER with models trained with only the MLM loss (Our RoBERTa) and only the KE loss (KEPLERKE) on TACRED. As demonstrated in Table 10, compared to KEPLER-Wiki, both ablation models suffer significant drops. It suggests that the performance gain of KEPLER is credited to the joint training towards both objectives.</p>
<h3>5.2 Knowledge Probing Experiment</h3>
<p>Section 4.2 shows that KEPLER can achieve significant improvements on NLP tasks requiring factual knowledge. To further verify whether KEPLER can better integrate factual knowledge into PLMs and help to recall them, we conduct experiments on LAMA (Petroni et al., 2019), a widely used knowledge probe. LAMA examines PLMs' abilities on recalling relational facts by cloze-style questions. For instance, given a natural language template "Paris is the capital of <mask>", PLMs are required to predict the masked token without fine-tuning. LAMA reports the micro-averaged precision at one (P@1) scores. However, Poerner et al. (2020) present that LAMA contains some easy questions which can be answered with superficial clues like entity names. Hence we also evaluate the models on LAMAUHN (Poerner et al., 2020), which filters out the questionable templates from the Google-RE and T-REx corpora of LAMA.</p>
<p>The evaluation results are shown in Table 11, from which we have the following observations: (1) KEPLER consistently outperforms the vanilla PLM baseline Our RoBERTa in almost all the settings except ConceptNet, which focuses on commonsense knowledge rather than factual knowledge. It indicates that KEPLER can indeed better integrate factual knowledge. (2) Although</p>
<p>KEPLER-W+W cannot outperform KEPLERWiki on NLP tasks (Section 4.2), it shows significant improvements in LAMA-UHN, which suggests that we should explore which kind of knowledge is needed on different scenarios in the future. (3) All the RoBERTa-based models perform worse than vanilla BERT ${ }_{\text {BASE }}$ by a large margin, which is consistent with the results of Wang et al. (2020). This may be due to different vocabularies used in BERT and RoBERTa, which presents the vulnerability of LAMA-style probing again (Kassner and SchÃ¼tze, 2020). We will leave developing a better knowledge probing framework as our future work.</p>
<h3>5.3 Running Time Comparison</h3>
<p>Compared to vanilla PLMs, KEPLER does not introduce any additional parameters or computations during fine-tuning and inference, which is efficient for practice use. We compare the running time of KEPLER and other knowledgeenhanced PLMs (ERNIE and KnowBert) in Table 12. The time is evaluated on TACRED training set for one epoch with one NVIDIA Tesla V100 ( 32 GB ), and all models use 32 batch size and 128 sequence length. The "entity linking" time of KnowBert is for entity candidate generation. We can observe that KEPLER requires much less running time since it does not need entity linking or entity embedding fusion, which will benefit time-sensitive applications.</p>
<h3>5.4 Correlation with Entity Frequency</h3>
<p>To better understand how KEPLER helps the entity-centric tasks, we provide analyses on the correlations between KEPLER performance and entity frequency in this section. The motivation is to verify a natural hypothesis that KEPLER improvements mainly come from better representing the entity mentions in text, especially the rare entities, which do not show up frequently in the pre-training corpora and thus cannot be well learned by the language modeling objectives.</p>
<p>We perform entity linking for the TACRED dataset with BLINK (Wu et al., 2020) to link the entity mentions in text to their corresponding Wikipedia identifiers. Then we count the occurrences of the entities in Wikipedia with the hyperlinks in rich text, denoting the entity frequencies. We conduct two experiments to analyze the correlations between KEPLER performance and entity frequency: (1) In Table 13, we divide the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">LAMA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LAMA-UHN</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">:--</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: left;">:--</td>
<td style="text-align: center;">Google-RE</td>
<td style="text-align: center;">T-REx</td>
<td style="text-align: center;">ConceptNet</td>
<td style="text-align: center;">SQuAD</td>
<td style="text-align: center;">Google-RE</td>
<td style="text-align: center;">T-REx</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">21.8</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">$\mathbf{1 9 . 0}$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">15.7</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">$\mathbf{7 . 3}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 6}$</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">$\mathbf{1 4 . 3}$</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-W+W</td>
<td style="text-align: center;">$\mathbf{7 . 3}$</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">$\mathbf{4 . 1}$</td>
<td style="text-align: center;">$\mathbf{1 7 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 11: P@1 results on knowledge probing benchmark LAMA and LAMA-UHN.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Entity <br> Linking</th>
<th style="text-align: center;">Fine- <br> tuning</th>
<th style="text-align: center;">Inference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ERNIE $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">780 s</td>
<td style="text-align: center;">730 s</td>
<td style="text-align: center;">194 s</td>
</tr>
<tr>
<td style="text-align: left;">KnowBert $_{\text {RoBERTa }}$</td>
<td style="text-align: center;">190 s</td>
<td style="text-align: center;">677 s</td>
<td style="text-align: center;">235 s</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER</td>
<td style="text-align: center;">$\mathbf{0 s}$</td>
<td style="text-align: center;">$\mathbf{5 0 8 s}$</td>
<td style="text-align: center;">$\mathbf{1 5 2 s}$</td>
</tr>
</tbody>
</table>
<p>Table 12: Three parts of running time for one epoch of TACRED training set.
entity mentions into five parts by their frequencies, and compare the TACRED performances while only keeping entities in one part and masking the other. (2) In Figure 3, we sequentially mask the entity mentions in the ascending order of entity frequencies and see the F-1 changes.</p>
<p>From the results, we can observe that:
(1) Figure 3 shows that when the entity masking rate is low, the improvements of KEPLER over RoBERTa are generally much higher than when the entity masking rate is high. It indicates that the improvements of KEPLER do mainly come from better modeling entities in context. However, even when all the entity mentions are masked, KEPLER still outperforms RoBERTa. We claim this is because the KE objective can also help to learn to understand fact-related text since it requires the model to recall facts from textual descriptions. This claim is further substantiated in Section 5.5.
(2) From Table 13, we can observe that the improvement in the " $0 \%-20 \%$ " setting is marginally higher than the other settings, which demonstrates that KEPLER does have special advantages on modeling rare entities compared to vanilla PLMs. But the improvements in the frequent settings are also significant and we cannot say that the overall improvements of KEPLER are mostly from the rare entities. In general, the results in Table 13 show that KEPLER can better model all the entities, no matter rare or frequent.</p>
<h3>5.5 Understanding Text or Storing Knowledge</h3>
<p>We argue that by jointly training the KE and the MLM objectives, KEPLER (1) can better understand fact-related text and better extract knowledge from text, and also (2) can remember factual knowledge. To investigate the two abilities of KEPLER in a quantitative aspect, we carry out an experiment on TACRED, in which the head and tail entity mentions are masked (masked-entity, ME ) or only head and tail entity mentions are shown (only-entity, OE). The ME setting shows to what extent the models can extract facts only from the textual context without the clues in entity names. The OE setting demonstrates to what extent the models can store and predict factual knowledge, as only the entity names are given to the models.</p>
<p>As shown in Table 14, KEPLER-Wiki shows significant improvements over Our RoBERTa in both settings, which suggests that KEPLER has indeed possessed superior abilities on both extracting and storing knowledge compared to vanilla PLMs without knowledge infusion. And the KEPLER-KE model performs poorly on the ME setting but achieves marginal improvements on the OE setting. It indicates that without the help of the MLM objective, KEPLER only learns the entity description embeddings and degenerates in general language understanding, while it can still remember knowledge into entity names to some extent.</p>
<h2>6 Related Work</h2>
<p>Pre-training in NLP There has been a long history of pre-training in NLP. Early works focus on distributed word representations (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014), many of which are often adopted in current models as word embeddings. These</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entity Frequency</th>
<th style="text-align: center;">$\mathbf{0 \% - 2 0 \%}$</th>
<th style="text-align: center;">$\mathbf{2 0 \% - 4 0 \%}$</th>
<th style="text-align: center;">$\mathbf{4 0 \% - 6 0 \%}$</th>
<th style="text-align: center;">$\mathbf{6 0 \% - 8 0 \%}$</th>
<th style="text-align: center;">$\mathbf{8 0 \% - 1 0 0 \%}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">68.5</td>
</tr>
<tr>
<td style="text-align: left;">Improvement</td>
<td style="text-align: center;">+0.6</td>
<td style="text-align: center;">+0.1</td>
<td style="text-align: center;">+0.3</td>
<td style="text-align: center;">+0.4</td>
<td style="text-align: center;">+0.3</td>
</tr>
</tbody>
</table>
<p>Table 13: F-1 scores on TACRED (\%) under different settings by entity frequencies. We sort the entity mentions in TACRED by their corresponding entity frequencies in Wikipedia. The " $0 \%-20 \%$ " setting indicates only keeping the least frequent $20 \%$ entity mentions and masking all the other entity mentions (for both training and validation), and so on. The results are averaged over 5 runs.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: TACRED performance (F-1) of KEPLER and RoBERTa change with the rate of entity mentions being masked.
pre-trained embeddings can capture the semantics of words from large-scale corpora and thus benefit NLP applications. Peters et al. (2018) push this trend a step forward by using a bidirectional LSTM to form contextualized word embeddings (ELMo) for richer semantic meanings under different circumstances.</p>
<p>Apart from word embeddings, there is another trend exploring pre-trained language models. Dai and Le (2015) propose to train an auto-encoder on unlabeled textual data and then fine-tune it on downstream tasks. Howard and Ruder (2018) propose a universal language model (ULMFiT). With the powerful Transformer architecture (Vaswani et al., 2017), Radford et al. (2018) demonstrate an effective pre-trained generative model (GPT). Later, Devlin et al. (2019) release a pre-trained deep Bidirectional Encoder Representation from Transformers (BERT), achieving state-of-the-art performance on a wide range of NLP benchmarks.</p>
<p>After BERT, similar PLMs spring up recently. Yang et al. (2019) propose a permutation language model (XLNet). Later, Liu et al. (2019c) show that</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">ME</th>
<th style="text-align: center;">OE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Our RoBERTa</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">46.8</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-KE</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">47.0</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER-Wiki</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">48.9</td>
</tr>
</tbody>
</table>
<p>Table 14: Masked-entity (ME) and onlyentity (OE) F-1 scores on TACRED (\%).</p>
<p>more data and more parameter tuning can benefit PLMs, and release a new state-of-the-art model (RoBERTa). Other works explore how to add more tasks (Liu et al., 2019b) and more parameters (Raffel et al., 2020; Lan et al., 2020) to PLMs.
Knowledge-Enhanced PLMs Recently, many works have investigated how to incorporate knowledge into PLMs. MTB (Baldini Soares et al., 2019) takes a straightforward "matching the blank" pre-training objective to help the relation classification task. ERNIE (Zhang et al., 2019) identifies entity mentions in text and links pre-processed knowledge embeddings to the corresponding positions, which shows improvements on several NLP benchmarks. With a similar idea as ERNIE, KnowBert (Peters et al., 2019) incorporates an integrated entity linker in their model and adopts end-to-end training. Besides, Logan et al. (2019) and Hayashi et al. (2020) utilize relations between entities inside one sentence to train better generation models. Xiong et al. (2019) adopt entity replacement knowledge learning for improving entity-related tasks.</p>
<p>Some contemporaneous or following works try to inject factual knowledge into PLMs in different ways. E-BERT (Poerner et al., 2020) aligns entity embeddings with word embeddings and then directly adds the aligned embeddings into BERT to avoid additional pre-training. K-Adapter (Wang et al., 2020) injects knowledge with additional neural adapters to support continuous learning.</p>
<p>Knowledge Embedding KE methods have been extensively studied. Conventional KE models define different scoring functions for relational triplets. For example, TransE (Bordes et al., 2013) treats tail entities as translations of head entities and uses $L_{1}$-norm or $L_{2}$-norm to score triplets, while DistMult (Yang et al., 2015) uses matrix multiplications and ComplEx (Trouillon et al., 2016) adopts complex operations based on it. RotatE (Sun et al., 2019) combines the advantages of both of them.</p>
<p>Inductive Embedding Above KE methods learn entity embeddings only from KG and are inherently transductive, while some works (Wang et al., 2014; Xie et al., 2016; Yamada et al., 2016; Cao et al., 2017; Shi and Weninger, 2018; Cao et al., 2018) incorporate textual metadata such as entity names or descriptions to enhance the KE methods and hence can do inductive KE to some extent. Besides KG, it is also common for general inductive graph embedding methods (Hamilton et al., 2017; Bojchevski and GÃ¼nnemann, 2018) to utilize additional node features like text attributes, degrees, and so on. KEPLER follows this line of studies and takes full advantage of textual information with an effective PLM.</p>
<p>Hamaguchi et al. (2017) and Wang et al. (2019c) perform inductive KE by aggregating the trained embeddings of the known neighboring nodes with graph neural networks, and thus do not need additional features. But these methods require the unseen nodes to be surrounded by known nodes and cannot embed new (sub)graphs. We leave how to develop KEPLER to do fully inductive KE without additional features as future work.</p>
<h2>7 Conclusion and Future Work</h2>
<p>In this paper, we propose KEPLER, a simple but effective unified model for knowledge embedding and pre-trained language representation. We train KEPLER with both the KE and MLM objectives to align the factual knowledge and language representation into the same semantic space, and experimental results on extensive tasks demonstrate its effectiveness on both NLP and KE applications. Besides, we propose Wikidata5M, a large-scale KG dataset to facilitate future research.</p>
<p>In the future, we will (1) explore advanced ways for more smoothly unifying the two semantic space, including different KE forms and different training objectives, and (2) investigate better
knowledge probing methods for PLMs to shed light on knowledge-integrating mechanisms.</p>
<h2>Acknowledgments</h2>
<p>This work is supported by the National Key Research and Development Program of China (No. 2018YFB1004503), the National Natural Science Foundation of China (NSFC No. U1736204, 61533018, 61772302, 61732008), grants from Institute for Guo Qiang, Tsinghua University (2019GQB0003), and Beijing Academy of Artificial Intelligence (BAAI2019ZD0502). Prof. Jian Tang is supported by the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant and the Canada CIFAR AI Chair Program. Xiaozhi Wang and Tianyu Gao are supported by Tsinghua University Initiative Scientific Research Program. We also thank our action editor, Prof. Doug Downey, and the anonymous reviewers for their consistent help and insightful suggestions.</p>
<h2>References</h2>
<p>Ivana Balazevic, Carl Allen, and Timothy Hospedales. 2019. TuckER: Tensor factorization for knowledge graph completion. In Proceedings of EMNLP-IJCNLP, pages 5185-5194. DOI: https://doi.org /10.18653/v1/D19-1522</p>
<p>Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. In Proceedings of ACL, pages 2895-2905. DOI: https://doi.org /10.18653/v1/P19-1279</p>
<p>Aleksandar Bojchevski and Stephan GÃ¼nnemann. 2018. Deep Gaussian embedding of graphs: Unsupervised inductive learning via ranking. In Proceedings of ICLR.</p>
<p>Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NIPS), pages 2787-2795.</p>
<p>Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Chengjiang Li, Xu Chen, and Tiansi Dong.</p>
<ol>
<li>Joint representation learning of crosslingual words and entities via attentive distant supervision. In Proceedings of EMNLP, pages 227-237. DOI: https://doi.org /10.18653/v1/D18-1021</li>
</ol>
<p>Yixin Cao, Lifu Huang, Heng Ji, Xu Chen, and Juanzi Li. 2017. Bridge text and knowledge by learning multi-prototype entity mention embedding. In Proceedings of ACL, pages 1623-1633. DOI: https://doi.org /10.18653/v1/P17-1149</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. In Proceedings of ACL, pages 87-96.</p>
<p>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML, pages $160-167$.</p>
<p>Andrew M. Dai and Quoc V. Le. 2015. Semisupervised sequence learning. In Advances in Neural Information Processing Systems (NIPS), pages 3079-3087.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186.</p>
<p>Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0: Towards more challenging fewshot relation classification. In Proceedings of EMNLP-IJCNLP, pages 6251-6256.</p>
<p>Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto. 2017. Knowledge transfer for out-of-knowledge-base entities: A graph neural network approach. In Proceedings of IJCAI, pages 1802-1808. DOI: https://doi.org/10.24963/ijcai .2017/250</p>
<p>William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (NIPS), pages 1025-1035.</p>
<p>Xu Han, Zhiyuan Liu, and Maosong Sun. 2018a. Neural knowledge acquisition via mutual attention between knowledge graph and text. In Proceedings of AAAI, pages 4832-4839.</p>
<p>Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018b. FewRel: A large-scale supervised fewshot relation classification dataset with state-of-the-art evaluation. In Proceedings of EMNLP, pages 4803-4809. DOI: https://doi.org /10.18653/v1/D18-1514</p>
<p>Hiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Graham Neubig. 2020. Latent relation language models. In Proceedings of AAAI, pages 7911-7918. DOI: https://doi.org /10.1609/aaai.v34i05.6298</p>
<p>Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of ACL, pages 328-339. DOI: https://doi.org/10 .18653/v1/P18-1031, PMID: 28889062</p>
<p>Nora Kassner and Hinrich SchÃ¼tze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of ACL, pages 7811-7818. DOI: https://doi.org/10.18653/v1/2020 .acl-main. 698</p>
<p>Seyed Mehran Kazemi and David Poole. 2018. SimplE embedding for link prediction in knowledge graphs. In Advances in Neural Information Processing Systems (NeurIPS), pages 4284-4295.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In Proceedings of ICLR.</p>
<p>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of AAAI, pages 2181-2187.</p>
<p>Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019a. Linguistic knowledge and transferabilityof</p>
<p>contextual representations. In Proceedings of NAACL-HLT, pages 1073-1094.</p>
<p>Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: Enabling language representation with knowledge graph. In Proceedings of AAAI, pages 2901-2908. DOI: https://doi.org/10.1609/aaai .v34i03.5681</p>
<p>Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019b. Multi-task deep neural networks for natural language understanding. In Proceedings of ACL, pages 4487-4496.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019c. RoBERTa: A robustly optimized BERT pretraining approach. CoRR, cs.CL/1907.11692v1.</p>
<p>Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Barack's Wife hillary: Using knowledge graphs for fact-aware language modeling. In Proceedings of ACL, pages 5962-5971. DOI: https://doi.org/10.18653/v1/P19 -1598</p>
<p>Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-shot entity linking by reading entity descriptions. In Proceedings of ACL, pages 3449-3460. DOI: https://doi.org/10.18653/v1/P19 -1335</p>
<p>Michael McCloskey and Neal J. Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and motivation, volume 24, pages 109-165. Elsevier. DOI: https://doi.org/10.1016/S0079 -7421(08)60536-8</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS), pages 3111-3119.</p>
<p>George A. Miller. 1995. WordNet: A lexical database for english. Communications of the ACM, 38(11):39-41. DOI: https://doi .org/10.1145/219717.219748</p>
<p>Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT (Demonstrations), pages 48-53. DOI: https://doi.org/10.18653/v1/N19 -4009</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of EMNLP, pages 1532-1543. DOI: https://doi.org/10.3115/v1/D14 -1162</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227-2237. DOI: https://doi.org/10.18653/v1/N18 -1202</p>
<p>Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of EMNLP-IJCNLP, pages 43-54. DOI: https://doi.org/10.18653/v1/D19 -1005, PMID: 31383442</p>
<p>Fabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of EMNLP-IJCNLP, pages 2463-2473. DOI: https://doi.org/10.18653/v1/D19 -1250</p>
<p>Nina Poerner, Ulli Waltinger, and Hinrich SchÃ¼tze. 2020. E-BERT: Efficient-yet-effective entity embeddings for BERT. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 803-818. DOI: https://doi.org/10.18653/v1/2020 .findings-emnlp. 71</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving</p>
<p>language understanding by generative pretraining. In Technical report, OpenAI.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of ACL, pages 1715-1725. DOI: https://doi.org/10.18653/v1/P16 -1162</p>
<p>Baoxu Shi and Tim Weninger. 2018. Open-world knowledge graph completion. In Proceedings of AAAI, pages 1957-1964.</p>
<p>Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems (NIPS), pages 4077-4087.</p>
<p>Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowledge graph embedding by relational rotation in complex space. In Proceedings of ICLR.</p>
<p>ThÃ©o Trouillon, Johannes Welbl, Sebastian Riedel, Ã‰ric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proceedings of ICML, pages 2071-2080.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you Need. In Advances in Neural Information Processing Systems (NIPS), pages 5998-6008.</p>
<p>Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. 2019a. Can you tell me how to get past sesame street? Sentence-level pretraining beyond language modeling. In Proceedings of $A C L$, pages 4465-4476. DOI:
https://doi.org/10.18653/v1/P19 -1439</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of ICLR. DOI: https://doi.org/10.18653/v1/W18 -5446</p>
<p>PeiFeng Wang, Jialong Han, Chenliang Li, and Rong Pan. 2019c. Logic attention based neighborhood aggregation for inductive knowledge graph embedding. In Proceedings of AAAI, pages 7152-7159. DOI: https:// doi.org/10.1609/aaai.v33i01.33017152</p>
<p>Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Cuihong Cao, Daxin Jiang, and Ming Zhou. 2020. K-Adapter: Infusing knowledge into pre-trained models with adapters. CoRR, cs.CL/2002.01808v3.</p>
<p>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In Proceedings of EMNLP, pages 1591-1601. DOI: https://doi.org /10.3115/v1/D14-1167</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of NAACL-HLT, pages 1112-1122. DOI: https://doi.org /10.18653/v1/N18-1101</p>
<p>Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable zero-shot entity linking with dense entity retrieval. In Proceedings of EMNLP, pages 6397-6407.</p>
<p>Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge graphs with entity descriptions. In Proceedings of AAAI, pages 2659-2665.</p>
<p>Wenhan Xiong, Jingfei Du, William Yang Wang, and Stoyanov Veselin. 2019. Pretrained encyclopedia: Weakly supervised knowledgepretrained language model. In Proceedings of ICLR.</p>
<p>Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. Joint learning of the embedding of words and entities for named entity disambiguation. In Proceedings of CoNLL, pages 250-259. DOI: https://doi.org/10.18653/v1/K16 -1025</p>
<p>Bishan Yang and Tom Mitchell. 2017. Leveraging knowledge bases in LSTMs for improving machine reading. In Proceedings of ACL, pages 1436-1446. DOI: https://doi.org /10.18653/v1/P17-1132</p>
<p>Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of ICLR.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems (NeurIPS), pages 5754-5764.</p>
<p>Poorya Zaremoodi, Wray Buntine, and Gholamreza Haffari. 2018. Adaptive knowledge sharing in multi-task learning: Improving low-resource neural machine translation. In</p>
<p>Proceedings of ACL, pages 656-661. DOI: https://doi.org/10.18653/v1/P18 -2104</p>
<p>Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017. Position-aware attention and supervised data improve slot filling. In Proceedings of EMNLP, pages 35-45. DOI: https://doi .org/10.18653/v1/D17-1004</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of ACL, pages 1441-1451. DOI: https://doi .org/10.18653/v1/P19-1139</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of ICCV, pages 19-27. DOI: https://doi.org/10.1109 /ICCV.2015.11</p>
<p>Zhaocheng Zhu, Shizhen Xu, Jian Tang, and Meng Qu. 2019. GraphVite: A high-performance CPU-GPU hybrid system for node embedding. In Proceedings of WWW, pages 2494-2504.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/allenai/kb.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://www.wikidata.org.
${ }^{4}$ https://en.wikipedia.org.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
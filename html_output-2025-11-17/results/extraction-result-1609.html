<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1609 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1609</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1609</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-77668573e9180b9fe9ae932a5ce9de53c81b045e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/77668573e9180b9fe9ae932a5ce9de53c81b045e" target="_blank">Transfer in Deep Reinforcement Learning Using Knowledge Graphs</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper explores the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents and demonstrates that their transfer learning methods let us learn a higher-quality control policy faster.</p>
                <p><strong>Paper Abstract:</strong> Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy learning. In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1609.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1609.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG Seeding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Seeding from Static Walkthroughs/Guides</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Seeding the agent's knowledge-graph state representation with RDF triples extracted from static text-adventure guides and walkthroughs to provide a domain/genre prior (object affordances, common actions) that prunes the action space and speeds early exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep Q-network agent that represents state as an evolving knowledge graph (RDF triples) extracted from observations (OpenIE + rules), encodes state and actions separately, and computes Q-values by a pairwise interaction between state and action encodings; uses action pruning based on the knowledge graph and prioritized replay.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld and Jericho interactive fiction games (slice-of-life TextWorld generated games; human-authored games via Jericho: 9:05, Anchorhead, Afflicted, Lurking Horror)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based interactive fiction where the agent receives textual observations (descriptions of rooms, objects, inventory) and issues text commands drawn from a parser template set; environments include generated short quests (TextWorld 'home' theme) and longer human-authored adventure/horror games (Jericho).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / household and narrative puzzle tasks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Short household quests in TextWorld (e.g., 'home' theme completing a 5-step quest), 9:05 household interactions, horror-puzzle interactions in Anchorhead and Afflicted (multi-step puzzle solving, object affordance usage).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are multi-step quests decomposable into sequential subtasks (primitive actions combined into longer procedures); the knowledge graph links objects, locations, and affordances enabling composition of multi-object actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Seeded prior (genre/domain prior) within staged transfer curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>A prior graph is extracted from static online guides/walkthroughs using OpenIE and custom rules; the KG-DQN is initialized with this seed graph before exploration so early action-pruning/ranking uses this generalizable information (affordances, common actions). Seeding is used together with QA pre-training and source-to-target parameter transfer in a staged curriculum (pretrain → source game → transfer → target game).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task similarity/domain/genre prior (seeded from general/genre-specific textual guides) and staged progression from pretraining to source task to target task</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Applied across games ranging from very short TextWorld quests (≈5 steps) to longer human-authored chapters (≈20–39 steps) and up to very long games (reported Lurking Horror oracle length 289 steps for whole game; experiments use partitioned sections).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Improves initial reward and final policy quality versus unseeded baselines. Example (slice-of-life target 9:05, dense reward): KG-DQN Full (seed+QA+transfer) Init Rwd = 2.7 ± 0.65, Final Rwd = 19.7 ± 2.0, Steps = 274.76 ± 21.45 (50 episodes after convergence).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Unseeded baseline (KG-DQN no transfer, dense reward) on same target: Final Rwd = 16.5 ± 1.58, Steps = 1267.2 ± 7.5; untuned or sparse baselines perform substantially worse (e.g., KG-DQN untuned (dense) on 9:05 had final reward lower and many more steps).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper compares seeding-only vs QA-only vs full pipeline. Example (horror target Anchorhead, dense reward): KG-DQN w/ QA Final Rwd = 24.8 ± 0.6, Steps = 4874 ± 90.74; KG-DQN seeded Final Rwd = 26.6 ± 0.42, Steps = 4937 ± 42.93; KG-DQN full (seed+QA+transfer) Final Rwd = 39.9 ± 0.53, Steps = 4334.3 ± 56.13 — full pipeline gives the largest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Yes — staged seeding improves transfer to more complex target games in the same genre: transferring pretrained networks and seeded graphs yields faster learning and higher final reward (e.g., Anchorhead final reward improves from ~6.8 to ~39.9 across baselines to full transfer). Results indicate transfer works better when source and target share domain vocabulary and affordances; for very hard bottlenecks (novel proper nouns) seeding/QA help but full pipeline is needed to overcome specific bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Seeding with a domain prior provides an immediate improvement in initial reward and reduces ineffective early exploration by pruning actions; seeding and QA pretraining perform similarly in many cases; combining them and then transferring DQN parameters from a simpler source to a harder target (full curriculum) produces the largest gains — e.g., up to ~80% improvement in convergence steps in some cases (paper reports up to 80% gain on convergence metrics), and statistically significant improvements (p < 0.05) over baseline KG-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1609.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1609.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-Answering Pre-training on Oracle Traces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-training portions of the KG-DQN observation/action encoders using a question-answering model trained on oracle playthrough traces so the agent learns to map textual observations to high-utility actions before RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN (pretrained components via QA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same KG-DQN architecture; parts of the network responsible for encoding observations, graph and actions are pre-trained using a reading-based QA model (Chen et al., 2017) on state-action traces from oracle walkthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (generated home-theme games) for QA training; transferred and evaluated on target games in Jericho (9:05, Anchorhead, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>QA training uses datasets of state-action pairs extracted from oracle walkthroughs of many generated games (TextWorld) or human walkthroughs; the QA model learns mapping from observation text to the correct action(s).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / gameplay action selection (object affordance and action choice)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Mapping textual room/object descriptions to the next oracle action (e.g., 'take key', 'open door', 'place OBJ in OBJ').</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Learns mappings from observations to actions that can be composed into longer sequences (multi-step quests); pretraining captures local decision patterns that compose across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Pretraining-to-Transfer staged curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Train a QA model on oracle traces from many simple/generated games, use the QA-trained weights to initialize KG-DQN encoders, optionally train KG-DQN on an intermediate source task, then transfer network parameters to the target game — effectively ordering learning from synthetic/simple to complex/real games.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task similarity and progressive complexity (from many simple/generated games → source human-authored game → target human-authored game)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>QA pretraining dataset included very short generated quests (≈5 steps) and was used to bootstrap learning on longer source/target games (≈20–39 steps; longer games partitioned).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>QA pretraining improves both initial and final performance. Examples: Source (Afflicted) KG-DQN w/ QA Init Rwd = 4.3 ± 1.34, Final Rwd = 15.1 ± 1.60, Steps = 1179 ± 32.07 (better than no-transfer); Target (Anchorhead) KG-DQN w/ QA Final Rwd = 24.8 ± 0.6, Steps = 4874 ± 90.74.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Without QA pretraining, same-agent baselines show lower initial and final rewards and take more steps (e.g., Afflicted no transfer Final Rwd = 14.1 ± 1.73, Steps = 1934.7 ± 85.67; Anchorhead no transfer Final Rwd = 6.8 ± 0.42).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared QA-only vs seeding-only vs combined: QA-only yields substantial gains over baseline; seeding-only yields similar gains in many cases; combined (seed + QA + parameter transfer) yields the best results (see KG-DQN Full numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Pretrained QA weights transfer to different games within the same domain and help learn higher-quality policies faster; however, QA pretraining alone is not always sufficient for the hardest bottlenecks (e.g., novel proper nouns in Anchorhead) — full staged transfer performs best.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QA pretraining of encoders provides a useful inductive bias that improves exploration and final policy quality; it reduces steps-to-convergence and raises initial reward, but combining QA pretraining with seeded KGs and parameter transfer yields additive improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1609.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1609.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DQN Parameter Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Source-to-Target Deep Q-Network Parameter Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train KG-DQN on a (simpler) source game and initialize the KG-DQN for a target game with those learned weights (all weights transferred, then finetuned on the target). This is a staged transfer rather than freezing parameters or joint decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN (source-trained → target-initialized)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-DQN trained on a source game (with or without QA pretraining and KG seeding) whose learned network parameters are used to initialize the KG-DQN for a target game; all weights are transferred and then fine-tuned on target.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same as above: TextWorld-generated source games and Jericho human-authored target games</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Source tasks are simpler/generated or smaller sections of games; target tasks are more complex human-authored interactive fiction chapters.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures / puzzle-based multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Training on a TextWorld-generated 'home' game or on the Afflicted source chapter, then transferring to 9:05 or Anchorhead target chapters.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Transfer leverages shared state/action structure and graph embeddings so that learned policies for combining primitives on the source can be composed on the target; tasks are hierarchically more complex in target than source.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Staged source→target transfer curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Order training as: (1) QA pretraining on oracle traces (optional), (2) seed knowledge graph (optional), (3) train KG-DQN on a source task (simpler/shorter), (4) initialize target KG-DQN with source weights and fine-tune on target (possibly with dense oracle-augmented reward); this progression constitutes a curriculum from easy/synthetic to hard/real.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>increasing task complexity and domain similarity (start on simpler or similar source task, then transfer to more complex target)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Source games: short/generated quests (≈5 steps) or shorter human-authored sections (≈20 steps); Target games: longer chapters (≈25–39 steps) or larger puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Substantial reductions in steps-to-convergence and increases in final reward when parameter transfer is included in full pipeline. Example (slice-of-life 9:05): KG-DQN Full (seed+QA+transfer) Steps = 274.76 ± 21.45 vs KG-DQN w/ QA (D) Steps = 1127.0 ± 31.22 or no-transfer Steps = 1267.2 ± 7.5 — large speedup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Without parameter transfer (training target from scratch) agents have lower final reward and require many more steps (see examples above: Anchorhead no transfer final reward ≈ 6.8 vs KG-DQN full final ≈ 39.9).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper contrasts: no transfer, QA pretrain only, seeding only, and full (seed+QA+transfer). The full approach consistently outperforms single-component variants in both initial reward and convergence speed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Parameter transfer generalizes across games within the same genre, improving both learning speed and asymptotic policy quality; effectiveness correlates with vocabulary and structure overlap between source and target.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initializing target networks with parameters learned on a source game provides a strong inductive bias that accelerates learning and improves final performance, especially when combined with seeding and QA pretraining; transferring without freezing allows the target to adapt to structural differences in graph extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1609.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1609.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Dense Reward</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle-Based Reward Augmentation (Dense Reward via Oracle Checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augment the sparse environment reward with dense shaped rewards that give the agent small positive signals when it reproduces state-action pairs from oracle walkthrough traces, to alleviate sparse-reward exploration problems in complex text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN (with oracle-augmented reward)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-DQN trained using an augmented reward that adds scaled positive reward when the agent's state-action matches oracle checkpoints; scaling ensures oracle rewards do not overpower original environment rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld and Jericho interactive fiction games (used to make learning feasible in long/hard games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Oracle walkthroughs provide a sequence of state-action pairs; during RL training, encountering these pairs yields additional shaped reward in addition to normal game rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense/puzzle procedures (used to help exploration over long multi-step quests)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Encounters of oracle state-action pairs from walkthroughs of generated TextWorld quests and partitioned chapters of human-authored games.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Oracle checkpoints correspond to subgoals in the overall multi-step quest; awarding upon these encourages learning of subprocedures that compose into full solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Subgoal-based reward shaping within staged curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Use an oracle's state-action trace as a sequence of checkpoints/subgoals; during training, matching a checkpoint gives a scaled positive reward (less than smallest built-in reward) — effectively turning a sparse global objective into a denser sequence of subgoals aligned with curriculum stages (pretraining/source→target).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>prerequisite/subgoal progression (oracle-ordered checkpoints reflecting a correct sequence of subtasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Used for games with completion steps from very short (5) up to long (20–289); dense reward required for convergence in large/hard horror games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Dense reward is necessary for effective training in complex horror games; examples: On Anchorhead and other horror targets, agents trained with dense reward converge and achieve much higher final rewards (KG-DQN full on Anchorhead Final Rwd = 39.9 ± 0.53). Paper reports that without dense reward some horror games do not converge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>With sparse reward only, many target horror-game experiments fail to converge (e.g., agent training on Anchorhead without dense reward only reached reward ≈7 and stalled); steps-to-completion and final reward are substantially worse.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper compares dense vs sparse reward regimes; dense reward yields convergence where sparse fails (particularly in horror domain), and when combined with seeding/QA/transfer gives best results.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Oracle checkpoint shaping helps transfer because it provides dense subgoal signals aligned with oracle traces from source/target; however, it relies on availability of oracle traces and does not by itself generalize to wholly novel procedures without such traces.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dense oracle-based shaping makes otherwise-intractable text-adventure training feasible, especially for complex domains; combining dense rewards with seeding, QA pretraining and parameter transfer yields the largest gains. Sparse reward alone leads to failure on many hard games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1609.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1609.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task Partitioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Game Partitioning into Intermediate Checkpoints (Curriculum via Reduced Goals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manually segment large/hard human-authored games into smaller subtasks/chapters (intermediate checkpoints) using walkthroughs so agents train on manageable sections as curriculum steps toward full games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN (trained on partitioned chapters when needed)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-DQN trained on reduced/partitioned versions of human-authored games where the final goal is replaced by an intermediate checkpoint (identified by human walkthroughs) to reduce complexity for training and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho human-authored games (Anchorhead, Afflicted, Lurking Horror), with experiments run on partitioned chapters/sections</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Games are segmented by manual identification of natural intermediate goals (e.g., first chapter of Anchorhead), and the environment ends when the checkpoint is reached; action space is pre-pruned to relevant actions for the partition.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense/puzzle procedures (domain-specific subgoals extracted from full game narratives)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Using only Anchorhead's first chapter objectives instead of full-game objectives; reducing Afflicted to an intermediate goal.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Full-game tasks decompose naturally into chapters/subtasks; partitioning enforces a curriculum of increasing task complexity by training on earlier/lower-complexity chapters first.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Manual subtask curriculum (chapter/section sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Identify natural intermediate goals from walkthroughs and train agents on these smaller segments (with action pre-pruning) before attempting larger or subsequent segments; this reduces the effective horizon and complexity the agent must learn at once.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>prerequisite/subtask progression (human-identified chapters/subgoals ordered as in the game narrative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Partitions used reduce very long games (originally hundreds of steps) to sections with completion steps in the tens (e.g., Anchorhead partitioned to 39 steps, Afflicted to 20 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Partitioning makes training feasible and allows successful convergence in otherwise intractable games when combined with other techniques; paper reports improved convergence when using partitioned sections plus dense reward and transfer (see KG-DQN Full numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Training on full unpartitioned complex games often fails to converge or is impractically slow; examples: horror games without partitioning and without dense reward did not converge.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper does not compare automated vs manual partitioning strategies, but shows that manual partitioning combined with seeding/QA/dense reward enables learning where full-game training fails.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Partitioning aids transfer by reducing horizon and focusing learning on prerequisite subtasks that are transferable; however, manual identification of checkpoints is required in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Manually breaking down long interactive fiction into intermediate checkpoints is an effective curriculum step that, together with seeding, QA pretraining, dense rewards and parameter transfer, makes otherwise infeasible learning problems solvable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transfer in Deep Reinforcement Learning Using Knowledge Graphs', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep transfer in reinforcement learning by language grounding <em>(Rating: 2)</em></li>
                <li>TextWorld : A Learning Environment for Text-based Games <em>(Rating: 1)</em></li>
                <li>Actor-mimic: Deep multitask and transfer reinforcement learning <em>(Rating: 1)</em></li>
                <li>Progressive neural networks <em>(Rating: 1)</em></li>
                <li>Knowledge transfer for deep reinforcement learning with hierarchical experience replay <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1609",
    "paper_id": "paper-77668573e9180b9fe9ae932a5ce9de53c81b045e",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "KG Seeding",
            "name_full": "Knowledge Graph Seeding from Static Walkthroughs/Guides",
            "brief_description": "Seeding the agent's knowledge-graph state representation with RDF triples extracted from static text-adventure guides and walkthroughs to provide a domain/genre prior (object affordances, common actions) that prunes the action space and speeds early exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN",
            "agent_description": "A deep Q-network agent that represents state as an evolving knowledge graph (RDF triples) extracted from observations (OpenIE + rules), encodes state and actions separately, and computes Q-values by a pairwise interaction between state and action encodings; uses action pruning based on the knowledge graph and prioritized replay.",
            "agent_size": null,
            "environment_name": "TextWorld and Jericho interactive fiction games (slice-of-life TextWorld generated games; human-authored games via Jericho: 9:05, Anchorhead, Afflicted, Lurking Horror)",
            "environment_description": "Text-based interactive fiction where the agent receives textual observations (descriptions of rooms, objects, inventory) and issues text commands drawn from a parser template set; environments include generated short quests (TextWorld 'home' theme) and longer human-authored adventure/horror games (Jericho).",
            "procedure_type": "commonsense procedures / household and narrative puzzle tasks",
            "procedure_examples": "Short household quests in TextWorld (e.g., 'home' theme completing a 5-step quest), 9:05 household interactions, horror-puzzle interactions in Anchorhead and Afflicted (multi-step puzzle solving, object affordance usage).",
            "compositional_structure": "Tasks are multi-step quests decomposable into sequential subtasks (primitive actions combined into longer procedures); the knowledge graph links objects, locations, and affordances enabling composition of multi-object actions.",
            "uses_curriculum": true,
            "curriculum_name": "Seeded prior (genre/domain prior) within staged transfer curriculum",
            "curriculum_description": "A prior graph is extracted from static online guides/walkthroughs using OpenIE and custom rules; the KG-DQN is initialized with this seed graph before exploration so early action-pruning/ranking uses this generalizable information (affordances, common actions). Seeding is used together with QA pre-training and source-to-target parameter transfer in a staged curriculum (pretrain → source game → transfer → target game).",
            "curriculum_ordering_principle": "task similarity/domain/genre prior (seeded from general/genre-specific textual guides) and staged progression from pretraining to source task to target task",
            "task_complexity_range": "Applied across games ranging from very short TextWorld quests (≈5 steps) to longer human-authored chapters (≈20–39 steps) and up to very long games (reported Lurking Horror oracle length 289 steps for whole game; experiments use partitioned sections).",
            "performance_with_curriculum": "Improves initial reward and final policy quality versus unseeded baselines. Example (slice-of-life target 9:05, dense reward): KG-DQN Full (seed+QA+transfer) Init Rwd = 2.7 ± 0.65, Final Rwd = 19.7 ± 2.0, Steps = 274.76 ± 21.45 (50 episodes after convergence).",
            "performance_without_curriculum": "Unseeded baseline (KG-DQN no transfer, dense reward) on same target: Final Rwd = 16.5 ± 1.58, Steps = 1267.2 ± 7.5; untuned or sparse baselines perform substantially worse (e.g., KG-DQN untuned (dense) on 9:05 had final reward lower and many more steps).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper compares seeding-only vs QA-only vs full pipeline. Example (horror target Anchorhead, dense reward): KG-DQN w/ QA Final Rwd = 24.8 ± 0.6, Steps = 4874 ± 90.74; KG-DQN seeded Final Rwd = 26.6 ± 0.42, Steps = 4937 ± 42.93; KG-DQN full (seed+QA+transfer) Final Rwd = 39.9 ± 0.53, Steps = 4334.3 ± 56.13 — full pipeline gives the largest gains.",
            "transfer_generalization": "Yes — staged seeding improves transfer to more complex target games in the same genre: transferring pretrained networks and seeded graphs yields faster learning and higher final reward (e.g., Anchorhead final reward improves from ~6.8 to ~39.9 across baselines to full transfer). Results indicate transfer works better when source and target share domain vocabulary and affordances; for very hard bottlenecks (novel proper nouns) seeding/QA help but full pipeline is needed to overcome specific bottlenecks.",
            "key_findings": "Seeding with a domain prior provides an immediate improvement in initial reward and reduces ineffective early exploration by pruning actions; seeding and QA pretraining perform similarly in many cases; combining them and then transferring DQN parameters from a simpler source to a harder target (full curriculum) produces the largest gains — e.g., up to ~80% improvement in convergence steps in some cases (paper reports up to 80% gain on convergence metrics), and statistically significant improvements (p &lt; 0.05) over baseline KG-DQN.",
            "uuid": "e1609.0",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "QA Pretraining",
            "name_full": "Question-Answering Pre-training on Oracle Traces",
            "brief_description": "Pre-training portions of the KG-DQN observation/action encoders using a question-answering model trained on oracle playthrough traces so the agent learns to map textual observations to high-utility actions before RL fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN (pretrained components via QA)",
            "agent_description": "Same KG-DQN architecture; parts of the network responsible for encoding observations, graph and actions are pre-trained using a reading-based QA model (Chen et al., 2017) on state-action traces from oracle walkthroughs.",
            "agent_size": null,
            "environment_name": "TextWorld (generated home-theme games) for QA training; transferred and evaluated on target games in Jericho (9:05, Anchorhead, etc.)",
            "environment_description": "QA training uses datasets of state-action pairs extracted from oracle walkthroughs of many generated games (TextWorld) or human walkthroughs; the QA model learns mapping from observation text to the correct action(s).",
            "procedure_type": "commonsense procedures / gameplay action selection (object affordance and action choice)",
            "procedure_examples": "Mapping textual room/object descriptions to the next oracle action (e.g., 'take key', 'open door', 'place OBJ in OBJ').",
            "compositional_structure": "Learns mappings from observations to actions that can be composed into longer sequences (multi-step quests); pretraining captures local decision patterns that compose across steps.",
            "uses_curriculum": true,
            "curriculum_name": "Pretraining-to-Transfer staged curriculum",
            "curriculum_description": "Train a QA model on oracle traces from many simple/generated games, use the QA-trained weights to initialize KG-DQN encoders, optionally train KG-DQN on an intermediate source task, then transfer network parameters to the target game — effectively ordering learning from synthetic/simple to complex/real games.",
            "curriculum_ordering_principle": "task similarity and progressive complexity (from many simple/generated games → source human-authored game → target human-authored game)",
            "task_complexity_range": "QA pretraining dataset included very short generated quests (≈5 steps) and was used to bootstrap learning on longer source/target games (≈20–39 steps; longer games partitioned).",
            "performance_with_curriculum": "QA pretraining improves both initial and final performance. Examples: Source (Afflicted) KG-DQN w/ QA Init Rwd = 4.3 ± 1.34, Final Rwd = 15.1 ± 1.60, Steps = 1179 ± 32.07 (better than no-transfer); Target (Anchorhead) KG-DQN w/ QA Final Rwd = 24.8 ± 0.6, Steps = 4874 ± 90.74.",
            "performance_without_curriculum": "Without QA pretraining, same-agent baselines show lower initial and final rewards and take more steps (e.g., Afflicted no transfer Final Rwd = 14.1 ± 1.73, Steps = 1934.7 ± 85.67; Anchorhead no transfer Final Rwd = 6.8 ± 0.42).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared QA-only vs seeding-only vs combined: QA-only yields substantial gains over baseline; seeding-only yields similar gains in many cases; combined (seed + QA + parameter transfer) yields the best results (see KG-DQN Full numbers).",
            "transfer_generalization": "Pretrained QA weights transfer to different games within the same domain and help learn higher-quality policies faster; however, QA pretraining alone is not always sufficient for the hardest bottlenecks (e.g., novel proper nouns in Anchorhead) — full staged transfer performs best.",
            "key_findings": "QA pretraining of encoders provides a useful inductive bias that improves exploration and final policy quality; it reduces steps-to-convergence and raises initial reward, but combining QA pretraining with seeded KGs and parameter transfer yields additive improvements.",
            "uuid": "e1609.1",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "DQN Parameter Transfer",
            "name_full": "Source-to-Target Deep Q-Network Parameter Transfer",
            "brief_description": "Train KG-DQN on a (simpler) source game and initialize the KG-DQN for a target game with those learned weights (all weights transferred, then finetuned on the target). This is a staged transfer rather than freezing parameters or joint decision-making.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN (source-trained → target-initialized)",
            "agent_description": "KG-DQN trained on a source game (with or without QA pretraining and KG seeding) whose learned network parameters are used to initialize the KG-DQN for a target game; all weights are transferred and then fine-tuned on target.",
            "agent_size": null,
            "environment_name": "Same as above: TextWorld-generated source games and Jericho human-authored target games",
            "environment_description": "Source tasks are simpler/generated or smaller sections of games; target tasks are more complex human-authored interactive fiction chapters.",
            "procedure_type": "commonsense procedures / puzzle-based multi-step tasks",
            "procedure_examples": "Training on a TextWorld-generated 'home' game or on the Afflicted source chapter, then transferring to 9:05 or Anchorhead target chapters.",
            "compositional_structure": "Transfer leverages shared state/action structure and graph embeddings so that learned policies for combining primitives on the source can be composed on the target; tasks are hierarchically more complex in target than source.",
            "uses_curriculum": true,
            "curriculum_name": "Staged source→target transfer curriculum",
            "curriculum_description": "Order training as: (1) QA pretraining on oracle traces (optional), (2) seed knowledge graph (optional), (3) train KG-DQN on a source task (simpler/shorter), (4) initialize target KG-DQN with source weights and fine-tune on target (possibly with dense oracle-augmented reward); this progression constitutes a curriculum from easy/synthetic to hard/real.",
            "curriculum_ordering_principle": "increasing task complexity and domain similarity (start on simpler or similar source task, then transfer to more complex target)",
            "task_complexity_range": "Source games: short/generated quests (≈5 steps) or shorter human-authored sections (≈20 steps); Target games: longer chapters (≈25–39 steps) or larger puzzles.",
            "performance_with_curriculum": "Substantial reductions in steps-to-convergence and increases in final reward when parameter transfer is included in full pipeline. Example (slice-of-life 9:05): KG-DQN Full (seed+QA+transfer) Steps = 274.76 ± 21.45 vs KG-DQN w/ QA (D) Steps = 1127.0 ± 31.22 or no-transfer Steps = 1267.2 ± 7.5 — large speedup.",
            "performance_without_curriculum": "Without parameter transfer (training target from scratch) agents have lower final reward and require many more steps (see examples above: Anchorhead no transfer final reward ≈ 6.8 vs KG-DQN full final ≈ 39.9).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper contrasts: no transfer, QA pretrain only, seeding only, and full (seed+QA+transfer). The full approach consistently outperforms single-component variants in both initial reward and convergence speed.",
            "transfer_generalization": "Parameter transfer generalizes across games within the same genre, improving both learning speed and asymptotic policy quality; effectiveness correlates with vocabulary and structure overlap between source and target.",
            "key_findings": "Initializing target networks with parameters learned on a source game provides a strong inductive bias that accelerates learning and improves final performance, especially when combined with seeding and QA pretraining; transferring without freezing allows the target to adapt to structural differences in graph extraction.",
            "uuid": "e1609.2",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Oracle Dense Reward",
            "name_full": "Oracle-Based Reward Augmentation (Dense Reward via Oracle Checkpoints)",
            "brief_description": "Augment the sparse environment reward with dense shaped rewards that give the agent small positive signals when it reproduces state-action pairs from oracle walkthrough traces, to alleviate sparse-reward exploration problems in complex text games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN (with oracle-augmented reward)",
            "agent_description": "KG-DQN trained using an augmented reward that adds scaled positive reward when the agent's state-action matches oracle checkpoints; scaling ensures oracle rewards do not overpower original environment rewards.",
            "agent_size": null,
            "environment_name": "TextWorld and Jericho interactive fiction games (used to make learning feasible in long/hard games)",
            "environment_description": "Oracle walkthroughs provide a sequence of state-action pairs; during RL training, encountering these pairs yields additional shaped reward in addition to normal game rewards.",
            "procedure_type": "commonsense/puzzle procedures (used to help exploration over long multi-step quests)",
            "procedure_examples": "Encounters of oracle state-action pairs from walkthroughs of generated TextWorld quests and partitioned chapters of human-authored games.",
            "compositional_structure": "Oracle checkpoints correspond to subgoals in the overall multi-step quest; awarding upon these encourages learning of subprocedures that compose into full solutions.",
            "uses_curriculum": true,
            "curriculum_name": "Subgoal-based reward shaping within staged curriculum",
            "curriculum_description": "Use an oracle's state-action trace as a sequence of checkpoints/subgoals; during training, matching a checkpoint gives a scaled positive reward (less than smallest built-in reward) — effectively turning a sparse global objective into a denser sequence of subgoals aligned with curriculum stages (pretraining/source→target).",
            "curriculum_ordering_principle": "prerequisite/subgoal progression (oracle-ordered checkpoints reflecting a correct sequence of subtasks)",
            "task_complexity_range": "Used for games with completion steps from very short (5) up to long (20–289); dense reward required for convergence in large/hard horror games.",
            "performance_with_curriculum": "Dense reward is necessary for effective training in complex horror games; examples: On Anchorhead and other horror targets, agents trained with dense reward converge and achieve much higher final rewards (KG-DQN full on Anchorhead Final Rwd = 39.9 ± 0.53). Paper reports that without dense reward some horror games do not converge.",
            "performance_without_curriculum": "With sparse reward only, many target horror-game experiments fail to converge (e.g., agent training on Anchorhead without dense reward only reached reward ≈7 and stalled); steps-to-completion and final reward are substantially worse.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper compares dense vs sparse reward regimes; dense reward yields convergence where sparse fails (particularly in horror domain), and when combined with seeding/QA/transfer gives best results.",
            "transfer_generalization": "Oracle checkpoint shaping helps transfer because it provides dense subgoal signals aligned with oracle traces from source/target; however, it relies on availability of oracle traces and does not by itself generalize to wholly novel procedures without such traces.",
            "key_findings": "Dense oracle-based shaping makes otherwise-intractable text-adventure training feasible, especially for complex domains; combining dense rewards with seeding, QA pretraining and parameter transfer yields the largest gains. Sparse reward alone leads to failure on many hard games.",
            "uuid": "e1609.3",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Task Partitioning",
            "name_full": "Game Partitioning into Intermediate Checkpoints (Curriculum via Reduced Goals)",
            "brief_description": "Manually segment large/hard human-authored games into smaller subtasks/chapters (intermediate checkpoints) using walkthroughs so agents train on manageable sections as curriculum steps toward full games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN (trained on partitioned chapters when needed)",
            "agent_description": "KG-DQN trained on reduced/partitioned versions of human-authored games where the final goal is replaced by an intermediate checkpoint (identified by human walkthroughs) to reduce complexity for training and evaluation.",
            "agent_size": null,
            "environment_name": "Jericho human-authored games (Anchorhead, Afflicted, Lurking Horror), with experiments run on partitioned chapters/sections",
            "environment_description": "Games are segmented by manual identification of natural intermediate goals (e.g., first chapter of Anchorhead), and the environment ends when the checkpoint is reached; action space is pre-pruned to relevant actions for the partition.",
            "procedure_type": "commonsense/puzzle procedures (domain-specific subgoals extracted from full game narratives)",
            "procedure_examples": "Using only Anchorhead's first chapter objectives instead of full-game objectives; reducing Afflicted to an intermediate goal.",
            "compositional_structure": "Full-game tasks decompose naturally into chapters/subtasks; partitioning enforces a curriculum of increasing task complexity by training on earlier/lower-complexity chapters first.",
            "uses_curriculum": true,
            "curriculum_name": "Manual subtask curriculum (chapter/section sequencing)",
            "curriculum_description": "Identify natural intermediate goals from walkthroughs and train agents on these smaller segments (with action pre-pruning) before attempting larger or subsequent segments; this reduces the effective horizon and complexity the agent must learn at once.",
            "curriculum_ordering_principle": "prerequisite/subtask progression (human-identified chapters/subgoals ordered as in the game narrative)",
            "task_complexity_range": "Partitions used reduce very long games (originally hundreds of steps) to sections with completion steps in the tens (e.g., Anchorhead partitioned to 39 steps, Afflicted to 20 steps).",
            "performance_with_curriculum": "Partitioning makes training feasible and allows successful convergence in otherwise intractable games when combined with other techniques; paper reports improved convergence when using partitioned sections plus dense reward and transfer (see KG-DQN Full numbers).",
            "performance_without_curriculum": "Training on full unpartitioned complex games often fails to converge or is impractically slow; examples: horror games without partitioning and without dense reward did not converge.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper does not compare automated vs manual partitioning strategies, but shows that manual partitioning combined with seeding/QA/dense reward enables learning where full-game training fails.",
            "transfer_generalization": "Partitioning aids transfer by reducing horizon and focusing learning on prerequisite subtasks that are transferable; however, manual identification of checkpoints is required in this work.",
            "key_findings": "Manually breaking down long interactive fiction into intermediate checkpoints is an effective curriculum step that, together with seeding, QA pretraining, dense rewards and parameter transfer, makes otherwise infeasible learning problems solvable.",
            "uuid": "e1609.4",
            "source_info": {
                "paper_title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Deep transfer in reinforcement learning by language grounding",
            "rating": 2
        },
        {
            "paper_title": "TextWorld : A Learning Environment for Text-based Games",
            "rating": 1
        },
        {
            "paper_title": "Actor-mimic: Deep multitask and transfer reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Progressive neural networks",
            "rating": 1
        },
        {
            "paper_title": "Knowledge transfer for deep reinforcement learning with hierarchical experience replay",
            "rating": 1
        }
    ],
    "cost": 0.015556999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transfer in Deep Reinforcement Learning using Knowledge Graphs</h1>
<p>Prithviraj Ammanabrolu<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>Text adventure games, in which players must make sense of the world through text descriptions and declare actions through text descriptions, provide a stepping stone toward grounding action in language. Prior work has demonstrated that using a knowledge graph as a state representation and question-answering to pre-train a deep Q-network facilitates faster control policy learning. In this paper, we explore the use of knowledge graphs as a representation for domain knowledge transfer for training text-adventure playing reinforcement learning agents. Our methods are tested across multiple computer generated and human authored games, varying in domain and complexity, and demonstrate that our transfer learning methods let us learn a higher-quality control policy faster.</p>
<h2>1 Introduction</h2>
<p>Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more realworld environments where agents must communicate to understand the state of the world and affect change in the world. Despite the steadily increasing body of research on text-adventure games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019), and in addition to the ubiquity of deep reinforcement learning applications (Parisotto et al., 2016; Zambaldi et al., 2019), teaching an agent to play text-adventure games remains a challenging task. Learning a control policy for a text-adventure game requires a significant amount of exploration, resulting in training runs that take hundreds of thousands of simulations (Narasimhan et al., 2015; Ammanabrolu and Riedl, 2019).</p>
<p>One reason that text-adventure games require so much exploration is that most deep reinforcement learning algorithms are trained on a task without a real prior. In essence, the agent must learn everything about the game from only its interactions with the environment. Yet, text-adventure games make ample use of commonsense knowledge (e.g., an axe can be used to cut wood) and genre themes (e.g., in a horror or fantasy game, a coffin is likely to contain a vampire or other undead monster). This is in addition to the challenges innate to the text-adventure game itself—games are puzzleswhich results in inefficient training.</p>
<p>Ammanabrolu and Riedl (2019) developed a reinforcement learning agent that modeled the text environment as a knowledge graph and achieved state-of-the-art results on simple text-adventure games provided by the TextWorld (Côté et al., 2018) environment. They observed that a simple form of transfer from very similar games greatly improved policy training time. However, games beyond the toy TextWorld environments are beyond the reach of state-of-the-art techniques.</p>
<p>In this paper, we explore the use of knowledge graphs and associated neural embeddings as a medium for domain transfer to improve training effectiveness on new text-adventure games. Specifically, we explore transfer learning at multiple levels and across different dimensions. We first look at the effects of playing a text-adventure game given a strong prior in the form of a knowledge graph extracted from generalized textual walk-throughs of interactive fiction as well as those made specifically for a given game. Next, we explore the transfer of control policies in deep Q-learning (DQN) by pre-training portions of a deep Q-network using question-answering and by DQN-to-DQN parameter transfer between games. We evaluate these techniques on two different sets of human authored and computer generated</p>
<p>games, demonstrating that our transfer learning methods enable us to learn a higher-quality control policy faster.</p>
<h2>2 Background and Related Work</h2>
<p>Text-adventure games, in which an agent must interact with the world entirely through natural language, provide us with two challenges that have proven difficult for deep reinforcement learning to solve (Narasimhan et al., 2015; Haroush et al., 2018; Ammanabrolu and Riedl, 2019): (1) The agent must act based only on potentially incomplete textual descriptions of the world around it. The world is thus partially observable, as the agent does not have access to the state of the world at any stage. (2) the action space is combinatorially large-a consequence of the agent having to declare commands in natural language. These two problems together have kept commercial text adventure games out of the reach of existing deep reinforcement learning methods, especially given the fact that most of these methods attempt to train on a particular game from scratch.</p>
<p>Text-adventure games can be treated as partially observable Markov decision processes (POMDPs). This can be represented as a 7-tuple of $\langle S, T, A, \Omega, O, R, \gamma\rangle$ : the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, conditional observation probabilities, the reward function, and the discount factor respectively (Côté et al., 2018).</p>
<p>Multiple recent works have explored the challenges associated with these games (Bordes et al., 2010; He et al., 2016; Narasimhan et al., 2015; Fulda et al., 2017; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018; Ammanabrolu and Riedl, 2019). Narasimhan et al. (2015) introduce the LSTM-DQN, which learns to score the action verbs and corresponding objects separately and then combine them into a single action. He et al. (2016) propose the Deep Reinforcement Relevance Network that consists of separate networks to encode state and action information, with a final Q-value for a state-action pair that is computed between a pairwise interaction function between these. Haroush et al. (2018) present the Action Elimination Network (AEN), which restricts actions in a state to the top-k most likely ones, using the emulator's feedback. Hausknecht et al. (2019b) design an agent that uses multiple mod- ules to identify a general set of game play rules for text games across various domains. None of these works study how to transfer policies between different text-adventure games in any depth and so there exists a gap between the two bodies of work.</p>
<p>Transferring policies across different textadventure games requires implicitly learning a mapping between the games' state and action spaces. The more different the domain of the two games, the harder this task becomes. Previous work (Ammanabrolu and Riedl, 2019) introduced the use of knowledge graphs and questionanswering pre-training to aid in the problems of partial observability and a combinatorial action space. This work made use of a system called TextWorld (Côté et al., 2018) that uses grammars to generate a series of similar (but not exact same) games. An oracle was used to play perfect games and the traces were used to pre-train portions of the agent's network responsible for encoding the observations, graph, and actions. Their results show that this form of pre-training improves the quality of the policy at convergence it does not show a significant improvement in the training time required to reach convergence. Further, it is generally unrealistic to have a corpus of very similar games to draw from. We build on this work, and explore modifications of this algorithm that would enable more efficient transfer in textadventure games.</p>
<p>Work in transfer in reinforcement learning has explored the idea of transferring skills (Konidaris and Barto, 2007; Konidaris et al., 2012) or transferring value functions/policies (Liu and Stone, 2006). Other approaches attempt transfer in model-based reinforcement learning (Taylor et al., 2008; Nguyen et al., 2012; Gasic et al., 2013; Wang et al., 2015; Joshi and Chowdhary, 2018), though traditional approaches here rely heavily on hand crafting state-action mappings across domains. Narasimhan et al. (2017) learn to play games by predicting mappings across domains using a both deep Q-networks and value iteration networks, finding that that grounding the game state using natural language descriptions of the game itself aids significantly in transferring useful knowledge between domains.</p>
<p>In transfer for deep reinforcement learning, Parisotto et al. (2016) propose the Actor-Mimic network which learns from expert policies for a source task using policy distillation and then ini-</p>
<p>tializes the network for a target task using these parameters. <em>Yin and Pan (2017)</em> also use policy distillation, using task specific features as inputs to a multi-task policy network and use a hierarchical experience sampling method to train this multitask network. Similarly, <em>Rusu et al. (2016)</em> attempt to transfer parameters by using frozen parameters trained on source tasks to help learn a new set of parameters on target tasks. <em>Rajendran et al. (2017)</em> attempt something similar but use attention networks to transfer expert policies between tasks. These works, however, do not study the requirements for enabling efficient transfer for tasks rooted in natural language, nor do they explore the use of knowledge graphs as a state representation.</p>
<h2>3 Knowledge Graphs for DQNs</h2>
<p>A knowledge graph is a directed graph formed by a set of semantic, or RDF, triples in the form of <em>〈subject, relation, object〉</em>—for example, <em>〈vampires, are, undead〉</em>. We follow the open-world assumption that what is not in our knowledge graph can either be true or false.</p>
<p><em>Ammanabrolu and Riedl (2019)</em> introduced the Knowledge Graph DQN (KG-DQN) and touched on some aspects of transfer learning, showing that pre-training portions of the deep Q-network using question answering system on perfect playthroughs of a game increases the quality of the learned control policy for a generated text-adventure game. We build on this work and use KG-DQN to explore transfer with both knowledge graphs and network parameters. Specifically we seek to transfer skills and knowledge from (a) static text documents describing game play and (b) from playing one text-adventure game to a second complete game in in the same genre (e.g., horror games). The rest of this section describes KG-DQN in detail and summarizes our modifications.</p>
<p>For each step that the agent takes, it automatically extracts a set of RDF triples from the received observation through the use of OpenIE ( [Angeli et al., 2015]) in addition to a few rules to account for the regularities of text-adventure games. The graph itself is more or less a map of the world, with information about objects' affordances and attributes linked to the rooms that they are place in in a map. The graph also makes a distinction with respect to items that are in the agent's</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The KG-DQN architecture.</p>
<p>possession or in their immediate surrounding environment. We make minor modifications to the rules used in <em>Ammanabrolu and Riedl (2019)</em> to better achieve such a graph in general interactive fiction environments.</p>
<p>The agent also has access to all actions accepted by the game's parser, following <em>Narasimhan et al. (2015)</em>. For general interactive fiction environments, we develop our own method to extract this information. This is done by extracting a set of templates accepted by the parser, with the objects or noun phrases in the actions replaces with a OBJ tag. An example of such a template is "place OBJ in OBJ". These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of $A = \mathcal{O(|V| \times |O|^2)}$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in <em>Ammanabrolu and Riedl (2019)</em>.</p>
<p>The architecture for the deep Q-network consists of two separate neural networks—encoding state and action separately—with the final $Q$-value for a state-action pair being the result of a pairwise interaction function between the two (Figure 1). We train with a standard DQN training loop; the policy is determined by the $Q$-value of a particular state-action pair, which is updated using</p>
<p><sup>1</sup>We use the implementation of KG-DQN found at <a href="https://github.com/zajammanabrolu/KG-DQN">https://github.com/zajammanabrolu/KG-DQN</a></p>
<p>the Bellman equation (Sutton and Barto, 2018):</p>
<p>$$
\begin{aligned}
&amp; Q_{t+1}\left(s_{t+1}, a_{t+1}\right)= \
&amp; \qquad E\left[r_{t+1}+\gamma \max <em t="t">{a \in A</em>\right]
\end{aligned}
$$}} Q_{t}(s, a) \mid s_{t}, a_{t</p>
<p>where $\gamma$ refers to the discount factor and $r_{t+1}$ is the observed reward. The whole system is trained using prioritized experience replay [16], a modified version of $\epsilon$-greedy learning, and a temporal difference loss that is computed as:</p>
<p>$$
\begin{aligned}
L(\theta)= &amp; r_{k+1}+ \
&amp; \gamma \max <em _mathbf_k="\mathbf{k">{\mathbf{a} \in \mathbf{A}</em>}+1}} Q\left(\mathbf{s<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a} ; \theta\right)-Q\left(\mathbf{s}</em> ; \theta\right)
\end{aligned}
$$}}, \mathbf{a}_{\mathbf{t}</p>
<p>where $\mathbf{A}<em _mathbf_t="\mathbf{t">{\mathbf{k + 1}}$ represents the action set at step $k+$ 1 and $\mathbf{s}</em>$ refer to the encoded state and action representations respectively.}}, \mathbf{a}_{\mathbf{t}</p>
<h2>4 Knowledge Graph Seeding</h2>
<p>In this section we consider the problem of transferring a knowledge graph from a static text resource to a DQN-which we refer to as seeding. KG-DQN uses a knowledge graph as a state representation and also to prune the action space. This graph is built up over time, through the course of the agent's exploration. When the agent first starts the game, however, this graph is empty and does not help much in the action pruning process. The agent thus wastes a large number of steps near the beginning of each game exploring ineffectively.</p>
<p>The intuition behind seeding the knowledge graph from another source is to give the agent a prior on which actions have a higher utility and thereby enabling more effective exploration. Textadventure games typically belong to a particular genre of storytelling-e.g., horror, sci-fi, or soap opera-and an agent is at a distinct disadvantage if it doesn't have any genre knowledge. Thus, the goal of seeding is to give the agent a strong prior.</p>
<p>This seed knowledge graph is extracted from online general text-adventure guides as well as game/genre specific guides when available. ${ }^{2}$ The graph is extracted from this the guide using a subset of the rules described in Section 3 used to extract information from the game observations, with the remainder of the RDF triples coming from OpenIE. There is no map of rooms in the environment that can be built, but it is possible to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Select partial example of what a seed knowledge graph looks like. Ellipses indicate other similar entities and relations not shown.
extract information regarding affordances of frequently occurring objects as well as common actions that can be performed across a wide range of text-adventure games. This extracted graph is thus potentially disjoint, containing only this generalizable information, in contrast to the graph extracted during the rest of the exploration process. An example of a graph used to seed KG-DQN is given in Fig. 2. The KG-DQN is initialized with this knowledge graph.</p>
<h2>5 Task Specific Transfer</h2>
<p>The overarching goal of transfer learning in textadventure games is to be able to train an agent on one game and use this training on to improve the learning capabilities of another. There is growing body of work on improving training times on target tasks by transferring network parameters trained on source tasks (Rusu et al., 2016; Yin and Pan, 2017; Rajendran et al., 2017). Of particular note is the work by Rusu et al. (2016), where they train a policy on a source task and then use this to help learn a new set of parameters on a target task. In this approach, decisions made during the training of the target task are jointly made using the frozen parameters of the transferred policy network as well as the current policy network.</p>
<p>Our system first trains a question-answering system (Chen et al., 2017) using traces given by an oracle, as in Section 4. For commercial textadventure games, these traces take the form of state-action pairs generated using perfect walk-</p>
<p>through descriptions of the game found online as described in Section 4.</p>
<p>We use the parameters of the questionanswering system to pre-train portions of the deep Q-network for a different game within in the same domain. The portions that are pre-trained are the same parts of the architecture as in Ammanabrolu and Riedl (2019). This game is referred to as the source task. The seeding of the knowledge graph is not strictly necessary but given that state-of-theart DRL agents cannot complete real games, this makes the agent more effective at the source task.</p>
<p>We then transfer the knowledge and skills acquired from playing the source task to another game from the same genre-the target task. The parameters of the deep Q-network trained on the source game are used to initialize a new deep Qnetwork for the target task. All the weights indicated in the architecture of KG-DQN as shown in Fig. 1 are transferred. Unlike Rusu et al. (2016), we do not freeze the parameters of the deep Qnetwork trained on the source task nor use the two networks to jointly make decisions but instead just use it to initialize the parameters of the target task deep Q-network. This is done to account for the fact that although graph embeddings can be transferred between games, the actual graph extracted from a game is non-transferable due to differences in structure between the games.</p>
<h2>6 Experiments</h2>
<p>We test our system on two separate sets of games in different domains using the Jericho and TextWorld frameworks (Hausknecht et al., 2019a; Côté et al., 2018). The first set of games is "slice of life" themed and contains games that involve mundane tasks usually set in textual descriptions of normal houses. The second set of games is "horror" themed and contains noticeably more difficult games with a relatively larger vocabulary size and action set, non-standard fantasy names, etc. We choose these domains because of the availability of games in popular online gaming communities, the degree of vocabulary overlap within each theme, and overall structure of games in each theme. Specifically, there must be at least three games in each domain: at least one game to train the question-answering system on, and two more to train the parameters of the source and target task deep Q-networks. A summary of the statistics for the games is given in Table 1. Vocabulary overlap
is calculated by measuring the percentage of overlap between a game's vocabulary and the domain's vocabulary, i.e. the union of the vocabularies for all the games we use within the domain. We observe that in both of these domains, the complexity of the game increases steadily from the game used for the question-answering system to the target and then source task games.</p>
<p>We perform ablation tests within each domain, mainly testing the effects of transfer from seeding, oracle-based question-answering, and source-to-target parameter transfer. Additionally, there are a couple of extra dimensions of ablations that we study, specific to each of the domains and explained below. All experiments are run three times using different random seeds. For all the experiments we report metrics known to be important for transfer learning tasks (Taylor and Stone, 2009; Narasimhan et al., 2017): average reward collected in the first 50 episodes (init. reward), average reward collected for 50 episodes after convergence (final reward), and number of steps taken to finish the game for 50 episodes after convergence (steps). For the metrics tested after convergence, we set $\epsilon=0.1$ following both Narasimhan et al. (2015) and Ammanabrolu and Riedl (2019). We use similar hyperparameters to those reported in (Ammanabrolu and Riedl, 2019) for training the KG-DQN with action pruning, with the main difference being that we use 100 dimensional word embeddings instead of 50 dimensions for the horror genre.</p>
<h3>6.1 Slice of Life Experiments</h3>
<p>TextWorld uses a grammar to generate similar games. Following Ammanabrolu and Riedl (2019), we use TextWorld's "home" theme to generate the games for the question-answering system. TextWorld is a framework that uses a grammar to randomly generate game worlds and quests. This framework also gives us information such as instructions on how to finish the quest, and a list of actions that can be performed at each step based on the current world state. We do not let our agent access this additional solution information or admissible actions list. Given the relatively small quest length for TextWorld games-games can be completed in as little as 5 steps-we generate 50 such games and partition them into train and test sets in a 4:1 ratio. The traces are generated on the training set, and the question-answering system is</p>
<p>Table 1: Game statistics</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Slice of life</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Horror</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QA/Source <br> TextWorld</td>
<td style="text-align: center;">Target <br> 9:05</td>
<td style="text-align: center;">QA <br> Lurking Horror</td>
<td style="text-align: center;">Source <br> Afflicted</td>
<td style="text-align: center;">Target <br> Anchorhead</td>
</tr>
<tr>
<td style="text-align: center;">Vocab size</td>
<td style="text-align: center;">788</td>
<td style="text-align: center;">297</td>
<td style="text-align: center;">773</td>
<td style="text-align: center;">761</td>
<td style="text-align: center;">2256</td>
</tr>
<tr>
<td style="text-align: center;">Branching factor</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">677</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">947</td>
<td style="text-align: center;">1918</td>
</tr>
<tr>
<td style="text-align: center;">Number of rooms</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td style="text-align: center;">Completion steps</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">289</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">39</td>
</tr>
<tr>
<td style="text-align: center;">Words per obs.</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">114.2</td>
</tr>
<tr>
<td style="text-align: center;">New triples per obs.</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">\% Vocab overlap</td>
<td style="text-align: center;">19.70</td>
<td style="text-align: center;">21.45</td>
<td style="text-align: center;">22.80</td>
<td style="text-align: center;">14.40</td>
<td style="text-align: center;">66.34</td>
</tr>
<tr>
<td style="text-align: center;">Max. aug. reward</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">43</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h2>Bedroom</h2>
<p>This bedroom is extremely spare, with dirty laundry scattered haphazardly all over the floor. Cleaner clothing can be found in the dresser. A bathroom lies to the south, while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys.
$&gt;$ inventory
You are carrying:
some soiled clothing (being worn)
a gold watch (being worn)
$&gt;$ go south</p>
<h2>Bathroom</h2>
<p>This is a far from luxurious but still quite functional bathroom, with a sink, toilet and shower. The bedroom lies to the north.</p>
<p>Figure 3: Partial unseeded knowledge graph example given observations and actions in the game 9:05.
evaluated on the test set.
We then pick a random game from the test set to train our source task deep Q-network for this domain. For this training, we use the reward function provided by TextWorld: +1 for each action taken that moves the agent closer to finishing the quest; -1 for each action taken that extends the minimum number of steps needed to finish the quest from the current stage; 0 for all other situations.</p>
<p>We choose the game, 9:05 ${ }^{3}$ as our target task game due to similarities in structure in addition to the vocabulary overlap. Note that there are multiple possible endings to this game and we pick the simplest one for the purpose of training our agent.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h2>Outside the Real Estate Office</h2>
<p>A grim little cul-de-sac, tucked away in a corner of the claustrophobic tangle of narrow, twisting avenues that largely constitute the older portion of Anchorhead. Like most of the streets in this city, it is ancient, shadowy, and leads essentially nowhere. The lane ends here at the real estate agent's office, which lies to the east, and winds its way back toward the center of town to the west. A narrow, garbage-choked alley opens to the southeast.
$&gt;$ go southeast
Alley
This narrow aperture between two buildings is nearly blocked with piles of rotting cardboard boxes and overstuffed garbage cans. Ugly, halfcrumbling brick walls to either side totter oppressively over you. The alley ends here at a tall, wooden fence. High up on the wall of the northern building there is a narrow, transom-style window.</p>
<p>Figure 4: Partial unseeded knowledge graph example given observations and actions in the game Anchorhead.</p>
<h3>6.2 Horror Experiments</h3>
<p>For the horror domain, we choose Lurking Hor$r o r^{k}$ to train the question-answering system on. The source and target task games are chosen as Afflicted $^{5}$ and Anchorhead ${ }^{6}$ respectively. However, due to the size and complexity of these two games some modifications to the games are required for the agent to be able to effectively solve them.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward curve for select experiments in the slice of life domain.</p>
<p>We partition each of these games and make them smaller by reducing the final goal of the game to an intermediate checkpoint leading to it. This checkpoints were identified manually using walkthroughs of the game; each game has a natural intermediate goal. For example, <em>Anchorhead</em> is segmented into 3 chapters in the form of objectives spread across 3 days, of which we use only the first chapter. The exact details of the games after partitioning is described in Table 1. For Lurking Horror, we report numbers relevant for the oracle walkthrough. We then pre-prune the action space and use only the actions that are relevant for the sections of the game that we have partitioned out. The majority of the environment is still available for the agent to explore but the game ends upon completion of the chosen intermediate checkpoint.</p>
<h3>6.3 Reward Augmentation</h3>
<p>The combined state-action space for a commercial text-adventure game is quite large and the corresponding reward function is very sparse in comparison. The default, implied reward signal is to receive positive value upon completion of the game, and no reward value elsewhere. This is problematic from an experimentation perspective as text-adventure games are too complex for even state-of-the-art deep reinforcement learning agents to complete. Even using transfer learning methods, a sparse reward signal usually results in ineffective exploration by the agent.</p>
<p>To make experimentation feasible, we augment the reward to give the agent a dense reward signal. Specifically, we use an oracle to generate state-action traces (identical to how as when training the question-answering system). An oracle is an agent that is capable of playing and finishing a game perfectly in the least number of steps possible. The state-action pairs generated using perfect walkthroughs of the game are then used as checkpoints and used to give the agent additional reward. If the agent encounters any of these state-action pairs when training, i.e. performs the right action given a corresponding state, it receives a proportional reward in addition to the standard reward built into the game. This reward is scaled based on the game and is designed to be less than the smallest reward given by the original reward function to prevent it from overpowering the built-in reward. We refer to agents using this technique as having "dense" reward and "sparse" reward otherwise. The agent otherwise receives no information from the oracle about how to win the game.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Reward curve for select experiments in the horror domain.</p>
<h3>7 Results/Discussion</h3>
<p>The structure of the experiments are such that the for each of the domains, the target task game is more complex than the source task game. The slice of life games are also generally less complex than the horror games; they have a simpler vocabulary and a more linear quest structure. Additionally, given the nature of interactive fiction games, it is nearly impossible—even for human players—to achieve completion in the minimum number of steps (as given by the steps to completion in Table 1); each of these games are puzzle based and require extensive exploration and interaction with various objects in the environment to complete.</p>
<p>Table 2 and Table 3 show results for the slice of life and horror domains, respectively. In both do-</p>
<p>Table 2: Results for the slice of life games. "KG-DQN Full" refers to KG-DQN when seeded first, trained on the source, then transferred to the target. All experiment with QA indicate pre-training. S, D indicate sparse and dense reward respectively.</p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>Init. Rwd.</th>
<th>Final Rwd.</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Source Game (TextWorld)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN no transfer</td>
<td>$2.6 \pm 0.73$</td>
<td>$4.7 \pm 0.23$</td>
<td>$110.83 \pm 4.92$</td>
</tr>
<tr>
<td>KG-DQN w/ QA</td>
<td>$2.8 \pm 0.61$</td>
<td>$4.9 \pm 0.09$</td>
<td>$88.57 \pm 3.45$</td>
</tr>
<tr>
<td>KG-DQN seeded</td>
<td>$3.2 \pm 0.57$</td>
<td>$4.8 \pm 0.16$</td>
<td>$91.43 \pm 1.89$</td>
</tr>
<tr>
<td>Target Game (9:05)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN untuned (D)</td>
<td>-</td>
<td>$2.5 \pm 0.48$</td>
<td>$1479.0 \pm 22.3$</td>
</tr>
<tr>
<td>KG-DQN no transfer (S)</td>
<td>-</td>
<td>-</td>
<td>$1916.0 \pm 33.17$</td>
</tr>
<tr>
<td>KG-DQN no transfer (D)</td>
<td>$0.8 \pm 0.32$</td>
<td>$16.5 \pm 1.58$</td>
<td>$1267.2 \pm 7.5$</td>
</tr>
<tr>
<td>KG-DQN w/ QA (S)</td>
<td>-</td>
<td>-</td>
<td>$1428.0 \pm 11.26$</td>
</tr>
<tr>
<td>KG-DQN w/ QA (D)</td>
<td>$1.3 \pm 0.24$</td>
<td>$17.4 \pm 1.84$</td>
<td>$1127.0 \pm 31.22$</td>
</tr>
<tr>
<td>KG-DQN seeded (D)</td>
<td>$1.4 \pm 0.35$</td>
<td>$16.7 \pm 2.41$</td>
<td>$1393.33 \pm 26.5$</td>
</tr>
<tr>
<td>KG-DQN Full (D)</td>
<td>$2.7 \pm 0.65$</td>
<td>$19.7 \pm 2.0$</td>
<td>$274.76 \pm 21.45$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results for horror games. Note that the reward type is dense for all results. "KG-DQN Full" refers to KG-DQN seeded, transferred from source. All experiment with QA indicate pre-training.</p>
<table>
<thead>
<tr>
<th>Experiment</th>
<th>Init. Rwd.</th>
<th>Final Rwd.</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Source Game (Afflicted)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN no transfer</td>
<td>$3.0 \pm 1.3$</td>
<td>$14.1 \pm 1.73$</td>
<td>$1934.7 \pm 85.67$</td>
</tr>
<tr>
<td>KG-DQN w/ QA</td>
<td>$4.3 \pm 1.34$</td>
<td>$15.1 \pm 1.60$</td>
<td>$1179 \pm 32.07$</td>
</tr>
<tr>
<td>KG-DQN seeded</td>
<td>$4.1 \pm 1.19$</td>
<td>$14.6 \pm 1.26$</td>
<td>$1125.3 \pm 49.57$</td>
</tr>
<tr>
<td>Target Game (Anchorhead)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>KG-DQN untuned</td>
<td>-</td>
<td>$3.8 \pm 0.23$</td>
<td>-</td>
</tr>
<tr>
<td>KG-DQN no transfer</td>
<td>$1.0 \pm 0.34$</td>
<td>$6.8 \pm 0.42$</td>
<td>-</td>
</tr>
<tr>
<td>KG-DQN w/ QA</td>
<td>$3.6 \pm 0.91$</td>
<td>$24.8 \pm 0.6$</td>
<td>$4874 \pm 90.74$</td>
</tr>
<tr>
<td>KG-DQN seeded</td>
<td>$1.7 \pm 0.62$</td>
<td>$26.6 \pm 0.42$</td>
<td>$4937 \pm 42.93$</td>
</tr>
<tr>
<td>KG-DQN full</td>
<td>$4.1 \pm 0.9$</td>
<td>$39.9 \pm 0.53$</td>
<td>$4334.3 \pm 56.13$</td>
</tr>
</tbody>
</table>
<p>mains seeding and QA pre-training improve performance by similar amounts from the baseline on both the source and target task games. A series of t-tests comparing the results of the pre-training and graph seeding with the baseline KG-DQN show that all results are significant with $p&lt;0.05$. Both the pre-training and graph seeding perform similar functions in enabling the agent to explore more effectively while picking high utility actions.</p>
<p>Even when untuned, i.e. evaluating the agent on the target task after having only trained on the source task, the agent shows better performance than training on the target task from scratch using the sparse reward. As expected, we see a further gain in performance when the dense reward function is used for both of these domains as well. In the horror domain, the agent fails to converge to a state where it is capable of finishing the game without the dense reward function due to the horror games being more complex.</p>
<p>When an agent is trained using on just the target task horror game, Anchorhead, it does not converge to completion and only gets as far as achieving a reward of approximately 7 (max. observed reward from the best model is 41). This corre-
sponds to a point in the game where the player is required to use a term in an action that the player has never observed before, "look up Verlac" when in front of a certain file cabinet-"Verlac" being the unknown entity. Without seeding or QA pretraining, the agent is unable to cut down the action space enough to effectively explore and find the solution to progress further. The relative effectiveness of the gains in initial reward due to seeding appears to depend on the game and the corresponding static text document. In all situations except Anchohead, seeding provides comparable gains in initial reward as compared to QA - there is no statistical difference between the two when performing similar t-tests.</p>
<p>When the full system is used-i.e. we seed the knowledge graph, pre-train QA, then train the source task game, then the target task game using the augmented reward function-we see a significant gain in performance, up to an $80 \%$ gain in terms of completion steps in some cases. The bottleneck at reward 7 is still difficult to pass, however, as seen in Fig. 6, in which we can see that the agent spends a relatively long time around this reward level unless the full transfer technique is</p>
<p>used. We further see in Figures 5, 6 that transferring knowledge results in the agent learning this higher quality policy much faster. In fact, we note that training a full system is more efficient than just training the agent on a single task, i.e. training a QA system then a source task game for 50 episodes then transferring and training a seeded target task game for 50 episodes is more effective than just training the target task game by itself for even 150+ episodes.</p>
<h2>8 Conclusions</h2>
<p>We have demonstrated that using knowledge graphs as a state representation enables efficient transfer between deep reinforcement learning agents designed to play text-adventure games, reducing training times and increasing the quality of the learned control policy. Our results show that we are able to extract a graph from a general static text resource and use that to give the agent knowledge regarding domain specific vocabulary, object affordances, etc. Additionally, we demonstrate that we can effectively transfer knowledge using deep Q-network parameter weights, either by pretraining portions of the network using a questionanswering system or by transferring parameters from a source to a target game. Our agent trains faster overall, including the number of episodes required to pre-train and train on a source task, and performs up to $80 \%$ better on convergence than an agent not utilizing these techniques.</p>
<p>We conclude that knowledge graphs enable transfer in deep reinforcement learning agents by providing the agent with a more explicit-and interpretable-mapping between the state and action spaces of different games. This mapping helps overcome the challenges twin challenges of partial observability and combinatorially large action spaces inherent in all text-adventure games by allowing the agent to better explore the stateaction space.</p>
<h2>9 Acknowledgements</h2>
<p>This material is based upon work supported by the National Science Foundation under Grant No. IIS1350339. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2>References</h2>
<p>Prithviraj Ammanabrolu and Mark O. Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019.</p>
<p>Gabor Angeli, Johnson Premkumar, Melvin Jose, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).</p>
<p>Antoine Bordes, Nicolas Usunier, Ronan Collobert, and Jason Weston. 2010. Towards understanding situated natural language. In Proceedings of the 2010 International Conference on Artificial Intelligence and Statistics.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Association for Computational Linguistics (ACL).</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld : A Learning Environment for Text-based Games. In Proceedings of the ICML/IJCAI 2018 Workshop on Computer Games, page 29.</p>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.</p>
<p>Milica Gasic, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve J. Young. 2013. Pomdp-based dialogue manager adaptation to extended domains. In SIGDIAL Conference.</p>
<p>Matan Haroush, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. 2018. Learning How Not to Act in Text-Based Games. In Workshop Track at ICLR 2018, pages 1-4.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D. Williams. 2019b. Nail: A general interactive fiction agent. CoRR, abs/1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural Language Action Space. In Association for Computational Linguistics (ACL).</p>
<p>Girish Joshi and Girish Chowdhary. 2018. Crossdomain transfer in reinforcement learning using target apprentice. In Proceedings of the International Conference on Robotics and Automation, pages $7525-7532$.</p>
<p>George Konidaris and Andrew G. Barto. 2007. Building portable options: Skill transfer in reinforcement learning. In IJCAI.</p>
<p>George Konidaris, Ilya Scheidwasser, and Andrew G. Barto. 2012. Transfer in reinforcement learning via shared features. The Journal of Machine Learning Research, 13:1333-1371.</p>
<p>Long-Ji Lin. 1993. Reinforcement learning for robots using neural networks. Ph.D. thesis, Carnegie Mellon University.</p>
<p>Yaxin Liu and Peter Stone. 2006. Value-function-based transfer for reinforcement learning using structure mapping. In $A A A I$.</p>
<p>Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. 2017. Deep transfer in reinforcement learning by language grounding. Journal of Artificial Intelligence Research, 63.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language Understanding for Textbased Games Using Deep Reinforcement Learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Trung Thanh Nguyen, Tomi Silander, and Tze-Yun Leong. 2012. Transferring expectations in modelbased reinforcement learning. In NIPS.</p>
<p>Emilio Parisotto, Jimmy Ba, and Ruslan R. Salakhutdinov. 2016. Actor-mimic: Deep multitask and transfer reinforcement learning. CoRR, abs/1511.06342.</p>
<p>Janarthanan Rajendran, Aravind S. Lakshminarayanan, Mitesh M. Khapra, P. Prasanna, and Balaraman Ravindran. 2017. Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain. In $I C L R$.</p>
<p>Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks. CoRR, abs/1606.04671.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT Press.</p>
<p>Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. 2018. Towards solving text-based
games by producing adaptive action spaces. In Proceedings of the 2018 NeurIPS Workshop on Wordplay: Reinforcement and Language Learning in Text-based Games.</p>
<p>Matthew E. Taylor, Nicholas K. Jong, and Peter Stone. 2008. Transferring instances for model-based reinforcement learning. In ECML/PKDD.</p>
<p>Matthew E. Taylor and Peter Stone. 2009. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10:1633-1685.</p>
<p>Zhuoran Wang, Tsung-Hsien Wen, Pei hao Su, and Yannis Stylianou. 2015. Learning domainindependent dialogue policies via ontology parameterisation. In SIGDIAL Conference.
H. Yin and S. J. Pan. 2017. Knowledge transfer for deep reinforcement learning with hierarchical experience replay. In Proceedings of the ThirtyFirst AAAI Conference on Artificial Intelligence, AAAI'17, pages 1640-1646. AAAI Press.</p>
<p>Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia. 2019. Deep reinforcement learning with relational inductive biases. In International Conference on Learning Representations.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://ifdb.tads.org/viewgame?id= qzftg3j8nh5f34i2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://ifdb.tads.org/viewgame?id= jhbd0kja1t57uop
${ }^{5}$ https://ifdb.tads.org/viewgame?id= ep14q2933rczoo9x
${ }^{6}$ https://ifdb.tads.org/viewgame?id= op0uw1gn1tjqmjt7&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
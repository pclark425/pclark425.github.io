<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-418a2d5a6c6027e079357d872a0596ec5b344289</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/418a2d5a6c6027e079357d872a0596ec5b344289" target="_blank">Symbolic Plans as High-Level Instructions for Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Automated Planning and Scheduling</p>
                <p><strong>Paper TL;DR:</strong> An empirical evaluation shows that the use of high-level symbolic action models as a framework for defining final-state goal tasks and automatically producing their corresponding reward functions converges to near-optimal solutions faster than standard RL and HRL methods.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) agents seek to maximize the cumulative reward obtained when interacting with their environment. Users define tasks or goals for RL agents by designing specialized reward functions such that maximization aligns with task satisfaction. This work explores the use of high-level symbolic action models as a framework for defining final-state goal tasks and automatically producing their corresponding reward functions. We also show how automated planning can be used to synthesize high-level plans that can guide hierarchical RL (HRL) techniques towards efficiently learning adequate policies. We provide a formal characterization of taskable RL environments and describe sufficient conditions that guarantee we can satisfy various notions of optimality (e.g., minimize total cost, maximize probability of reaching the goal). In addition, we do an empirical evaluation that shows that our approach converges to near-optimal solutions faster than standard RL and HRL methods and that it provides an effective framework for transferring learned skills across multiple tasks in a given environment.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Planning-guided HRL curriculum (seq/pop)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-sequence curriculum via symbolic planning and hierarchical reinforcement learning (seq, pop, seq_n, pop_n)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum-like evaluation protocol where agents learn a sequence of 4 tasks of increasing complexity in two grid-world environments, using symbolic planning to produce high-level instructions (sequential or partial-order plans) whose associated low-level option policies are trained and transferred across tasks; variants include regression-based execution monitoring (seq_n, pop_n).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Planning-guided hierarchical agent (seq/pop variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A hierarchical RL agent: a meta-controller selects among high-level actions (from a sequential or partial-order plan) and each high-level action maps to an option whose low-level policy is trained with Q-learning; meta-controller also trained with Q-learning. Variants: seq (execute sequential plan), pop (use partial-order plan), seq_n / pop_n (regression-based execution monitoring), and baseline hrl (options framework without planning) and ql (flat Q-learning).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>OFFICEWORLD and MINECRAFTWORLD (gridworld benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Low-level grid-world environments (not interactive text): OFFICEWORLD is a navigation and delivery domain where an agent moves on a grid, picks up/delivers coffee and mail, and must avoid penalty cells; MINECRAFTWORLD is a Minecraft-inspired grid with raw materials, crafting/workbench locations, and composition actions to make tools and goods. Interactions are movement, pickup/automatic transformations on visiting workbench cells, and other environment-specific events.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>compositional procedural tasks (household-like and crafting procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>OFFICEWORLD: get-coffee, get-mail, deliver-coffee, deliver-mail (deliver both coffee and mail to office). MINECRAFTWORLD: make a pickaxe (collect wood and iron and take to locations), get a gem (requires making a pickaxe and then reaching gem location).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Hierarchical task structure: complex tasks decompose into high-level planning actions (e.g., get-coffee) that correspond to options; plans (sequential or partial-order) specify ordering/constraints over these high-level actions allowing composition of primitive actions into multi-step procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Sequential task-sequence with option transfer (complexity-ordered tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Authors define a sequence of 4 tasks per environment ordered roughly by increasing complexity. For each algorithm, training proceeds on each task for a fixed budget of training steps; when a task's budget is exhausted (or episode terminates on goal or after 1000 steps) training moves to the next task. Option policies learned during earlier tasks are transferred to later tasks (i.e., option policies persist across tasks), while the meta-controller is reset between tasks. This produces a curriculum where earlier tasks provide learned low-level skills reused for later, harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>prerequisite skills / increasing task complexity</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Sequence of 4 tasks from simple one-or-two-step behaviors up to multi-step compositional tasks (e.g., final OFFICEWORLD task: deliver both coffee and mail; final MINECRAFTWORLD tasks: multi-stage crafting plus navigation to obtain gem).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>OFFICEWORLD (final task): pop (partial-order guided HRL with option transfer) converged in ≈70,000 training steps to a policy whose traces were typically ~10 steps from optimal. hrl (options without planning) required ≥1,800,000 steps to reach comparable quality and did not appear stable within 5,000,000 steps; hrl eventually reached solutions ~5 steps worse than optimal after long training. q-learning (flat) converged to optimality after ≈3,850,000 training steps. MINECRAFTWORLD: seq yielded reasonable results with little training; pop did not outperform hrl; executing regression-based monitoring (pop_n) significantly improved pop in MINECRAFTWORLD (qualitative improvement reported), though exact step counts for Minecraft are not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Baselines trained without planning-guided curriculum: q-learning required ≈3,850,000 steps to optimality on OFFICEWORLD final task; hrl required ≥1.8M steps to reach policies comparable to pop (but still worse by a few steps). The paper does not present an explicit ablation that trains with the same tasks in random order (i.e., no explicit random-order curriculum comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper compares multiple variants of the planning-guided approach (seq, pop, seq_n, pop_n) and baselines (hrl, ql). Key contrasts: pop (partial-order plan) outperforms seq and baselines in OFFICEWORLD under the transfer protocol; regression/monitoring variants (seq_n, pop_n) yield more stable performance in OFFICEWORLD and pop_n significantly outperforms pop in MINECRAFTWORLD, indicating that monitoring and handling unexpected outcomes matters when models are inconsistent. There is no comparison of different task-ordering strategies (e.g., random vs. difficulty-ordered) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The paper reports transfer of learned option policies across the sequence of tasks, which substantially speeds learning on later tasks; this is shown empirically (large reduction in training steps needed in OFFICEWORLD). There is no explicit test of compositional generalization to novel unseen combinations beyond the pre-defined task sequence (i.e., no results on zero-shot generalization to new task compositions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Using symbolic planning to generate high-level instructions and restricting HRL to follow partial-order plans (pop) yields large sample-efficiency gains: e.g., pop reached near-good policies in ~70k steps vs millions for baselines on OFFICEWORLD. 2) Transferring option policies across tasks (the curriculum-like sequence) is an effective way to reuse low-level skills and accelerate learning of harder tasks. 3) Regression-based execution monitoring (seq_n/pop_n) improves robustness, especially when the symbolic model is inconsistent with low-level dynamics (notably in MINECRAFTWORLD). 4) The benefit depends on model quality: when the symbolic model is less consistent, seq can outperform pop, and monitoring becomes important. 5) The paper does not address interactive text environments nor explicitly target commonsense or science-procedure teaching tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notes_on_relevance_to_query</strong></td>
                            <td>The paper describes a curriculum-like protocol (ordered tasks with option transfer) and compares multiple planning-guided HRL strategies, but (a) the environments are gridworlds (OFFICEWORLD, MINECRAFTWORLD), not interactive text environments, and (b) the procedures learned are household-like delivery and crafting tasks rather than explicit commonsense or science lab procedures. There is no explicit curriculum targeted at teaching commonsense or science procedures in interactive text environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Symbolic Plans as High-Level Instructions for Reinforcement Learning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modular multitask reinforcement learning with policy sketches <em>(Rating: 2)</em></li>
                <li>Using reward machines for high-level task specification and decomposition in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Using abstract models of behaviours to automatically generate reinforcement learning hierarchies <em>(Rating: 2)</em></li>
                <li>Plan-based reward shaping for reinforcement learning <em>(Rating: 2)</em></li>
                <li>SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1570",
    "paper_id": "paper-418a2d5a6c6027e079357d872a0596ec5b344289",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Planning-guided HRL curriculum (seq/pop)",
            "name_full": "Task-sequence curriculum via symbolic planning and hierarchical reinforcement learning (seq, pop, seq_n, pop_n)",
            "brief_description": "A curriculum-like evaluation protocol where agents learn a sequence of 4 tasks of increasing complexity in two grid-world environments, using symbolic planning to produce high-level instructions (sequential or partial-order plans) whose associated low-level option policies are trained and transferred across tasks; variants include regression-based execution monitoring (seq_n, pop_n).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Planning-guided hierarchical agent (seq/pop variants)",
            "agent_description": "A hierarchical RL agent: a meta-controller selects among high-level actions (from a sequential or partial-order plan) and each high-level action maps to an option whose low-level policy is trained with Q-learning; meta-controller also trained with Q-learning. Variants: seq (execute sequential plan), pop (use partial-order plan), seq_n / pop_n (regression-based execution monitoring), and baseline hrl (options framework without planning) and ql (flat Q-learning).",
            "agent_size": null,
            "environment_name": "OFFICEWORLD and MINECRAFTWORLD (gridworld benchmarks)",
            "environment_description": "Low-level grid-world environments (not interactive text): OFFICEWORLD is a navigation and delivery domain where an agent moves on a grid, picks up/delivers coffee and mail, and must avoid penalty cells; MINECRAFTWORLD is a Minecraft-inspired grid with raw materials, crafting/workbench locations, and composition actions to make tools and goods. Interactions are movement, pickup/automatic transformations on visiting workbench cells, and other environment-specific events.",
            "procedure_type": "compositional procedural tasks (household-like and crafting procedures)",
            "procedure_examples": "OFFICEWORLD: get-coffee, get-mail, deliver-coffee, deliver-mail (deliver both coffee and mail to office). MINECRAFTWORLD: make a pickaxe (collect wood and iron and take to locations), get a gem (requires making a pickaxe and then reaching gem location).",
            "compositional_structure": "Hierarchical task structure: complex tasks decompose into high-level planning actions (e.g., get-coffee) that correspond to options; plans (sequential or partial-order) specify ordering/constraints over these high-level actions allowing composition of primitive actions into multi-step procedures.",
            "uses_curriculum": true,
            "curriculum_name": "Sequential task-sequence with option transfer (complexity-ordered tasks)",
            "curriculum_description": "Authors define a sequence of 4 tasks per environment ordered roughly by increasing complexity. For each algorithm, training proceeds on each task for a fixed budget of training steps; when a task's budget is exhausted (or episode terminates on goal or after 1000 steps) training moves to the next task. Option policies learned during earlier tasks are transferred to later tasks (i.e., option policies persist across tasks), while the meta-controller is reset between tasks. This produces a curriculum where earlier tasks provide learned low-level skills reused for later, harder tasks.",
            "curriculum_ordering_principle": "prerequisite skills / increasing task complexity",
            "task_complexity_range": "Sequence of 4 tasks from simple one-or-two-step behaviors up to multi-step compositional tasks (e.g., final OFFICEWORLD task: deliver both coffee and mail; final MINECRAFTWORLD tasks: multi-stage crafting plus navigation to obtain gem).",
            "performance_with_curriculum": "OFFICEWORLD (final task): pop (partial-order guided HRL with option transfer) converged in ≈70,000 training steps to a policy whose traces were typically ~10 steps from optimal. hrl (options without planning) required ≥1,800,000 steps to reach comparable quality and did not appear stable within 5,000,000 steps; hrl eventually reached solutions ~5 steps worse than optimal after long training. q-learning (flat) converged to optimality after ≈3,850,000 training steps. MINECRAFTWORLD: seq yielded reasonable results with little training; pop did not outperform hrl; executing regression-based monitoring (pop_n) significantly improved pop in MINECRAFTWORLD (qualitative improvement reported), though exact step counts for Minecraft are not reported in the paper.",
            "performance_without_curriculum": "Baselines trained without planning-guided curriculum: q-learning required ≈3,850,000 steps to optimality on OFFICEWORLD final task; hrl required ≥1.8M steps to reach policies comparable to pop (but still worse by a few steps). The paper does not present an explicit ablation that trains with the same tasks in random order (i.e., no explicit random-order curriculum comparison).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper compares multiple variants of the planning-guided approach (seq, pop, seq_n, pop_n) and baselines (hrl, ql). Key contrasts: pop (partial-order plan) outperforms seq and baselines in OFFICEWORLD under the transfer protocol; regression/monitoring variants (seq_n, pop_n) yield more stable performance in OFFICEWORLD and pop_n significantly outperforms pop in MINECRAFTWORLD, indicating that monitoring and handling unexpected outcomes matters when models are inconsistent. There is no comparison of different task-ordering strategies (e.g., random vs. difficulty-ordered) reported.",
            "transfer_generalization": "The paper reports transfer of learned option policies across the sequence of tasks, which substantially speeds learning on later tasks; this is shown empirically (large reduction in training steps needed in OFFICEWORLD). There is no explicit test of compositional generalization to novel unseen combinations beyond the pre-defined task sequence (i.e., no results on zero-shot generalization to new task compositions).",
            "key_findings": "1) Using symbolic planning to generate high-level instructions and restricting HRL to follow partial-order plans (pop) yields large sample-efficiency gains: e.g., pop reached near-good policies in ~70k steps vs millions for baselines on OFFICEWORLD. 2) Transferring option policies across tasks (the curriculum-like sequence) is an effective way to reuse low-level skills and accelerate learning of harder tasks. 3) Regression-based execution monitoring (seq_n/pop_n) improves robustness, especially when the symbolic model is inconsistent with low-level dynamics (notably in MINECRAFTWORLD). 4) The benefit depends on model quality: when the symbolic model is less consistent, seq can outperform pop, and monitoring becomes important. 5) The paper does not address interactive text environments nor explicitly target commonsense or science-procedure teaching tasks.",
            "notes_on_relevance_to_query": "The paper describes a curriculum-like protocol (ordered tasks with option transfer) and compares multiple planning-guided HRL strategies, but (a) the environments are gridworlds (OFFICEWORLD, MINECRAFTWORLD), not interactive text environments, and (b) the procedures learned are household-like delivery and crafting tasks rather than explicit commonsense or science lab procedures. There is no explicit curriculum targeted at teaching commonsense or science procedures in interactive text environments.",
            "uuid": "e1570.0",
            "source_info": {
                "paper_title": "Symbolic Plans as High-Level Instructions for Reinforcement Learning",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modular multitask reinforcement learning with policy sketches",
            "rating": 2
        },
        {
            "paper_title": "Using reward machines for high-level task specification and decomposition in reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Using abstract models of behaviours to automatically generate reinforcement learning hierarchies",
            "rating": 2
        },
        {
            "paper_title": "Plan-based reward shaping for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning",
            "rating": 1
        }
    ],
    "cost": 0.011443499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Symbolic Plans as High-Level Instructions for Reinforcement Learning</h1>
<p>León Illanes, Xi Yan, Rodrigo Toro Icarte, Sheila A. McIlraith<br>Department of Computer Science, University of Toronto<br>Vector Institute<br>Toronto, Canada<br>{lillanes, rntoro, sheila}@cs.toronto.edu, xi.yan@mail.utoronto.ca</p>
<h4>Abstract</h4>
<p>Reinforcement learning (RL) agents seek to maximize the cumulative reward obtained when interacting with their environment. Users define tasks or goals for RL agents by designing specialized reward functions such that maximization aligns with task satisfaction. This work explores the use of highlevel symbolic action models as a framework for defining final-state goal tasks and automatically producing their corresponding reward functions. We also show how automated planning can be used to synthesize high-level plans that can guide hierarchical RL (HRL) techniques towards efficiently learning adequate policies. We provide a formal characterization of taskable RL environments and describe sufficient conditions that guarantee we can satisfy various notions of optimality (e.g., minimize total cost, maximize probability of reaching the goal). In addition, we do an empirical evaluation that shows that our approach converges to near-optimal solutions faster than standard RL and HRL methods and that it provides an effective framework for transferring learned skills across multiple tasks in a given environment.</p>
<h2>1 Introduction</h2>
<p>Reinforcement learning (RL) methods represent the state of the art for solving complex continuous control problems in robotics (Van Hoof et al. 2015; Kumar, Todorov, and Levine 2016; Kumar et al. 2016; Falco et al. 2018; Andrychowicz et al. 2018; Akkaya et al. 2019). For instance, the OpenAI lab recently showed that model-free RL can be used to learn to control a human-like robot hand to purposefully manipulate complex objects, such as a Rubik's Cube (Akkaya et al. 2019). The strength of model-free RL comes from being able to learn policies that maximize an external reward signal by directly interacting with the environmentwithout requiring a predefined model of the complex physics equations that control it (nor trying to learn them).</p>
<p>This generality comes with a cost, though. As the environment dynamics and reward structures are initially unknown, RL methods mostly rely on random exploration to collect rewards and then improve their current policy accordingly. As such, these methods are sample inefficient (i.e., require</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>billions of interactions with the environment before learning better-than-random policies). Further, these systems are typically not taskable. If you would like an RL agent to solve task A , then you would have to program a reward function such that its optimal policy would solve A. If, later on, you would like the agent to perform task B, then you would have to program a new reward function for B and the RL agent would have to learn a brand new policy for B from scratchwhich is a problem known as transfer learning (Taylor and Stone 2009). A number of approaches have been proposed to address these shortcomings including efforts to learn hierarchical representations (Dietterich 2000), to define options or macro-actions that can be used by the RL system (Sutton, Precup, and Singh 1999), or to learn skills that are independent of the state space where they were learned (Konidaris and Barto 2007).</p>
<p>Our interest in this paper is in leveraging high-level symbolic planning models and automated plan synthesis techniques, in concert with state-of-the-art RL techniques, with the objective of improving sample efficiency and creating systems that are human taskable. Our efforts are based on the observation that some approximated understanding of the environment can be characterized as a symbolic planning model-a set of properties of the world and a formal description of actions that cause those properties to change in predictable ways-while leaving (possibly complex) lowlevel aspects of the environment (e.g., the frame by frame outcome of dropping a pen) unspecified.</p>
<p>As a result, our AI agent gets the best of both worlds: (1) it is taskable as the user can define tasks as goal conditions in the symbolic domain (and a reward function is automatically computed for such a task), (2) it improves sample efficiency as the high-level plans can be used for transferring learning from previously learned policies, and (3) it can learn complex low-level control policies as it relies on model-free RL to accommodate for all the information missing in the highlevel model. To achieve this, we build on ideas for learning by instructions in RL (Andreas, Klein, and Levine 2017; Toro Icarte et al. 2018a; 2018c). That work shows that sample efficiency can be improved if a manually generated description of the task is given to the agent. In this work, we propose to automatically generate useful instructions for RL</p>
<p>agents using the high-level model of the environment and describe an approach-based on hierarchical reinforcement learning (HRL)—that exploits such instructions. We compare our approach to standard forms of HRL and show that the combination of high-level symbolic planning and lowlevel reinforcement learning is an effective method for specifying tasks to RL agents and, more importantly, for learning high-quality policies-for previously unseen tasks-up to an order of magnitude faster than using standard RL.</p>
<p>Note that the idea of combining high-level symbolic planning with low-level RL has a long history. Some well-known examples include work done by Ryan (2002), Grounds and Kudenko (2008), Grześ and Kudenko (2008), Yang et al. (2018), and Lyu et al. (2019). Informed by this previous work, we contribute a formal characterization of a relevant problem, which we call taskable $R L$, and a novel approach to transfer learned policies and guide exploration in RL based on high-level plans. Building on this, we provide theoretical analysis regarding sufficient conditions for ensuring our approach satisfies various notions of optimality and show empirical results that validate the efficiency of our approach.</p>
<h2>2 Preliminaries</h2>
<p>In this section we establish relevant notation and review key aspects of reinforcement learning and automated planning. In addition, we describe a simple running example.</p>
<h2>Reinforcement Learning</h2>
<p>For the purposes of this work, we will say that the environment in which an RL agent acts is formalized as an Markov Decision Process (MDP) $M=\langle S, A, r, p, \gamma\rangle$, where $S$ is its set of states, $A$ is the set of available actions, $r: S \times A \times S \rightarrow$ $\mathbb{R}$ is its reward function, $p\left(s_{t+1} \mid s_{t}, a_{t}\right)$ is its transition probability distribution, and $\gamma \in(0,1]$ is the discount factor. A policy for $M$ is defined as a probability distribution $\pi(a \mid s)$ that establishes the probability of the agent taking action $a$ given that its current state is $s$. Then, the RL problem consists of finding an optimal policy $\pi^{*}$ that maximizes the expected discounted future reward obtained from all $s \in S$ :</p>
<p>$$
\pi^{*}=\underset{\pi}{\arg \max } \sum_{s \in S} v_{\pi}(s)
$$</p>
<p>where $v_{\pi}(s)$ is known as the value function and models the expected discounted future reward obtained when starting at state $s \in S$ and selecting actions according to $\pi$ :</p>
<p>$$
v_{\pi}(s)=\mathbb{E}<em t="0">{\pi}\left[\sum</em>=s\right]
$$}^{\infty} \gamma^{t} r_{t} \mid s_{0</p>
<p>Crucially, the agent is not given access to the model of the environment (i.e., $p$ and $r$ ). Instead, the agent must learn optimal behaviour by interacting with the environment. At any time step, the agent observes the current state $s \in S$ and executes an action $a \in A$ according to its current policy $\pi$. As a result, the environment returns the next state $s^{\prime} \in S$ (sampled from $p\left(s^{\prime} \mid s, a\right)$ ) and an immediate reward $r^{\prime}=r\left(s, a, s^{\prime}\right)$. The experience $\left(s, a, r^{\prime}, s^{\prime}\right)$ is then used by
the agent to improve its current policy $\pi$. The main distinctions between RL methods are on how to select the next action and how to improve the current policy using sampled experiences.</p>
<p>For example, $q$-learning (Watkins and Dayan 1992) is an RL approach that learns optimal policies (in the limit) by using sampled experiences to estimate the optimal q-function $q^{<em>}(s, a)$ for every state $s \in S$ and action $a \in A$. The optimal q-function $q^{</em>}(s, a)$ is equal to the expected discounted future reward received by performing action $a$ in state $s$ and following an optimal policy afterwards. Given an experience $\left(s, a, r^{\prime}, s^{\prime}\right)$, the q-value estimate $\hat{q}(s, a)$ is updated as follows:</p>
<p>$$
\hat{q}(s, a) \stackrel{\alpha}{\longleftrightarrow}\left(r^{\prime}+\gamma \max _{a^{\prime} \in A} \hat{q}\left(s^{\prime}, a^{\prime}\right)\right)
$$</p>
<p>where $x \stackrel{\alpha}{\longleftrightarrow} y$ is shorthand notation for $x \leftarrow x+\alpha \cdot(y-x)$ and $\alpha \in(0,1]$ is a hyperparameter called the learning rate. Note that an optimal policy $\pi^{<em>}$ can be extracted from $q^{</em>}(s, a)$ by always selecting the action $a \in A$ with the largest q-value for the current state $s$. To explore the environment, q-learning uses an $\epsilon$-greedy exploratory policy. This is, it selects a random action with probability $\epsilon$ and the action with the largest $\hat{q}(s, \cdot)$ value with probability $1-\epsilon$.</p>
<p>Temporal Abstraction Standard RL techniques are faced with significant issues when applied on large-scale problems. In practical terms, RL algorithms need a large amount of training steps in order to converge. A popular technique for dealing with these issues is to consider temporallyextended macro-actions that represent useful high-level behaviours. In particular, the options framework proposes the use of policies that are trained for achieving specific highlevel behaviours, coupled with well-defined criteria for their termination (Sutton, Precup, and Singh 1999). Given the current state, an agent acting within this framework chooses one among the high-level options and executes its policy until it terminates. In order to select which options will be executed, the agent has a higher order policy or meta-controller which can also be learned through RL.</p>
<p>For a given environment $M=\langle S, A, r, p, \gamma\rangle$, an option is formalized as $o=\left\langle\pi_{o}, r_{o}, T_{o}\right\rangle$ where $\pi_{o}$ is the option's policy, $r_{o}$ is a reward function used for training option $o$, and $T_{o} \subseteq S$ is the set of states where the option terminates.</p>
<p>In the options framework, a set of predefined options $O$ is given to the agent. Then, learning occurs at two levels. At the options' level, the policies are updated using the usual experiences $s, a, s^{\prime}$, and the reward that comes from $r_{o}$. If qlearning were used, then one q-function $\tilde{q}<em o="o">{o}$ would be learned per option $o \in O$ (where $\tilde{q}</em>$ ) and all of them would be updated as follows:}(s, a)=0$ for all $s \in T_{o</p>
<p>$$
\tilde{q}<em o="o">{o}(s, a) \stackrel{\alpha}{\longleftarrow}\left(r</em>\right)+\gamma \max }\left(s, a, s^{\prime<em o="o">{a^{\prime} \in A} \tilde{q}</em>\right)\right)
$$}\left(s^{\prime}, a^{\prime</p>
<p>At the level of the meta-controller, the learning task consists of finding an optimal policy $\pi^{*}(o \mid s)$ to select the next option $o \in O$ to execute given the current state $s \in S$. Learning at the level of the meta-controller occurs only when the current option terminates. If option $o$ was executed in state $s$ and terminated in state $s^{\prime}$ after executing $k$ actions $a_{t}$ and receiving</p>
<p>$k$ immediate rewards $r_{t}$, then a q-learning algorithm would use this experience to update the meta-controller estimate of the q-function $\tilde{q}$ as follows:</p>
<p>$$
\tilde{q}(s, o) \stackrel{\alpha}{\leftarrow}\left(\sum_{t=1}^{k} \gamma^{k-1} r_{t}+\gamma^{k} \max _{o^{\prime} \in \mathcal{O}} \tilde{q}\left(s^{\prime}, o^{\prime}\right)\right)
$$</p>
<h2>Symbolic Planning</h2>
<p>We specify planning domains in terms of a tuple $\mathcal{D}=$ $\langle\mathcal{F}, \mathcal{A}\rangle . \mathcal{F}$ is a set of propositional symbols, called the fluents of $\mathcal{D}$, and $\mathcal{A}$ is the set of planning actions in the domain. Planning states are specified as subsets of $\mathcal{F}$, so that state $\mathcal{S} \subseteq \mathcal{F}$ represents the situation in which the fluents in $\mathcal{S}$ are all true and those not in $\mathcal{S}$ are false. An action $a \in \mathcal{A}$ is defined by a tuple $a=\left\langle p r e^{+}, p r e^{-}, e f f^{+}, e f f^{-}\right\rangle$where each of its elements is a subset of $\mathcal{F}$. Here, $p r e^{+}$and $p r e^{-}$(eff ${ }^{+}$ and eff ${ }^{-}$) are disjoint and represent the positive and negative preconditions (effects) of $a$, respectively. We say $a \in \mathcal{A}$ is applicable over state $\mathcal{S}$ when $p r e^{+} \subseteq \mathcal{S}$ and $p r e^{-} \cap \mathcal{S}=\varnothing$. The result of applying $a$ over state $\mathcal{S}$ is a state given by the function $\delta(\mathcal{S}, a)=\left(\mathcal{S} \backslash e f f^{-}\right) \cup e f f^{+}$. When $a$ is not applicable over $\mathcal{S}, \delta(\mathcal{S}, a)$ is undefined.</p>
<p>A planning task for domain $\mathcal{D}$ is formalized as $\mathcal{T}=$ $\langle\mathcal{D}, \mathcal{I}, \mathcal{G}\rangle$, where $\mathcal{I}$ is an initial state and $\mathcal{G}$ is the task's goal condition. The goal $\mathcal{G}=\left\langle\mathcal{G}^{+}, \mathcal{G}^{-}\right\rangle$where $\mathcal{G}^{+}$and $\mathcal{G}^{-}$is a pair of disjoint subsets of $\mathcal{F}$ that, respectively, represent positive and negative subgoals. Any state $\mathcal{S}$ such that $\mathcal{G}^{+} \subseteq \mathcal{S}$ and $\mathcal{G}^{-} \cap \mathcal{S}=\varnothing$ is said to be a goal state. A sequence of actions $\Pi=\left[a_{0}, a_{1}, \cdots, a_{n}\right]$ is known as a sequential plan for a task when it is possible to sequentially apply the actions starting at $\mathcal{I}$, and doing so reaches a goal state.</p>
<p>Partial-Order Plans Partial-order plans generalize sequential plans by relaxing the ordering condition over the actions. A partial-order plan is represented by a tuple $\overline{\Pi}=$ $\langle\mathcal{A}, \prec\rangle$, where $\mathcal{A}$ is its set of action occurrences and $\prec$ is a partial order over $\overline{\mathcal{A}}$. The set of linearizations of $\overline{\Pi}$, denoted $\Lambda(\overline{\Pi})$, is the set of all sequences of the action occurrences in $\overline{\mathcal{A}}$ that respect the partial order $\prec$. Any linearization $\Pi \in \Lambda(\overline{\Pi})$ is a sequential plan for the task. Intuitively, a partial-order plan represents a family of related sequential plans. Note that a plan may require using the same action twice. As such, two action occurrences in $\overline{\mathcal{A}}$ may be repetitions of the same action, distinguished only for the purposes of defining the particular partial-order plan.</p>
<h2>Running Example</h2>
<p>We consider a version of the OFFICEWORLD domain described by Toro Icarte et al. (2018c). The low-level environment is represented by the grid displayed in Figure 1. A robot situated in any cell can try to move in any of the four cardinal directions, succeeding only if the movement does not go through a wall. The symbols in the grid represent important features of the environment and different events occur whenever the robot reaches a marked cell. Specifically, the robot will pick up coffee or mail when it reaches the locations marked with blue cups or the green envelope, respectively. If the robot is already carrying coffee or mail, it
will deliver it to the office upon reaching the cell marked with the purple writing hand. The robot can also visit the four named locations (A, B, C, D). The locations marked $#$ are places that the robot should not step on, and doing so results in a large penalty or negative reward $(-10)$. All other successful actions result in a small penalty $(-1)$, whereas failed actions (i.e., attempting to move through a wall) are heavily penalized $(-10)$.</p>
<h2>3 Taskable Reinforcement Learning</h2>
<p>One of the great advantages of symbolic planning is that specifying new simple tasks for a given domain model is very easy. It is with this in mind that we define the problem of taskable RL, where final-state goal tasks can be specified trivially for a given RL environment.</p>
<p>To define goals for tasks in such an environment, we assume the existence of a set of high-level propositions, $P$, and a labeling function $L: S \rightarrow 2^{P}$ that establishes a mapping between low-level states and high-level propositions. These propositions are supposed to represent important state properties that may affect the outcomes of actions or their costs, or that may be of significance to an end user of the system. Finally, we also assume the existence of a constant $R$ that establishes a reward bonus received by the agent when it accomplishes a task. With this, we define taskable RL environments and their associated final-state goal tasks.
Definition 1 (Taskable RL Environment). A taskable reinforcement learning environment is defined by a tuple $E=$ $\langle S, A, r, p, \gamma, P, L, R\rangle$, where $\langle S, A, r, p, \gamma\rangle$ is an MDP, $P$ is a set of propositional variables, $L: S \rightarrow 2^{P}$ is a labeling function, and $R \in \mathbb{R}$ is a parameter called the goal reward.
Definition 2 (Final-state Goal Task). A final-state goal task for taskable RL environment $E=\langle S, A, r, p, \gamma, P, L, R\rangle$ is defined as a tuple $G=\left\langle G^{+}, G^{-}\right\rangle$where its elements are disjoint subsets of $P$. We say a state $g \in S$ is a goal state when $G^{+} \subseteq L(g)$ and $G^{-} \cap L(g)=\varnothing$. We denote the set of all goal states as $\mathbb{G}$. The objective for this task is to find the optimal policy for the MDP $M_{G}=\left\langle S, A, r_{G}, p_{G}, \gamma\right\rangle$, where $r_{G}$ and $p_{G}$ are defined as follows:</p>
<p>$$
\begin{aligned}
&amp; r_{G}\left(s, a, s^{\prime}\right)= \begin{cases}R+r\left(s, a, s^{\prime}\right) &amp; \text { if } s^{\prime} \in \mathbb{G} \text { and } s \neq s^{\prime} \
0 &amp; \text { if } s^{\prime} \in \mathbb{G} \text { and } s=s^{\prime} \
r\left(s, a, s^{\prime}\right) &amp; \text { otherwise }\end{cases} \
&amp; p_{G}\left(s^{\prime} \mid s, a\right)= \begin{cases}0 &amp; \text { if } s \in \mathbb{G} \text { and } s \neq s^{\prime} \
1 &amp; \text { if } s \in \mathbb{G} \text { and } s=s^{\prime} \
p\left(s^{\prime} \mid s, a\right) &amp; \text { otherwise }\end{cases}
\end{aligned}
$$</p>
<p>Intuitively, the goal conditions are used for defining fictional terminal states in the environment. The modified transition probabilities ensure that exiting a goal state is impossible. In turn, the modified reward function ensures that a reward bonus is given when a goal state is first reached, and that no further reward can be accrued after that.</p>
<p>The main motivation behind Definitions 1 and 2 is to allow end-users to define tasks for RL agents with minimal effort. In the same spirit, the main guarantee that we should provide to that end-user is that the RL agent will optimize its</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The OFFICEWORLD running example.
behaviour towards actually accomplishing their tasks. Interestingly, whether a taskable RL environment provides such a guarantee depends on how it defines $r, \gamma$, and $R$.</p>
<h2>Important Properties of Taskable RL</h2>
<p>The purpose of this section is to identify sufficient conditions under which optimal low-level policies would reach the high-level final-state goal defined by the user. We also provide some understanding of the quality of such policies. To do so, we begin by introducing the notion of proper policies, which comes from the stochastic shortest path literature (Eaton and Zadeh 1962; Bertsekas and Tsitsiklis 1991):
Definition 3 (Proper Policy). Given a final-state goal task $G$, we say a policy $\pi$ for its corresponding MDP $M_{G}=$ $\left\langle S, A, r_{G}, p_{G}, \gamma\right\rangle$ is a proper policy if the probability of eventually reaching a goal state by starting at any $s \in S$ and following $\pi$ is 1 . A policy that is not proper is said to be improper.</p>
<p>Proper policies formalize the intuition behind the main property that we would expect from any taskable RL environment: the optimal low-level policies are proper policies for reaching the high-level goals defined by the user. With this, we can establish three theorems that identify sufficient conditions under which such a property holds (Theorems 1 and 2), or a best-case scenario for environments where the property does not hold (Theorem 3). The conditions outlined by the theorems are the ones that should be taken into consideration when designing taskable RL environments.
Theorem 1 (Cost based problems). Let taskable RL environment $E=\langle S, A, r, p, \gamma, P, L, R\rangle$ be such that $r$ is of the restricted form $r: S \times A \times S \rightarrow \mathbb{R}^{-}, \gamma=1$, and $R=0$. Let $G$ be a final-state goal task for $E$. If there exists at least one proper policy for $M_{G}$, then the optimal policy is a proper policy that minimizes the expected cost of reaching a goal state.</p>
<p>Proof sketch. First, assume that the optimal policy, $\pi^{<em>}$, is improper. Then, there exists at least one state $s \in S$ such that the probability of never reaching a goal state when following $\pi^{</em>}$ from $s$ is greater than 0 . Since rewards are negative everywhere except when reaching a goal state, this means that the expected future reward obtained by following $\pi^{<em>}$ from $s$ tends to $-\infty$ and, therefore, the value obtained from summing over all states in $S$ also tends to $-\infty$. Note, however, that any proper policy has, for any state, finite expected
future reward. Then, any proper policy has higher expected reward than $\pi^{</em>}$, and since we know that proper policies exist this leads to a contradiction that proves $\pi^{<em>}$ must be proper. Now, by definition, $\pi^{</em>}$ maximizes the cumulative expected reward. Since all traces must eventually reach a goal state, and no reward is given after this, then $\pi^{*}$ minimizes the expected cost of reaching a goal state.</p>
<p>Theorem 2 (Goal reward problems). Let taskable RL environment $E=\langle S, A, r, p, \gamma, P, L, R\rangle$ be such that $\gamma&lt;1$, $R=1$, and $r\left(s, a, s^{\prime}\right)=0$ for any $s, a, s^{\prime}$. Let $G$ be a finalstate goal task for $E$. If there exists at least one proper policy for $M_{G}$, then the optimal policy is a proper policy that minimizes the expected number of steps before reaching a goal state.</p>
<p>Proof sketch. By following a similar argument to the one used in the proof of Theorem 1, we know that $\pi^{<em>}$ must be a proper policy. Again, by definition, we know that $\pi^{</em>}$ maximizes the cumulative discounted expected reward. Since reward is only given when a goal is reached, and it is discounted with $\gamma&lt;1$, we immediately know that $\pi^{*}$ minimizes the expected steps before reaching a goal.</p>
<p>Theorem 3 (Max-prob problems). Let taskable RL environment $E=\langle S, A, r, p, \gamma, P, L, R\rangle$ be such that $\gamma=1, R=1$, and $r\left(s, a, s^{\prime}\right)=0$ for any $s, a, s^{\prime}$. Let $G$ be a final-state goal task for $E$. Here, the optimal policy for $M_{G}$ maximizes the probability of reaching a goal state.</p>
<p>Proof sketch. Define a function $g: S \rightarrow \mathbb{R}$ as follows:</p>
<p>$$
g(s)= \begin{cases}1 &amp; s \in \mathbb{G} \ v^{*}(s) &amp; \text { otherwise }\end{cases}
$$</p>
<p>Where, $v^{<em>}$ is the value function for the optimal policy of $M_{G}$. Since $v^{</em>}(s)=0$ at every $s \in \mathbb{G}$ and $v^{<em>}$ is optimal, we immediately know that $g$ is optimal. We will show that $g(s)$ represents the maximum achievable probability of eventually reaching a goal state from $s$. The proposition is trivial for $s \in \mathbb{G}$. For $s \notin \mathbb{G}$, we know:
$g(s)=v^{</em>}(s)=\max <em s_prime="s^{\prime">{a \in A} \sum</em>\right)+v^{} \in S} p_{G}\left(s^{\prime} \mid s, a\right) \cdot\left(r\left(s, a, s^{\prime<em>}\left(s^{\prime}\right)\right)$
We can split the sum over $S$ into a sum over $\mathbb{G}$ and one over $S \backslash \mathbb{G}$. Note that for every $s^{\prime} \in \mathbb{G}$ we know that $r\left(s, a, s^{\prime}\right)=$ 1 and $v^{</em>}\left(s^{\prime}\right)=0$, and that for every $s^{\prime} \notin \mathbb{G}$ we know that $r\left(s, a, s^{\prime}\right)=0$ and $v^{*}\left(s^{\prime}\right)=g\left(s^{\prime}\right)$. Then, we can rewrite as:</p>
<p>$$
g(s)=\max <em s_prime="s^{\prime">{a \in A} \sum</em>\right)
$$} \in \mathbb{G}} p_{G}\left(s^{\prime} \mid s, a\right)+\sum_{s^{\prime} \notin \mathbb{G}} p_{G}\left(s^{\prime} \mid s, a\right) \cdot g\left(s^{\prime</p>
<p>This corresponds exactly to the functional equation that defines $g(s)$ as the maximum probability of eventually reaching a goal state from $s$.</p>
<p>Finally, it is worth noting that other combinations of forms of $r$ and values of $\gamma$ and $R$ may occasionally result in undesirable behaviour. For instance, allowing for positive rewards might cause the agent to prefer to collect those rewards instead of achieving the goals set by the user. On the</p>
<p>other extreme, using only negative rewards but $\gamma&lt;1$ might cause the agent to prefer to stay in an area where it will pay a small penalty for eternity instead of paying the possibly high immediate penalty required for achieving a goal state. The same behaviour might occur if we use only negative rewards, $\gamma=1$, and a finite horizon.</p>
<h2>Taskable RL and the Running Example</h2>
<p>We now explain how to represent the OFFICEWORLD running example as a taskable environment. Given that its lowlevel states must be able to distinguish the current position of the robot, whether it is carrying coffee or mail, whether it has already delivered coffee or mail, and whether it has already visited the office or any of the other named locations, then we can define a set of high-level propositions that contains most of the same information, but omits the exact position of the robot, including instead some propositions that represent if the robot is currently at some of the marked locations:</p>
<p>$$
\begin{aligned}
P= &amp; {\text { have-coffee, have-mail, } \
&amp; \text { delivered-coffee,delivered-mail, } \
&amp; \text { visited-office,visited-A, } \
&amp; \text { visited-B,visited-C,visited-D, } \
&amp; \text { at-office, at-A, at-B, at-C, at-D }}
\end{aligned}
$$</p>
<p>This would allow an end-user to easily define tasks for the agent as goal conditions over these propositions. For example, we could ask the agent to deliver both coffee and mail to the office by defining the following goal condition: $\langle{$ delivered-coffee, delivered-mail $},\varnothing\rangle$.</p>
<p>Finally, as the objective of a final-state goal task is still to learn policies for a particular MDP, it can be potentially solved by any RL algorithm. Given, however, that we know of the existence of the high-level propositions in $P$ and the labeling function $L$, we devise a specialized RL algorithm based on symbolic planning that can learn effective policies considerably faster than off-the-shelf RL methods.</p>
<h2>4 Planning Models in RL</h2>
<p>The general idea is to use planning models and solutions computed for them as guidance for solving RL tasks in the low-level environment. To do so, we associate symbolic models to taskable RL environments. Given a taskable environment $E=\langle S, A, r, p, \gamma, P, L, R\rangle$, a symbolic model for $E$ is specified as $\mathcal{M}=\langle\mathcal{D}, \alpha\rangle$, where $\mathcal{D}=\langle\mathcal{F}, \mathcal{A}\rangle$ is a planning domain with $\mathcal{F}=P$ and $\alpha: \mathcal{A} \rightarrow 2^{P} \times 2^{P}$ is a function that associates planning actions with conditions over $P$.</p>
<p>We will use $\alpha$ to associate finite-state goal tasks for the taskable RL environment $E$ with the planning actions. Each such task defines an option. Whenever the task is accomplished, the option terminates. Formally, given a condition $C=\left\langle C^{+}, C^{-}\right\rangle$, we can define the set of all low-level states that satisfy it as $T(C)={s \in S \mid C^{+} \subseteq L(s), C^{-} \cap L(s)=$ $\varnothing}$. Then, for each planning action $a \in \mathcal{A}$, we can define an associated option with termination set $T_{a}=T(\alpha(a))$ and reward function $r_{a}=r_{\alpha(a)}$ (see Definition 2). Finally, we will define an option set $O$ consisting of one option for each distinct $\alpha(a)$ generated this way. Note that two or more planning actions may be associated with the same option.</p>
<h2>Model Quality</h2>
<p>Intuitively, we would expect that following a high-level plan for a task by sequentially executing its associated low-level option policies should reach a goal state with probability 1 . However, to provide such a formal guarantee we need to assume certain properties about the relation between the planning models and the low-level environment. These properties are formalized in the following definition:
Definition 4 (Consistency of symbolic models). We say a symbolic model $\mathcal{M}=\langle\mathcal{D}, \alpha\rangle$ is consistent with a taskable environment $E=\langle S, A, r, p, \gamma, P, L, R\rangle$ if $\mathcal{D}=\langle\mathcal{F}, \mathcal{A}\rangle$, $\mathcal{F}=P, \alpha: \mathcal{A} \rightarrow 2^{P} \times 2^{P}$, and every optimal option policy $\pi_{\alpha(a)}^{<em>}$ associated with any planning action $a \in \mathcal{A}$ terminates with probability 1 and respects $\delta(\mathcal{S}, a)$. This is, if $\pi_{\alpha(a)}^{</em>}$ initiates in state $s_{i} \in S$ and might terminate in state $s_{t} \in S$, then $L\left(s_{t}\right)=\left(L\left(s_{i}\right) \backslash e f f_{a}^{-}\right) \cup e f f_{a}^{+}$for every state $s_{i} \in S$ where $a$ is applicable ( $\operatorname{pre}^{+} \subseteq L\left(s_{i}\right)$ and $\operatorname{pre}^{-} \cap L\left(s_{i}\right)=\varnothing$ ).</p>
<p>Note that defining consistent models is non-trivial since it requires considering the behaviour of optimal policies acting in a possibly complex low-level environment. As such, we do not assume that models are necessarily consistent throughout the rest of the paper (except when analyzing formal properties of our approach).</p>
<h2>Planning Models in the Running Example</h2>
<p>In the running example domain, we consider the following set of high-level actions:</p>
<p>$$
\begin{aligned}
\mathcal{A}= &amp; {\text { get-mail, get-coffee, deliver-mail, } \
&amp; \text { deliver-coffee,visit-A,visit-B, } \
&amp; \text { visit-C,visit-D,visit-office }}
\end{aligned}
$$</p>
<p>Figure 2 shows the preconditions and effects for two of these actions. In addition, it shows the termination conditions for their corresponding options. Note, however, that the finalstate goal task associated with the deliver-coffee action makes no reference to the coffee itself. This is by design and it serves an important purpose: the same final-</p>
<div class="codehilite"><pre><span></span><code><span class="n">get</span><span class="o">-</span><span class="n">coffee</span><span class="p">:</span>
<span class="w">    </span><span class="n">pre</span><span class="o">^</span><span class="p">:</span>
<span class="w">    </span><span class="n">pre</span><span class="o">^</span><span class="p">:</span>
<span class="w">    </span><span class="n">eff</span><span class="o">^</span><span class="p">:</span>
<span class="w">    </span><span class="p">(</span><span class="n">have</span><span class="o">-</span><span class="n">coffee</span><span class="p">)</span>
<span class="w">    </span><span class="n">eff</span><span class="o">-</span><span class="p">:</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">alpha</span>\<span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">get</span><span class="o">-</span><span class="n">coffee</span><span class="p">):</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">langle</span>\<span class="p">{</span>\<span class="p">)</span><span class="w"> </span><span class="n">have</span><span class="o">-</span><span class="n">coffee</span><span class="w"> </span>\<span class="p">(</span>\<span class="p">},</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="n">rangle</span>\<span class="p">)</span>
<span class="n">deliver</span><span class="o">-</span><span class="n">coffee</span><span class="p">:</span>
<span class="w">    </span><span class="n">pre</span><span class="o">^</span><span class="p">:</span>
<span class="w">    </span><span class="n">pre</span><span class="o">^</span><span class="p">:</span>
<span class="w">    </span><span class="n">eff</span><span class="o">^</span><span class="p">:</span>
<span class="w">    </span><span class="p">(</span><span class="n">delivered</span><span class="o">-</span><span class="n">coffee</span><span class="p">,</span>
<span class="w">                        </span><span class="n">at</span><span class="o">-</span><span class="n">office</span>\<span class="p">}</span>
<span class="w">    </span><span class="n">eff</span><span class="o">-</span><span class="p">:</span><span class="w"> </span>\<span class="p">{</span><span class="n">have</span><span class="o">-</span><span class="n">coffee</span>\<span class="p">}</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">alpha</span>\<span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">deliver</span><span class="o">-</span><span class="n">coffee</span><span class="p">):</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">langle</span>\<span class="p">{</span>\<span class="p">)</span><span class="w"> </span><span class="n">at</span><span class="o">-</span><span class="n">office</span><span class="w"> </span>\<span class="p">(</span>\<span class="p">},</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="n">rangle</span>\<span class="p">)</span>
</code></pre></div>

<p>Figure 2: Example actions in the OFFICEWORLD, including associated option termination.</p>
<p>state goal-and option-will be associated with other planning actions that have different preconditions and effects but are realized through the same low-level actions as deliver-coffee (e.g., the deliver-mail action).</p>
<p>For the example task of delivering both coffee and mail, we consider a planning task with initial state $\mathcal{I}=\varnothing$ and goal $\mathcal{G}=\langle{$ delivered-coffee, delivered-mail $}, \varnothing\rangle$. Here, a reasonable solution plan might be the following:</p>
<p>$$
\begin{aligned}
\Pi= &amp; {[\text { get-coffee, get-mail, deliver-coffee, } \
&amp; \text { deliver-mail }] . }
\end{aligned}
$$</p>
<p>Note that the actual number of low-level actions needed to execute the high-level actions depends on the actual position of the robot in the grid. This means that-even if we have optimal low-level policies for performing each of the high-level actions-the plan may be optimal or suboptimal depending on the exact low-level initial state.</p>
<h2>5 Executing Plans</h2>
<p>For a given symbolic model and planning task, we can easily compute a sequential plan by using an off-the-shelf planner. Such a plan can subsequently be used as a naive metacontroller for an HRL system by directly executing-in the provided order-each of the options associated with the actions in the plan. For the example task and corresponding plan, this would result in first executing a policy that achieves have-coffee. After this, the system would execute a policy that terminates when have-mail becomes true. It would continue by executing the option policy associated with the deliver-coffee action, which achieves at-office. At this point, the system would attempt to execute the policy associated with deliver-mail. Since at-office is already true in the current state, the policy would immediately terminate.</p>
<p>This approach has a number of issues, though. First, it may be significantly better to execute the actions in a different order than the strict one defined by the plan. In the example, it may be better to attempt getting the mail before the coffee. Second, if the model is inconsistent with the environment, then executing an option policy might affect propositions that are not mentioned in the description of its high-level action-possibly invalidating the current plan.</p>
<p>To address the first problem, we consider the use of partial-order plans instead of sequential ones. The execution of a partial-order plan $\widetilde{\Pi}=\langle\mathcal{A}, \prec\rangle$ consists on selecting, at every step, some action occurrence $a \in \overline{\mathcal{A}}$ such that there is no other action occurrence $a^{\prime} \prec a$ that has not already been executed. Thus, we can use a partial-order plan as a meta-controller that is restricted to choose among the options that correspond to the high-level actions that can advance the execution of the plan. This meta-controller can be further trained in order to eventually learn the optimal way to execute the partial-order plan.</p>
<p>In the running example, the plan $\Pi$ can be relaxed into a partial-order plan with the same four actions and the following ordering constraints:</p>
<p>$$
\begin{aligned}
&amp; \text { get-coffee } \prec \text { deliver-coffee } \
&amp; \text { get-mail } \prec \text { deliver-mail. }
\end{aligned}
$$</p>
<p>In the initial state, the get-coffee and get-mail actions are valid for the execution of the plan. The metacontroller policy will, then, learn to select one of them based on the low-level state, which includes relevant information about the position of the robot.</p>
<p>To address the second issue, where an option policy affects some propositions other than those mentioned in the corresponding high-level action's effects, we use the notion of regression of a plan to identify-for each action in the plan-the conditions that ensure the plan is still valid. Formally, the regression of a condition given as a pair of positive and negative conditions $\mathcal{C}=\left\langle\mathcal{C}^{+}, \mathcal{C}^{-}\right\rangle$for an action $a=\left\langle p r e^{+}, p r e^{-}, e f f^{+}, e f f^{-}\right\rangle$such that $e f f^{+} \in \mathcal{C}^{+}$and $e f f^{-} \in \mathcal{C}^{-}$is defined as</p>
<p>$$
\begin{aligned}
\mathcal{R}(\mathcal{C}, a)= &amp; \left\langle\left(\mathcal{C}^{+} \backslash e f f^{+}\right) \cup p r e^{+}\right. \
&amp; \left.\left(\mathcal{C}^{-} \backslash e f f^{-}\right) \cup p r e^{-}\right\rangle
\end{aligned}
$$</p>
<p>The regression of a goal $\mathcal{G}$ for a plan $\Pi$, denoted $\mathcal{R}^{\star}(\mathcal{G}, \Pi)$, is just the repeated application of $\mathcal{R}$ backwards through all actions in the plan starting from $\mathcal{G}$. For example, if $\Pi=$ $\left[a_{0}, a_{1}, a_{2}\right]$ then $\mathcal{R}^{\star}(\mathcal{G}, \Pi)=\mathcal{R}\left(\mathcal{R}\left(\mathcal{R}\left(\mathcal{G}, a_{2}\right), a_{1}\right), a_{0}\right)$.</p>
<p>Note that $\mathcal{R}^{\star}(\mathcal{G}, \Pi)$ represents the necessary and sufficient conditions that a state $\mathcal{I}$ must satisfy for $\Pi$ to be a valid plan for reaching $\mathcal{G}$ from $\mathcal{I}$. Moreover, given $\Pi=$ $\left[a_{0}, a_{1}, \cdots, a_{n}\right]$, we have that $\mathcal{R}^{\star}\left(\mathcal{G},\left[a_{i}, a_{i+1}, \cdots, a_{n}\right]\right)$ represents the conditions that need to be satisfied before applying the suffix $\left[a_{i}, a_{i+1}, \cdots, a_{n}\right]$ of $\Pi$. Following this idea, Fritz and McIlraith (2007) defined an execution monitoring policy that, given a state, checks for the shortest suffix such that its condition is satisfied and returns the first action in it. This system takes advantage of serendipitous unexpected outcomes, skipping parts of the plan that become unnecessary, and can recover from some negative outcomes.</p>
<p>This approach can be extended to work for partial-order plans, effectively regressing through all possible linearizations without having to explicitly construct them (Muise, McIlraith, and Beck 2011). Intuitively, the regression exploits the structure of the partial-order, taking advantage of the shared conditions and actions across multiple linearization suffixes. In our work, we use this approach and the simple sequential plan regression approach to produce highlevel policies to guide the selection of options.</p>
<p>A generic overview of our approach is shown in Algorithm 1. The algorithm maintains a policy $\pi$ for the meta-controller, and keeps references to the option policies through the high-level actions ( $a . \pi$ for every $a \in A$ ). It first computes a high-level plan ( $\Pi$ ), and keeps track of the lowlevel state $(s)$ and the high-level action currently being executed (current). If no high-level action has been selected, it queries the plan for the set of actions that could be applied (line 8) and queries the meta-controller policy for the best action out of that set. The algorithm proceeds by evaluating the policy of the option associated with the current high-level action to get a low-level action (line 11). After executing the action in the environment and receiving reward, it updates all the option policies based on that experience (line 16). If the option terminates, the algorithm updates the meta-controller policy and the state of the plan (line 19).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Reinforcement Learning guided by a
partial-order plan.
Input: \(\mathcal{F}, \mathcal{A}, \mathcal{I}, \mathcal{G}, O\)
1 Initialize \(\pi\) and \(a . \pi\) for \(a \in \mathcal{A}\)
2 \(\Pi \leftarrow \operatorname{plan}(\mathcal{F}, \mathcal{A}, \mathcal{I}, \mathcal{G})\)
3 foreach episode do
    \(s \leftarrow\) get state from environment
    current \(\leftarrow\) None
    foreach step do
        if current is None then
            HLAs \(\leftarrow \Pi . n e x t()\)
            current \(\leftarrow \pi(s\), HLAs \()\)
            \(s_{0} \leftarrow s, R \leftarrow 0\)
        \(\mathrm{a} \leftarrow\) current. \(\pi(s)\)
        \(s^{\prime} \leftarrow \operatorname{apply}(s, a)\)
        \(r^{\prime}=r\left(s, a, s^{\prime}\right)\)
        \(R \leftarrow R+r^{\prime}\)
        foreach option \(o \in O\) do
            o. \(\pi . u p d a t e\left(s, a, r^{\prime}, s^{\prime}\right)\)
        if current.terminates \(\left(s^{\prime}\right)\) then
            \(\pi\).update \(\left(s_{0}\right.\), current, \(R, s^{\prime}\)
            \(\Pi\).advance (current)
            current \(\leftarrow\) None
        \(s \leftarrow s^{\prime}\)
</code></pre></div>

<p>In what remains of this paper, we consider different instances of this approach. The basic approach in which we only use a sequential plan is denoted seq. An approach that first relaxes the sequential plan into a partial-order plan is called pop. For both cases, we also consider the use of regression-based plan execution monitoring as described above (seq $<em _mathrm_n="\mathrm{n">{\mathrm{n}}$ and pop $</em>$ ). Finally, we compare and contrast against two basic benchmark approaches. The first is the direct use of q-learning over the low-level environment ( ql ). The second is the use of the options framework over the set of options associated with the model (hrl).}</p>
<h2>Theoretical Analysis</h2>
<p>Before analyzing the properties of our approaches, it is important to understand the role that hierarchies play in RL. Hierarchies impose constraints over policies. These constraints effectively prune otherwise feasible policies from consideration, allowing RL agents to focus their learning efforts on the smaller set of hierarchically consistent policies. Unfortunately, hierarchies might (unintentionally) prune optimal policies too, denying the agent the possibility of converging to globally optimal policies. As such, our HRL methods can at best converge to a hierarchically optimal policy-which were first introduced by Dietterich (2000):
Definition 5 (Hierarchically optimal policies). A hierarchically optimal policy for an MDP $M$ is a policy that achieves the highest cumulative reward among all policies consistent with the given hierarchy.</p>
<p>We now analyze properties of hierarchically optimal policies for hrl, pop, and seq. We begin by identifying suffi-
cient conditions under which those policies are proper policies for reaching a goal state and then we compare their quality w.r.t. the amount of reward they get.
Theorem 4 (Optimal policies are proper policies). Let $E=$ $\langle S, A, r, p, \gamma, P, L, R\rangle$ be a taskable RL environment where:</p>
<ul>
<li>$r: S \times A \times S \rightarrow \mathbb{R}^{-}, \gamma=1$, and $R=0$, or</li>
<li>$r\left(s, a, s^{\prime}\right)=0$ for any $s$, $a$, and $s^{\prime}$, and $R=1$,
and $\mathcal{M}=\langle\mathcal{D}, \alpha\rangle$ be a consistent symbolic model for $E$. Let $G$ be a final-sate goal task for $E$. If there exists a high-level sequential plan $\Pi$ that solves the planning task $\langle\mathcal{D}, L(s), G\rangle$ from the initial low-level state $s$, then all hierarchically optimal policies for hrl, pop, and seq reach a goal state from $s$ with probability 1.
Proof sketch. Since $\mathcal{M}$ is a consistent symbolic model for $E$, we know that the policy $\pi$ resulting from sequentially executing the optimal low-level policies $\pi_{a}^{*}$ of each macro action in the plan $\Pi$ would reach a goal state with probability 1 from the initial state $s$. Moreover, the conditions imposed on the taskable RL environment ensure that $v_{\pi}(s)&gt;v_{\pi^{\prime}}(s)$ for every policy $\pi^{\prime}$ that does not reach a goal state with probability 1 (this follows from Theorems 1, 2, and 3). Finally, as policy $\pi$ is consistent with the hierarchies induced by hrl, pop, and seq, then the hierarchically optimal policies for each of those approaches must reach a goal state from $s$ with probability 1 .</li>
</ul>
<p>Theorem 5 (Dominance among optimal policies). Let $E$, $\mathcal{M}$, and $G$ be defined as in Theorem 4. Let $\pi_{h}^{<em>}, \pi_{p}^{</em>}$, and $\pi_{s}^{*}$ be the hierarchically optimal policy for the MDP corresponding to $G$ when using algorithms hrl, pop, and seq, respectively. If there exists a high-level sequential plan $\Pi$ that solves the planning task $\langle\mathcal{D}, L(s), G\rangle$ from the initial lowlevel state $s$, then,</p>
<p>$$
\pi^{<em>} \geq \pi_{h}^{</em>} \geq \pi_{p}^{<em>} \geq \pi_{s}^{</em>}
$$</p>
<p>where $\pi^{<em>}$ represents the optimal policy for the MDP corresponding to $G$ and $\pi_{1} \geq \pi_{2}$ iff $v_{\pi_{1}}(s) \geq v_{\pi_{2}}(s)$.
Proof sketch. Hierarchies constrain the space of feasible policies. As such, the quality of the best hierarchically optimal policy can only decrease as more policies are pruned. Given any high-level plan $\Pi$, we know that all the policies consistent with seq are consistent with pop and all the policies consistent with pop are consistent with hrl. Therefore, $\pi_{h}^{</em>} \geq \pi_{p}^{<em>} \geq \pi_{s}^{</em>}$. Finally, as hrl imposes some constraints over the set of feasible policies, then $\pi^{<em>} \geq \pi_{h}^{</em>}$.</p>
<p>Finally, it is worth discussing (at least informally) the following properties. First, ql does converge to globally optimal policies regardless of the quality of the symbolic model (since it does not use it). For the same reasons, ql is expected to learn slower than our hierarchical methods. Second, hierarchically optimal policies for seq $<em _mathrm_n="\mathrm{n">{\mathrm{n}}$ and $\mathrm{pop}</em>$ also reach goal states with probability 1 under the same conditions detailed in Theorem 4. However, this is the case only if monitoring is exclusively used for moving from a longer to a shorter high-level plan. Third, there is no dominance on the quality of hierarchically optimal policies when monitoring}</p>
<p>is included. In fact, the hierarchically optimal policies for seq $<em _mathrm_n="\mathrm{n">{\mathrm{n}}$ or pop $</em>^{}}$ might be better than $\pi_{\mathrm{n}<em>}$ under certain conditions. However, they might also be worse than $\pi_{\mathrm{e}}^{</em>}$. Lastly, note that the gap between the different policies in Theorem 5 partially depends on the quality of the symbolic model.</p>
<h2>6 Empirical Evaluation</h2>
<p>We evaluated our approach by considering two low-level environments and respective high-level models. To best evaluate the effectiveness at leveraging previous experience, we also defined a sequence of 4 tasks for each environment, ordered roughly by level of complexity. For each tested algorithm, the evaluation proceeded as follows. Each task was trained on for a fixed number of training steps. Whenever a goal state was reached, the task was restarted. If a task ran for more than 1,000 steps without reaching the goal, it was also restarted. When the limit of total training steps for a task was reached, the meta-controller policy was reset and training began on the next task. When using seq, pop, seq $<em _mathrm_n="\mathrm{n">{\mathrm{n}}$, pop $</em>$, or hrl, the trained policies for the options were transferred between tasks. In all cases, option policies and meta-controllers were trained by q-learning. To actually evaluate the quality of the learned policies, we paused the training every 10,000 training steps and ran a number of independent trials using the policy as learned at that point.}</p>
<h2>Benchmark Environments</h2>
<p>The first test environment is the OFFICEWORLD running example. Each training episode was initialized with a random initial state, and the evaluation trials were done from 10 different predefined initial states. To account for different outcomes when tie-breaking, each such trial was run 10 times.</p>
<p>Our second environment is the Minecraft-inspired gridworld described by Andreas, Klein, and Levine (2017). The grid contains raw materials (e.g., wood, iron) and locations where the agent can combine materials to produce refined materials (e.g., wooden planks), tools (e.g., hammer), and goods (e.g., goldware). The high-level actions allow for collecting each of the raw materials, and for achieving the combinations. The types of tasks that we evaluated on include examples such as "make a pickaxe," which requires getting wood and iron and taking them to various locations, or "get a gem," which requires first making a pickaxe and then going to the location with the gem. We ran experiments using random initial states for training and evaluating on 5 predefined initial states. Each experiment was run 5 times.</p>
<h2>Results and Discussion</h2>
<p>To adequately display how our approach is capable of converging quickly to high-quality solutions, Figure 3 displays a comparison between our main approaches-seq and pop-and the two basic baseline algorithms in both benchmark domains. Each graph displays the reward obtained after training with the labeled algorithm for the specified number of steps.</p>
<p>The experimental results show that-once the option policies are sufficiently well trained-our approach can significantly outperform q1 and hrl when the number of training
steps is limited. For instance, in the last task of the OFFICEWORLD, pop needed only 70,000 training steps to converge to a policy that resulted in traces that were typically 10 steps away from optimal. In contrast, hrl needed at least $1,800,000$ steps before finding a policy of comparable quality and did not appear to converge to stability in less than the $5,000,000$ training steps we allowed. That said, hrl did reach policies that resulted in slightly better solutions-only 5 steps worse than optimal. Q-learning converged to optimality after $3,850,000$ training steps.</p>
<p>The tasks in the MINECRAFTWORLD domain are significantly harder than those of the OFFICEWORLD domain and serve as a stress test for our approaches. In particular, the planning model is not strictly consistent with the low-level environment so the tasks exhibit a variety of pitfalls that make accidentally undoing previous work very easy. For example, if the agent is carrying a piece of wood and walks through a cell marked as a tool bench, it will automatically convert the wood into a wooden plank, even if it actually needs the wood for some other reason. Despite this, our results show that seq leads to reasonable results after very little training. In contrast, pop does not perform any better than hrl. In Figure 4, we show what happens when we address the unexpected outcomes with execution monitoring. In the OFFICEWORLD, the policies obtained by seq $<em _mathrm_n="\mathrm{n">{\mathrm{n}}$ seem to result in slightly more stable performance. For the MINECRAFTWORLD, pop ${ }</em>$ significantly outperforms pop, even if it still does not converge to high-quality policies.}</p>
<h2>7 Related Work</h2>
<p>In this section, we discuss a variety of work that touches upon aspects that are related to taskable RL, or to the use of symbolic planning to enhance RL systems.</p>
<h2>Symbolic Planning and Reinforcement Learning</h2>
<p>The idea of using a symbolic planning model to define tasks, hierarchically decompose them, and provide highlevel guidance was first introduced by Ryan (2002). Our work advances on the same direction by relaxing some key assumptions and by developing further theoretical foundations for the use of symbolic models in RL. In particular, Ryan's work uses a high-level planner to produce universal and complete plans. That is, the plans produced are policies that tell exactly what must be done for every possible highlevel state. In addition, the high-level actions are assumed to be teleo-operators (Nilsson 1994): they are defined in terms of preconditions and postconditions, but also require that their preconditions hold throughout their whole execution. Finally, the overall reward signal was restricted to be 1 when a goal state is reached and 0 anywhere else.</p>
<p>Other work explored the use of a symbolic planner coupled into an RL agent (Grounds and Kudenko 2008). There, the planner produces an initial high-level plan and is subsequently used to replan when the plan's preconditions are violated. A related approach uses a sequential plan to modify the reward via reward shaping (Grześ and Kudenko 2008). Both approaches rely on modifying the reward signal in order to make progress towards solving the task. In contrast,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Experimental performance obtained in two separate environments. We report the mean reward obtained by two baseline algorithms, q-learning (ql) and standard HRL (hrl), by our basic approach in which a sequential plan is executed directly (seq), and by our main approach in which HRL is restricted to execute a partial-order plan (pop).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Comparison of different variants of our approach. In addition to seq and pop, we display the results obtained when regressing sequential and partial-order plans into structured policies (seq<sup>m</sup> and pop<sup>m</sup>, respectively).</p>
<p>Our approach relies on exploiting temporal abstractions from Hierarchical RL—which allows for transferring previously learned policies to solve new tasks faster. In fact, we performed additional experiments where we included a reward shaping approach to enhance ql and hrl. However, as the results show, this had limited impact in the performance over our benchmark domains (see Appendix).</p>
<p>Recently, Lyu et al. (2019)—building on the work done by Yang et al. (2018)—proposed an agent that uses hierarchical RL to integrate symbolic planning and reinforcement learning. Their problem setup considerably departs from taskable RL, though. In their case, the high-level model is given as prior knowledge of the environment without any particular goal condition. As such, their main contribution relies on</p>
<p>how to generate meaningful goals for the planner in order to learn a policy that optimizes the unknown reward functionwhich is not an issue that arises in taskable RL.</p>
<p>There has also been work that has focused on learning explicit state-transition systems that represent high-level models (Zhang et al. 2018; Nasiriany et al. 2019; Eysenbach, Salakhutdinov, and Levine 2019). Our work considers implicit state-transition systems described as classical planning domains. This allows us to consider highly combinatorial problems that correspond to state-transition systems that are far too large to be represented explicitly.</p>
<h2>Taskable RL vs Multi-Task RL vs Multi-Goal RL</h2>
<p>In Multi-Task RL, the goal is to create RL agents that get better at finding policies for novel tasks over time. Formally, the agent must learn strategies to maximize expected future rewards over a probability distribution of MDPs. On every episode, a completely new MDP is sampled and the agent is expected to do well on it (Brunskill and Li 2013). While related, the focus of taskable RL is different. It considers an environment with fixed dynamics but changing goal conditions-which are easy to specify by the user and are given to the agent as key information for solving the task.</p>
<p>In Multi-Goal RL, the objective is to learn one policy that can achieve different goals (e.g., Kaelbling (1993), Schaul et al. (2015), and Andrychowicz et al. (2017)). A goal $g \in G$ is defined by a reward function $r_{g}$, a function $f_{g}: S \rightarrow$ ${0,1}$ that identifies when the goal is achieved, and a set of features $\phi_{g}$ to describe the goal. Then, the idea is to learn a policy that can achieve any goal $g \in G$ from any state $s \in S$. While Multi-Goal RL takes a state-centric approach to define a fixed set of possible goal conditions $G$, taskable RL takes a property-centric approach-where a set of highlevel properties of the states are composed by the user to define novel tasks for the agent on-demand.</p>
<h2>Instructions and Advice in RL</h2>
<p>Finally, our work exploits ideas from the learning from instructions (and advice) literature in RL. The main observation is that RL agents can benefit from having a formal description of the task to be accomplished. These descriptions can take the form of a policy sketch (Andreas, Klein, and Levine 2017), a Linear Temporal Logic (LTL) formula (Toro Icarte et al. 2018b), or an Automaton (Toro Icarte et al. 2018c; Toro Icarte et al. 2019), among others. Given such description, the sample efficiency of an RL agent can be improved by task decomposition (Andreas, Klein, and Levine 2017), reward shaping (Camacho et al. 2019), or guiding the exploration policy (Toro Icarte et al. 2018a).</p>
<h2>8 Conclusions and Future Work</h2>
<p>To conclude, we believe that the automatic generation of goal-relevant instructions is one of the key aspects that will enable RL systems to be both taskable and scalable. The combination of symbolic action models with model-free RL allows for solving problems that require both intricate control and long term combinatorial planning. Taskable RL represents a valuable formalism for describing problems of this
kind, and planning has shown to be a useful technique to aid in improving sample efficiency by enabling structured methods of exploration and transfer learning.</p>
<h2>Acknowledgements</h2>
<p>We gratefully acknowledge funding from NSERC, MSR, and ANID (Becas Chile). A preliminary variant of this work was presented at RLDM (Illanes et al. 2019).</p>
<h2>Appendix: Reward Shaping</h2>
<p>The idea behind reward shaping is that artificially modifying the reward signal of an MDP can improve sample efficiency by providing better feedback to an RL agent. Potential-based reward shaping methods (Ng, Harada, and Russell 1999) guarantee that the optimal policy for the modified MDP is also optimal for the original MDP by requiring the shaped reward to be of the form $r^{\prime}\left(s, a, s^{\prime}\right)=r\left(s, a, s^{\prime}\right)+F\left(s, s^{\prime}\right)$, where $F\left(s, s^{\prime}\right)=\gamma \Phi\left(s^{\prime}\right)-\Phi(s)$ for some potential function $\Phi: S \rightarrow \mathbb{R}$. Grześ and Kudenko (2008) proposed a planbased potential function that serves to guide an RL agent towards following a given high-level sequential plan $\Pi$. Given the sequence of high-level states $\left[\mathcal{S}<em 1="1">{0}, \mathcal{S}</em>}, \cdots, \mathcal{S<em i="i">{n}\right]$ that results by following the execution of $\Pi$, define $\Phi(s)=i$ for every $s \in S$ such that $L(s)=\mathcal{S}</em>$. That is, low-level states increase in potential when their corresponding highlevel counterparts are closer to the goal according to $\Pi$. In addition, states whose high-level representation does not appear in the plan are assigned the potential of the last state visited that did correspond to the plan.</p>
<p>In Figure 5, we show the results of using this reward shaping approach over ql and hrl (labeled sql and shrl, respectively) in the MINECRAFTWORLD. We omit displaying the results for the OFFICEWORLD, since the approach had no discernible impact in the performance over this domain.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Comparison of the basic benchmarks applied directly (ql and hrl) and over the plan-based shaped MDPs (sql and shrl) in the MINECRAFTWORLD.</p>
<h2>References</h2>
<p>Akkaya, I.; Andrychowicz, M.; Chociej, M.; Litwin, M.; McGrew, B.; Petron, A.; Paino, A.; Plappert, M.; Powell, G.; Ribas, R.; et al. 2019. Solving Rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113.
Andreas, J.; Klein, D.; and Levine, S. 2017. Modular multitask reinforcement learning with policy sketches. In ICML, volume 70 of $P M L R, 166-175$.
Andrychowicz, M.; Wolski, F.; Ray, A.; Schneider, J.; Fong, R.; Welinder, P.; McGrew, B.; Tobin, J.; Abbeel, P.; and Zaremba, W. 2017. Hindsight experience replay. In NIPS, 5048-5058.
Andrychowicz, M.; Baker, B.; Chociej, M.; Jozefowicz, R.; McGrew, B.; Pachocki, J.; Petron, A.; Plappert, M.; Powell, G.; Ray, A.; et al. 2018. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177.
Bertsekas, D. P., and Tsitsiklis, J. N. 1991. An analysis of stochastic shortest path problems. Mathematics of Operations Research 16(3):580-595.
Brunskill, E., and Li, L. 2013. Sample complexity of multi-task reinforcement learning. In UAI, 122-131.
Camacho, A.; Toro Icarte, R.; Klassen, T. Q.; Valenzano, R.; and McIlraith, S. A. 2019. LTL and beyond: Formal languages for reward function specification in reinforcement learning. In IJCAI, $6065-6073$.
Dietterich, T. G. 2000. Hierarchical reinforcement learning with the MAXQ value function decomposition. Journal of Artificial Intelligence Research 13:227-303.
Eaton, J., and Zadeh, L. 1962. Optimal pursuit strategies in discrete-state probabilistic systems. Journal of Basic Engineering 84(1):23-29.
Eysenbach, B.; Salakhutdinov, R. R.; and Levine, S. 2019. Search on the replay buffer: Bridging planning and reinforcement learning. In NeurIPS, 15246-15257.
Falco, P.; Attawia, A.; Saveriano, M.; and Lee, D. 2018. On policy learning robust to irreversible events: An application to robotic in-hand manipulation. IEEE Robotics and Automation Letters 3(3):1482-1489.
Fritz, C., and McIlraith, S. A. 2007. Monitoring plan optimality during execution. In ICAPS, 144-151.
Grounds, M. J., and Kudenko, D. 2008. Combining reinforcement learning with symbolic planning. In AAMAS III, volume 4865 of LNCS, 75-86.
Grześ, M., and Kudenko, D. 2008. Plan-based reward shaping for reinforcement learning. In IS, volume 2, 10-22-10-29.
Illanes, L.; Yan, X.; Toro Icarte, R.; and McIlraith, S. A. 2019. Symbolic planning and model-free reinforcement learning: Training taskable agents. In RLDM, 191-195.
Kaelbling, L. P. 1993. Learning to achieve goals. In IJCAI, 10941099.</p>
<p>Konidaris, G., and Barto, A. G. 2007. Building portable options: Skill transfer in reinforcement learning. In IJCAI, 895-900.
Kumar, V.; Gupta, A.; Todorov, E.; and Levine, S. 2016. Learning dexterous manipulation policies from experience and imitation. arXiv preprint arXiv:1611.05095.
Kumar, V.; Todorov, E.; and Levine, S. 2016. Optimal control with learned local models: Application to dexterous manipulation. In ICRA, 378-383.
Lyu, D.; Yang, F.; Liu, B.; and Gustafson, S. 2019. SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning. In AAAI.</p>
<p>Muise, C. J.; McIlraith, S. A.; and Beck, J. C. 2011. Monitoring the execution of partial-order plans via regression. In IJCAI, 19751982.</p>
<p>Nasiriany, S.; Pong, V.; Lin, S.; and Levine, S. 2019. Planning with goal-conditioned policies. In NeurIPS, 14843-14854.
Ng, A. Y.; Harada, D.; and Russell, S. J. 1999. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 278-287.
Nilsson, N. J. 1994. Teleo-reactive programs for agent control. Journal of Artificial Intelligence Research 1:139-158.
Ryan, M. R. K. 2002. Using abstract models of behaviours to automatically generate reinforcement learning hierarchies. In ICML, $522-529$.
Schaul, T.; Horgan, D.; Gregor, K.; and Silver, D. 2015. Universal value function approximators. In ICML, 1312-1320.
Sutton, R. S.; Precup, D.; and Singh, S. P. 1999. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence 112(1-2):181-211.
Taylor, M. E., and Stone, P. 2009. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research 10(Jul):1633-1685.
Toro Icarte, R.; Klassen, T. Q.; Valenzano, R.; and McIlraith, S. A. 2018a. Advice-based exploration in model-based reinforcement learning. In Canadian Conf. on Artificial Intelligence , 72-83.
Toro Icarte, R.; Klassen, T. Q.; Valenzano, R.; and McIlraith, S. A. 2018b. Teaching multiple tasks to an RL agent using LTL. In AAMAS, 452-461.
Toro Icarte, R.; Klassen, T. Q.; Valenzano, R.; and McIlraith, S. A. 2018c. Using reward machines for high-level task specification and decomposition in reinforcement learning. In ICML, volume 80 of PMLR, 2112-2121.
Toro Icarte, R.; Waldie, E.; Klassen, T.; Valenzano, R.; Castro, M.; and McIlraith, S. 2019. Learning reward machines for partially observable reinforcement learning. In NeurIPS, 15523-15534.
Van Hoof, H.; Hermans, T.; Neumann, G.; and Peters, J. 2015. Learning robot in-hand manipulation with tactile features. In Humanoids, 121-127.
Watkins, C. J. C. H., and Dayan, P. 1992. Q-learning. Machine Learning 8(3-4):279-292.
Yang, F.; Lyu, D.; Liu, B.; and Gustafson, S. 2018. PEORL: integrating symbolic planning and hierarchical reinforcement learning for robust decision-making. In IJCAI, 4860-4866.
Zhang, A.; Sukhbaatar, S.; Lerer, A.; Szlam, A.; and Fergus, R. 2018. Composable planning with attributes. In ICML, volume 80 of $P M L R, 5837-5846$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (c) 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
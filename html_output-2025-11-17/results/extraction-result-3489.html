<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3489 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3489</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3489</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b39eed03d345f5c244eac12fd1315d26eba77d62</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b39eed03d345f5c244eac12fd1315d26eba77d62" target="_blank">Deep Learning for Symbolic Mathematics</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that neural networks can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations, and a syntax for representing these mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models.</p>
                <p><strong>Paper Abstract:</strong> Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing these mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3489.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3489.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-Seq2Seq (Lample&Charton)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer sequence-to-sequence model used in "Deep Learning for Symbolic Mathematics"</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard Transformer-based sequence-to-sequence model trained on large synthetic datasets of symbolic mathematics (prefix-tokenized expression trees) to perform exact symbolic tasks: function integration and first- and second-order ODE solving; outputs are verified for exact equivalence by an external CAS (SymPy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Learning for Symbolic Mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer seq2seq (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder-decoder (Vaswani et al. architecture) with 6 layers, 8 attention heads, model dimensionality 512; trained from scratch on synthetic datasets of symbolic expressions (prefix notation) for integration and ODE solving. Training used Adam (lr=1e-4), batch size 256, sequence length limit 512, beam search (beam sizes 1/10/50) at inference, and external verification with SymPy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Symbolic integration and ordinary differential equation (ODE) solving (1st and 2nd order)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict symbolic reasoning tasks requiring exact algebraic equivalence: compute an antiderivative for a given expression (symbolic integration), and produce a closed-form solution y(x) that satisfies a given first- or second-order ODE. Correctness is judged by symbolic equivalence (different algebraic forms counted correct if mathematically equal or satisfy the ODE).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Key methods: represent expressions as trees and linearize to prefix token sequences; generate large supervised datasets via three generators (Forward using CAS integrator, Backward via differentiation, and Backward plus Integration-By-Parts (IBP) augmentation); data cleaning/simplification and invalid-expression filtering; train standard Transformer seq2seq; use beam search (1/10/50) with length-normalized log-probabilities; verify candidate outputs with SymPy and accept any beam hypothesis that is symbolically correct. They explicitly experimented with (and emphasize) dataset engineering (FWD/BWD/IBP) as an intervention to improve correctness/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported test-set accuracies (exact / symbolic correctness) on held-out test samples of 5000 (Table 2) and comparison subset of 500 (Table 3). Integration (FWD): beam1 93.6%, beam10 95.6%, beam50 96.2%. Integration (BWD): beam1 98.4%, beam10 99.4%, beam50 99.7%. Integration (IBP): beam1 96.8%, beam10 99.2%, beam50 99.5%. ODE order 1: beam1 77.6%, beam10 90.5%, beam50 94.0%. ODE order 2: beam1 43.0%, beam10 73.0%, beam50 81.2%. On the 500-example comparison subset vs CAS (Table 3): Integration (BWD) model beam50 99.6% (on 500) vs Mathematica (30s timeout) 84.0%; ODE order 1 model beam50 97.0% vs Mathematica 77.2%; ODE order 2 model beam50 81.0% vs Mathematica 61.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baselines: Mathematica (30s timeout) on same 500-example subset: Integration (BWD) 84.0%, ODE (order 1) 77.2%, ODE (order 2) 61.6%. Matlab: Integration (BWD) 65.2%. Maple: Integration (BWD) 67.4%. (Where '-' indicates not reported for some tasks.)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Transformer model substantially outperforms available CAS baselines on these synthetic distributions: e.g., Integration (BWD) beam50 99.6% vs Mathematica 84.0% (+~15.6 percentage points); ODE order 1 beam50 97.0% vs Mathematica 77.2% (+~19.8 pp); ODE order 2 beam50 81.0% vs Mathematica 61.6% (+~19.4 pp). Improvements depend on beam search (large beams yield major gains) and training-data composition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported limitations and failure modes: (1) Greedy decoding often fails (notably for ODEs); large beam sizes are frequently required (beam=50 gave substantial gains) — model does not provide a built-in correctness certificate, so external symbolic checking is needed. (2) Poor cross-distribution generalization if trained on a single generator: e.g., a FWD-trained model attains only ~17.2% on BWD tests (beam50) and a BWD-trained model only ~27.5% on FWD tests (beam50) due to differences in input/output length distributions and structure. (3) Model can generate invalid prefix expressions; invalid outputs are treated as incorrect. (4) Some hypotheses are incorrect and must be filtered by an external CAS; success is contingent on the verification oracle. (5) Increasing model size beyond the reported configuration did not improve performance (authors note larger models did not help). (6) Timeouts and implementation choices in CAS baselines affect comparison (Mathematica timed out in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Analyses and ablations reported: (a) Beam-size ablation: accuracy vs beam size (1, 10, 50) showing large gains, especially for ODEs; (b) Training-data-composition ablation: models trained on different mixes of FWD/BWD/IBP datasets (Table 6) showing that mixing generators improves cross-distribution generalization (e.g., BWD+IBP+FWD yields high accuracy across all three test sets), and that generator choice strongly shapes learned behavior (integration tends to shorten vs lengthen sequences depending on generator); (c) Model-capacity check: authors tried larger models but saw no accuracy improvement; (d) Qualitative analysis: model produces many algebraically equivalent but syntactically different solutions (Table 5), suggesting it has learned structural transformations beyond memorization; (e) External-CAS generalization experiment: FWD-trained model sometimes integrates functions that SymPy could not (Table 7), demonstrating some out-of-distribution generalization beyond the CAS used to create part of the training data.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>All performance numbers are those explicitly reported in the paper (Tables 1-7). Verification of model outputs uses SymPy; where the model's hypothesis satisfies the differential equation or differentiates to the original integrand, it is counted as correct. Model parameter count is not specified in the paper and therefore set to null.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Learning for Symbolic Mathematics', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Towards solving differential equations through neural programming <em>(Rating: 2)</em></li>
                <li>Combining symbolic expressions and black-box function evaluations for training neural programs <em>(Rating: 1)</em></li>
                <li>Learning to execute <em>(Rating: 1)</em></li>
                <li>Learning continuous semantic representations of symbolic expressions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3489",
    "paper_id": "paper-b39eed03d345f5c244eac12fd1315d26eba77d62",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "Transformer-Seq2Seq (Lample&Charton)",
            "name_full": "Transformer sequence-to-sequence model used in \"Deep Learning for Symbolic Mathematics\"",
            "brief_description": "A standard Transformer-based sequence-to-sequence model trained on large synthetic datasets of symbolic mathematics (prefix-tokenized expression trees) to perform exact symbolic tasks: function integration and first- and second-order ODE solving; outputs are verified for exact equivalence by an external CAS (SymPy).",
            "citation_title": "Deep Learning for Symbolic Mathematics",
            "mention_or_use": "use",
            "model_name": "Transformer seq2seq (this work)",
            "model_description": "Transformer encoder-decoder (Vaswani et al. architecture) with 6 layers, 8 attention heads, model dimensionality 512; trained from scratch on synthetic datasets of symbolic expressions (prefix notation) for integration and ODE solving. Training used Adam (lr=1e-4), batch size 256, sequence length limit 512, beam search (beam sizes 1/10/50) at inference, and external verification with SymPy.",
            "model_size": null,
            "reasoning_task_name": "Symbolic integration and ordinary differential equation (ODE) solving (1st and 2nd order)",
            "reasoning_task_description": "Strict symbolic reasoning tasks requiring exact algebraic equivalence: compute an antiderivative for a given expression (symbolic integration), and produce a closed-form solution y(x) that satisfies a given first- or second-order ODE. Correctness is judged by symbolic equivalence (different algebraic forms counted correct if mathematically equal or satisfy the ODE).",
            "method_or_intervention": "Key methods: represent expressions as trees and linearize to prefix token sequences; generate large supervised datasets via three generators (Forward using CAS integrator, Backward via differentiation, and Backward plus Integration-By-Parts (IBP) augmentation); data cleaning/simplification and invalid-expression filtering; train standard Transformer seq2seq; use beam search (1/10/50) with length-normalized log-probabilities; verify candidate outputs with SymPy and accept any beam hypothesis that is symbolically correct. They explicitly experimented with (and emphasize) dataset engineering (FWD/BWD/IBP) as an intervention to improve correctness/generalization.",
            "performance": "Reported test-set accuracies (exact / symbolic correctness) on held-out test samples of 5000 (Table 2) and comparison subset of 500 (Table 3). Integration (FWD): beam1 93.6%, beam10 95.6%, beam50 96.2%. Integration (BWD): beam1 98.4%, beam10 99.4%, beam50 99.7%. Integration (IBP): beam1 96.8%, beam10 99.2%, beam50 99.5%. ODE order 1: beam1 77.6%, beam10 90.5%, beam50 94.0%. ODE order 2: beam1 43.0%, beam10 73.0%, beam50 81.2%. On the 500-example comparison subset vs CAS (Table 3): Integration (BWD) model beam50 99.6% (on 500) vs Mathematica (30s timeout) 84.0%; ODE order 1 model beam50 97.0% vs Mathematica 77.2%; ODE order 2 model beam50 81.0% vs Mathematica 61.6%.",
            "baseline_performance": "Baselines: Mathematica (30s timeout) on same 500-example subset: Integration (BWD) 84.0%, ODE (order 1) 77.2%, ODE (order 2) 61.6%. Matlab: Integration (BWD) 65.2%. Maple: Integration (BWD) 67.4%. (Where '-' indicates not reported for some tasks.)",
            "improvement_over_baseline": "Transformer model substantially outperforms available CAS baselines on these synthetic distributions: e.g., Integration (BWD) beam50 99.6% vs Mathematica 84.0% (+~15.6 percentage points); ODE order 1 beam50 97.0% vs Mathematica 77.2% (+~19.8 pp); ODE order 2 beam50 81.0% vs Mathematica 61.6% (+~19.4 pp). Improvements depend on beam search (large beams yield major gains) and training-data composition.",
            "limitations_or_failures": "Reported limitations and failure modes: (1) Greedy decoding often fails (notably for ODEs); large beam sizes are frequently required (beam=50 gave substantial gains) — model does not provide a built-in correctness certificate, so external symbolic checking is needed. (2) Poor cross-distribution generalization if trained on a single generator: e.g., a FWD-trained model attains only ~17.2% on BWD tests (beam50) and a BWD-trained model only ~27.5% on FWD tests (beam50) due to differences in input/output length distributions and structure. (3) Model can generate invalid prefix expressions; invalid outputs are treated as incorrect. (4) Some hypotheses are incorrect and must be filtered by an external CAS; success is contingent on the verification oracle. (5) Increasing model size beyond the reported configuration did not improve performance (authors note larger models did not help). (6) Timeouts and implementation choices in CAS baselines affect comparison (Mathematica timed out in some cases).",
            "ablation_or_analysis": "Analyses and ablations reported: (a) Beam-size ablation: accuracy vs beam size (1, 10, 50) showing large gains, especially for ODEs; (b) Training-data-composition ablation: models trained on different mixes of FWD/BWD/IBP datasets (Table 6) showing that mixing generators improves cross-distribution generalization (e.g., BWD+IBP+FWD yields high accuracy across all three test sets), and that generator choice strongly shapes learned behavior (integration tends to shorten vs lengthen sequences depending on generator); (c) Model-capacity check: authors tried larger models but saw no accuracy improvement; (d) Qualitative analysis: model produces many algebraically equivalent but syntactically different solutions (Table 5), suggesting it has learned structural transformations beyond memorization; (e) External-CAS generalization experiment: FWD-trained model sometimes integrates functions that SymPy could not (Table 7), demonstrating some out-of-distribution generalization beyond the CAS used to create part of the training data.",
            "notes": "All performance numbers are those explicitly reported in the paper (Tables 1-7). Verification of model outputs uses SymPy; where the model's hypothesis satisfies the differential equation or differentiates to the original integrand, it is counted as correct. Model parameter count is not specified in the paper and therefore set to null.",
            "uuid": "e3489.0",
            "source_info": {
                "paper_title": "Deep Learning for Symbolic Mathematics",
                "publication_date_yy_mm": "2019-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Towards solving differential equations through neural programming",
            "rating": 2
        },
        {
            "paper_title": "Combining symbolic expressions and black-box function evaluations for training neural programs",
            "rating": 1
        },
        {
            "paper_title": "Learning to execute",
            "rating": 1
        },
        {
            "paper_title": "Learning continuous semantic representations of symbolic expressions",
            "rating": 1
        }
    ],
    "cost": 0.011460749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DEEP LEARNING FOR SYMBOLIC MATHEMATICS</h1>
<p>Guillaume Lample ${ }^{\star}$<br>Facebook AI Research<br>glample@fb.com<br>François Charton*<br>Facebook AI Research<br>fcharton@fb.com</p>
<h4>Abstract</h4>
<p>Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.</p>
<h2>1 INTRODUCTION</h2>
<p>A longstanding tradition in machine learning opposes rule-based inference to statistical learning (Rumelhart et al., 1986), and neural networks clearly stand on the statistical side. They have proven to be extremely effective in statistical pattern recognition and now achieve state-of-the-art performance on a wide range of problems in computer vision, speech recognition, natural language processing (NLP), etc. However, the success of neural networks in symbolic computation is still extremely limited: combining symbolic reasoning with continuous representations is now one of the challenges of machine learning.</p>
<p>Only a few studies investigated the capacity of neural network to deal with mathematical objects, and apart from a small number of exceptions (Zaremba et al., 2014; Loos et al., 2017; Allamanis et al., 2017; Arabshahi et al., 2018b), the majority of these works focus on arithmetic tasks like integer addition and multiplication (Zaremba \&amp; Sutskever, 2014; Kaiser \&amp; Sutskever, 2015; Trask et al., 2018). On these tasks, neural approaches tend to perform poorly, and require the introduction of components biased towards the task at hand (Kaiser \&amp; Sutskever, 2015; Trask et al., 2018).</p>
<p>In this paper, we consider mathematics, and particularly symbolic calculations, as a target for NLP models. More precisely, we use sequence-to-sequence models (seq2seq) on two problems of symbolic mathematics: function integration and ordinary differential equations (ODEs). Both are difficult, for trained humans and computer software. For integration, humans are taught a set of rules (integration by parts, change of variable, etc.), that are not guaranteed to succeed, and Computer Algebra Systems use complex algorithms (Geddes et al., 1992) that explore a large number of specific cases. For instance, the complete description of the Risch algorithm (Risch, 1970) for function integration is more than 100 pages long.</p>
<p>Yet, function integration is actually an example where pattern recognition should be useful: detecting that an expression is of the form $y y^{\prime}\left(y^{2}+1\right)^{-1 / 2}$ suggests that its primitive will contain $\sqrt{y^{2}+1}$. Detecting this pattern may be easy for small expressions $y$, but becomes more difficult as the number of operators in $y$ increases. However, to the best of our knowledge, no study has investigated the ability of neural networks to detect patterns in mathematical expressions.</p>
<p>We first propose a representation of mathematical expressions and problems that can be used by seq2seq models, and discuss the size and structure of the resulting problem space. Then, we show how to generate datasets for supervised learning of integration and first and second order differential equations. Finally, we apply seq2seq models to these datasets, and show that they achieve a better performance than state-of-the-art computer algebra programs, namely Matlab and Mathematica.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 Mathematics as a Natural Language</h1>
<h3>2.1 EXPRESSIONS AS TREES</h3>
<p>Mathematical expressions can be represented as trees, with operators and functions as internal nodes, operands as children, and numbers, constants and variables as leaves. The following trees represent expressions $2+3 \times(5+2), 3 x^{2}+\cos (2 x)-1$, and $\frac{\partial^{2} \psi}{\partial x^{2}}-\frac{1}{\nu^{2}} \frac{\partial^{2} \psi}{\partial t^{2}}$ :
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Trees disambiguate the order of operations, take care of precedence and associativity and eliminate the need for parentheses. Up to the addition of meaningless symbols like spaces, punctuation or redundant parentheses, different expressions result in different trees. With a few assumptions, discussed in Section A of the appendix, there is a one-to-one mapping between expressions and trees.</p>
<p>We consider expressions as sequences of mathematical symbols. $2+3$ and $3+2$ are different expressions, as are $\sqrt{4} x$ and $2 x$, and they will be represented by different trees. Most expressions represent meaningful mathematical objects. $x / 0, \sqrt{-2}$ or $\log (0)$ are also legitimate expressions, even though they do not necessarily make mathematical sense.</p>
<p>Since there is a one-to-one correspondence between trees and expressions, equality between expressions will be reflected over their associated trees, as an equivalence : since $2+3=5=12-7=1 \times 5$, the four trees corresponding to these expressions are equivalent.</p>
<p>Many problems of formal mathematics can be reframed as operations over expressions, or trees. For instance, expression simplification amounts to finding a shorter equivalent representation of a tree. In this paper, we consider two problems: symbolic integration and differential equations. Both boil down to transforming an expression into another, e.g. mapping the tree of an equation to the tree of its solution. We regard this as a particular instance of machine translation.</p>
<h3>2.2 TREES AS SEQUENCES</h3>
<p>Machine translation systems typically operate on sequences (Sutskever et al., 2014; Bahdanau et al., 2015). Alternative approaches have been proposed to generate trees, such as Tree-LSTM (Tai et al., 2015) or Recurrent Neural Network Grammars (RNNG) (Dyer et al., 2016; Eriguchi et al., 2017). However, tree-to-tree models are more involved and much slower than their seq2seq counterparts, both at training and at inference. For the sake of simplicity, we use seq2seq models, which were shown to be effective at generating trees, e.g. in the context of constituency parsing (Vinyals et al., 2015), where the task is to predict a syntactic parse tree of input sentences.</p>
<p>Using seq2seq models to generate trees requires to map trees to sequences. To this effect, we use prefix notation (also known as normal Polish notation), writing each node before its children, listed from left to right. For instance, the arithmetic expression $2+3 <em>(5+2)$ is represented as the sequence $[+2 * 3+52]$. In contrast to the more common infix notation $2+3 </em>(5+2)$, prefix sequences need no parentheses and are therefore shorter. Inside sequences, operators, functions or variables are represented by specific tokens, and integers by sequences of digits preceded by a sign. As in the case between expressions and trees, there exists a one-to-one mapping between trees and prefix sequences.</p>
<h3>2.3 Generating RANDOM EXPRESSIONS</h3>
<p>To create training data, we need to generate sets of random mathematical expressions. However, sampling uniformly expressions with $n$ internal nodes is not a simple task. Naive algorithms (such as</p>
<p>recursive methods or techniques using fixed probabilities for nodes to be leaves, unary, or binary) tend to favour deep trees over broad trees, or left-leaning over right leaning trees. Here are examples of different trees that we want to generate with the same probability.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>In Section C of the appendix, we present an algorithm to generate random trees and expressions, where the four expression trees above are all generated with the same probability.</p>
<h1>2.4 Counting EXPRESSIONS</h1>
<p>We now investigate the number of possible expressions. Expressions are created from a finite set of variables (i.e. literals), constants, integers, and a list of operators that can be simple functions (e.g. $\cos$ or $\exp$ ) or more involved operators (e.g. differentiation or integration). More precisely, we define our problem space as:</p>
<ul>
<li>trees with up to $n$ internal nodes</li>
<li>a set of $p_{1}$ unary operators (e.g. $\cos , \sin , \exp , \log$ )</li>
<li>a set of $p_{2}$ binary operators (e.g.,$+,-$,$\times$, pow)</li>
<li>a set of $L$ leaf values containing variables (e.g. $x, y, z$ ), constants (e.g. $e, \pi$ ), integers (e.g. ${-10, \ldots, 10})$</li>
</ul>
<p>If $p_{1}=0$, expressions are represented by binary trees. The number of binary trees with $n$ internal nodes is given by the $n$-th Catalan numbers $C_{n}$ (Sloane, 1996). A binary tree with $n$ internal nodes has exactly $n+1$ leaves. Each node and leaf can take respectively $p_{2}$ and $L$ different values. As a result, the number of expressions with $n$ binary operators can be expressed by:</p>
<p>$$
E_{n}=C_{n} p_{2}^{n} L^{n+1} \approx \frac{4^{n}}{n \sqrt{\pi n}} p_{2}^{n} L^{n+1} \quad \text { with } \quad C_{n}=\frac{1}{n+1}\binom{2 n}{n}
$$</p>
<p>If $p_{1}&gt;0$, expressions are unary-binary trees, and the number of trees with $n$ internal nodes is the $n$-th large Schroeder number $S_{n}$ (Sloane, 1996). It can be computed by recurrence using the following equation:</p>
<p>$$
(n+1) S_{n}=3(2 n-1) S_{n-1}-(n-2) S_{n-2}
$$</p>
<p>Finally, the number $E_{n}$ of expressions with $n$ internal nodes, $p_{1}$ unary operator, $p_{2}$ binary operators and $L$ possible leaves is recursively computed as</p>
<p>$$
(n+1) E_{n}=\left(p_{1}+2 L p_{2}\right)(2 n-1) E_{n-1}-p_{1}(n-2) E_{n-2}
$$</p>
<p>If $p_{1}=p_{2}=L=1$, Equation 2 boils down to Equation 1. If $p_{2}=L=1, p_{1}=0$, we have $(n+1) E_{n}=2(2 n-1) E_{n-1}$ which is the recurrence relation satisfied by Catalan numbers. The derivations and properties of all these formulas are provided in Section B of the appendix.</p>
<p>In Figure 1, we represent the number of binary trees $\left(C_{n}\right)$ and unary-binary trees $\left(S_{n}\right)$ for different numbers of internal nodes. We also represent the number of possible expressions $\left(E_{n}\right)$ for different sets of operators and leaves.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 1: Number of trees and expressions for different numbers of operators and leaves. $p_{1}$ and $p_{2}$ correspond to the number of unary and binary operators respectively, and $L$ to the number of possible leaves. The bottom two curves correspond to the number of binary and unary-binary trees (enumerated by Catalan and Schroeder numbers respectively). The top two curves represent the associated number of expressions. We observe that adding leaves and binary operators significantly increases the size of the problem space.</p>
<h1>3 GENERATING DATASETS</h1>
<p>Having defined a syntax for mathematical problems and techniques to randomly generate expressions, we are now in a position to build the datasets our models will use. In the rest of the paper, we focus on two problems of symbolic mathematics: function integration and solving ordinary differential equations (ODE) of the first and second order.</p>
<p>To train our networks, we need datasets of problems and solutions. Ideally, we want to generate representative samples of the problem space, i.e. randomly generate functions to be integrated and differential equations to be solved. Unfortunately, solutions of random problems sometimes do not exist (e.g. the integrals of $f(x)=\exp \left(x^{2}\right)$ or $f(x)=\log (\log (x))$ cannot be expressed with usual functions), or cannot be easily derived. In this section, we propose techniques to generate large training sets for integration and first and second order differential equations.</p>
<h3>3.1 InteGRATION</h3>
<p>We propose three approaches to generate functions with their associated integrals.
Forward generation (FWD). A straightforward approach is to generate random functions with up to $n$ operators (using methods from Section 2) and calculate their integrals with a computer algebra system. Functions that the system cannot integrate are discarded. This generates a representative sample of the subset of the problem space that can be successfully solved by an external symbolic mathematical framework.</p>
<p>Backward generation (BWD). An issue with the forward approach is that the dataset only contains functions that symbolic frameworks can solve (they sometimes fail to compute the integral of integrable functions). Also, integrating large expressions is time expensive, which makes the overall method particularly slow. Instead, the backward approach generates a random function $f$, computes its derivative $f^{\prime}$, and adds the pair $\left(f^{\prime}, f\right)$ to the training set. Unlike integration, differentiation is always possible and extremely fast even for very large expressions. As opposed to the forward approach, this method does not depend on an external symbolic integration system.</p>
<p>Backward generation with integration by parts (IBP). An issue with the backward approach is that it is very unlikely to generate the integral of simple functions like $f(x)=x^{3} \sin (x)$. Its integral, $F(x)=-x^{3} \cos (x)+3 x^{2} \sin (x)+6 x \cos (x)-6 \sin (x)$, a function with 15 operators, has a very low probability of being generated randomly. Besides, the backward approach tends to generate examples where the integral (the solution) is shorter than the derivative (the problem), while forward generation favors the opposite (see Figure 2 in section E in the Appendix). To address this issue, we</p>
<p>leverage integration by parts: given two randomly generated functions $F$ and $G$, we compute their respective derivatives $f$ and $g$. If $f G$ already belongs to the training set, we know its integral, and we can compute the integral of $F g$ as:</p>
<p>$$
\int F g=F G-\int f G
$$</p>
<p>Similarly, if $F g$ is in the training set, we can infer the integral of $f G$. Whenever we discover the integral of a new function, we add it to the training set. If none of $f G$ or $F g$ are in the training set, we simply generate new functions $F$ and $G$. With this approach, we can generate the integrals of functions like $x^{10} \sin (x)$ without resorting to an external symbolic integration system.</p>
<p>Comparing different generation methods. Table 1 in Section 4.1 summarizes the differences between the three generation methods. The FWD method tends to generate short problems with long solutions (that computer algebras can solve). The BWD approach, on the other hand, generates long problems with short solutions. IBP generates datasets comparable to FWD (short problems and long solutions), without an external computer algebra system. A mixture of BWD and IBP generated data should therefore provide a better representation of problem space, without resorting to external tools. Examples of functions / integrals for the three approaches are given in Table 9 of the Appendix.</p>
<h1>3.2 FIRST ORDER DIFFERENTIAL EQUATION (ODE 1)</h1>
<p>We now present a method to generate first order differential equations with their solutions. We start from a bivariate function $F(x, y)$ such that the equation $F(x, y)=c$ (where $c$ is a constant) can be analytically solved in $y$. In other words, there exists a bivariate function $f$ that satisfies $\forall(x, c), F(x, f(x, c))=c$. By differentiation with respect to $x$, we have that $\forall x, c$ :</p>
<p>$$
\frac{\partial F\left(x, f_{c}(x)\right)}{\partial x}+f_{c}^{\prime}(x) \frac{\partial F\left(x, f_{c}(x)\right)}{\partial y}=0
$$</p>
<p>where $f_{c}=x \mapsto f(x, c)$. As a result, for any constant $c, f_{c}$ is solution of the first order differential equation:</p>
<p>$$
\frac{\partial F(x, y)}{\partial x}+y^{\prime} \frac{\partial F(x, y)}{\partial y}=0
$$</p>
<p>With this approach, we can use the method described in Section C of the appendix to generate arbitrary functions $F(x, y)$ analytically solvable in $y$, and create a dataset of differential equations with their solutions.</p>
<p>Instead of generating a random function $F$, we can generate a solution $f(x, c)$, and determine a differential equation that it satisfies. If $f(x, c)$ is solvable in $c$, we compute $F$ such that $F(x, f(x, c))=c$. Using the above approach, we show that for any constant $c, x \mapsto f(x, c)$ is a solution of differential Equation 3. Finally, the resulting differential equation is factorized, and we remove all positive factors from the equation.</p>
<p>A necessary condition for this approach to work is that the generated functions $f(x, c)$ can be solved in $c$. For instance, the function $f(x, c)=c \times \log (x+c)$ cannot be analytically solved in $c$, i.e. the function $F$ that satisfies $F(x, f(x, c))=c$ cannot be written with usual functions. Since all the operators and functions we use are invertible, a simple condition to ensure the solvability in $c$ is to guarantee that $c$ only appears once in the leaves of the tree representation of $f(x, c)$. A straightforward way to generate a suitable $f(x, c)$ is to sample a random function $f(x)$ by the methods described in Section C of the appendix, and to replace one of the leaves in its tree representation by $c$. Below is an example of the whole process:</p>
<p>Generate a random function
Solve in $c$
Differentiate in $x$
Simplify</p>
<p>$$
\begin{aligned}
&amp; f(x)=x \log (c / x) \
&amp; c=x e^{\frac{f(x)}{x}}=F(x, f(x)) \
&amp; e^{\frac{f(x)}{x}}\left(1+f^{\prime}(x)-\frac{f(x)}{x}\right)=0 \
&amp; x y^{\prime}-y+x=0
\end{aligned}
$$</p>
<h1>3.3 SECOND ORDER DIFFERENTIAL EQUATION (ODE 2)</h1>
<p>Our method for generating first order equations can be extended to the second order, by considering functions of three variables $f\left(x, c_{1}, c_{2}\right)$ that can be solved in $c_{2}$. As before, we derive a function of three variables $F$ such that $F\left(x, f\left(x, c_{1}, c_{2}\right), c_{1}\right)=c_{2}$. Differentiation with respect to $x$ yields a first order differential equation:</p>
<p>$$
\left.\frac{\partial F\left(x, y, c_{1}\right)}{\partial x}+f_{c_{1}, c_{2}}^{\prime}(x) \frac{\partial F\left(x, y, c_{1}\right)}{\partial y}\right|<em c__1="c_{1">{y=f</em>=0
$$}, c_{2}}(x)</p>
<p>where $f_{c_{1}, c_{2}}=x \mapsto f\left(x, c_{1}, c_{2}\right)$. If this equation can be solved in $c_{1}$, we can infer another threevariable function $G$ satisfying $\forall x, G\left(x, f_{c_{1}, c_{2}}(x), f_{c_{1}, c_{2}}^{\prime}(x)\right)=c_{1}$. Differentiating with respect to $x$ a second time yields the following equation:</p>
<p>$$
\left.\frac{\partial G(x, y, z)}{\partial x}+f_{c_{1}, c_{2}}^{\prime}(x) \frac{\partial G(x, y, z)}{\partial y}+f_{c_{1}, c_{2}}^{\prime \prime}(x) \frac{\partial G(x, y, z)}{\partial z}\right|<em c__1="c_{1">{\substack{y=f</em>=0
$$}, c_{2}}(x) \ z=f_{c_{1}, c_{2}}(x)}</p>
<p>Therefore, for any constants $c_{1}$ and $c_{2}, f_{c_{1}, c_{2}}$ is solution of the second order differential equation:</p>
<p>$$
\frac{\partial G\left(x, y, y^{\prime}\right)}{\partial x}+y^{\prime} \frac{\partial G\left(x, y, y^{\prime}\right)}{\partial y}+y^{\prime \prime} \frac{\partial G\left(x, y, y^{\prime}\right)}{\partial z}=0
$$</p>
<p>Using this approach, we can create pairs of second order differential equations and solutions, provided we can generate $f\left(x, c_{1}, c_{2}\right)$ is solvable in $c_{2}$, and that the corresponding first order differential equation is solvable in $c_{1}$. To ensure the solvability in $c_{2}$, we can use the same approach as for first order differential equation, e.g. we create $f_{c_{1}, c_{2}}$ so that $c_{2}$ has exactly one leaf in its tree representation. For $c_{1}$, we employ a simple approach where we simply skip the current equation if we cannot solve it in $c_{1}$. Although naive, we found that the differentiation equation can be solved in $c_{1}$ about $50 \%$ the time. As an example:</p>
<p>Generate a random function
Solve in $c_{2}$
Differentiate in $x$
Solve in $c_{1}$
Differentiate in $x$
Simplify</p>
<p>$$
\begin{aligned}
&amp; f(x)=c_{1} e^{x}+c_{2} e^{-x} \
&amp; c_{2}=f(x) e^{x}-c_{1} e^{2 x}=F\left(x, f(x), c_{1}\right) \
&amp; e^{x}\left(f^{\prime}(x)+f(x)\right)-2 c_{1} e^{2 x}=0 \
&amp; c_{1}=\frac{1}{2} e^{-x}\left(f^{\prime}(x)+f(x)\right)=G\left(x, f(x), f^{\prime}(x)\right) \
&amp; 0=\frac{1}{2} e^{-x}\left(f^{\prime \prime}(x)-f(x)\right) \
&amp; y^{\prime \prime}-y=0
\end{aligned}
$$</p>
<h3>3.4 DATASET CLEANING</h3>
<p>Equation simplification In practice, we simplify generated expressions to reduce the number of unique possible equations in the training set, and to reduce the length of sequences. Also, we do not want to train our model to predict $x+1+1+1+1+1$ when it can simply predict $x+5$. As a result, sequences $[+2+x 3]$ and $[+3+2 x]$ will both be simplified to $[+x 5]$ as they both represent the expression $x+5$. Similarly, the expression $\log \left(e^{x+3}\right)$ will be simplified to $x+3$, the expression $\cos ^{2}(x)+\sin ^{2}(x)$ will be simplified to 1 , etc. On the other hand, $\sqrt{(x-1)^{2}}$ will not be simplified to $x-1$ as we do not make any assumption on the sign of $x-1$.</p>
<p>Coefficients simplification In the case of first order differential equations, we modify generated expressions by equivalent expressions up to a change of variable. For instance, $x+x \tan (3)+c x+1$ will be simplified to $c x+1$, as a particular choice of the constant $c$ makes these two expressions identical. Similarly, $\log \left(x^{2}\right)+c \log (x)$ becomes $c \log (x)$.</p>
<p>We apply a similar technique for second order differential equations, although simplification is sometimes a bit more involved because there are two constants $c_{1}$ and $c_{2}$. For instance, $c_{1}-c_{2} x / 5+$ $c_{2}+1$ is simplified to $c_{1} x+c_{2}$, while $c_{2} e^{c_{1}} e^{c_{1} x e-1}$ can be expressed with $c_{2} e^{c_{2} x}$, etc.</p>
<p>We also perform transformations that are not strictly equivalent, as long as they hold under specific assumptions. For instance, we simplify $\tan \left(\sqrt{c_{2}} x\right)+\cosh \left(c_{1}+1\right)+4$ to $c_{1}+\tan \left(c_{2} x\right)$, although the constant term can be negative in the second expression, but not the first one. Similarly $e^{3} e^{c_{1} x} e^{c_{1} \log \left(c_{2}\right)}$ is transformed to $c_{2} e^{c_{1} x}$.</p>
<p>Invalid expressions Finally, we also remove invalid expressions from our dataset. For instance, expressions like $\log (0)$ or $\sqrt{-2}$. To detect them, we compute in the expression tree the values of subtrees that do not depend on $x$. If a subtree does not evaluate to a finite real number (e.g. $-\infty$, $+\infty$ or a complex number), we discard the expression.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 DATASET</h3>
<p>For all considered tasks, we generate datasets using the method presented in Section 3, with:</p>
<ul>
<li>expressions with up to $n=15$ internal nodes</li>
<li>$L=11$ leaf values in ${x} \cup{-5, \ldots, 5} \backslash{0}$</li>
<li>$p_{2}=4$ binary operators:,$+,- \times$,</li>
<li>$p_{1}=15$ unary operators: $\exp , \log$, sqrt, $\sin , \cos , \tan , \sin ^{-1}, \cos ^{-1}, \tan ^{-1}, \sinh , \cosh , \tanh$, $\sinh ^{-1}, \cosh ^{-1}, \tanh ^{-1}$</li>
</ul>
<p>Statistics about our datasets are presented in Table 1. As discussed in Section 3.1, we observe that the backward approach generates derivatives (i.e. inputs) significantly longer than the forward generator. We discuss this in more detail in Section E of the appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Forward</th>
<th style="text-align: center;">Backward</th>
<th style="text-align: center;">Integration by parts</th>
<th style="text-align: center;">ODE 1</th>
<th style="text-align: center;">ODE 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training set size</td>
<td style="text-align: center;">20 M</td>
<td style="text-align: center;">40 M</td>
<td style="text-align: center;">20 M</td>
<td style="text-align: center;">40 M</td>
<td style="text-align: center;">40 M</td>
</tr>
<tr>
<td style="text-align: left;">Input length</td>
<td style="text-align: center;">$18.9 \pm 6.9$</td>
<td style="text-align: center;">$70.2 \pm 47.8$</td>
<td style="text-align: center;">$17.5 \pm 9.1$</td>
<td style="text-align: center;">$123.6 \pm 115.7$</td>
<td style="text-align: center;">$149.1 \pm 130.2$</td>
</tr>
<tr>
<td style="text-align: left;">Output length</td>
<td style="text-align: center;">$49.6 \pm 48.3$</td>
<td style="text-align: center;">$21.3 \pm 8.3$</td>
<td style="text-align: center;">$26.4 \pm 11.3$</td>
<td style="text-align: center;">$23.0 \pm 15.2$</td>
<td style="text-align: center;">$24.3 \pm 14.9$</td>
</tr>
<tr>
<td style="text-align: left;">Length ratio</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Input max length</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">450</td>
<td style="text-align: center;">226</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">508</td>
</tr>
<tr>
<td style="text-align: left;">Output max length</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">206</td>
<td style="text-align: center;">474</td>
<td style="text-align: center;">335</td>
</tr>
</tbody>
</table>
<p>Table 1: Training set sizes and length of expressions (in tokens) for different datasets. FWD and IBP tend to generate examples with outputs much longer than the inputs, while the BWD approach generates shorter outputs. Like in the BWD case, ODE generators tend to produce solutions much shorter than their equations.</p>
<h3>4.2 MODEL</h3>
<p>For all our experiments, we train a seq2seq model to predict the solutions of given problems, i.e. to predict a primitive given a function, or predict a solution given a differential equation. We use a transformer model (Vaswani et al., 2017) with 8 attention heads, 6 layers, and a dimensionality of 512. In our experiences, using larger models did not improve the performance. We train our models with the Adam optimizer (Kingma \&amp; Ba, 2014), with a learning rate of $10^{-4}$. We remove expressions with more than 512 tokens, and train our model with 256 equations per batch.</p>
<p>At inference, expressions are generated by a beam search (Koehn, 2004; Sutskever et al., 2014), with early stopping. We normalize the log-likelihood scores of hypotheses in the beam by their sequence length. We report results with beam widths of 1 (i.e. greedy decoding), 10 and 50.</p>
<p>During decoding, nothing prevents the model from generating an invalid prefix expression, e.g. $[+2 * 3]$. To address this issue, Dyer et al. (2016) use constraints during decoding, to ensure</p>
<p>that generated sequences can always be converted to valid expression trees. In our case, we found that model generations are almost always valid and we do not use any constraint. When an invalid expression is generated, we simply consider it as an incorrect solution and ignore it.</p>
<h1>4.3 Evaluation</h1>
<p>At the end of each epoch, we evaluate the ability of the model to predict the solutions of given equations. In machine translation, hypotheses given by the model are compared to references written by human translators, typically with metrics like the BLEU score (Papineni et al., 2002) that measure the overlap between hypotheses and references. Evaluating the quality of translations is a very difficult problem, and many studies showed that a better BLEU score does not necessarily correlate with a better performance according to human evaluation. Here, however, we can easily verify the correctness of our model by simply comparing generated expressions to their reference solutions.</p>
<p>For instance, for the given differential equation $x y^{\prime}-y+x=0$ with a reference solution $x \log (c / x)$ (where $c$ is a constant), our model may generate $x \log (c)-x \log (x)$. We can check that these two solutions are equal, although they are written differently, using a symbolic framework like SymPy (Meurer et al., 2017).</p>
<p>However, our model may also generate $x c-x \log (x)$ which is also a valid solution, that is actually equivalent to the previous one for a different choice of constant $c$. In that case, we replace $y$ in the differential equation by the model hypothesis. If $x y^{\prime}-y+x=0$, we conclude that the hypothesis is a valid solution. In the case of integral computation, we can simply differentiate the model hypothesis, and compare it with the function to integrate. For the three problems, we measure the accuracy of our model on equations from the test set.</p>
<p>Since we can easily verify the correctness of generated expressions, we consider all hypotheses in the beam, and not only the one with the highest score. We verify the correctness of each hypothesis, and consider that the model successfully solved the input equation if one of them is correct. As a result, results with "Beam size 10 " indicate that at least one of the 10 hypotheses in the beam was correct.</p>
<h3>4.4 RESULTS</h3>
<p>Table 2 reports the accuracy of our model for function integration and differential equations. For integration, the model achieves close to $100 \%$ performance on a held-out test set, even with greedy decoding (beam size 1). This performance is consistent over the three integration datasets (FWD, BWD, and IBP). Greedy decoding (beam size 1) does not work as well for differential equations. In particular, we observe an improvement in accuracy of almost $40 \%$ when using a large beam size of 50 for second order differential equations. Unlike in machine translation, where increasing the beam size does not necessarily increase the performance (Ott et al., 2018), we always observe significant improvements with wider beams. Typically, using a beam size of 50 provides an improvement of $8 \%$ accuracy compared to a beam size of 10 . This makes sense, as increasing the beam size will provide more hypotheses, although a wider beam may displace a valid hypothesis to consider invalid ones with better log-probabilities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Integration (FWD)</th>
<th style="text-align: center;">Integration (BWD)</th>
<th style="text-align: center;">Integration (IBP)</th>
<th style="text-align: center;">ODE (order 1)</th>
<th style="text-align: center;">ODE (order 2)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Beam size 1</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">43.0</td>
</tr>
<tr>
<td style="text-align: left;">Beam size 10</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: left;">Beam size 50</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">81.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy of our models on integration and differential equation solving. Results are reported on a held out test set of 5000 equations. For differential equations, using beam search decoding significantly improves the accuracy of the model.</p>
<h3>4.5 COMPARISON WITH MATHEMATICAL FRAMEWORKS</h3>
<p>We compare our model with three popular mathematical frameworks: Mathematica (WolframResearch, 2019), Maple and Matlab (MathWorks, 2019) ${ }^{1}$. Prefix sequences in our test set are</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>converted back to their infix representations, and given as input to the computer algebra. For a specific input, the computer algebra either returns a solution, provides no solution (or a solution including integrals or special functions), or, in the case of Mathematica, times out after a preset delay. When Mathematica times out, we conclude that it is not able to compute a solution (although it might have found a solution given more time). For integration, we evaluate on the BWD test set. By construction, the FWD data only consists of integrals generated by computer algebra systems, which makes comparison uninteresting.</p>
<p>In Table 3, we present accuracy for our model with different beam sizes, and for Mathematica with a timeout delay of 30 seconds. Table 8 in the appendix provides detailed results for different values of timeout, and explains our choice of 30 seconds. In particular, we find that with 30 seconds, only $20 \%$ of failures are due to timeouts, and only $10 \%$ when the timeout is set to 3 minutes. Even with timeout limits, evaluation would take too long on our 5000 test equations, so we only evaluate on a smaller test subset of 500 equations, on which we also re-evaluate our model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Integration (BWD)</th>
<th style="text-align: center;">ODE (order 1)</th>
<th style="text-align: center;">ODE (order 2)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mathematica (30s)</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">61.6</td>
</tr>
<tr>
<td style="text-align: left;">Matlab</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Maple</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Beam size 1</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">Beam size 10</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: left;">Beam size 50</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">81.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of our model with Mathematica, Maple and Matlab on a test set of 500 equations. For Mathematica we report results by setting a timeout of 30 seconds per equation. On a given equation, our model typically finds the solution in less than a second.</p>
<p>On all tasks, we observe that our model significantly outperforms Mathematica. On function integration, our model obtains close to $100 \%$ accuracy, while Mathematica barely reaches $85 \%$. On first order differential equations, Mathematica is on par with our model when it uses a beam size of 1, i.e. with greedy decoding. However, using a beam search of size 50 our model accuracy goes from $81.2 \%$ to $97.0 \%$, largely surpassing Mathematica. Similar observations can be made for second order differential equations, where beam search is even more critical since the number of equivalent solutions is larger. On average, Matlab and Maple have slightly lower performance than Mathematica on the problems we tested.</p>
<p>Table 4 shows examples of functions that our model was able to solve, on which Mathematica and Matlab did not find a solution. The denominator of the function to integrate, $-16 x^{8}+112 x^{7}-$ $204 x^{6}+28 x^{5}-x^{4}+1$, can be rewritten as $1-\left(4 x^{4}-14 x^{3}+x^{2}\right)^{2}$. With the simplified input:</p>
<p>$$
\frac{16 x^{3}-42 x^{2}+2 x}{\left(1-\left(4 x^{4}-14 x^{3}+x^{2}\right)^{2}\right)^{1 / 2}}
$$</p>
<p>integration becomes easier and Mathematica is able to find the solution.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Equation</th>
<th style="text-align: center;">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$y^{\prime}=\frac{16 x^{3}-42 x^{2}+2 x}{\left(-16 x^{8}+112 x^{7}-204 x^{6}+28 x^{5}-x^{4}+1\right)^{1 / 2}}$</td>
<td style="text-align: center;">$y=\sin ^{-1}\left(4 x^{4}-14 x^{3}+x^{2}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">$3 x y \cos (x)-\sqrt{9 x^{2} \sin (x)^{2}+1} y^{\prime}+3 y \sin (x)=0$</td>
<td style="text-align: center;">$y=c \exp \left(\sinh ^{-1}(3 x \sin (x))\right)$</td>
</tr>
<tr>
<td style="text-align: center;">$4 x^{4} y y^{\prime \prime}-8 x^{4} y^{\prime 2}-8 x^{3} y y^{\prime}-3 x^{3} y^{\prime \prime}-8 x^{2} y^{2}-6 x^{2} y^{\prime}-3 x^{2} y^{\prime \prime}-9 x y^{\prime}-3 y=0$</td>
<td style="text-align: center;">$y=\frac{c_{1}+3 x+3 \log (x)}{x\left(c_{2}+4 x\right)}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Examples of problems that our model is able to solve, on which Mathematica and Matlab were not able to find a solution. For each equation, our model finds a valid solution with greedy decoding.</p>
<h1>4.6 EQUIVALENT SOLUTIONS</h1>
<p>An interesting property of our model is that it is able to generate solutions that are exactly equivalent, but written in different ways. For instance, we consider the following first order differential equation, along with one of its solutions:</p>
<p>$$
162 x \log (x) y^{\prime}+2 y^{3} \log (x)^{2}-81 y \log (x)+81 y=0 \quad y=\frac{9 \sqrt{x} \sqrt{\frac{1}{\log (x)}}}{\sqrt{c+2 x}}
$$</p>
<p>In Table 5, we report the top 10 hypotheses returned by our model for this equation. We observe that all generations are actually valid solutions, although they are expressed very differently. They are however not all equal: merging the square roots within the first and third equations would give the same expression except that the third one would contain a factor 2 in front of the constant $c$, but up to a change of variable, these two solutions are actually equivalent. The ability of the model to recover equivalent expressions, without having been trained to do so, is very intriguing.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hypothesis</th>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Hypothesis</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\frac{9 \sqrt{x} \sqrt{\frac{1}{\log (x)}}}{\sqrt{c+2 x}}$</td>
<td style="text-align: center;">-0.047</td>
<td style="text-align: center;">$\frac{9}{\sqrt{\frac{c \log (x)}{x}+2 \log (x)}}$</td>
<td style="text-align: center;">-0.124</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{9 \sqrt{x}}{\sqrt{c+2 x} \sqrt{\log (x)}}$</td>
<td style="text-align: center;">-0.056</td>
<td style="text-align: center;">$\frac{9 \sqrt{x}}{\sqrt{c \log (x)+2 x \log (x)}}$</td>
<td style="text-align: center;">-0.139</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{9 \sqrt{2} \sqrt{x} \sqrt{\frac{1}{\log (x)}}}{2 \sqrt{c+x}}$</td>
<td style="text-align: center;">-0.115</td>
<td style="text-align: center;">$\frac{9}{\sqrt{\frac{c}{x}+2 \sqrt{\log (x)}}}$</td>
<td style="text-align: center;">-0.144</td>
</tr>
<tr>
<td style="text-align: center;">$9 \sqrt{x} \sqrt{\frac{1}{c \log (x)+2 x \log (x)}}$</td>
<td style="text-align: center;">-0.117</td>
<td style="text-align: center;">$9 \sqrt{\frac{1}{\frac{c \log (x)}{x}+2 \log (x)}}$</td>
<td style="text-align: center;">-0.205</td>
</tr>
<tr>
<td style="text-align: center;">$\frac{9 \sqrt{2} \sqrt{x}}{2 \sqrt{c+x} \sqrt{\log (x)}}$</td>
<td style="text-align: center;">-0.124</td>
<td style="text-align: center;">$9 \sqrt{x} \sqrt{\frac{1}{c \log (x)+2 x \log (x)+\log (x)}}$</td>
<td style="text-align: center;">-0.232</td>
</tr>
</tbody>
</table>
<p>Table 5: Top 10 generations of our model for the first order differential equation $162 x \log (x) y^{\prime}+2 y^{3} \log (x)^{2}-$ $81 y \log (x)+81 y=0$, generated with a beam search. All hypotheses are valid solutions, and are equivalent up to a change of the variable $c$. Scores are log-probabilities normalized by sequence lengths.</p>
<h3>4.7 GENERALIZATION ACROSS GENERATORS</h3>
<p>Models for integration achieve close to $100 \%$ performance on held-out test samples generated with the same method as their training data. In Table 6, we compare the accuracy on the FWD, BWD and IBP test sets for 4 models trained using different combinations of training data. When the test set is generated with the same generator as the training set, the model performs extremely well. For instance, the three models trained either on BWD, BWD + IBP or BWD + IBP + FWD achieve $99.7 \%$ accuracy on the BWD test set with a beam size of 50 .
On the other hand, even with a beam size of 50 , a FWD-trained model only achieves $17.2 \%$ accuracy on the BWD test set, and a BWD-trained model achieves $27.5 \%$ on the FWD test set. This results from the very different structure of the FWD and BWD data sets (cf. Table 1 and the discussion in Section E of the appendix). Overall, a model trained on BWD samples learns that integration tends to shorten expressions, a property that does not hold for FWD samples. Adding diversity to the training set improves the results. For instance, adding IBP-generated examples to the BWD-trained model raises the FWD test accuracy from $27.5 \%$ to $56.1 \%$, and with additional FWD training data the model reaches $94.3 \%$ accuracy. Generalization is further discussed in Section E of the appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training data</th>
<th style="text-align: center;">Forward (FWD)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Backward (BWD)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Integration by parts (IBP)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beam 1</td>
<td style="text-align: center;">Beam 10</td>
<td style="text-align: center;">Beam 50</td>
<td style="text-align: center;">Beam 1</td>
<td style="text-align: center;">Beam 10</td>
<td style="text-align: center;">Beam 50</td>
<td style="text-align: center;">Beam 1</td>
<td style="text-align: center;">Beam 10</td>
<td style="text-align: center;">Beam 50</td>
</tr>
<tr>
<td style="text-align: center;">FWD</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">88.9</td>
</tr>
<tr>
<td style="text-align: center;">BWD</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">59.2</td>
</tr>
<tr>
<td style="text-align: center;">BWD + IBP</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.5</td>
</tr>
<tr>
<td style="text-align: center;">BWD + IBP + FWD</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy of our models on function integration. We report the accuracy of our model on the three integration datasets: forward (FWD), backward (BWD), and integration by parts (IBP), for four models trained with different combinations of training data. We observe that a FWD-trained model performs poorly when it tries to integrate functions from the BWD dataset. Similarly, a BWD-trained model only obtain $27.5 \%$ accuracy on the FWD dataset, as it fails to integrate simple functions like $x^{2} \sin (x)$. On the other hand, training on both the BWD + IBP datasets allows the model to reach up to $56.1 \%$ accuracy on FWD. Training on all datasets allows the model to perform well on the three distributions.</p>
<h1>4.8 Generalization beyond the generator - SYMPy</h1>
<p>Our forward generator, FWD, generates a set of pairs $(f, F)$ of functions with their integrals. It relies on an external symbolic framework, SymPy (Meurer et al., 2017), to compute the integral of randomly generated functions. SymPy is not perfect, and fails to compute the integral of many integrable functions. In particular, we found that the accuracy of SymPy on the BWD test set is only $30 \%$. Our FWD-trained model only obtains an accuracy of $17.2 \%$ on BWD. However, we observed that the FWD-trained model is sometimes able to compute the integral of functions that SymPy cannot compute. This means that by only training on functions that SymPy can integrate, the model was able to generalize to functions that SymPy cannot integrate. Table 7 presents examples of such functions with their integrals.</p>
<p>$$
\begin{array}{rr}
x^{2}\left(\tan ^{2}(x)+1\right)+2 x \tan (x)+1 &amp; x^{2} \tan (x)+x \
1+\frac{2 \cos (2 x)}{\sqrt{\sin ^{2}(2 x)+1}} &amp; x+\operatorname{asinh}(\sin (2 x)) \
\frac{x \tan (x)+\log (x \cos (x))-1}{\log (x \cos (x))^{2}} &amp; \frac{x}{\log (x \cos (x))} \
-\frac{2 x \cos \left(\operatorname{asin}^{2}(x)\right) \operatorname{asin}(x)}{\sqrt{1-x^{2}} \sin ^{2}\left(\operatorname{asin}^{2}(x)\right)}+\frac{1}{\sin \left(\operatorname{asin}^{2}(x)\right)} &amp; \frac{x}{\sin \left(\operatorname{asin}^{2}(x)\right)} \
\sqrt{x}+x\left(\frac{2 x}{\sqrt{x^{4}+1}}+1+\frac{1}{2 \sqrt{x}}\right)+x+\operatorname{asinh}\left(x^{2}\right) &amp; x\left(\sqrt{x}+x+\operatorname{asinh}\left(x^{2}\right)\right) \
\frac{-3-\frac{3\left(-3 x^{2} \sin \left(x^{3}\right)+\frac{1}{2 \sqrt{x}}\right)}{\sqrt{x}+\cos \left(x^{3}\right)}}{\left(x+\log \left(\sqrt{x}+\cos \left(x^{3}\right)\right)\right)^{2}}}{\frac{3}{x+\log \left(\sqrt{x}+\cos \left(x^{3}\right)\right)}} \
\frac{-2 \tan ^{2}(\log (\log (x))) -2}{\log (x) \tan ^{2}(\log (\log (x)))}+\frac{2}{\tan (\log (\log (x)))} &amp; \frac{2 x}{\tan (\log (\log (x)))}
\end{array}
$$</p>
<p>Table 7: Examples of functions / integrals that the FWD-trained model can integrate, but not SymPy. Although the FWD model was only trained on a subset of functions that SymPy can integrate, it learned to generalize to functions that SymPy cannot integrate.</p>
<h1>5 RELATED WORK</h1>
<p>Computers were used for symbolic mathematics since the late 1960s (Moses, 1974). Computer algebra systems (CAS), such as Matlab, Mathematica, Maple, PARI and SAGE, are used for a variety of mathematical tasks (Gathen \&amp; Gerhard, 2013). Modern methods for symbolic integration are based on Risch algorithm (Risch, 1970). Implementations can be found in Bronstein (2005) and Geddes et al. (1992). However, the complete description of the Risch algorithm takes more than 100 pages, and is not fully implemented in current mathematical framework.</p>
<p>Deep learning networks have been used to simplify treelike expressions. Zaremba et al. (2014) use recursive neural networks to simplify complex symbolic expressions. They use tree representations for expressions, but provide the model with problem related information: possible rules for simplification. The neural network is trained to select the best rule. Allamanis et al. (2017) propose a framework called neural equivalence networks to learn semantic representations of algebraic expressions. Typically, a model is trained to map different but equivalent expressions (like the 10 expressions proposed in Table 5) to the same representation. However, they only consider Boolean and polynomial expressions. More recently, Arabshahi et al. (2018a;b) used tree-structured neural networks to verify the correctness of given symbolic entities, and to predict missing entries in incomplete mathematical equations. They also showed that these networks could be used to predict whether an expression is a valid solution of a given differential equation.</p>
<p>Most attempts to use deep networks for mathematics have focused on arithmetic over integers (sometimes over polynomials with integer coefficients). For instance, Kaiser \&amp; Sutskever (2015) proposed the Neural-GPU architecture, and train networks to perform additions and multiplications of numbers given in their binary representations. They show that a model trained on numbers with up-to 20 bits can be applied to much larger numbers at test time, while preserving a perfect accuracy. Freivalds \&amp; Liepins (2017) proposed an improved version of the Neural-GPU by using hard non-linear activation functions, and a diagonal gating mechanism.</p>
<p>Saxton et al. (2019) use LSTMs (Hochreiter \&amp; Schmidhuber, 1997) and transformers on a wide range of problems, from arithmetic to simplification of formal expressions. However, they only consider polynomial functions, and the task of differentiation, which is significantly easier than integration. Trask et al. (2018) propose the Neural arithmetic logic units, a new module designed to learn systematic numerical computation, and that can be used within any neural network. Like Kaiser \&amp; Sutskever (2015), they show that at inference their model can extrapolate on numbers orders of magnitude larger than the ones seen during training.</p>
<h2>6 CONCLUSION</h2>
<p>In this paper, we show that standard seq2seq models can be applied to difficult tasks like function integration, or solving differential equations. We propose an approach to generate arbitrarily large datasets of equations, with their associated solutions. We show that a simple transformer model trained on these datasets can perform extremely well both at computing function integrals, and solving differential equations, outperforming state-of-the-art mathematical frameworks like Matlab or Mathematica that rely on a large number of algorithms and heuristics, and a complex implementation (Risch, 1970). Results also show that the model is able to write identical expressions in very different ways.</p>
<p>These results are surprising given the difficulty of neural models to perform simpler tasks like integer addition or multiplication. However, proposed hypotheses are sometimes incorrect, and considering multiple beam hypotheses is often necessary to obtain a valid solution. The validity of a solution itself is not provided by the model, but by an external symbolic framework (Meurer et al., 2017). These results suggest that in the future, standard mathematical frameworks may benefit from integrating neural components in their solvers.</p>
<h1>REFERENCES</h1>
<p>Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles Sutton. Learning continuous semantic representations of symbolic expressions. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, pp. 80-88. JMLR.org, 2017.</p>
<p>Forough Arabshahi, Sameer Singh, and Animashree Anandkumar. Combining symbolic expressions and black-box function evaluations for training neural programs. In International Conference on Learning Representations, 2018a.</p>
<p>Forough Arabshahi, Sameer Singh, and Animashree Anandkumar. Towards solving differential equations through neural programming. 2018b.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.
M. Bronstein. Symbolic Integration I: Transcendental Functions. Algorithms and combinatorics. Springer, 2005. ISBN 978-3-540-21493-9.</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 199-209, 2016.</p>
<p>Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. Learning to parse and translate improves neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 72-78, 2017.</p>
<p>Philippe Flajolet and Andrew M. Odlyzko. Singularity analysis of generating functions. SIAM J. Discrete Math., 3(2):216-240, 1990.</p>
<p>Philippe Flajolet and Robert Sedgewick. Analytic Combinatorics. Cambridge University Press, New York, NY, USA, 1 edition, 2009. ISBN 0521898064, 9780521898065.</p>
<p>Karlis Freivalds and Renars Liepins. Improving the neural gpu architecture for algorithm learning. ArXiv, abs/1702.08727, 2017.</p>
<p>Joachim von zur Gathen and Jurgen Gerhard. Modern Computer Algebra. Cambridge University Press, New York, NY, USA, 3rd edition, 2013. ISBN 1107039037, 9781107039032.</p>
<p>Keith O. Geddes, Stephen R. Czapor, and George Labahn. Algorithms for Computer Algebra. Kluwer Academic Publishers, Norwell, MA, USA, 1992. ISBN 0-7923-9259-0.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.</p>
<p>Lukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. CoRR, abs/1511.08228, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Donald E. Knuth. The Art of Computer Programming, Volume 1 (3rd Ed.): Fundamental Algorithms. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA, 1997. ISBN 0-201-89683-4.</p>
<p>Philipp Koehn. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Conference of the Association for Machine Translation in the Americas, pp. 115-124. Springer, 2004.</p>
<p>Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk. Deep network guided proof search. arXiv preprint arXiv:1701.06972, 2017.</p>
<p>MathWorks. Matlab optimization toolbox (r2019a), 2019. The MathWorks, Natick, MA, USA.</p>
<p>Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štěpán Roučka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, January 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs.103. URL https://doi.org/10.7717/peerj-cs. 103.</p>
<p>Joel Moses. Macsyma - the fifth year. SIGSAM Bull., 8(3):105-110, August 1974. ISSN 0163-5824.
Myle Ott, Michael Auli, David Grangier, et al. Analyzing uncertainty in neural machine translation. In International Conference on Machine Learning, pp. 3953-3962, 2018.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.</p>
<p>Robert H. Risch. The solution of the problem of integration in finite terms. Bull. Amer. Math. Soc., 76(3):605-608, 051970.</p>
<p>David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations. MIT Press, Cambridge, MA, USA, 1986. ISBN 0-262-68053-X.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019 .
N. J. A. Sloane. The encyclopedia of integer sequences, 1996.</p>
<p>Richard P. Stanley. Enumerative Combinatorics: Volume 1. Cambridge University Press, New York, NY, USA, 2nd edition, 2011. ISBN 1107602629, 9781107602625.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pp. 3104-3112, 2014.</p>
<p>Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1556-1566, 2015.</p>
<p>Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, pp. 8035-8044, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000-6010, 2017.</p>
<p>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a foreign language. In Advances in neural information processing systems, pp. 2773-2781, 2015 .
H.S. Wilf. generatingfunctionology: Third Edition. CRC Press, 2005. ISBN 978-1-4398-6439-5. URL https://www.math.upenn.edu/ wilf/gfologyLinked2.pdf.</p>
<p>Wolfram-Research. Mathematica, version 12.0, 2019. Champaign, IL, 2019.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.
Wojciech Zaremba, Karol Kurach, and Rob Fergus. Learning to discover efficient mathematical identities. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1, NIPS'14, pp. 1278-1286, Cambridge, MA, USA, 2014. MIT Press.</p>
<h1>A A SYNTAX FOR MATHEMATICAL EXPRESSIONS</h1>
<p>We represent mathematical expressions as trees with operators as internal nodes, and numbers, constants or variables, as leaves. By enumerating nodes in prefix order, we transform trees into sequences suitable for seq2seq architectures.</p>
<p>For this representation to be efficient, we want expressions, trees and sequences to be in a one-to-one correspondence. Different expressions will always result in different trees and sequences, but for the reverse to hold, we need to take care of a few special cases.</p>
<p>First, expressions like sums and products may correspond to several trees. For instance, the expression $2+3+5$ can be represented as any one of those trees:
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>We will assume that all operators have at most two operands, and that, in case of doubt, they are associative to the right. $2+3+5$ would then correspond to the rightmost tree.</p>
<p>Second, the distinction between internal nodes (operators) and leaves (mathematical primitive objects) is somewhat arbitrary. For instance, the number -2 could be represented as a basic object, or as a unary minus operator applied to the number 2 . Similarly, there are several ways to represent $\sqrt{5}$, $42 x^{5}$, or the function $\log _{10}$. For simplicity, we only consider numbers, constants and variables as possible leaves, and avoid using a unary minus. In particular, expressions like $-x$ are represented as $-1 \times x$. Here are the trees for $-2, \sqrt{5}, 42 x^{5}$ and $-x$ :
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Integers are represented in positional notation, as a sign followed by a sequence of digits (from 0 to 9 in base 10). For instance, 2354 and -34 are represented as $+2354$ and -34 . For zero, a unique representation is chosen $(+0$ or -0$)$.</p>
<h2>B MATHEMATICAL DERIVATIONS OF THE PROBLEM SPACE SIZE</h2>
<p>In this section, we investigate the size of the problem space by computing the number of expressions with $n$ internal nodes. We first deal with the simpler case where we only have binary operators $\left(p_{1}=0\right.$ ), then consider trees and expressions composed of unary and binary operators. In each case, we calculate a generating function (Flajolet \&amp; Sedgewick, 2009; Wilf, 2005) from which we derive a closed formula or recurrence on the number of expressions, and an asymptotic expansion.</p>
<h2>B. 1 BINARY TREES AND EXPRESSIONS</h2>
<p>The main part of this derivation follows (Knuth, 1997) (pages 388-389).</p>
<p>Generating function Let $b_{n}$ be the number of binary trees with $n$ internal nodes. We have $b_{0}=1$ and $b_{1}=1$. Any binary tree with $n$ internal nodes can be generated by concatenating a left and a right subtree with $k$ and $n-1-k$ internal nodes respectively. By summing over all possible values of $k$, we have that:</p>
<p>$$
b_{n}=b_{0} b_{n-1}+b_{1} b_{n-2}+\cdots+b_{n-2} b_{1}+b_{n-1} b_{0}
$$</p>
<p>Let $B(z)$ be the generating function of $b_{n}, B(z)=b_{0}+b_{1} z+b_{2} z^{2}+b_{3} z^{3}+\ldots$</p>
<p>$$
\begin{aligned}
B(z)^{2} &amp; =b_{0}^{2}+\left(b_{0} b_{1}+b_{1} b_{0}\right) z+\left(b_{0} b_{2}+b_{1} b_{1}+b 2 b_{0}\right) z^{2}+\ldots \
&amp; =b_{1}+b_{2} z+b_{3} z^{2}+\ldots \
&amp; =\frac{B(z)-b_{0}}{z}
\end{aligned}
$$</p>
<p>So, $z B(z)^{2}-B(z)+1=0$. Solving for $B(z)$ gives:</p>
<p>$$
B(z)=\frac{1 \pm \sqrt{1-4 z}}{2 z}
$$</p>
<p>and since $B(0)=b_{0}=1$, we derive the generating function for sequence $b_{n}$</p>
<p>$$
B(z)=\frac{1-\sqrt{1-4 z}}{2 z}
$$</p>
<p>We now derive a closed formula for $b_{n}$. By the binomial theorem,</p>
<p>$$
\begin{aligned}
B(z) &amp; =\frac{1}{2 z}\left(1-\sum_{k=0}^{\infty}\binom{1 / 2}{k}(-4 z)^{k}\right) \
&amp; =\frac{1}{2 z}\left(1+\sum_{k=0}^{\infty} \frac{1}{2 k-1}\binom{2 k}{k} z^{k}\right) \
&amp; =\frac{1}{2 z} \sum_{k=1}^{\infty} \frac{1}{2 k-1}\binom{2 k}{k} z^{k} \
&amp; =\sum_{k=1}^{\infty} \frac{1}{2(2 k-1)}\binom{2 k}{k} z^{k-1} \
&amp; =\sum_{k=0}^{\infty} \frac{1}{2(2 k+1)}\binom{2 k+2}{k+1} z^{k} \
&amp; =\sum_{k=0}^{\infty} \frac{1}{k+1}\binom{2 k}{k} z^{k}
\end{aligned}
$$</p>
<p>Therefore</p>
<p>$$
b_{n}=\frac{1}{n+1}\binom{2 n}{n}=\frac{(2 n)!}{(n+1)!n!}
$$</p>
<p>These are the Catalan numbers, a closed formula for the number of binary trees with $n$ internal nodes. We now observe that a binary tree with $n$ internal nodes has exactly $n+1$ leaves. Since each node in a binary tree can represent $p_{2}$ operators, and each leaf can take $L$ values, we have that a tree with $n$ nodes can take $p_{2}^{n} L^{n+1}$ possible combinations of operators and leaves. As a result, the number of binary expressions with $n$ operators is given by:</p>
<p>$$
E_{n}=\frac{(2 n)!}{(n+1)!n!} p_{2}^{n} L^{n+1}
$$</p>
<p>Asymptotic estimate To derive an asymptotic approximation of $b_{n}$, we apply the Stirling formula:</p>
<p>$$
n!\approx \sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n} \quad \text { so } \quad\binom{2 n}{n} \approx \frac{4^{n}}{\sqrt{\pi n}} \quad \text { and } \quad b_{n} \approx \frac{4^{n}}{n \sqrt{\pi n}}
$$</p>
<p>Finally, we have the following formulas for the number of expressions with $n$ internal nodes:</p>
<p>$$
E_{n} \approx \frac{1}{n \sqrt{\pi n}}\left(4 p_{2}\right)^{n} L^{n+1}
$$</p>
<h1>B. 2 UNARY-BINARY TREES</h1>
<p>Generating function Let $s_{n}$ be the number of unary-binary trees (i.e. trees where internal nodes can have one or two children) with $n$ internal nodes. We have $s_{0}=1$ and $s_{1}=2$ (the only internal node is either unary or binary).</p>
<p>Any tree with $n$ internal nodes is obtained either by adding a unary internal node at the root of a tree with $n-1$ internal nodes, or by concatenating with a binary operator a left and a right subtree with $k$ and $n-1-k$ internal nodes respectively. Summing up as before, we have:</p>
<p>$$
s_{n}=s_{n-1}+s_{0} s_{n-1}+s_{1} s_{n-2}+\cdots+s_{n-1} s_{0}
$$</p>
<p>Let $S(z)$ be the generating function of the $s_{n}$. The above formula translates into</p>
<p>$$
\begin{gathered}
S(z)^{2}=\frac{S(z)-s_{0}}{z}-S(z) \
z S(z)^{2}+(z-1) S(z)+1=0
\end{gathered}
$$</p>
<p>solving and taking into account the fact that $S(0)=1$, we obtain the generating function of the $s_{n}$</p>
<p>$$
S(z)=\frac{1-z-\sqrt{1-6 z+z^{2}}}{2 z}
$$</p>
<p>The numbers $s_{n}$ generated by $S(z)$ are known as the Schroeder numbers (OEIS A006318) (Sloane, 1996). They appear in different combinatorial problems (Stanley, 2011). Notably, they correspond to the number of paths from $(0,0)$ to $(n, n)$ of a $n \times n$ grid, moving north, east, or northeast, and never rising above the diagonal.</p>
<p>Calculation Schroeder numbers do not have a simple closed formula, but a recurrence allowing for their calculation can be derived from their generating function. Rewriting $S(z)$ as</p>
<p>$$
2 z S(z)+z-1=-\sqrt{1-6 z+z^{2}}
$$</p>
<p>and differentiating, we have</p>
<p>$$
\begin{aligned}
&amp; 2 z S^{\prime}(z)+2 S(z)+1=\frac{3-z}{\sqrt{1-6 z+z^{2}}}=\frac{3-z}{1-6 z+z^{2}}(1-z-2 z S(z)) \
&amp; 2 z S^{\prime}(z)+2 S(z)\left(1+\frac{3 z-z^{2}}{1-6 z+z^{2}}\right)=\frac{(3-z)(1-z)}{1-6 z+z^{2}}-1 \
&amp; 2 z S^{\prime}(z)+2 S(z) \frac{1-3 z}{1-6 z+z^{2}}=\frac{2+2 z}{1-6 z+z^{2}} \
&amp; z\left(1-6 z+z^{2}\right) S^{\prime}(z)+(1-3 z) S(z)=1+z
\end{aligned}
$$</p>
<p>Replacing $S(z)$ and $S^{\prime}(z)$ with their n-th coefficient yields, for $n&gt;1$</p>
<p>$$
\begin{gathered}
n s_{n}-6(n-1) s_{n-1}+(n-2) s_{n-2}+s_{n}-3 s_{n-1}=0 \
(n+1) s_{n}=3(2 n-1) s_{n-1}-(n-2) s_{n-2}
\end{gathered}
$$</p>
<p>Together with $s_{0}=1$ and $s_{1}=2$, this allows for fast $(O(n))$ calculation of Schroeder numbers.
Asymptotic estimate To derive an asymptotic formula of $s_{n}$, we develop the generating function around its smallest singularity (Flajolet \&amp; Odlyzko, 1990), i.e. the radius of convergence of the power series. Since</p>
<p>$$
1-6 z+z^{2}=(1-(3-\sqrt{8}) z)(1-(3+\sqrt{8}) z)
$$</p>
<p>The smallest singular value is</p>
<p>$$
r_{1}=\frac{1}{(3+\sqrt{8})}
$$</p>
<p>and the asymptotic formula will have the exponential term</p>
<p>$$
r_{1}^{-n}=(3+\sqrt{8})^{n}=(1+\sqrt{2})^{2 n}
$$</p>
<p>In a neighborhood of $r_{1}$, the generating function can be rewritten as</p>
<p>$$
S(z) \approx(1+\sqrt{2})\left(1-2^{1 / 4} \sqrt{1-(3+\sqrt{8}) z}\right)+O(1-(3+\sqrt{8}) z)^{3 / 2}
$$</p>
<p>Since</p>
<p>$$
\left[z_{n}\right] \sqrt{1-a z} \approx-\frac{a^{n}}{\sqrt{4 \pi n^{3}}}
$$</p>
<p>where $\left[z_{n}\right] F(z)$ denotes the n-th coefficient in the formal series of F , we have</p>
<p>$$
s_{n} \approx \frac{(1+\sqrt{2})(3+\sqrt{8})^{n}}{2^{3 / 4} \sqrt{\pi n^{3}}}=\frac{(1+\sqrt{2})^{2 n+1}}{2^{3 / 4} \sqrt{\pi n^{3}}}
$$</p>
<p>Comparing with the number of binary trees, we have</p>
<p>$$
s_{n} \approx 1.44(1.46)^{n} b_{n}
$$</p>
<h1>B. 3 UNARY-BINARY EXPRESSIONS</h1>
<p>In the binary case, the number of expressions can be derived from the number of trees. This cannot be done in the unary-binary case, as the number of leaves in a tree with $n$ internal nodes depends on the number of binary operators $\left(n_{2}+1\right)$.</p>
<p>Generating function The number of trees with $n$ internal nodes and $n_{2}$ binary operators can be derived from the following observation: any unary-binary tree with $n_{2}$ binary internal nodes can be generated from a binary tree by adding unary internal nodes. Each node in the binary tree can receive one or several unary parents.
Since the binary tree has $2 n_{2}+1$ nodes and the number of unary internal nodes to be added is $n-n_{2}$, the number of unary-binary trees that can be created from a specific binary tree is the number of multisets with $2 n_{2}+1$ elements on $n-n_{2}$ symbols, that is</p>
<p>$$
\binom{n+n_{2}}{n-n_{2}}=\binom{n+n_{2}}{2 n_{2}}
$$</p>
<p>If $b_{q}$ denotes the q-th Catalan number, the number of trees with $n_{2}$ binary operators among $n$ is</p>
<p>$$
\binom{n+n_{2}}{2 n_{2}} b_{n_{2}}
$$</p>
<p>Since such trees have $n_{2}+1$ leaves, with $L$ leaves, $p_{2}$ binary and $p_{1}$ unary operators to choose from, the number of expressions is</p>
<p>$$
E\left(n, n_{2}\right)=\binom{n+n_{2}}{2 n_{2}} b_{n_{2}} p_{2}^{n_{2}} p_{1}^{n-n_{2}} L^{n_{2}+1}
$$</p>
<p>Summing over all values of $n_{2}$ (from 0 to $n$ ) yields the number of different expressions</p>
<p>$$
E_{n}=\sum_{n_{2}=0}^{n}\binom{n+n_{2}}{2 n_{2}} b_{n_{2}} p_{2}^{n_{2}} p_{1}^{n-n_{2}} L^{n_{2}+1} z^{n}
$$</p>
<p>Let $E(z)$ be the corresponding generating function.</p>
<p>$$
\begin{aligned}
E(z) &amp; =\sum_{n=0}^{\infty} E_{n} z^{n} \
&amp; =\sum_{n=0}^{\infty} \sum_{n_{2}=0}^{n}\binom{n+n_{2}}{2 n_{2}} b_{n_{2}} p_{2}^{n_{2}} p_{1}^{n-n_{2}} L^{n_{2}+1} z^{n} \
&amp; =L \sum_{n=0}^{\infty} \sum_{n_{2}=0}^{n}\binom{n+n_{2}}{2 n_{2}} b_{n_{2}}\left(\frac{L p_{2}}{p_{1}}\right)^{n_{2}} p_{1}^{n} z^{n} \
&amp; =L \sum_{n=0}^{\infty} \sum_{n_{2}=0}^{\infty}\binom{n+n_{2}}{2 n_{2}} b_{n_{2}}\left(\frac{L p_{2}}{p_{1}}\right)^{n_{2}}\left(p_{1} z\right)^{n}
\end{aligned}
$$</p>
<p>since $\binom{n+n_{2}}{2 n_{2}}=0$ when $n&gt;n_{2}$</p>
<p>$$
\begin{aligned}
E(z) &amp; =L \sum_{n_{2}=0}^{\infty} b_{n_{2}}\left(\frac{L p_{2}}{p_{1}}\right)^{n_{2}} \sum_{n=0}^{\infty}\binom{n+n_{2}}{2 n_{2}}\left(p_{1} z\right)^{n} \
&amp; =L \sum_{n_{2}=0}^{\infty} b_{n_{2}}\left(\frac{L p_{2}}{p_{1}}\right)^{n_{2}} \sum_{n=0}^{\infty}\binom{n+2 n_{2}}{2 n_{2}}\left(p_{1} z\right)^{n+n_{2}} \
&amp; =L \sum_{n_{2}=0}^{\infty} b_{n_{2}}\left(L p_{2} z\right)^{n_{2}} \sum_{n=0}^{\infty}\binom{n+2 n_{2}}{2 n_{2}}\left(p_{1} z\right)^{n}
\end{aligned}
$$</p>
<p>applying the binomial formula</p>
<p>$$
\begin{aligned}
E(z) &amp; =L \sum_{n_{2}=0}^{\infty} b_{n_{2}}\left(L p_{2} z\right)^{n_{2}} \frac{1}{\left(1-p_{1} z\right)^{2 n_{2}+1}} \
&amp; =\frac{L}{1-p_{1} z} \sum_{n_{2}=0}^{\infty} b_{n_{2}}\left(\frac{L p_{2} z}{\left(1-p_{1} z\right)^{2}}\right)^{n_{2}}
\end{aligned}
$$</p>
<p>applying the generating function for binary trees</p>
<p>$$
\begin{aligned}
E(z) &amp; =\frac{L}{1-p_{1} z}\left(\frac{1-\sqrt{1-4 \frac{L p_{2} z}{\left(1-p_{1} z\right)^{2}}}}{2 \frac{L p_{2} z}{\left(1-p_{1} z\right)^{2}}}\right) \
&amp; =\frac{1-p_{1} z}{2 p_{2} z}\left(1-\sqrt{1-4 \frac{L p_{2} z}{\left(1-p_{1} z\right)^{2}}}\right) \
&amp; =\frac{1-p_{1} z-\sqrt{\left(1-p_{1} z\right)^{2}-4 L p_{2} z}}{2 p_{2} z}
\end{aligned}
$$</p>
<p>Reducing, we have</p>
<p>$$
E(z)=\frac{1-p_{1} z-\sqrt{1-2\left(p_{1}+2 L p_{2} k\right) z+p_{1} z^{2}}}{2 p_{2} z}
$$</p>
<p>Calculation As before, there is no closed simple formula for $E_{n}$, but we can derive a recurrence formula by differentiating the generating function, rewritten as</p>
<p>$$
\begin{aligned}
&amp; 2 p_{2} z E(z)+p_{1} z-1=-\sqrt{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}} \
&amp; 2 p_{2} z E^{\prime}(z)+2 p_{2} E(z)+p_{1}=\frac{p_{1}+2 p_{2} L-p_{1} z}{\sqrt{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}}} \
&amp; 2 p_{2} z E^{\prime}(z)+2 p_{2} E(z)+p_{1}=\frac{\left(p_{1}+2 p_{2} L-p_{1} z\right)\left(1-p_{1} z-2 p_{2} z E(z)\right)}{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}} \
&amp; 2 p_{2} z E^{\prime}(z)+2 p_{2} E(z)\left(1+\frac{z\left(p_{1}+2 p_{2} L-p_{1} z\right)}{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}}\right)=\frac{\left(p_{1}+2 p_{2} L-p_{1} z\right)\left(1-p_{1} z\right)}{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}}-p_{1} \
&amp; 2 p_{2} z E^{\prime}(z)+2 p_{2} E(z)\left(\frac{1-\left(p_{1}+2 p_{2} L\right) z}{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}}\right)=\frac{2 p_{2} L\left(1+p_{1} z\right)+p_{1}\left(p_{1}-1\right) z}{1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}} \
&amp; 2 p_{2} z E^{\prime}(z)\left(1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}\right)+2 p_{2} E(z)\left(1-\left(p_{1}+2 p_{2} L\right) z\right)=\left(2 p_{2} L\left(1+p_{1} z\right)+p_{1}\left(p_{1}-1\right) z\right)
\end{aligned}
$$</p>
<p>replacing $E(z)$ and $E^{\prime}(z)$ with their coefficients
$2 p_{2}\left(n E_{n}-2\left(p_{1}+2 p_{2} L\right)(n-1) E_{n-1}+p_{1}(n-2) E(n-2)\right)+2 p_{2}\left(E_{n}-\left(p_{1}+2 p_{2} L\right) E_{n-1}\right)=0$
$(n+1) E_{n}-\left(p_{1}+2 p_{2} L\right)(2 n-1) E_{n-1}+p_{1}(n-2) E_{n-2}=0$
$(n+1) E_{n}=\left(p_{1}+2 p_{2} L\right)(2 n-1) E_{n-1}-p_{1}(n-2) E_{n-2}$</p>
<p>which together with</p>
<p>$$
\begin{aligned}
&amp; E_{0}=L \
&amp; E_{1}=\left(p_{1}+p_{2} L\right) L
\end{aligned}
$$</p>
<p>provides a formula for calculating $E_{n}$.
Asymptotic estimate As before, approximations of $E_{n}$ for large $n$ can be found by developing $E(z)$ in the neighbourhood of the root with the smallest module of</p>
<p>$$
1-2\left(p_{1}+2 p_{2} L\right) z+p_{1} z^{2}
$$</p>
<p>The roots are</p>
<p>$$
\begin{aligned}
&amp; r_{1}=\frac{p_{1}}{p_{1}+2 p_{2} L-\sqrt{p_{1}^{2}+4 p_{2}^{2} L^{2}+4 p_{2} p_{1} L-p_{1}}} \
&amp; r_{2}=\frac{p_{1}}{p_{1}+2 p_{2} L+\sqrt{p_{1}^{2}+4 p_{2}^{2} L^{2}+4 p_{2} p_{1} L-p_{1}}}
\end{aligned}
$$</p>
<p>both are positive and the smallest one is $r_{2}$
To alleviate notation, let</p>
<p>$$
\begin{gathered}
\delta=\sqrt{p_{1}^{2}+4 p_{2}^{2} L^{2}+4 p_{2} p_{1} L-p_{1}} \
r_{2}=\frac{p_{1}}{p_{1}+2 p_{2} L+\delta}
\end{gathered}
$$</p>
<p>developing $E(z)$ near $r_{2}$,</p>
<p>$$
\begin{gathered}
E(z) \approx \frac{1-p_{1} r_{2}-\sqrt{1-r_{2}\left(\frac{p_{1}+2 p_{2} L-\delta}{p_{1}}\right)} \sqrt{1-\frac{z}{r_{2}}}}{2 p_{2} r_{2}}+O\left(1-\frac{z}{r_{2}}\right)^{3 / 2} \
E(z) \approx \frac{p_{1}+2 p_{2} L+\delta-p_{1}^{2}-\sqrt{p_{1}+2 p_{2} L+\delta} \sqrt{2 \delta} \sqrt{1-\frac{z}{r_{2}}}}{2 p_{2} p_{1}}+O\left(1-\frac{z}{r_{2}}\right)^{3 / 2}
\end{gathered}
$$</p>
<p>and therefore</p>
<p>$$
E_{n} \approx \frac{\sqrt{\delta} r_{2}^{-n-\frac{1}{2}}}{2 p_{2} \sqrt{2 \pi p_{1} n^{3}}}=\frac{\sqrt{\delta}}{2 p_{2} \sqrt{2 \pi n^{3}}} \frac{\left(p_{1}+2 p_{2} L+\delta\right)^{n+\frac{1}{2}}}{p_{1}^{n+1}}
$$</p>
<h1>C GENERATING RANDOM EXPRESSIONS</h1>
<p>In this section we present algorithms to generate random expressions with $n$ internal nodes. We achieve this by generating random trees, and selecting randomly their nodes and leaves. We begin with the simpler binary case $\left(p_{1}=0\right)$.</p>
<h2>C. 1 Binary TREES</h2>
<p>To generate a random binary tree with $n$ internal nodes, we use the following one-pass procedure. Starting with an empty root node, we determine at each step the position of the next internal nodes among the empty nodes, and repeat until all internal nodes are allocated.</p>
<p>Start with an empty node, set $e=1$;
while $n&gt;0$ do
Sample a position $k$ from $K(e, n)$;
Sample the $k$ next empty nodes as leaves;
Sample an operator, create two empty children;
Set $e=e-k+1$ and $n=n-1$;
end
Algorithm 1: Generate a random binary tree</p>
<p>We denote by $e$ the number of empty nodes, by $n&gt;0$ the number of operators yet to be generated, and by $K(e, n)$ the probability distribution of the position ( 0 -indexed) of the next internal node to allocate.</p>
<p>To calculate $K(e, n)$, let us define $D(e, n)$, the number of different binary subtrees that can be generated from $e$ empty elements, with $n$ internal nodes to generate. We have</p>
<p>$$
\begin{aligned}
&amp; D(0, n)=0 \
&amp; D(e, 0)=1 \
&amp; D(e, n)=D(e-1, n)+D(e+1, n-1)
\end{aligned}
$$</p>
<p>The first equation states that no tree can be generated with zero empty node and $n&gt;0$ operators. The second equation says that if no operator is to be allocated, empty nodes must all be leaves and there is only one possible tree. The last equation states that if we have $e&gt;0$ empty nodes, the first one is either a leaf (and there are $D(e-1, n)$ such trees) or an internal node ( $D(e+1, n-1)$ trees). This allows us to compute $D(e, n)$ for all $e$ and $n$.</p>
<p>To calculate distribution $K(e, n)$, observe that among the $D(e, n)$ trees with $e$ empty nodes and $n$ operators, $D(e+1, n-1)$ have a binary node in their first position. Therefore</p>
<p>$$
P(K(e, n)=0)=\frac{D(e+1, n-1)}{D(e, n)}
$$</p>
<p>Of the remaining $D(e-1, n)$ trees, $D(e, n-1)$ have a binary node in their first position (same argument for $e-1$ ), that is</p>
<p>$$
P(K(e, n)=1)=\frac{D(e, n-1)}{D(e, n)}
$$</p>
<p>By induction over $k$, we have the general formula</p>
<p>$$
P(K(e, n)=k)=\frac{D(e-k+1, n-1)}{D(e, n)}
$$</p>
<h1>C. 2 UNARY-BINARY TREES</h1>
<p>In the general case, internal nodes can be of two types: unary or binary. We adapt the previous algorithm by considering the two-dimensional probability distribution $L(e, n)$ of position ( 0 -indexed) and arity of the next internal node (i.e. $P(L(e, n)=(k, a)$ is the probability that the next internal node is in position $k$ and has arity $a$ ).</p>
<p>Start with an empty node, set $e=1$;
while $n&gt;0$ do
Sample a position $k$ and arity $a$ from $L(e, n)$ (if $a=1$ the next internal node is unary);
Sample the $k$ next empty nodes as leaves;
if $a=1$ then
Sample a unary operator;
Create one empty child;
Set $e=e-k$;
end
else
Sample a binary operator;
Create two empty children;
Set $e=e-k+1$;
end
Set $n=n-1$;
end
Algorithm 2: Generate a random unary-binary tree</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ All experiments were run with Mathematica 12.0.0.0, Maple 2019 and Matlab R2019a.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
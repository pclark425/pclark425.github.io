<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1849 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1849</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1849</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-e2b5f5c3205dfcd255a311821e4241d293e929c8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e2b5f5c3205dfcd255a311821e4241d293e929c8" target="_blank">Can LLMs Replace Manual Annotation of Software Engineering Artifacts?</a></p>
                <p><strong>Paper Venue:</strong> IEEE Working Conference on Mining Software Repositories</p>
                <p><strong>Paper TL;DR:</strong> This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and coderelated artifacts, and proposes model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators.</p>
                <p><strong>Paper Abstract:</strong> Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and coderelated artifacts. We study this idea by applying six state-of-theart LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1849.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1849.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeSumm-LLM-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code summarization LLM-as-judge vs. human raters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of multiple LLMs (GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, GPT-3.5, Llama3, Mixtral) with professional human raters on Likert-style evaluation of code summaries (accuracy, adequacy, conciseness, similarity), reporting Krippendorff's alpha inter-rater agreements and effects of selecting LLM outputs by confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (Likert-style ratings via few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, GPT-3.5, Llama3(70B), Mixtral(8x22B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Few-shot prompting (3 examples) asking models to select one of four Likert options ('Strongly agree','Agree','Disagree','Strongly disagree') per criterion; single-rating with rubric-like instructions</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>code documentation / code summaries</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>function-level source code (LeClair dataset; language: Java functions)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>accuracy, adequacy, conciseness, similarity</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>professional programmers recruited via Upwork; each sample rated by at least three independent human raters</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>experienced programmers (paid professionals via Upwork at USD60/hr)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Human-Human mean alpha: accuracy 0.38, adequacy 0.40, conciseness 0.24, similarity 0.64; Human-Model mean alpha: accuracy 0.48, adequacy 0.41, conciseness 0.21, similarity 0.44; Model-Model mean alpha: accuracy 0.76, adequacy 0.74, conciseness 0.74, similarity 0.68</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Tasks/criteria with clearer, more objective mapping to text (e.g., similarity) and high model-model agreement; use of strong LLMs (top-3 models) and few-shot prompting improved alignment; selecting samples by model output probability (high-confidence) maintained agreement up to substantial fractions</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Criteria with inherent subjectivity or low human-human agreement (e.g., conciseness) show weaker or inconsistent human-model alignment; smaller models exhibited bias toward extreme choices</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Some harder samples (ambiguous or context-dependent summaries) show lower agreement; overall, models tended to reflect majority training signal and may disagree on nuanced cases where humans also disagree</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear, well-specified Likert prompts and few-shot examples increased model alignment; where human-human agreement was low, model-human agreement was also unstable</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>420</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>See Human-Human alpha above (per-criterion means and medians reported in paper/table)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Human-model agreement is in the same order as human-human for many criteria (e.g., name-value and some summarization criteria); for some criteria (similarity) human-human > human-model, while for accuracy human-model exceeded mean human-human in reported mean values (accuracy human-model 0.48 vs human-human 0.38).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No fine-tuning; used few-shot prompting with 3-4 illustrative examples per prompt (manual selection of shots).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Top LLMs often reach human-level agreement for many code-summary criteria; model-model agreement is high and models tend to reflect majority opinions; selecting model outputs by high output-probability preserves inter-rater agreement and allows replacing one human rater for many samples without statistically significant change.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human-model agreement varies by criterion; replacing many humans inflates agreement (models agree with each other more than humans); some human raters and samples not uniformly overlapping; potential data overlap with LLM training corpora; few-shot examples selection can affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1849.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1849.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NameValue-LLM-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Name-value inconsistency detection: LLM Likert ratings vs. multiple human raters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of LLMs with 11 human raters on assessing name-value consistency (Likert 1–5) in code examples (Nalin dataset), using Krippendorff's alpha to quantify agreement; found human-model agreement similar to human-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge using Likert-style ordinal ratings (1–5) via few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (top-3 reported), plus GPT-3.5, Llama3, Mixtral in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Few-shot prompting with ordinal Likert instruction (1 to 5 scale) and illustrative examples</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code variable assignments / name-value examples</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Jupyter notebooks / general code snippets (Nalin dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ease of understanding / name-value consistency (Likert 1-5 where 1 indicates difficult/inconsistent, 5 indicates easy/consistent)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>11 human participants rated all 40 samples independently</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>11</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>participants as in original Nalin study (paper does not further specify professional seniority in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Human-Human mean alpha: 0.52; Human-Model mean alpha: 0.49; Model-Model mean alpha (top-3): 0.66</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Task is relatively objective and constrained (clear mapping of name to type/value); high model-model agreement and strong LLMs increase alignment; few-shot examples help</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Fewer samples lead to wider confidence intervals; highly ambiguous naming cases still cause disagreements</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Simpler, direct name-value mismatches show high agreement; more ambiguous or domain-specific naming reduces agreement</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear ordinal scale and examples facilitated good alignment between models and humans</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>40</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Human-Human alpha = 0.52 (mean and median 0.52 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Human-model agreement (0.49) closely approximates human-human agreement (0.52), suggesting LLMs can meaningfully replace some human ratings for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Few-shot prompting (3–4 examples) used; no model fine-tuning on labels.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (top models) nearly match human inter-rater agreement on name-value inconsistency judgments, enabling potential replacement of one human rater without degrading agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small dataset size (40) increases uncertainty; human expertise details not fully described; results rely on few-shot examples selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1849.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1849.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causality-LLM-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causality detection in requirements: LLM binary judgments vs. human raters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs were compared to humans on binary causality detection in natural-language requirement sentences (CiRA dataset subset); observed substantially lower human-model agreement and low model-model agreement, indicating poor alignment for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge using binary classification via few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (and others in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Few-shot prompting with binary labels (presence or absence of causal relation)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>natural language requirements sentences</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>requirements engineering text / NL sentences</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>presence of causal relation (binary: 1 present, 0 absent)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Original CiRA dataset annotated by 6 raters; paper selected 1,000 samples where at least two raters assessed each sentence</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators in original CiRA study (paper does not elaborate expertise level here)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Human-Human mean alpha: 0.44; Human-Model mean alpha: 0.22; Model-Model mean alpha: 0.39</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Higher agreement when model-model agreement is higher (not the case here); high-confidence model outputs (top-probability samples) maintain better alignment for a subset (~first 50-60% high-confidence samples)</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Task requires nuanced causal reasoning across sentences and domain knowledge; models struggled, leading to low human-model and model-model agreement</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Complex or implicit causal relations reduce alignment drastically; models perform worse on nuanced causality than on more objective tasks</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Binary criterion is clear, but inherent task difficulty (subjectivity in causal inference) leads to poor model alignment despite clear labels</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>1000</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Human-Human alpha = 0.44 (mean)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Human-model agreement (0.22) is much lower than human-human (0.44), indicating LLMs are currently a poor substitute for humans on causality detection.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Few-shot prompting used; no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs struggle on causality detection: low model-model agreement and low human-model alignment; however, selecting high-confidence outputs preserves agreement for a fraction of samples (approx. 50-60%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Unequal annotation distribution in original dataset (not all humans labeled same samples) complicates pairwise agreement calculations; models may lack deep causal reasoning and domain context; model training data overlap not fully excluded but unlikely to explain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1849.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1849.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemSim-LLM-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional semantic similarity LLM evaluation vs. human raters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs were asked to judge functional similarity (goals, operations, effects) between pairs of functions (Java methods with JavaDoc), yielding high agreement with humans across all three criteria (Krippendorff's alpha values high for human-human, human-model, and model-model).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge using categorical similarity judgments via few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (top-3 reported) plus other models in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Few-shot prompting; three annotation tasks per pair (goals, operations, effects) with Likert-style or categorical choices matching original study</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>pairs of functions / code examples (semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Java methods and JavaDoc comments (SESAME dataset originally mined from Java repositories)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>goals similarity, operations similarity, effects similarity</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Each sample rated by three different people selected from a pool of eight raters</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators from original semantic-similarity study (paper does not further specify seniority)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Human-Human mean alpha: goals 0.83, operations 0.74, effects 0.71; Human-Model mean alpha: goals 0.77, operations 0.67, effects 0.64; Model-Model mean alpha: goals 0.82, operations 0.77, effects 0.69</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Well-defined similarity categories and availability of both function bodies (good context) led to high alignment; top LLMs performed strongly; model-model agreement high and predictive of human-model alignment</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>N/A for main findings—this task exhibited uniformly high agreement across pairings</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>When both function bodies available and the similarity concept is concrete, agreement is high; very different implementations but clear semantic overlap still yielded good alignment</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Multiple, specific similarity criteria (goals/operations/effects) improved clarity and alignment between humans and models</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>786</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Human-Human alphas: goals 0.83, operations 0.74, effects 0.71 (means)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Human-model agreement (0.64–0.77) closely approaches human-human agreement (0.71–0.83), indicating LLMs can substitute for some human judgments in semantic similarity tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Few-shot prompting used; no model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic similarity yields some of the highest human-model alignment: top LLMs approach human inter-rater agreement across goals/operations/effects; model-model agreement is a good predictor of human-model agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Original dataset lacked some function bodies requiring retrieval; human expertise details not fully described; results may depend on sample retrieval/preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1849.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1849.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StaticWarn-LLM-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static analysis warning triage: LLM judgments vs. two human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs judged whether code changes closed static analysis warnings (labels: open/closed/unknown) and were compared to two human annotators; LLM-human and model-model agreement were very low despite high inter-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge with categorical (open/closed/unknown) labels via long, metadata-rich few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (top-3), GPT-3.5, Llama3, Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Few-shot prompting; prompts included diffs and metadata; models asked to pick one of three labels (open/closed/unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>code diffs / static analysis warnings (warning triage)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>software repository diffs from multiple projects (dataset by Kang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>whether a code change effectively addressed (closed) a static analysis warning, or left it open/unknown</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Two human annotators labeled each sample independently</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>annotators from original static analysis study (paper does not specify further expertise here)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Human-Human mean alpha: 0.80; Human-Model mean alpha: 0.15; Model-Model mean alpha: 0.12</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Selecting high-confidence model outputs preserved agreement for a subset (~first 50% of high-confidence samples); however, overall model-model agreement was too low to safely replace humans routinely</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Tasks requiring reasoning over diffs, repository history, or implicit developer intentions led to low human-model agreement; low model-model agreement (<0.2) is indicative of poor suitability</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Complex change histories, deleted/renamed files, or ambiguous diffs decreased model alignment severely; human reasoning over repository context outperformed LLMs here</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Labels are clear, but mapping from diff to label often requires repository context beyond a single diff; clarity of criteria alone did not salvage model performance</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>200</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Human-Human alpha = 0.80 (strong agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Human-model agreement (0.15) and model-model agreement (0.12) are far lower than human-human agreement (0.80), indicating LLMs cannot safely substitute for human triage in this task in its current form.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Few-shot prompting; long prompts including diffs and metadata; no fine-tuning. Due to rate limits, experiments used 200 randomly chosen samples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Static-analysis warning triage is a challenging task for LLMs: despite clear labels and high human-human agreement, LLMs show very low alignment. High-confidence subset selection can preserve agreement for a portion of samples but not suffice to replace humans broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Potential partial overlap with model training data not ruled out; model context/window and repository history limitations; small experimental subsample (200) due to rate/cost constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1849.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1849.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModelModel-Agreement-Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-model agreement as a predictor of human-model alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper proposes and empirically validates that automated, cheap model-model inter-rater agreement correlates with human-model agreement (Spearman rho=0.65, p<0.05), and suggests using a model-model agreement threshold (>0.5 in their experiments) to decide when to substitute human raters with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>model-model agreement (automated inter-model Krippendorff's alpha) as a proxy to predict LLM suitability for replacing human raters</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>same ensemble of LLMs used in experiments (GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro primarily)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Compute inter-model Krippendorff's alpha across models using same few-shot prompts used for humans</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>applies across artifact types evaluated in the paper (summaries, name-value, causality, semantic similarity, static warnings)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>various (source code, JavaDoc, requirement text, diffs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>task-dependent (see per-task entries); predictor evaluates whether models agree among themselves</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha for inter-model agreement; Spearman rank correlation used to assess correlation with human-model agreement</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Spearman correlation between mean model-model agreement and mean human-model agreement: rho=0.65 (p<0.05). Empirically observed model-model alphas: 0.66–0.83 in high-alignment tasks; 0.12–0.39 in low-alignment tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>When multiple strong LLMs independently produce similar judgments (model-model alpha > ~0.5), human-model agreement tends to be high and LLMs are suitable for replacing at least one human rater.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Low model-model agreement (<0.5) predicts poor human-model alignment (e.g., causality and static analysis warnings); outliers may occur when human-human agreement itself is very low.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Model-model agreement tends to be lower on tasks requiring deep contextual or multi-step reasoning (complex artifacts), matching lower human-model alignment</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear criteria increase model-model agreement; when human-human agreement is low due to ambiguous criteria, model-model agreement may be high but task remains problematic (noted as an outlier for conciseness)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Model-model agreement is a cheap automated predictor that correlates positively with human-model agreement and can be used to decide usability of LLMs for a task.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No model fine-tuning; predictor relies on running multiple pre-existing LLMs with same few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model-model alpha can predict whether LLM outputs will align with humans; a practical workflow is to compute model-model agreement first and, if >0.5, safely replace one human rater per sample, otherwise use confidence-based selective replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Threshold (0.5) is empirical and may not generalize; requires access to multiple strong LLMs; cases exist where model-model agreement is high but human-human agreement is low, complicating the decision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1849.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1849.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConfidenceSelection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model output-probability (confidence) selection to choose samples for LLM replacement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLM output probabilities as a confidence proxy to select samples where replacing a human rater with the LLM preserves inter-rater agreement; shown to allow replacing up to 50-100% of one rater depending on task while keeping agreement statistically indistinguishable from human-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Confidence-based sample selection (select top-k samples by LLM output probability) combined with LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (confidence/probability available and used), other models evaluated but confidence analysis reported for GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Few-shot prompting; samples ranked by GPT-4 output probability; replace one human rating in selected fraction with model output and recompute Krippendorff's alpha</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>applies across artifact types evaluated (code summaries, causality, name-value, semantic similarity, static warnings)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>various (functions, natural language requirements, code diffs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>task-dependent (e.g., similarity, accuracy, binary causality, open/closed/unknown labels)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Krippendorff's alpha (computed after replacing a fraction of human ratings with model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Examples: For code summarization accuracy, replacing up to 100% of one rater with GPT-4 (confidence-selected) produced no significant change in inter-rater agreement; for similarity, GPT-4 maintained agreement up to 50% replacement; for causality/static warnings, high-confidence selection preserved agreement up to ~50-60% only for top-confidence samples.</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>High model confidence scores correlate with agreement with majority human ratings; selecting top-confidence samples preserves overall inter-rater agreement and allows substantial human-effort savings</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>When model confidence distribution drops quickly or task requires external context, confidence-based selection covers fewer safe samples; confidence can be overconfident on some failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>For complex artifacts, fewer high-confidence samples exist, limiting safe replacement fraction</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clear, well-specified labeling tasks lead to more calibrated confidence and more samples eligible for replacement</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Confidence-based selection improved proxy-vs-human alignment compared to random replacement; selecting samples by high LLM output probability outperformed random replacement and allowed replacing one human rater without statistically significant change in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>No explicit calibration other than using model output probabilities; few-shot prompts used to steer outputs</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model output probability is an effective heuristic to select sample subsets where LLM substitutions are safe; this enables substantial human-effort reductions (reported % effort savings per task in paper) while preserving inter-rater agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Model output probability is an imperfect proxy for correctness; can be dataset- and model-dependent; cutoff threshold selection remains empirical and task-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Replace Manual Annotation of Software Engineering Artifacts?', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Semantic similarity metrics for evaluating source code summarization <em>(Rating: 2)</em></li>
                <li>Nalin: learning from runtime behavior to find name-value inconsistencies in jupyter notebooks <em>(Rating: 2)</em></li>
                <li>Automatic detection of causality in requirement artifacts: the cira approach <em>(Rating: 2)</em></li>
                <li>Sesame: A data set of semantically similar java methods <em>(Rating: 2)</em></li>
                <li>Detecting false alarms from automatic static analysis tools: How far are we? <em>(Rating: 2)</em></li>
                <li>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks <em>(Rating: 1)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1849",
    "paper_id": "paper-e2b5f5c3205dfcd255a311821e4241d293e929c8",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "CodeSumm-LLM-vs-Human",
            "name_full": "Code summarization LLM-as-judge vs. human raters",
            "brief_description": "Comparison of multiple LLMs (GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, GPT-3.5, Llama3, Mixtral) with professional human raters on Likert-style evaluation of code summaries (accuracy, adequacy, conciseness, similarity), reporting Krippendorff's alpha inter-rater agreements and effects of selecting LLM outputs by confidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (Likert-style ratings via few-shot prompts)",
            "llm_judge_model": "GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, GPT-3.5, Llama3(70B), Mixtral(8x22B)",
            "llm_judge_prompt_approach": "Few-shot prompting (3 examples) asking models to select one of four Likert options ('Strongly agree','Agree','Disagree','Strongly disagree') per criterion; single-rating with rubric-like instructions",
            "artifact_type": "code documentation / code summaries",
            "artifact_domain": "function-level source code (LeClair dataset; language: Java functions)",
            "evaluation_criteria": "accuracy, adequacy, conciseness, similarity",
            "human_evaluation_setup": "professional programmers recruited via Upwork; each sample rated by at least three independent human raters",
            "human_expert_count": 6,
            "human_expert_expertise": "experienced programmers (paid professionals via Upwork at USD60/hr)",
            "agreement_metric": "Krippendorff's alpha",
            "agreement_score": "Human-Human mean alpha: accuracy 0.38, adequacy 0.40, conciseness 0.24, similarity 0.64; Human-Model mean alpha: accuracy 0.48, adequacy 0.41, conciseness 0.21, similarity 0.44; Model-Model mean alpha: accuracy 0.76, adequacy 0.74, conciseness 0.74, similarity 0.68",
            "high_agreement_conditions": "Tasks/criteria with clearer, more objective mapping to text (e.g., similarity) and high model-model agreement; use of strong LLMs (top-3 models) and few-shot prompting improved alignment; selecting samples by model output probability (high-confidence) maintained agreement up to substantial fractions",
            "low_agreement_conditions": "Criteria with inherent subjectivity or low human-human agreement (e.g., conciseness) show weaker or inconsistent human-model alignment; smaller models exhibited bias toward extreme choices",
            "artifact_complexity_effect": "Some harder samples (ambiguous or context-dependent summaries) show lower agreement; overall, models tended to reflect majority training signal and may disagree on nuanced cases where humans also disagree",
            "criteria_clarity_effect": "Clear, well-specified Likert prompts and few-shot examples increased model alignment; where human-human agreement was low, model-human agreement was also unstable",
            "sample_size": 420,
            "inter_human_agreement": "See Human-Human alpha above (per-criterion means and medians reported in paper/table)",
            "proxy_vs_human_comparison": "Human-model agreement is in the same order as human-human for many criteria (e.g., name-value and some summarization criteria); for some criteria (similarity) human-human &gt; human-model, while for accuracy human-model exceeded mean human-human in reported mean values (accuracy human-model 0.48 vs human-human 0.38).",
            "calibration_or_training": "No fine-tuning; used few-shot prompting with 3-4 illustrative examples per prompt (manual selection of shots).",
            "key_findings": "Top LLMs often reach human-level agreement for many code-summary criteria; model-model agreement is high and models tend to reflect majority opinions; selecting model outputs by high output-probability preserves inter-rater agreement and allows replacing one human rater for many samples without statistically significant change.",
            "limitations_noted": "Human-model agreement varies by criterion; replacing many humans inflates agreement (models agree with each other more than humans); some human raters and samples not uniformly overlapping; potential data overlap with LLM training corpora; few-shot examples selection can affect outcomes.",
            "uuid": "e1849.0",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "NameValue-LLM-vs-Human",
            "name_full": "Name-value inconsistency detection: LLM Likert ratings vs. multiple human raters",
            "brief_description": "Comparison of LLMs with 11 human raters on assessing name-value consistency (Likert 1–5) in code examples (Nalin dataset), using Krippendorff's alpha to quantify agreement; found human-model agreement similar to human-human agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge using Likert-style ordinal ratings (1–5) via few-shot prompts",
            "llm_judge_model": "GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (top-3 reported), plus GPT-3.5, Llama3, Mixtral in experiments",
            "llm_judge_prompt_approach": "Few-shot prompting with ordinal Likert instruction (1 to 5 scale) and illustrative examples",
            "artifact_type": "source code variable assignments / name-value examples",
            "artifact_domain": "Jupyter notebooks / general code snippets (Nalin dataset)",
            "evaluation_criteria": "ease of understanding / name-value consistency (Likert 1-5 where 1 indicates difficult/inconsistent, 5 indicates easy/consistent)",
            "human_evaluation_setup": "11 human participants rated all 40 samples independently",
            "human_expert_count": 11,
            "human_expert_expertise": "participants as in original Nalin study (paper does not further specify professional seniority in this paper)",
            "agreement_metric": "Krippendorff's alpha",
            "agreement_score": "Human-Human mean alpha: 0.52; Human-Model mean alpha: 0.49; Model-Model mean alpha (top-3): 0.66",
            "high_agreement_conditions": "Task is relatively objective and constrained (clear mapping of name to type/value); high model-model agreement and strong LLMs increase alignment; few-shot examples help",
            "low_agreement_conditions": "Fewer samples lead to wider confidence intervals; highly ambiguous naming cases still cause disagreements",
            "artifact_complexity_effect": "Simpler, direct name-value mismatches show high agreement; more ambiguous or domain-specific naming reduces agreement",
            "criteria_clarity_effect": "Clear ordinal scale and examples facilitated good alignment between models and humans",
            "sample_size": 40,
            "inter_human_agreement": "Human-Human alpha = 0.52 (mean and median 0.52 reported)",
            "proxy_vs_human_comparison": "Human-model agreement (0.49) closely approximates human-human agreement (0.52), suggesting LLMs can meaningfully replace some human ratings for this task.",
            "calibration_or_training": "Few-shot prompting (3–4 examples) used; no model fine-tuning on labels.",
            "key_findings": "LLMs (top models) nearly match human inter-rater agreement on name-value inconsistency judgments, enabling potential replacement of one human rater without degrading agreement.",
            "limitations_noted": "Small dataset size (40) increases uncertainty; human expertise details not fully described; results rely on few-shot examples selection.",
            "uuid": "e1849.1",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Causality-LLM-vs-Human",
            "name_full": "Causality detection in requirements: LLM binary judgments vs. human raters",
            "brief_description": "LLMs were compared to humans on binary causality detection in natural-language requirement sentences (CiRA dataset subset); observed substantially lower human-model agreement and low model-model agreement, indicating poor alignment for this task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge using binary classification via few-shot prompts",
            "llm_judge_model": "GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (and others in experiments)",
            "llm_judge_prompt_approach": "Few-shot prompting with binary labels (presence or absence of causal relation)",
            "artifact_type": "natural language requirements sentences",
            "artifact_domain": "requirements engineering text / NL sentences",
            "evaluation_criteria": "presence of causal relation (binary: 1 present, 0 absent)",
            "human_evaluation_setup": "Original CiRA dataset annotated by 6 raters; paper selected 1,000 samples where at least two raters assessed each sentence",
            "human_expert_count": 6,
            "human_expert_expertise": "annotators in original CiRA study (paper does not elaborate expertise level here)",
            "agreement_metric": "Krippendorff's alpha",
            "agreement_score": "Human-Human mean alpha: 0.44; Human-Model mean alpha: 0.22; Model-Model mean alpha: 0.39",
            "high_agreement_conditions": "Higher agreement when model-model agreement is higher (not the case here); high-confidence model outputs (top-probability samples) maintain better alignment for a subset (~first 50-60% high-confidence samples)",
            "low_agreement_conditions": "Task requires nuanced causal reasoning across sentences and domain knowledge; models struggled, leading to low human-model and model-model agreement",
            "artifact_complexity_effect": "Complex or implicit causal relations reduce alignment drastically; models perform worse on nuanced causality than on more objective tasks",
            "criteria_clarity_effect": "Binary criterion is clear, but inherent task difficulty (subjectivity in causal inference) leads to poor model alignment despite clear labels",
            "sample_size": 1000,
            "inter_human_agreement": "Human-Human alpha = 0.44 (mean)",
            "proxy_vs_human_comparison": "Human-model agreement (0.22) is much lower than human-human (0.44), indicating LLMs are currently a poor substitute for humans on causality detection.",
            "calibration_or_training": "Few-shot prompting used; no fine-tuning reported.",
            "key_findings": "LLMs struggle on causality detection: low model-model agreement and low human-model alignment; however, selecting high-confidence outputs preserves agreement for a fraction of samples (approx. 50-60%).",
            "limitations_noted": "Unequal annotation distribution in original dataset (not all humans labeled same samples) complicates pairwise agreement calculations; models may lack deep causal reasoning and domain context; model training data overlap not fully excluded but unlikely to explain performance.",
            "uuid": "e1849.2",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "SemSim-LLM-vs-Human",
            "name_full": "Functional semantic similarity LLM evaluation vs. human raters",
            "brief_description": "LLMs were asked to judge functional similarity (goals, operations, effects) between pairs of functions (Java methods with JavaDoc), yielding high agreement with humans across all three criteria (Krippendorff's alpha values high for human-human, human-model, and model-model).",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge using categorical similarity judgments via few-shot prompts",
            "llm_judge_model": "GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (top-3 reported) plus other models in experiments",
            "llm_judge_prompt_approach": "Few-shot prompting; three annotation tasks per pair (goals, operations, effects) with Likert-style or categorical choices matching original study",
            "artifact_type": "pairs of functions / code examples (semantic similarity)",
            "artifact_domain": "Java methods and JavaDoc comments (SESAME dataset originally mined from Java repositories)",
            "evaluation_criteria": "goals similarity, operations similarity, effects similarity",
            "human_evaluation_setup": "Each sample rated by three different people selected from a pool of eight raters",
            "human_expert_count": 3,
            "human_expert_expertise": "annotators from original semantic-similarity study (paper does not further specify seniority)",
            "agreement_metric": "Krippendorff's alpha",
            "agreement_score": "Human-Human mean alpha: goals 0.83, operations 0.74, effects 0.71; Human-Model mean alpha: goals 0.77, operations 0.67, effects 0.64; Model-Model mean alpha: goals 0.82, operations 0.77, effects 0.69",
            "high_agreement_conditions": "Well-defined similarity categories and availability of both function bodies (good context) led to high alignment; top LLMs performed strongly; model-model agreement high and predictive of human-model alignment",
            "low_agreement_conditions": "N/A for main findings—this task exhibited uniformly high agreement across pairings",
            "artifact_complexity_effect": "When both function bodies available and the similarity concept is concrete, agreement is high; very different implementations but clear semantic overlap still yielded good alignment",
            "criteria_clarity_effect": "Multiple, specific similarity criteria (goals/operations/effects) improved clarity and alignment between humans and models",
            "sample_size": 786,
            "inter_human_agreement": "Human-Human alphas: goals 0.83, operations 0.74, effects 0.71 (means)",
            "proxy_vs_human_comparison": "Human-model agreement (0.64–0.77) closely approaches human-human agreement (0.71–0.83), indicating LLMs can substitute for some human judgments in semantic similarity tasks.",
            "calibration_or_training": "Few-shot prompting used; no model fine-tuning.",
            "key_findings": "Semantic similarity yields some of the highest human-model alignment: top LLMs approach human inter-rater agreement across goals/operations/effects; model-model agreement is a good predictor of human-model agreement.",
            "limitations_noted": "Original dataset lacked some function bodies requiring retrieval; human expertise details not fully described; results may depend on sample retrieval/preprocessing.",
            "uuid": "e1849.3",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "StaticWarn-LLM-vs-Human",
            "name_full": "Static analysis warning triage: LLM judgments vs. two human annotators",
            "brief_description": "LLMs judged whether code changes closed static analysis warnings (labels: open/closed/unknown) and were compared to two human annotators; LLM-human and model-model agreement were very low despite high inter-human agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge with categorical (open/closed/unknown) labels via long, metadata-rich few-shot prompts",
            "llm_judge_model": "GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro (top-3), GPT-3.5, Llama3, Mixtral",
            "llm_judge_prompt_approach": "Few-shot prompting; prompts included diffs and metadata; models asked to pick one of three labels (open/closed/unknown)",
            "artifact_type": "code diffs / static analysis warnings (warning triage)",
            "artifact_domain": "software repository diffs from multiple projects (dataset by Kang et al.)",
            "evaluation_criteria": "whether a code change effectively addressed (closed) a static analysis warning, or left it open/unknown",
            "human_evaluation_setup": "Two human annotators labeled each sample independently",
            "human_expert_count": 2,
            "human_expert_expertise": "annotators from original static analysis study (paper does not specify further expertise here)",
            "agreement_metric": "Krippendorff's alpha",
            "agreement_score": "Human-Human mean alpha: 0.80; Human-Model mean alpha: 0.15; Model-Model mean alpha: 0.12",
            "high_agreement_conditions": "Selecting high-confidence model outputs preserved agreement for a subset (~first 50% of high-confidence samples); however, overall model-model agreement was too low to safely replace humans routinely",
            "low_agreement_conditions": "Tasks requiring reasoning over diffs, repository history, or implicit developer intentions led to low human-model agreement; low model-model agreement (&lt;0.2) is indicative of poor suitability",
            "artifact_complexity_effect": "Complex change histories, deleted/renamed files, or ambiguous diffs decreased model alignment severely; human reasoning over repository context outperformed LLMs here",
            "criteria_clarity_effect": "Labels are clear, but mapping from diff to label often requires repository context beyond a single diff; clarity of criteria alone did not salvage model performance",
            "sample_size": 200,
            "inter_human_agreement": "Human-Human alpha = 0.80 (strong agreement)",
            "proxy_vs_human_comparison": "Human-model agreement (0.15) and model-model agreement (0.12) are far lower than human-human agreement (0.80), indicating LLMs cannot safely substitute for human triage in this task in its current form.",
            "calibration_or_training": "Few-shot prompting; long prompts including diffs and metadata; no fine-tuning. Due to rate limits, experiments used 200 randomly chosen samples.",
            "key_findings": "Static-analysis warning triage is a challenging task for LLMs: despite clear labels and high human-human agreement, LLMs show very low alignment. High-confidence subset selection can preserve agreement for a portion of samples but not suffice to replace humans broadly.",
            "limitations_noted": "Potential partial overlap with model training data not ruled out; model context/window and repository history limitations; small experimental subsample (200) due to rate/cost constraints.",
            "uuid": "e1849.4",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ModelModel-Agreement-Predictor",
            "name_full": "Model-model agreement as a predictor of human-model alignment",
            "brief_description": "The paper proposes and empirically validates that automated, cheap model-model inter-rater agreement correlates with human-model agreement (Spearman rho=0.65, p&lt;0.05), and suggests using a model-model agreement threshold (&gt;0.5 in their experiments) to decide when to substitute human raters with LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "model-model agreement (automated inter-model Krippendorff's alpha) as a proxy to predict LLM suitability for replacing human raters",
            "llm_judge_model": "same ensemble of LLMs used in experiments (GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro primarily)",
            "llm_judge_prompt_approach": "Compute inter-model Krippendorff's alpha across models using same few-shot prompts used for humans",
            "artifact_type": "applies across artifact types evaluated in the paper (summaries, name-value, causality, semantic similarity, static warnings)",
            "artifact_domain": "various (source code, JavaDoc, requirement text, diffs)",
            "evaluation_criteria": "task-dependent (see per-task entries); predictor evaluates whether models agree among themselves",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "Krippendorff's alpha for inter-model agreement; Spearman rank correlation used to assess correlation with human-model agreement",
            "agreement_score": "Spearman correlation between mean model-model agreement and mean human-model agreement: rho=0.65 (p&lt;0.05). Empirically observed model-model alphas: 0.66–0.83 in high-alignment tasks; 0.12–0.39 in low-alignment tasks.",
            "high_agreement_conditions": "When multiple strong LLMs independently produce similar judgments (model-model alpha &gt; ~0.5), human-model agreement tends to be high and LLMs are suitable for replacing at least one human rater.",
            "low_agreement_conditions": "Low model-model agreement (&lt;0.5) predicts poor human-model alignment (e.g., causality and static analysis warnings); outliers may occur when human-human agreement itself is very low.",
            "artifact_complexity_effect": "Model-model agreement tends to be lower on tasks requiring deep contextual or multi-step reasoning (complex artifacts), matching lower human-model alignment",
            "criteria_clarity_effect": "Clear criteria increase model-model agreement; when human-human agreement is low due to ambiguous criteria, model-model agreement may be high but task remains problematic (noted as an outlier for conciseness)",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Model-model agreement is a cheap automated predictor that correlates positively with human-model agreement and can be used to decide usability of LLMs for a task.",
            "calibration_or_training": "No model fine-tuning; predictor relies on running multiple pre-existing LLMs with same few-shot prompts",
            "key_findings": "Model-model alpha can predict whether LLM outputs will align with humans; a practical workflow is to compute model-model agreement first and, if &gt;0.5, safely replace one human rater per sample, otherwise use confidence-based selective replacement.",
            "limitations_noted": "Threshold (0.5) is empirical and may not generalize; requires access to multiple strong LLMs; cases exist where model-model agreement is high but human-human agreement is low, complicating the decision.",
            "uuid": "e1849.5",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ConfidenceSelection",
            "name_full": "Model output-probability (confidence) selection to choose samples for LLM replacement",
            "brief_description": "Using LLM output probabilities as a confidence proxy to select samples where replacing a human rater with the LLM preserves inter-rater agreement; shown to allow replacing up to 50-100% of one rater depending on task while keeping agreement statistically indistinguishable from human-only baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "Confidence-based sample selection (select top-k samples by LLM output probability) combined with LLM-as-a-judge",
            "llm_judge_model": "GPT-4 (confidence/probability available and used), other models evaluated but confidence analysis reported for GPT-4",
            "llm_judge_prompt_approach": "Few-shot prompting; samples ranked by GPT-4 output probability; replace one human rating in selected fraction with model output and recompute Krippendorff's alpha",
            "artifact_type": "applies across artifact types evaluated (code summaries, causality, name-value, semantic similarity, static warnings)",
            "artifact_domain": "various (functions, natural language requirements, code diffs)",
            "evaluation_criteria": "task-dependent (e.g., similarity, accuracy, binary causality, open/closed/unknown labels)",
            "human_evaluation_setup": null,
            "human_expert_count": null,
            "human_expert_expertise": null,
            "agreement_metric": "Krippendorff's alpha (computed after replacing a fraction of human ratings with model outputs)",
            "agreement_score": "Examples: For code summarization accuracy, replacing up to 100% of one rater with GPT-4 (confidence-selected) produced no significant change in inter-rater agreement; for similarity, GPT-4 maintained agreement up to 50% replacement; for causality/static warnings, high-confidence selection preserved agreement up to ~50-60% only for top-confidence samples.",
            "high_agreement_conditions": "High model confidence scores correlate with agreement with majority human ratings; selecting top-confidence samples preserves overall inter-rater agreement and allows substantial human-effort savings",
            "low_agreement_conditions": "When model confidence distribution drops quickly or task requires external context, confidence-based selection covers fewer safe samples; confidence can be overconfident on some failure modes",
            "artifact_complexity_effect": "For complex artifacts, fewer high-confidence samples exist, limiting safe replacement fraction",
            "criteria_clarity_effect": "Clear, well-specified labeling tasks lead to more calibrated confidence and more samples eligible for replacement",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Confidence-based selection improved proxy-vs-human alignment compared to random replacement; selecting samples by high LLM output probability outperformed random replacement and allowed replacing one human rater without statistically significant change in many tasks.",
            "calibration_or_training": "No explicit calibration other than using model output probabilities; few-shot prompts used to steer outputs",
            "key_findings": "Model output probability is an effective heuristic to select sample subsets where LLM substitutions are safe; this enables substantial human-effort reductions (reported % effort savings per task in paper) while preserving inter-rater agreement.",
            "limitations_noted": "Model output probability is an imperfect proxy for correctness; can be dataset- and model-dependent; cutoff threshold selection remains empirical and task-specific.",
            "uuid": "e1849.6",
            "source_info": {
                "paper_title": "Can LLMs Replace Manual Annotation of Software Engineering Artifacts?",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Semantic similarity metrics for evaluating source code summarization",
            "rating": 2
        },
        {
            "paper_title": "Nalin: learning from runtime behavior to find name-value inconsistencies in jupyter notebooks",
            "rating": 2
        },
        {
            "paper_title": "Automatic detection of causality in requirement artifacts: the cira approach",
            "rating": 2
        },
        {
            "paper_title": "Sesame: A data set of semantically similar java methods",
            "rating": 2
        },
        {
            "paper_title": "Detecting false alarms from automatic static analysis tools: How far are we?",
            "rating": 2
        },
        {
            "paper_title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks",
            "rating": 1
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 1
        }
    ],
    "cost": 0.01984325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can LLMs Replace Manual Annotation of Software Engineering Artifacts?</h1>
<p>Toufique Ahmed ${ }^{<em> \dagger \S}$, Premkumar Devanbu</em>, Christoph Treude ${ }^{\ddagger}$ Michael Pradel ${ }^{\S}$<br>*University of California, Davis, USA<br>${ }^{\dagger}$ IBM Research, Yorktown Heights, New York, USA<br>${ }^{\ddagger}$ Singapore Management University, Singapore<br>${ }^{\S}$ University of Stuttgart, Germany</p>
<h4>Abstract</h4>
<p>Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and coderelated artifacts. We study this idea by applying six state-of-theart LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering.</p>
<p>Index Terms-LLMs, human subjects, evaluation</p>
<h2>I. INTRODUCTION</h2>
<p>Given that developer effectiveness heavily depends on good tools and processes, researchers constantly seek more and better automation in these areas. To mention just a few examples, there are now many research efforts [1], [2] aimed, e.g., at automated code summarization, (i.e., techniques that generate a natural language summary for a given piece of code), at detecting bugs and other issues in a program [3], and at determining whether a warning produced by a static analysis tool is actually worth addressing [4].</p>
<p>But the value of such innovations ultimately is dependent on human judgment and practice. For example, a developer might read a generated English summary of a method, and then decide whether and how to use that method. As another example, a developer might look at a warning produced by a static analysis tool, and then decide whether to fix it or not. Because measuring human usefulness is difficult, many research efforts rely on proxy metrics. For the example of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>code summarization, a summary might rate a reasonable BLEU score, indicating at least some level of similarity to the original, human-written summary. However, even given a summary with a high BLEU score, can we be sure that it is clear, precise, relevant to the input code, and humancomprehensible?</p>
<p>To answer such questions, researchers often use human subject-based evaluations, e.g., when evaluating code summarization techniques [5], [6]. Besides code summarization, such evaluations are becoming increasingly common in many different settings; indeed in most cases where a tool is used to automate some aspect of software development, the output from the tool will be used by a human. Since the content and form of the output may affect how well a developer can use it, a human-subject evaluation is often vital.</p>
<p>But human-subject studies in software engineering are costly. For external validity, such studies demand both a representative sample of developers and a representative sample of relevant artifact samples. Each sample may need to be evaluated by multiple humans, to get more stable results. For example, Haque et al. [5], hired professional developers, at $\$ 60$ per hour, to rate a total of 420 human and machine-generated code summaries, with three ratings for each sample to gauge inter-rater agreements. Due to the high costs, such evaluations are sometimes done using free or low-cost participants, such as students, which carries risks of the results not generalizing to professional developers. Yet, even performing a human-subject study with students can be quite time-consuming. Thus, the main motivation for researching new tools and techniques in software engineering, viz. the high cost of developers, is also a hurdle to the proper evaluation of such tools and techniques.</p>
<p>Given the ability of advanced large language models (LLMs), to rival human performance in a range of tasks [7][9], the question naturally arises: Can we use LLMs to reduce the cost of human-subject studies in software engineering? If LLMs can help, even if only partially, this could impact the practice of evaluation studies in software engineering. Thus, we, ask: When, and how, can human subject responses be safely replaced by LLMs, in a mixed human-LLM evaluation scenario?</p>
<p>This question has arisen in other disciplines, e.g., psychology, linguistics, and medicine [10]-[13]. There are several reasons why the software engineering domain is particularly</p>
<div class="codehilite"><pre><span></span><code><span class="n">public</span><span class="w"> </span><span class="n">boolean</span><span class="w"> </span><span class="n">pluginRegistryContains</span><span class="p">(</span><span class="n">String</span><span class="w"> </span><span class="n">pluginName</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">synchronized</span><span class="w"> </span><span class="p">(</span><span class="n">registry</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Iterator</span><span class="w"> </span><span class="n">iter</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">registry</span><span class="p">.</span><span class="n">iterator</span><span class="p">();</span><span class="w"> </span><span class="n">iter</span><span class="p">.</span><span class="n">hadNext</span><span class="p">();)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">PluginRegistryNode</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">PluginRegistryNode</span><span class="p">)</span><span class="n">iter</span><span class="p">.</span><span class="n">next</span><span class="p">();</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">plugin</span><span class="p">.</span><span class="n">getName</span><span class="p">().</span><span class="n">equals</span><span class="p">(</span><span class="n">pluginName</span><span class="p">))</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">true</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">false</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">#</span><span class="nl">Reference:</span><span class="w"> </span><span class="n">tests</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">plugin</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">registry</span>
<span class="p">#</span><span class="n">Model</span><span class="w"> </span><span class="nl">generated:</span><span class="w"> </span><span class="n">checks</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">plugin</span><span class="w"> </span><span class="n">contains</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">plugin</span><span class="w"> </span><span class="n">name</span>
</code></pre></div>

<p>(a) Sample where human raters agreed
public void fileInfoGenerated(FileInfoEvent e) {
FileInfoContainer fileInfo = e.getInfoContainer();
if (fileInfo.get(Class(), equals(StatusInformation.class)) {
System.err.println("A file status event was received.");
System.err.println("The status information object is: " + fileInfo);
}
}
}
#Reference: a file info generated for the file
#Model Generated: called when file status information has been received
(b) A sample where humans disagreed</p>
<p>Fig. 1: Annotation tasks with different difficulty level.
interesting for this exploration: First, relative cost: LLMs can run hundreds of queries for the cost of using a single human subject for an hour. Second, LLMs now perform well on a wide range of software engineering tasks, and are routinely used in industry [7], [14]-[16]. Third, the impressive capabilities of LLMs at in-context learning [17], [18], suggest that with a few illustrative examples, LLMs can execute fairly complex software engineering tasks. Finally, software artifacts are arguably more complex than natural language artifacts: they can require deep knowledge of both application domain and programming; also, they comprise multiple formal \&amp; informal elements (e.g, code, summary, warnings).</p>
<p>This paper studies the possibility of substituting human subjects with LLMs when annotating software engineering artifacts. We study ten tasks from five datasets created by prior work, covering research ranging from requirements engineering to reasoning about static analysis warnings. We apply six state-of-the-art LLMs, including both open- and closed-source models, to these tasks, and then compare the LLMs' responses to those from human subjects.</p>
<p>We focus on tasks whose subjective, nuanced and contextdependent nature have traditionally required human judgment. This subjectivity is evident in the fact prior studies reveal considerable inter-rater disagreement. Figure 1 shows two examples from code summarization, where human subjects are asked to rate the accuracy of generated summaries. In Figure 1a, all the three raters and the GPT-4 model rated "Strongly agree". However, other examples are inherently hard for humans, such as Figure 1b, where the three raters give different ratings: "Agree", "Strongly Disagree", and "Disagree", and GPT-4 agrees with the first rater. Our study asks: Can powerful LLMs (partially) substitute for humans on inherently subjective annotation tasks?. Our findings are:</p>
<ul>
<li>On some tasks, LLMs agree with human subjects about as much as human subjects agree with each other. For
example, when judging whether an identifier name and the value it refers to are consistent [19], we observe a mean human-human inter-rater agreement of 0.52 , and a mean human-model agreement of 0.49 . This results suggests that LLMs can sometimes be a meaningful replacement for humans on some annotation tasks. But when, exactly?</li>
<li>To help researchers decide whether an LLM may be suitable for a specific annotation task, we investigate the correlation between model-model agreement, which can be measured programmatically, and the ability of LLMs to meaningfully replace humans. We find model-model agreement strongly correlates with human-model agreement, suggesting that model-model agreement can be used to decide whether to involve LLMs in an evaluation.</li>
<li>Some examples are easier to annotate than others. For a given example, can an LLM substitute for a human? We find that the LLM's confidence (its output probability) helps select examples that do not necessarily require human annotators. For example, for the task of judging whether a code snippet and its summary are similar, delegating $50 \%$ of the effort performed by one annotator to an LLM (selected based on LLM confidence), does not lead to a statistically significant change of the overall inter-rater agreement.</li>
</ul>
<p>In summary, we make the following contributions.</p>
<ul>
<li>We offer a first study as to whether LLMs can replace human annotation on software engineering artifacts.</li>
<li>We present a methodology designed to answer this question and empirical results from applying six LLMs to ten tasks addressed by humans in prior work.</li>
<li>We propose a method for selecting which tasks are suitable for an LLM-augmented evaluation and which samples can be safely delegated to an LLM.</li>
</ul>
<h2>II. BACKGROUND</h2>
<p>In software engineering research, human-subject annotations provide insight into the potential impact of innovations on productivity or quality [20]; but the nuanced, subjective nature of these evaluations often requires multiple human raters to ensure consistency and reliability in the data [21].</p>
<p>Inter-rater reliability provides a vital gauge for the objectivity of the annotations [22]. It is commonly measured using statistical metrics such as Cohen's $\kappa$ [23] or Krippendorff's $\alpha$ [24], which measure agreement among annotators by considering the possibility of pure-chance. Higher values suggest a strong, reliable, and robust agreement among annotators.</p>
<p>Annotations based on a single annotator carry a risk of personal biases and subjective interpretations, and lead to inaccurate, non-generalizable results [25]. In contrast, multiple annotators working independently can reveal an inherently shared understanding, which indicates that the annotations accurately capture the underlying phenomena being studied. However, achieving high inter-rater reliability is effortful, time-consuming, and challenging; human developer time is costly, and they often disagree!</p>
<p>LLMs, since they are trained on very large corpora, could help these challenges by potentially providing more consistent annotations which normatively reflect "most" developers. They could reduce the biases introduced by human annotators, LLM-based automation of annotations could make it feasible to analyze datasets that were previously too large or complex to annotate manually. In addition, software engineering data is dynamic and constantly evolving, requiring tools that can quickly adapt to new information. LLMs, with their ability to learn from and adapt to new data, could provide up-to-date annotations, complementing the work of human annotators.</p>
<p>To effectively incorporate LLMs into qualitative annotation, it is essential to establish a structured process. This includes deciding when and how to use LLMs, integrating them into existing workflows, distributing tasks between humans and LLMs, and determining the degree of manual work required. This paper addresses these considerations through four research questions (See subsection III-C), focusing on annotations with pre-existing categories derived from related work or developed in an initial research phase. Future work will explore the potential of LLMs for annotating qualitative data without pre-existing categories (i.e., open coding) within a software engineering context.</p>
<h2>III. Methodology</h2>
<p>We now describe the tasks, datasets, the models under consideration, our research questions, and the methodology to answer these questions.</p>
<h2>A. Tasks \&amp; Datasets</h2>
<p>We select five datasets from previous work, which together include 10 human-annotation tasks. For some datasets, multiple tasks were assigned to the raters. For example, in the code summarization dataset, raters were given four tasks: rating accuracy, adequacy, similarity, and conciseness. Similarly, for the semantic similarity dataset, raters were assigned three different rating tasks. This is why we have five datasets or major tasks but a total of 10 human-annotation tasks. In general, we ask LLMs to perform annotations previously done by humans; we give the LLMs the same instructions as given to humans. The datasets and tasks are selected to represent a diverse range of software engineering research.
Automatic Code Summarization This is an active area, aiming to generate helpful summaries of code [3]. One survey-based study reports that $80 \%$ of respondents found code comment generation tools useful [26]. 78\% agree that these tools help them understand the source code, especially helping code readability in projects with few comments. Several metrics are used to evaluate code summary quality. All metrics aim to indirectly measure human perception of quality, but have limitations. But what criteria are important to evaluate human perception? Haque et al. [5] considered four criteria: accuracy, adequacy, conciseness, and similarity-that should be considered when evaluating code summary quality.</p>
<p>Haque et al. [5] recruited experienced programmers via Upwork, paying them USD60/hr (the applicable market rate).</p>
<p>The programmers were presented with 210 functions (from LeClair et al [27]) along with associated human-generated, and model-generated summaries ( 420 in total). Subjects responded to four questions, related to each of the criteria above. Each participant rated a subset of the 420 samples ${ }^{1}$, while ensuring that each sample was rated by at least three people.</p>
<p>To elicit LLM responses, we prompt the model with the same guidelines and questions as in the original study. That is, the LLM receives the functions and associated comments, and is tasked to rate its agreement with four statements:</p>
<ul>
<li>Independent of other factors, I feel that the summary is accurate.</li>
<li>The summary contains all the important information, and that can help the understanding of the method.</li>
<li>The summary contains only necessary information. ${ }^{2}$</li>
<li>These two comments are similar.</li>
</ul>
<p>There are four options: 'Strongly agree', 'Agree', 'Disagree', and 'Strongly disagree'. To enhance model performance and maintain a specific output format, we use few-shot learning [17], [18]: we prompt the model with a few illustrative query-response pairs and expect the model to respond to the "test" query of our interest. Prompt-size constraints limited us to three shots for most of the experiments.
Name-Value Inconsistencies Variable names are crucial for code understanding [19]. If a variable name does not match the value stored in it, we have an undesirable "name-value inconsistency". Patra and Pradel [19] user study had 11 participants, and created a dataset evaluating name-value consistency ${ }^{3}$. This dataset includes 40 samples rated by all 11 raters. The raters assigned a Likert score from 1 to 5 , where 1 indicates difficulty in understanding (or the presence of namevalue inconsistency), and 5 indicates ease of understanding. Suppose there is a variable named name to which a float, 2.5 , is assigned. This is confusing for developers, since the expected value is a string. All annotators rated it as difficult to understand. On the contrary, another variable done, assigned the boolean value False, is easy to understand and was rated as easy to understand by most annotators. We challenge the LLM to perform the same tasks, again with a few-shot prompt.
Causality Causal knowledge supports reasoning about requirements dependencies, particularly for tasks such as test-case construction. Causal relations (e.g., 'If event 1, then event 2') are common in system behavior. However, extracting causality from natural language documents is difficult. Fischbach et al. [29] introduced a dataset, built using 6 raters to develop and evaluate a tool for extracting causal dependencies. ${ }^{4}$ The dataset is quite large, with over 10,000 samples. We randomly select 1,000 samples where at least two raters assessed each sentence. The ratings are binary: 1 indicates the presence of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a causal relation, and 0 indicates its absence. We elicit such annotations from LLM using a few-shot prompt.
Semantic Similarity Functions are often re-implemented several times, with similar functionality, in software projects. However, semantically-similar functions may be implemented in different ways. Finding and merging such functions could reduce maintenance costs. Kamp et al. [30] applied text similarity measures to JavaDoc comments mined from 11 open-source repositories and then manually classified a selection of 857 pairs to create a realistic dataset. ${ }^{5}$ There are three annotation tasks related to functional similarity: goals, operations, and effects. Like for code summarization, each sample was rated by three different people from a pool of eight raters. The original dataset includes some metadata and URLs to the functions but does not contain the function bodies. We retrieve the function bodies and found both bodies for 786 pairs, so we conduct our evaluation on these samples. For each of these pairs, we query LLMs to perform the same three annotation tasks as in the original study.
Static Analysis Warnings Automatic static analysis tools, (e.g. FindBugs), can find coding errors, but can raise false alarms. Pruning false alarms and presenting only (or mostly) actionable warnings to developers can be beneficial. A dataset by Kang et al. [4] provides human annotations on the task of determining whether a particular code change effectively addresses a static analysis warning. The dataset has 1,306 samples that cover different warning categories. ${ }^{6}$ Two annotators assigned each sample one of three labels: open, closed, or unknown. A static analysis warning is considered "closed" if it was reported in a previous software revision but not in a subsequent reference revision, indicating that the problematic code was altered or removed. Conversely, a warning is labeled as "open" if it appears in both the current testing revision and a later reference revision, suggesting that the warning was not actionable or it was ignored by the developers. Lastly, a warning is marked as "unknown" if the file containing the warning was deleted or modified in a (possibly unrelated) way in the reference revision, making it difficult to confirm whether the warning was actionable. We use diff to present the changes made to the repository. Given the extensive metadata, the prompts for this problem are the longest. Due to rate limitations imposed by some models (e.g., Claude and Gemini) and the need to manage costs, we use 200 samples, chosen uniformly at random, for our experiments. For each of these samples, we ask LLMs to perform the annotation task.</p>
<p>Detailed prompts (including guidelines and few-shot samples) for each dataset are provided in the supplementary material.</p>
<h2>B. Models under Consideration</h2>
<p>We use both closed and open models. Among closed models, we chose GPT-4, Claude-3.5-Sonnet, and Gemini-1.5-Pro, which are all the best models from their respective families or</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>organizations at the time of the work. We also used the GPT3.5 model. From the open models, we chose Llama3 (70B) and Mixtral (8x22B), which are also the latest models from the open-model family.</p>
<h2>C. Research Questions</h2>
<p>We seek to study whether LLM prompting can substitute for human annotation labour, for artifact annotation tasks in SE. Specifically, we study how annotator (inter-rater) agreement changes when replacing human annotations with automated LLM annotations. In the first research question, we examine all three categories of inter-rater agreement: human-human, human-model, and model-model. Specifically, if human-model inter-rater agreement is similar to that of human-human agreement, it may be possible to substitute (some) human ratings with model ratings. We will also examine the inter-model agreement and how it changes across datasets.</p>
<p>RQ 1. When humans are replaced by LLMs to provide answers for "human-rater" questions in software engineering research, what level of agreement is observed between humans and models?</p>
<p>In Section IV-A we discuss the findings of human-human, human-model and model-model inter-rater agreement. Depending on the dataset and annotation task, we find varying levels of agreement. This raises the question of how one might decide whether a particular task is amenable to replacing human effort with an LLM.</p>
<p>RQ 2. How can we determine if LLMs are NOT usable for a specific task?</p>
<p>In contrast to RQ2, where we consider basic feasibility of replacing human effort with a model, the next question focuses on identifying specific samples where we can replace one human rating with a model rating. Although a model may not perform well for all samples, selecting samples where models are successful can still significantly reduce human effort.</p>
<p>RQ 3. How can we determine if an answer from an LLM for a specific sample is likely to be in agreement with a human answer?</p>
<p>In case we have determined if and when to delegate parts of a human-subject study to LLMs, the final research question is about the cost benefits of doing so.</p>
<p>RQ 4. How much human annotation effort could be saved without sacrificing inter-rater agreement?</p>
<h2>D. Evaluation Methodology</h2>
<p>For RQ1: With each task, we record a) number of annotators and b) samples rated by each annotator. Note that an annotator may not have annotated all the samples in the dataset but rather a subset. ${ }^{7}$ After identifying the samples annotated by each human, we compute the inter-rater agreement metric Krippendorff's $\alpha$ for each human-human, human-model, and model-model pair. To clarify, if a human (P1) has rated 5</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>samples ${\mathrm{s} 1, \mathrm{~s} 2, \mathrm{~s} 3, \mathrm{~s} 4$, and s 5$}$, and another human (P2) has rated 5 samples ${\mathrm{s} 3, \mathrm{~s} 4, \mathrm{~s} 5, \mathrm{~s} 6, \mathrm{~s} 7}$, we consider only ${\mathrm{s} 3$, $\mathrm{s} 4, \mathrm{~s} 5}$ for computing Krippendorff's $\alpha$ for P1 and P2. Other inter-rater agreement metrics exist; but Krippendorff's $\alpha$ is applicable to any number of annotators, each assigning one value to one unit of analysis, to incomplete (missing) data, to any number of values available for annotating a variable, and to different kinds of annotations (binary, nominal, ordinal, interval, ratio, etc.). Given the diversity of the datasets we study, this metric allows us to uniformly apply inter-rater agreement to all datasets.
For RQ2: As mentioned earlier, we compute human-human, human-model, and model-model agreement $\alpha$. Note that model-model agreement is cheap and automated to determine. To answer this question, we examine the model-model agreements and human-model agreement. If these two are correlated, then model-model agreement can help judge the possibility of having high human-model agreement.
For RQ3: To determine whether asking an LLM for a specific sample is a good idea, we use the output probability of a model's answer as a proxy for the model's confidence. We then study the impact of replacing those human answers where an LLM gives the highest-confidence responses with an LLM answer. To assess the impact, we investigate how the interrater agreement changes if we replace one human annotator with an LLM for a specific fraction of all samples.
For RQ4: We measure the human effort one could save without reducing inter-rater agreement in a statistically significant way. Our unit of effort here is the work required, on average, to label one sample. Saving $100 \%$ of the effort for one rating corresponds to getting one annotation on each sample in a dataset from an LLM. Note that this saving typically saves at least one, and sometimes multiple human participants. For example, if a problem requires a total of 300 annotations for 100 samples, then saving $100 \%$ rating effort means to replace 100 annotations (which otherwise would be annotated by one or more humans).</p>
<h2>IV. ReSULtS</h2>
<h2>A. RQ1: Level of Agreement between Humans and Models</h2>
<p>We now present the observed level of agreement between humans and models across different datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: center;">Human to Human</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human to Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model to Model</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Inter-rater agreement</td>
<td style="text-align: center;">Inter-rater agreement</td>
<td style="text-align: center;">Inter-rater agreement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Median</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy (Code Summarization)</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: left;">Adequacy (Code Summarization)</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: left;">Conciseness (Code Summarization)</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Similarity (Code Summarization)</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.68</td>
</tr>
<tr>
<td style="text-align: left;">Name-Value Inconsistencies</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Causality</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">Goals (Semantic Similarity)</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">Operations (Semantic Similarity)</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: left;">Effects (Semantic Similarity)</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: left;">Static Analysis Warning</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.09</td>
</tr>
</tbody>
</table>
<p>TABLE I: Summary of inter-rater agreements. The three best performing models (Claude, Gemini, and GPT-4) have been used to report the agreement in this table.</p>
<p>Code Summarization Figure 2 shows heatmaps of inter-rater agreement over individual ratings on experimental samples, for human-human, human-model, and model-model pairs for code summarization criteria: Accuracy, Adequacy, Conciseness, and Similarity. The original study used six human raters. We add ratings from six different models, along with (for comparison) an automated rater which randomly assigned labels to the samples. Figure 2-(a) shows three zones representing three categories of inter-rater agreement, based on available data. ${ }^{8}$</p>
<p>Model-model agreement is high, for all criteria, especially for the three large models (GPT-4, Gemini, and Claude). Table I indicates that the mean Krippendorff's $\alpha$ is 0.68 0.76. Second, we see that human-model and human-human agreements are in similar ranges, $0.24-0.40$ and $0.21-0.48$ for the first three categories. Because of the way models are trained, we can expect powerful models to tend to reflect the "majority opinion" they learn from the training corpus.</p>
<p>For similarity, human-human agreement is higher than human-model agreement. Although smaller models are also useful, they show some bias towards specific answers. For example, Llama-3 consistently chose between "Strongly agree" and "Strongly disagree", ignoring the options "Agree" and "Disagree". Similarly, human rater "P2" was not particularly satisfied with the conciseness of the program, resulting in lower inter-rater agreement with all humans and models.</p>
<p>To summarize, for all criteria, we observed generally similar human-human and human-model inter-rater agreements, with a few exceptions.
Name-Value Inconsistencies. We find that human-model and human-human agreement are quite similar for this task (mean Krippendorff's $\alpha 0.49$ vs. 0.52 ). Figure 3 shows a heat-map for all 11 humans, 6 models, and a random rater. We have very high values in all three zones: human-human, humanmodel, and model-model. Here, too, we find higher agreement between the top 3 models (mean Krippendorff's $\alpha 0.66$ ). Note that we will consider only the top three models for reporting Krippendorff's $\alpha$ from here on because we found that only these three models do not show bias or preference towards any specific answer.
Causality The models struggle more to detect causality, relative to the two prior tasks (Figure 4). The mean human-model agreement ( 0.22 ) is much less than human-human ( 0.44 ). The mean model-model agreement ( 0.39 ) is less than 0.5 . To summarize, we found lower level of agreement between human-model and model-model for this task.
Semantic Similarity For the functional semantic similarity dataset in all categories (e.g., goals, operations, and effects), we have observed high agreement for all human-human ( 0.71 0.83 ), human-model ( $0.64-0.77$ ), and model-model ( $0.69-0.83$ ) pairs (see Table I and Figure 5). We found the highest level of inter-rater agreement in all three pairings.
Static Analysis Warning For the static analysis warnings rating task, we have only two human raters, and they are in strong</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2: Inter-rater agreement (Krippendorff's $\alpha$ ) for code summarization accuracy and similarity. Results for adequacy and conciseness are similar (omitted due to space).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Inter-rater agreement for name-value inconsistencies.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Inter-rater agreement for causality.
agreement (Krippendorff's $\alpha 0.80$ ). However, human-model (0.15) and model-model (0.12) agreements are low (Figure 6).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: Inter-rater agreement for semantic similarity with evaluation criteria "Goals". Other criteria ("Operations" and "Effects") give similar heatmaps (omitted due to space).</p>
<p>These findings suggest that LLMs cannot safely substitute for human ratings in this task.</p>
<p>We note that human-model agreement varies from task to task. While it resembles human-human agreement values for some tasks (code summarization, name-value inconsistencies, semantic similarity), it is lower for other tasks (causal relation detection and static analysis warnings). For some tasks, all reviewers labeled all the samples (e.g., static analysis warning and name-value inconsistency). For code summarization, the annotation load was equally distributed among the annotators. In code summarization, each pair of humans had 105 overlapping annotations. However, for causality, the load was not equally distributed. In some cases, there are no samples labeled by two individuals, and in those cases, the corresponding cell is empty.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6: Inter-rater agreement for static analysis warnings.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7: Human-model inter-rater agreements are positively correlated with model-model inter-rater agreements (p &lt; 0.05). Human-human agreements are shown in blue text.</p>
<h3><em>B. RQ2: LLMs' Applicability for a Specific Task</em></h3>
<p>We observed that for a majority of tasks, human-model inter-rater agreement is very similar to human-human agreement, while for some tasks, it is lower. What about model-model inter-rater agreement, which does not require any human effort? Figure 7 shows that the mean model-model agreement of the top 3 models is positively correlated with the mean human-model agreement, with a Spearman correlation of 0.65 (p &lt; 0.05). This suggests that higher model-model agreement is helpful in indicating good human-model agreement; this observation is potentially valuable in deciding whether to replace humans with models. There is an outlier (high model-model agreement but low human-model agreement) in the bottom-right corner, but for this case (code summarization conciseness), <em>human-human agreement is also low</em> (0.24) suggesting that human ratings for this task <em>per se</em> are not consistent. Our results suggest that if multiple LLMs reach similar solutions independently, then LLMs are likely suitable for the annotation task.</p>
<h3><em>C. RQ3: LLMs' Applicability for a Specific Sample</em></h3>
<p>If we observe the tasks where the models and humans are reasonably agreeing with each other, the model-model agreements are quite high (0.66-0.82). For two datasets, causality detection and static analysis warning, the inter-model agreements are too low (0.39 and 0.12). Now we will see whether we can replace one human rating in each sample with a model. We will discuss the results with respect to the GPT-4 model output from here on, because this model allows us to observe the output probability and is the best-performing model overall.</p>
<p>Figure 8(a &amp; b) shows how inter-rater agreement changes with increasing fraction of samples from GPT-4 for code summarization accuracy and similarity. For adequacy and conciseness, we have similar plots to accuracy, so we have omitted them from the paper due to page constraints. Consider Figure 8(a), where we gradually replace the human ratings with model ratings. On the x-axis, 10% means we take 10% of the samples and replace one randomly chosen rating with GPT-4 output, then observe the inter-rater agreement (now including 10% model ratings).</p>
<p>We can choose the 10% samples in two ways: based on GPT-4 output probability or randomly. The red lines show the results when we choose the samples based on probability, and the blue line indicate when samples are chosen randomly. After selecting the samples, we have three ratings for each sample. We randomly select a position, replace the rating with GPT-4 output, and calculate Krippendorff's α as in RQ1. We repeat the process 100 times for each selection criterion. For each data point on the x-axis, we present the mean value and 95% confidence interval. For human-human agreement, we present the inter-rater agreement with a dotted line and have drawn the confidence interval using bootstrapping by randomly selecting 50% of the samples and repeating the process 1,000 times.</p>
<p>In general, the probability-based selection criterion works better than randomly selected samples. However, at 100%, there is no difference between them because all the samples have been considered for replacement. For the accuracy task of code summarization (Figure 8), we find that the inter-rater agreement does not change at all, even when we replace one rating in all the samples. It slightly increases with the probability-based selection approach, the improvement is insignificant and within the 95% confidence interval of human-human agreement. For the similarity task (Figure 8-b), we have seen that GPT-4 maintains inter-rater agreement up to 50% of the samples while chosen based on probability, potentially saving 16.5% of overall human effort.</p>
<p>For name-value inconsistencies (Figure 9) and functional similarity (figure omitted for space constraints), we observed that we can replace one rating in all samples with GPT-4 and still maintain the same inter-rater agreement. Note that for name-value inconsistency, we have few samples. Therefore, the confidence interval of human-human agreement is much wider.</p>
<p>For causality and static analysis warning, we have low inter-model agreement (0.39 and 0.12). We have already discussed that we should expect low human-model agreement because they are correlated. Figures 10 &amp; 11 show that the inter-rater agreement decreases as we increase the samples from the GPT-4 model, indicating that the model may not be applicable in this setup. However, using the confidence-based approach, we can still see that GPT-4 maintains similar inter-rater agreement.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8: Inter-rater agreement with increasing % of samples from GPT-4 for code summarization accuracy and similarity. The dotted line indicates the inter-human agreement.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 9: Inter-rater agreement with increasing % of samples from GPT-4 for name-value inconsistencies.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 10: Inter-rater agreement with increasing % of samples from GPT-4 for causality.</p>
<p>up to 60% for the causality dataset and 50% for the static analysis dataset, showing some promise for using GPT-4 for partial datasets. The confidence interval for these datasets up to that point completely overlaps with the confidence interval of the human-human rating. High-probability samples are likely to be more correct or align better with human preferences compared to other samples.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 11: Inter-rater agreement with increasing % of samples from GPT-4 for static analysis warnings.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>#of Rating for Each Sample</th>
<th>% Effort Saved for One Rating</th>
<th>% Effort Saved for Overall Process</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy (Code Summarization)</td>
<td>5</td>
<td>100%</td>
<td>33%</td>
</tr>
<tr>
<td>Adequacy (Code Summarization)</td>
<td>3</td>
<td>100%</td>
<td>33%</td>
</tr>
<tr>
<td>Conciseness (Code Summarization)</td>
<td>3</td>
<td>100%</td>
<td>33%</td>
</tr>
<tr>
<td>Similarity (Code Summarization)</td>
<td>3</td>
<td>50%</td>
<td>16.5%</td>
</tr>
<tr>
<td>Naïm</td>
<td>11</td>
<td>100%</td>
<td>9%</td>
</tr>
<tr>
<td>Causality</td>
<td>2</td>
<td>60%</td>
<td>30%</td>
</tr>
<tr>
<td>Goals (Resume)</td>
<td>3</td>
<td>100%</td>
<td>33%</td>
</tr>
<tr>
<td>Operations (Resume)</td>
<td>3</td>
<td>100%</td>
<td>33%</td>
</tr>
<tr>
<td>Effects (Resume)</td>
<td>3</td>
<td>100%</td>
<td>33%</td>
</tr>
<tr>
<td>Static Analysis Warning</td>
<td>2</td>
<td>50%</td>
<td>25%</td>
</tr>
</tbody>
</table>
<p>TABLE II: % of human effort saved for one rating and overall annotation process.</p>
<h3><em>D. RQ4: Human Effort to Potentially Save</em></h3>
<p>By replacing one human rating with an LLM's answer for some fraction of all samples, as studied in RQ3, we can save some human effort. The saved effort depends on two factors: First, the number of ratings required per sample. As we replace only one human's rating for each sample, more required ratings per sample means less potential for saving human effort. Second, the fraction of samples for which we replace a human rating with an LLM's rating. To decide on this fraction, we use the results from RQ3 to determine the maximum fraction that keeps the resulting inter-rater agreement statistically indistinguishable from a human-only study. For example, in Figure 8-b, asking an LLM for help for up to 50% of all samples keeps the agreement within the zone marked with gray background, but going beyond that fraction would</p>
<p>change the inter-rater agreement in a statistically significant way.</p>
<p>Based on this reasoning, Table II summarizes the amount of effort that could potentially be saved. For the accuracy task of code summarization, we can save $100 \%$ of the effort to obtain one rating per sample, which is $33 \%$ of the overall rating effort for this tasks. For name-value inconsistencies, we can save only $9 \%$ of the overall effort because there are 11 ratings per sample, whereas for the semantic similarity dataset, we can save up to $33 \%$ of the effort because it involves only three ratings per sample. Note that we ignore the few-shot labeling effort (which is 3-4 samples only) for this estimation in Table II. Overall, for seven of the ten tasks, we can safely replace one human rater with a model.</p>
<h2>V. Discussion</h2>
<p>Deciding Whether and When to Ask an LLM Our findings suggest that we can replace one human rating by a model for a significant number of samples ( $50 \%-100 \%$ ). However, it appears risky to fully replace all humans, even though the probability of the model output at least somewhat indicates the quality of annotations. Based on our findings, we propose the process as given in Figure 12, which involves two steps: 1) Create few-shot examples (3-4 in our setup) to query multiple strong LLMs with all samples, and compute the model-model agreement. 2) If the agreement is high ( $&gt;0.5$ based on our data), one can safely replace one human rating per sample with an LLM-provided answer. Otherwise, selectively replace one rating only for those samples where the LLM gives a highconfidence answer. Beyond saving human effort, our findings can also help extend a dataset by automatically labeling additional samples. We should emphasize that replacing more than one human can inflate inter-rater agreement because model-model agreements are much higher. This may not fulfill or reflect the original goals of the data annotations.
Confidence Distribution over Samples? We did find that picking the samples based on model confidence yields better results, even when model-model agreement is low, for almost $50 \%$ of the data. However, the question remains: what should be our cutoff confidence? Figure 13, which orders fraction of the sample, at decreasing confidence levels, shows that GPT-4 is usually quite confident with its solutions. However, after a certain point, the probability starts to decrease. For causality and static analysis warning, we can see that for the first $50 \%-60 \%$ of samples, the model maintains a high output probability, around 0.80 . Thus, the probability cutoff point can be computed when we have low model-model agreement.
Deciding Whether All Humans Are Replaceable In RQ3, we asked if one rating in all samples can be replaced by an LLM. Can we replace all humans, but just for specific samples? First, we clarify when we can replace all the humans for a sample. Note that there are very few samples where all annotators would agree. For name-value inconsistencies, no sample exists where all annotators agree on a particular rating. Therefore, we will use majority voting to decide whether all humans
are replaceable for a sample or not. For a dataset with 2 annotators, the model needs to agree with both annotators, and for a dataset with 11 annotators, the model needs to agree with at least 6 annotators to replace all humans.</p>
<p>Figure 14 shows that samples with higher GPT-4 output probability are more likely to agree with the majority of annotations; however, even with $10 \%$ of the samples, there is some possibility of error. Apart from semantic similarity, across all datasets, we can't find any probability split where the model consistently agrees with majority voting. Therefore, we conclude that we cannot replace all the humans for a sample with the current models. We can replace humans for selective samples, but there is some risk associated with it. More study is required for this; we leave it for future research.
Further Investigations It is now well-established that few-shot learning performs better than zero-shot learning. We also tried zero-shot learning in our preliminary stage and found that the human-model agreement was not satisfactory, even for datasets where few-shot learning performed very well. We tried repeated sampling (at higher temperature): the models mostly generate the same samples. We could improve the multi-sample technique by using chain-of-thought [31] or selfconsistency [32], but we do not have the original thoughts used by the original human annotators. In future studies, we strongly recommend collecting human thoughts, which could be used for few-shotting.</p>
<h2>VI. ThREATS TO VALIDITY</h2>
<p>We consider only 10 human-annotation tasks and 6 models. It is difficult to find publicly available datasets where the ratings from individual annotators are still available; many artifacts contain only the final annotation after resolving disagreements. Even though we see fairly consistent results across the datasets and models, finding may vary with others. Only categorical Krippendorff's $\alpha$ was applicable to all studied datasets. For some of our tasks, ordinal Krippendorff's $\alpha$ is also applicable (e.g., code summarization). However, we observe negligible change from using ordinal Krippendorff's $\alpha$.</p>
<p>Cohen's $\kappa$ is another popular measure but is applicable to only two raters. We calculated Cohen's $\kappa$ for each human and model pairs. Although we observe lower values with Cohen's $\kappa$, the difference between human-human and human-model agreements remains similar. We share the detailed results in the supplementary material. We proposed a 0.50 inter-model threshold based on our experiments; this may not always work (e.g., in scenarios where typical agreement values are higher or lower). Another possibility (besides using an inter-model agreement threshold), is to manually label a subset of the data; if human-human and human-model IRAs, are similar in this subset, the model could replace one human.</p>
<p>Few-shot learning requires some human effort initially, to get the "shots". However, labeling 3-4 samples is much cheaper compared to labeling all the samples. Also, in fewshot learning, we present the model with one randomly cho-</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 12: Steps to decide human rating replaceability.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 13: Change of output probability (by GPT-4) with % of samples sorted based on GPT-4 model's output probability. Some datasets were omitted for clarity of the plot.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 14: Change of fraction of samples where GPT-4 agrees with majority voting for % of samples sorted based on GPT-4 model's output probability.</p>
<p>Sen example from each category to prevent bias. However, outcomes may depend on the set of few-shot samples used.</p>
<p>Although all of our datasets are relatively recent, some of them might be part of the training data of some of the LLMs we evaluate. As our methodology performs several preprocessing steps, such as function body accumulation (for the semantic similarity task) and diff generation (for the static analysis warnings), it is highly unlikely that the model has memorized the exact results. However, we acknowledge that some bias may exist due to the model's exposure to the underlying GitHub repositories.</p>
<h2>VII. RELATED WORK</h2>
<p><em>ML and LLMs in Software Engineering</em> Learning-based approaches and LLMs have been applied to various software development tasks [33], including code completion [34]–[36], test generation [37]–[40], fuzzing [41], [42], automated repair [43]–[47] and coding agents [48]–[51], agents for other software engineering tasks [52], and type prediction [53]–[56]. We follow this trend, but deviate by not targeting a specific task, but by studying the potential for automating human-subject (manual) evaluations, using LLMs.</p>
<p><em>LLMs to Complement or Replace Human Annotators</em> Recent and concurrent work explores the idea of complementing or replacing human annotators with LLMs in domains beyond software engineering. Work in natural language processing (NLP) compares LLMs and human participants, typically recruited via crowd-working platforms, and report mixed results. While some studies show that adding LLM labels can improve the aggregated labels of a dataset [57] and sometimes outperform crowd-workers [58], others note that LLMs fail to accurately represent differences between demographic groups [59]. Others suggest to use LLMs to perform subtasks in a crowd-sourcing pipeline [60]. Bavaresco et al. [13] propose a benchmark of NLP datasets to compare human and LLM performance. We envision our work to serve as such a benchmark in software engineering. To the best of our knowledge, we are the first to carefully evaluate automated LLM-annotations for software engineering artifacts, and propose actionable guidelines for when and how to use LLMs to create annotations.</p>
<p><em>Collaborative Human-LLM Annotation</em> Our suggested workflow (Section V) relates to work on human-LLM collaboration for data labeling [61]. Some papers [12], [62] suggest first querying an LLM, and asking humans to refine some of the LLM-provided labels. Instead, we find that inter-model agreement and LLM output probabilities provide an effective way of deciding which labeling tasks can be safely delegated to a model.</p>
<p><em>LLMs for Data Synthesis and Assessment</em> Beyond using LLMs to partially automate evaluations that would otherwise be performed only by humans, others propose to use LLMs</p>
<p>to synthesize additional data to train or fine-tune models for text annotation tasks [63], [64]. There is a recent related survey [65]. A key difference from our work is that training and fine-tuning datasets must be large-scale, but some amount of noise is acceptable, whereas annotations in human-subject studies are typically of smaller scale, but should have high confidence. LLMs have also been proposed as judges of outputs generated by other LLMs, either with a single LLM as the judge [66] or with a set of LLMs that discuss until reaching an agreement [67]. These efforts are primarily about determining human preference, i.e., tasks where different humans may legitimately disagree, whereas many annotation tasks in software engineering have an objectively correct answer that humans can eventually agree upon.</p>
<h2>VIII. CONCLUSION</h2>
<p>In this paper, we investigate if LLM responses can substitute for human raters in software engineering annotation tasks, and how such substitutions may affect inter-rater agreement. We find that human-model agreements can be fairly consistent with human-agreements, for the majority of the settings we studied. We also find model-model agreement is a good indication of human-model agreement, suggesting that when powerful models trained on giant human corpora agree with each other, they tend to agree also with humans. Finally, we find that an LLM's confidence (output probability) for a given sample output is a good indication of whether LLM output agrees with majority human rating, for that sample. We caution that the paper considers only discrete (multiple choice) LLM responses, not free-form annotations; furthermore, we have not studied issues of model output bias, nor issues of demographics.</p>
<p>Our scripts and datasets are publicly available: https:// zenodo.org/doi/10.5281/zenodo. 13146386</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work was partially supported by the National Science Foundation under CISE SHF MEDIUM 2107592, the European Research Council (ERC, grant agreements 851895 and 101155832) and by the German Research Foundation within the ConcSys, DeMoCo, and QPTest projects.</p>
<h2>REFERENCES</h2>
<p>[1] Y. Zhu and M. Pan, "Automatic code summarization: A systematic literature review," arXiv preprint arXiv:1909.04352, 2019.
[2] C. Zhang, J. Wang, Q. Zhou, T. Xu, K. Tang, H. Gui, and F. Liu, "A survey of automatic source code summarization," Symmetry, vol. 14, no. 3, p. 471, 2022.
[3] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker, "Towards automatically generating summary comments for java methods," in Proceedings of the 25th IEEE/ACM international conference on Automated software engineering, 2010, pp. 43-52.
[4] H. J. Kang, K. L. Aw, and D. Lo, "Detecting false alarms from automatic static analysis tools: How far are we?" in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 698-709.
[5] S. Haque, Z. Eberhart, A. Bansal, and C. McMillan, "Semantic similarity metrics for evaluating source code summarization," in Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension, 2022, pp. 36-47.
[6] D. Roy, S. Fakhoury, and V. Arnaoudova, "Reassessing automatic evaluation metrics for code summarization tasks," in Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2021, pp. $1105-1116$.
[7] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.
[8] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser et al., "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context," arXiv preprint arXiv:2403.05530, 2024.
[9] Anthropic, "Introducing the next generation of claude." [Online]. Available: https://www.anthropic.com/news/claude-3-family
[10] G. V. Aher, R. I. Arriaga, and A. T. Kalai, "Using large language models to simulate multiple humans and replicate human subject studies," in International Conference on Machine Learning. PMLR, 2023, pp. $337-371$.
[11] R. Futrell, E. Wilcox, T. Morita, P. Qian, M. Ballesteros, and R. Levy, "Neural language models as psycholinguistic subjects: Representations of syntactic state," arXiv preprint arXiv:1903.03260, 2019.
[12] A. Goel, A. Gueta, O. Gilon, C. Liu, S. Erell, L. H. Nguyen, X. Hao, B. Jaber, S. Reddy, R. Kartha et al., "Llms accelerate annotation for medical information extraction," in Machine Learning for Health (ML4H). PMLR, 2023, pp. 82-100.
[13] A. Bavaresco, R. Bernardi, L. Bertolazzi, D. Elliott, R. Fernández, A. Gatt, E. Ghaleb, M. Giulianelli, M. Hanna, A. Koller et al., "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks," arXiv preprint arXiv:2406.18403, 2024.
[14] O. Dunay, D. Cheng, A. Tait, P. Thakkar, P. C. Rigby, A. Chiu, I. Ahmad, A. Ganesan, C. Maddila, V. Murali et al., "Multi-line ai-assisted code authoring," in Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering, 2024, pp. 150160.
[15] S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer, "The impact of ai on developer productivity: Evidence from github copilot," arXiv preprint arXiv:2302.06590, 2023.
[16] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., "Gemini: a family of highly capable multimodal models," arXiv preprint arXiv:2312.11805, 2023.
[17] T. Ahmed and P. Devanbu, "Few-shot training llms for project-specific code-summarization," in Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 1-5.
[18] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[19] J. Patra and M. Pradel, "Nalin: learning from runtime behavior to find name-value inconsistencies in jupyter notebooks," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 14691481.
[20] S. Easterbrook, J. Singer, M.-A. Storey, and D. Damian, "Selecting empirical methods for software engineering research," Guide to advanced empirical software engineering, pp. 285-311, 2008.
[21] J. M. Morse, M. Barrett, M. Mayan, K. Olson, and J. Spiers, "Verification strategies for establishing reliability and validity in qualitative research," International journal of qualitative methods, vol. 1, no. 2, pp. $13-22,2002$.
[22] M. Lombard, J. Snyder-Duch, and C. C. Bracken, "Content analysis in mass communication: Assessment and reporting of intercoder reliability," Human communication research, vol. 28, no. 4, pp. 587-604, 2002.
[23] J. Cohen, "A coefficient of agreement for nominal scales," Educational and psychological measurement, vol. 20, no. 1, pp. 37-46, 1960.
[24] K. Krippendorff, Content analysis: An introduction to its methodology. Sage publications, 2018.
[25] L. Fistlay, "'outing" the researcher: The provenance, process, and practice of reflexivity," Qualitative health research, vol. 12, no. 4, pp. $531-545,2002$.
[26] X. Hu, X. Xia, D. Lo, Z. Wan, Q. Chen, and T. Zimmermann, "Practitioners' expectations on automated code comment generation," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 1693-1705.
[27] A. LeClair, S. Jiang, and C. McMillan, "A neural model for generating natural language summaries of program subroutines," in 2019</p>
<p>IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 2019, pp. 795-806.
[28] T. H. Truong, T. Baldwin, K. Verspoor, and T. Cohn, "Language models are not naysayers: an analysis of language models on negation benchmarks," arXiv preprint arXiv:2306.08189, 2023.
[29] J. Fischbach, J. Frattini, A. Spaans, M. Kummeth, A. Vogelsang, D. Mendez, and M. Unterkalmsteiner, "Automatic detection of causality in requirement artifacts: the cira approach," in Requirements Engineering: Foundation for Software Quality: 27th International Working Conference, REF3Q 2021, Essen, Germany, April 12-15, 2021, Proceedings 27. Springer, 2021, pp. 19-36.
[30] M. Kamp, P. Kreutzer, and M. Philippsen, "Sesame: A data set of semantically similar java methods," in 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE, 2019, pp. $529-533$.
[31] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in neural information processing systems, vol. 35, pp. 24 824-24 837, 2022.
[32] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," arXiv preprint arXiv:2203.11171, 2022.
[33] M. Pradel and S. Chandra, "Neural software analysis," Commun. ACM, vol. 65, no. 1, pp. 86-96, 2022. [Online]. Available: https://doi.org/10.1145/3460348
[34] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," CoRR, vol. abs/2107.03374, 2021. [Online]. Available: https://arxiv.org/abs/2107.03374
[35] P. Nie, R. Banerjee, J. J. Li, R. J. Mooney, and M. Gligoric, "Learning deep semantics for test completion," in ICSE, 2023.
[36] A. Eghbali and M. Pradel, "De-hallucinator: Iterative grounding for llm-based code completion," CoRR, vol. abs/2401.01701, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2401.01701
[37] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, "Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models," in 45th International Conference on Software Engineering, ser. ICSE, 2023.
[38] M. Schäfer, S. Nadi, A. Eghbali, and F. Tip, "An empirical evaluation of using large language models for automated unit test generation," IEEE Transactions on Software Engineering, 2023.
[39] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan, and B. Ray, "Code-aware prompting: A study of coverage guided test generation in regression setting using llm," in FSE, 2024.
[40] J. A. Pizzorno and E. D. Berger, "Coverup: Coverage-guided llm-based test generation," 2024.
[41] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, "Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models," in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISETA, R. Just and G. Fraser, Eds. ACM, 2023, pp. 423-435. [Online]. Available: https://doi.org/10.1145/3597926.3598067
[42] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang, "Fuzz4all: Universal fuzzing with large language models," in ICSE, 2024.
[43] Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk, and M. Monperrus, "SequenceR: Sequence-to-sequence learning for end-to-end program repair," IEEE Trans. Software Eng., vol. 47, no. 9, pp. 1943-1959, 2021. [Online]. Available: https://doi.org/10.1109/TSE. 2019.2940179
[44] C. S. Xia and L. Zhang, "Keep the conversation going: Fixing 162 out of 337 bugs for S0.42 each using ChatGPT," 2023.
[45] H. Ye and M. Monperrus, "Iter: Iterative neural repair for multi-location patches," in ICSE, 2024.
[46] A. Silva, S. Fang, and M. Monperrus, "Repairllama: Efficient representations and fine-tuned adapters for program repair," 2024.
[47] S. B. Hossain, N. Jiang, Q. Zhou, X. Li, W.-H. Chiang, Y. Lyu, H. Nguyen, and O. Tripp, "A deep dive into large language models for automated bug localization and repair," in FSE, 2024.
[48] I. Bouzenia, P. Devanbu, and M. Pradel, "RepairAgent: An autonomous, LLM-based agent for program repair," Preprint, 2024.
[49] J. Yang, C. E. Jimenez, K. Lieret, S. Yao, A. Wettig, K. Narasimhan, and O. Press, "Swe-agent: Agent-computer interfaces enable automated software engineering," 2024.
[50] Y. Zhang, H. Ruan, Z. Fan, and A. Roychoudhury, "Autocoderover: Autonomous program improvement," 2024.
[51] W. Tao, Y. Zhou, W. Zhang, and Y. Cheng, "Magic: Llm-based multi-agent framework for github issue resolution," arXiv preprint arXiv:2403.17927, 2024.
[52] I. Bouzenia and M. Pradel, "You name it, i run it: An LLM agent to execute tests of arbitrary projects," 2024. [Online]. Available: https://arxiv.org/abs/2412.10133
[53] V. J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis, "Deep learning type inference," in Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT, 2018, pp. 152-162. [Online]. Available: https://doi.org/10.1145/3236024.3236051
[54] R. S. Malik, J. Patra, and M. Pradel, "NL2Type: Inferring JavaScript function types from natural language information," in Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, 2019, pp. 304-315. [Online]. Available: https://doi.org/10.1109/ICSE.2019.00045
[55] M. Pradel, G. Gousios, J. Liu, and S. Chandra, "Typewriter: Neural type prediction with search-based validation," in ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, 2020, pp. 209-220. [Online]. Available: https://doi.org/10.1145/3368089.3409715
[56] M. Allamanis, E. T. Barr, S. Ducousso, and Z. Gao, "Typilus: neural type hints," in Proceedings of the 41st ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI, 2020, pp. 91-105. [Online]. Available: https: //doi.org/10.1145/3385412.3385997
[57] J. Li, "A comparative study on annotation quality of crowdsourcing and LLM via label aggregation," CoRR, vol. abs/2401.09760, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2401.09760
[58] Z. He, C. Huang, C. C. Ding, S. Rohatgi, and T. K. Huang, "If in a crowdsourced data annotation pipeline, a GPT-4," in Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, F. F. Mueller, P. Kyburz, J. R. Williamson, C. Sas, M. L. Wilson, P. O. T. Dugas, and I. Shklovski, Eds. ACM, 2024, pp. 1040:1-1040:25. [Online]. Available: https://doi.org/10.1145/3613904.3642834
[59] A. Wang, J. Morgenstern, and J. P. Dickerson, "Large language models cannot replace human participants because they cannot portray identity groups," CoRR, vol. abs/2402.01908, 2024. [Online]. Available: https://doi.org/10.48550/arXiv. 2402.01908
[60] T. Wu, H. Zhu, M. Albayrak, A. Axon, A. Bertsch, W. Deng, Z. Ding, B. Guo, S. Gururaja, T. Kao, J. T. Liang, R. Liu, I. Mandal, J. Milbauer, X. Ni, N. Padmanabhan, S. Ramkumar, A. Sudjianto, J. Taylor, Y. Tseng, P. Vaidos, Z. Wu, W. Wu, and C. Yang, "Llms as workers in human-computational algorithms? replicating crowdsourcing pipelines with llms," CoRR, vol. abs/2307.10168, 2023. [Online]. Available: https://doi.org/10.48550/arXiv. 2307.10168
[61] H. Kim, K. Mitra, R. L. Chen, S. Rahman, and D. Zhang, "Meganno+: A human-llm collaborative annotation system," in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - System Demonstrations, St. Julians, Malta, March 17-22, 2024, N. Aletras and O. D. Clercq, Eds. Association for Computational Linguistics, 2024, pp. 168-176. [Online]. Available: https://aclanthology.org/2024.eacl-demo. 18
[62] X. Wang, H. Kim, S. Rahman, K. Mitra, and Z. Miao, "Human-llm collaborative annotation through effective verification of LLM labels," in Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, F. F. Mueller, P. Kyburz, J. R. Williamson, C. Sas, M. L. Wilson, P. O. T. Dugas, and I. Shklovski, Eds. ACM, 2024, pp. 303:1-303:21. [Online]. Available: https://doi.org/10.1145/3613904.3641960
[63] N. Pangakis, S. Wolken, and N. Fasching, "Automated annotation with</p>
<p>generative ai requires validation," arXiv preprint arXiv:2306.00176, 2023.
[64] Z. Li, H. Zhu, Z. Lu, and M. Yin, "Synthetic data generation with large language models for text classification: Potential and limitations," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Association for Computational Linguistics, 2023, pp. 10 443-10 461. [Online]. Available: https://doi.org/10.18653/v1/2023.emnlp-main. 647
[65] Z. Tan, A. Beigi, S. Wang, R. Guo, A. Bhattacharjee, B. Jiang, M. Karami, J. Li, L. Cheng, and H. Liu, "Large language models for data annotation: A survey," CoRR, vol. abs/2402.13446, 2024. [Online]. Available: https://doi.org/10.48550/arXiv. 2402.13446
[66] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing et al., "Judging llm-as-a-judge with mt-bench and chatbot arena," Advances in Neural Information Processing Systems, vol. 36, 2024.
[67] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu, "Chateval: Towards better llm-based evaluators through multiagent debate," CoRR, vol. abs/2308.07201, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2308.07201</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Some human raters did not rate the same samples.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ For example, Haque et al used six annotators, but each sample only received three annotations.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
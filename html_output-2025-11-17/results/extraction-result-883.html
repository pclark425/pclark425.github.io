<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-883 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-883</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-883</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-8465e6976515309e9ca13d431e377336f73032c0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8465e6976515309e9ca13d431e377336f73032c0" target="_blank">The Text-Based Adventure AI Competition</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Games</p>
                <p><strong>Paper TL;DR:</strong> This paper summarizes the three competitions ran in 2016–2018 (including details of open-source implementations of both the competition framework and competitors) and presents the results of an improved evaluation of these competitors across 20 games.</p>
                <p><strong>Paper Abstract:</strong> In 2016–2018 at the IEEE Conference on Computational Intelligence in Games, the authors of this paper ran a competition for agents that can play classic text-based adventure games. This competition fills a gap in existing game artificial intelligence (AI) competitions that have typically focused on traditional card/board games or modern video games with graphical interfaces. By providing a platform for evaluating agents in text-based adventures, the competition provides a novel benchmark for game AI with unique challenges for natural language understanding and generation. This paper summarizes the three competitions ran in 2016–2018 (including details of open-source implementations of both the competition framework and our competitors) and presents the results of an improved evaluation of these competitors across 20 games.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e883.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e883.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BYUAGENT2016</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BYUAGENT 2016</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A command-generation agent that uses word2vec embeddings and an affordance vector to propose verb-noun commands, stores commands that produced environment changes for one-shot learning, and includes a curated verb list including navigation verbs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What Can You Do with a Rock? Affordance Extraction via Word Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BYUAGENT 2016</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates commands by combining nouns extracted from game text with verbs drawn from a filtered verb set; uses word2vec embeddings (trained on Wikipedia) and an 'affordance vector' computed from known verb-noun pairs to find verbs matching a noun. Attempts generated commands exhaustively; when an attempted command produces a change in the game, that command is stored and later re-used in identical environments (one-shot learning). Also issues periodic observation commands ('look', 'inventory') and 'get all' on new locations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Z-Machine text-based adventure games via the competition framework (20-game test set)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Classic text-adventure games run under the ZPLET/Z-Machine interpreter; the agent receives short natural-language narrative descriptions each turn (partial observations) and must output text commands. Challenges include partial observability, varied natural-language output formats, out-of-game messages, sparse and heterogenous rewards, and unstructured action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>word2vec embeddings (trained on the Wikipedia corpus), a curated verb list derived from Wikipedia and human filtering, an affordance vector computed from embeddings. (The competition framework also supports IOAgent to connect external processes, but BYUAGENT used embedding tools internally.)</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Dense vector embeddings for words (numeric vectors); affordance vectors (numeric vector differences); stored command lists (structured text commands).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Simple memory of successful commands per encountered environment: when a command causes a change in the game state it is stored and later retried when generated commands fail in the same environment. No explicit spatial map or structured world model is described.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When a generated command produces a detectable change in the game's output, that command is recorded and associated with the current environment; later, if generated commands fail in that environment, the stored successful commands are attempted (one-shot memory). Periodic observation commands update the agent's perception but not a rich belief graph.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Reactive affordance-driven action generation (embedding-based affordance retrieval) with simple memory-based replay of previously successful commands; not a search or explicit model-based planner.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>0.79% completion average over 20 games (1000 steps per game); achieved non-zero score in 15% of games (single run reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding-based affordance extraction and one-shot storage of successful commands provides a modest boost in heterogeneous text-adventure domains, but BYUAGENT lacks an explicit world model or navigation planning; success relies on discovering actions that change the environment and replaying them when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e883.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e883.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Golovin (2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent with multiple command generators that uses large command-pattern templates, word2vec synonyms, an LSTM scorer, and a map graph to guide exploration and route selection to promising destinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Textbased Adventures of the Golovin AI Agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Golovin</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates candidate commands by inserting nouns from the current narrative into a large set (~250k) of command-pattern templates mined from walkthroughs and tutorials; expands nouns via word2vec synonyms (model trained on 3000 fantasy books). Each candidate command is weighted using factors including cosine similarity and an LSTM-based language model score; a roulette-wheel selection samples commands. Five prioritized command generators exist (battle, gather, inventory, general actions, exploration). Maintains a map graph of locations and movement commands; when movement occurs the graph is updated and exploration uses graph-based routing to promising locations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Z-Machine text-based adventure games via the competition framework (20-game test set)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same competition environment: partially-observable text adventures presented as narrative paragraphs; agents must parse natural language, propose text actions and handle out-of-game messages and heterogeneous scoring. Challenges include partial observability, need to discover navigation verbs/links, and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>word2vec embeddings (trained on 3000 fantasy books) to propose synonyms; an LSTM language model to score commands; a large corpus of command-pattern templates extracted from walkthroughs/tutorials; internal map graph data structure (built from observations).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Word embedding vectors (numeric), LSTM-derived scalar scores (numeric), generated textual command candidates, and a structured map graph of locations and movement-commands (graph data).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A map graph that records discovered locations and the movement-commands connecting them; blacklists of failed commands per location; inventory-change observations used to reset blacklists. No full probabilistic belief distribution—rather an explicit graph memory and per-location histories of tried/failed commands.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>On successful movement, the map graph is updated with a new node/location and an edge labeled by the movement command; when a command yields no observable change (game text identical), the command is blacklisted for that location until inventory changes; exploration chooses unexplored directions or computes a route to a promising destination based on graph distance and proportion of unattempted commands.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Heuristic graph-based routing for exploration combined with weighted sampling of generated commands (embedding + LSTM scoring); mainly reactive with a simple planning/routing component to reach promising nodes in the discovered graph.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based routing on an internally built map graph (selects routes to 'promising destinations' using distance and a heuristic of unattempted-command proportion); exact shortest-path algorithm is not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>1.45% completion average over 20 games (1000 steps per game; mean over 10 runs); achieved non-zero score in 31% of games (mean).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining large pattern templates with embedding-based synonym expansion and an LSTM scorer yields effective candidate commands; an explicit map graph and blacklisting of failed actions enables directed exploration and routing to promising locations, improving generalization, though domain-specific modules (e.g., battle mode) may overfit training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e883.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e883.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARL (BYUAGENT 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An affordance-based agent that detects state-information via skip-thought sentence embeddings, hashes state identifiers for memory of successful actions, and uses word2vec affordance manipulations to propose verb-noun commands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CARL (BYUAGENT 2017)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Identifies sentences in the narrative that convey state-information by converting sentences to skip-thought vectors and classifying proximity to labeled examples; concatenates identified state sentences and hashes them to produce a unique state identifier. Extracts nouns from state-information sentences; uses word2vec embeddings (trained on Wikipedia) and linear algebra manipulations to infer matching verbs (affordance detection). Stores actions that change state associated with hashed state identifiers for later recollection and reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Z-Machine text-based adventure games via the competition framework (20-game test set)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially-observable text adventures delivered as narrative text; agents must extract state-relevant information, propose text actions including prepositional commands, and handle sparse/delayed rewards and variable formatting across games.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Skip-thought sentence embeddings (pretrained model) to detect state-relevant sentences; word2vec embeddings (trained on Wikipedia) for noun/verb affordance inference; hashed state identifiers and action memory.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Dense sentence vectors (numeric) from skip-thought, word vectors (numeric) from word2vec, hashed state identifiers (discrete keys), stored action lists (text commands).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Hashed state identifiers derived from concatenated skip-thought vectors of sentences identified as state-information; these identifiers are used as keys in a memory mapping to store actions that previously changed the state in that hashed context.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Sentences are evaluated each turn via skip-thought proximity to labeled examples to decide which contain state-information; concatenated state sentences are hashed to form the current state id; when an action causes a state change, that action is recorded under the current hash for later reuse when the same hashed state recurs.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Affordance-driven candidate generation (embedding algebra) with memory-based replay of previously successful actions per hashed state; reactive with state-lookup based recall rather than explicit multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>1.59% completion average over 20 games (1000 steps per game; single run reported); achieved non-zero score in 30% of games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>State identification using sentence embeddings plus hashed state identifiers enables effective one-shot recollection of previously successful actions in recurring states; embedding-based affordance inference produces good candidate actions, but CARL lacks an explicit spatial map or multi-step navigation planner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e883.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e883.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL (Navigate Acquire Interact Learn)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular agent architecture (Examiner, Interactor, Navigator, etc.) using a shared knowledge graph to represent objects, locations, interactions and connections, a language-model based scorer for verb-object interactions, and a validity detector to decide whether actions succeeded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Composed of independent modules (Examiner, Interactor, Navigator, plus specialized modules) that bid for control each step by reporting 'eagerness'; the winning module issues commands. All modules update and consult a shared knowledge graph that tracks known objects, interactions, locations, and connections (and stores attempted interactions). The Interactor uses an LM-based language model to score verb-object pairs and falls back to a predefined verb list when uncertain. A validity detector (word-embedding-based text classifier) decides if an action 'succeeded' or 'failed' and prevents incorrect knowledge-graph updates. The Navigator module is responsible for movement and uses the knowledge graph to choose navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Z-Machine text-based adventure games via the competition framework (20-game test set)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially-observable narrative-driven text-adventures presented turn-by-turn; challenges include partial observability, noisy heterogeneous text outputs, varied scoring conventions, sparse rewards, and need for multi-step interaction and navigation across locations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Knowledge graph data structure (internal but structured world-representation); LM-based language model for scoring verb-object combos; a word-embedding-based text-classifier (validity detector, e.g., fastText-like) to detect action success/failure; predefined command sets for many modules.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph data (nodes for locations/objects/interactions and edges for connections), LM scalar probabilities/scores for candidate verb-object actions, classifier binary/soft labels indicating action validity, textual game responses.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A centralized knowledge graph recording objects, interactions, locations and connections plus histories of attempted interactions; it serves as a compact structured belief state accessible to all modules.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Any detected change in the game's textual output triggers updates to the knowledge graph; the validity detector classifies game responses as 'success' or 'failure' to determine whether to update the graph (success leads to addition/update of nodes/edges and recording of successful interactions; failures are recorded to avoid retries). The graph also stores attempted interactions to prevent repeated failed attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Modular control arbitration with LM-scored action selection; uses the knowledge graph for model-like reasoning (tracking connections and known interactions) enabling directed multi-step behaviors; primarily reactive per-module control with graph-based support for navigation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based navigation using the knowledge graph of locations and connections maintained by the agent; the Navigator module selects movement commands based on the known connectivity (exact pathfinding algorithm not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>2.56% completion average over 20 games (1000 steps per game; mean over 10 runs); achieved non-zero score in 45.5% of games (mean).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A shared structured belief (knowledge) graph plus a validity detector enables safer updates and avoids propagating spurious information; combining LM-scoring for interactions with a graph-based memory and a navigator module yields the strongest performance among submitted agents, indicating that explicit structured belief representations and module-based control help in partially-observable text environments, though overall completion rates remain very low (~2.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Text-Based Adventure AI Competition', 'publication_date_yy_mm': '2018-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What Can You Do with a Rock? Affordance Extraction via Word Embeddings <em>(Rating: 2)</em></li>
                <li>Textbased Adventures of the Golovin AI Agent <em>(Rating: 2)</em></li>
                <li>Language Understanding for Text-based Games Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Knowledge-gathering Agents in Adventure Games <em>(Rating: 1)</em></li>
                <li>Learning partially observable deterministic action models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-883",
    "paper_id": "paper-8465e6976515309e9ca13d431e377336f73032c0",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "BYUAGENT2016",
            "name_full": "BYUAGENT 2016",
            "brief_description": "A command-generation agent that uses word2vec embeddings and an affordance vector to propose verb-noun commands, stores commands that produced environment changes for one-shot learning, and includes a curated verb list including navigation verbs.",
            "citation_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
            "mention_or_use": "use",
            "agent_name": "BYUAGENT 2016",
            "agent_description": "Generates commands by combining nouns extracted from game text with verbs drawn from a filtered verb set; uses word2vec embeddings (trained on Wikipedia) and an 'affordance vector' computed from known verb-noun pairs to find verbs matching a noun. Attempts generated commands exhaustively; when an attempted command produces a change in the game, that command is stored and later re-used in identical environments (one-shot learning). Also issues periodic observation commands ('look', 'inventory') and 'get all' on new locations.",
            "environment_name": "Z-Machine text-based adventure games via the competition framework (20-game test set)",
            "environment_description": "Classic text-adventure games run under the ZPLET/Z-Machine interpreter; the agent receives short natural-language narrative descriptions each turn (partial observations) and must output text commands. Challenges include partial observability, varied natural-language output formats, out-of-game messages, sparse and heterogenous rewards, and unstructured action spaces.",
            "is_partially_observable": true,
            "external_tools_used": "word2vec embeddings (trained on the Wikipedia corpus), a curated verb list derived from Wikipedia and human filtering, an affordance vector computed from embeddings. (The competition framework also supports IOAgent to connect external processes, but BYUAGENT used embedding tools internally.)",
            "tool_output_types": "Dense vector embeddings for words (numeric vectors); affordance vectors (numeric vector differences); stored command lists (structured text commands).",
            "belief_state_mechanism": "Simple memory of successful commands per encountered environment: when a command causes a change in the game state it is stored and later retried when generated commands fail in the same environment. No explicit spatial map or structured world model is described.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When a generated command produces a detectable change in the game's output, that command is recorded and associated with the current environment; later, if generated commands fail in that environment, the stored successful commands are attempted (one-shot memory). Periodic observation commands update the agent's perception but not a rich belief graph.",
            "planning_approach": "Reactive affordance-driven action generation (embedding-based affordance retrieval) with simple memory-based replay of previously successful commands; not a search or explicit model-based planner.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "0.79% completion average over 20 games (1000 steps per game); achieved non-zero score in 15% of games (single run reported).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Embedding-based affordance extraction and one-shot storage of successful commands provides a modest boost in heterogeneous text-adventure domains, but BYUAGENT lacks an explicit world model or navigation planning; success relies on discovering actions that change the environment and replaying them when appropriate.",
            "uuid": "e883.0",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        },
        {
            "name_short": "Golovin",
            "name_full": "Golovin (2017)",
            "brief_description": "An agent with multiple command generators that uses large command-pattern templates, word2vec synonyms, an LSTM scorer, and a map graph to guide exploration and route selection to promising destinations.",
            "citation_title": "Textbased Adventures of the Golovin AI Agent",
            "mention_or_use": "use",
            "agent_name": "Golovin",
            "agent_description": "Generates candidate commands by inserting nouns from the current narrative into a large set (~250k) of command-pattern templates mined from walkthroughs and tutorials; expands nouns via word2vec synonyms (model trained on 3000 fantasy books). Each candidate command is weighted using factors including cosine similarity and an LSTM-based language model score; a roulette-wheel selection samples commands. Five prioritized command generators exist (battle, gather, inventory, general actions, exploration). Maintains a map graph of locations and movement commands; when movement occurs the graph is updated and exploration uses graph-based routing to promising locations.",
            "environment_name": "Z-Machine text-based adventure games via the competition framework (20-game test set)",
            "environment_description": "Same competition environment: partially-observable text adventures presented as narrative paragraphs; agents must parse natural language, propose text actions and handle out-of-game messages and heterogeneous scoring. Challenges include partial observability, need to discover navigation verbs/links, and sparse rewards.",
            "is_partially_observable": true,
            "external_tools_used": "word2vec embeddings (trained on 3000 fantasy books) to propose synonyms; an LSTM language model to score commands; a large corpus of command-pattern templates extracted from walkthroughs/tutorials; internal map graph data structure (built from observations).",
            "tool_output_types": "Word embedding vectors (numeric), LSTM-derived scalar scores (numeric), generated textual command candidates, and a structured map graph of locations and movement-commands (graph data).",
            "belief_state_mechanism": "A map graph that records discovered locations and the movement-commands connecting them; blacklists of failed commands per location; inventory-change observations used to reset blacklists. No full probabilistic belief distribution—rather an explicit graph memory and per-location histories of tried/failed commands.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "On successful movement, the map graph is updated with a new node/location and an edge labeled by the movement command; when a command yields no observable change (game text identical), the command is blacklisted for that location until inventory changes; exploration chooses unexplored directions or computes a route to a promising destination based on graph distance and proportion of unattempted commands.",
            "planning_approach": "Heuristic graph-based routing for exploration combined with weighted sampling of generated commands (embedding + LSTM scoring); mainly reactive with a simple planning/routing component to reach promising nodes in the discovered graph.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Graph-based routing on an internally built map graph (selects routes to 'promising destinations' using distance and a heuristic of unattempted-command proportion); exact shortest-path algorithm is not specified.",
            "performance_with_tools": "1.45% completion average over 20 games (1000 steps per game; mean over 10 runs); achieved non-zero score in 31% of games (mean).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Combining large pattern templates with embedding-based synonym expansion and an LSTM scorer yields effective candidate commands; an explicit map graph and blacklisting of failed actions enables directed exploration and routing to promising locations, improving generalization, though domain-specific modules (e.g., battle mode) may overfit training sets.",
            "uuid": "e883.1",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        },
        {
            "name_short": "CARL",
            "name_full": "CARL (BYUAGENT 2017)",
            "brief_description": "An affordance-based agent that detects state-information via skip-thought sentence embeddings, hashes state identifiers for memory of successful actions, and uses word2vec affordance manipulations to propose verb-noun commands.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "CARL (BYUAGENT 2017)",
            "agent_description": "Identifies sentences in the narrative that convey state-information by converting sentences to skip-thought vectors and classifying proximity to labeled examples; concatenates identified state sentences and hashes them to produce a unique state identifier. Extracts nouns from state-information sentences; uses word2vec embeddings (trained on Wikipedia) and linear algebra manipulations to infer matching verbs (affordance detection). Stores actions that change state associated with hashed state identifiers for later recollection and reuse.",
            "environment_name": "Z-Machine text-based adventure games via the competition framework (20-game test set)",
            "environment_description": "Partially-observable text adventures delivered as narrative text; agents must extract state-relevant information, propose text actions including prepositional commands, and handle sparse/delayed rewards and variable formatting across games.",
            "is_partially_observable": true,
            "external_tools_used": "Skip-thought sentence embeddings (pretrained model) to detect state-relevant sentences; word2vec embeddings (trained on Wikipedia) for noun/verb affordance inference; hashed state identifiers and action memory.",
            "tool_output_types": "Dense sentence vectors (numeric) from skip-thought, word vectors (numeric) from word2vec, hashed state identifiers (discrete keys), stored action lists (text commands).",
            "belief_state_mechanism": "Hashed state identifiers derived from concatenated skip-thought vectors of sentences identified as state-information; these identifiers are used as keys in a memory mapping to store actions that previously changed the state in that hashed context.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Sentences are evaluated each turn via skip-thought proximity to labeled examples to decide which contain state-information; concatenated state sentences are hashed to form the current state id; when an action causes a state change, that action is recorded under the current hash for later reuse when the same hashed state recurs.",
            "planning_approach": "Affordance-driven candidate generation (embedding algebra) with memory-based replay of previously successful actions per hashed state; reactive with state-lookup based recall rather than explicit multi-step planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": "1.59% completion average over 20 games (1000 steps per game; single run reported); achieved non-zero score in 30% of games.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "State identification using sentence embeddings plus hashed state identifiers enables effective one-shot recollection of previously successful actions in recurring states; embedding-based affordance inference produces good candidate actions, but CARL lacks an explicit spatial map or multi-step navigation planner.",
            "uuid": "e883.2",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL (Navigate Acquire Interact Learn)",
            "brief_description": "A modular agent architecture (Examiner, Interactor, Navigator, etc.) using a shared knowledge graph to represent objects, locations, interactions and connections, a language-model based scorer for verb-object interactions, and a validity detector to decide whether actions succeeded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NAIL",
            "agent_description": "Composed of independent modules (Examiner, Interactor, Navigator, plus specialized modules) that bid for control each step by reporting 'eagerness'; the winning module issues commands. All modules update and consult a shared knowledge graph that tracks known objects, interactions, locations, and connections (and stores attempted interactions). The Interactor uses an LM-based language model to score verb-object pairs and falls back to a predefined verb list when uncertain. A validity detector (word-embedding-based text classifier) decides if an action 'succeeded' or 'failed' and prevents incorrect knowledge-graph updates. The Navigator module is responsible for movement and uses the knowledge graph to choose navigation actions.",
            "environment_name": "Z-Machine text-based adventure games via the competition framework (20-game test set)",
            "environment_description": "Partially-observable narrative-driven text-adventures presented turn-by-turn; challenges include partial observability, noisy heterogeneous text outputs, varied scoring conventions, sparse rewards, and need for multi-step interaction and navigation across locations.",
            "is_partially_observable": true,
            "external_tools_used": "Knowledge graph data structure (internal but structured world-representation); LM-based language model for scoring verb-object combos; a word-embedding-based text-classifier (validity detector, e.g., fastText-like) to detect action success/failure; predefined command sets for many modules.",
            "tool_output_types": "Structured graph data (nodes for locations/objects/interactions and edges for connections), LM scalar probabilities/scores for candidate verb-object actions, classifier binary/soft labels indicating action validity, textual game responses.",
            "belief_state_mechanism": "A centralized knowledge graph recording objects, interactions, locations and connections plus histories of attempted interactions; it serves as a compact structured belief state accessible to all modules.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Any detected change in the game's textual output triggers updates to the knowledge graph; the validity detector classifies game responses as 'success' or 'failure' to determine whether to update the graph (success leads to addition/update of nodes/edges and recording of successful interactions; failures are recorded to avoid retries). The graph also stores attempted interactions to prevent repeated failed attempts.",
            "planning_approach": "Modular control arbitration with LM-scored action selection; uses the knowledge graph for model-like reasoning (tracking connections and known interactions) enabling directed multi-step behaviors; primarily reactive per-module control with graph-based support for navigation decisions.",
            "uses_shortest_path_planning": true,
            "navigation_method": "Graph-based navigation using the knowledge graph of locations and connections maintained by the agent; the Navigator module selects movement commands based on the known connectivity (exact pathfinding algorithm not specified).",
            "performance_with_tools": "2.56% completion average over 20 games (1000 steps per game; mean over 10 runs); achieved non-zero score in 45.5% of games (mean).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "A shared structured belief (knowledge) graph plus a validity detector enables safer updates and avoids propagating spurious information; combining LM-scoring for interactions with a graph-based memory and a navigator module yields the strongest performance among submitted agents, indicating that explicit structured belief representations and module-based control help in partially-observable text environments, though overall completion rates remain very low (~2.6%).",
            "uuid": "e883.3",
            "source_info": {
                "paper_title": "The Text-Based Adventure AI Competition",
                "publication_date_yy_mm": "2018-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What Can You Do with a Rock? Affordance Extraction via Word Embeddings",
            "rating": 2
        },
        {
            "paper_title": "Textbased Adventures of the Golovin AI Agent",
            "rating": 2
        },
        {
            "paper_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Knowledge-gathering Agents in Adventure Games",
            "rating": 1
        },
        {
            "paper_title": "Learning partially observable deterministic action models",
            "rating": 1
        }
    ],
    "cost": 0.01432125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Text-Based Adventure AI Competition</h1>
<p>Timothy Atkinson ${ }^{1 <em>}$, Hendrik Baier ${ }^{2 </em>}$, Tara Copplestone ${ }^{1}$, Sam Devlin ${ }^{2}$ and Jerry Swan ${ }^{1}$</p>
<h4>Abstract</h4>
<p>In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence in Games, the authors of this paper ran a competition for agents that can play classic text-based adventure games. This competition fills a gap in existing game AI competitions that have typically focussed on traditional card/board games or modern video games with graphical interfaces. By providing a platform for evaluating agents in textbased adventures, the competition provides a novel benchmark for game AI with unique challenges for natural language understanding and generation. This paper summarises the three competitions ran in 2016, 2017, and 2018 (including details of open source implementations of both the competition framework and our competitors) and presents the results of an improved evaluation of these competitors across 20 games.</p>
<h2>I. INTRODUCTION</h2>
<p>Before the widespread availability of graphical displays, text adventures were one of the few game genres that owed their existence solely to computing. The first text adventure was Colossal Cave (also known simply as Adventure), written in 1976 by Will Crowther for the PDP-10 mainframe [1]. With the advent of home computing in the late 1970s, Colossal Cave and other games such as ZORK were enjoyed by many. The majority of early text adventures used a narration-action loop that accepted simple commands of the general form VERB or VERB NOUN (e.g. 'look', 'go west', 'take box') via console input. In response to such commands, the programs provided a description of the immediate environment, e.g.
'You are in an open field on the west side of a white house with a boarded front door. There is a small mailbox here.'</p>
<p>Early adventures typically involved exploration and treasure hunting, but more sophisticated narratives emerged in the 1980s (e.g. in the INFOCOM range of games). An active "Interactive Fiction" community still exists, using powerful natural language authoring tools such as Inform [2] to create new and diverse titles.</p>
<p>Despite the continued existence of this community, one might ask what a competition concerned with text-based games has to offer, given that there are numerous competitions involving modern graphics-based games. The underlying motivation can be traced back to an early divergence between AI philosophy and practice. John McCarthy proposed the well-known "Monkey and Bananas Problem" in 1963 [3]: given a room containing a chair, a stick and a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>bunch of bananas hanging on a hook, the monkey's task is to find a sequence of actions that results in acquiring the bananas. McCarthy made a key distinction between the physical feasibility of the task (i.e. is there a physically realisable sequence of actions that achieves the goal?) and its epistemic feasibility (i.e. can the knowledge that a particular action even exists be efficiently derived?) [4]. We claim that recent game AI competitions have tended to de-emphasise the epistemic aspect, instead working in domains that are strongly operationalised. What this means is that the set of actions instantaneously available have been constrained to be knowable in advance. Of course, the space of plans for lengthy sequences of actions is still combinatorially huge, but the key question of how to derive and represent knowledge about an uncertain world is circumvented.</p>
<p>The hypothesis motivating the Text-Based Adventure AI competition is that the determination of relevant affordances (i.e. the set of behaviors that are possible in a given situation [5]) from a non-trivial environment is likely to require more than a good choice of credit assignment strategy, even if the latter would suffice in an operationalised domain. This then re-emphasises the following research questions:</p>
<ul>
<li>In the absence of operationalisation, what are the minimum priors (in terms of domain knowledge) that are required for success (even for a "toy" domain such as "Monkey and Bananas")?</li>
<li>How do model-free and model-based approaches compare? How can any difficulties with the former in this domain better inform such approaches in general?
In operationalised environments, the goal is to learn a policy mapping from game states to actions, where the set of available actions is predetermined. In a nonoperationalised environment, the set of available actions first has to be generated as a function of previously encountered game states. A policy mapping that works with these generated actions can then be learned subsequently. This function space is in general vastly larger than in the operationalised case.</li>
</ul>
<p>It could be argued that text adventure domains are also "operationalised", since the space of possible inputs is discrete. However, this does not take into account the fact that effective action requires a mapping from game output space to player input space for which random input text will be vastly less likely to have any effect than randomly pressing buttons in a first-person shooter, or randomly making legal moves in chess.</p>
<p>This article is structured as follows: Section II discusses related work on natural language processing in gameplaying. Section III describes the competition framework.</p>
<p>Section IV outlines the agents submitted to the 2016, 2017, and 2018 competitions. Section V describes an improved evaluation methodology based on the experience gained from the 2016 and 2017 competitions. Section VI presents new results from applying this methodology to the existing agents, which we hope can serve as a baseline for future research, and Section VII concludes.</p>
<h2>II. RELATED WORK</h2>
<p>The essential task of a competitor in the Text-Based Adventure AI Competition is to create an agent that can act effectively in a partially-observable environment, perceived via a number of short natural language descriptions of the agent's surroundings. Historically, work in the general area of natural language processing and narrative representation has mirrored the overall tendency for AI methods to move from symbolist to data-intensive methods such as Neural Networks, Reinforcement Learning or Monte Carlo Tree Search. In the 1970s and 1980s there was significant interest in symbolic representations of natural language and narrative, via semantic nets, frame systems and scripts, using approaches such as Case-Based Reasoning [6] and Abstraction Units [7]. More recently, there has been a tendency for a directly symbolist approach to Natural Language Processing to be eclipsed by more overtly numerical methods, including "Bag of Words" based approaches such as Latent Semantic Analysis [8], with the goals of approaches such as [7] being revisited via contemporary corpus-based techniques [9].</p>
<p>With specific reference to work on games, Branavan et al. [10] applied Monte Carlo Tree Search to the strategy game Civilization II. The value function was approximated via a neural net and included linguistic features extracted from the game manual, providing a significant improvement against the game's built-in opponent. Narrowing our focus further to Interactive Fiction, AI can play many roles in this genre of games [11] with this competition's focus being specifically on game playing AI. Previous work has often attempted to reduce the challenge of text-based games in order to make them more feasible domains for existing AI methods - such as for example by making the game output its environment descriptions not in natural language, but in first-order logic that can be directly stored and processed by an agent [12], [13]. Prior to our first competition in 2016, the previous work of greatest relevance in playing natural-language games was by Narasimhan et al [14], in which Deep Reinforcement Learning was used to jointly learn state representations and action policies in two Multi-User Dungeons (MUDs) - a specific type of text-based adventure game. The approach was demonstrated to outperform the use of both bag-of-words and bag-of-bigrams for state representations. This paper and early discussions with the first two authors helped shape the first competition we ran in 2016, which attempted to engage the game AI community more broadly in the challenging topic of text-based adventure AI by providing an accessible framework and regular schedule for agent evaluation via competitions.</p>
<h2>III. COMPETITION FRAMEWORK</h2>
<p>Our framework for evaluating a software agent's ability to play text-based adventures ${ }^{1}$ is based on the ZPLET [15] Java interpreter for the widely-used Z-Machine format [16], created by Infocom in 1979. The framework is defined in terms of a traditional agent-based perspective [17]. The console input and Z-Machine output text of ZPLET are redirected to an Agent interface, to be implemented by competitors. This interface consists of a single action method with a string argument. At each turn, the narrative text (typically at most a paragraph in length) that would otherwise be presented to the human player is provided as this argument. For example:
'You are standing at the end of a road before a small brick building. Around you is a forest. A small stream flows out of the building and down a gully'.</p>
<p>The action method then returns a string describing the action that the agent wishes to perform. A default Rando$m A g e n t$ is provided, with an action implementation that simply ignores the input text and chooses uniformly from 8 basic commands: 'north', 'south', 'east', 'west', 'verbose' (switching some games to a mode where they always output full descriptions of locations even if the agent has been there before), 'take all', 'yes', and 'no'.</p>
<p>By default, agents can be developed in Java ${ }^{2}$, but to facilitate the development and implementation of agents in other programming languages an additional agent, IOAgent, is provided. This agent's action method forwards the narrative text to the application's output stream and then returns the next line of text from the application's input stream. This allows entrants to interface external agents by starting the implementation with the IOAgent and then interacting with that process's IO. A random agent implemented in Python 3.5 is provided to demonstrate how this can be implemented.</p>
<p>Additionally, there are 3 predefined actions which are meant to help train an agent and may be accessed by the IOAgent through hard-coded string equivalents. These are Quit, which quits the running game; Restart, which restarts both the running game and the current agent; and SoftRestart, which restarts the running game but not the current agent.</p>
<h2>IV. COMPETITORS</h2>
<p>The first Text-Based Adventure AI Competition was announced on 15 May 2016 and ran later that year at the IEEE Conference on Computational Intelligence in Games after closing for entries on 31 August 2016. The competition ran again in 2017, officially being announced on 8 March 2017 and closing for entries on 18 July 2017.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In 2018, the competition was officially announced on 3 May 2018 and the closing for entries was 20 July 2018. In the following subsections we will summarise the four agents submitted to these competitions, three of which have been made available as open source by the respective participants. Future papers based on our framework can use these agents as baselines for evaluation and comparison.</p>
<h2>A. BYUAGENT 2016</h2>
<p>BYUAGENT $2016^{3}$ [18] interacts with a text-based adventure by generating commands combining nouns extracted from the game text with verbs drawn from a predefined set. More complex commands, such as propositional structures, may also be generated. Generated commands are attempted exhaustively and those commands which successfully produce a change in the game environment are stored. When the agent encounters a game environment where generated commands fail to produce a change, these stored commands may then be attempted, effectively implementing a simple form of one-shot learning.</p>
<p>The set of verbs the agent uses is drawn from the Wikipedia text corpus. The 1000 most commonlyappearing verbs were extracted, and then filtered by human play-testers according to their usefulness on a variety of text-based adventure games in order to produce a smaller set of approximately 100 verbs. A small number of additional human-selected verbs, including basic navigational commands, are also included.</p>
<p>Commands are generated using the word2vec [19] algorithm to produce vector embeddings of verbs and the nouns extracted from the game text. An "affordance vector", derived as the average vector difference in a known set of verb-noun pairs, is used to find a set of verbs which "match" a given noun. Additional hard-coded behaviour, such as periodic 'look' and 'inventory' commands to observe the game state, and 'get all' whenever entering a new location, aids the agent in interacting with the game.</p>
<h2>B. Golovin (2017)</h2>
<p>The Golovin ${ }^{4}$ agent [20] uses command generators to propose a non-empty set of commands for a given game environment by inserting nouns taken from the game's narrative text into "command patterns" - a set of 250,000 verb phrases extracted from various game walkthroughs, tutorials, and raw narrative text. To do this, a word2vec model trained on 3000 fantasy books is used to propose synonyms for each noun using n-best cosine similarity. Commands are then proposed by finding command patterns containing these synonyms and replacing that synonym with the original noun. Each generated command is associated with a weight, consisting of multiple factors including the cosine similarity between the noun and its</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>synonym's word2vec vector representation and a value given by a LSTM neural network operating on words [21]. Finally, a roulette wheel selection (based on the commands' associated weights) is used to choose a command from the generated set.</p>
<p>When a command is attempted and the game description remains the same, that command is assumed to have failed and is blacklisted for that game location until the agent observes a change in its inventory.</p>
<p>Five distinct command generators exist for different purposes. Each of these generators is fired, in this order, until a non-empty set of commands is proposed:</p>
<p>1) Battle mode. This command generator, specifically designed to aid the agent in combat scenarios, is limited to a subset of around 70 "fighting" command patterns containing one of the verbs 'attack', 'kill', 'fight', 'shoot' or 'punch'. This generator does not blacklist "failed" commands, as combat may require multiple iterations, and is only fired if a "fighting" command has been used previously.
2) Gathering items. This command generator is fired whenever the agent enters a new area, proposing 'take' commands for nouns in the area's narrative text.
3) Inventory commands. Once a noun has been successfully taken by the gathering items generator, this generator may fire, generating commands under the usual approach using only that noun's synonyms.
4) General actions. This command generator proposes commands in the usual approach using nouns from the game environment.
5) Exploration. A small fixed set of movement commands are proposed when an area has been exhausted by other command generators. Once the agent moves to a new area, a map graph storing locations and the commands which move between them is updated. If an area has unexplored directions, then those directions are proposed at random. Otherwise a route to a "promising destination", measured by its distance away and the proportion of possible commands left unattempted, is attempted.</p>
<h2>C. CARL (BYUAGENT 2017)</h2>
<p>The CARL agent uses affordance detection to suggest commands based on objects observed in a game's narrative text. Additionally, when an action is perceived to change the game state, that action is stored in memory so that it can be repeated during subsequent encounters with that state.</p>
<p>States are identified and stored by converting the individual sentences of the narrative text into skip-thought vectors [22]. The vector of each sentence is then classified as either state-information or not based on its proximity to the vector representations of a set of labelled example sentences. Those sentences which are identified as state-information are concatenated and hashed to create a unique identifier of the current state for the action recollection strategy described above.</p>
<p>Commands are then generated by first extracting nouns from those sentences identified as state-information. The vector representations of these nouns, generated by a word2vec model, trained on the Wikipedia text corpus, are manipulated using linear algebra to identify likely "matching" verbs. The best candidate verb-noun pairs are attempted first, with the search broadening to less "wellmatching" pairs should these fail to change the game's state. A complementary algorithm also attempts to generate prepositional combinations.</p>
<h2>D. NAIL (2018)</h2>
<p>The NAIL ("Navigate Acquire Interact Learn") agent ${ }^{5}$ consists of multiple independent modules which compete for control of the agent. Each module has a specific purpose; the main modules are the Examiner, Interactor and Navigator modules. These modules are responsible for identifying relevant objects in the current location, interacting with identified objects, and navigating to a new location, respectively. Additionally, there are further modules including specific modules for yes-no questions and for the acquisition of objects. Each module observes changes in the game and in each step of the game reports how 'eager' it is to assume control of the agent, with the most eager module in any step gaining full control.</p>
<p>Any change in the game's state is used to update a knowledge graph, which all modules have access to. This knowledge graph tracks known objects, interactions, locations and connections between locations, and serves as a compact representation of the world state. Additionally, the knowledge graph stores all previously attempted interactions to avoid retrying failed interactions.</p>
<p>Most modules use pre-defined sets of common commands to interact with the game. The Interactor module deviates here, constructing verb-noun phrases to interact with objects observed in the game. This module employs a LM-Based language model to assign probabilities to different verb-object combinations, the most promising of which are then executed. When there are no promising verb-object combinations, the Interactor falls back to a predefined set of verbs which are attempted in combination with observed objects from the game.</p>
<p>The NAIL agent uses a validity detector to determine whether its actions are having an effect on the game. A word-embedding-based text classifier [23] is used to establish whether a game's response to a given action either "failed" (had no effect) or "succeeded" (had an effect). Validity detection is needed e.g. to decide whether an object is relevant in the Examiner module, and to prevent incorrect updates to the knowledge graph when an action "failed'.</p>
<h2>V. EVALUATION METHODOLOGY</h2>
<p>The agents submitted to the 2016 and 2017 competitions were evaluated on a single game developed specifically</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>for the competition in order to provide a gradually increasing level of difficulty. The game was written by a game designer without a formal education in AI, with the intention of creating a game unbiased towards any existing approaches to game playing AI. The winner in both years was BYUAGENT 2016.</p>
<p>In hindsight, we realise the evaluation via a single game was flawed. Due to the mechanics of text-based adventures, if an agent is unable to solve a single puzzle, it is often unable to progress further in the game. This became obvious in the 2017 competition, where both the Golovin and CARL agents failed to score a single point because they could not solve the first puzzle. We had assumed the first puzzle to be simple, but our assessment of the complexity of text-based puzzles was entirely subjective. Moreover, the relative performance of agents in this competition compared to their performance as reported in the competitors' own publications [18], [20] suggest that our game was biased towards certain types of agents (e.g. BYUAGENT 2016).</p>
<p>This motivated the need for an evaluation across multiple games in order to fairly judge agents capable of general text-based adventure game playing ${ }^{6}$. The advantages of AIs that can play multiple games instead of a single game have frequently been promoted by the research community [24], [25], and there is a growing trend towards recognising the advantages of evaluating agents on commercial games not specifically designed to challenge AIs as well [26], [27]. Specifically, by evaluating on multiple games instead of a single test game, we avoid the tendency of competitors to overfit to the single test game; creating a contribution to that specific game instead of to AI research more broadly. Furthermore, the use of commercial games avoids bias in the game design towards specific AI methods the game designer may wish to promote if the game is created specifically to test AI. By instead using games created for human gameplay, this bias is presumed to be removed or at least averaged out over multiple games.</p>
<p>In the 2018 competition, we therefore switched to a more general evaluation framework using a test set of 20 different text adventures, which we downloaded from the web. The games were written in different styles by different authors, and some had been previously either commercially released, or submitted to interactive fiction writing competitions. There is no overlap between our test set and the training set used by [18], [20]. Unlike some of the games in this training set, all games in our test set provide a numerical score that allows for a fine-grained measurement of game playing performance. The precise actions for which a player gets rewarded with points, such as the successful solving of puzzles or winning of fights, are determined by the authors of the individual games. We expect that this removes the biases mentioned above both regarding our own style of writing, as well as our own style of scoring text-based adventure games. In addition,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>agents that fail at solving the first puzzle of any given game will still be able to make progress on other games. Agents can be evaluated on their performance given a fixed number of game steps (calls to the action method) per game, and their evaluation can be averaged over multiple runs with different random seeds in order to reduce the impact of run-to-run variance.</p>
<p>The following section presents a novel evaluation of all agents submitted to previous competitions using our new framework, with the aim of more accurately determining which agent is currently the best general text adventure game playing AI. We evaluated BYUAGENT 2016, Golovin, CARL (BYUAGENT 2017), and NAIL on each of the 20 test games for 1000 game steps. This is to make our results comparable to those in [18], [20] with the same number of steps. The results reported here are averages over 10 such runs for Golovin and NAIL; unfortunately, we were only able to do one run for each of the BYU agents (discussion below). In order to give an indication of how agent performance can scale with the number of game steps, NAIL and Golovin were additionally tested in 10 runs with 100 steps each, and Golovin was also tested in 10 runs with 10,000 steps each on all 20 games.</p>
<p>The evaluation metrics are the average percentage of points an agent achieved over all games and test runs, and the percentage of games in which an agent achieved any points, averaged over test runs. The first metric is our primary evaluation metric, and is expressed as an average percentage instead of an average number of points due to the large differences in the maximum number of points achievable in each game. 1 point out of a maximum of 10 should count for more than 1 point out of a maximum of 500 . The secondary metric gives an impression of the generalizability of current text-based adventure AIs, and can be used as a tie breaker in case two or more agents perform equally well according to the first metric. In case of two agents performing the same on both metrics (which has not happened so far), we would use an additional tie breaking criterion, preferring the agent that is using the least prior domain knowledge of text-based adventure games.</p>
<h2>VI. RESULTS</h2>
<p>The BYUAGENT 2016 was the strongest agent in our first competition in 2016. In 2017, CARL was the strongest agent, improving on both BYUAGENT 2016 and Golovin on the primary metric. As of 2018, NAIL is the strongest text-based adventure game playing agent. It is a clear improvement over all previously submitted agents in both evaluation metrics.</p>
<p>Our new testing framework also enables us to conduct more in-depth comparisons of different agents' performance over time. Three such experiments have been done so far. The additional experiments with Golovin and NAIL at 100 time steps per game demonstrate that NAIL does not start out stronger than Golovin in the first 100 time steps (at least not with respect to the primary metric), but makes more effective use of additional</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Agent</th>
<th style="text-align: center;">\% completion</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">\% non-zero</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">SD</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">SD</td>
</tr>
<tr>
<td style="text-align: center;">BYUAGENT 2016</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Golovin</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">3.94</td>
</tr>
<tr>
<td style="text-align: center;">CARL (BYUAGENT 2017)</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">NAIL</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">2.84</td>
</tr>
<tr>
<td style="text-align: center;">Golovin (100 steps)</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">3.53</td>
</tr>
<tr>
<td style="text-align: center;">NAIL (100 steps)</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2.11</td>
</tr>
<tr>
<td style="text-align: center;">Golovin (10k steps)</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">4.25</td>
</tr>
<tr>
<td style="text-align: center;">RandomAgent</td>
<td style="text-align: center;">1.66</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2.11</td>
</tr>
</tbody>
</table>
<p>TABLE I: Performance on the test set of 20 games in (unless stated otherwise) 1000 time steps per game. "\% completion" is the average score percentage an agent achieved over all games and runs; "\% non-zero" is the percentage of games in which an agent achieved any score, averaged over all runs. Standard deviations (SD), whereever given, refer to 10 runs over all games. Where they are not given, only 1 run could be completed.
time when scaling up to the 1000 time steps required by the competition. Additionally, the experiment running Golovin for 10,000 time steps per game shows that NAIL and CARL are stronger than Golovin even if Golovin is given a ten-fold time advantage. This is particularly impressive considering that NAIL and CARL are arguably using less domain-specific knowledge than Golovin, e.g. Golovin's "battle mode" [20]. It is possible that this domain-specific knowledge overfits to the training set used by the authors, and generalizes less well to the games in our test set. However, NAIL and CARL do require much longer thinking time than Golovin, and we can confirm the competitors' claim [20] that Golovin is an improvement over BYUAGENT 2016. The general level of text adventure AIs has increased from every competition to the next so far.</p>
<p>The last row of Table I shows the performance of our RandomAgent, described in Section III. Only with NAIL in 2018 has the first agent managed to beat it. This is probably due to its set of pre-specified commands being extremely simple and generalizing very well across games - changing locations and trying to pick up all possible objects is typical player behavior in text-based adventure games. Again, overfitting to their own training sets might have been the problem for our competitors before NAIL, using too many time steps on larger vocabularies and more intricate strategies that do not work in as many different game situations. We are optimistic for future agents now that the RandomAgent has been convincingly surpassed.</p>
<p>Considering the overall results, it is clear that all agents are facing major unsolved challenges. Even the strongest agent NAIL can currently only complete about $2.6 \%$ of a given test game on average, and gets no points at all in more than half of them. Many of the points achieved are given by the respective game just for starting, for initially submitting the 'get all' command in order to pick up any suitable objects in the first scene, or for walking into one of the compass directions. Despite some progress from</p>
<p>2016 to 2018, no agent is anywhere close to completing a single game. The problems of course include the very difficult scientific questions of how to extract information from natural language text, and how to map it to effective action in turn described in natural language, possibly via an intermediary model of the agent's environment. For example, the BYUAGENT 2016 is unable to deal with games that require giving prepositional commands [18] such as 'give dagger to wizard', or inferring the correct term for manipulable objects (such as requiring the command 'get shiny object' after describing "something shiny").
Looking at the design of all agents, they appear to share a common assumption that all actions which lead to new states are beneficial. Therefore these agents are essentially overcoming the challenge of sparse rewards from the environment by following an innate behaviour akin to work on curiosity as an intrinsic motivation [28] or novelty search [29]. Whilst these approaches have shown broad applicability, it is feasible to construct a pathological text-adventure game where such strong exploration would be punished, and an agent only rewarded for reaching a small subset of states. In the future hopefully more goaloriented agents will be developed, such that good and bad state changes can be distinguished and maybe even predicted.
From analysing the gameplay logs we can also conclude that more technical challenges are common to most agents as well, namely the correct parsing of game output, and differentiating between in-game and out-of-game messages to the player. All games conform to the type of narrationaction loop typical for text adventure games; but as they are originally written for human players, many games require a greater amount of flexibility than currently supported by AI agents. As an example, many games begin with an out-of-game message such as 'Would you like to resume a saved game ( $\mathrm{Y} / \mathrm{N})$ ?'. Reacting to these as if they were in-game narratives often leads to complete failure at the game. Some games respond to the input 'hint' with an in-game hint regarding the puzzle at hand, from which the agents can successfully retrieve information such as nouns and verbs to try in future actions; however, some games respond to 'hint' by opening an out-ofgame multiple-choice menu, which agents currently cannot handle. Furthermore, some agents have trouble identifying the score they have achieved, because they are expecting it in a slightly different format from what the game at hand returns. The game could for example respond to the input 'score' with 'If you were to stop now, you would score 50 points out of a maximum of $1500^{\prime}$, instead of the expected 'You have so far scored 50 out of a possible 1500, in 23 turns'. Golovin for example stops playing if it cannot identify the game score for 10 successive attempts - however, it aborts many games with valid but unexpected score formats, while playing others that actually do not keep a score but return a message in the expected format because the author did not care to remove this functionality (e.g. 'You have so far scored 0 out of a possible 0 , in 23 turns').</p>
<p>The creativity of interactive fiction writers can lead to even greater challenges in parsing and/or scoring, but along with scoreless games these challenges have been removed from the test set for the foreseeable future. For example, games can have a non-numerical ranking ("captain") instead of a score; they can use made-up words or languages (even for reporting the score); or they can only reward points after the player has finished significant parts of the game, giving no clear indication that the current score is zero before those milestones are reached.</p>
<p>Additionally, as the 2016 and 2017 competitions were not originally planned to use a test set of multiple games, we did not require functionality from the participants that would make (repeated) testing on such a set feasible or convenient. While Golovin for example recovered gracefully from failed interactions with individual games by reporting a result of zero points, the BYU agents frequently did not start at all on a given game, or even froze the OS. In combination with parsing and scoring problems, this made it necessary to manually re-start and supervise the agents on each game, which is why we can only report the result from a single run per test game in Table I. In future competitions, we will improve our descriptions of the necessary functionality, probably setting time limits per time step as well, to allow for a more streamlined, fair, and reliable agent evaluation.</p>
<p>Finally, we note that the published version of BYUAGENT 2016 [18] as well as some prior work [14] use reinforcement learning, playing a given game many times and gradually improving performance by learning successful commands for the game at hand. So far however, our competition has not provided a learning track, and therefore required the submission of fully trained agents, or agents capable of one-shot learning within a single game-playing episode. This does not fully reflect the capabilities of some agents. Therefore, we are considering the introduction of a learning track. In addition to the improvements for future competitions described above, this track could profit from the lessons learned by the broader reinforcement learning community on the topic of evaluating and comparing agents [30], [31]. For example, (1) reporting agent's average performance at several fixed stages of training to enable comparison of both final performance and rate of learning; (2) running multiple repeats to evaluate variance in learning performance due to the known issues of robustness and reproducibility with modern reinforcement learning algorithms; and (3) requiring all entrants to open source agent submissions and fully document hyper-parameter settings.</p>
<h2>VII. CONCLUSION</h2>
<p>Contemporary machine learning techniques have recently had many successes in game-playing domains such as Go that are traditionally hard for AI [32]. While it is clear that games provide an artificially restricted domain, we claim in this article that there is a tendency for game AI domains to be chosen such that the applicable operators are known in advance. This bias effectively means that</p>
<p>the epistemological problems of AI, first raised by John McCarthy [4], [33], are neglected. Broadly, such problems are concerned with extracting salient knowledge in nontrivial environments. This is unfortunate, since they are highly relevant for many real-world applications of natural language processing: The BYU team, for example, has recently acquired funding from Amazon as part of the "Alexa Prize", a challenge to create social bots that can converse coherently and engagingly with humans. They informed us "Some of the research that went into CARL was foundational in our approach to creating [the Alexa competitor] EVE. So it might please you to know that the CIG competition is having ripples with pretty wide impact" (Nancy Fulda, personal communication, Feb. 6, 2018).</p>
<p>The Text-Based Adventure AI Competition arose out of a desire to re-emphasise these neglected aspects of AI and motivate further experimentation. To date, mostly model-free approaches have been used as predicting the state transitions that might be caused by future actions proves to be very challenging. Golovin's exploration command generator and NAIL's knowledge graph have made some first steps, but there is still much room for improvement, and we are far from a consensus on how to optimally tackle the related problems. We hope that the community will perceive the challenge offered by this competition as both long-standing and meaningful, and respond with new approaches that push the envelope of existing wisdom regarding both hard problems and good solution mechanisms.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We would like to acknowledge the hard work of all competitors. Specifically, our thanks go to Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate from Brigham Young University, USA; Bartosz Kostka, Jarosław Kwiecień, Jakub Kowalski, and Pawel Rychlikowski from the University of Wrocław, Poland; and Matthew Hausknecht, Ricky Loynd, Shuohang Wang, and Greg Yang from Microsoft Research. We would also like to thank Karthik Narasimhan and Tejas D. Kulkarni for their input when originally defining the format of the first competition.</p>
<h2>REFERENCES</h2>
<p>[1] N. Montfort, Twisty Little Passages: An Approach to Interactive Fiction. MIT Press, 2004.
[2] D. K. Nelson, The Inform Designer's Manual. Sanderson, 2000.
[3] J. McCarthy, Situations, Actions, and Causal Laws, ser. Memo (Stanford AI Project). Comtex Scientific, 1963.
[4] , "Epistemological Problems of Artificial Intelligence," in Proceedings of the 5th International Joint Conference on Artificial Intelligence - Volume 2, 1977.
[5] J. Gibson, The Senoes Considered as Perceptual Systems. Allen \&amp; Unwin, 1968.
[6] R. Schank and R. Abelson, Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. Hillsdale, NJ.: Lawrence Erlbaum Associates, 1977.
[7] W. G. Lehnert, "Plot Units and Narrative Summarization," Cognitive Science, vol. 5, no. 4, pp. 293-331, 1981.
[8] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman, "Indexing by Latent Semantic Analysis," Journal of the American Society for Information Science, vol. 41, no. 6, pp. 391-407, 1990.
[9] A. Goyal, E. Riloff, and H. Daumé, III, "Automatically Producing Plot Unit Representations for Narrative Text," in Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, 2010, pp. 77-86.
[10] S. R. K. Branaean, D. Silver, and R. Barzilay, "Non-Linear Monte-Carlo Search in Civilization II," in Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI 2011), 2011, pp. 2404-2410.
[11] M. O. Riedl and V. Bulitko, "Interactive narrative: An intelligent systems approach," AI Magazine, vol. 34, no. 1, p. 67, 2012.
[12] B. Hlubocky and E. Amir, "Knowledge-gathering Agents in Adventure Games," in AAAI-04 Workshop on Challenges in Game AI, 2004.
[13] E. Amir and A. Chang, "Learning partially observable deterministic action models," Journal of Articial Intelligence Research, vol. 33, pp. 349-402, 2008.
[14] K. Narasimhan, T. D. Kulkarni, and R. Barzilay, "Language Understanding for Text-based Games Using Deep Reinforcement Learning," CoRR, vol. abs/1506.08941, 2015.
[15] M. T. Russotto, "ZPlet: A Z-Machine for Java," Available online at https://sourceforge.net/projects/zplet, accessed Feb 6th Feb, 2018.
[16] D. K. Kevin Bracey, Jason C. Penney, "The ZMachine Standards Document, version 1.1," Available at http://inform-fiction.org/zmachine/standards/z1point1/index.html, February 2014, Accessed 1st December 2017.
[17] S. J. Russell and P. Norvig, Artificial Intelligence: A Modern Approach, 2nd ed. Pearson Education, 2003.
[18] N. Fulda, D. Ricks, B. Murdoch, and D. Wingate, "What Can You Do with a Rock? Affordance Extraction via Word Embeddings," in Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI 2017), C. Sierra, Ed., 2017, pp. 1039-1045.
[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient Estimation of Word Representations in Vector Space," CoRR, vol. abs/1301.3781, 2013.
[20] B. Kostka, J. Kwiecień, J. Kowalski, and P. Rychlikowski, "Textbased Adventures of the Golovin AI Agent," in Proceedings of the 2017 IEEE Conference on Computational Intelligence and Games (CIG 2017), 2017, pp. 181-188.
[21] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, "A Neural Probabilistic Language Model," Journal of Machine Learning Research, vol. 3, pp. 1137-1155, 2003.
[22] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler, "Skip-Thought Vectors," CoRR, vol. abs/1506.06726, 2015.
[23] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, "Bag of tricks for efficient text classification," CoRR, vol. abs/1607.01759, 2016. [Online]. Available: http://arxiv.org/abs/1607.01759
[24] D. P. Liebana, S. Samothrakis, J. Togelius, T. Schaul, and S. M. Lucas, "General Video Game AI: Competition, Challenges and Opportunities," in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016), 2016, pp. 43354337.
[25] M. Genesereth, N. Love, and B. Pell, "General Game Playing: Overview of the AAAI Competition," AI magazine, vol. 26, no. 2, p. 62, 2005.
[26] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, "The arcade learning environment: An evaluation platform for general agents." J. Artif. Intell. Res.(JAIR), vol. 47, pp. 253-279, 2013.
[27] J. Togelius, "AI researchers, Video Games are your friends!" in International Joint Conference on Computational Intelligence. Springer, 2015, pp. 3-18.
[28] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, "Curiositydriven exploration by self-supervised prediction," in International Conference on Machine Learning, 2017, pp. 2778-2787.
[29] J. Lehman and K. O. Stanley, "Exploiting open-endedness to solve problems through the search for novelty," in ALIFE, 2008, pp. 329-336.
[30] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, "Deep Reinforcement Learning That Matters," in Proceedings of the Thirty-Second AAAI Conference on Artificial</p>
<p>Intelligence (AAAI-18), S. A. McIlraith and K. Q. Weinberger, Eds. AAAI Press, 2018, pp. 3207-3214.
[31] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling, "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents," arXiv preprint arXiv:1709.06009, 2017.
[32] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., "Mastering the Game of Go without Human Knowledge," Nature, vol. 550, no. 7676, pp. 354-359, 2017.
[33] J. McCarthy and P. J. Hayes, "Readings in Nonmonotonic Reasoning," M. L. Ginsberg, Ed. Morgan Kaufmann Publishers Inc., 1987, ch. Some Philosophical Problems from the Standpoint of Artificial Intelligence, pp. 26-45.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The NAIL agent is available open source at: http://aka.ms/nail&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ The authors of Golovin and the BYU agents themselves seem to have already agreed on a training set of 50 text-based adventure games for algorithm development and testing [18], [20].&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
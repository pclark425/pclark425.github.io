<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3514 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3514</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3514</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-dda6fb309f62e2557a071522354d8c2c897a2805</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dda6fb309f62e2557a071522354d8c2c897a2805" target="_blank">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs, and presents a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1.</p>
                <p><strong>Paper Abstract:</strong> Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3514.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3514.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerically-Aware QANet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reading-comprehension model that extends QANet with explicit numeric answer types (passage spans, question spans, counts 0–9, and simple arithmetic by assigning +/−/0 to extracted passage numbers), trained weakly by marginalizing over executions that yield the gold answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Builds on the QANet architecture (embedding, encoder, passage-question attention) and adds specialized output heads for question-span prediction, passage-span prediction, a 10-way count classifier (0–9), and a sign-assignment mechanism over extracted passage numbers (plus/minus/zero) to form simple arithmetic expressions; trained weakly by marginalizing over all executions that yield the gold answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (Discrete Reasoning Over Paragraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Paragraph-level reading comprehension questions requiring discrete operations (counting, addition/subtraction, sorting, comparison, selection, multi-span aggregation) and reference resolution over paragraph text.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Neural + symbolic hybrid: augment QANet with discrete numeric output heads (counts and sign-assigned arithmetic) and train weakly by marginalizing over executions; use marginal-likelihood objective over matching executions; at test time greedily choose answer type then decode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>47.0% F1 on DROP (reported overall best model in the paper). Dev-set breakdown: for 'Numbers' answers NAQANet Exact Match 44.0 and F1 44.2; for 'Single Span' F1 64.6; for '>1 Spans' F1 17.13 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Best baseline reading-comprehension systems (pre-NAQANet) achieved 32.7% F1 on DROP (paper's reported best baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NAQANet improved absolute F1 by 14.3 percentage points over the best baseline (47.0% vs 32.7% F1). Biggest gains concentrated on numeric answer types.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limited arithmetic expressivity (search limited to addition/subtraction of up to two numbers during search; sign-assignment scheme limited), frequent errors on more complex arithmetic and composed reasoning; error analysis shows common failure modes: arithmetic errors (51% of analyzed errors), counting (30%), domain knowledge/common sense (23%), coreference (6%); struggles on multi-span aggregation and domain-knowledge questions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Weakly-supervised marginalization over executions is used; authors report NAQANet gains are concentrated on numeric answer types (Table 6). Error analysis on a random sample of 100 erroneous predictions identifies the dominant failure categories and quantifies them. Training marginalizes over all executions that produce the gold answer (search for arithmetic limited to addition/subtraction of two numbers due to exponential search).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3514.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3514.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KDG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Krishnamurthy et al. grammar-constrained neural semantic parser (KDG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grammar-constrained semantic parsing model (from Krishnamurthy et al. 2017) adapted to run over table-like representations extracted from paragraphs, producing logical forms that can be executed to answer discrete queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural semantic parsing with type constraints for semi-structured tables</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KDG (grammar-constrained neural semantic parser)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Semantic parsing model that induces a grammar from argument/return types and searches for logical forms; here applied to tables induced from paragraph representations (SRL/SynDep/OpenIE) and trained weakly to maximize marginal likelihood of logical forms yielding the gold denotation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (via semantic parsing over extracted tables)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Translate natural-language questions into logical forms executable over a structured representation of paragraph content, to perform discrete operations (filters, counters, numeric comparisons, aggregation) to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Pipeline: extract predicate-argument structures from paragraphs via three representation schemes (syntactic dependencies, SRL, OpenIE), convert to table rows/columns, induce grammar/functions (counters, filters, numeric comparisons), exhaustive search up to a tree depth for logical forms, marginalize over logical forms that evaluate to the gold answer during training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to perform poorly on DROP overall (semantic parsing baselines perform poorly compared to reading-comprehension baselines). Exact numeric overall performance not given in-text, but coverage/diagnostic stats: logical-form search yielded candidates for 34% of training data with SRL, 25% with OpenIE; only ~25% of SRL tables contained required information in a sampled analysis; only 13% of sampled logical forms were non-spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement over reading-comprehension baselines; semantic parsers underperformed due to low coverage, noisy extraction, and spurious logical forms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Key limitations: pipeline information extraction from paragraphs is noisy (low coverage of required facts), many extracted tables lack the needed information, weak supervision leads to spurious logical forms (logical forms that evaluate correctly but do not reflect question semantics), making generalization poor; search and marginalization can only be applied to a subset of training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Quantitative analysis of representation schemes: SRL and dependency schemes allowed logical-form search for 34% of training data, OpenIE for 25%; manual inspection found only 25% of SRL-extracted tables contained needed answer information; only 13% of logical forms were non-spurious in a 60-question sample—highlighting the importance of high-quality information extraction and the spuriousness problem in weakly supervised semantic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3514.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3514.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiDAF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional Attention Flow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SQuAD-style neural reading-comprehension model that computes bidirectional attention between passage and question representations; used here as an adversarial baseline during data collection and as a baseline system on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bidirectional attention flow for machine comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BiDAF</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural QA model that uses bidirectional attention flow between passage and question to predict answer spans; AllenNLP implementation used with marginal-likelihood objective adaptation for DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (span-based reading comprehension baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-question span extraction for reading comprehension; limited to predicting contiguous spans in the passage (cannot directly output numeric computations or multi-span aggregations).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Applied marginal-likelihood objective (Clark & Gardner, 2018) to handle multiple matching spans; used as a live adversarial model in crowdsourcing to force workers to produce questions that BiDAF could not answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not given as a single-number in-text for DROP; used as adversarial model during data collection. For reference, BiDAF on SQuAD 1.1 is quoted as 66.8% EM in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Part of reading-comprehension baselines that collectively achieved the paper's reported 'best baseline' F1 of 32.7% on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NA (baseline model).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot produce non-span answers (counts, arithmetic); 45% of DROP questions are not span-answerable and were omitted when training span-only models; thus BiDAF-like architectures are fundamentally limited for many DROP phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Used in adversarial annotation loop; the authors adapted BiDAF to use marginal-likelihood training over matching spans for DROP. No specific ablations beyond that adaptation are reported for BiDAF in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3514.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3514.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QANet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QANet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SQuAD-style reading-comprehension model combining local convolutions with global self-attention, used as a strong baseline and as the backbone for NAQANet.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QANet: Combining local convolution with global self-attention for reading comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QANet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-like architecture combining convolutional layers (local features) with global self-attention blocks for reading comprehension; used in the paper both as a baseline and as the encoder component for NAQANet.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (span-based baseline and backbone for NAQANet)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict answer spans and (with NAQANet extensions) numeric outputs for questions requiring discrete reasoning over paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Applied marginal-likelihood objective for non-unique spans; QANet hyperparameters adjusted due to GPU limits; QANet+ELMo variant concatenates pretrained ELMo embeddings; NAQANet extends QANet with numeric heads.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On SQuAD the paper cites QANet (72.7% EM) and QANet+ELMo (78.7% EM). On DROP, QANet-derived baselines are part of the group achieving the best baseline F1 of 32.7%; NAQANet (QANet + numeric heads) achieves 47.0% F1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline group (reading-comprehension models including QANet variants) best F1 = 32.7% on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>NAQANet (built on QANet) adds numeric reasoning heads and attains +14.3% absolute F1 over the best baseline (47.0% vs 32.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Vanilla QANet cannot produce counts or arbitrary arithmetic; must be extended (as NAQANet) to handle numeric answer types; span-only models must drop ~45% of questions that are non-span answers.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors note hyperparameter adjustments (batch size, blocks in modeling layer) due to GPU constraints; QANet+ELMo uses pretrained ELMo embeddings. The NAQANet used the same hyperparameters as QANet for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3514.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3514.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QANet+ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QANet with ELMo contextual embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>QANet enhanced by concatenating pretrained ELMo contextual representations to input embeddings; used as a stronger SQuAD-style baseline on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep contextualized word representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QANet + ELMo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>QANet augmented with pretrained deep contextualized ELMo embeddings concatenated to the original token embeddings to improve representational capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (span-based baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Span-extraction reading comprehension; still limited for numeric/non-span answers without additional mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Use of pretrained contextual embeddings (ELMo) to improve representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>QANet+ELMo cited SQuAD EM 78.7% in paper; on DROP it contributes to reading-comprehension baseline performance (best baseline group F1 = 32.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Compared to base QANet, adding ELMo helps on SQuAD; on DROP the main gains on numeric reasoning required NAQANet-style numeric heads rather than only contextual embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Cannot natively produce counts or arithmetic; still a span-extraction architecture so non-span DROP questions must be omitted for training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No detailed ablation separating ELMo contribution on DROP; hyperparameters otherwise same as QANet baseline (except reported batch size/blocks changes).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3514.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3514.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (large, uncased)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained bidirectional Transformer model that achieved state-of-the-art on many NLP tasks; used as a strong reading-comprehension baseline on DROP (converted to span-format).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bert: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (large, uncased)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained Transformer-based language model (bidirectional) fine-tuned for span-extraction QA; paper used an open-source large uncased BERT implementation from Hugging Face and converted DROP to SQuAD format by using the first matching span as gold.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (span-extraction baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Span-extraction reading comprehension applied to DROP after conversion to SQuAD-like format; limited for non-span numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuned pretrained BERT large uncased as a span-extraction model; for DROP they converted dataset to SQuAD format by taking the first match as the gold span (for black-box BERT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that BERT, despite SQuAD SOTA, suffers large drops on DROP: 'BERT drops by more than 50 absolute F1 points' relative to SQuAD performance. Per-type dev-set numbers in Table 6: for 'Numbers' answers BERT Exact Match 14.5 and F1 14.8; for 'Single Span' BERT Exact Match 64.6 and F1 70.1; for '>1 Spans' BERT F1 25.0.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>On SQuAD BERT reported 84.7% EM (cited in paper), but on DROP its numeric-answer handling is poor (see per-type numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>BERT is part of the reading-comprehension baselines (collective best baseline F1 = 32.7%); NAQANet outperforms these span-only baselines notably on numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severely degraded performance on DROP numeric questions: low Exact Match/F1 on 'Numbers' (14.5 EM / 14.8 F1) indicating inability of vanilla BERT span-extraction fine-tuning to handle numeric aggregation, multi-span answers, or arithmetic reasoning; also depends on dataset conversion heuristic (first match) which may be suboptimal.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Per-answer-type analysis (Table 6) reveals BERT's relative strengths (single-span answers) and weaknesses (numeric answers, multi-span). The authors note that span-only models had to omit ~45% of training questions that are not representable as passage spans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3514.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3514.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-only, Paragraph-only, and Frequency-based heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple heuristic baselines that test for dataset artifacts: models trained with question input or paragraph input zeroed out, and a frequency-based majority predictor of common answers per question-word.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Heuristic baselines (question-only, paragraph-only, majority answer per question word)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Question-only and paragraph-only variants of QANet (zeroing out the other input) to detect dataset artifacts; a frequency-based baseline that predicts the top-3 most common answer spans per question-word from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>DROP (artifact checks / simple baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Simple prediction heuristics to measure annotation artifacts and answer distribution biases; do not perform explicit reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Input ablations (zeroing question or paragraph embeddings) and answer-frequency majority baseline per WH-word.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that heuristic baselines do poorly on DROP, suggesting relatively small annotation biases; exact numeric results are not provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable; these are negative controls showing that simple artifacts do not explain performance of stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As expected, these heuristics cannot perform the discrete reasoning DROP requires (counting, arithmetic, multi-span aggregation) and perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>These baselines act as an ablation to quantify dataset artifacts: question-only and paragraph-only QANet variants test how much signal comes from question vs passage; frequency baseline inspects answer distribution conditional on question-word.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural semantic parsing with type constraints for semi-structured tables <em>(Rating: 2)</em></li>
                <li>Compositional semantic parsing on semi-structured tables <em>(Rating: 2)</em></li>
                <li>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision <em>(Rating: 2)</em></li>
                <li>Neural programmer: Inducing latent programs with gradient descent <em>(Rating: 2)</em></li>
                <li>Neural programmer-interpreters <em>(Rating: 2)</em></li>
                <li>Bidirectional attention flow for machine comprehension <em>(Rating: 1)</em></li>
                <li>QANet: Combining local convolution with global self-attention for reading comprehension <em>(Rating: 1)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3514",
    "paper_id": "paper-dda6fb309f62e2557a071522354d8c2c897a2805",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "NAQANet",
            "name_full": "Numerically-Aware QANet",
            "brief_description": "A reading-comprehension model that extends QANet with explicit numeric answer types (passage spans, question spans, counts 0–9, and simple arithmetic by assigning +/−/0 to extracted passage numbers), trained weakly by marginalizing over executions that yield the gold answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "NAQANet",
            "model_description": "Builds on the QANet architecture (embedding, encoder, passage-question attention) and adds specialized output heads for question-span prediction, passage-span prediction, a 10-way count classifier (0–9), and a sign-assignment mechanism over extracted passage numbers (plus/minus/zero) to form simple arithmetic expressions; trained weakly by marginalizing over all executions that yield the gold answer.",
            "model_size": null,
            "reasoning_task_name": "DROP (Discrete Reasoning Over Paragraphs)",
            "reasoning_task_description": "Paragraph-level reading comprehension questions requiring discrete operations (counting, addition/subtraction, sorting, comparison, selection, multi-span aggregation) and reference resolution over paragraph text.",
            "method_or_intervention": "Neural + symbolic hybrid: augment QANet with discrete numeric output heads (counts and sign-assigned arithmetic) and train weakly by marginalizing over executions; use marginal-likelihood objective over matching executions; at test time greedily choose answer type then decode.",
            "performance": "47.0% F1 on DROP (reported overall best model in the paper). Dev-set breakdown: for 'Numbers' answers NAQANet Exact Match 44.0 and F1 44.2; for 'Single Span' F1 64.6; for '&gt;1 Spans' F1 17.13 (Table 6).",
            "baseline_performance": "Best baseline reading-comprehension systems (pre-NAQANet) achieved 32.7% F1 on DROP (paper's reported best baseline).",
            "improvement_over_baseline": "NAQANet improved absolute F1 by 14.3 percentage points over the best baseline (47.0% vs 32.7% F1). Biggest gains concentrated on numeric answer types.",
            "limitations_or_failures": "Limited arithmetic expressivity (search limited to addition/subtraction of up to two numbers during search; sign-assignment scheme limited), frequent errors on more complex arithmetic and composed reasoning; error analysis shows common failure modes: arithmetic errors (51% of analyzed errors), counting (30%), domain knowledge/common sense (23%), coreference (6%); struggles on multi-span aggregation and domain-knowledge questions.",
            "ablation_or_analysis": "Weakly-supervised marginalization over executions is used; authors report NAQANet gains are concentrated on numeric answer types (Table 6). Error analysis on a random sample of 100 erroneous predictions identifies the dominant failure categories and quantifies them. Training marginalizes over all executions that produce the gold answer (search for arithmetic limited to addition/subtraction of two numbers due to exponential search).",
            "uuid": "e3514.0",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "KDG",
            "name_full": "Krishnamurthy et al. grammar-constrained neural semantic parser (KDG)",
            "brief_description": "A grammar-constrained semantic parsing model (from Krishnamurthy et al. 2017) adapted to run over table-like representations extracted from paragraphs, producing logical forms that can be executed to answer discrete queries.",
            "citation_title": "Neural semantic parsing with type constraints for semi-structured tables",
            "mention_or_use": "use",
            "model_name": "KDG (grammar-constrained neural semantic parser)",
            "model_description": "Semantic parsing model that induces a grammar from argument/return types and searches for logical forms; here applied to tables induced from paragraph representations (SRL/SynDep/OpenIE) and trained weakly to maximize marginal likelihood of logical forms yielding the gold denotation.",
            "model_size": null,
            "reasoning_task_name": "DROP (via semantic parsing over extracted tables)",
            "reasoning_task_description": "Translate natural-language questions into logical forms executable over a structured representation of paragraph content, to perform discrete operations (filters, counters, numeric comparisons, aggregation) to produce answers.",
            "method_or_intervention": "Pipeline: extract predicate-argument structures from paragraphs via three representation schemes (syntactic dependencies, SRL, OpenIE), convert to table rows/columns, induce grammar/functions (counters, filters, numeric comparisons), exhaustive search up to a tree depth for logical forms, marginalize over logical forms that evaluate to the gold answer during training.",
            "performance": "Reported to perform poorly on DROP overall (semantic parsing baselines perform poorly compared to reading-comprehension baselines). Exact numeric overall performance not given in-text, but coverage/diagnostic stats: logical-form search yielded candidates for 34% of training data with SRL, 25% with OpenIE; only ~25% of SRL tables contained required information in a sampled analysis; only 13% of sampled logical forms were non-spurious.",
            "baseline_performance": null,
            "improvement_over_baseline": "No improvement over reading-comprehension baselines; semantic parsers underperformed due to low coverage, noisy extraction, and spurious logical forms.",
            "limitations_or_failures": "Key limitations: pipeline information extraction from paragraphs is noisy (low coverage of required facts), many extracted tables lack the needed information, weak supervision leads to spurious logical forms (logical forms that evaluate correctly but do not reflect question semantics), making generalization poor; search and marginalization can only be applied to a subset of training examples.",
            "ablation_or_analysis": "Quantitative analysis of representation schemes: SRL and dependency schemes allowed logical-form search for 34% of training data, OpenIE for 25%; manual inspection found only 25% of SRL-extracted tables contained needed answer information; only 13% of logical forms were non-spurious in a 60-question sample—highlighting the importance of high-quality information extraction and the spuriousness problem in weakly supervised semantic parsing.",
            "uuid": "e3514.1",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "BiDAF",
            "name_full": "Bidirectional Attention Flow",
            "brief_description": "A SQuAD-style neural reading-comprehension model that computes bidirectional attention between passage and question representations; used here as an adversarial baseline during data collection and as a baseline system on DROP.",
            "citation_title": "Bidirectional attention flow for machine comprehension",
            "mention_or_use": "use",
            "model_name": "BiDAF",
            "model_description": "Neural QA model that uses bidirectional attention flow between passage and question to predict answer spans; AllenNLP implementation used with marginal-likelihood objective adaptation for DROP.",
            "model_size": null,
            "reasoning_task_name": "DROP (span-based reading comprehension baseline)",
            "reasoning_task_description": "Passage-question span extraction for reading comprehension; limited to predicting contiguous spans in the passage (cannot directly output numeric computations or multi-span aggregations).",
            "method_or_intervention": "Applied marginal-likelihood objective (Clark & Gardner, 2018) to handle multiple matching spans; used as a live adversarial model in crowdsourcing to force workers to produce questions that BiDAF could not answer.",
            "performance": "Not given as a single-number in-text for DROP; used as adversarial model during data collection. For reference, BiDAF on SQuAD 1.1 is quoted as 66.8% EM in the paper.",
            "baseline_performance": "Part of reading-comprehension baselines that collectively achieved the paper's reported 'best baseline' F1 of 32.7% on DROP.",
            "improvement_over_baseline": "NA (baseline model).",
            "limitations_or_failures": "Cannot produce non-span answers (counts, arithmetic); 45% of DROP questions are not span-answerable and were omitted when training span-only models; thus BiDAF-like architectures are fundamentally limited for many DROP phenomena.",
            "ablation_or_analysis": "Used in adversarial annotation loop; the authors adapted BiDAF to use marginal-likelihood training over matching spans for DROP. No specific ablations beyond that adaptation are reported for BiDAF in this paper.",
            "uuid": "e3514.2",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "QANet",
            "name_full": "QANet",
            "brief_description": "A SQuAD-style reading-comprehension model combining local convolutions with global self-attention, used as a strong baseline and as the backbone for NAQANet.",
            "citation_title": "QANet: Combining local convolution with global self-attention for reading comprehension",
            "mention_or_use": "use",
            "model_name": "QANet",
            "model_description": "Transformer-like architecture combining convolutional layers (local features) with global self-attention blocks for reading comprehension; used in the paper both as a baseline and as the encoder component for NAQANet.",
            "model_size": null,
            "reasoning_task_name": "DROP (span-based baseline and backbone for NAQANet)",
            "reasoning_task_description": "Predict answer spans and (with NAQANet extensions) numeric outputs for questions requiring discrete reasoning over paragraphs.",
            "method_or_intervention": "Applied marginal-likelihood objective for non-unique spans; QANet hyperparameters adjusted due to GPU limits; QANet+ELMo variant concatenates pretrained ELMo embeddings; NAQANet extends QANet with numeric heads.",
            "performance": "On SQuAD the paper cites QANet (72.7% EM) and QANet+ELMo (78.7% EM). On DROP, QANet-derived baselines are part of the group achieving the best baseline F1 of 32.7%; NAQANet (QANet + numeric heads) achieves 47.0% F1.",
            "baseline_performance": "Baseline group (reading-comprehension models including QANet variants) best F1 = 32.7% on DROP.",
            "improvement_over_baseline": "NAQANet (built on QANet) adds numeric reasoning heads and attains +14.3% absolute F1 over the best baseline (47.0% vs 32.7%).",
            "limitations_or_failures": "Vanilla QANet cannot produce counts or arbitrary arithmetic; must be extended (as NAQANet) to handle numeric answer types; span-only models must drop ~45% of questions that are non-span answers.",
            "ablation_or_analysis": "Authors note hyperparameter adjustments (batch size, blocks in modeling layer) due to GPU constraints; QANet+ELMo uses pretrained ELMo embeddings. The NAQANet used the same hyperparameters as QANet for fair comparison.",
            "uuid": "e3514.3",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "QANet+ELMo",
            "name_full": "QANet with ELMo contextual embeddings",
            "brief_description": "QANet enhanced by concatenating pretrained ELMo contextual representations to input embeddings; used as a stronger SQuAD-style baseline on DROP.",
            "citation_title": "Deep contextualized word representations",
            "mention_or_use": "use",
            "model_name": "QANet + ELMo",
            "model_description": "QANet augmented with pretrained deep contextualized ELMo embeddings concatenated to the original token embeddings to improve representational capacity.",
            "model_size": null,
            "reasoning_task_name": "DROP (span-based baseline)",
            "reasoning_task_description": "Span-extraction reading comprehension; still limited for numeric/non-span answers without additional mechanisms.",
            "method_or_intervention": "Use of pretrained contextual embeddings (ELMo) to improve representations.",
            "performance": "QANet+ELMo cited SQuAD EM 78.7% in paper; on DROP it contributes to reading-comprehension baseline performance (best baseline group F1 = 32.7%).",
            "baseline_performance": null,
            "improvement_over_baseline": "Compared to base QANet, adding ELMo helps on SQuAD; on DROP the main gains on numeric reasoning required NAQANet-style numeric heads rather than only contextual embeddings.",
            "limitations_or_failures": "Cannot natively produce counts or arithmetic; still a span-extraction architecture so non-span DROP questions must be omitted for training.",
            "ablation_or_analysis": "No detailed ablation separating ELMo contribution on DROP; hyperparameters otherwise same as QANet baseline (except reported batch size/blocks changes).",
            "uuid": "e3514.4",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (large, uncased)",
            "brief_description": "A large pretrained bidirectional Transformer model that achieved state-of-the-art on many NLP tasks; used as a strong reading-comprehension baseline on DROP (converted to span-format).",
            "citation_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT (large, uncased)",
            "model_description": "Pretrained Transformer-based language model (bidirectional) fine-tuned for span-extraction QA; paper used an open-source large uncased BERT implementation from Hugging Face and converted DROP to SQuAD format by using the first matching span as gold.",
            "model_size": null,
            "reasoning_task_name": "DROP (span-extraction baseline)",
            "reasoning_task_description": "Span-extraction reading comprehension applied to DROP after conversion to SQuAD-like format; limited for non-span numeric answers.",
            "method_or_intervention": "Fine-tuned pretrained BERT large uncased as a span-extraction model; for DROP they converted dataset to SQuAD format by taking the first match as the gold span (for black-box BERT).",
            "performance": "Authors report that BERT, despite SQuAD SOTA, suffers large drops on DROP: 'BERT drops by more than 50 absolute F1 points' relative to SQuAD performance. Per-type dev-set numbers in Table 6: for 'Numbers' answers BERT Exact Match 14.5 and F1 14.8; for 'Single Span' BERT Exact Match 64.6 and F1 70.1; for '&gt;1 Spans' BERT F1 25.0.",
            "baseline_performance": "On SQuAD BERT reported 84.7% EM (cited in paper), but on DROP its numeric-answer handling is poor (see per-type numbers above).",
            "improvement_over_baseline": "BERT is part of the reading-comprehension baselines (collective best baseline F1 = 32.7%); NAQANet outperforms these span-only baselines notably on numeric answers.",
            "limitations_or_failures": "Severely degraded performance on DROP numeric questions: low Exact Match/F1 on 'Numbers' (14.5 EM / 14.8 F1) indicating inability of vanilla BERT span-extraction fine-tuning to handle numeric aggregation, multi-span answers, or arithmetic reasoning; also depends on dataset conversion heuristic (first match) which may be suboptimal.",
            "ablation_or_analysis": "Per-answer-type analysis (Table 6) reveals BERT's relative strengths (single-span answers) and weaknesses (numeric answers, multi-span). The authors note that span-only models had to omit ~45% of training questions that are not representable as passage spans.",
            "uuid": "e3514.5",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Heuristic baselines",
            "name_full": "Question-only, Paragraph-only, and Frequency-based heuristics",
            "brief_description": "Simple heuristic baselines that test for dataset artifacts: models trained with question input or paragraph input zeroed out, and a frequency-based majority predictor of common answers per question-word.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Heuristic baselines (question-only, paragraph-only, majority answer per question word)",
            "model_description": "Question-only and paragraph-only variants of QANet (zeroing out the other input) to detect dataset artifacts; a frequency-based baseline that predicts the top-3 most common answer spans per question-word from training data.",
            "model_size": null,
            "reasoning_task_name": "DROP (artifact checks / simple baselines)",
            "reasoning_task_description": "Simple prediction heuristics to measure annotation artifacts and answer distribution biases; do not perform explicit reasoning.",
            "method_or_intervention": "Input ablations (zeroing question or paragraph embeddings) and answer-frequency majority baseline per WH-word.",
            "performance": "Authors report that heuristic baselines do poorly on DROP, suggesting relatively small annotation biases; exact numeric results are not provided in the main text.",
            "baseline_performance": null,
            "improvement_over_baseline": "Not applicable; these are negative controls showing that simple artifacts do not explain performance of stronger models.",
            "limitations_or_failures": "As expected, these heuristics cannot perform the discrete reasoning DROP requires (counting, arithmetic, multi-span aggregation) and perform poorly.",
            "ablation_or_analysis": "These baselines act as an ablation to quantify dataset artifacts: question-only and paragraph-only QANet variants test how much signal comes from question vs passage; frequency baseline inspects answer distribution conditional on question-word.",
            "uuid": "e3514.6",
            "source_info": {
                "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
                "publication_date_yy_mm": "2019-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural semantic parsing with type constraints for semi-structured tables",
            "rating": 2
        },
        {
            "paper_title": "Compositional semantic parsing on semi-structured tables",
            "rating": 2
        },
        {
            "paper_title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision",
            "rating": 2
        },
        {
            "paper_title": "Neural programmer: Inducing latent programs with gradient descent",
            "rating": 2
        },
        {
            "paper_title": "Neural programmer-interpreters",
            "rating": 2
        },
        {
            "paper_title": "Bidirectional attention flow for machine comprehension",
            "rating": 1
        },
        {
            "paper_title": "QANet: Combining local convolution with global self-attention for reading comprehension",
            "rating": 1
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 1
        }
    ],
    "cost": 0.015995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</h1>
<p>Dheeru Dua ${ }^{\text {® }}$, Yizhong Wang ${ }^{\text {® }}$, Pradeep Dasigi ${ }^{\circ}$, Gabriel Stanovsky ${ }^{\circ \prime+}$, Sameer Singh ${ }^{\text {, }}$, and Matt Gardner ${ }^{\text {® }}$<br>${ }^{1}$ University of California, Irvine, USA<br>${ }^{\text {® }}$ Peking University, Beijing, China<br>${ }^{\circ}$ Allen Institute for Artificial Intelligence, Seattle, Washington, USA<br>${ }^{1}$ Allen Institute for Artificial Intelligence, Irvine, California, USA<br>${ }^{+}$University of Washington, Seattle, Washington, USA<br>ddua@uci.edu</p>
<h4>Abstract</h4>
<p>Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96 k question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve $32.7 \%$ $F_{1}$ on our generalized accuracy metric, while expert human performance is $96.4 \%$. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve $47.0 \% F_{1}$.</p>
<h2>1 Introduction</h2>
<p>The task of reading comprehension, where systems must understand a single passage of text well enough to answer arbitrary questions about it, has seen significant progress in the last few years, so much that the most popular datasets available for this task have been solved (Chen et al., 2016; Devlin et al., 2019). We introduce a substantially more challenging English reading comprehension dataset aimed at pushing the field towards more comprehensive analysis of paragraphs of text. In</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>this new benchmark, which we call DROP, a system is given a paragraph and a question and must perform some kind of Discrete Reasoning Over the text in the Paragraph to obtain the correct answer.</p>
<p>These questions that require discrete reasoning (such as addition, sorting, or counting; see Table 1) are inspired by the complex, compositional questions commonly found in the semantic parsing literature. We focus on this type of questions because they force a structured analysis of the content of the paragraph that is detailed enough to permit reasoning. Our goal is to further paragraph understanding; complex questions allow us to test a system's understanding of the paragraph's semantics.</p>
<p>DROP is also designed to further research on methods that combine distributed representations with symbolic, discrete reasoning. In order to do well on this dataset, a system must be able to find multiple occurrences of an event described in a question (presumably using some kind of soft matching), extract arguments from the events, then perform a numerical operation such as a sort, to answer a question like "Who threw the longest touchdown pass?".</p>
<p>We constructed this dataset through crowdsourcing, first collecting passages from Wikipedia that are easy to ask hard questions about, then encouraging crowd workers to produce challenging questions. This encouragement was partially through instructions given to workers, and partially through the use of an adversarial baseline: we ran a baseline reading comprehension method (BiDAF) (Seo et al., 2017) in the background as crowd workers were writing questions, requiring them to give questions that the baseline system could not correctly answer. This resulted in a dataset of 96,567 questions from a variety of categories in Wikipedia, with a particular emphasis on sports game summaries and history passages. The answers to the questions are required to be spans in the passage or</p>
<p>question, numbers, or dates, which allows for easy and accurate evaluation metrics.</p>
<p>We present an analysis of the resulting dataset to show what phenomena are present. We find that many questions combine complex question semantics with SQuAD-style argument finding; e.g., in the first question in Table 1, BiDAF correctly finds the amount the painting sold for, but does not understand the question semantics and cannot perform the numerical reasoning required to answer the question. Other questions, such as the fifth question in Table 1, require finding all events in the passage that match a description in the question, then aggregating them somehow (in this instance, by counting them and then performing an argmax). Very often entity coreference is required. Table 1 gives a number of different phenomena, with their proportions in the dataset.</p>
<p>We used three types of systems to judge baseline performance on DROP: (1) heuristic baselines, to check for biases in the data; (2) SQuAD-style reading comprehension methods; and (3) semantic parsers operating on a pipelined analysis of the passage. The reading comprehension methods perform the best, with our best baseline achieving $32.7 \%$ $F_{1}$ on our generalized accuracy metric, while expert human performance is $96.4 \%$. Finally, we contribute a new model for this task that combines limited numerical reasoning with standard reading comprehension methods, allowing the model to answer questions involving counting, addition and subtraction. This model reaches $47 \% F_{1}$, a $14.3 \%$ absolute increase over the best baseline system.</p>
<p>The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop.</p>
<h2>2 Related Work</h2>
<p>Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kociský et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang
et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of "multi-step" reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain datasets (Pampari et al., 2018; Šuster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions. ${ }^{1}$ One could argue that we are adding numerical reasoning as an "additional complexity", and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding.</p>
<p>Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding.</p>
<p>Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly.</p>
<p>Adversarial dataset construction We continue</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reasoning</th>
<th style="text-align: center;">Passage (some parts shortened)</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;">BiDAF</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Subtraction (28.8\%)</td>
<td style="text-align: center;">That year, his Untitled (1981), a painting of a haloed, black-headed man with a bright red skeletal body, depicted amid the artists signature scrawls, was sold by Robert Lehrman for $\mathbf{\$ 1 6 . 3}$ million, well above its $\mathbf{\$ 1 2}$ million high estimate.</td>
<td style="text-align: center;">How many more dollars was the Untitled (1981) painting sold for than the 12 million dollar estimation?</td>
<td style="text-align: center;">4300000</td>
<td style="text-align: center;">$\$ 16.3$ <br> million</td>
</tr>
<tr>
<td style="text-align: center;">Comparison (18.2\%)</td>
<td style="text-align: center;">In 1517, the seventeen-year-old King sailed to Castile. There, his Flemish court .... In May 1518, Charles traveled to Barcelona in Aragon.</td>
<td style="text-align: center;">Where did Charles travel to first, Castile or Barcelona?</td>
<td style="text-align: center;">Castile</td>
<td style="text-align: center;">Aragon</td>
</tr>
<tr>
<td style="text-align: center;">Selection (19.4\%)</td>
<td style="text-align: center;">In 1970, to commemorate the 100th anniversary of the founding of Baldwin City, Baker University professor and playwright Don Mueller and Phyllis E. Braun, Business Manager, produced a musical play entitled The Ballad Of Black Jack to tell the story of the events that led up to the battle.</td>
<td style="text-align: center;">Who was the University professor that helped produce The Ballad Of Black Jack, Ivan Boyd or Don Mueller?</td>
<td style="text-align: center;">Don <br> Mueller</td>
<td style="text-align: center;">Baker</td>
</tr>
<tr>
<td style="text-align: center;">Addition (11.7\%)</td>
<td style="text-align: center;">Before the UNPROFOR fully deployed, the HV clashed with an armed force of the RSK in the village of Nos Kalik, located in a pink zone near Šibenik, and captured the village at 4:45 p.m. on 2 March 1992. The JNA formed a battlegroup to counterattack the next day.</td>
<td style="text-align: center;">What date did the JNA form a battlegroup to counterattack after the village of Nos Kalik was captured?</td>
<td style="text-align: center;">3 March 1992</td>
<td style="text-align: center;">2 March 1992</td>
</tr>
<tr>
<td style="text-align: center;">Count (16.5\%) and Sort (11.7\%)</td>
<td style="text-align: center;">Denver would retake the lead with kicker Matt Prater nailing a 43-yard field goal, yet Carolina answered as kicker John Kasay ties the game with a 39-yard field goal. ... Carolina closed out the half with Kasay nailing a 44-yard field goal. ... In the fourth quarter, Carolina sealed the win with Kasay's 42-yard field goal.</td>
<td style="text-align: center;">Which kicker kicked the most field goals?</td>
<td style="text-align: center;">John <br> Kasay</td>
<td style="text-align: center;">Matt <br> Prater</td>
</tr>
<tr>
<td style="text-align: center;">Coreference <br> Resolution <br> (3.7\%)</td>
<td style="text-align: center;">James Douglas was the second son of Sir George Douglas of Pittendreich, and Elizabeth Douglas, daughter David Douglas of Pittendreich. Before 1543 he married Elizabeth, daughter of James Douglas, 3rd Earl of Morton. In 1553 James Douglas succeeded to the title and estates of his father-in-law.</td>
<td style="text-align: center;">How many years after he married Elizabeth did James Douglas succeed to the title and estates of his father-in-law?</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1553</td>
</tr>
<tr>
<td style="text-align: center;">Other <br> Arithmetic <br> (3.2\%)</td>
<td style="text-align: center;">Although the movement initially gathered some 60,000 adherents, the subsequent establishment of the Bulgarian Exarchate reduced their number by some $75 \%$.</td>
<td style="text-align: center;">How many adherents were left after the establishment of the Bulgarian Exarchate?</td>
<td style="text-align: center;">15000</td>
<td style="text-align: center;">60,000</td>
</tr>
<tr>
<td style="text-align: center;">Set of spans (6.0\%)</td>
<td style="text-align: center;">According to some sources 363 civilians were killed in Kavadarci, 230 in Negotino and 40 in Vatasha.</td>
<td style="text-align: center;">What were the 3 villages that people were killed in?</td>
<td style="text-align: center;">Kavadarci, <br> Negotino, <br> Vatasha</td>
<td style="text-align: center;">Negotino and 40 in Vatasha</td>
</tr>
<tr>
<td style="text-align: center;">Other (6.8\%)</td>
<td style="text-align: center;">This Annual Financial Report is our principal financial statement of accountability. The AFR gives a comprehensive view of the Department's financial activities ...</td>
<td style="text-align: center;">What does AFR stand for?</td>
<td style="text-align: center;">Annual Financial Report</td>
<td style="text-align: center;">one of the Big Four audit firms</td>
</tr>
</tbody>
</table>
<p>Table 1: Example questions and answers from the DROP dataset, showing the relevant parts of the associated passage and the reasoning required to answer the question.
a recent trend in creating datasets with adversarial baselines in the loop (Paperno et al., 2016; Minervini and Riedel, 2018; Zellers et al., 2018; Zhang et al., 2019; Zellers et al., 2019). In our case, instead of using an adversarial baseline to filter automatically generated examples, we use it in a crowdsourcing task, to teach crowd workers to avoid easy questions, raising the difficulty level of the questions they provide.</p>
<p>Neural symbolic reasoning DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning.</p>
<p>We present one such model in Section 6. Other related work along these lines has been done by Reed and de Freitas (2016), Neelakantan et al. (2016), and Liang et al. (2017).</p>
<h2>3 DROP Data Collection</h2>
<p>In this section, we describe our annotation protocol, which consists of three phases. First, we automatically extract passages from Wikipedia which are expected to be amenable to complex questions. Second, we crowdsource question-answer pairs on these passages, eliciting questions which require</p>
<p>discrete reasoning. Finally, we validate the development and test portions of DROP to ensure their quality and report inter-annotator agreement.</p>
<p>Passage extraction We searched Wikipedia for passages that had a narrative sequence of events, particularly with a high proportion of numbers, as our initial pilots indicated that these passages were the easiest to ask complex questions about. We found that National Football League (NFL) game summaries and history articles were particularly promising, and we additionally sampled from any Wikipedia passage that contained at least twenty numbers. ${ }^{2}$ This process yielded a collection of about 7,000 passages.</p>
<p>Question collection We used Amazon Mechanical Turk ${ }^{3}$ to crowdsource the collection of questionanswer pairs, where each question could be answered in the context of a single Wikipedia passage. In order to allow some flexibility during the annotation process, in each human intelligence task (HIT) workers were presented with a random sample of 5 of our Wikipedia passages, and were asked to produce a total of at least 12 question-answer pairs on any of these.</p>
<p>We presented workers with example questions from five main categories, inspired by questions from the semantic parsing literature (addition/subtraction, minimum/maximum, counting, selection and comparison; see examples in Table 1), to elicit questions that require complex linguistic understanding and discrete reasoning. In addition, to further increase the difficulty of the questions in DROP, we employed a novel adverserial annotation setting, where workers were only allowed to submit questions which a real-time QA model BiDAF could not solve. ${ }^{4}$</p>
<p>Next, each worker answered their own question with one of three answer types: spans of text from either question or passage, a date (which was common in history and open-domain text) and numbers, allowed only for questions which explicitly stated a specific unit of measurement (e.g., "How many yards did Brady run?"), in an attempt to simplify the evaluation process.</p>
<p>Initially, we opened our HITs to all United States</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Dataset statistics across the different splits.
workers and gradually reduced our worker pool to workers who understood the task and annotated it well. Each HIT paid 5 USD and could be completed within 30 minutes, compensating a trained worker with an average pay of 10 USD/ hour.</p>
<p>Overall, we collected a total of 96,567 questionanswer pairs with a total Mechanical Turk budget of 60k USD (including validation). The dataset was randomly partitioned by passage into training ( $80 \%$ ), development ( $10 \%$ ) and test ( $10 \%$ ) sets, so all questions about a particular passage belong to only one of the splits.</p>
<p>Validation In order to test inter-annotator agreement and to improve the quality of evaluation against DROP, we collected at least two additional answers for each question in the development and test sets.</p>
<p>In a separate HIT, workers were given context passages and a previously crowdsourced question, and were asked to either answer the question or mark it as invalid (this occurred for $0.7 \%$ of the data, which we subsequently filtered out). We found that the resulting inter-annotator agreement was good and on par with other QA tasks; overall Cohen's $\kappa$ was 0.74 , with 0.81 for numbers, 0.62 for spans, and 0.65 for dates.</p>
<h2>4 DROP Data Analysis</h2>
<p>In the following, we quantitatively analyze properties of passages, questions, and answers in DROP. Different statistics of the dataset are depicted in Table 2. Notably, questions have a diverse vocabulary of around 30k different words in our training set.</p>
<p>Question analysis To assess the question type distribution, we sampled 350 questions from the training and development sets and manually annotated the categories of discrete operations required to answer the question. Table 1 shows the distribution of these categories in the dataset. In addition, to get a better sense of the lexical diversity of questions in the dataset, we find the most frequent</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Answer Type</th>
<th style="text-align: center;">Percent</th>
<th style="text-align: right;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NUMBER</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">PERSON</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: right;">Jerry Porter</td>
</tr>
<tr>
<td style="text-align: left;">OTHER</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: right;">males</td>
</tr>
<tr>
<td style="text-align: left;">OTHER ENTITIES</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: right;">Seahawks</td>
</tr>
<tr>
<td style="text-align: left;">VERB PHRASE</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: right;">Tom arrived at Acre</td>
</tr>
<tr>
<td style="text-align: left;">DATE</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: right;">3 March 1992</td>
</tr>
</tbody>
</table>
<p>Table 3: Distribution of answer types in training set, according to an automatic named entity recognition.
trigram patterns in the questions per answer type. We find that the dataset offers a huge variety of linguistic constructs, with the most frequent pattern ("Which team scored") appearing in only $4 \%$ of the span type questions. For number type questions, the 5 most frequent question patterns all start with "How many", indicating the need to perform counting and other arithmetic operations. A distribution of the trigrams containing the start of the questions are shown in Figure 1.</p>
<p>Answer analysis To discern the level of passage understanding needed to answer the questions in DROP, we annotate the set of spans in the passage that are necessary for answering the 350 questions mentioned above. We find that on an average 2.18 spans need to be considered to answer a question and the average distance between these spans is 26 words, with $20 \%$ of samples needing at least 3 spans (see appendix for examples). Finally, we assess the answer distribution in Table 3, by running the part-of-speech tagger and named entity recognizer from spaCy ${ }^{5}$ to automatically partition all the answers into various categories. We find that a majority of the answers are numerical values and proper nouns.</p>
<h2>5 Baseline Systems</h2>
<p>In this section we describe the initial baselines that we evaluated on the DROP dataset. We used three types of baselines: state-of-the-art semantic parsers (§5.1), state-of-the-art reading comprehension models (§5.2), and heuristics looking for annotation artifacts (§5.3). We use two evaluation metrics to compare model performance: ExactMatch, and a numeracy-focused (macro-averaged) $F_{1}$ score, which measures overlap between a bag-of-words representation of the gold and predicted answers. We employ the same implementation of Exact-Match accuracy as used by SQuAD, which</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>removes articles and does other simple normalization, and our $F_{1}$ score is based on that used by SQuAD. Since DROP is numeracy-focused, we define $F_{1}$ to be 0 when there is a number mismatch between the gold and predicted answers, regardless of other word overlap. When an answer has multiple spans, we first perform a one-to-one alignment greedily based on bag-of-word overlap on the set of spans and then compute average $F_{1}$ over each span. When there are multiple annotated answers, both metrics take a max over all gold answers.</p>
<h3>5.1 Semantic Parsing</h3>
<p>Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP's questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the WikiTableQuestions tabular dataset (Pasupat and Liang, 2015).</p>
<p>Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntactic relations, (2) Open Information Extraction (Banko et al., 2007, Open IE), a shallow semantic representation which directly links predicates and arguments; and (3) Semantic Role Labeling (Carreras and Màrquez, 2005, SRL), which disambiguates senses for polysemous predicates and assigns predicate-specific argument roles. ${ }^{6}$ To adhere to KDG's structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles.</p>
<p>Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Distribution of the most popular question prefixes for two different subsets of the training data.
bers, and dates. In addition, it defines functions that operate on these elements, such as counters and filters. ${ }^{7}$ Following Krishnamurthy et al. (2017), we use the argument and return types of these functions to automatically induce a grammar to constrain the parser. We also add context-specific rules to produce strings occurring in both question and paragraph, and those paragraph strings that are neighbors of question tokens in the GloVe embedding space (Pennington et al., 2014), up to a cosine distance of $d .{ }^{8}$ The complete set of functions used in our language and their induced grammar can be found in the code release.</p>
<p>Training and inference During training, the KDG parser maximizes the marginal likelihood of a set of (possibly spurious) question logical forms that evaluate to the correct answer. We obtain this set by performing an exhaustive search over the grammar up to a preset tree depth. At test time, we use beam search to produce the most likely logical form, which is then executed to predict an answer.</p>
<h3>5.2 SQuAD-style Reading Comprehension</h3>
<p>We test four different SQuAD-style reading comprehension models on DROP: (1) BiDAF (Seo et al., 2017), which is the adversarial baseline</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>we used in data construction ( $66.8 \%$ EM on SQuAD 1.1); (2) QANet (Yu et al., 2018), currently the best-performing published model on SQuAD 1.1 without data augmentation or pretraining ( $72.7 \%$ EM); (3) QANet + ELMo, which enhances the QANet model by concatenating pretrained ELMo representations (Peters et al., 2018) to the original embeddings ( $78.7 \%$ EM); (4) BERT (Devlin et al., 2019), which recently achieved improvements on many NLP tasks with a novel pretraining technique ( $84.7 \%$ EM). ${ }^{9}$</p>
<p>These models require a few minor adaptations when training on DROP. While SQuAD provides answer indices in the passage, our dataset only provides the answer strings. To address this, we use the marginal likelihood objective function proposed by Clark and Gardner (2018), which sums over the probabilities of all the matching spans. ${ }^{10} \mathrm{We}$ also omitted the training questions which cannot be answered by a span in the passage ( $45 \%$ ), and therefore cannot be represented by these systems.</p>
<p>For the BiDAF baseline, we use the implementation in AllenNLP but change it to use the marginal objective. For the QANet model, our settings differ from the original paper only in the batch size (16 v.s. 32) and number of blocks in the modeling layer</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>(6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline.</p>
<h3>5.3 Heuristic Baselines</h3>
<p>A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only predict answer spans from either the question or the paragraph.</p>
<p>In addition, we devise a baseline that estimates the answer variance in DROP. We start by counting the unigram and bigram answer frequency for each wh question-word in the train set (as the first word in the question). The majority baseline then predicts an answer as the set of 3 most common answer spans for the input question word (e.g., for "when", these were "quarter", "end" and "October").</p>
<h2>6 NAQANet</h2>
<p>DROP is designed to encourage models that combine neural reading comprehension with symbolic reasoning. None of the baselines we described in Section 5 can do this. As a preliminary attempt toward this goal, we propose a numerically-aware QANet model, NAQANet, which allows the state-of-the-art reading comprehension system to produce three new answer types: (1) spans from the question; (2) counts; (3) addition or subtraction over numbers. To predict numbers, the model first predicts whether the answer is a count or an arithmetic expression. It then predicts the specific numbers involved in the expression. This can be viewed as the neural model producing a partially executed logical form, leaving the final arithmetic to a symbolic system. While this model can currently only handle a very limited set of operations, we believe this is a promising approach to combining neural
methods and symbolic reasoning. The model is trained by marginalizing over all execution paths that lead to the correct answer.</p>
<h3>6.1 Model Description</h3>
<p>Our NAQANet model follows the typical architecture of previous reading comprehension models, which is composed of embedding, encoding, passage-question attention, and output layers. We use the original QANet architecture for everything up to the output layer. This gives us a question representation $\mathbf{Q} \in \mathbb{R}^{m \times d}$, and a projected questionaware passage representation $\overline{\mathbf{P}} \in \mathbb{R}^{n \times d}$. We have four different output layers, for the four different kinds of answers the model can produce:</p>
<p>Passage span As in the original QANet model, to predict an answer in the passage we apply three repetitions of the QANet encoder to the passage representation $\overline{\mathbf{P}}$ and get their outputs as $\mathbf{M}<em 1="1">{0}, \mathbf{M}</em>$ respectively. Then the probabilities of the starting and ending positions from the passage can be computed as:}$, $\mathbf{M}_{2</p>
<p>$$
\begin{aligned}
&amp; \mathbf{p}^{\mathrm{p}, \text { start }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{M}<em 1="1">{0} ; \mathbf{M}</em>\right]\right)\right. \
&amp; \mathbf{p}^{\mathrm{p}, \text { end }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{M}<em 2="2">{0} ; \mathbf{M}</em>\right]\right)\right.
\end{aligned}
$$</p>
<p>where FFN is a two-layer feed-forward network with the RELU activation.</p>
<p>Question span Some questions in DROP have their answer in the question instead of the passage. To predict an answer from the question, the model first computes a vector $\mathbf{h}^{P}$ that represents the information it finds in the passage:</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\alpha}^{P}=\operatorname{softmax}\left(\mathbf{W}^{P} \overline{\mathbf{P}}\right) \
&amp; \mathbf{h}^{P}=\boldsymbol{\alpha}^{P} \overline{\mathbf{P}}
\end{aligned}
$$</p>
<p>Then it computes the probabilities of the starting and ending positions from the question as:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{p}^{\mathrm{q}, \text { start }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{Q} ; \mathbf{e}^{|Q|} \otimes \mathbf{h}^{P}\right]\right)\right. \
&amp; \mathbf{p}^{\mathrm{q}, \text { end }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{Q} ; \mathbf{e}^{|Q|} \otimes \mathbf{h}^{P}\right]\right)\right.
\end{aligned}
$$</p>
<p>where the outer product with the identity $\left(\mathbf{e}^{|Q|} \otimes \cdot\right)$ simply repeats $\mathbf{h}^{P}$ for each question word.</p>
<p>Count We model the capability of counting as a multi-class classification problem. Specifically, we consider ten numbers ( $0-9$ ) in this preliminary model and the probabilities of choosing these numbers is computed based on the passage vector $\mathbf{h}^{P}$ :</p>
<p>$$
\mathbf{p}^{\text {count }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\mathbf{h}^{P}\right)\right)
$$</p>
<p>Arithmetic expression Many questions in DROP require the model to locate multiple numbers in the passage and add or subtract them to get the final answer. To model this process, we first extract all the numbers from the passage and then learn to assign a plus, minus or zero for each number. In this way, we get an arithmetic expression composed of signed numbers, which can be evaluated to give the final answer.</p>
<p>To do this, we first apply another QANet encoder to $\mathbf{M}<em 3="3">{2}$ and get a new passage representation $\mathbf{M}</em>}$. Then we select an index over the concatenation of $\mathbf{M<em 3="3">{0}$ and $\mathbf{M}</em>$ and the probabilities of this number being assigned a plus, minus or zero are:}$, to get a representation for each number in this passage. The $i^{t h}$ number can be represented as $\mathbf{h}_{i}^{N</p>
<p>$$
\mathbf{p}<em i="i">{\mathbf{i}}^{\text {sign }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\mathbf{h}</em>\right)\right)
$$}^{N</p>
<p>Answer type prediction We use a categorical variable to decide between the above four answer types, with probabilities computed as:</p>
<p>$$
\mathbf{p}^{\text {type }}=\operatorname{softmax}\left(\operatorname{FFN}\left(\left[\mathbf{h}^{P}, \mathbf{h}^{Q}\right]\right)\right)
$$</p>
<p>where $\mathbf{h}^{Q}$ is computed over $\mathbf{Q}$, in a similar way as we did for $\mathbf{h}^{P}$. At test time, we first determine this answer type greedily and then get the best answer from the selected type.</p>
<h3>6.2 Weakly-Supervised Training</h3>
<p>For supervision, DROP contains only the answer string, not which of the above answer types is used to arrive at the answer. To train our model, we adopt the weakly supervised training method widely used in the semantic parsing literature (Berant et al., 2013a). We find all executions that evaluate to the correct answer, including matching passage spans and question spans, correct count numbers, as well as sign assignments for numbers. Our training objective is then to maximize the marginal likelihood of these executions. ${ }^{11}$</p>
<h2>7 Results and Discussion</h2>
<p>The performance of all tested models on the DROP dataset is presented in Table 4. Most notably, all models perform significantly worse than on other prominent reading comprehension datasets, while human performance remains at similar high</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: Performance of the different models on our development and test set, in terms of Exact Match (EM), and numerically-focused $F_{1}$ (§5). Both metrics are calculated as the maximum against a set of gold answers.
levels. ${ }^{12}$ For example, BERT, the current state-of-the-art on SQuAD, drops by more than 50 absolute F1 points. This is a positive indication that DROP is indeed a challenging reading comprehension dataset, which opens the door for tackling new and complex reasoning problems on a large scale.</p>
<p>The best performance is obtained by our NAQANet model. Table 6 shows that our gains are obtained on the challenging and frequent number answer type, which requires various complex types of reasoning. Future work may also try combining our model with BERT. Furthermore, we find that all heuristic baselines do poorly on our data, hopefully attesting to relatively small biases in DROP.</p>
<p>Difficulties of building semantic parsers We see that all the semantic parsing baselines perform quite poorly on DROP. This is mainly because of our pipeline of extracting tabular information from paragraphs, followed by the denotation-driven logical form search, can yield logical forms only for a subset of the training data. For SRL and syntactic dependency sentence representation schemes,</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Phenomenon</th>
<th style="text-align: left;">Passage Highlights</th>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Answer</th>
<th style="text-align: center;">Our <br> model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Subtraction <br> + Coreference</td>
<td style="text-align: left;">Twenty-five of his 150 men were <br> sick, and his advance stalled $\ldots$</td>
<td style="text-align: left;">How many of Bartolom de Ams- <br> queta's 150 men were not sick?</td>
<td style="text-align: left;">125</td>
<td style="text-align: center;">145</td>
</tr>
<tr>
<td style="text-align: left;">Count + Filter</td>
<td style="text-align: left;">Macedonians were the largest ethnic <br> group in Skopje, with 338,358 inhabi- <br> tants $\ldots$ Then came $\ldots$ Serbs (14,298 <br> inhabitants), Turks $(8,595)$, Bosniaks <br> $(7,585)$ and Vlachs $(2,557) \ldots$</td>
<td style="text-align: left;">How many ethnicities had less than <br> 10000 people?</td>
<td style="text-align: left;">3</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Domain <br> knowledge</td>
<td style="text-align: left;">Smith was sidelined by a torn pec- <br> toral muscle suffered during practice $\ldots$</td>
<td style="text-align: left;">How many quarters did Smith play?</td>
<td style="text-align: left;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Addition</td>
<td style="text-align: left;">culminating in the Battle of Vienna <br> of 1683, which marked the start of the <br> 15-year-long Great Turkish War $\ldots$</td>
<td style="text-align: left;">What year did the Great Turkish <br> War end?</td>
<td style="text-align: left;">1698</td>
<td style="text-align: center;">1668</td>
</tr>
</tbody>
</table>
<p>Table 5: Representative examples from our model's error analysis. We list the identified semantic phenomenon, the relevant passage highlights, a gold question-answer pair, and the erroneous prediction by our model.
the search was able to yield logical forms for $34 \%$ of the training data, whereas with OpenIE, it was only $25 \%$. On closer examination of a sample of 60 questions and the information extracted by the SRL scheme (the best performing of the three), we found that only $25 \%$ of the resulting tables contained information needed to the answer the questions. These observations show that high quality information extraction is a strong prerequisite for building semantic parsers for DROP. Additionally, the fact that this is a weakly supervised semantic parsing problem also makes training hard. The biggest challenge in this setup is the spuriousness of logical forms used for training, where the logical form evaluates to the correct denotation but does not actually reflect the semantics of the question. This makes it hard for the model trained on these spurious logical forms to generalize to unseen data. From the set of logical forms for a sample of 60 questions analyzed, we found that only 8 questions (13\%) contained non-spurious logical forms.</p>
<p>Error Analysis Finally, in order to better understand the outstanding challenges in DROP, we conducted an error analysis on a random sample of 100 erroneous NAQANet predictions. The most common errors were on questions which required complex type of reasoning, such as arithmetic operations (evident in $51 \%$ of the errors), counting (30\%), domain knowledge and common sense (23\%), co-reference (6\%), or a combination of different types of reasoning (40\%). See Table 5 for examples of some of the common phenomena.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">$(\%)$</th>
<th style="text-align: center;">Exact Match</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">QN+</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">QN+</td>
<td style="text-align: center;">BERT</td>
</tr>
<tr>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">1.57</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: center;">Numbers</td>
<td style="text-align: center;">61.94</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">14.8</td>
</tr>
<tr>
<td style="text-align: center;">Single Span</td>
<td style="text-align: center;">31.71</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">70.1</td>
</tr>
<tr>
<td style="text-align: center;">$&gt;1$ Spans</td>
<td style="text-align: center;">4.77</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17.13</td>
<td style="text-align: center;">25.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Dev set performance breakdown by different answer types; our model (NAQANet, marked as $Q N+$ ) vs. BERT, the best-performing baseline.</p>
<h2>8 Conclusion</h2>
<p>We have presented DROP, a dataset of complex reading comprehension questions that require Discrete Reasoning Over Paragraphs. This dataset is substantially more challenging than existing datasets, with the best baseline achieving only $32.7 \% \mathrm{~F} 1$, while humans achieve $96 \%$. We hope this dataset will spur research into more comprehensive analysis of paragraphs, and into methods that combine distributed representations with symbolic reasoning. We have additionally presented initial work in this direction, with a model that augments QANet with limited numerical reasoning capability, achieving 47\% F1 on DROP.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Noah Smith, Yoav Goldberg, and Jonathan Berant for insightful discussions that informed the direction of this work. The computations on beaker. org were supported in part by credits from Google Cloud.</p>
<h2>References</h2>
<p>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew G Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013a. Semantic parsing on freebase from question-answer pairs. In EMNLP.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013b. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533-1544.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP.</p>
<p>Xavier Carreras and Lluís Màrquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of CONLL, pages 152-164.</p>
<p>Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A thorough examination of the cnn/daily mail reading comprehension task.</p>
<p>David L Chen and Raymond J Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In $A A A I$, volume 2, pages $1-2$.</p>
<p>Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke S. Zettlemoyer. 2018. Quac: Question answering in context. In EMNLP.</p>
<p>Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In $A C L$.</p>
<p>Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Turney, and Daniel Khashabi. 2016. Combining retrieval, statistics, and inference to answer elementary science questions. In Thirtieth AAAI Conference on Artificial Intelligence.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, abs/1810.04805.</p>
<p>Timothy Dozat, Peng Qi, and Christopher D. Manning. 2017. Stanford's graph-based neural dependency parser at the conll 2017 shared task. In CoNLL Shared Task.</p>
<p>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. 2017. Allennlp: A deep semantic natural language processing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS). Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proc. of NAACL.</p>
<p>Luheng He, Kenton Lee, Mike Lewis, and Luke S. Zettlemoyer. 2017. Deep semantic role labeling: What works and what's next. In $A C L$.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523-533.</p>
<p>Mandar S. Joshi, Eunsol Choi, Daniel S. Weld, and Luke S. Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In $A C L$.</p>
<p>Divyansh Kaushik and Zachary Chase Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In EMNLP.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACLHLT.</p>
<p>Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. TACL, 6:317-328.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. TACL, 3:585-597.</p>
<p>Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 271-281.</p>
<p>Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. 2017. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In $A C L$.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In $A C L$.</p>
<p>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In CFCFPE@COLING.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>Pasquale Minervini and Sebastian Riedel. 2018. Adversarially regularising neural nli models to integrate logical background knowledge. In CoNLL.</p>
<p>Bhavana Dalvi Mishra, Lifu Huang, Niket Tandon, Wen-tau Yih, and Peter Clark. 2018. Tracking state changes in procedural text: A challenge dataset and models for process paragraph comprehension.</p>
<p>Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. 2016. Neural programmer: Inducing latent programs with gradient descent. ICLR.</p>
<p>Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, and Manfred Pinkal. 2018. Mcscript: a novel dataset for assessing machine comprehension using script knowledge. LREC Proceedings, 2018.</p>
<p>Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrqa: A large corpus for question answering on electronic medical records.</p>
<p>Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring a broad discourse context. $A C L$.</p>
<p>Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In ACL.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL-HLT.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. In $A C L$.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D. Manning. 2019. Coqa: A conversational question answering challenge. TACL.</p>
<p>Scott E. Reed and Nando de Freitas. 2016. Neural programmer-interpreters. ICLR.</p>
<p>Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. Duorc: Towards complex language understanding with paraphrased reading comprehension. In $A C L$.</p>
<p>Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. ICLR.</p>
<p>Gabriel Stanovsky, Julian Michael, Luke S. Zettlemoyer, and Ido Dagan. 2018. Supervised open information extraction. In NAACL-HLT.</p>
<p>Simon Šuster and Walter Daelemans. 2018. Clicr: a dataset of clinical case reports for machine reading comprehension.</p>
<p>Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In NAACL-HLT.</p>
<p>Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. TACL, 6:287-302.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP.</p>
<p>Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. In ACL'17.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. ICLR.</p>
<p>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In AAAI/IAAI, Vol. 2.</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. CVPR, abs/1811.10830.</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In EMNLP.</p>
<p>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2019. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ Due to the exponential search space and the possible noise, we only search the addition/subtraction of two numbers. Given this limited search space, the search and marginalization are exact.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{12}$ Human performance was estimated by the authors collectively answering 560 questions from the test set, which were then evaluated using the same metric as learned systems. This is in contrast to holding out one gold annotation and evaluating it against the other annotations, as done in prior work, which underestimates human performance relative to systems.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
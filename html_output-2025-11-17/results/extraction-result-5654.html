<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5654 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5654</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5654</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-3718212</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1803.02395v1.pdf" target="_blank">Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM</a></p>
                <p><strong>Paper Abstract:</strong> We propose a simple mathematical definition and new neural architecture for finding anomalies within discrete sequence datasets. Our model comprises of a modified LSTM autoencoder and an array of One-Class SVMs. The LSTM takes in elements from a sequence and creates context vectors that are used to predict the probability distribution of the following element. These context vectors are then used to train an array of One-Class SVMs. These SVMs are used to determine an outlier boundary in context space.We show that our method is consistently more stable and also outperforms standard LSTM and sliding window anomaly detection systems on two generated datasets.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5654.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5654.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero Boundary LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero Boundary LSTM (LSTM encoder + MLP decoder + per-symbol One-Class SVM array)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that trains an LSTM to predict next-element probability distributions, uses the LSTM's per-position context vectors as fixed-size representations, and trains a separate One-Class SVM for each alphabet symbol to define a zero-probability boundary in context space for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zero Boundary LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LSTM-based next-element predictor (embedding size 128, 5 LSTM layers with 128 hidden units each) producing per-position context vectors; an MLP decoder (seven hidden layers {128,128,64,32,64,128,256}, ReLU except linear bottleneck) maps contexts to next-element logits; context vectors for each symbol are collected and used to train a per-symbol One-Class SVM (Gaussian kernel, ν=0.001) to estimate density boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Supervised training of LSTM to predict next element (cross-entropy); extract fixed-size context vectors per position; train per-symbol One-Class SVMs on context vectors; classify a sequence as anomalous if any position's corresponding OCSVM score is below a threshold (t_σ, near but slightly less than zero).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete sequences (byte/character sequences); experiments on generated IPv4 address strings and generated JSON strings (nested JSON objects as strings).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Syntactic and structural sequence anomalies (zero-probability next-token events): for IPv4 — trivial non-digit/dot chars, incorrect length/number of groups, numeric groups >255, malformed dot placements; for JSON — misplaced/missing colon/comma/quote, incorrect nesting/braces).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Generated IPv4 addresses (10,000 training examples) and generated nested JSON strings (toy dataset with up to 4 nesting levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include true positives/true negatives and overall accuracy figures shown in the paper's figures; authors report that Zero Boundary LSTM outperforms the standard LSTM baseline across IPv4 anomaly categories and nearly matches the best standard LSTM results on JSONs. The paper emphasizes improved training stability (consistent decision boundary across epochs) rather than providing exhaustive numeric metrics in text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to a standard next-character LSTM and a naive n-gram sliding-window detector: Zero Boundary LSTM outperforms the standard LSTM in IPv4 experiments and is more stable during training; n-gram (short window) outperforms Zero Boundary on the 'digit' IPv4 anomaly because it can memorize valid numeric tokens, but n-grams fail on long-range anomalies (e.g., wrong number of groups) that Zero Boundary and standard LSTM detect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Method is sensitive to anomalies present in the training set (requires ν>0 workaround that increases false positives), OCSVMs scale poorly (training O(n^3)), false positives occur for context vectors near the decision boundary, and overall scaling to very large datasets is problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5654.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5654.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard LSTM next-character predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard LSTM trained to predict the next character/byte (used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard LSTM next-element prediction model trained with minibatch SGD and cross-entropy; anomalies are flagged when predicted next-token probability falls below a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Standard LSTM next-character predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM-based sequential model with embedding size 128 and 5 LSTM layers of 128 units each (baseline in experiments used similar architecture to Zero Boundary but with two MLP hidden layers of size 256 instead of the Zero Boundary bottleneck); outputs next-token logits over 257 symbols (256 bytes + delimiter).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Train LSTM to predict next symbol using cross-entropy; at inference, compute predicted probability of observed next token and flag sequences as anomalous if probability < threshold (threshold chosen empirically, e.g., 1 in 8103 in one experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete sequences (byte/character sequences); applied to generated IPv4 strings and generated JSON strings.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Syntactic and structural anomalies detectable as low-probability next tokens (same anomaly categories as Zero Boundary experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Generated IPv4 addresses and generated JSON strings (toy datasets used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated using true positive / true negative counts and accuracy in the paper's figures; authors report that the standard LSTM is less stable across training epochs, producing high variance in predicted probabilities for infrequent tokens which complicates choosing an anomaly threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Standard LSTM is outperformed by Zero Boundary LSTM on IPv4 categories and is roughly comparable on JSONs (Zero Boundary provides more stable decision boundaries); standard LSTM suffers from high variance for infrequent tokens causing instability in thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High variance in predicted probabilities for infrequent tokens due to SGD/cross-entropy training makes threshold selection unstable; less stable decision boundary across training runs and epochs compared to Zero Boundary approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5654.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5654.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>n-gram sliding-window</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive n-gram / sliding window anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional sliding-window n-gram model that flags as anomalous any short subsequence/window not seen (or unlikely) in training; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n-gram sliding-window detector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Counts or models fixed-length windows (n-grams) of tokens; windows not observed or below probability threshold are treated as anomalies; window size varied per experiment (e.g., size 4 for IPv4, size 3 for JSON experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Frequency / n-gram based scoring over fixed-size sliding windows; declare sequence anomalous if any window is improbable or unseen relative to training data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete sequences (strings); used on IPv4 and JSON string datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Local syntactic anomalies detectable within short contexts (e.g., invalid digit sequences); limited for long-range structural anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Generated IPv4 addresses and generated JSON strings (toy datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported by comparison in figures: n-gram model performed best on some local-digit IPv4 anomalies (can memorize valid numeric tokens) but failed to detect long-range anomalies like incorrect number of groups; authors report per-category performance comparisons in figures but do not provide full numeric tables in text.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms Zero Boundary LSTM on the specific 'digit' IPv4 anomaly due to memorization of short valid tokens, but fails on long-distance/structural anomalies that Zero Boundary and standard LSTM detect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot capture long-term dependencies; increasing window size requires much more data and causes many false positives; unsuitable for structural anomalies requiring long-range context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5654.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5654.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Per-symbol One-Class SVM (OCSVM) array</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Array of per-symbol One-Class Support Vector Machines (Gaussian kernel) trained on LSTM context vectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Density-boundary estimators (one OCSVM per alphabet symbol) trained on context vectors output by the LSTM encoder to estimate whether a context makes the given next symbol impossible (zero-probability) under the training language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Support vector method for novelty detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-Class SVM (per-symbol array, Gaussian kernel)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classic One-Class SVM formulation (Schölkopf et al.) using Gaussian (RBF) kernel K(x,y)=exp(-|x-y|^2/γ) and ν=0.001 in experiments; each OCSVM is trained on the set of context vectors observed before occurrences of its corresponding symbol.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Train one OCSVM per possible output symbol on the LSTM-derived context vectors that preceded that symbol in training; at inference, score the context vector corresponding to the observed next symbol by that symbol's OCSVM and compare to threshold t_σ to determine anomaly.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Fixed-size context vectors derived from discrete sequences (used for sequence anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Detects zero-probability contexts for specific next symbols—thus identifying syntactic/structural anomalies manifested as impossible next tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>OCSVMs are used as the decision mechanism; experimental performance is reported at the system level (with Zero Boundary LSTM) in terms of true positives/negatives and stability; training hyperparameter ν and threshold t_σ were tuned (ν=0.001 used).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>OCSVM-based boundary provides more stable decision boundaries than thresholding raw LSTM next-token probabilities, reducing variance-induced thresholding problems encountered by the standard LSTM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>OCSVM training scales poorly (O(n^3) runtime), making per-symbol OCSVM arrays impractical for large datasets; sensitivity to anomalies in the training set forces ν>0 (increasing false positives); decision thresholds near the boundary lead to false positives for borderline context vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM', 'publication_date_yy_mm': '2018-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One class support vector machines for detecting anomalous windows registry accesses <em>(Rating: 2)</em></li>
                <li>Support vector method for novelty detection <em>(Rating: 2)</em></li>
                <li>Anomaly detection: A survey. <em>(Rating: 2)</em></li>
                <li>Deep structured energy based models for anomaly detection. <em>(Rating: 1)</em></li>
                <li>Anomaly detection in aircraft data using Recurrent Neural Networks (RNN). <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5654",
    "paper_id": "paper-3718212",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "Zero Boundary LSTM",
            "name_full": "Zero Boundary LSTM (LSTM encoder + MLP decoder + per-symbol One-Class SVM array)",
            "brief_description": "A method that trains an LSTM to predict next-element probability distributions, uses the LSTM's per-position context vectors as fixed-size representations, and trains a separate One-Class SVM for each alphabet symbol to define a zero-probability boundary in context space for anomaly detection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Zero Boundary LSTM",
            "model_description": "An LSTM-based next-element predictor (embedding size 128, 5 LSTM layers with 128 hidden units each) producing per-position context vectors; an MLP decoder (seven hidden layers {128,128,64,32,64,128,256}, ReLU except linear bottleneck) maps contexts to next-element logits; context vectors for each symbol are collected and used to train a per-symbol One-Class SVM (Gaussian kernel, ν=0.001) to estimate density boundaries.",
            "model_size": null,
            "anomaly_detection_method": "Supervised training of LSTM to predict next element (cross-entropy); extract fixed-size context vectors per position; train per-symbol One-Class SVMs on context vectors; classify a sequence as anomalous if any position's corresponding OCSVM score is below a threshold (t_σ, near but slightly less than zero).",
            "data_type": "Discrete sequences (byte/character sequences); experiments on generated IPv4 address strings and generated JSON strings (nested JSON objects as strings).",
            "anomaly_type": "Syntactic and structural sequence anomalies (zero-probability next-token events): for IPv4 — trivial non-digit/dot chars, incorrect length/number of groups, numeric groups &gt;255, malformed dot placements; for JSON — misplaced/missing colon/comma/quote, incorrect nesting/braces).",
            "dataset_name": "Generated IPv4 addresses (10,000 training examples) and generated nested JSON strings (toy dataset with up to 4 nesting levels)",
            "performance_metrics": "Reported metrics include true positives/true negatives and overall accuracy figures shown in the paper's figures; authors report that Zero Boundary LSTM outperforms the standard LSTM baseline across IPv4 anomaly categories and nearly matches the best standard LSTM results on JSONs. The paper emphasizes improved training stability (consistent decision boundary across epochs) rather than providing exhaustive numeric metrics in text.",
            "baseline_comparison": "Compared to a standard next-character LSTM and a naive n-gram sliding-window detector: Zero Boundary LSTM outperforms the standard LSTM in IPv4 experiments and is more stable during training; n-gram (short window) outperforms Zero Boundary on the 'digit' IPv4 anomaly because it can memorize valid numeric tokens, but n-grams fail on long-range anomalies (e.g., wrong number of groups) that Zero Boundary and standard LSTM detect.",
            "limitations_or_failure_cases": "Method is sensitive to anomalies present in the training set (requires ν&gt;0 workaround that increases false positives), OCSVMs scale poorly (training O(n^3)), false positives occur for context vectors near the decision boundary, and overall scaling to very large datasets is problematic.",
            "uuid": "e5654.0",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Standard LSTM next-character predictor",
            "name_full": "Standard LSTM trained to predict the next character/byte (used as a baseline)",
            "brief_description": "A standard LSTM next-element prediction model trained with minibatch SGD and cross-entropy; anomalies are flagged when predicted next-token probability falls below a threshold.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Standard LSTM next-character predictor",
            "model_description": "LSTM-based sequential model with embedding size 128 and 5 LSTM layers of 128 units each (baseline in experiments used similar architecture to Zero Boundary but with two MLP hidden layers of size 256 instead of the Zero Boundary bottleneck); outputs next-token logits over 257 symbols (256 bytes + delimiter).",
            "model_size": null,
            "anomaly_detection_method": "Train LSTM to predict next symbol using cross-entropy; at inference, compute predicted probability of observed next token and flag sequences as anomalous if probability &lt; threshold (threshold chosen empirically, e.g., 1 in 8103 in one experiment).",
            "data_type": "Discrete sequences (byte/character sequences); applied to generated IPv4 strings and generated JSON strings.",
            "anomaly_type": "Syntactic and structural anomalies detectable as low-probability next tokens (same anomaly categories as Zero Boundary experiments).",
            "dataset_name": "Generated IPv4 addresses and generated JSON strings (toy datasets used in paper)",
            "performance_metrics": "Evaluated using true positive / true negative counts and accuracy in the paper's figures; authors report that the standard LSTM is less stable across training epochs, producing high variance in predicted probabilities for infrequent tokens which complicates choosing an anomaly threshold.",
            "baseline_comparison": "Standard LSTM is outperformed by Zero Boundary LSTM on IPv4 categories and is roughly comparable on JSONs (Zero Boundary provides more stable decision boundaries); standard LSTM suffers from high variance for infrequent tokens causing instability in thresholding.",
            "limitations_or_failure_cases": "High variance in predicted probabilities for infrequent tokens due to SGD/cross-entropy training makes threshold selection unstable; less stable decision boundary across training runs and epochs compared to Zero Boundary approach.",
            "uuid": "e5654.1",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "n-gram sliding-window",
            "name_full": "Naive n-gram / sliding window anomaly detector",
            "brief_description": "A traditional sliding-window n-gram model that flags as anomalous any short subsequence/window not seen (or unlikely) in training; used as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "n-gram sliding-window detector",
            "model_description": "Counts or models fixed-length windows (n-grams) of tokens; windows not observed or below probability threshold are treated as anomalies; window size varied per experiment (e.g., size 4 for IPv4, size 3 for JSON experiment).",
            "model_size": null,
            "anomaly_detection_method": "Frequency / n-gram based scoring over fixed-size sliding windows; declare sequence anomalous if any window is improbable or unseen relative to training data.",
            "data_type": "Discrete sequences (strings); used on IPv4 and JSON string datasets.",
            "anomaly_type": "Local syntactic anomalies detectable within short contexts (e.g., invalid digit sequences); limited for long-range structural anomalies.",
            "dataset_name": "Generated IPv4 addresses and generated JSON strings (toy datasets)",
            "performance_metrics": "Reported by comparison in figures: n-gram model performed best on some local-digit IPv4 anomalies (can memorize valid numeric tokens) but failed to detect long-range anomalies like incorrect number of groups; authors report per-category performance comparisons in figures but do not provide full numeric tables in text.",
            "baseline_comparison": "Outperforms Zero Boundary LSTM on the specific 'digit' IPv4 anomaly due to memorization of short valid tokens, but fails on long-distance/structural anomalies that Zero Boundary and standard LSTM detect.",
            "limitations_or_failure_cases": "Cannot capture long-term dependencies; increasing window size requires much more data and causes many false positives; unsuitable for structural anomalies requiring long-range context.",
            "uuid": "e5654.2",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        },
        {
            "name_short": "Per-symbol One-Class SVM (OCSVM) array",
            "name_full": "Array of per-symbol One-Class Support Vector Machines (Gaussian kernel) trained on LSTM context vectors",
            "brief_description": "Density-boundary estimators (one OCSVM per alphabet symbol) trained on context vectors output by the LSTM encoder to estimate whether a context makes the given next symbol impossible (zero-probability) under the training language.",
            "citation_title": "Support vector method for novelty detection",
            "mention_or_use": "use",
            "model_name": "One-Class SVM (per-symbol array, Gaussian kernel)",
            "model_description": "Classic One-Class SVM formulation (Schölkopf et al.) using Gaussian (RBF) kernel K(x,y)=exp(-|x-y|^2/γ) and ν=0.001 in experiments; each OCSVM is trained on the set of context vectors observed before occurrences of its corresponding symbol.",
            "model_size": null,
            "anomaly_detection_method": "Train one OCSVM per possible output symbol on the LSTM-derived context vectors that preceded that symbol in training; at inference, score the context vector corresponding to the observed next symbol by that symbol's OCSVM and compare to threshold t_σ to determine anomaly.",
            "data_type": "Fixed-size context vectors derived from discrete sequences (used for sequence anomaly detection).",
            "anomaly_type": "Detects zero-probability contexts for specific next symbols—thus identifying syntactic/structural anomalies manifested as impossible next tokens.",
            "dataset_name": null,
            "performance_metrics": "OCSVMs are used as the decision mechanism; experimental performance is reported at the system level (with Zero Boundary LSTM) in terms of true positives/negatives and stability; training hyperparameter ν and threshold t_σ were tuned (ν=0.001 used).",
            "baseline_comparison": "OCSVM-based boundary provides more stable decision boundaries than thresholding raw LSTM next-token probabilities, reducing variance-induced thresholding problems encountered by the standard LSTM baseline.",
            "limitations_or_failure_cases": "OCSVM training scales poorly (O(n^3) runtime), making per-symbol OCSVM arrays impractical for large datasets; sensitivity to anomalies in the training set forces ν&gt;0 (increasing false positives); decision thresholds near the boundary lead to false positives for borderline context vectors.",
            "uuid": "e5654.3",
            "source_info": {
                "paper_title": "Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM",
                "publication_date_yy_mm": "2018-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One class support vector machines for detecting anomalous windows registry accesses",
            "rating": 2,
            "sanitized_title": "one_class_support_vector_machines_for_detecting_anomalous_windows_registry_accesses"
        },
        {
            "paper_title": "Support vector method for novelty detection",
            "rating": 2,
            "sanitized_title": "support_vector_method_for_novelty_detection"
        },
        {
            "paper_title": "Anomaly detection: A survey.",
            "rating": 2,
            "sanitized_title": "anomaly_detection_a_survey"
        },
        {
            "paper_title": "Deep structured energy based models for anomaly detection.",
            "rating": 1,
            "sanitized_title": "deep_structured_energy_based_models_for_anomaly_detection"
        },
        {
            "paper_title": "Anomaly detection in aircraft data using Recurrent Neural Networks (RNN).",
            "rating": 1,
            "sanitized_title": "anomaly_detection_in_aircraft_data_using_recurrent_neural_networks_rnn"
        }
    ],
    "cost": 0.00926025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM</p>
<p>Chase Roberts roberc4@rpi.edu 
Rensselaer Polytechnic Institute</p>
<p>Manish Nair Bloomberg 
Rensselaer Polytechnic Institute</p>
<p>Arbitrary Discrete Sequence Anomaly Detection with Zero Boundary LSTM</p>
<p>We propose a simple mathematical definition and new neural architecture for finding anomalies within discrete sequence datasets. Our model comprises of a modified LSTM autoencoder and an array of One-Class SVMs. The LSTM takes in elements from a sequence and creates context vectors that are used to predict the probability distribution of the following element. These context vectors are then used to train an array of One-Class SVMs. These SVMs are used to determine an outlier boundary in context space. We show that our method is consistently more stable and also outperforms standard LSTM and sliding window anomaly detection systems on two generated datasets. * Work done while an intern at Bloomberg</p>
<p>Introduction</p>
<p>Discrete sequence anomaly detection is the process of discovering sequences in a dataset that do not conform to the pattern seen in a normal dataset. Detecting anomalies in discrete sequence data, especially text or byte datasets, is an incredibly difficult but valuable problem. Discovering these anomalies can help in many domains such as cybersecurity [3] and flight safety [4]. However, implementing these detection systems has proven to be a challenge. This is mainly because formally defining what makes a sequence anomalous is a non-trivial task. As such, metrics for an algorithm's success becomes ill-defined, mainly relying on human validation. In this paper, we give a theoretical definition of anomalies for discrete sequences and develop a machine learning architecture that is derived directly from this definition.</p>
<p>Prior Work</p>
<p>Existing discrete sequence anomaly detection systems are comprised of naive sliding window, Markov chain, or subsequence frequency analysis [1]. While these systems are effective in catching local anomalies within a sequence, they tend to to miss long-term dependencies on more structured sequences. Errors like grammatical mistakes in sentences may require looking back several dozen characters to determine an anomaly. Increasing window sizes for an n-gram model will either require significantly more training data or cause a huge increase in false positives.</p>
<p>Recently, more modern deep learning approaches have consisted of training an LSTM to predict the probability distribution of the next element in a sequence [14], and flagging that sequence as an anomaly if the predicted probability is below a certain threshold. However, if we use standard minibatch and cross entropy loss to train our LSTM, then the predict variance of infrequent values becomes quite large. This makes deciding the threshold for anomalies very difficult, and grays out the boundary between real and anomalous sequences. The instability of this decision bound makes this system difficult to use as an alerting system, as it either raises too many false positive alerts or misses an unacceptable number of true positives.</p>
<p>Background</p>
<p>Autoencoders</p>
<p>Autoencoders are an unsupervised way of decreasing the dimensionality of a given input sequence. They have been shown to outperform other dimensionality reduction techniques, such as PCA, for feature extraction [15]. Researchers have used autoencoders in the past for anomaly detection [5], image generation [6], and various input interpolation, including sentence interpolation [7].</p>
<p>Autoencoders work by training two functions: the "encoder" Enc(x) : R n → R m and the "decoder" Dec(x) : R m → R n where n &gt;&gt; m. These two functions are trained simultaneously by decreasing the loss function L = d(Dec(Enc(x)), x) where d is some reconstruction distance metric such as mean squared error or cross entropy. By forcing the neural network to reconstruct its own input through a bottleneck, the vector at the bottleneck becomes a low-dimensional rich description of the input data.</p>
<p>Past methods of anomaly detection using autoencoders would use the reconstruction loss as the metric for detection [5]. However, Vincent [8] showed that the reconstruction loss is actually equal to the gradient of the energy function for the underlying probability distribution. This causes certain anomalous inputs to be misclassified as normal inputs if they happen to have a near zero gradient. Energy-based models [9] have been developed to correct for this, but these models do not extend to discrete sequences.</p>
<p>One Class SVMs</p>
<p>One-Class SVMs (OCSVMs) [10] are density boundary estimators for fixed space inputs. Given a set of training vectors X and a value ν between 0 and 1, OCSVMs are able to create a boundary where a lower bound of 1 − ν of the vectors will be given a positive value and an upper bound of ν of the vectors will be given a negative value. Any input that is assigned a value less than 0 will be considered an outlier or anomalous input.</p>
<p>Training a OCSVM requires solving the quadratic programming problem:
min A ij α i α j K(x i , x j ) subject to 0 ≤ α i ≤ 1 νl , i α i = 1
where A is a vector of scalars α ∈ A, K is the Gaussian kernel K(x, y) = e |x−y| 2 /γ , x ∈ X are our datapoints , and l = |X|. The support vector expansion is then:
SV (y) = i α i K(x i , y) − ρ where ρ = j α j K(x j , x i ) for any given i where α i ≈ 0.
The decision function is usually the sign of SV (y). However, for our method we will be using a variable cutoff that may not be zero.</p>
<p>Proposed Method</p>
<p>Theoretical Foundation</p>
<p>Let Σ represent our finite alphabet. Let x ∈ Σ n represent a discrete sequence (we implicitly assume that each of our sequences start and end with a special delimiter element). Let L = {x 1 , x 2 , ..., x m } be the language of our system. We define x as an anomaly simply if:</p>
<p>x ∈ L Equivalently, it also follows that x is a anomaly if there exists x i ∈ x such that:
P (x i |x 1 , ..., x i−1 ; L) = 0
From this definition, we wish to build a neural network architecture to approximate the function f (x; L) where:
f (x; L) = 1, if ∃x i ∈ x : P (x i |x 1 , ..., x i−1 ; L) = 0 0, otherwise
Since we require a probability of zero, it follows that:
P (x i |x 1 , ..., x i−1 ; L) = P (x 1 , ..., x i−1 |x i ; L) = 0
Because we have now flipped the conditional of the probability distribution, we can now use a separate probability distribution for each element in our alphabet. Because of this, f (x) is equivalently:
f (x; L) = 1, if ∃x i ∈ x : P xi (x 1 , ..., x i−1 ; L) = 0 0, otherwise
where P x is the probability distribution under a specified byte x. If we are able to reduce the input for our probability distribution {x 1 , ..., x i−1 } into a vector of fixed size, then we are able to use standard density estimators for P x . In the following section, we describe exactly how we achieved this. We define:</p>
<p>Zero Boundary LSTM
E(x; θ E ) : Σ n → R n×e D(y; θ D ) : R e → R |Σ| O σ (cv) : R e → R where E(x; θ E )
is an LSTM encoder, D(y; θ D ) is a MLP decoder, and O σ (cv) is the OCSVM for element σ ∈ Σ, n is the length of the sequence x, and e is the size of our bottleneck. We define the OCSVM array as O = {O σ ∀σ ∈ Σ}. We also define E(x; θ E ) i as the ith row of E(x; θ E ). This vector will also be referenced as a context vector, as it encodes the context up to the ith element.</p>
<p>Normal LSTM autoencoders are trained by reconstructing their own input. However, as a modification, our method outputs the expected probability distribution for the i+1 element given the first i elements. Thus, we can then learn θ E and θ D by using stochastic gradient decent where our loss is the cross entropy loss of the predicted character.
L = n−1 i=1 − log P (x i+1 |D(E(x; θ E ) i ; θ D ))
We will now describe how to train the OCSVM array. Let θ E and θ D be pretrained parameters. Let X = {x 1 , ..., x N } ⊆ L be our training set. Let Y = {Y σ ∀σ ∈ Σ} where Y σ is a set of context vectors such that:
Y σ = {E(x; θ E ) i : x i = σ ∀x ∈ X}
We can then train each OCSVM O σ ∈ O with the context vector set from the corresponding Y σ ∈ Y using the method from Schölkopf [10].</p>
<p>We now describe how to approximate the function f (x). Given, θ E and O, we define g ≈ f as
g(x) = 1, if ∃x i ∈ x : O xi (E(x; θ E ) i−1 ) &lt; t σ 0
, otherwise where t σ is a threshold hyperparameter used to control the Type I error for each OCSVM. The value tends to be slightly less than zero.</p>
<p>The motivation behind our architecture comes from Heller et al. [2]. They stated that the failure of their window registry anomaly detection came from the fact that their kernel was unable to incorporate prior distributions of features. By pretraining our LSTM autoencoder to predict the distribution of the following element, the vector at the bottleneck now incorporates all of our prior information of both the sequence and the data distribution.</p>
<p>Experiments</p>
<p>We compare our method against two other anomaly detection algorithms: standard LSTM next character prediction and naive sliding window. Our experiments consist of two generated toy datasets. The toy datasets are generated IPv4 addresses and generated string only JSON objects.</p>
<p>Our LSTMs were implemented using tensorflow [11] and we used the scikit learn implementation of OCSVM [12]. Our OCSVMs were all trained with ν = 0.001 and the remaining parameters were left as their defaults. For both of our LSTMs, we used an embedding size of 128 for each of the elements in our alphabet. Our LSTMs used 5 layered cells with 128 hidden units in each layer. For the Zero Boundary LSTM, we used an MLP bottleneck with seven hidden layers of shape {128, 128, 64, 32, 64, 128, 256}. Each layer used the ReLU activation function except for the bottleneck layer which used linear activation. The standard LSTM used as a baseline has the same architecture and training procedure as the Zero Boundary LSTM, except we use two hidden layers of size 256 rather than a bottleneck. Both LSTMs used a logits of size 257 (256 possible bytes + 1 for special start/end of sequence element). We trained both networks with the Adam optimizer [16] with default parameters. The cutoff used for anomaly detection is variable and we explicitly state the cutoff used for each experiment. Our naive sliding window used a different window size for each experiment and we only publish on the window size that had the best results.</p>
<p>IPv4</p>
<p>In our first experiment, all of our models were trained on 10,000 randomly generated IPv4 addresses. These strings were generated by selecting four random integers between 0 -255 uniformly and placing a dot between each number. We then created four different classes of anomalies to try to detect: trivial, length, digit, and dot placement. Trivial anomalies are anomalies that contain characters other than a digit or a dot. Length anomalies are anomalies where the IP address was either cut off too soon or extends for more than four digit groupings. Digit anomalies are strings that contain digit groupings of values larger than 255. Finally, dot placement anomalies are anomalies where a string either contains a double dot (for example: "123.123..123.123") or either starts or ends with a dot.</p>
<p>The t σ values of our Zero Boundary LSTM were set to the lowest value calculated from our training and validation sets. The cutoff for the standard LSTM was set to be 1 in 8103. Both LSTMs were trained with two epochs of the training data. The n-gram model used a window of size four. The results for this experiment can be seen in Figure 2.</p>
<p>JSONs</p>
<p>The next toy dataset experiment used randomly generated JSON strings. These strings contain nested objects up to four levels deep. Each object contains between zero and four random entries. The In this experiment, we trained both LSTMs for four epochs of the data. We used an n-gram window size of three. The results of this experiment can be seen in Figure 3. Our Zero Boundary LSTM was able to nearly match the performance of the normal LSTM. However, these results are from highest performing models after many attempts and hyper parameter searches. One of the main advantages of our system is the stability we see with training.</p>
<p>In the next experiment, we showcased the models trained with different epoch values. As you can see in Figure 4, the Zero Boundary LSTM has a much easier time consistently finding a stable decision boundary than the normal LSTM.</p>
<p>Future Work</p>
<p>One of the largest drawbacks to our system is its intolerance to having anomalies within our training set. To get around this, we use a ν value that is not zero. However, this workaround also tends to increase false positive rating, especially for sequences that have context vectors right on the edge of the decision bound. Also, using OCSVMs as our density estimators makes scaling to larger datasets difficult, as OCSVM have a training run time of O(n 3 ). Solving these two issues will be a great improvement to our system and may even make unsupervised anomaly detection in areas like network traffic finally viable.</p>
<p>Conclusion</p>
<p>In this paper, we developed a mathematical definition of anomalies for discrete sequence datasets and created a neural network architecture to approximate this definition. Our method is able to give a stable decision boundary that does not suffer from the variance of SGD training while also  outperforming or matching the performance of both the sliding window and standard unsupervised LSTM algorithms. Even with these successes, our system still suffers heavily when scaled to very large datasets due to the cubic nature of OCSVMs. In order to use this system for more practical situations, this will need to be solved.</p>
<p>Figure 1 :
1A diagram of our neural network architecture. Here, to detect if element x 5 is an anomaly, we calculate the context vector up to x 5 and pass it into the corresponding OCSVM.</p>
<p>Figure 2 :
2Experimental results with our IPv4 dataset. Here, we see that our Zero Boundary LSTM outperforms the standard LSTM algorithm in all categories. The Zero boundary does not perform as well as the n-gram model for anomalous digit dataset, as the ngram has the capacity to memorize all possible acceptable numbers. However, due to the window size being so short, n-grams can not detect anomalies where there are too many digit groupings, unlike the Zero Boundary and normal LSTMs. entries for any given JSON object are either lowercase strings with lowercase string keys, or nested JSON objects with lowercase string keys. The probability of nesting for any given entry was set to be 1 in 5. We then created four different classes of anomalies to detect: colon, comma, quote, and nesting. Colon anomalies are anomalies where a colon is misplaced. Comma anomalies are when commas are either incorrectly placed or missing. Quote anomalies are where quote markers are either misplaced or completely missing. Nesting anomalies are when the curly braces do not add up correctly.</p>
<p>Figure 4 :
4Training stability results from our Zero Boundary LSTM and normal LSTM models. Accuracies are determined from the same datasets used for the other JSONs experiment.</p>
<p>Figure 3: Experimental results with our JSON dataset. Results are from the best models that we could train.Zero Boundary True Positives Zero Boundary True NegativesNormal True Positives Normal True NegativesColon </p>
<p>Comma 
Nesting 
Quote 
Test Set </p>
<p>960 </p>
<p>980 </p>
<p>1,000 </p>
<p>Dataset Type </p>
<p>Count </p>
<p>JSONs </p>
<p>Zero Boundary 
Normal LSTM 
n-gram-3 </p>
<p>Epoch 1 
Epoch 2 
Epoch 3 
Epoch 4 </p>
<p>0.4 </p>
<p>0.6 </p>
<p>0.8 </p>
<p>1 </p>
<p>Epochs </p>
<p>Percentage </p>
<p>JSON Training Stability </p>
<p>AcknowledgmentsWe would like to thank Raphael Meyer, Connie Yee and Sheryl Zhang of Bloomberg for their valuable contributions to this work.
Anomaly detection: A survey. Varun Chandola, Arindam Banerjee, Vipin Kumar, ACM computing surveys (CSUR). 4115Chandola, Varun, Arindam Banerjee, and Vipin Kumar. "Anomaly detection: A survey." ACM computing surveys (CSUR) 41.3 (2009): 15.</p>
<p>One class support vector machines for detecting anomalous windows registry accesses. Katherine A Heller, Proc. of the workshop on Data Mining for Computer Security. of the workshop on Data Mining for Computer Security9Heller, Katherine A., et al. "One class support vector machines for detecting anomalous windows registry accesses." Proc. of the workshop on Data Mining for Computer Security. Vol. 9. 2003.</p>
<p>Anomalous payload-based network intrusion detection. Ke Wang, Salvatore J Stolfo, 4Wang, Ke, and Salvatore J. Stolfo. "Anomalous payload-based network intrusion detection." RAID. Vol. 4. 2004.</p>
<p>Discovering atypical flights in sequences of discrete flight parameters. Budalakoti, Ashok N Suratna, Ram Srivastava, Akella, Aerospace Conference. IEEEBudalakoti, Suratna, Ashok N. Srivastava, and Ram Akella. "Discovering atypical flights in sequences of discrete flight parameters." Aerospace Conference, 2006 IEEE. IEEE, 2006.</p>
<p>Variational Autoencoder based Anomaly Detection using Reconstruction Probability. Jinwon An, Sungzoon Cho, Technical ReportAn, Jinwon, and Sungzoon Cho. Variational Autoencoder based Anomaly Detection using Reconstruction Probability. Technical Report, 2015.</p>
<p>Optimizing neural networks that generate images. Tijmen Tieleman, Diss. University of Toronto (CanadaTieleman, Tijmen. Optimizing neural networks that generate images. Diss. University of Toronto (Canada), 2014.</p>
<p>Generating sentences from a continuous space. Samuel R Bowman, arXiv:1511.06349arXiv preprintBowman, Samuel R., et al. "Generating sentences from a continuous space." arXiv preprint arXiv:1511.06349 (2015).</p>
<p>A connection between score matching and denoising autoencoders. Pascal Vincent, Neural computation. 23Vincent, Pascal. "A connection between score matching and denoising autoencoders." Neural computation 23.7 (2011): 1661-1674.</p>
<p>Deep structured energy based models for anomaly detection. Shuangfei Zhai, International Conference on Machine Learning. Zhai, Shuangfei, et al. "Deep structured energy based models for anomaly detection." International Confer- ence on Machine Learning. 2016.</p>
<p>Advances in neural information processing systems. Bernhard Schölkopf, Support vector method for novelty detectionSchölkopf, Bernhard, et al. "Support vector method for novelty detection." Advances in neural information processing systems. 2000.</p>
<p>Tensorflow: Large-scale machine learning on heterogeneous distributed systems. Martín Abadi, arXiv:1603.04467arXiv preprintAbadi, Martín, et al. "Tensorflow: Large-scale machine learning on heterogeneous distributed systems." arXiv preprint arXiv:1603.04467 (2016).</p>
<p>Scikit-learn: Machine learning in Python. Fabian Pedregosa, Journal of Machine Learning Research. 12Pedregosa, Fabian, et al. "Scikit-learn: Machine learning in Python." Journal of Machine Learning Research 12.Oct (2011): 2825-2830.</p>
<p>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. Lantao Yu, Yu, Lantao, et al. "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient." AAAI. 2017.</p>
<p>Anomaly detection in aircraft data using Recurrent Neural Networks (RNN). Anvardh Nanduri, Lance Sherry, IEEEIntegrated Communications Navigation and Surveillance (ICNSNanduri, Anvardh, and Lance Sherry. "Anomaly detection in aircraft data using Recurrent Neural Networks (RNN)." Integrated Communications Navigation and Surveillance (ICNS), 2016. IEEE, 2016.</p>
<p>Unsupervised learning of invariant feature hierarchies with applications to object recognition. Fu Huang, Y-Lan Jie, Yann Boureau, Lecun, IEEEComputer Vision and Pattern RecognitionHuang, Fu Jie, Y-Lan Boureau, and Yann LeCun. "Unsupervised learning of invariant feature hierarchies with applications to object recognition." Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on. IEEE, 2007.</p>
<p>Adam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintKingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic optimization." arXiv preprint arXiv:1412.6980 (2014).</p>            </div>
        </div>

    </div>
</body>
</html>
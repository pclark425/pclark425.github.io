<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2699 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2699</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2699</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-269741195</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.06059v1.pdf" target="_blank">A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds</a></p>
                <p><strong>Paper Abstract:</strong> Open-ended worlds are those in which there are no pre-specified goals or environmental reward signal. As a consequence, an agent must know how to perform a multitude of tasks. However, when a new task is presented to an agent, we expect it to be able to reuse some of what it knows from previous tasks to rapidly learn that new task. We introduce a novel technique whereby policies for different a priori known tasks are combined into a Mixture-of-Experts model with an attention mechanism across a mix of frozen and unfrozen experts. The model learns when to attend to frozen task-specific experts when appropriate and learns new experts to handle novel situations. We work in an open-ended text-based environment in which the agent is tasked with behaving like different types of character roles and must rapidly learn behaviors associated with new character role types. We show that our agent both obtains more rewards in the zero-shot setting, and discovers these rewards with greater sample efficiency in the few-shot learning settings.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2699.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2699.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph Augmented Advantage Actor-Critic (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Advantage Actor-Critic RL agent augmented with an external, persistent knowledge graph that is updated from observations and embedded via graph-attention; used as the base policy architecture and as frozen experts in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An actor-critic (A2C) agent that encodes textual observations with a GRU, constructs and maintains a persistent knowledge graph (world KG) of observed facts/relations, embeds the KG via a graph-attention network, and uses the combined representation to produce verb/template and object distributions for text-game actions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (custom role-playing game built in LIGHT environment)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>An open-ended text-based role-playing environment (built on LIGHT) with many NPCs, locations and objects; tasks are role-aligned behaviors (e.g., thief, hunter, adventurer) and reaching a goal room, with rewards for role-specific actions in particular locations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (knowledge graph, persistent world KG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Knowledge graph (nodes and relations) representing world state; embeddings produced via a graph-attention network and combined with GRU observation embedding into a single representation vector.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Facts and relations about rooms, objects in rooms, inventory items, NPCs and their relations — i.e., observed world state used as persistent memory; also story KGs used for Story Shaping are stored/represented similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Implicit embedding and attention-based readout: the world KG is embedded with a graph-attention mechanism and combined with the GRU observation embedding to produce the agent state representation (no explicit key-value retrieval procedure specified).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated online as the agent explores: at each time step observations update the world knowledge graph (i.e., after each action/observation the KG is augmented with newly observed facts/relations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Persistent state representation for policy conditioning (action/template/object distributions), shaping intrinsic reward via similarity between story KGs and world KG for role alignment, and informing critics (value estimates) used in the Mixture-of-Experts attention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: KG-A2C-based experts (used as frozen experts in the MoE) provide strong zero-shot signals and, when composed in the MoE, lead to higher zero-shot and faster few-shot learning than training a new KG-A2C from scratch; exact numeric metrics are not reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative baseline: KG-A2C trained from scratch (i.e., single new agent without pre-trained experts) and fine-tuned single experts perform worse — fine-tuned experts often fail to find additional role-specific rewards beyond their pre-trained behaviors, and the new KG-A2C finds some early rewards but fails to obtain rewards deeper in the environment; no numeric values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The persistent world KG provides a useful state representation that enables story-shaped intrinsic rewards and role transfer: pre-trained KG-A2C experts capture behavior-relevant knowledge that can be leveraged zero-shot via value seeding and attention. Memory is effective when experts' stored knowledge is relevant to the new role; the MoE leverages experts' KG-informed policies to accelerate few-shot learning. The paper does not empirically isolate KG utility via explicit ablation, so findings are qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2699.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2699.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoE (KG-A2C Mixture-of-Experts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Experts agent composed of frozen KG-A2C experts plus a trainable KG-A2C expert</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed agent that mixes several frozen, pre-trained KG-A2C role experts with an additional trainable ('hot') KG-A2C expert via an attention module; uses critics' value estimates to seed expert probabilities and a graph-based KG state representation from each expert.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mixture-of-Experts (MoE) KG-A2C agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An ensemble agent that at each time step feeds the shared observation representation into multiple frozen pre-trained KG-A2C experts and one trainable KG-A2C expert; each expert outputs logits for templates/objects and a critic value; action logits are scaled by each expert's critic (value seeding), projected and combined by an attention module (separately for template and object distributions), averaged with the trainable expert's logits, and sampled to produce actions.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>LIGHT (custom role-playing game built in LIGHT environment)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same open-ended text role-playing setting; tasks require role-consistent behaviors (e.g., thief vs hunter) with rewards given for role-specific actions and reaching a goal room; some roles require behaviors deeper in the map.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (each KG-A2C expert uses a persistent world knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Each expert maintains/embeds a world knowledge graph (nodes/relations) using a graph-attention embedding; the MoE uses the experts' outputs and critics — it does not introduce a new memory structure beyond the experts' KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Per-expert world KG contents: observed rooms, objects, inventory items, NPCs and their relations; story KGs are used offline to shape expert training but the running MoE uses experts' world KGs for state embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Integrated embedding and attention: expert KGs are embedded via graph-attention and combined with recurrent observation embeddings to form the per-expert action distributions and value estimates; the MoE attention operates over experts' action distributions (already conditioned on their KGs) rather than performing explicit KG retrieval across experts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Each expert's world KG is updated online as the agent explores (i.e., after each observation/action the corresponding expert's KG reflects newly observed facts); frozen experts' parameters do not change but their KGs are constructed from the current episode observations when used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Experts' KGs provide state grounding that informs action logits and critic values; value-seeded expert logits guide the MoE attention, enabling reuse of previous role-aligned memories/strategies for zero-shot and few-shot transfer; story KGs (offline) are used during pre-training to shape experts so they store role-relevant memories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: The MoE (built from KG-A2C experts) achieves higher zero-shot scores and faster few-shot learning than single KG-A2C baselines (trained from scratch) and fine-tuned single experts; the MoE discovers rewards earlier and attains higher final scores in the tested blended and partial-blend target roles. The paper reports this in aggregate/plots but does not provide precise numeric scores in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative baselines without the MoE composition (but still using KG-A2C single agents) perform worse: fine-tuned experts typically fail to find new role rewards, and a newly trained KG-A2C finds only near-start rewards and fails deeper; no explicit experiment removes the knowledge-graph memory itself to compare performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Memory (expert KGs) is effective because pre-trained experts store role-aligned behaviors that can be reused via value-seeded attention; value seeding (using critics) combined with KG-informed expert outputs gives strong zero-shot signals. Memory-driven transfer is ineffective when none of the experts' stored knowledge is relevant (adversarial/irrelevant-expert setting) because experts bias attention away from promising exploration; adding irrelevant (random) experts causes only marginal early slowdowns if relevant experts exist, but performance degrades drastically if no relevant experts are available.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit limitations of the KG memory are empirically analyzed; however, the MoE faces exploration and attention-related failures (expert starvation, premature convergence) caused by value seeding and strong expert signals — these are not presented as KG structural failures but as interaction effects between critics, attention, and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Story shaping: Teaching agents human-like behavior with stories <em>(Rating: 2)</em></li>
                <li>How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds <em>(Rating: 1)</em></li>
                <li>GATA <em>(Rating: 1)</em></li>
                <li>Q*BERT <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2699",
    "paper_id": "paper-269741195",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "KG-A2C",
            "name_full": "Knowledge-Graph Augmented Advantage Actor-Critic (KG-A2C)",
            "brief_description": "An Advantage Actor-Critic RL agent augmented with an external, persistent knowledge graph that is updated from observations and embedded via graph-attention; used as the base policy architecture and as frozen experts in this paper's experiments.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "An actor-critic (A2C) agent that encodes textual observations with a GRU, constructs and maintains a persistent knowledge graph (world KG) of observed facts/relations, embeds the KG via a graph-attention network, and uses the combined representation to produce verb/template and object distributions for text-game actions.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (custom role-playing game built in LIGHT environment)",
            "game_description": "An open-ended text-based role-playing environment (built on LIGHT) with many NPCs, locations and objects; tasks are role-aligned behaviors (e.g., thief, hunter, adventurer) and reaching a goal room, with rewards for role-specific actions in particular locations.",
            "uses_memory": true,
            "memory_type": "graph-based memory (knowledge graph, persistent world KG)",
            "memory_structure": "Knowledge graph (nodes and relations) representing world state; embeddings produced via a graph-attention network and combined with GRU observation embedding into a single representation vector.",
            "memory_content": "Facts and relations about rooms, objects in rooms, inventory items, NPCs and their relations — i.e., observed world state used as persistent memory; also story KGs used for Story Shaping are stored/represented similarly.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Implicit embedding and attention-based readout: the world KG is embedded with a graph-attention mechanism and combined with the GRU observation embedding to produce the agent state representation (no explicit key-value retrieval procedure specified).",
            "memory_update_strategy": "Updated online as the agent explores: at each time step observations update the world knowledge graph (i.e., after each action/observation the KG is augmented with newly observed facts/relations).",
            "memory_usage_purpose": "Persistent state representation for policy conditioning (action/template/object distributions), shaping intrinsic reward via similarity between story KGs and world KG for role alignment, and informing critics (value estimates) used in the Mixture-of-Experts attention.",
            "performance_with_memory": "Qualitative: KG-A2C-based experts (used as frozen experts in the MoE) provide strong zero-shot signals and, when composed in the MoE, lead to higher zero-shot and faster few-shot learning than training a new KG-A2C from scratch; exact numeric metrics are not reported in the paper.",
            "performance_without_memory": "Qualitative baseline: KG-A2C trained from scratch (i.e., single new agent without pre-trained experts) and fine-tuned single experts perform worse — fine-tuned experts often fail to find additional role-specific rewards beyond their pre-trained behaviors, and the new KG-A2C finds some early rewards but fails to obtain rewards deeper in the environment; no numeric values reported.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The persistent world KG provides a useful state representation that enables story-shaped intrinsic rewards and role transfer: pre-trained KG-A2C experts capture behavior-relevant knowledge that can be leveraged zero-shot via value seeding and attention. Memory is effective when experts' stored knowledge is relevant to the new role; the MoE leverages experts' KG-informed policies to accelerate few-shot learning. The paper does not empirically isolate KG utility via explicit ablation, so findings are qualitative.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2699.0",
            "source_info": {
                "paper_title": "A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MoE (KG-A2C Mixture-of-Experts)",
            "name_full": "Mixture-of-Experts agent composed of frozen KG-A2C experts plus a trainable KG-A2C expert",
            "brief_description": "A proposed agent that mixes several frozen, pre-trained KG-A2C role experts with an additional trainable ('hot') KG-A2C expert via an attention module; uses critics' value estimates to seed expert probabilities and a graph-based KG state representation from each expert.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Mixture-of-Experts (MoE) KG-A2C agent",
            "agent_description": "An ensemble agent that at each time step feeds the shared observation representation into multiple frozen pre-trained KG-A2C experts and one trainable KG-A2C expert; each expert outputs logits for templates/objects and a critic value; action logits are scaled by each expert's critic (value seeding), projected and combined by an attention module (separately for template and object distributions), averaged with the trainable expert's logits, and sampled to produce actions.",
            "base_model_size": null,
            "game_benchmark_name": "LIGHT (custom role-playing game built in LIGHT environment)",
            "game_description": "Same open-ended text role-playing setting; tasks require role-consistent behaviors (e.g., thief vs hunter) with rewards given for role-specific actions and reaching a goal room; some roles require behaviors deeper in the map.",
            "uses_memory": true,
            "memory_type": "graph-based memory (each KG-A2C expert uses a persistent world knowledge graph)",
            "memory_structure": "Each expert maintains/embeds a world knowledge graph (nodes/relations) using a graph-attention embedding; the MoE uses the experts' outputs and critics — it does not introduce a new memory structure beyond the experts' KGs.",
            "memory_content": "Per-expert world KG contents: observed rooms, objects, inventory items, NPCs and their relations; story KGs are used offline to shape expert training but the running MoE uses experts' world KGs for state embedding.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Integrated embedding and attention: expert KGs are embedded via graph-attention and combined with recurrent observation embeddings to form the per-expert action distributions and value estimates; the MoE attention operates over experts' action distributions (already conditioned on their KGs) rather than performing explicit KG retrieval across experts.",
            "memory_update_strategy": "Each expert's world KG is updated online as the agent explores (i.e., after each observation/action the corresponding expert's KG reflects newly observed facts); frozen experts' parameters do not change but their KGs are constructed from the current episode observations when used.",
            "memory_usage_purpose": "Experts' KGs provide state grounding that informs action logits and critic values; value-seeded expert logits guide the MoE attention, enabling reuse of previous role-aligned memories/strategies for zero-shot and few-shot transfer; story KGs (offline) are used during pre-training to shape experts so they store role-relevant memories.",
            "performance_with_memory": "Qualitative: The MoE (built from KG-A2C experts) achieves higher zero-shot scores and faster few-shot learning than single KG-A2C baselines (trained from scratch) and fine-tuned single experts; the MoE discovers rewards earlier and attains higher final scores in the tested blended and partial-blend target roles. The paper reports this in aggregate/plots but does not provide precise numeric scores in the main text.",
            "performance_without_memory": "Qualitative baselines without the MoE composition (but still using KG-A2C single agents) perform worse: fine-tuned experts typically fail to find new role rewards, and a newly trained KG-A2C finds only near-start rewards and fails deeper; no explicit experiment removes the knowledge-graph memory itself to compare performance.",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Memory (expert KGs) is effective because pre-trained experts store role-aligned behaviors that can be reused via value-seeded attention; value seeding (using critics) combined with KG-informed expert outputs gives strong zero-shot signals. Memory-driven transfer is ineffective when none of the experts' stored knowledge is relevant (adversarial/irrelevant-expert setting) because experts bias attention away from promising exploration; adding irrelevant (random) experts causes only marginal early slowdowns if relevant experts exist, but performance degrades drastically if no relevant experts are available.",
            "memory_limitations": "No explicit limitations of the KG memory are empirically analyzed; however, the MoE faces exploration and attention-related failures (expert starvation, premature convergence) caused by value seeding and strong expert signals — these are not presented as KG structural failures but as interaction effects between critics, attention, and exploration.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2699.1",
            "source_info": {
                "paper_title": "A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Story shaping: Teaching agents human-like behavior with stories",
            "rating": 2,
            "sanitized_title": "story_shaping_teaching_agents_humanlike_behavior_with_stories"
        },
        {
            "paper_title": "How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds",
            "rating": 1,
            "sanitized_title": "how_to_motivate_your_dragon_teaching_goaldriven_agents_to_speak_and_act_in_fantasy_worlds"
        },
        {
            "paper_title": "GATA",
            "rating": 1
        },
        {
            "paper_title": "Q*BERT",
            "rating": 2
        }
    ],
    "cost": 0.01191125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds
9 May 2024</p>
<p>Christopher Z Cui 
Georgia Institute of Technology</p>
<p>Xiangyu Peng 
Georgia Institute of Technology</p>
<p>Mark O Riedl riedl@cc.gatech.edu 
Georgia Institute of Technology</p>
<p>A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended Text Worlds
9 May 20243364EDD763F05F3CBE44E8CBA70B0A3EarXiv:2405.06059v1[cs.CL]
Open-ended worlds are those in which there are no pre-specified goals or environmental reward signal.As a consequence, an agent must know how to perform a multitude of tasks.However, when a new task is presented to an agent, we expect it to be able to reuse some of what it knows from previous tasks to rapidly learn that new task.We introduce a novel technique whereby policies for different a priori known tasks are combined into a Mixture-of-Experts model with an attention mechanism across a mix of frozen and unfrozen experts.The model learns when to attend to frozen task-specific experts when appropriate and learns new experts to handle novel situations.We work in an open-ended text-based environment in which the agent is tasked with behaving like different types of character roles and must rapidly learn behaviors associated with new character role types.We show that our agent both obtains more rewards in the zero-shot setting, and discovers these rewards with greater sample efficiency in the few-shot learning settings.</p>
<p>Introduction</p>
<p>Open-ended worlds are those in which there are no pre-specified goals or environmental reward signal.In such environments, we might want to have an agent that is capable of carrying out a variety of different behaviors and achieving a variety of goals based on the what is needed.In this work we look at the challenge of task-transfer in open-ended environments.In an open-ended environment, an agent must know how to perform a multitude of tasks.In this setting, we assume an agent has learned one or more policies, each for a specific task, but now must acquire a new policy for a new task.Policy models that perform distinct tasks can be gathered into a Mixture of Experts (MoE) where an attention mechanism learns which expert to listen to when performing actions.We show that the MoE can learn new tasks in a few-shot fashion when there are aspects of the new task that result behaviors from the experts.</p>
<p>We work in an open-ended text environment based loosely on Dungeons &amp; Dragons.Text environments are those in which an agent receives natural-language descriptions of its immediate locale in the environment, and performs actions by describing its actions with text.Text-based worlds have become a benchmark challenge for reinforcement learning agents (Hausknecht et al., 2020;Narasimhan et al., 2015;Ammanabrolu &amp; Riedl, 2019;Ammanabrolu &amp; Hausknecht, 2019;Ammanabrolu et al., 2020;Adhikari et al., 2020;Shridhar et al., 2021;Murugesan et al., 2020;Wang et al., 2022;Peng et al., 2023;Pan et al., 2023;Prasad et al., 2023;Abdulhai et al., 2023;Carta et al., 2023;Wang et al., 2023;Ryu et al., 2023;Chhikara et al., 2023).They are exemplified by the classical game, Zork, in which an agent must solve puzzles to get to the end of the game.Text-based games are challenging for the following reasons (Hausknecht et al., 2020): First, they are partiallyobservable environments; what can be observed is usually limited to a "room".Second, they have very large action spaces.For example, the game, Zork can accept commands up to four words in length and has a vocabulary of 700, meaning that there are 700 4 possible actions available in each state.Not all word sequences create meaningful change to the environment.Third, commonsense and trope knowledge is usually required to complete games.Action sequences that make sense in the real world usually also make sense in text games (e.g., "open mailbox" vs "eat mailbox") and have similar effects, though science fiction and fantasy tropes may also be present (e.g., silver bullets kill vampires).Fourth, unlike many computer games, text-games often involve solving puzzles with long-horizon causal and temporal relations.</p>
<p>Most commercial text games-such as those in the Jericho benchmark suite (Hausknecht et al., 2020)-have an objective to solve, and thus a reward signal can be constructed around completion of the game and/or progress through the game.On the other hands, Table-top role-playing games such as Dungeons &amp; Dragons are open-ended worlds in the sense that one can go nearly anywhere and do nearly anything.Often there is a quest or mission, but that quest or mission is not preordained and often unknown in advance.A character may have a particular role-fighter, thief, etc.-which is accompanied by particular behavioral expectations.</p>
<p>These behavioral expectations can be used to frame acting in a particlar role as a task.That is, absent a mission or quest, an agent has a task of role-playing in character.If a mission or quest is presented, then the task is to accomplish the goal by incorporating behavior expectations into their actions (a hunter may be more interested in making progress by engaging in combat whereas a thief may be more interested in progress via stealth and trickery).</p>
<p>From this, we can define the challenge of task transfer in role-playing games as a single agent that must transfer knowledge from one or more known character roles to a new character role type.Transfer is feasible because, although a role such as a thief will perform many actions that a hunter would never perform, the thief will also need to handle common situations such as navigating through the starting town, though perhaps visiting separate locations or interacting with the townsfolk in a different manner.As is often the case in task transfer in text games, some of an original policy is applicable to the new task without adaptation, but not all of it.By assembing expert, role-specific policy models into a Mixture of Experts, one can learn a new role faster because one of the experts may be able to make an informed guess about what the agent should do in the new context.</p>
<p>We introduce a novel technique whereby N frozen experts in the MoE model propose actions in the next task, and attention is applied across experts.An N + 1 th , untrained "expert" is added to the ensemble but remains hot so that it can learn from interactions with the environment.This "hot" expert learns how to interact with the environment when all the other frozen experts are insufficient, and the MoE as a whole learns the new task more rapidly than training a new expert from scratch or fine-tuning existing experts.This result holds even when the new task is not a strict blend of existing experts.</p>
<p>Related Work</p>
<p>Text Adventure Game Playing Agents</p>
<p>Text based games have shown great potential for use as Reinforcement Learning benchmark environments (Hausknecht et al., 2020;Narasimhan et al., 2015).Ammanabrolu &amp; Riedl (2019) proposed augmenting reinforcement learning with knowledge graphs as external memory about world state.Ammanabrolu &amp; Hausknecht (2019) proposed KG-A2C, which integrates knowledge graphs into the actor-critic (Bahdanau et al., 2016) RL framework.The Q*BERT agent (Ammanabrolu et al., 2020) further extended KG-A2C to incorporate the BERT (Devlin et al., 2019a) language model into the model architecture.We build on top of the KG-A2C family of models since they have shown state-ofthe-art performance.Other techniques for playing textgames include GATA (Adhikari et al., 2020), which builds a knowledge-graph based representation of the world on top of a transformer-based agent, training through a combination of RL and self-supervised learning.</p>
<p>Whereas text adventure games have pre-defined progression toward a goal state, table-top role playing games involve open-ended game play.We refer to text-based environments that support open-ended game play as text-based role playing to signify the interaction with the environment through reading and writing text instead of verbal interactions with other players and game masters.</p>
<p>The LIGHT environment (Urbanek et al., 2019) is a crowdsourced text-based role playing game with a rich environment with interactable NPCs, objects and locations, each with a short paragraph description, demonstrating the value of grounding in training agents that can not only act but also converse successfully.Ammanabrolu et al. (2021) propose agents that can switch seamlessly between generating natural language and action declarations.These agents can learn to play different characters when given a motivation that includes character type and goal as part of the world state.</p>
<p>Story Shaping (Peng et al., 2023) is a technique for training RL agents to play text role-playing games wherein a story is converted into a rich reward signal.The technique can be used to train different characters, but can only train a single agent to emulate a single character.Our character-based reward strategy is related, but our rewards are manually crafted instead of inferred from stories.</p>
<p>Ensembling in RL</p>
<p>To our knowledge, transfer learning with a mixture of pre-trained experts for RL has not been explored but there exists a large body of literature on transfer learning and ensemble methods in RL.Prior works explore ensemble methods of using multiple, frozen experts (teachers) to train a new agent as a student where the goal is to minimize the difference between the teachers' and student's policies (Hinton et al., 2015;Rusu et al., 2016;Yin &amp; Pan, 2017;Schmitt et al., 2018;Schulman et al., 2018;Teh et al., 2017;Parisotto et al., 2016).Other approaches instead make direct use of the teachers' policies, either with some heuristic to evaluate the 'goodness' of the teacher's action with respect to the current state or via a learned discriminator (Cheng et al., 2020;Kurenkov et al., 2020;Fang et al., 2021;Li et al., 2023).Some works further explore methods for compensating for sub-optimal teachers (Kurenkov et al., 2020;Kessler Faulkner &amp; Thomaz, 2021).Attention modules have been utilized in RL before, but only with experts that are learned through segmenting features or inputs at training time in a multi-task setting (Sodhani et al., 2021;Cheng et al., 2023).Our work resides in the intersection of these ideas.We leverage multiple, frozen sub-optimal experts in a MoE, using an attention module to combine the outputs of the experts.Rather than a student, an additional hot expert learns to bridge the gap when the sub-optimality of the pre-trained experts would leave the MoE otherwise unable to proceed.</p>
<p>Few-shot Adaption</p>
<p>Large pre-trained Language models have emerged as extremely powerful tools for NLP tasks (Devlin et al., 2019b;Raffel et al., 2020;Brown et al., 2020).However, a limitation of these powerful models is their size, some with parameters numbering in the billions (Brown et al., 2020).This makes them prohibitively expensive when it comes to further training or fine-tuning.Low-Rank Adaptation (LoRA) circumvents this by keeping the model frozen and introducing trainable rank decomposition matrices (Hu et al., 2021).Our proposed technique also freezes the core model(s) and trains additional layers on top, though the specific mechanics needed for reinforcement learning are different.</p>
<p>Background</p>
<p>Text-Adventure Games</p>
<p>A text-adventure or text-based role playing game can be modeled as a partially-observable Markov decision process (POMDP) M = ⟨S, T, A, ω, O, R, γ⟩ where S is the set of ground truth world states, A is the set of actions, T is the probability of transitioning from one state to another given an executed action, R is a reward function, O is the set of possible observations, ω is the probability of observations given the ground truth world state, and γ is a parameter estimating the reward horizon (Hausknecht et al., 2020).In our setting, we will use a deterministic transition function T , which is common in text-based games.However, nothing in our proposed technique strictly requires it.The objective of reinforcement learning is to learn a policy, π : S → A that maps states to actions, such that taking the action mapped to the current state and following the policy henceforth maximizes expected reward.</p>
<p>KG-A2C and Story Shaping</p>
<p>We consider the standard reinforcement learning setting where an agent interacts with a text game environment over a number of discrete time steps.State-of-the-art approaches to RL in text environments use a knowledge graph as external, persistent memory of the world state (Ammanabrolu &amp; Riedl, 2018;Ammanabrolu &amp; Hausknecht, 2019;Ammanabrolu et al., 2020).As the agent explores the game world, a knowledge graph is constructed and used as state representation.In this paper, we choose KG-A2C agent framework (Ammanabrolu &amp; Hausknecht, 2019) as our reinforcement learning agent.KG-A2C's space of observations, o t , includes (a) text description of the room the agent is in, (b) text descriptions of the character's inventory, (c) the agent's last command, and (d) feedback from the last command.These state observations are processed using a GRU based encoder using the hidden state from the previous step, combining them into a single observation embedding.Simultaneously, the state observation is used to update a knowledge graph G t of persistent memory of the world state.This includes facts and relations about rooms, objects in rooms, inventory items, etc.This knowledge graph is then embedded using a graph attention mechanism (Veličković et al., 2018).The overall representation vector is updated as o t .The agent is trained via the Advantage Actor Critic (A2C) (Mnih et al., 2016) method to maximize long term expected reward in the game in a manner otherwise unchanged from Ammanabrolu et al. (2020).More details about this agent can be found in Appendix A.</p>
<p>Agents output a language string into the game to describe the actions that they want to perform.To ensure tractability, this action space can be simplified down into templates.Templates consist of interchangeable verbs phrases (V P ), optionally followed by prepositional phrases (V P P P ), e.g.([carry/take] ) and ([throw/discard/put] [against/on/down] ), where the verbs and prepositions within [.] are aliases.Actions are constructed from templates by filling in the template's blanks using objects from the game's vocabulary.</p>
<p>In order to train different role-aligned policy models π i , we follow the Story Shaping technique (Peng et al., 2023).Different stories are first converted into knowledge graphs G i .As the agent explores the game world, a knowledge graph-called the World KG-is constructed and used as state representation G t .The similarity between G i and G t is used to provide an intrinsic reward for the KG-A2C model, subsequently shaping the RL-Agent to conform to the specified roles.More details about how we train KG-A2C with story shaping method can be found in Appendix B.</p>
<p>LIGHT Environment</p>
<p>In our paper, we create a game in LIGHT environment (Urbanek et al., 2019), which provides a text world environment with a database of 1775 Non-Player Characters (NPCs), 663 locations, and 3462 objects with rich text descriptions.We've developed an open-world game where players can play various roles, each with its own set of activities.For example, the player/agent can play a "adventurer" in our game, and numerous activities have been specifically tailored for this role, including the presence of dragons specifically designed for 'adventurers' to confront and defeat.</p>
<p>Method</p>
<p>Given a collection of pre-trained, role-aligned expert models e 1 , e 2 , ...e N −1 from source characters p 1 , p 2 , ..., p N −1 pre-trained using story shaping methods (Peng et al., 2023), our agent aims to use their knowledge on a new role task under few-shot settings.</p>
<p>Action Probabilities</p>
<p>We first initialize an empty expert e T and add it to the mixture-of-experts.This empty expert is a repository for any policy that is unique to the new task role and not sufficiently handled by another, existing expert.We obtain a probability distribution over actions at time t for each expert, a t,1 , a t,2 , ..., a t,N , where a t,j ∈ R v , by feeding the overall representation vector o t into frozen, pre-trained character-aligned agents, e 1 , e 2 , ...e N −1 and trainable policy model e N .v is the action space of the game environment.</p>
<p>The KG-A2C splits action generation into a multi-step process.A template distribution τ t,j is generated from o t and is sampled to select template T act .Both of these are then used to generate a probability distribution over all game objects.A mask is applied to reduce this distribution to only admissible objects for the current state O t,j and sampled to product an object, T obj .The resulting action a t,j is a combination of T act and T obj .</p>
<p>In our MoE agent, the template T act sampled by the original model is used to generate the corresponding object distribution O t,j .This means for each pre-trained, character-aligned model e 1 , e 2 , ...e N −1 that results in action templates τ t,1 , τ t,2 , ..., τ t,N−1 , the templates sampled from these distributions are used to generate O t,1 , O t,2 , ..., O t,N −1 .We represent both τ t,j and O t,j with a t,j going forward.</p>
<p>Value Seeding.Each expert e 1 , e 2 , ..., e N possesses a critic C 1 , C 2 , ..., C N that predicts an expected return v t,1 , v t,2 , ..., v t,N given state S.These values represent how rewarding each expert believes their current state to be, or equivalently, the importance of the state to the respective, pre-trained model.These values can be used to infuse the MoE model with knowledge on which pre-trained expert to place more weight on at any given state due to an expectation of a higher return.We do this by multiplying each a t,j with the corresponding v t,j prior to being input to the attention module.The critic modules are not explicitly shown in Figure 1.</p>
<p>Expert Attention.An expert attention module G merges the output action distributions from each expert module.It learns weights based on which expert to attend to and produces a final action probability distribution.We use two separate attention modules, G act and G obj , to account for the KG-A2C's multi-step action generation.However, these attention modules are identical outside of being modified to process the different sized inputs and will be collectively referred to as G going forward.The approach outlined in the following section can be generalized to any architecture with a single-step action generation.</p>
<p>The input of G is the representation of observations ôt in time step t and action probabilities [a t,1 ; a t,2 ; ...; a t,N ].We believe the distribution of action probabilities multiplied by the corresponding expected returns v 1,j , v 2,j , ..., v N,j represent the knowledge possessed by the experts in our game environment; the expected returns represent how valuable each model perceives its current state to be.Multiplying the former by the latter and mixing the experts' action probabilities is a good way to use this knowledge and save training time under few-shot setting.</p>
<p>We first apply a feed-forward network to project ôt non-linearly to a new representational space:
h t = LayerNorm(W ⊤ up,o • γ(W ⊤ down,o • ôt ))
where γ(•) is a non-linear activation function.We used SiLU (Elfwing et al., 2018)
h l,j = LayerNorm(W ⊤ up,l • γ(W ⊤ down,l • a t,j ))
where W down,l ∈ R v×v ′ and W up,l ∈ R v ′ ×v are projection parameters to be updated during training.</p>
<p>We compute the attentions by calculating the product between h l,j and h o and apply softmax over the experts, as follows:
α t,j = e (h l,j •ho) T k=1 e (h l,k •ho)
.</p>
<p>We then obtain the output logits a t ∈ R v by computing a linear combination of [a t,1 * v t,1 ; ...; a t,N * v t,N ] given the computed input-logit attention,
l t = G(ô, [a t,1 ; ...; a t,N ]) = (a t,N + N j=1 α t,j a t,j )/2
Then we follow the standard approach of Reinforcement learning agents, where the combined logits l t are processed by second softmax transformation and then used to sample the action.This sampling is done in two stages where the output of G act , τ t,N , is sampled and used to generate O t,N .</p>
<p>Exploration.Our agent tends towards premature convergence.Due to the Value Seeding we apply to the probabilities prior to the attention module, the probability mass of the resulting distribution post softmax tends to be most heavily weighted on the top action of the expert whose critic predicts the highest value.When this action results in a reward, this becomes a positive feedback loop where the attention module continuously places more weight on the expert associated with this reward until all other experts are ignored.This results in a pre-mature convergence on the portion of the reward function associated with the over-fit expert.This also leads to expert starvation, where an expert whose rewards do not appear until later in the environment are always assigned an extremely low weight by the attention module due to higher weights being assigned to experts whose rewards are found early.</p>
<p>We implement two measures to solve this and drive exploration.We use (1) Epsilon-Greedy sampling with decay instead of multinomial sampling, and (2) implement a new loss term.Epsilon-Greedy sampling helps with exploration because the original KG-A2C uses an entropy loss to push the agent to explore and prevent the agent from prematurely convergence.While this is sufficient for training any single expert KG-A2C module, it is not sufficient to prevent the attention module from premature convergence.This in turn pushes the entire MoE agent towards prematurely converging on the scores associated with a specific pre-trained role.Epsilon-Greedy avoids this by sampling a new action in a manner that is influenced by neither the attention module nor the underlying A2C.</p>
<p>We add a new loss term to the KG-A2C loss.This loss is calculated as the log sum of the action logits with the object portion of the loss normalized across the number of decoded objects.Let l act and l obj be the outputs of G a and G o respectively, where π act (ô t ) = {τ 0 , τ 1 , ..., τ K } and π obj (ô t ) = {O 0 , O 1 , ..., O M } represent all valid template and objects for a given state and n is the number of objects used in T act :
L(o t ; θ t ) = 1 K K i=1 (1 − log(τ i |o t )) + 1 n n p=1 1 M M j=1 (1 − log(O j |o t ))
Functionally, this a cross entropy loss that flattens the action probability distribution by penalizing any particular action the closer its probability is to 1 and motivating a probability distribution that is more uniformly distributed between the most probable actions.</p>
<p>Experiments</p>
<p>Experts.We use as experts four KG-A2C models which have each been trained to emulate a role: e thief , e adventurer , e hunter , and e hoarder .The parameters of these experts are frozen.Another untrained KG-A2C model acts as the empty expert, e hot , and attention modules G act and G obj are randomly initialized and trained during the few-shot finetuning on the game with a new role.</p>
<p>Environment Details.We execute the MoE agent and our baselines in the same open-ended environment that has multiple opportunities for actions that align with various personas.The environment (see Figure A.1 in the Appendix) has a common starting room and an exit room that terminates the game when the agent enters it.Reward is earned when certain role-specific behaviors are performed in particular locations.Some roles have more opportunities for role-aligned actions near the start room.Closer to the exit room there is a branch such that one role prefers one branch with the other roless preferring the other branch.</p>
<p>Baselines.We test our MoE agent against two sets of baselines.All models used are from the KG-A2C (Ammanabrolu &amp; Hausknecht, 2019) family of models and are trained with Story-Shaping (Peng et al., 2023).The first baseline is a KG-A2C agent trained from scratch on the new task.We also use individual pre-trained expert policies, separated from the MoE and fine-tuned on the new role.</p>
<p>We train one Fine-tuned Expert for each expert that goes into the full MoE agent.</p>
<p>Target Roles.We evaluate our MoE agent across a number of different target roles.Blends are target roles composed of behaviors drawn from a subset of original four roles.Partial blends are target roles that are a mix of behaviors drawn from the original four roles but also completely new behaviors.All target character are crafted to have roughly the same maximum scalar reward as the pre-trained experts were pre-trained on, and all target roles must reach the goal location.We show the results from a blended target role that has a roughly equal mix of behaviors from all previous roles (but is not a strict union of all rewards from all roles) and a partial blend target role that requires behaviors from only some pre-trained experts.Results from other target roles are available in the appendix.</p>
<p>Results</p>
<p>The performance of the MoE agent can be measured through two metrics: sample efficiency and total score.Total Score.In both target roles, most Fine-tuned Experts fail to find any additional rewards outside of those associated with their pre-trained roles and the small reward for reaching the goal room.This is especially evident on the partial blend target roles, where the pre-trained experts whose roles did not contribute to the target role achieved only the small reward for navigating to the goal location.The New KG-A2C discovers some rewards closer to the starting location over the course of the training period but fails to obtain rewards deeper in the environment.From testing time performance at training step 0 and after a short period of training, we see the MoE agent obtains a higher score in both a zero-shot setting due to the value seeding and a few-shot setting over time.The large standard deviation of the MoE agent is largely an artifact of the bias we place on exploration: across 20 random seeds over all target roles, only one seed failed to produce a model that failed to achieve the max score when evaluated over 10 random seeds.</p>
<p>Expert Composition Study</p>
<p>In this section, we look at whether the composition of experts matter, with regard to having more experts and less, and the extent to which the relevance of experts helps or hurts.The first test examines the robustness of the MoE agent for e 1 , e 2 , ...e N−1 as N increases.The second examines the performance of the MoE agent in the absence of any experts relevant to the target role.</p>
<p>Distractor Experts.To examine the MoE agent's robustness to a large number of irrelevant experts, we double the number of pre-trained, frozen experts by adding 4 randomly initialized KG-A2C models.These new models are all kept frozen as with all other pre-trained experts.We follow</p>
<p>Original Four Experts Max Score</p>
<p>Original Four Plus Four Random</p>
<p>Blend Partial Blend</p>
<p>Figure 3: The testing-time performance of the MoE agent with only the original four experts versus the same agent with four added random experts.When all experts contribute some relevant information to the new task, the MoE agent's performance suffers slightly as the attention module needs more time to distinguish relevant experts from irrelevant experts.When there are only a few relevant experts, additional irrelevant experts have little to no impact on the MoE agent's performance.</p>
<p>the same training and evaluation procedure and show the results for the same roles used in Section 5.1.For visual clarity, we only plot the performance of the regular MoE agent against the same agent with four additional, randomly initialized experts.</p>
<p>For the blended target, we see only a marginal drop in early performance due to the noise from the random agents causing the MoE agent to take longer to learn which experts to attend to and thus discover rewards.However, performance recovers as the attention module learns to ignore the random agents after an extended period of training.</p>
<p>Conversely, in the partial blend we see the 4 random agents having virtually no impact on the performance of the MoE agent.While the random agents have a negative impact on the attention module in its ability to identify the most relevant experts for particular state, this becomes less of an issue with a lower number of relevant experts.</p>
<p>Irrelevant Experts.To examine the MoE agent's robustness to fewer, or irrelevant, experts, we remove the relevant experts to the partial blend and run the MoE agent with only irrelevant experts.This can be considered an adverarial attack on the agent, requiring it to perform a task that is completely unlike any that it has ever mastered.Doing this drastically degrades the MoE agent's performance due to the adversarial nature of this arrangement.In a case where no expert has any useful knowledge with respect to the target role, the MoE agent is effectively reduced to training KG-A2C from scratch, but experts want to pull the agent away from promising exploration.Even with epsilon-greedy exploration style, the experts will not let the agent explore too far.</p>
<p>Conclusions</p>
<p>In this paper, we introduce a novel method for task transfer in open-ended, text-based environments.</p>
<p>We demonstrate how performing a role can be framed as a task such that a variety of frozen experts, each trained in a specific role, can be mixed by an attention module along with a new, trainable expert to create a MoE agent capable of rapidly learning a new, related task.In this MoE agent, the attention module assigns scores to each expert based on the current observation.Pre-trained experts are more closely attended to in contextually similar states with the new, trainable experts filling in the parts of the policy that cannot be transferred from the pre-trained experts.We show our MoE agent, when not facing adversarial settings, far outperforms any single individual of its parts in a few-shot setting and demonstrate its robustness to a large number of irrelevant experts.</p>
<p>Appendix A KG-A2C</p>
<p>For our source agents, we build off the KG-A2C agent framework Ammanabrolu &amp; Hausknecht (2019), an Advantage-Actor Critic architecture augmented with a knowledge-graph based attention.KG-A2C's space of observations includes (a) text description of the room the agent is in via the "look" command, (b) text descriptions of the character's inventory via the "inventory" command, (c) feedback from the last command, and (d) the agent's last command.The state observations are concatenated and embedded using a recurrent GRU.</p>
<p>Simultaneously, the state observation is used to update a knowledge graph of facts about the world that have been observed to date.This includes facts and relations about rooms, objects in rooms, inventory items, etc.This knowledge graph is then embedded using a graph attention mechanism Veličković et al. (2018).</p>
<p>Advantage-actor critic networks Mnih et al. (2016) have two heads.The actor head generates logit scores, one for each possible action, which can be converted to a probability distribution via softmax and sampled to determine which action the agent takes.The critic head estimates the utility of the state.Actions are made up of verbs and optional object names.The KG-A2C agent generates a verb, which maps to a pre-defined template, and the generated object name is used to populate the template.</p>
<p>A.1 Game Map</p>
<p>A.1.1 Game Map</p>
<p>Forest:</p>
<p>The trail path is a well designed path in the middle of a deep, dark forest.It is made purely from the bones of the innocent that were sacrificed to the ogre.There are eyes looking in the dark at you as you walk on the trail.</p>
<p>Nothing Nothing</p>
<p>Swamp:</p>
<p>The swamp is glowing with wonder and color.There are parts that range from dark red to bright yellow.People often visit here to speak with the gods and claim it can be both harmful to those it dislikes and healing to those who it deems worthy.</p>
<p>frog bug</p>
<p>Small Graveyard: This is a small grassy area located under a large oak tree.There are two home made wooden crosses set up along with mounds where the bodies are buried.</p>
<p>graveyard keeper ghost</p>
<p>Herbs</p>
<p>Family Heirlooms Clearing: Few trees surround the area.There's a low section of the grass on the floor along with some stone boulders.</p>
<p>The Troll's Lair:</p>
<p>Covered with rotting meat from nearby wildlife, the Troll's lair reeks of death.If the stench alone wasn't enough to scare off an unfortunate travelers who stumbled upon it, the bones of unlucky spelunkers would be.The Troll places his spoils near the numerous stalagmites on the damp cave wall, his red eyes patiently scanning the dark cave interior for anything that may disturb his isolation.</p>
<p>Nothing</p>
<p>Cave Troll</p>
<p>Map Part A</p>
<p>Map Part B</p>
<p>B Story Shaping</p>
<p>We train all KG-A2C models with Story Shaping Peng et al. (2023).The following are the stories used to align the pre-trained agents with their respective roles.</p>
<p>Adventurer.I am an adventurer.I have come to this town seeking treasure and challenge.In the library, I find a tattered map that tells of a dungeon and the treasure within.I first go to the armory to purchase a sword and armor.Leaving town, I make my way to the dungeon.I slay the skeleton at the entrance and travel deeper into the dungeon.In the lower dungeon, I encounter a huge dragon.After an arduous battle, I slay the huge dragon.Past the dragon, I find a gold room and take the treasure.On the way to the clearing, I note the bodies on my path and find a Troll's Lair.Within, I slay the cave troll.My thirst for battle and treasure sated, I travel to the Clearing and set off for the next town.</p>
<p>Thief.I am a thief.I value stealth and wealth, only getting into a fight when I have the advantage.In the tavern, I find a hidden dagger and take it for my own.I explore the town, looking for easy targets.In the sermon hall, I steal the donations but avoid the priest due to the knight.In the marketplace, I find a fishing store with a lone fisherman.I kill the fisherman and steal the pearl.Fleeing town, I avoid the dungeon and take a detour through the swamp.In the swamp, I see a small graveyard with single graveyard keeper and some graves.I stab the graveyard keeper in the back, killing him instantly.I dig up the graves and take the valuable family heirlooms.I travel to the Clearing and make my escape.</p>
<p>Hunter.I am a hunter.My trade is in navigating natural environments and dealing with wildlife.I buy a skinning knife from the markets and get a bow from the armory before I set off.As I travel through the forest, I deal with the wildlife as they come.I kill and collect a wolf and a snake.I</p>
<p>Figure 1 :
1
Figure1: Pipeline of our MoE Agent.At each time-step, all experts produces a logit distribution over actions, a 1 , a 2 , ..., a N .These are each passed through a softmax to get the resulting probabilities and multiplied by the v 1 , v 2 , ..., v N produced by each expert's critic module.The scaled probabilities are then mixed by the attention module and averaged (operation represented by µ) with the a N produced by the trainable expert to obtain l t .These averaged logits are then passed through a softmax and sampled to produce an action.</p>
<p>Figure 2 :
2
Figure 2: The full agent versus unfrozen experts allowed to continue training and a new KG-A2C.The left graph illustrates the test time performance on a persona composed only of pre-existing expert behaviors while the right illustrates the performance on a persona that is a mix of both pre-existing and new behaviors.</p>
<p>full of stone buildings, most of which are two or 3 stories.They have ornate tin gutter fixtures with the water seeming to pour from the mouths of different animals.They almost all have wooden plaques next to the doors describing the nature of their business.the buildings are almost touching each other, and the narrow streets are so filled with people, donkeys pulling carts, and unruly children its near impossible to walk by without bumping into something.Inside the massive Church you can see a stage to the far back.Benches lines the hall where some are kneeling in prayer.To the far right there's a row of candles, some lit, some not.On the stage there are bleachers for people to stand on to sing the songs of their people.Sitting on the center of the stage is a large podium where the priest gives This room is filled with weapons and armor on manikins made of wood and iron.The weapons here are well used, time and age have made their dents and marks, however they remain reliable for any man</p>
<p>(Ba et al., 2016)re.W down,o ∈ R l×l ′ and W up,o ∈ R l ′ ×l are projection parameters to be updated during training.Then we apply Layer Norm(Ba et al., 2016)to get h t ∈ R l -the final projected representation of observations o t .Similarly, we project each action probability a t,j ∈ R v of the role-aligned c j given observations o t into the same space:</p>
<p>A.1.2 Game Map Part B Forest Entrance: The path quickly narrows and darkens upon entering the forest. Dense branches overhead block all but a few stray rays of sunshine. Fallen leaves just beginning to rot blanket the forest floor, and every footstep kicks up a rich, loamy scent as fresh ground is unearthed. Trees wolf snake Dungeon: Dark and gloomy, the dungeon is the ugly underbelly of the castle. A few torches line the black stone walls, throwing a meager light on the iron bars of the cells. The air is damp and still; the only disruptions are the occasional anguished moans and cries from the prisoners. skeleton Nothing Lower Dungeon: The lower dungeon is set below the main hall. It is dark, lit by only a few dim torches. Water is constantly dripping down the walls, giving it a dank and musty smell. There are manacles and cages set along the walls. In the middle of the room sits a table with manacles on all corners which are attached to a large rotating lever. captured knight huge dragon Nothing Gold Room: A hidden room, Gold is adorned with gold candle fixtures on either side, and a gold plated wooden table in the center. Riches lay all around the room and on top of the table. Here can be found the crowns of past kings and the royalty of conquered nations. treasure golden sword of kings
Hidden Passage:A rough, narrowpassageway descendsthrough the thick outerwall of the dungeon.Stair-like stones leaddownward intodarkness.NothingNothingNothing
opt to take the route through the swamps rather than the dungeon, due to my comfort with nature.I encounter and shoot a poisonous frog but otherwise face no issue as I travel to the Clearing and make my way to the next town.Hoarder.I am a hoarder.I love to collect useless things and anything I can get my hands on.I get a drink from the tavern but keep the beer keg.The owner chases me out but I grab the grimy bar stools as I go.In the marketplace, I can't control myself and take the finest wines.I am promptly chased out of town by the guards.Feeling hungry, I grab some herbs to tide me over.With no weapons, I make sure to avoid the dungeon and instead travel through the swamp, accidentally stepping on a bug.I exit through the clearing with my strange collection of items.C Best ModelsThe following table contains the average total score and steps of the best MoE agent for each of the 5 seeds for all target roles.Models were ranked in order of highest average total score, number of steps and total training steps.Blend MetricTarget
Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, Sergey Levine, 2023</p>
<p>Learning dynamic belief graphs to generalize on text-based games. Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, Will Hamilton, Advances in Neural Information Processing Systems. 202033</p>
<p>How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. Ammanabrolu, Urbanek, Li, Szlam, Rocktäschel, Weston, NAACL-HLT. Association for Computational Linguistics2021</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2019</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark O Riedl, arXiv:1812.016282018arXiv preprint</p>
<p>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, Mark O Riedl, arXiv:2006.074092020arXiv preprint</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>An actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.070862016arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, 2023</p>
<p>Policy improvement via imitation of multiple oracles. Ching-An Cheng, Andrey Kolobov, Alekh Agarwal, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Multi-task reinforcement learning with attention-based mixture of experts. Guangran Cheng, Lu Dong, Wenzhe Cai, Changyin Sun, 10.1109/LRA.2023.3271445IEEE Robotics and Automation Letters. 862023</p>
<p>Knowledgeenhanced agents for interactive text games. Prateek Chhikara, Jiarui Zhang, Filip Ilievski, Jonathan Francis, Kaixin Ma, Proceedings of the 12th Knowledge Capture Conference 2023. the 12th Knowledge Capture Conference 2023Association for Computing Machinery2023</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2019a1</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies2019b1</p>
<p>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Elfwing, Uchibe, Doya, Neural Networks: the Official Journal of the International Neural Network Society. 1072018</p>
<p>Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. Kuan Fang, Yuke Zhu, Silvio Savarese, Li Fei-Fei, CoRR, abs/2106.13935Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021. 202034Discovering generalizable skills via automated generation of diverse tasks</p>
<p>Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, 2015</p>
<p>Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2021</p>
<p>Interactive reinforcement learning from imperfect teachers. HRI '21 Companion. A Kessler Taylor, Andrea Faulkner, Thomaz, 10.1145/3434074.34463612021Association for Computing MachineryNew York, NY, USA</p>
<p>Ac-teach: A bayesian actor-critic method for policy learning with an ensemble of suboptimal teachers. Andrey Kurenkov, Ajay Mandlekar, Roberto Martin-Martin, Silvio Savarese, Animesh Garg, Conference on Robot Learning. PMLR2020</p>
<p>Iob: Integrating optimization transfer and behavior transfer for multi-policy reuse. Siyuan Li, Hao Li, Jin Zhang, Zhen Wang, Peng Liu, Chongjie Zhang, 2023</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Enhancing text-based reinforcement learning agents with commonsense knowledge. Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan, Pavan Kapanipathi, Kartik Talamadupula, arXiv:2005.008112020arXiv preprint</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, Dan Hendrycks, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. 2023. 2016</p>
<p>Xiangyu Peng, Christopher Cui, Wei Zhou, Renee Jia, Mark Riedl, arXiv:2301.10107Story shaping: Teaching agents human-like behavior with stories. 2023arXiv preprint</p>
<p>Adapt: As-needed decomposition and planning with language models. Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, 2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell, 2016Policy distillation</p>
<p>A minimal approach for natural language action space in text-based games. Dongwon Ryu, Meng Fang, Gholamreza Haffari, Shirui Pan, Ehsan Shareghi, Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL). Jing Jiang, David Reitter, Shumin Deng, the 27th Conference on Computational Natural Language Learning (CoNLL)SingaporeDecember 2023Association for Computational Linguistics</p>
<p>Simon Schmitt, Jonathan J Hudson, Augustin Zidek, Simon Osindero, Carl Doersch, Wojciech M Czarnecki, Joel Z Leibo, Heinrich Kuttler, Andrew Zisserman, Karen Simonyan, S M Ali Eslami, Kickstarting deep reinforcement learning. 2018</p>
<p>Equivalence between policy gradients and soft qlearning. John Schulman, Xi Chen, Pieter Abbeel, 2018</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Multi-task reinforcement learning with contextbased representations. Shagun Sodhani, Amy Zhang, Joelle Pineau, 2021</p>
<p>Distral: Robust multitask reinforcement learning. Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, Razvan Pascanu, CoRR, abs/1707.041752017</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations. 2018</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 2022</p>
<p>Behavior cloned transformers are neurosymbolic reasoners. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Knowledge transfer for deep reinforcement learning with hierarchical experience replay. Haiyan Yin, Sinno Jialin Pan, 2017AAAI Press17</p>            </div>
        </div>

    </div>
</body>
</html>
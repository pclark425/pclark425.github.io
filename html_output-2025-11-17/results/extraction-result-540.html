<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-540 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-540</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-540</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-33af17e7cc5cd4ee18c9e6e8c5fca7e224592ec0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/33af17e7cc5cd4ee18c9e6e8c5fca7e224592ec0" target="_blank">Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This guided decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e540.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e540.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tabletop-GD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulated Tabletop Rearrangement with Grounded Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grounded Decoding (GD) couples a frozen LLM (InstructGPT) with token-conditioned grounding functions (affordances from a CLIPort policy, rule-based safety and preference scorers) to iteratively decode executable, open-vocabulary action commands for multi-step tabletop manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A frozen autoregressive instruction-tuned LLM (InstructGPT) used via prompts and few-shot examples; no fine-tuning on the robotic domains. The model provides implicit semantic and procedural knowledge in its weights and outputs natural-language step commands which are decoded token-by-token while being re-scored by grounded models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Simulated Tabletop Rearrangement (RAVENS-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon, multi-step rearrangement tasks (Letters, Blocks & Bowls, Box Packing) in a top-down RGB-D simulated tabletop environment. Given a high-level instruction, GD decodes a sequence of natural-language step commands which are executed by a pre-trained language-conditioned low-level primitive policy (CLIPort) producing pick/place actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation; multi-step planning; household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial (sequencing of steps, object relations such as stacking/containment, spatial pick/place locations and reachability)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM: pre-training on large text corpora and instruction tuning (InstructGPT); Grounding: CLIPort trained on demonstrations (supervised), plus rule-based safety and preference functions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting / instruction prefixing to InstructGPT; token-level guided decoding (Grounded Decoding) combining LLM token probabilities with token-conditioned grounding scores (affordance logits, rule-based scorers); greedy and beam search variants</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>LLM: implicit procedural and semantic knowledge encoded in network weights and producible as natural-language action sequences; Grounding: explicit token-conditioned affordance scores (CLIPort per-pixel logits aggregated to a token score), and explicit rule-based binary/soft scorers for safety and preferences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (task completion %), averaged over episodes/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Grounded Decoding substantially improved success rates vs ungrounded LLM and solitary CLIPort baselines. Representative aggregated numbers (reported in paper): Seen tasks (GD Beam) — Letters 57%, Blocks & Bowls 77%, Box Packing 78%; Unseen tasks (GD Beam) — Letters 41%, Blocks & Bowls 50%. Ungrounded LLM and CLIPort variants are substantially lower (e.g., Ungrounded LLM seen tasks often ~20-35%; CLIPort Long varies and fails to generalize to unseen tasks). See Appendix Table 6 for per-task breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>LLM reliably supplies high-level procedural sequencing (multi-step plans and decomposition); when coupled with affordance grounding, the system selects object tokens and verbs that are physically feasible (CLIPort affordance peaks guide selection of pick/place targets), and rule-based grounding enforces safety/preferences (avoids hazardous objects, prefers designated objects). Beam search further helps by considering full single-step instructions during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Ungrounded LLMs frequently propose semantically plausible but scene-infeasible commands (e.g., selecting absent or unreachable objects). Remaining failures are often due to low-level primitive execution errors. When grounding is removed, performance drops dramatically (e.g., many Box Packing tasks drop from ~75–100% with grounding to single-digit or low-double-digit percentages ungrounded). Ambiguous instructions and tasks requiring fine scene detail without explicit grounding are problematic for the LLM alone.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baselines include CLIPort (Short: single-step training, Long: trained on high-level instructions), Ungrounded LLM planner (LLM decoding without grounding). Example baseline numbers: CLIPort (Short) often very low (e.g., Letters ~5–15%); CLIPort (Long) higher on seen tasks but poor on unseen (e.g., Blocks & Bowls seen 62% in Table 1 but unseen low). Ungrounded LLM: seen Letters ~20%, Blocks&Bowls ~35%, BoxPacking ~11% (paper tables). GD outperforms these baselines consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablation of grounding (i.e., using LLM decoding without grounding) reduces success drastically (examples: Box Packing tasks Ungrounded ~5–20% vs GD Greedy/Beam ~75–100%). Removing specific grounded components (affordance, safety, preference) causes predictable failure modes: without affordance, token choices are not grounded to feasible pick/place locations; without safety rules, hazardous interactions occur; without preference functions, user-specific soft-requirements are ignored. Beam search vs greedy: beam improves success (beam > greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs encode rich procedural and semantic knowledge (step decomposition, action sequencing, object-relational semantics) implicitly in their weights and can produce long-horizon plans in text; however, they lack scene- and embodiment-specific spatial/affordance knowledge. Token-level grounding functions (affordance/value nets, detectors, rule-based scorers) that score partially decoded token sequences can be combined with LLM decoding to steer generation toward physically realizable, safe, and preference-aware actions without finetuning the LLM, enabling open-vocabulary, actionable planning for manipulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e540.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e540.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minigrid-GD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2D Maze Navigation (Minigrid) with Grounded Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grounded Decoding uses an InstructGPT planner together with a token-conditioned value (affordance) function derived from a PPO-trained goal-conditioned policy to plan long-horizon navigation sequences in Minigrid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frozen instruction-tuned LLM used as a high-level planner (few-shot prompted) to sequence textual skill tokens; grounded by a learned goal-conditioned value function (from PPO) that maps partially decoded skill strings + observation to feasibility/affordance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Minigrid 2D Maze (gym-minigrid variants)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Top-down full-grid 2D maze navigation tasks grouped into Easy (short horizon), Medium (longer, partly specified), and Hard (very long-horizon >100 steps, ambiguous instructions). The LLM plans over low-level skills (e.g., go to <obj>, open <obj>) executed by a multi-task RL policy; grounding is provided by the token-conditioned value function evaluating feasibility given the current observation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation; multi-step planning; exploration</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + procedural (layout and reachability, sequential planning over navigation and key/door interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLM: pre-training on text corpora (InstructGPT); Grounding: value function from policy trained with PPO on short-horizon skills; CLIP embeddings used to encode skill strings</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting; Grounded Decoding token-by-token combining LLM token probabilities with the token-conditioned value (affordance) function; greedy and beam search variants</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>LLM: implicit procedural plans as natural-language sequences; Grounding: explicit value/affordance estimates from a goal-conditioned RL policy (numeric scores evaluating feasibility of partially decoded skill strings given the observation), skill strings encoded with CLIP embeddings for conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (task completion %), averaged across 100 episodes per difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Representative numbers reported: Baselines — PPO flat agent: Easy 28%, Medium 13%, Hard 6%; Hierarchical RL (HRL): Easy 68%, Medium 48%, Hard 31%; Ungrounded LLM planner: Easy 96%, Medium 87%, Hard 54%; GD (Greedy/Beam): Easy 100%, Medium ~93%, Hard 78% (beam improves especially on hard long-horizon tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>The LLM provides strong long-horizon procedural sequencing (decomposing goals into textual skills), and the token-conditioned value function supplies scene-specific reachability/feasibility (spatial knowledge) enabling selection among candidate skills; this combination is especially effective on hard, ambiguous, and long-horizon tasks requiring efficient exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Ungrounded LLM struggles when many skills are potentially viable but the LLM does not see observations (leads to poor selection among skill options); flat RL lacks high-level language reasoning causing poor performance; remaining failures include low-level policy errors and exploration inefficiency in very complex layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Flat PPO: 28%/13%/6% (Easy/Medium/Hard); HRL over skills: 68%/48%/31%; Ungrounded LLM planning: 96%/87%/54%; GD improves these to nearly perfect on Easy and significantly on Hard (up to 78%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablating the affordance/value grounding (i.e., using LLM planning without the token-conditioned value function) reduces performance notably on long-horizon/hard tasks (e.g., Hard drops from GD ~78% to Ungrounded ~54%). Beam search further improves success over greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs encode procedural knowledge enabling decomposition into skill sequences but cannot by themselves account for scene-specific spatial feasibility; token-conditioned value/affordance functions trained on embodiment data provide the necessary spatial/embodiment grounding and when combined at token-level with LLM decoding yield large gains in long-horizon navigation and planning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e540.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e540.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kitchen-GD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Real-World Kitchen Mobile Manipulation with Grounded Decoding and Multimodal Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>In a real kitchen setup GD couples large LLMs (PaLM for planning, InstructGPT for chain-of-thought prompting) with off-the-shelf open-vocabulary detectors (owl-vit) and low-level RT-1/BC primitives to ground ambiguous instruction following via visually-grounded chain-of-thought and token-level grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (540B) for planning; InstructGPT (text-davinci-002) for generating grounded chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B (PaLM) for main planner; InstructGPT size not specified</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM (large pre-trained LLM) used as frozen high-level planner; InstructGPT used to elicit multimodal chain-of-thought where the LLM decides when to enable grounded decoding (square-bracket gating). Grounding provided by open-vocabulary object detector (owl-vit) and token-conditioned scoring (SayCan-like scoring mode) combined with low-level RT-1 / BC policies for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Kitchen mobile manipulation (real world) instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Robot must follow natural language instructions to fetch and deliver objects in a kitchen (examples: 'Bring an apple', 'Bring me the fruit'). Tasks split into Unambiguous (object named) and Ambiguous (object class only). Objects assumed in field of view; pipeline uses LLM planning, chain-of-thought to decide when to consult vision, detectors to ground object mentions, and lower-level learned and scripted controllers for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>mobile manipulation; instruction following; multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial (sequencing of sub-tasks, object identification and relations, reachability/navigation to objects)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>LLMs: pre-training on large text + instruction tuning (PaLM and InstructGPT); Grounding: open-vocabulary detector (owl-vit), value- / affordance-style scoring via SayCan approach, low-level policies trained with BC and RT-1 data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>chain-of-thought prompting (InstructGPT) to produce text that indicates when to enable grounded decoding (gating); GD decoding uses the detector/value scores when grounded; final planning uses SayCan scoring mode on tokens; execution with RT-1/BC learned primitives</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>LLM: implicit procedural knowledge as natural-language plans and chain-of-thought tokens; Grounding: visual detections (open-vocab object labels & confidences) and value/affordance scores; rule-based gating controlling when to apply grounded scores; low-level policies provide embodiment-conditioned feasibility implicitly via learned controllers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Planning success rate (%) and execution success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported results: Unambiguous tasks — GD planning 85%, execution 57% (SayCan planning 85%, execution 57% as well); Ambiguous tasks — GD planning 58%, execution 44% vs SayCan planning 33%, execution 25%. GD thus yields a ~25% absolute planning improvement on Ambiguous tasks over SayCan.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Using multimodal chain-of-thought that gates when to consult vision enables the LLM to disambiguate object classes into specific visible objects using the detector scores; LLM provides procedural ordering (find → pick → bring → place) and detectors supply object-relational grounding (which fruits are present) and spatial reachability informs feasible plan selection.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Off-the-shelf detectors not trained on robot-specific data can be noisy, requiring conservative gating; grounding imperfections and low-level execution failures (navigation, grasping) still cause execution failures; without the gated chain-of-thought and grounding, ambiguous tasks suffer large planning drops (SayCan baseline lower on ambiguous instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>SayCan baseline: Unambiguous planning/execution 85%/57% (same planning for unambiguous), Ambiguous planning/execution 33%/25% (GD: 58%/44%). This shows GD improves disambiguation via multimodal chain-of-thought and token-level grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Constraining the influence of off-the-shelf detector (via LLM-chosen grounded regions / gating) improved reliability; removing grounded decoding (relying only on LLM or on SayCan without gated chain-of-thought) substantially reduced planning success on ambiguous instructions (~33% SayCan planning vs 58% GD planning).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large LLMs encode procedural and object-relational knowledge useful for decomposing and sequencing manipulation tasks, but they lack direct perceptual grounding. Prompting strategies (grounded chain-of-thought) that let the LLM decide when to consult visual detectors, combined with token-level grounded scoring, allow the LLM to utilize visual/spatial/object-relational information selectively and improve planning for ambiguous, real-world instruction-following without finetuning the LLM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can and not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Cliport: What and where pathways for robotic manipulation <em>(Rating: 1)</em></li>
                <li>Rt-1: Robotics transformer for real-world control at scale <em>(Rating: 1)</em></li>
                <li>Socratic models: Composing zero-shot multimodal reasoning with language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-540",
    "paper_id": "paper-33af17e7cc5cd4ee18c9e6e8c5fca7e224592ec0",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "Tabletop-GD",
            "name_full": "Simulated Tabletop Rearrangement with Grounded Decoding",
            "brief_description": "Grounded Decoding (GD) couples a frozen LLM (InstructGPT) with token-conditioned grounding functions (affordances from a CLIPort policy, rule-based safety and preference scorers) to iteratively decode executable, open-vocabulary action commands for multi-step tabletop manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_size": null,
            "model_description": "A frozen autoregressive instruction-tuned LLM (InstructGPT) used via prompts and few-shot examples; no fine-tuning on the robotic domains. The model provides implicit semantic and procedural knowledge in its weights and outputs natural-language step commands which are decoded token-by-token while being re-scored by grounded models.",
            "task_name": "Simulated Tabletop Rearrangement (RAVENS-based)",
            "task_description": "Long-horizon, multi-step rearrangement tasks (Letters, Blocks & Bowls, Box Packing) in a top-down RGB-D simulated tabletop environment. Given a high-level instruction, GD decodes a sequence of natural-language step commands which are executed by a pre-trained language-conditioned low-level primitive policy (CLIPort) producing pick/place actions.",
            "task_type": "object manipulation; multi-step planning; household tasks",
            "knowledge_type": "procedural + object-relational + spatial (sequencing of steps, object relations such as stacking/containment, spatial pick/place locations and reachability)",
            "knowledge_source": "LLM: pre-training on large text corpora and instruction tuning (InstructGPT); Grounding: CLIPort trained on demonstrations (supervised), plus rule-based safety and preference functions",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting / instruction prefixing to InstructGPT; token-level guided decoding (Grounded Decoding) combining LLM token probabilities with token-conditioned grounding scores (affordance logits, rule-based scorers); greedy and beam search variants",
            "knowledge_representation": "LLM: implicit procedural and semantic knowledge encoded in network weights and producible as natural-language action sequences; Grounding: explicit token-conditioned affordance scores (CLIPort per-pixel logits aggregated to a token score), and explicit rule-based binary/soft scorers for safety and preferences",
            "performance_metric": "Success rate (task completion %), averaged over episodes/tasks",
            "performance_result": "Grounded Decoding substantially improved success rates vs ungrounded LLM and solitary CLIPort baselines. Representative aggregated numbers (reported in paper): Seen tasks (GD Beam) — Letters 57%, Blocks & Bowls 77%, Box Packing 78%; Unseen tasks (GD Beam) — Letters 41%, Blocks & Bowls 50%. Ungrounded LLM and CLIPort variants are substantially lower (e.g., Ungrounded LLM seen tasks often ~20-35%; CLIPort Long varies and fails to generalize to unseen tasks). See Appendix Table 6 for per-task breakdown.",
            "success_patterns": "LLM reliably supplies high-level procedural sequencing (multi-step plans and decomposition); when coupled with affordance grounding, the system selects object tokens and verbs that are physically feasible (CLIPort affordance peaks guide selection of pick/place targets), and rule-based grounding enforces safety/preferences (avoids hazardous objects, prefers designated objects). Beam search further helps by considering full single-step instructions during decoding.",
            "failure_patterns": "Ungrounded LLMs frequently propose semantically plausible but scene-infeasible commands (e.g., selecting absent or unreachable objects). Remaining failures are often due to low-level primitive execution errors. When grounding is removed, performance drops dramatically (e.g., many Box Packing tasks drop from ~75–100% with grounding to single-digit or low-double-digit percentages ungrounded). Ambiguous instructions and tasks requiring fine scene detail without explicit grounding are problematic for the LLM alone.",
            "baseline_comparison": "Baselines include CLIPort (Short: single-step training, Long: trained on high-level instructions), Ungrounded LLM planner (LLM decoding without grounding). Example baseline numbers: CLIPort (Short) often very low (e.g., Letters ~5–15%); CLIPort (Long) higher on seen tasks but poor on unseen (e.g., Blocks & Bowls seen 62% in Table 1 but unseen low). Ungrounded LLM: seen Letters ~20%, Blocks&Bowls ~35%, BoxPacking ~11% (paper tables). GD outperforms these baselines consistently.",
            "ablation_results": "Ablation of grounding (i.e., using LLM decoding without grounding) reduces success drastically (examples: Box Packing tasks Ungrounded ~5–20% vs GD Greedy/Beam ~75–100%). Removing specific grounded components (affordance, safety, preference) causes predictable failure modes: without affordance, token choices are not grounded to feasible pick/place locations; without safety rules, hazardous interactions occur; without preference functions, user-specific soft-requirements are ignored. Beam search vs greedy: beam improves success (beam &gt; greedy).",
            "key_findings": "LLMs encode rich procedural and semantic knowledge (step decomposition, action sequencing, object-relational semantics) implicitly in their weights and can produce long-horizon plans in text; however, they lack scene- and embodiment-specific spatial/affordance knowledge. Token-level grounding functions (affordance/value nets, detectors, rule-based scorers) that score partially decoded token sequences can be combined with LLM decoding to steer generation toward physically realizable, safe, and preference-aware actions without finetuning the LLM, enabling open-vocabulary, actionable planning for manipulation.",
            "uuid": "e540.0"
        },
        {
            "name_short": "Minigrid-GD",
            "name_full": "2D Maze Navigation (Minigrid) with Grounded Decoding",
            "brief_description": "Grounded Decoding uses an InstructGPT planner together with a token-conditioned value (affordance) function derived from a PPO-trained goal-conditioned policy to plan long-horizon navigation sequences in Minigrid.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_size": null,
            "model_description": "Frozen instruction-tuned LLM used as a high-level planner (few-shot prompted) to sequence textual skill tokens; grounded by a learned goal-conditioned value function (from PPO) that maps partially decoded skill strings + observation to feasibility/affordance scores.",
            "task_name": "Minigrid 2D Maze (gym-minigrid variants)",
            "task_description": "Top-down full-grid 2D maze navigation tasks grouped into Easy (short horizon), Medium (longer, partly specified), and Hard (very long-horizon &gt;100 steps, ambiguous instructions). The LLM plans over low-level skills (e.g., go to &lt;obj&gt;, open &lt;obj&gt;) executed by a multi-task RL policy; grounding is provided by the token-conditioned value function evaluating feasibility given the current observation.",
            "task_type": "navigation; multi-step planning; exploration",
            "knowledge_type": "spatial + procedural (layout and reachability, sequential planning over navigation and key/door interactions)",
            "knowledge_source": "LLM: pre-training on text corpora (InstructGPT); Grounding: value function from policy trained with PPO on short-horizon skills; CLIP embeddings used to encode skill strings",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting; Grounded Decoding token-by-token combining LLM token probabilities with the token-conditioned value (affordance) function; greedy and beam search variants",
            "knowledge_representation": "LLM: implicit procedural plans as natural-language sequences; Grounding: explicit value/affordance estimates from a goal-conditioned RL policy (numeric scores evaluating feasibility of partially decoded skill strings given the observation), skill strings encoded with CLIP embeddings for conditioning",
            "performance_metric": "Success rate (task completion %), averaged across 100 episodes per difficulty",
            "performance_result": "Representative numbers reported: Baselines — PPO flat agent: Easy 28%, Medium 13%, Hard 6%; Hierarchical RL (HRL): Easy 68%, Medium 48%, Hard 31%; Ungrounded LLM planner: Easy 96%, Medium 87%, Hard 54%; GD (Greedy/Beam): Easy 100%, Medium ~93%, Hard 78% (beam improves especially on hard long-horizon tasks).",
            "success_patterns": "The LLM provides strong long-horizon procedural sequencing (decomposing goals into textual skills), and the token-conditioned value function supplies scene-specific reachability/feasibility (spatial knowledge) enabling selection among candidate skills; this combination is especially effective on hard, ambiguous, and long-horizon tasks requiring efficient exploration.",
            "failure_patterns": "Ungrounded LLM struggles when many skills are potentially viable but the LLM does not see observations (leads to poor selection among skill options); flat RL lacks high-level language reasoning causing poor performance; remaining failures include low-level policy errors and exploration inefficiency in very complex layouts.",
            "baseline_comparison": "Flat PPO: 28%/13%/6% (Easy/Medium/Hard); HRL over skills: 68%/48%/31%; Ungrounded LLM planning: 96%/87%/54%; GD improves these to nearly perfect on Easy and significantly on Hard (up to 78%).",
            "ablation_results": "Ablating the affordance/value grounding (i.e., using LLM planning without the token-conditioned value function) reduces performance notably on long-horizon/hard tasks (e.g., Hard drops from GD ~78% to Ungrounded ~54%). Beam search further improves success over greedy decoding.",
            "key_findings": "LLMs encode procedural knowledge enabling decomposition into skill sequences but cannot by themselves account for scene-specific spatial feasibility; token-conditioned value/affordance functions trained on embodiment data provide the necessary spatial/embodiment grounding and when combined at token-level with LLM decoding yield large gains in long-horizon navigation and planning.",
            "uuid": "e540.1"
        },
        {
            "name_short": "Kitchen-GD",
            "name_full": "Real-World Kitchen Mobile Manipulation with Grounded Decoding and Multimodal Chain-of-Thought",
            "brief_description": "In a real kitchen setup GD couples large LLMs (PaLM for planning, InstructGPT for chain-of-thought prompting) with off-the-shelf open-vocabulary detectors (owl-vit) and low-level RT-1/BC primitives to ground ambiguous instruction following via visually-grounded chain-of-thought and token-level grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM (540B) for planning; InstructGPT (text-davinci-002) for generating grounded chain-of-thought",
            "model_size": "540B (PaLM) for main planner; InstructGPT size not specified",
            "model_description": "PaLM (large pre-trained LLM) used as frozen high-level planner; InstructGPT used to elicit multimodal chain-of-thought where the LLM decides when to enable grounded decoding (square-bracket gating). Grounding provided by open-vocabulary object detector (owl-vit) and token-conditioned scoring (SayCan-like scoring mode) combined with low-level RT-1 / BC policies for execution.",
            "task_name": "Kitchen mobile manipulation (real world) instruction following",
            "task_description": "Robot must follow natural language instructions to fetch and deliver objects in a kitchen (examples: 'Bring an apple', 'Bring me the fruit'). Tasks split into Unambiguous (object named) and Ambiguous (object class only). Objects assumed in field of view; pipeline uses LLM planning, chain-of-thought to decide when to consult vision, detectors to ground object mentions, and lower-level learned and scripted controllers for execution.",
            "task_type": "mobile manipulation; instruction following; multi-step planning",
            "knowledge_type": "procedural + object-relational + spatial (sequencing of sub-tasks, object identification and relations, reachability/navigation to objects)",
            "knowledge_source": "LLMs: pre-training on large text + instruction tuning (PaLM and InstructGPT); Grounding: open-vocabulary detector (owl-vit), value- / affordance-style scoring via SayCan approach, low-level policies trained with BC and RT-1 data",
            "has_direct_sensory_input": false,
            "elicitation_method": "chain-of-thought prompting (InstructGPT) to produce text that indicates when to enable grounded decoding (gating); GD decoding uses the detector/value scores when grounded; final planning uses SayCan scoring mode on tokens; execution with RT-1/BC learned primitives",
            "knowledge_representation": "LLM: implicit procedural knowledge as natural-language plans and chain-of-thought tokens; Grounding: visual detections (open-vocab object labels & confidences) and value/affordance scores; rule-based gating controlling when to apply grounded scores; low-level policies provide embodiment-conditioned feasibility implicitly via learned controllers",
            "performance_metric": "Planning success rate (%) and execution success rate (%)",
            "performance_result": "Reported results: Unambiguous tasks — GD planning 85%, execution 57% (SayCan planning 85%, execution 57% as well); Ambiguous tasks — GD planning 58%, execution 44% vs SayCan planning 33%, execution 25%. GD thus yields a ~25% absolute planning improvement on Ambiguous tasks over SayCan.",
            "success_patterns": "Using multimodal chain-of-thought that gates when to consult vision enables the LLM to disambiguate object classes into specific visible objects using the detector scores; LLM provides procedural ordering (find → pick → bring → place) and detectors supply object-relational grounding (which fruits are present) and spatial reachability informs feasible plan selection.",
            "failure_patterns": "Off-the-shelf detectors not trained on robot-specific data can be noisy, requiring conservative gating; grounding imperfections and low-level execution failures (navigation, grasping) still cause execution failures; without the gated chain-of-thought and grounding, ambiguous tasks suffer large planning drops (SayCan baseline lower on ambiguous instructions).",
            "baseline_comparison": "SayCan baseline: Unambiguous planning/execution 85%/57% (same planning for unambiguous), Ambiguous planning/execution 33%/25% (GD: 58%/44%). This shows GD improves disambiguation via multimodal chain-of-thought and token-level grounding.",
            "ablation_results": "Constraining the influence of off-the-shelf detector (via LLM-chosen grounded regions / gating) improved reliability; removing grounded decoding (relying only on LLM or on SayCan without gated chain-of-thought) substantially reduced planning success on ambiguous instructions (~33% SayCan planning vs 58% GD planning).",
            "key_findings": "Large LLMs encode procedural and object-relational knowledge useful for decomposing and sequencing manipulation tasks, but they lack direct perceptual grounding. Prompting strategies (grounded chain-of-thought) that let the LLM decide when to consult visual detectors, combined with token-level grounded scoring, allow the LLM to utilize visual/spatial/object-relational information selectively and improve planning for ambiguous, real-world instruction-following without finetuning the LLM.",
            "uuid": "e540.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can and not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation",
            "rating": 1
        },
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale",
            "rating": 1
        },
        {
            "paper_title": "Socratic models: Composing zero-shot multimodal reasoning with language",
            "rating": 1
        }
    ],
    "cost": 0.01998825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents</h1>
<p>Wenlong Huang ${ }^{1^{<em>}}$, Fei Xia ${ }^{2}$, Dhruv Shah ${ }^{3}$, Danny Driess ${ }^{2}$, Andy Zeng ${ }^{2}$, Yao Lu ${ }^{2}$, Pete Florence ${ }^{2}$, Igor Mordatch ${ }^{4}$, Sergey Levine ${ }^{23}$, Karol Hausman ${ }^{1}$ and Brian Ichter ${ }^{2}$<br></em>Work done as an intern at Google, ${ }^{1}$ Stanford University, ${ }^{2}$ Robotics at Google, ${ }^{3}$ UC Berkeley, ${ }^{4}$ Google Research</p>
<p>Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models. The project's website can be found at grounded-decoding.github.io.</p>
<h2>1. Introduction</h2>
<p>Recent works have demonstrated robots that are increasingly proficient at understanding and acting upon natural language, whether through planning or conditioned policies. Complementing such progress, the field of natural language processing has recently seen large language models (LLMs) become ubiquitously used as pre-trained or few-shot prompted models, due to their impressive few-shot performance and vast knowledge-base. These LLMs have efficiently learned from web-scale data through
autoregressively modeling the probability distribution over text tokens and thus generate text. However, the nature of this process is such that applying such models to embodied settings remains a challenge. They have not interacted with their environment, lack observability of non-language observation modalities (e.g., images), and may not know what is safe or possible for a particular embodiment.</p>
<p>Determining how to execute long-horizon behaviors based on high-level verbal commands is one
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Grounded Decoding solves robotic tasks by taking an instruction as input and selecting tokens that have high probability under a Large Language Model (LLM) and a set of Grounded Models (GM). Thus, it leverages the open-vocabulary and semantic knowledge of LLMs while being grounded in the environment and in the robot's capabilities. Furthermore, the whole process does not require expensive fine-tuning of the LLM.</p>
<p>particular area of robotics where the rich semantic knowledge in large language models can be especially useful. This problem combines elements of semantic reasoning and planning: the robot must understand the instruction, determine the steps needed to fulfill it, and also determine how to sequence those steps appropriately given its capabilities and the current state of the environment. However, this is not a problem that can be solved purely with semantics, as it requires sufficient grounding to understand how the task should be performed in context - for example, in the example in Figure 1, the language model alone has no way of knowing which block to pick up because this requires knowledge of which blocks are present, and also what manipulations the robot is capable of performing on them. Thus, although a language model can assign probabilities for how likely various steps are to correspond to the desired task semantically, the constraints of the planning problem must also enter into the process. These constraints could themselves be represented as probabilities that mirror the token probabilities generated by a language model, reflecting their applicability to the current environment rather than their semantic likelihood. We can frame this as a problem similar to probabilistic filtering: decode a sequence (i.e., a task description) that both has a high probability under the language model and a high probability under a grounded model that predicts how applicable this sequence is to the current scene.</p>
<p>Herein, we present Grounded Decoding (GD), a scalable, general approach to planning with LLMs embodied domains. Grounded Decoding jointly decodes the token probability of an LLM and token probabilities from token-conditioned, robotic functions, such as affordance functions capturing the abilities of a robot given its embodiment, safety functions, or more. By guiding the LLM directly at its output, Grounded Decoding enables a general and flexible family of planning algorithms that combines LLM's strength of long-horizon and semantic reasoning and grounded models' strength of local and embodiment grounding.</p>
<p>Our contributions are as followed: 1) we present a robot-centric formulation for decoding language models to perform long-horizon robotic tasks with token-conditioned grounded models, 2) we demonstrate techniques for learning such grounded models, serving different purposes such as affordances and safety requirements, and 3) we show empirical
evidence, across three simulation and real-world domains, that the proposed method performs strongly on a wide range of tasks while also significantly outperforming prior methods in efficiency.</p>
<h2>2. Related Work</h2>
<p>Guided Decoding for Language Models. Decoding strategies for large language models is an active area of research within natural language processing $[1,2,3,4,5]$. A number of recent works have focused on developing decoding heuristics for natural text generation $[6,7,8,9,3,10,11,12]$. Another line of works use external classifiers for maximizing certain language-space utilities when decoding language models $[13,14,15,16,17,18,5,19,20,21]$. Most closely related to our work are classifier-guided decoding methods developed for offline domains, such as image captioning [22, 23] and task-oriented dialog [24, 25]. However, extensions to embodied domains, which we investigate exclusively in this work, remain non-trivial because grounding in embodied domains is bounded by the abilities of the agent and by environment state transition as the agent actively interacts with the environment.
Embodied and Multimodal Language Models. Training language models to understand embodiment is an active area of research. Training multimodal models can enable some degree of embodied reasoning, such as understanding images and videos [26, 27, 28, 29]. Directly finetuning language models to output actions has also been investigated [30, 31, 32]. Lastly, training downstream models on language model embeddings shows promise [33, 34, 35, 36, 37, 38]. In this work, we investigate leveraging large frozen language models for embodied applications [39, 40, 41, 42, 43, 44, $45,46,47,48,49,50,51,52,53,54,55,56,57,58,59]$, with grounded models to provide domain-specific grounding during decoding process.
Comparison to SayCan. The most closely related work to our work is SayCan [40]. SayCan uses a large language model and a value function to select robotic skills among a constrained set of primitives. This constrained set of primitives enables SayCan to use the so-called "scoring-mode" of the LLM to get the probability of a skill being useful to a highlevel instruction. This requirement to consider only a fixed and enumerated set of primitives limits the applicability of SayCan in scenarios with many possible skills, such as open vocabulary or combinatorial</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of Grounded Decoding. Given a free-form language instruction, a language model and grounded models jointly decide the next candidate token to be decoded by combining their respective likelihood. Language model proposes likely tokens that produce goal-directed and coherent long-horizon behaviors, while grounded models connect them to the physical scene, through a flexible composition of multiple objective functions from multiple modalities, such as affordance, preferences, and safety.
tasks. Grounded Decoding on the other hand jointly decodes the LLM and the grounded model at the token level, allowing for expressive decoding with an open vocabulary. Furthermore, SayCan considers only grounding functions derived from RL-trained value functions for affordance grounding functions, while Grounded Decoding explores many types of grounding functions to propose a broad family of algorithms.
Task and Motion Planning. Task and motion planning [60] seeks to solve high-level instructions via sequencing tasks in dynamically feasible manner. Research within this area generally focuses on symbolic planning [61] or optimization-based [62] approaches. Machine learning has increasingly been used to accelerate planning and enable new domains [63, 64, $65,66,67,68,69,70,71,72,73,74,75,76]$. However, planning constraints are often explicitly specified for TAMP methods. In contrast, we specify constraints as (learned) probabilities, which are baked into the decoding process and provided by domain-specific grounded models.
Comparison to SayCan. The most closely related work to our work is SayCan [40]. SayCan uses a large language model and a value function to select robotic skills among a constrained set of primitives. This constrained set of primitives enables SayCan to use the so-called "scoring-mode" of the LLM to get the probability of a skill being useful to a highlevel instruction. This requirement to consider only a fixed and enumerated set of primitives limits the applicability of SayCan in scenarios with many possible skills, such as open vocabulary or combinatorial tasks. Grounded Decoding on the other hand jointly
decodes the LLM and the grounded model at the token level, allowing for flexible and expressive decoding of open vocabulary tasks. Furthermore, SayCan considers only grounding functions derived from RL-trained value functions for affordance grounding functions, while Grounded Decoding explores many types of grounding functions to propose a broad family of algorithms.
Task and Motion Planning. Task and motion planning [60] seeks to solve high-level instructions via sequencing tasks in dynamically feasible manner. Research within this area generally focuses on symbolic planning [61] or optimization-based [62] approaches. Machine learning has increasingly been used to accelerate planning and enable new domains [63, 64, $65,66,67,68,69,70,71,72,73,74,75,76]$. However, planning constraints are often explicitly specified for TAMP methods. In contrast, we specify constraints as (learned) probabilities, which are baked into the decoding process and provided by domain-specific grounded models.</p>
<h2>3. Grounded Decoding</h2>
<h3>3.1. LLMs and Grounding Models</h3>
<p>Large Language Models. LLMs are trained to predict the probability $p(W)$ of a text sequence $W$, represented as a sequence of tokens $W=$ $w_{1: N}=\left(w_{1}, \ldots, w_{N}\right)$. The tokens are elements of a fixed vocabulary $\mathcal{W}$. Typical neural architectures factorize the joint probability into $p(W)=$ $\prod_{n=1}^{N} p_{\text {LLM }}\left(w_{n} \mid w_{1: n-1}\right)$, where $p_{\text {LLM }}$ is predicted by a transformer network [77]. Given $p_{\text {LLM }}$, generating a text consisting of $N$-many tokens, the so-called decoding process, can be seen as the optimization prob-</p>
<p>lem $\arg \max <em 1:="1:" N="N">{w</em>\right)$, which in practice is solved, e.g., using greedy search, beam search, or sampling strategies. To further ensure the LLM is solving a desired task, one typically starts with a given text, the so-called prefix or prompt, that describes the task, and then the LLM completes this task in its decoding process.} \in W} \prod_{n=1}^{N} p_{\mathrm{LLM}}\left(w_{n} \mid w_{1: n-1</p>
<p>Grounding Functions. We use the concept of grounding functions, $p_{\mathrm{G}}\left(w_{1: n} \mid s\right)$, which seek to model a probability of tokens $w_{1: n}$ given (potentially non-textual) state $s \in S$. This state is intended to capture the embodiment of the robot and the environment, which may be an image, proprioception of the robot, or the environment state. Thus the grounding function models probabilities relevant to the robot embodiment and environment, such as whether the tokens are possible for the robot to execute given the state (affordances), or other values like safety, cost, or user preferences.</p>
<h3>3.2. Problem formulation.</h3>
<p>Given an instruction in language $\ell$, we look at the problem of using an LLM to decode a language plan $w_{1: N}$, which is typically done by finding the most likely tokens under the probability distribution predicted by the LLM, $p_{\mathrm{LLM}}\left(w_{1: N} \mid \ell\right)$, with $\ell$ being the prefix. However, based on the instruction $\ell$ as the prefix alone, the LLM can easily generate text that is not grounded in the physical state of the environment, rendering such plans useless in the real world. In order to ground the language model in an actual physical embodiment, we propose Grounded Decoding (GD): The main idea of GD is to guide the generation of token sequences with grounding function(s) that are conditioned on the embodiment of the system.</p>
<p>Formally, let $s \in S$ denote a representation of the state of the world. Then, GD attempts to generate text that is consistent with both the instruction $\ell$ and the physical state $s$ :</p>
<p>$$
w_{1: N}^{*}=\arg \max <em 1:="1:" N="N">{w</em> \mid s, \ell\right)
$$}, w_{n} \in W} p_{\mathrm{GD}}\left(w_{1: N</p>
<p>To leverage the Internet-scale knowledge of LLMs,
we factorize $p_{\mathrm{GD}}\left(w_{1: N} \mid s, \ell\right)$ as follows ${ }^{1}$ :</p>
<p>$$
\begin{aligned}
p_{\mathrm{GD}}\left(w_{1: N} \mid s, \ell\right) &amp; =\frac{p\left(s, \ell \mid w_{1: N}\right) p\left(w_{1: N}\right)}{p(s, \ell)} \
&amp; =\frac{p\left(s \mid w_{1: N}\right) p\left(\ell \mid w_{1: N}\right) p\left(w_{1: N}\right)}{p(s, \ell)} \
&amp; =\frac{p\left(w_{1: N} \mid \ell\right) p(\ell) p\left(w_{1: N} \mid s\right) p(s) p\left(w_{1: N}\right)}{p(s, \ell) p\left(w_{1: N}\right) p\left(w_{1: N}\right)} \
&amp; \propto \frac{p\left(w_{1: N} \mid \ell\right)}{p\left(w_{1: N}\right)} p\left(w_{1: N} \mid s\right) \
&amp; \propto p\left(w_{1: N} \mid \ell\right) p\left(w_{1: N} \mid s\right)
\end{aligned}
$$</p>
<p>To decode autoregressively with the formulation, we factorize above into token decoding:</p>
<p>$$
p_{\mathrm{GD}}\left(w_{1: N} \mid s, \ell\right) \propto \prod_{n=1}^{N} p_{\mathrm{LLM}}\left(w_{n} \mid w_{1: n-1}, \ell\right) p_{\mathrm{G}}\left(w_{1: n} \mid s\right)
$$</p>
<p>The first term, $p_{\mathrm{LLM}}\left(w_{n} \mid w_{1: n-1}, \ell\right)$, can be modeled as the probability of the LLM predicting the token for the given instruction $\ell$ appended previously decoded tokens $w_{1: n-1}$ without the state $s$ as input. The second term, $p_{\mathrm{G}}\left(w_{1: n} \mid s\right)$, is the grounding function that is only conditioned on the state $s$ and judges whether the generated text $w_{1: n}$ is consistent with the physical state. The core idea behind this factorization is that LLMs exhibit long-term planning capabilities, while the grounding function guides the planning of the LLM to be possible in the concrete embodied physical world without needing to be informed or capable of reasoning over the long-horizon instruction.</p>
<h3>3.3. Algorithm - Grounded Decoding.</h3>
<p>This work investigates grounded decoding exclusively in the context of task planning for embodied agents. Figure 2 visualizes a single step of the simplest greedy search form of GD, and accompanying pseudo-code can be found in Algorithm 1. Given a high-level language instruction and history of executed actions, GD proceeds through a process similar to probabilistic filtering by selecting tokens iteratively that have high probability under the language model and the grounded model. After each token is selected, it is appended to the prefix. The process</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>continues until a token in the terminal set $\mathcal{W}<em 1:="1:" i="i">{\text {term }}$ is selected, which could be a period sign "." indicating the end of a single-step skill (e.g., pick-and-place). Then the command $w</em>$ Additionally, we note that GD, in its essence, provides a grounded scoring function; thus, it can be easily extended to any search methods such as beam search, top-k sampling, etc.}$ is sent to a language-conditioned policy $\pi\left(a \mid s, w_{1: i}\right)$ that executes the action $a$ conditioned on the environment state $s$. Crucially, this grounding function must accept partial commands to enable grounding during decoding. ${ }^{2</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Grounded Decoding (GD) w/ Greedy
Search
    Given: state \(s\), instruction \(\ell\), terminal set \(\mathcal{W}_{\text {term }}\)
    Initialize: \(w=\{ \}, n=0\)
    while \(w_{n} \notin \mathcal{W}_{\text {term }}\) do
        \(n=n+1\)
        \(w_{n}=\arg \max <span class="ge">_{w_</span>{n} \in \mathcal{W}} p_{\mathrm{LLM}}\left(w_{n} \mid w_{1: n-1}, \ell\right) p_{\mathrm{G}}\left(w_{1: n} \mid s\right)\)
    Return: \(w\)
</code></pre></div>

<h3>3.4. Techniques for Obtaining Grounding</h3>
<p>Unlike language tasks, where a single model is capable of performing general semantic reasoning, a singular grounded model remains an open problem. Indeed, each domain may impose varied environmental and embodiment constraints. Despite these challenges, we present several techniques for obtaining grounded models that can be leveraged in GD's formulation, and validate them in three domains in Section 4.</p>
<p>Token-Conditioned Value Functions. Assuming a robot acts with action $a$ according to policy $\pi\left(a \mid s, w_{1: n}\right)$, that aims to maximize certain a utility and that the utility captures a task objective, a natural choice that can provide "grounding score" is the action-value function $Q\left(s, a \mid w_{1: n}\right)$ as it necessarily captures the embodiment of the robot. Additional objectives, such as task constraints, can also be encapsulated in $Q\left(s, a \mid w_{1: n}\right)$ to ensure grounding. Note that unlike the formulation proposed in [40], $w_{1: n}$ cannot be restricted to a fixed repertoire of token sequences. In practice, to obtain a $Q\left(s, a \mid w_{1: n}\right)$ that satisfies the requirements, one can train multi-task language-conditioned agents, either through rein-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>forcement learning (Section 4.2) or supervised learning (Section 4.1).</p>
<p>Multimodal Foundation Models. A general choice to ground LLMs is through using multimodal foundation models, such as CLIP [78] or open-vocabulary object detectors [79, 80, 81]. Although these models can connect language to other grounded modalities (e.g., vision), they often lack the capability for complex or long-horizon reasoning, and they do not consider embodiment constraints. As a result, to leverage them in the decoding process, they need to constrained to where they are the most applicable rather than always decoding jointly. To this end, we use a prompt-based technique that allows LLMs to choose when to jointly decode (Section 4.3), which we find to be effective in most cases. ${ }^{3}$.</p>
<p>Rule-based Methods. Another source of grounding may come from features $x=\phi\left(w_{1: n}\right)$ designed with expert domain knowledge, which can then be used to map $w_{1: n}$ to a "grounding score" using pamametric or non-parametric functions $f(x)$. Such techniques may be most applicable when interpretability and enforcing hard constraints are required, such as safety-critical settings, or when data are scarce, such as cases involving preferences of individual users (as shown in Section 4.1).</p>
<h3>3.5. Comparisons to Prompt-based Methods</h3>
<p>One alternative approach for grounding is including scene information as part of the prompt (e.g., object detection results [41]), which complements the grounding method proposed in this work. However, we note that prompting is often insufficient for grounding, as information about the scene and about the capabilities of the robot may not always be succinctly described in the prompt. Such examples include 1) in a block stacking task, a block that has been stacked on cannot be picked, 2) in a navigation task, to open a door, one must have a key and that door must be reachable, and 3) in a mobile manipulation domain, an object may be visible but is out of reach of the manipulator. Therefore, Grounded Decoding is a more general and flexible grounding method that injects continuous probabilities during decoding, which may even come from grounding</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example rollouts and likelihood of representative tokens under Grounded Decoding objective in three distinct domains: simulated tabletop rearrangement (top), Minigrid 2D Maze (middle), and real-world kitchen mobile manipulation (bottom). Each domain uses different prompts, grounded models, and low-level primitives. The GD formulation is shared across the domains, decoding a pre-trained langauge model with respect to domain-specific grounded models to decompose a open-ended instruction into actionable steps.</p>
<p>functions from other modalities (e.g., vision).</p>
<h2>4. Experiments</h2>
<h3>4.1. Long-Horizon Tabletop Manipulation</h3>
<p>Herein we experiment with a simulated tabletop manipulation environment based on RAVENS [83]. We create a custom set of 20 tasks, with 10 seen tasks and 10 unseen tasks. Seen tasks are used for training (for supervised baseline) or for few-shot prompting. They are grouped by following categories. Detailed breakdown can be found in Appendix A.2.</p>
<p>i. <strong>Letters</strong>: Rearranging alphabetical letters ("sort the letters in alphabetical order").</p>
<p>ii. <strong>Blocks &amp; Bowls</strong>: Rearranging or combining blocks and bowls ("put blocks in matching bowls").</p>
<p>iii. <strong>Box Packing</strong>: Sorting food items and utensils into boxes in accordance with safety constraints and user preferences ("Can you pack the picnic box for me?").</p>
<p>Given only high-level language instructions and top-down visual observation of the environment, Grounded Decoding decodes a sequence of text tokens representing the step command to be executed. Note that because GD generated grounded free-form actions, it does not require each step to strictly map to a repertoire of skill as in [39, 40]. After a complete command is generated, it is executed via a pre-trained multi-task language-conditioned CLIPort [84]. An example rollout is shown in Figure 4. To demonstrate the techniques proposed in Section 3.4 to obtain grounding functions, we study the composition of the following grounding functions (overall grounding score is calculated as $$p_G = \prod_{i=1}^{n} p_i$$) depending on the task categories. Refer to the Appendix A.2 for details.</p>
<p><strong>Affordance Grounding Function (AF).</strong> As the primitive policy CLIPort [84] already acts as an action-value function over the pixel space, we directly leverage its predictions for affordance grounding. In particular, given scene image $$s$$ and partially-decoded instruction $$w_{1:n}$$, CLIPort predicts unnormalized logits over the pixel space $$u_{\text{pick}}, u_{\text{place}} \in \mathbb{R}^{480 \times 640}$$, respectively for the pick location and the place location. Therefore, for any given $$s$$ and $$w_{1:n}$$, we can calculate the affordance as $$p_{\text{AF}}(w_{1:n}|s) = \max_{(x,y) \in 480 \times 640} (u_{\text{pick}}(x, y) + u_{\text{place}}(x, y))$$.</p>
<p><strong>Safety Grounding Function (S).</strong> To strictly enforce hard constraints such as safety requirements, we adopt the rule-based method proposed in Section 3.4. In particular, the features $$x$$ are indicator functions denoting whether knives and red boxes which we use as hazardous objects are involved in an action, i.e., $$x = \mathbb{I}$$ ("red" or "knife" in $$w_{1:n}$$). We then use constant mappings to convert the features</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Example decision-making with GD, where key decoded tokens are shown (yellow, purple, red, yellow). For each of the key token position, we show the top 10 candidates with their likelihoods under the language model, grounded model, and both. Combined scores are normalized to the maximum for visual clarity; others are normalized to the sum for showing their relative confidence. While only greedy search is illustrated for simplicity, alternative search methods may achieve better results, as shown in Section 4.
$p_{5}\left(w_{1: n} \mid s\right)=\frac{x}{Z} x+\frac{1}{Z}(1-x)$ to scores of 0 or 1 , where $Z$ is the normalizing term and $\epsilon \approx 0$ is a small value for ensuring the joint probability does not collapse to 0 .</p>
<p>Preference Grounding Function (P). We similarly use rule-based methods for preference grounding, as data of individual users may be scarce to learn a separate model. In particular, we choose two random objects $\left(o_{1}, o_{2}\right)$ as the preferred objects, i.e., $x=\mathbb{I}\left[o_{1}\right.$ or $o_{2}$ in $\left.w_{1: n}\right]$. Note that unlike safety functions, preferences often come in the form of "soft requirement". Therefore, the preference grounding function is implemented as $p_{\mathrm{P}}\left(w_{1: n} \mid s\right)=\frac{\alpha}{Z} x+\frac{\beta}{Z}(1-x)$, where we choose $\alpha=0.5$ and $\beta=0.1$ for our experiments.</p>
<p>Baselines. We study two variants of GD using beam search and greedy search. We also compare to "No Grounding" baseline that decodes only according to language model likelihood. Furthermore, we compare to solitary method CLIPort [84] that directly take in the high-level language instructions without a planner. We consider two variants of CLIPort: 1) "Short" that is trained with only singlestep pick-and-place commands, and 2) "Long" that is trained on high-level instructions from the 10 training tasks. For more details, please refer to Appendix A.2.</p>
<p>Analysis. Results grouped by each task category are shown in Table $1^{4}$. Please refer to the Appendix for detailed breakdown. Each method is evaluated</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  | CLIPort |  | +LLM | +GD |  |
| :--: | :--: | :--: | :--: | :--: | :--: |
|  | Short | Long | Ungrounded | Greedy | Beam |
| Seen Tasks |  |  |  |  |  |
| Letters | 7\% | $40 \%$ | $20 \%$ | $43 \%$ | 57\% |
| Blocks \&amp; Bowls | 2\% | $62 \%$ | $35 \%$ | $60 \%$ | 77\% |
| Box Packing* | $15 \%$ | $28 \%$ | $11 \%$ | $79 \%$ | $78 \%$ |
| Unseen Tasks |  |  |  |  |  |
| Letters | $6 \%$ | $10 \%$ | $19 \%$ | $37 \%$ | 41\% |
| Blocks \&amp; Bowls | $6 \%$ | $10 \%$ | $28 \%$ | $44 \%$ | 50\% |</p>
<p>Table 1: Average success rate for each tabletop task category. *Box Packing tasks are seen during training, but safety and preference requirements are only enforced during evaluation.
on 20 episodes for each task within each task category. Supervised methods, such as CLIPort, are found to perform poorly on unseen tasks. Methods that leverage language model planner show better generalization to unseen tasks but fall short due to lack of grounding. Grounded Decoding achieves the best results by enabling the LLM to plan actions using grounded information and is further improved with beam search.</p>
<h1>4.2. 2D Maze</h1>
<p>We further evaluate the long-horizon reasoning of Grounded Decoding for 2D maze-solving on Minigrid [85]. The agent receives a top-down view of the environment along with a natural language instruction. More details can be found in Appendix A.3. The tasks are grouped in three categories:</p>
<p>i. Easy: Simple tasks where the horizon is short (10-30 steps) and fully described by the textual instruction, e.g. OpenDoors and PutNext.
ii. Medium: Short and long-horizon tasks (up to 80 steps) with step-by-step textual instructions, e.g. LockedRoom.
iii. Hard: Complex, long-horizon instructions (over 100 steps) with ambiguous instructions that necessitate multi-step reasoning and efficient exploration, e.g. MultiRoom and BlockedUnlock.</p>
<p>Affordance Grounding Function. Following the recipe from Section 3.3, we train tokenconditioned affordance function to be used in GD. The difference is that the grounding function here is the value function from the goal-conditioned policy that is trained with PPO [86] instead of from demonstrations as in CLIPort [84]. The policy performs short-horizon skills such as "Go to red key" or "Open the door" and are conditioned on CLIP embeddings of the skill and an image of the scene. Accordingly, the goal-conditioned value function evaluates the feasibility given the current observation and the (partially) decoded skill.</p>
<p>Baselines. We compare the two variants of GD with greedy and beam search - with 1) a solitary PPO policy [86], 2) a hierarchical RL algorithm which plans over the low-level skills, and 3) a hierarchical method that uses an ungrounded language model for planning [39].</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">+Skills</th>
<th style="text-align: center;">+LLM</th>
<th style="text-align: center;">+GD</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">PPO</td>
<td style="text-align: center;">HRL</td>
<td style="text-align: center;">Ungrounded</td>
<td style="text-align: center;">Greedy Beam</td>
</tr>
<tr>
<td style="text-align: left;">Easy</td>
<td style="text-align: center;">$28 \%$</td>
<td style="text-align: center;">$68 \%$</td>
<td style="text-align: center;">$96 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">$13 \%$</td>
<td style="text-align: center;">$48 \%$</td>
<td style="text-align: center;">$87 \%$</td>
<td style="text-align: center;">$93 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Hard</td>
<td style="text-align: center;">$6 \%$</td>
<td style="text-align: center;">$31 \%$</td>
<td style="text-align: center;">$54 \%$</td>
<td style="text-align: center;">$\mathbf{7 8 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 2: 2D Maze success rates.</p>
<p>Analysis. Table 2 reports the success rate, averaged across 100 episodes of randomly initialized environments. The "flat" RL agent performs poorly in all but the simplest environments, owing to difficulties with understanding the high-level instruction and reasoning over long horizons (often over 100 steps). Planning over low-level skills using hierarchical RL [87] improves this performance, since the high-level decision-making problem is greatly simplified. However, the high-level RL agent still needs to reason over low-level (textual) skills by understanding their underlying capabilities and stitching them together. Using the planning capabilities of large language models to reason over textual skills significantly boosts this performance [39], since the language model can inherit the strong reasoning capabilities from its training data. This tends to be insufficient in challenging environments, however, since the number of potentially viable skills may be very large and the LLM has no information about the robot's observations. GD can leverage the learned affordance function (in this case, the goal-conditioned value function) to inform the language model's plans, enabling successful long-horizon reasoning. We further find that beam search improves performance modestly, particularly in long-horizon tasks.</p>
<h3>4.3. Mobile Manipulation in a Physical Kitchen</h3>
<p>Our last environment is a kitchen robot in the real world, and we follow the same implementations of the mobile manipulation platform and skills in SayCan [40]. We perform instruction following tasks, as in [40]. An example task is "Bring an apple", for which the robot needs to plan and execute a sequence of "1. Find an apple, 2. Pick up the apple, 3. Bring it to you. 4. Put it down, 5. Done". We split the tasks into two categories. Unambiguous means the instruction explicitly contains the object of interest, and Ambiguous means the instruction does not contain the object name. For example, when human asks "bring me the fruit", the robot needs to first determine available fruits. We assume all necessary objects are in the field of view. More details can be found in Appendix A.4.</p>
<p>Grounded Decoding with Chain-of-thought. We demonstrate using multimodal foundation models for Grounded Decoding, as proposed in Section 3.4. In particular, we use open-vocabulary object detector owl-vit [81]. Note that because these off-the-shelf models are not trained on robot domain data, we find that it works best by constraining their influence on decoding. We achieve this by making a slight modification to the SayCan algorithm [40]: before generating action plans, we prompt the LLM to generate visually-grounded chain-of-thought [88] by giving LLM the option of when to enable grounded decoding and disable grounded decoding, as visualized in Fig. 5. Specifically, LLMs can be prompted to generate a left bracket to start decoding jointly with grounded models and a right bracket to revert to ungrounded decoding. After chain-of-thought, we</p>
<p>use SayCan scoring mode for decoding the action plans.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GD (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SayCan</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tasks</td>
<td style="text-align: center;">Planning Execution</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Planning Execution</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Unambiguous</td>
<td style="text-align: center;">$85 \%$</td>
<td style="text-align: center;">$57 \%$</td>
<td style="text-align: center;">$85 \%$</td>
<td style="text-align: center;">$57 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ambiguous</td>
<td style="text-align: center;">$58 \%$</td>
<td style="text-align: center;">$44 \%$</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">$25 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Kitchen task planning and execution success.
Analysis. Table 3 shows that GD recovers similar performance on Unambiguous tasks, and gain 25\% in planning performance on Ambiguous tasks. This shows that GD with multimodal foundation models can effectively use visually-grounded chain-ofthought to disambiguate abstract tasks.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Example prompt and rollout in real-world kitchen environment.</p>
<h2>5. Analysis</h2>
<h3>5.1. Comparison to SayCan</h3>
<p>In this section, we directly compare GD to SayCan [40], which is related to our method in that both combine language model knowledge and grounded model knowledge (discussed in more detail in Section 2). However, SayCan uses the language model to score all pre-specified options, rendering it inefficient at dealing with large or combinatorial action spaces. In contrast, GD computation considers all possible language token in the autoregressive decoding process, which is independent of the size of the action space. Results shown in Table 4 demonstrate that GD is two orders of magnitude more efficient on our tasks, with comparable performance. Furthermore, by decoding at the most basic functioning unit of language, GD's formulation allows
open-vocabulary grounding beyond just affordances, e.g. safety, preferences, and multimodal embeddings such as CLIP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GD (Greedy)</th>
<th style="text-align: center;">GD (Beam)</th>
<th style="text-align: center;">SayCan</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Success Rate</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$\mathbf{6 4 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Token Count</td>
<td style="text-align: center;">$\mathbf{1 x}$</td>
<td style="text-align: center;">4.3 x</td>
<td style="text-align: center;">113.7 x</td>
</tr>
</tbody>
</table>
<p>Table 4: By avoiding full enumeration of the skills, GD is more efficient than SayCan while staying performant.</p>
<h3>5.2. Breakdown of Failure Reasons</h3>
<p>Because all hierarchical approaches share an imperfect low-level policy for step execution, the results reported in Table 1 are compounded with both planning failures and low-level policy failure. In Figure 6, we provide failure breakdown analysis for Grounded Decoding and associated baselines. Note that the CLIPort baselines are solitary methods that do not use a planner, so the failures are solely composed of policy failures. As shown in Figure 6, while all planning-based methods use the same underlying low-level policy, Grounded Decoding significantly reduces planning failure by being able to incorporate grounded scene information into the decoding process. Moreover, we observe that despite the shared affordance function across beam and greedy search, the beam search variant performs stronger by being aware of the full-length single-step instructions during decoding.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of actions colored by affordance values in different scenes. Every dot represents a possible action in the tabletop domain, where the majority of the actions are infeasible. We show how grounded models can identify the feasible actions for specific scenes. Notably, these actions are not always clustered in language space, requiring the grounding function to determine what action to perform.</p>
<h3>5.3. Grounded Action Manifold</h3>
<p>A central goal of this work is to investigate the integration of grounded information into language model decoding to output instructions actionable by a policy. To investigate this, we use a t-SNE [89] plot to illustrate the extent to which grounded models help narrow down the search space for language models. Specifically, we first enumerate all meaningful instructions in the tabletop domain, such as "pick x and place it on y," which are represented as dots in the figure. We then compute the affordance values with respect to four different scenes, where each color represents one scene. Finally, we group the dots using t-SNE and BERT embeddings [90]. Figure 7 shows that grounded models can effectively identify achievable skills to produce an actionable manifold within the language space and that this grounding is required, as language alone does not perfectly group actionable skills. It is worth noting that while we provide manual enumeration of all possible skills for practical analysis, the full language space is much larger. This highlights the even more pronounced narrowing of the search in the language space.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Visualization of actions colored by affordance values in different scenes. Every dot represents a possible action in the tabletop domain, where the majority of the actions are infeasible. We show how grounded models can identify the feasible actions for specific scenes. Notably, these actions are not always clustered in language space, requiring the grounding function to determine what action to perform.</p>
<h2>6. Conclusions, Limitations, \&amp; Future</h2>
<p>We presented Grounded Decoding (GD), an approach for leveraging the knowledge and capabilities of large language models in embodied settings through grounding functions, which model the probabilities of tokens given an embodiment. GD resembles probabilistic filtering, by decoding tokens that have high probabilities under the language model and under grounded model(s). By guiding the LLM's decoding directly at its output, GD is a general, flexible,
and expressive approach to embodied tasks. This is demonstrated on three embodied domains, showing GD is capable of solving complex, long-horizon tasks.</p>
<p>Though quite general and flexible, GD has a few limitations. First, although we present several techniques for obtaining grounding functions in different domains, it remains a question whether a capable and general grounding function can be obtained. We hope that recent progress in large-scale robotics models (e.g. [91] and [92]) can remove this bottleneck, and note that the flexibility of GD allows such progress to be straightforwardly leveraged. Second, prompt engineering is often required to steer LLMs to the desired action space (e.g., likely action verbs, likely present objects). Finally, while not requiring additional training, the joint decoding may be limiting compared to a single model capable of both grounding and language reasoning [29, 26, 54].</p>
<p>This work presented a family of algorithms for grounding LLMs in embodiment, for which there are many avenues for future work. The flexibility of the approach enables many other grounding functions and ways to integrate grounding. Furthermore, the development and integration of a foundation model for grounding would improve performance significantly. Finally, though GD's probabilistic filteringbased approach is quite general, fusing grounding information to the language model after each token decoding may be limiting and future works can investigate how such grounding can be elegantly integrated during decoding.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to acknowledge Pierre Sermanet, Carolina Parada, Jie Tan, Yevgen Chebotar, Vincent Vanhoucke, and Dorsa Sadigh for their feedback and contributions. This work is supported in part by OpenAI academic access program, granted to Wenlong Huang.</p>
<h2>References</h2>
<p>[1] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning with neural networks," Advances in neural information processing systems, vol. 27, 2014.
[2] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al., "Google's neural machine translation system: Bridging the gap be-</p>
<p>tween human and machine translation," arXiv preprint arXiv:1609.08144, 2016.
[3] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," arXiv preprint arXiv:1904.09751, 2019.
[4] S. Welleck, I. Kulikov, J. Kim, R. Y. Pang, and K. Cho, "Consistency of a recurrent language model with respect to incomplete decoding," arXiv preprint arXiv:2002.02492, 2020.
[5] R. Leblond, J.-B. Alayrac, L. Sifre, M. Pislar, J.-B. Lespiau, I. Antonoglou, K. Simonyan, and O. Vinyals, "Machine translation decoding beyond beam search," arXiv preprint arXiv:2104.05336, 2021.
[6] C. Meister, T. Vieira, and R. Cotterell, "If beam search is the answer, what was the question?," arXiv preprint arXiv:2010.02650, 2020.
[7] J. Kasai, K. Sakaguchi, R. L. Bras, D. Radev, Y. Choi, and N. A. Smith, "Beam decoding with controlled patience," arXiv preprint arXiv:2204.05424, 2022.
[8] C. Meister, T. Pimentel, G. Wiher, and R. Cotterell, "Typical decoding for natural language generation," arXiv preprint arXiv:2202.00666, 2022.
[9] A. Fan, M. Lewis, and Y. Dauphin, "Hierarchical neural story generation," arXiv preprint arXiv:1805.04833, 2018.
[10] S. Basu, G. S. Ramachandran, N. S. Keskar, and L. R. Varshney, "Mirostat: A neural text decoding algorithm that directly controls perplexity," arXiv preprint arXiv:2007.14966, 2020.
[11] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher, "Ctrl: A conditional transformer language model for controllable generation," arXiv preprint arXiv:1909.05858, 2019.
[12] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, and J. Staiano, "Discriminative adversarial search for abstractive summarization," in International Conference on Machine Learning, pp. 8555-8564, PMLR, 2020.
[13] C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine, "Offline rl for natural language generation with implicit language q learning," arXiv preprint arXiv:2206.11871, 2022.
[14] K. Yang and D. Klein, "Fudge: Controlled text generation with future discriminators," arXiv preprint arXiv:2104.05218, 2021.
[15] A. Holtzman, J. Buys, M. Forbes, A. Bosselut, D. Golub, and Y. Choi, "Learning to write with cooperative discriminators," arXiv preprint arXiv:1805.06087, 2018.
[16] J. Li, W. Monroe, and D. Jurafsky, "Learning to decode for future success," arXiv preprint arXiv:1701.06549, 2017.
[17] B. Krause, A. D. Gotmare, B. McCann, N. S. Keskar, S. Joty, R. Socher, and N. F. Rajani, "Gedi: Generative discriminator guided sequence generation," arXiv preprint arXiv:2009.06367, 2020.
[18] M. Ghazvininejad, X. Shi, J. Priyadarshi, and K. Knight, "Hafez: an interactive poetry generation system," in Proceedings of ACL 2017, System Demonstrations, pp. 43-48, 2017.
[19] A. Baheti, A. Ritter, J. Li, and B. Dolan, "Generating more interesting responses in neural conversation models with distributional constraints," arXiv preprint arXiv:1809.01215, 2018.
[20] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu, "Plug and play language models: A simple approach to controlled text generation," arXiv preprint arXiv:1912.02164, 2019.
[21] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar, "Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection," arXiv preprint arXiv:2203.09509, 2022.
[22] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf, "Zero-shot image-to-text generation for visual-semantic arithmetic," arXiv preprint arXiv:2111.14447, 2021.
[23] Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier, "Language models can see: plugging visual controls in text</p>
<p>generation," arXiv preprint arXiv:2205.02655, 2022.
[24] C. Snell, S. Yang, J. Fu, Y. Su, and S. Levine, "Context-aware language modeling for goaloriented dialogue systems," arXiv preprint arXiv:2204.10198, 2022.
[25] S. Verma, J. Fu, M. Yang, and S. Levine, "Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning," arXiv preprint arXiv:2204.08426, 2022.
[26] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al., "Pali: A jointly-scaled multilingual language-image model," arXiv preprint arXiv:2209.06794, 2022.
[27] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, "Visualbert: A simple and performant baseline for vision and language," arXiv preprint arXiv:1908.03557, 2019.
[28] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, "Videobert: A joint model for video and language representation learning," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.
[29] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., "Flamingo: a visual language model for few-shot learning," arXiv preprint arXiv:2204.14198, 2022.
[30] A. Suglia, Q. Gao, J. Thomason, G. Thattai, and G. Sukhatme, "Embodied bert: A transformer model for embodied, languageguided visual task completion," arXiv preprint arXiv:2108.04927, 2021.
[31] A. Pashevich, C. Schmid, and C. Sun, "Episodic transformer for vision-and-language navigation," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.
[32] P. Sharma, A. Torralba, and J. Andreas, "Skill induction and planning with latent language," arXiv preprint arXiv:2110.01517, 2021.
[33] C. Lynch and P. Sermanet, "Grounding language in play," arXiv preprint arXiv:2005.07648, 2020.
[34] S. Nair, E. Mitchell, K. Chen, B. Ichter, S. Savarese, and C. Finn, "Learning languageconditioned robot behavior from offline data and crowd-sourced annotation," in Conference on Robot Learning, pp. 1303-1315, PMLR, 2021.
[35] F. Hill, S. Mokra, N. Wong, and T. Harley, "Human instruction-following with deep reinforcement learning via transfer-learning from text," arXiv preprint arXiv:2005.09382, 2020.
[36] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi, "Merlot: Multimodal neural script knowledge models," Advances in Neural Information Processing Systems, 2021.
[37] M. Reid, Y. Yamada, and S. S. Gu, "Can wikipedia help offline reinforcement learning," arXiv preprint arXiv:2201.12122, 2022.
[38] S. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch, "Pre-trained language models for interactive decision-making," arXiv preprint arXiv:2202.01771, 2022.
[39] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," in International Conference on Machine Learning, PMLR, 2022.
[40] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan, "Do as i can and not as i say: Grounding language in robotic affordances," in arXiv preprint arXiv:2204.01691, 2022.
[41] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al., "Socratic models: Composing zero-shot multimodal reasoning with language," arXiv preprint arXiv:2204.00598, 2022.</p>
<p>[42] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler, "Open-vocabulary queryable scene representations for real world planning," arXiv preprint arXiv:2209.09874, 2022.
[43] D. Shah, B. Osinski, B. Ichter, and S. Levine, "Lm-nav: Robotic navigation with large pretrained models of language, vision, and action," arXiv preprint arXiv:2207.04429, 2022.
[44] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter, "Inner monologue: Embodied reasoning through planning with language models," in arXiv preprint arXiv:2207.05608, 2022.
[45] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," arXiv preprint arXiv:2209.07753, 2022.
[46] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, "Progprompt: Generating situated robot task plans using large language models," arXiv preprint arXiv:2209.11302, 2022.
[47] O. Mees, J. Borja-Diaz, and W. Burgard, "Grounding language with visual affordances over unstructured data," arXiv preprint arXiv:2210.01911, 2022.
[48] C. Huang, O. Mees, A. Zeng, and W. Burgard, "Visual language maps for robot navigation," arXiv preprint arXiv:2210.05714, 2022.
[49] S. S. Raman, V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex, "Planning with large language models via corrective re-prompting," arXiv preprint arXiv:2211.09935, 2022.
[50] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su, "Llm-planner: Fewshot grounded planning for embodied agents with large language models," arXiv preprint arXiv:2212.04088, 2022.
[51] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, "Llm+ p: Empowering large language models with optimal planning
proficiency," arXiv preprint arXiv:2304.11477, 2023.
[52] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, "Text2motion: From natural language instructions to feasible plans," arXiv preprint arXiv:2303.12153, 2023.
[53] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, "Task and motion planning with large language models for object rearrangement," arXiv preprint arXiv:2303.06247, 2023.
[54] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., "Palm-e: An embodied multimodal language model," arXiv preprint arXiv:2303.03378, 2023.
[55] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, "Chatgpt for robotics: Design principles and model abilities," 2023, 2023.
[56] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu, "Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks," arXiv preprint arXiv:2303.16563, 2023.
[57] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, "Translating natural language to planning goals with large-language models," arXiv preprint arXiv:2302.05128, 2023.
[58] Y. Lu, P. Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y. Wang, "Multimodal procedural planning via dual text-image prompting," arXiv preprint arXiv:2305.01795, 2023.
[59] D. Patel, H. Eghbalzadeh, N. Kamra, M. L. Iuzzolino, U. Jain, and R. Desai, "Pretrained language models as visual planners for human assistance," arXiv preprint arXiv:2304.09179, 2023.
[60] L. P. Kaelbling and T. Lozano-Pérez, "Hierarchical planning in the now," in Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.
[61] R. E. Fikes and N. J. Nilsson, "Strips: A new approach to the application of theorem proving to problem solving," Artificial intelligence, 1971.</p>
<p>[62] M. Toussaint, "Logic-geometric programming: An optimization-based approach to combined task and motion planning," in Twenty-Fourth International J̧oint Conference on Artificial Intelligence, 2015.
[63] D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. FeiFei, and S. Savarese, "Neural task programming: Learning to generalize across hierarchical tasks," in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018.
[64] N. Savinov, A. Dosovitskiy, and V. Koltun, "Semi-parametric topological memory for navigation," arXiv preprint arXiv:1803.00653, 2018.
[65] T. Silver, R. Chitnis, N. Kumar, W. McClinton, T. Lozano-Perez, L. P. Kaelbling, and J. Tenenbaum, "Inventing relational state and action abstractions for effective and efficient bilevel planning," arXiv preprint arXiv:2203.09634, 2022.
[66] C. R. Garrett, C. Paxton, T. Lozano-Pérez, L. P. Kaelbling, and D. Fox, "Online replanning in belief space for partially observable task and motion problems," in 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020.
[67] B. Eysenbach, R. R. Salakhutdinov, and S. Levine, "Search on the replay buffer: Bridging planning and reinforcement learning," Advances in Neural Information Processing Systems, 2019.
[68] D.-A. Huang, S. Nair, D. Xu, Y. Zhu, A. Garg, L. Fei-Fei, S. Savarese, and J. C. Niebles, "Neural task graphs: Generalizing to unseen tasks from a single video demonstration," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.
[69] D. Xu, A. Mandlekar, R. Martín-Martín, Y. Zhu, S. Savarese, and L. Fei-Fei, "Deep affordance foresight: Planning through what can be done in the future," in 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 6206-6213, IEEE, 2021.
[70] B. Ichter, P. Sermanet, and C. Lynch, "Broadlyexploring, local-policy trees for long-horizon task planning," Conference on Robot Learning (CoRL), 2021.
[71] C. Agia, T. Migimatsu, J. Wu, and J. Bohg, "Taps: Sequencing task-agnostic policies," arXiv preprint arXiv:2210.12250, 2022.
[72] D. Shah, P. Xu, Y. Lu, T. Xiao, A. Toshev, S. Levine, and B. Ichter, "Value function spaces: Skill-centric state abstractions for long-horizon reasoning," ICLR, 2022.
[73] A. V. Nair, V. Pong, M. Dalal, S. Bahl, S. Lin, and S. Levine, "Visual reinforcement learning with imagined goals," in Advances in Neural Information Processing Systems, 2018.
[74] B. Wu, S. Nair, L. Fei-Fei, and C. Finn, "Exampledriven model-based reinforcement learning for solving long-horizon visuomotor tasks," arXiv preprint arXiv:2109.10312, 2021.
[75] F. Xia, C. Li, R. Martín-Martín, O. Litany, A. Toshev, and S. Savarese, "Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation," in 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.
[76] D. Driess, O. Oguz, J.-S. Ha, and M. Toussaint, "Deep visual heuristics: Learning feasibility of mixed-integer programs for manipulation planning," in 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 9563-9569, IEEE, 2020.
[77] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[78] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," in Proceedings of the 38th International Conference on Machine Learning (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 8748-8763, PMLR, 18-24 Jul 2021.
[79] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, "Openvocabulary object detection via vision and language knowledge distillation," arXiv preprint arXiv:2104.13921, 2021.</p>
<p>[80] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion, "Mdetr-modulated detection for end-to-end multi-modal understanding," in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 17801790, 2021.
[81] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen, et al., "Simple open-vocabulary object detection with vision transformers," arXiv preprint arXiv:2205.06230, 2022.
[82] OpenAI, "Gpt-4 technical report," arXiv, 2023.
[83] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, and J. Lee, "Transporter networks: Rearranging the visual world for robotic manipulation," Conference on Robot Learning (CoRL), 2020.
[84] M. Shridhar, L. Manuelli, and D. Fox, "Cliport: What and where pathways for robotic manipulation," in Conference on Robot Learning, pp. 894-906, PMLR, 2022.
[85] M. Chevalier-Boisvert, L. Willems, and S. Pal, "Minimalistic gridworld environment for openai gym." https://github.com/ maximecb/gym-minigrid, 2018.
[86] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.
[87] A. G. Barto and S. Mahadevan, "Recent advances in hierarchical reinforcement learning," Discrete Event Dynamic Systems, vol. 13, p. 41-77, jan 2003.
[88] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou, "Chain of thought prompting elicits reasoning in large language models," arXiv preprint arXiv:2201.11903, 2022.
[89] L. Van der Maaten and G. Hinton, "Visualizing data using t-sne.," Journal of machine learning research, vol. 9, no. 11, 2008.
[90] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep
bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[91] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al., "Rt-1: Robotics transformer for real-world control at scale," arXiv preprint arXiv:2212.06817, 2022.
[92] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg, et al., "A generalist agent," arXiv preprint arXiv:2205.06175, 2022.
[93] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., "Training language models to follow instructions with human feedback," arXiv preprint arXiv:2203.02155, 2022.
[94] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[95] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, "Bc-z: Zero-shot task generalization with robotic imitation learning," in Conference on Robot Learning, pp. 991-1002, PMLR, 2021.
[96] M. Shridhar, L. Manuelli, and D. Fox, "Perceiveractor: A multi-task transformer for robotic manipulation," in Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.</p>
<h1>A. Appendix</h1>
<h2>A.1. Grounded Decoding Implementation Details</h2>
<p>We study three different implementations of Grounded Decoding for each of the experimental domains. While each instantiation applied Grounded Decoding to long-horizon planning and behavior synthesis, different components including language models and grounded models are used in each domain, as seen in Table 5. Grounded models used in these domains include Affordance Functions (AF), Safety Functions (S), Preference Functions (P), and Open-Vocabulary Object Detectors (D).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: left;">Tabletop Rearrangement (Sim)</th>
<th style="text-align: left;">MiniGrid 2D Maze (Sim)</th>
<th style="text-align: left;">Kitchen Mobile Manipulation (Real)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLM</td>
<td style="text-align: left;">InstructGPT [93]</td>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: left;">InstructGPT + PaLM[94]</td>
</tr>
<tr>
<td style="text-align: center;">Primitives</td>
<td style="text-align: left;">CLIPort [84]</td>
<td style="text-align: left;">PPO [86]</td>
<td style="text-align: left;">RT-1 [91]</td>
</tr>
<tr>
<td style="text-align: center;">Grounded Models</td>
<td style="text-align: left;">AF + S + P</td>
<td style="text-align: left;">AF</td>
<td style="text-align: left;">D</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison between different versions of Grounded Decoding implemented in three different environments.</p>
<h2>A.2. Implementation Details of Simulated Tabletop Rearrangement</h2>
<h2>A.2.1. Tasks</h2>
<p>There are a total of 20 tasks (templates of language instructions), listed in Table 6, grouped into three task category: Letters, Blocks\&amp;Bowls, and Box Packing. Three categories share a total of 57 objects. For Letters category, the goals are to rearrange the alphabetical letter objects such that they satisfy certain orders specified by the language instructions. At the beginning of each episode, task-relevant objects and a set of 1 to 3 randomly-sampled distractor objects (except for the Letters category) are initialized at random positions on the tabletop with fixed orientations. A minimum 15 cm distance is enforced between any two objects to avoid collision and penetration at initialization. To allow for automatic evaluations, a binary reward function is defined for each task using ground-truth state of the objects. Furthermore, we implement scripted policies for each task to collect demonstrations for training the CLIPort baseline. For certain tasks, we also randomize the attributes mentioned in the given instructions, which can be found below:</p>
<ul>
<li>〈word〉: hi, world, left, right, top, down, love, you</li>
<li>〈corner/side〉: left side, top left corner, top side, top right corner, bottom right corner, bottom side, bottom left corner</li>
</ul>
<h2>A.2.2. Low-level Primitives</h2>
<p>We use CLIPort [84] as the low-level primitive that can be invoked by the LLM planner, as it shows promising results of generalization across free-form language instructions. Additionally, since the policy predicts per-pixel affordance, it can be repurposed to serve as grounded models for planning for long-horizon tasks, which we leverage in this work. The single primitive policy is trained on 50,000 pre-collected demonstrations, across 10 training tasks, where each demonstration contains 1) language instruction of the format "pick up [x] and place it on [y]", 2) top-down RGB-D observation of the current scene, 3) expert pick location expressed as pixel coordinates, and 4) expert place location expressed as pixel coordinates. The expert actions are obtained by accessing ground-truth object pose in the simulator. We further apply substring augmentation during training as we find it helps with performance on partial commands: for example, "pick up [x]" is a substring of "pick up [x] and place it on [y]".</p>
<h2>A.2.3. Language Model</h2>
<p>We use InstructGPT [93] (text-davinci-002), accessed through OpenAI API.</p>
<h1>A.2.4. Grounding Functions</h1>
<p>Affordance Grounding Function (AF). The affordance should be a function of the robot, the environment, and the underlying policy in order to accurately determine what is possible in the scene. Given scene image $s$ and partially-decoded open-ended language instruction $\ell$, the primitive policy CLIPort predicts unnormalized logits over the pixel space $s_{\text {pick }}, s_{\text {place }} \in \mathbb{R}^{480 \times 640}$, respectively for the pick location and the place location. Therefore, for any observation $s$ and instruction $\ell$, we calculate the affordance as $p_{\mathrm{AF}}(s, \ell)=\max <em _pick="{pick" _text="\text">{(x, y) \in 480 \times 640}\left(s</em>(x, y)\right)$. The affordance grounding function is used for all tasks.}}(x, y)+s_{\text {place }</p>
<p>Safety Grounding Function (S). World grounding for embodiment may not only be affordances, it can also be crucial robotics functions like safety. While any safety function that can score a language instruction $\ell$ may be used, we implement a simple indicator to prevent the robot from interacting with knives and red boxes, which we use as hazardous objects in this domain: $p_{\mathrm{S}}(s, \ell)=\frac{1}{Z}\left(1-\mathrm{E}[\right.$ red or knife in $\left.\ell]\right)+\frac{\epsilon}{Z} \mathrm{E}[$ red or knife in $\ell]$, where $Z$ is the normalizing term and $\epsilon$ is a small value for ensuring the joint probability does not collapse to 0 . Safety grounding function is used for 3 tasks in Box Packing task family.</p>
<p>Preference Grounding Function (P). Robots operating alongside humans should also be aware of human preferences, which often differ based on the specific user. We choose two random objects $\left(o_{1}, o_{2}\right)$ as the preferred objects to be used for the household object sorting tasks. Note that unlike safety functions, preferences often come in the form of "soft requirement". Therefore, the preference grounding function is implemented as $p_{\mathrm{P}}(s, \ell)=\frac{\epsilon}{Z} \mathrm{E}\left[o_{1}\right.$ or $\left.o_{2} \operatorname{in} \ell\right]+\frac{\epsilon}{Z}\left(1-\mathrm{E}\left[o_{1}\right.\right.$ or $\left.o_{2} \operatorname{in} \ell\right]$ ), where we choose $x=0.5$ and $y=0.1$ for our experiments. More generic preference functions may also be learned or statistically calculated based on history of user data. Preference grounding function is used for 2 tasks in Box Packing task family.</p>
<h2>A.2.5. CLIPort Baseline</h2>
<p>As CLIPort [84] already takes as input a natural language instruction and is capable of directly outputting robot actions, it bears the question whether we need a high-level planner for completing long-horizon tasks. To this end, we additionally train two variants of multi-task CLIPort policy on 10 of the total 20 tasks as baselines (see Table 6 for the train/test split). One variant, which we referred as "CLIPort (Short)", is trained only on single-step pick-and-place instructions of the format "pick up [x] and place it on [y]" on the 10 training tasks. The decomposed pick-and-place instructions are obtained from scripted planners. At evaluation time, the policy is fed in only the high-level instructions without any planners. The other variant, which we referred as "CLIPort (Long)", is trained on the high-level instructions from the 10 training tasks (without decomposition from scripted planners). Similarly, at evaluation time, it is fed in only the high-level instructions and evaluated on both seen and unseen instructions. Both variants are trained on 50,000 demonstrations, similar to the Grounded Decoding primitive. The goal of these baselines is to evaluate whether solitary language-conditioned policies can perform well on long-horizon tasks and generalize to new task instructions. Note that the CLIPort baselines are different from the primitive used in Grounded Decoding, although they share the same architecture.</p>
<h1>A.2.6. Full Experimental Results in Simulated Tabletop Domain</h1>
<p>Below we show the full list of tasks and the full experimental results in the simulated tabletop domain. Each entry is the average success rate across 20 rollouts. The tasks with blue-colored background are seen tasks and the tasks with orange-colored background are the unseen tasks. Seen tasks may be used for training for supervised baselines (CLIPort) or may be used in the prompt for methods using language model planner. Note that for the "Box Packing" task category, although all tasks were seen in training or the prompts, we enforce additional safety and preference constraints for evaluation only at test time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">$p_{0}$</th>
<th style="text-align: center;">CLIPort</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">+LLM</th>
<th style="text-align: center;">+Grounded Decoding</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Short</td>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">Ungrounded</td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">Beam</td>
</tr>
<tr>
<td style="text-align: center;">Letters</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Put the letters in alphabetical order from left to right</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">40\%</td>
</tr>
<tr>
<td style="text-align: center;">Spell as much of word as you can</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">65\%</td>
</tr>
<tr>
<td style="text-align: center;">Separate the vowels from the remaining letters to the bottom side</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">65\%</td>
</tr>
<tr>
<td style="text-align: center;">Put the letters in reverse alphabetical order from left to right</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">25\%</td>
<td style="text-align: center;">25\%</td>
</tr>
<tr>
<td style="text-align: center;">Correctly spell out a sport using the present letters</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">30\%</td>
<td style="text-align: center;">30\%</td>
</tr>
<tr>
<td style="text-align: center;">Sort the geometrically symmetrical letters to the bottom side</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$35 \%$</td>
<td style="text-align: center;">50\%</td>
</tr>
<tr>
<td style="text-align: center;">Separate the consonants from the remaining letters to the bottom side</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">25\%</td>
<td style="text-align: center;">25\%</td>
<td style="text-align: center;">25\%</td>
</tr>
<tr>
<td style="text-align: center;">Sort the letters less than "D" according to ASCII to the bottom side</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$35 \%$</td>
<td style="text-align: center;">$70 \%$</td>
<td style="text-align: center;">75\%</td>
</tr>
<tr>
<td style="text-align: center;">Blocks \&amp; Bowls</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Stack all the blocks</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">90\%</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">75\%</td>
<td style="text-align: center;">90\%</td>
</tr>
<tr>
<td style="text-align: center;">Put all the blocks on the corner/side</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$65 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">70\%</td>
</tr>
<tr>
<td style="text-align: center;">Put all the blocks in the bowls with matching colors</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">70\%</td>
</tr>
<tr>
<td style="text-align: center;">Put the blocks in the bowls with mismatched colors</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">55\%</td>
</tr>
<tr>
<td style="text-align: center;">Put all the blocks in different corners</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$50 \%$</td>
<td style="text-align: center;">60\%</td>
</tr>
<tr>
<td style="text-align: center;">Stack only the blocks of cool colors</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">70\%</td>
<td style="text-align: center;">70\%</td>
</tr>
<tr>
<td style="text-align: center;">Stack only the blocks of warm colors</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">45\%</td>
<td style="text-align: center;">$35 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Sort the primary color blocks to the left side</td>
<td style="text-align: center;">AF</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$25 \%$</td>
<td style="text-align: center;">30\%</td>
</tr>
<tr>
<td style="text-align: center;">Box Packing</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Pack the objects into the brown box</td>
<td style="text-align: center;">$\mathrm{AF}+\mathrm{S}$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">$90 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Pack the objects into the boxes</td>
<td style="text-align: center;">$\mathrm{AF}+\mathrm{S}$</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">75\%</td>
<td style="text-align: center;">$70 \%$</td>
</tr>
<tr>
<td style="text-align: center;">I'd like some snacks on the right side</td>
<td style="text-align: center;">$\mathrm{AF}+\mathrm{P}$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$40 \%$</td>
<td style="text-align: center;">55\%</td>
</tr>
<tr>
<td style="text-align: center;">Pack me a picnic box</td>
<td style="text-align: center;">$\mathrm{AF}+\mathrm{S}+\mathrm{P}$</td>
<td style="text-align: center;">$15 \%$</td>
<td style="text-align: center;">$30 \%$</td>
<td style="text-align: center;">$20 \%$</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">$95 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Full Experimental Results in Simulated Tabletop Rearrangement Tasks. The tasks with blue-colored background are seen tasks and the tasks with orange-colored background are the unseen tasks. *Box Packing tasks are all seen during training, but safety and preference requirements are only enforced during evaluation.</p>
<h1>A.3. Implementation Details of MiniGrid 2D Maze</h1>
<h2>A.3.1. Environment Setup</h2>
<p>We use the open-source gym-minigrid suite of environments to evaluate our method with one simple change - instead of the default observation space which is a $7 \times 7$ egocentric window, our agent has access to entire grid - that allows us to simplify the tasks by removing partial observability [72].</p>
<h2>A.3.2. Tasks</h2>
<p>The tasks are grouped in three categories (please see Table 7 for example instructions):</p>
<ol>
<li>Easy: Simple tasks where the horizon is short (10-30 steps) and fully described by the textual instruction, e.g. OpenDoors and PutNext. The short horizon makes them relatively easy for a wide range of HRL algorithms. The instructions for these tasks generally spell out each individual skill, making them particularly easy for high-level planners based on language modeling.</li>
<li>Medium: Combination of short and long horizon tasks (up to 80 steps) with step-by-step textual instructions, e.g. LockedRoom. While being significantly longer, these tasks also tend to have instructions that spell out the low-level tasks (see Table 7).</li>
<li>Hard: Complex, long horizon instructions (over 100 steps) with short, ambiguous instructions that necessitate multi-step reasoning and efficient exploration, e.g. MultiRoom and BlockedUnlock. In addition to being long-horizon, the instructions in this case tend to be ambiguous and under-specified, e.g. "traverse through the rooms to get to the goal", which does not provide enough context for any blind planning agent.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Difficulty</th>
<th style="text-align: left;">Task Name</th>
<th style="text-align: left;">Example Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Easy</td>
<td style="text-align: left;">OpenDoors <br> PutNext</td>
<td style="text-align: left;">open door blue, then open door red <br> move the red ball next to the green box</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: left;">LockedRoom</td>
<td style="text-align: left;">get the red key from the purple room, open the red door and go to the goal</td>
</tr>
<tr>
<td style="text-align: center;">Hard</td>
<td style="text-align: left;">MultiRoom <br> BlockedUnlock</td>
<td style="text-align: left;">traverse the rooms to get to the goal <br> pick up the blue box</td>
</tr>
</tbody>
</table>
<p>Table 7: Example Instructions in Minigrid</p>
<h2>A.3.3. Language Model</h2>
<p>We use InstructGPT [93] (text-davinci-002), accessed through OpenAI API. The prompts used can be found in Section A.5.</p>
<p>We found the prompts to be generally sufficient for solving the "seen" tasks, as well as "unseen" tasks, i.e. tasks that do not have an example in the context. Empirically, we did not find any improvements by including more then 3 example tasks in the prompt - we hypothesize that this is likely due to the shared low-level primitives across tasks. For all Minigrid experiments presented in this paper, we used the prompt shown in Section A.5.</p>
<h2>A.3.4. Low-level Primitives</h2>
<p>To train low-level primitives, we train an RL agent to solve a wide range of short-horizon subtasks (under 10 steps) that are shared across the various Minigrid tasks - go to <obj>, pick up <obj>, drop $<o b j>$, open <obj>. Rather than training individual skills for each of them [72], we train a single multi-task policy that is conditioned on the CLIP embeddings [78] of the task strings. This scheme allows some robustness to synonyms and ambiguous task specifications, and has been widely used in learning language-grounded policies $[95,96]$.</p>
<p>We train these primitives using PPO [86], as recommended by the environment developers [85]. Each of these skills are trained with a sparse outcome reward ( +1 if a trajectory is successful, 0 otherwise). In addition to these low-level skills, we perform a form of hindsight relabeling where "substrings" of the task strings are masked to allow generalization to partial strings, e.g. "go to red" may be interpreted as "go to red key" or "go to red door", and our masking strategy allows the multi-task policy to execute tasks specified by partially complete strings, if necessary.</p>
<h1>A.3.5. Grounded Affordance Function</h1>
<p>We use the task string-conditioned value function estimates from our learned policy to obtain a visually grounded affordance function.</p>
<h2>A.3.6. Additional Qualitative Results</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Minigrid Domain</p>
<h1>A.4. Implementation Details of Real-World Mobile Manipulation</h1>
<h2>A.4.1. Tasks</h2>
<h2>Instruction</h2>
<p>put an energy bar and water bottle on the table
bring me a lime soda and a bag of chips
Can you throw away the apple and bring me a coke
bring me a 7 up can and a tea
move an multigrain chips to the table and an apple to the far counter
move the lime soda, the sponge, and the water bottle to the table
bring me an apple, a coke, and water bottle</p>
<p>Table 8: List of unambiguous SayCan instructions</p>
<h2>Instruction</h2>
<p>I want to wipe some spill.
Bring me a fruit
Bring me a snack
Bring me a bag of chips
Bring me a bag of snack
Bring me a bag of chips and something to wipe a spill
Bring me a bag of chips and something to drink
Bring me a bag of chips and a soda
Human: I want a soda that is not coke, and a fruit.
I want a fruit and a soda</p>
<p>Table 9: List of ambiguous SayCan instructions</p>
<h2>A.4.2. Language Model</h2>
<p>For planning, we use PaLM [94], a 540B parameter language model trained on a large datasets that include high-quality web documents, books, Wikipedia, conversations, and GitHub code. Before planning, we use InstructGPT [93] (text-davinci-002), accessed through OpenAI API. to generate the (grounded) chain of thought.</p>
<p>We used square bracket to indicate grounded decoding, as illustrated in Fig. 5. The prompts are shown in Listing 3.</p>
<h2>A.4.3. Low-level Primitives</h2>
<p>We use a combination of learned and scripted control policies for navigation and manipulation, following the implementation described in SayCan [40] and RT-1 [91]. The manipulation policies for the picking action are learned using Behavior Cloning (BC) on 68000 demonstrations and 12000 autonomous successes that were collected over the course of 11 months using a fleet of 10 robots. The demonstrations are collected by teleoperators using VR headset controllers to track the motion of their hand, which is then mapped onto the robot's end-effector pose. The navigation policies are scripted, based on a ground-truth map as well as a learned perception module for collision avoidance and planning. The placing actions follow pre-computed motions only when preceded by a navigation policy. The Value Functions used by SayCan for affordance</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Box Packing tasks are seen during training, but safety and preference requirements are only enforced during evaluation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Emerging multimodal language models [54, 82] provide strong baselines, but they similarly cannot serve generalpurpose grounding functions because they are not conditioned on embodiment, except for cases where embodiment data from each individual domain can be used to finetune the LLM [54].&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
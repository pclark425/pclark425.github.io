<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1684 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1684</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1684</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-159041677</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1905.07512v2.pdf" target="_blank">SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation</a></p>
                <p><strong>Paper Abstract:</strong> We propose SplitNet, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show dramatic improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, SplitNet generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, SplitNet can match the performance of traditional end-to-end pipelines which receive the entire dataset. Code and video are available at https://github.com/facebookresearch/splitnet and https://youtu.be/TJkZcsD2vrc</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1684",
    "paper_id": "paper-159041677",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0036825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation
21 May 2019</p>
<p>Daniel Gordon 
Paul G. Allen School of Computer Science
University of Washington</p>
<p>Abhishek Kadian 
Facebook AI Research</p>
<p>Devi Parikh 
Facebook AI Research</p>
<p>Georgia Institute of Technology</p>
<p>Judy Hoffman 
Facebook AI Research</p>
<p>Georgia Institute of Technology</p>
<p>Dhruv Batra 
Facebook AI Research</p>
<p>Georgia Institute of Technology</p>
<p>Facebook AI Research</p>
<p>SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation
21 May 20195D7932A08588833B7FB20560F089DAE2arXiv:1905.07512v2[cs.CV]
We propose SplitNet, a method for decoupling visual perception and policy learning.By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception.We show dramatic improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real.Additionally, SplitNet generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks.Further, given only a small sample from a target domain, SplitNet can match the performance of traditional end-to-end pipelines which receive the entire dataset 1 2 .</p>
<p>Introduction</p>
<p>A longstanding goal of computer vision is to enable robots to understand their surroundings, navigate efficiently and safely, and perform a large variety of tasks in complex environments.A practical application of the recent successes in Deep Reinforcement Learining is to train robots with minimal supervision to perform these tasks.Yet poorly-trained agents can easily injure themselves, the environment, or others.These concerns, as well as the difficulty in parallelizing and reproducing experiments at a low cost, have drawn research interest towards simulation environments [3,15,35,36,20].</p>
<p>However no simulator perfectly replicates reality, and agents trained in simulation often fail to generalize to the real-world.Transferring learned policies from simulation to the real-world (Sim2Real) has become an area of broad interest [25,27,33] yet there still exists a sizable performance gap for most algorithms.Furthermore, Sim2Real transfer reintroduces safety and reproducibility concerns.To mitigate this, we explore the related task of Sim2Sim, transfer- ring policies between simulators, for embodied visual navigation (Figure 1).Transferring between simulators incurs a similar "reality gap" as between simulation and reality, due to differences in data collection and rendering.Learning to transfer between simulation environments serves as an encouraging preliminary step towards true Sim2Real transfer.</p>
<p>To enable Sim2Sim transfer we propose SplitNet, a composable model for embodied visual tasks which allows for the sharing and reuse of information between different visual environments.SplitNet enables transfer across different embodied tasks (Task2Task), meaning our model can learn new skills quickly and adapt to the ever-changing requirements of end users.Our key insight is to observe that embodied visual tasks are naturally decomposable into visual representation learning to extract task agnostic salient information from the visual input, and policy learning to interpret the visual representation and determine a proper action for the agent.Rather than learning these components solely independently or completely tied, we introduce an algorithm for learning these embodied visual tasks which benefits both from the scalability and strong in-domain, ontask performance of an end-to-end system and from the gen-eralization and fast adaptability of modular systems.</p>
<p>SplitNet incorporates auxiliary visual tasks, such as depth prediction, as a source of intermediate supervision which guides the visual representation learning to extract information from the images extending beyond the initial embodied task.We demonstrate that initial pre-training of the visual representation on such auxiliary visual tasks produces a more robust initialization than the standard approach of pre-training on auxiliary visual datasets (e.g.Im-ageNet [5]) which may not be from an embodied perspective.Then we showcase the composability of our model by illustrating its ability to selectively adapt only the visual representation (when moving to a new visual environment) or only its policy (when moving to a new embodied task).</p>
<p>We center our evaluation on adapting between different simulators of varying fidelity and between different embodied tasks.Specifically, our experiments show that compared to end-to-end methods, SplitNet learns more transferable visual features for the task of visual point-to-point navigation, reduces overfitting to small samples from a new target simulator, and adapts faster and better to novel embodied tasks.</p>
<p>Related Work</p>
<p>This work introduces a learning approach for transferring visual representations between environments and for transferring policy information between different embodied tasks.The most related lines of work focus on adaptation and transfer of visual representations, deep reinforcement learning (especially from visual inputs), and transferring from simulation to the real-world (Sim2Real) both for visual and embodied tasks.</p>
<p>Visual Transfer and Adaptation.Many works have explicitly studied techniques for increasing the reusability of learned information across different visual tasks.Domain adaptation research has mainly focused on reusing a representation even as the input distribution changes, with most work focusing on representation alignment through explicit statistics [19,31] or through implicit discrepancy minimization with a domain adversarial loss [6,34].A related line of work focuses on sharing between two image collections through direct image-to-image transfer [26,40], whereby a mapping function is learned to take an image from one domain and translate it to mimic an image from the second domain [2,10,18,32].</p>
<p>In parallel, many works focus on reusing learned representations for solving related visual tasks.The most prevalent such technique is simply using the first representation parameters as initialization for learning the second, termed finetuning [7].A recent study proposed a technique for computing the similarity between a suite of visual tasks to create a Taskonomy [37] which may be used to determine, given a new task, which prior tasks should be used for the initialization before continued learning.This method focuses on "passive" visual understanding tasks such as recognition, reconstruction and depth estimation and does not delve into learning representations for "active" tasks such as embodied navigation where an agent must both understand the world and directly use its understanding for some underlying task.</p>
<p>Overall, much of the prior work has focused on representation learning for visual recognition.In contrast, this work studies transfer of visuomotor policies for embodied tasks and decomposes the problem into transfer of visual representations for embodied imagery (Sim2Sim) and transfer of policies across various downstream embodied tasks (Task2Task).</p>
<p>Visual RL Tasks: In parallel with the development of deep representation learning for passive visual tasks, there has also been a plethora of recent research on policy learning from visual inputs inspired by the success of end-toend visuomotor policy learning [16,17,22].Much of the success here comes from training on large-scale [17] data, frequently made possible by extensive use of simulation environments [9,22,24,42].These techniques often leverage the additional supervision and auxiliary tasks given by the simulators to bootstrap their learning [21,28].Perceptual Actor [28] specifically examines how 20 different pretraining tasks affect the learning speed and accuracy of a visual navigation policy as compared to random initialization.Others use unsupervised [14] or self-supervised [24] learning as an additional signal in domains with sparse rewards.We build on these approaches by explicitly separating the auxiliary learning from the policy layers to ensure a decoupling of the weights which enables better transfer to new environments.</p>
<p>For increased task generalization, others have proposed using the successor representation [38,41] which decomposes the reward and Q-functions into a state-action feature φ s,a , a successor feature ψ s,a and a task reward vector w.This decomposes the network into one which learns the dynamics of the environment separate from the specified task, which allows for faster transfer to new tasks by only retraining the task embedding w.Our proposed method allows quick transfer to new tasks as well as new environments.</p>
<p>Sim2Real: Significant progress has been made on adapting between simulated and real imagery for visual recognition, especially in the context of semantic segmentation in driving scenes [10,11,12,39].These techniques build on the visual domain adaptation methods described above.In parallel, there has been work on transferring visual policies learned in simulation to the real-world, but often limited to simple visual domains [27,33] which bear little resemblance to the complexity of true real-world scenes.Rusu et al. [27] train a network in simulation before initializing a new network which receives outputs from the simulationtrained network as well as real-world inputs.Yet their evaluation is limited to simple block picking experiments with no complex visual scenes.Peng et al. [25] use randomization over the robot dynamics to learn robust policies, but do not use visual inputs in simulation or reality and only perform simple puck-pushing tasks.Tobin et al. [33] randomize colors, lighting, and camera pose as a form of augmentation of the simulated imagery to better generalize to real-world imagery, but focus on primitive geometric objects for picking tasks.</p>
<p>A recent method [23], uses semantic segmentation as an intermediate objective to aid in transferring learned driving policies from simulation to the real-world.While we do not transfer our policies to real robots, we focus on visually diverse scenes which better match the complexity of the realworld then the simplistic setups of many of the prior policy transfer approaches.Similar to Müller et al. [23] we use auxiliary intermediate objectives to aid in transfer, but in our case focus on a set of auxiliary visual and motion tasks which generalize to many downstream embodied tasks and propose techniques to selectively transfer either across visual environments or across embodied tasks.</p>
<p>SplitNet: Decoupled Perception and Policy</p>
<p>Solving complex visual planning problems frequently requires different types of abstract understanding and reasoning based on the visual inputs.In order to learn compact representations and generalizable policies, it is often necessary to go beyond the end-to-end training paradigm.This is especially true when the initial learning setting (source domain) and current learning setting (target domain) have sufficiently different visual properties (e.g.differing visual fidelity as seen in Figure 1 left) or different objectives (e.g.transfer from one task to another as in Figure 1 right).In this section we outline the learning tasks we use, and our strategy for training a network which transfers to new visual domains and new embodied tasks.</p>
<p>Embodied Tasks</p>
<p>In this work, we focus on the following three visual navigation tasks which require memory, planning, and geometric understanding: Point-to-Point Navigation (Point-Nav), Scene Exploration (Exploration), and Run Away from Location (Flee).In our experiments, all tasks share a discrete action space: Move Forward by 0.25 meters and Rotate Left/Right by 10 degrees.</p>
<p>Point-to-Point Navigation (Point-Nav) An agent is directed to go to a point via a constantly updating tuple of (angle to goal, distance to goal).The agent succeeds if it ends the episode within a fixed radius of the goal.In our experiments we use a success radius of 0.2 meters and the agent is spawned anywhere from 1 to 30 meters from the Given source visual inputs, the visual encoder is trained using auxiliary visual and motion based tasks.Next, the policy decoder is trained on the source embodied tasks with a fixed visual encoder.</p>
<p>Gradients from the embodied task (depicted as blue arrows) are stopped before the shared visual encoder to ensure decoupling of the policy and perception.</p>
<p>goal.The agent is provided with a one-hot encoding of its previous action.Since the agent is given the distance to the goal, learning the Stop action is trivial, so we disregard it.Scene Exploration (Exploration) We discretize the world-space into 1 meter cubes and count the number of distinct cubes visited by the agent during a fixed duration.This task differs from Point-to-Point Navigation in that no absolute or relative spatial locations are provided to the agent.This prohibits agents from learning to detect collisions by comparing location values from two timesteps, requiring them to visually detect collisions.The agent still receives a one-hot encoding of its previous action.</p>
<p>Run Away from Location (Flee) The goal of this task is to maximize the geodesic distance from the start location and the agent's final location in episodes of fixed length.As in Exploration, no spatial locations are given to the agent.</p>
<p>Decomposing the Learning Problem</p>
<p>For visual navigation tasks, an agent must understand what it sees and it must use the perceived world to decide what to do.Thus, we decompose visual navigation into the subtasks of (1) encoding the visual information and (2) using the encoded information to navigate.At each time t the agent receives an egocentric image I t from the environment and must return a navigation action a t in order to accomplish the task.Instead of learning actions directly from pixels, we break the decision-making into two stages.First, a function F processes the image I t producing a feature embedding φ t = F(I t ).Next, the features are decoded into an action a t = G(φ t ).Our goal is to learn features φ t which extract salient information for completing navigation tasks and which generalize to new environments.Rather than passively expecting the end-to-end training to result in transferable features, we directly optimize portions of the network with distinct objectives to produce representations which are highly semantically meaningful and transferable.</p>
<p>Visual Encoder</p>
<p>Visual understanding comes in many forms and is highly dependent on the desired end task.In the case of visual navigation, the agent must convert pixel inputs into an implicit or explicit geometric understanding of the environment's layout.To encapsulate these ideas, we train a bottleneck encoder-decoder network supervised by several auxiliary visual and motion tasks.Each task uses a shared encoder, and produces a general purpose feature, φ t .This feature is then used as input to learn a set of task specific decoders.</p>
<p>Auxiliary Visual Tasks: We encourage the shared encoder to extract geometric information from the raw visual input by augmenting the learning objective with the following auxiliary visual tasks: (1) prediction of depth through a depth decoder, D, (2) prediction of surface normals through a surface normal decoder, S, and (3) RGB reconstruction through a reconstruction decoder, R (sample outputs are shown in the supplementary material).For an input image I t with ground truth depth, D t and ground truth surface normals S t , the learning objective for each of these auxiliary visual decoders is as follows:
L D = pixels D(φ t ) − D t 1(1)L S = 1 − pixels S(φ t ) • S t S(φ t ) 2 * S t 2
(2)
L R = pixels R(φ t ) − I t 1(3)
We use the 1 loss for reconstruction and depth to encourage edge sharpness.We use the cosine loss for the surface normals as it is a more natural fit for an angular output.</p>
<p>Auxiliary Motion Tasks: We additionally encourage the visual encoder to extract information which may be generically useful for future embodied tasks by adding the following auxiliary motion tasks: (1) predict the egomotion (discrete action) of the agent with motion decoder E, and</p>
<p>(2) forecast the next features given the current features and a one-hot encoding of the action performed with motion decoder P. For a visual encoding φ t at time t, previous encoding φ t−1 , and action a t that causes the agent to move from I t−1 to I t , the learning objective for each of these auxiliary motion decoders is as follows:
L E = − a∈A p(a t = a) log(E(φ t , φ t−1 ))(4)L P = 1 − f eatures P(φ t−1 , a t ) • φ t P(φ t−1 , a t ) 2 * φ t 2(5)
We use the cross-entropy loss as we use a discrete action space, and we use the cosine loss for next feature prediction as it directly normalizes for scale which stops the network from forcing all the features arbitrarily close to 0.</p>
<p>All objectives affecting the learning of the visual encoder can be summarized in the joint loss:
L = λ R L R + λ D L D + λ S L S + λ E L E + λ P L P
where λ R , λ D , λ S , λ E , λ P are scalar hyperparameters which control the trade-off between the various tasks in this multi-task learning objective.</p>
<p>Rather than expecting our network to learn to extract geometric information decoupled from the policy decoders, we force the visual representation to contain this information directly.This decreases the likelihood of overfitting to training environments and thus increases the likelihood that our model generalizes to unseen environments.</p>
<p>Policy Decoder</p>
<p>Our policy decoder takes as input the visual features φ t and learns to predict a desired action, a t+1 , supervised by a reward signal provided by the desired task.To avoid purely reactive policies, we employ a GRU [4] to add temporal context.The output of the policy layers predicts a probability distribution over the discretized action space and a value estimate for the current state.The probability distribution is sampled to determine which action to perform next.</p>
<p>When training the policy decoder, we fix our visual encoder and optimize only the policy decoder weights for the chosen task i.e. gradients do not propagate from the source task to the visual layers (see Figure 2 for an illustration of the gradient flow from the embodied task loss).This prevents policy information from leaking into the visual representation, ensuring the visual encoder generalizes well for many tasks.For the task of Point-to-Point Navigation we use two training strategies: BC and BC, PPO.BC: We train the agent using behavioral cloning (BC) where the ground truth represents the action which would maximally decrease the geodesic distance between the current position and the goal.This is trained in a "studentforcing" regime i.e. the agent executes actions based on its policy, but evaluates the actions using the ground truth.</p>
<p>BC, PPO:</p>
<p>We initialize the agent with the weights from the BC setting and update only the policy layers using the PPO algorithm [29] with a shaped reward based on the geodesic distance to the goal, Geo(P, G):
r pointnav t = Geo(P t−1 , G) − Geo(P t , G) + λ T(6)
where P t is the agent's location at time t, G is the goal location, and λ T is a small constant time penalty.All decoder weights are frozen (to prevent overfitting), but gradients propagate through all decoder layers to the encoder.</p>
<p>Shared Visual Encoder</p>
<p>Selective Transfer to New Domains and Tasks</p>
<p>Adapting to new Visual Domains</p>
<p>By decomposing the learning task into a perceptual encoder and a policy decoder, each supervised by their own objectives, our model is able to learn more transferable visual features than end-to-end methods.Furthermore, our model can quickly adapt its perceptual understanding with auxiliary visual and motion based training in the target environment without needing to modify the policy.Figure 3 illustrates the visual encoder adaptation learning procedure.Given a small sample of data and tasks in the target domain, we backpropagate gradients through the policy decoder 3 and the auxiliary task layers but freeze the weights for all but the shared visual encoder.By doing so, our model can quickly adapt its perception without overfitting the policy to the small sample.</p>
<p>Adapting to new Tasks</p>
<p>When transferring to a new embodied task operating in the same visual space, our model only needs to update the policy decoder parameters (see Figure 4).While reusing lower-level features for new tasks by replacing and retraining the final layers is a common technique in deep learning [7,13,8] our model naturally decouples perception and reasoning offering a clear solution as to which layers to freeze or finetune.Rich perceptual features often transfer to tasks which require different reasoning assuming that the representation encodes the necessary information for the new task.By using auxiliary tasks to inform the updates to the visual encoder, we aim to encourage learning of intermediate features that capture semantically meaningful in-  formation which should better transfer to new tasks than arbitrary latent features.For example, the latent features from a purely end-to-end learned model may represent a variety of different (sometimes spurious) correlations, while our features must contain enough information to reconstruct depth and surface normals etc., so the network should be able to, for instance, avoid obstacles using the exact same features.While this implies that the selection of an appropriate auxiliary task affects the success of our method, if necessary our network can still be trained end-to-end using the pretrained weights as initialization.</p>
<p>In the specific cases of transferring from Point-Nav to Exploration or Flee we initialize the model with the weights from the BC, PPO setting and update the policy decoder layers using PPO with the new reward functions:
r explore t = V isited t − V isited t−1 + λ T (7)
r f lee t = Geo(P t , P t0 ) − Geo(P t−1 , P t0 ) + λ T (8) where V isited t represents how many unique spatial locations the agent has visited at time t.</p>
<p>Experiments</p>
<p>To evaluate visual navigation tasks we use the Habitat scene renderer [20] on scenes from the SUNCG [30], Matterport 3D (MP3D) [3] and Gibson [35] datasets.</p>
<p>Baselines</p>
<p>We compare our results against traditional end-to-end (E2E) training algorithm results for all experiments.These can be trained via the PPO algorithm, via behavioral cloning (BC), or pretrained with BC and finetuned with PPO.One common technique across deep learning is to pretrain models on ImageNet [5], and finetune the entire network on the desired task, which we also include as a baseline.We do not freeze any weights when training E2E methods.We additionally include blind (but learned) agents for each task and random-action agents to benchmark task difficulty.For Point-Nav, we also include a Blind Goal Follower which aligns itself in the direction of goal vector and moves forward, realigning after it collides with obstacles.</p>
<p>Generalization to Unseen Environments</p>
<p>The ability for an algorithm to generalize to unseen environments represents its effectiveness in real-world scenarios.To begin analyzing our model, we experiment with the standard protocol of training and evaluating on data from the same simulator, partitioning the scenes into train and test.We compare performance for the Point-Nav task on three simulators (SUNCG [30], MP3D [3], Gibson [35]) evaluating in never-before-seen scenes.We use the SPL metric proposed in [1] which can be stated as
SP L = 1 N N i=1 S i i max(p i , i )(9)
where S i is a success indicator for episode i, p i is the path length, and i is the shortest path length.This combines the accuracy (success) of a navigation method with its efficiency (path length) where 1.0 would be an oracle agent.</p>
<p>Effective policies generalize by understanding the geometry of the scenes rather than trying to localize into a known map based on the visual inputs.SplitNet outperforms all other methods by a wide margin on all three environments SUNCG [30] MP3D (shown in Table 1).Surprisingly, pretraining on ImageNet does not offer better generalization, likely because the features required for ImageNet are sufficiently different from those needed to navigate effectively (note, the convolutional weights trained on ImageNet are not frozen during BC and PPO training).This is true even compared to E2E without pretraining on ImageNet.</p>
<p>As qualitative intuition about the performance of the various methods, we depict the policies for a subset of methods on an example MP3D episode from in Figure 5.For a fixed start (blue diamond) and goal (green star) location, we show the output trajectory from each method where the trajectory color (ranging from blue to red) denotes the number of steps so far.If a policy failed to reach the goal, the final destina-   We compare our method, SplitNet, to end-to-end (E2E) and blind learned baselines and report SPL performance as a function of starting geodesic distance from the goal.SplitNet outperforms on all starting distances, especially on the more difficult episodes.</p>
<p>tion is denoted with a red "x".From this visualization we can see that SplitNet using BC and PPO successfully completes the task and does so with the shortest overall path.At the beginning of the episode SplitNet BC is stuck behind the wall, but eventually is able to navigate away from the wall and reach the target.</p>
<p>We further analyze the performance of SplitNet compared to baselines as a function of the geodesic distance between the starting and goal locations in Figure 6.This distance is highly correlated with the difficulty of an episode.Unsurprisingly, all methods degrade as the starting location is moved further from the goal location, but SplitNet retains its advantage over baselines irrespective of the episode difficulty.Additionally, we see the performance gap widen over the more difficult episodes, meaning we handle difficult episodes better than the baselines.</p>
<p>Transfer Across Simulators</p>
<p>We now study the ability for our method to transfer between visual environments for the fixed task of Point-Nav.We denote Source to be the initial simulator in which we train our model using both BC and PPO and denote this initial model as "Source SplitNet BC, PPO."The baseline source model that uses end-to-end training is denoted as "Source E2E BC, PPO."We then compare our method for transfer to the new simulator Target, described in Section 3.5.1 and denoted as "Target SplitNet Transfer," against the end-to-end baseline finetuned on the target, "Target E2E Finetune."For reference, we also present the performance of training an end-to-end model using only the available target data, denoted as "Target E2E BC."</p>
<p>Table 2 reports our main results for this Sim2Sim transfer problem as a function of the amount of available target scenes during training.We report performance for the two transfer settings of SUNCG→MP3D and MP3D→Gibson.These simulators differ in terms of complexities of differing rendering appearance (as seen in Figure 1), different environment construction methods (synthetic vs. depthscan reconstruction), and different environment size.Again, SplitNet outperforms all baselines across all experiments in terms of the SPL metric and performs better or comparable to the baseline in terms of success for all transfer setups.Even with no extra data, our initially learned network is more generalizable to new environments, especially those which are significantly different in appearance (SUNCG→MP3D).Of note, in both cases, SplitNet given 10 scenes from the target dataset matches or outperforms the end-to-end baseline SPL given the entire target dataset.</p>
<p>Note, that our approach to visual environement transfer includes finetuning only the visual encoder in the target environment and leaving the policy decoder fixed.One may wonder whether this is the optimal approach or whether our method would benefit from target updates to the policy decoder as well.To answer this question, in Table 3 we report performance comparing the initial source SplitNet performance to that of finetuning either only the visual encoder ("V") which is our proposed approach or finetuning both the visual encoder and policy decoder ("V+P").Interestingly, we found that allowing updates to both the visual encoder and policy decoder in the target environment lead to significant overfitting which resulted in failed generalization to the unseen scenes from the validation sets.This confirms the benefit of our split training approach.</p>
<p>Transfer Across Tasks</p>
<p>We test the ability for SplitNet to learn a new task by first training the network on Point-Nav and using our approach to transfer the model to the novel tasks of Exploration and Flee.All three tasks require the ability to transform 2D visual inputs into 3D scene geometry, but the decisions about what to do based on the perceived geometry are drastically different.Since SplitNet decouples the policy from the perception, it learns features which readily transfer to a novel, but related task.</p>
<p>Figure 7 shows that SplitNet immediately begins to learn effective new policies, outperforming the other methods almost right away.In Exploration, our method is able to reuse its understanding of depth to quickly learn to approach walls, then turn at the last second and head off in a new direction.For the Flee task, our method identifies long empty hallways and navigates down those away from the start location.None of the other methods learn robust obstacle-avoidance behavior or geometric scene understanding, which can be seen in our supplemental video.Instead they latch on to simple dataset biases such as "repeatedly move forward then rotate."</p>
<p>Analysis of Auxiliary Objectives</p>
<p>Our method was designed as a solution for generalization on a downstream embodied task.However, SplitNet also learns outputs for the auxiliary visual and motion tasks.While our goal is not to surpass state-of-the-art performance on these auxiliary tasks it is still useful to verify that the visual encodings match our expectations.We therefore created videos which show both intermediate outputs, actions taken, and an overall predicted trajectory for SplitNet and baseline methods.As this result is best viewed in the video format, we refer the interested reader to the supplementary material to view this result.We show excerpts where the encoder and decoders from SplitNet are able to perform both the visual and motion based auxiliary tasks leading us to conclude that the expected geometric information is present within the representation from the visual encoder.</p>
<p>Conclusion</p>
<p>We introduce SplitNet, a method for decomposing embodied learning tasks to enable fast and accurate transfer to new environments and new tasks.By disentangling the visual encoding of the state from the policy for a task, we learn more robust features which can be frozen or adapted based on the changed domain.Our model matches the performance of end-to-end methods even with six times less data.We believe SplitNet may prove to be a useful stepping stone in transferring networks from simulation environments onto robots in the real-world.</p>
<p>Figure 1 .
1
Figure 1.We decompose learning of visual navigation tasks into learning of a visual encoder and learning of an embodied task decoder.Through this decomposition we enable fast transfer to new visual environments and transfer to new embodied tasks.</p>
<p>Figure 2 .
2
Figure 2. SplitNet initial learning on source data and task.Given source visual inputs, the visual encoder is trained using auxiliary visual and motion based tasks.Next, the policy decoder is trained on the source embodied tasks with a fixed visual encoder.Gradients from the embodied task (depicted as blue arrows) are stopped before the shared visual encoder to ensure decoupling of the policy and perception.</p>
<p>Figure 3 .
3
Figure 3. SplitNet visual domain transfer.When the target visual inputs differ from the source visual inputs while the desired embodied task remains fixed, our model updates the shared visual encoder using only auxiliary visual and motion based learning tasks.All decoder weights are frozen (to prevent overfitting), but gradients propagate through all decoder layers to the encoder.</p>
<p>Figure 4 .
4
Figure 4. SplitNet task transfer.When learning a new embodied task for the same visual inputs as in the source initial learning, our model fixes the shared visual encoder and updates the policy decoder using the new target embodied loss.</p>
<p>Figure 5 .
5
Figure 5. Qualitative comparison of Point-Nav policies on MP3D validation.An exemplar validation episode (fixed start and end location) and the predicted trajectories from baselines and SplitNet.</p>
<p>Figure 6 .
6
Figure 6.MP3D Point-Nav Performance vs episode difficulty.We compare our method, SplitNet, to end-to-end (E2E) and blind learned baselines and report SPL performance as a function of starting geodesic distance from the goal.SplitNet outperforms on all starting distances, especially on the more difficult episodes.</p>
<p>Visual Auxiliary Tasks Motion Auxiliary Tasks
Task DecodersTarget InputShared Visual EncoderFCFCFCEgomotion PredictionNext Feature PredictionLearn Parameter UpdatesGRUFCAct. Cri.Target Embodied Task Point-to-Point Navigation Scene ExplorationFreeze</p>
<p>Table 1 .
1
Performance on Unseen Environments.Blind methods are not provided with visual input but still receive an updated goal vector."BC, PPO" methods are first trained with a softmax loss to take the best next action and are finetuned with the PPO algorithm.
[3]Gibson [35]SPL Success SPL Success SPL SuccessRandom0.0120.0270.0110.0160.0460.028Blind Goal Follower0.1990.2030.1550.1580.3250.319Blind BC0.1590.3230.2320.3820.3510.603Blind BC, PPO0.2910.3770.3170.4710.4270.643Blind PPO0.2580.3710.3130.4630.5380.822E2E PPO0.3240.5290.3220.4770.6340.831E2E BC0.3430.5480.4590.7370.5090.824E2E BC, PPO0.3930.5930.5210.7330.6060.869ImageNet Pretrain, E2E BC0.2800.4990.3150.5520.5480.843ImageNet Pretrain, E2E BC, PPO 0.3380.4400.4500.5390.6420.737SplitNet BC0.4210.6870.5170.8080.5840.865SplitNet BC, PPO0.5600.7030.7160.8440.7010.855</p>
<p>Table 2 .
2
Performance transferring across simulation environments (Sim2Sim).Our method, SplitNet, significantly outperforms the end-to-end (E2E) baseline at the task of transferring across simulated environments.For reference, we also report the performance of a source only trained model (top two rows) or a target only trained model (bottom two rows)."Failure" indicates that performance on the target data decreases after finetuning.
Number Train ScenesTest DataNumber Train ScenesTest DataSUNCG MP3D (Train)MP3D (Val)MP3DGibson (Train)Gibson (Val)(Source)(Target)SPLSuccess (Source)(Target)SPLSuccessSource E2E BC, PPO99000.2570.4126100.6090.866Source SplitNet BC, PPO99000.3760.5396100.6510.764Target E2E BC010.2110.321010.3960.589Target E2E Finetune9901Failure Failure611Failure FailureTarget SplitNet Transfer99010.4470.5966110.6860.822Target E2E BC0100.2590.4630100.5010.782Target E2E Finetune990100.4010.61261100.6670.870Target SplitNet Transfer990100.5310.68161100.7270.854Target E2E BC, PPO0All (61)0.5210.7330All (72)0.6060.869Target SplitNet BC, PPO0All (61)0.7160.8440All (72)0.7010.855Mean SPL vs. Distance10.8SPL0.4 0.60.202468 10 12 14 16 18 20 22 24 26 28 30Starting Geodesic DistanceSplitNet BC, PPOE2E BC, PPOBlind BC, PPO</p>
<p>Table 3 .
3
Ablation of SplitNet Sim2Sim transfer strategy.Split-Net only updates the visual encoder ("V") and fixes the policy decoder ("P") when finetuning the source SplitNet model on a target visual environment.In contrast, finetuning both V+P on the target leads to degraded performance.
SplitNet ModelLayers Finetuned Target Scenes NumberSPLSuccessTransfer SUNCG → MP3D (train): Eval MP3D (val)Source Only--0.3760.539Finetune TargetV+P10.4350.586Finetune TargetV10.4470.596Finetune TargetV+P100.4000.552Finetune TargetV100.5310.681Transfer MP3D → Gibson (train): Eval Gibson (val)Source Only--0.6510.764Finetune TargetV+P1Failure FailureFinetune TargetV10.6860.822Finetune TargetV+P10Failure FailureFinetune TargetV100.7270.854</p>
<p>SUNCG Task2Task performance as a function of target training episodes.SplitNet Transfer and E2E Transfer are first trained on SUNCG Point-Nav, but SplitNet only updates the policy layers whereas E2E updates the entire network.E2E from scratch is randomly initialized a episode 0. The Blind method only receives its previous action as input and is randomly initialized.
SUNCG Exploration over TimeNumber of Explored Cells20 251502,0004,0006,0008,00010,00012,000Number of Training EpisodesSplitNet TransferBlindE2E From ScratchE2E FinetuneSUNCG Flee Performance over TimeGeodesic Distance from Start Location0 1 2 3 4 5 6 7 802,0004,000 Number of Training Episodes 6,000 8,00010,00012,000SplitNet TransferBlindE2E From ScratchE2E FinetuneFigure 7.
Without propagating gradients through the policy decoder, the encoder feature representation shifts and no longer matches the policy decoder.
Appendix B. Dataset DetailsWe constructed the Point-Nav datasets for each of SUNCG, MP3D, and Gibson environments using a sampling-based method which filtered out easy episodes (those with euclidean distance geodesic distance &lt; 1.1).We additionally filter out episodes where there is no path between the start and goal location.The start points from these episodes were additionally used for the Exploration and Flee tasks, but the goal locations were ignored.Per-environment statistics are listen in TableA1.Appendix C. Network Architecture
On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.067572018arXiv preprint</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, Computer Vision and Pattern Recogntion (CVPR). 20117</p>
<p>Matterport3D: Learning from RGB-D data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, International Conference on 3D Vision (3DV). 2017. 1, 5, 6</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>ImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Computer Vision and Pattern Recogntion (CVPR). 20095</p>
<p>Unsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). D Blei, F Bach, the 32nd International Conference on Machine Learning (ICML-15)2015JMLR Workshop and Conference Proceedings</p>
<p>Rich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, Computer Vision and Pattern Recogntion (CVPR). 20145</p>
<p>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Computer Vision and Pattern Recogntion (CVPR). 2017</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Computer Vision and Pattern Recogntion (CVPR). 2017</p>
<p>Cycada: Cycle-consistent adversarial domain adaptation. J Hoffman, E Tzeng, T Park, J.-Y Zhu, P Isola, K Saenko, A A Efros, T Darrell, International Conference in Machine Learning (ICML). 2018</p>
<p>Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. J Hoffman, D Wang, F Yu, T Darrell, CoRR, abs/1612.026492016</p>
<p>Domain transfer through deep activation matching. H Huang1, Q Huang2, P Krahenbuhl, European Conference on Computer Vision (ECCV). 2018</p>
<p>Fully convolutional networks for semantic segmentation. J Long, * , E Shelhamer, T Darrel, Computer Vision and Pattern Recogntion (CVPR). 2015</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. M Jaderberg, V Mnih, W M Czarnecki, T Schaul, J Z Leibo, D Silver, K Kavukcuoglu, International Conference in Learning Representations (ICLR). 2017</p>
<p>E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. 2017</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T D , Pieter Abbeel, Journal of Machine Learning Research. 22016</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International Journal of Robotics Research. 374-52018</p>
<p>Coupled generative adversarial networks. M.-Y Liu, O Tuzel, Neural Information Processing Symposium (NeurIPS). 2016</p>
<p>Learning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M I Jordan, International Conference in Machine Learning (ICML). 2015</p>
<p>M Savva, * , A Kadian, * , O Maksymets, * , Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, arXiv:1904.01201Habitat: A platform for embodied ai research. 201915arXiv preprint</p>
<p>Learning to navigate in complex environments. P Mirowski, R Pascanu, F Viola, H Soyer, A J Ballard, A Banino, M Denil, R Goroshin, L Sifre, K Kavukcuoglu, D Kumaran, R Hadsell, International Conference in Learning Representations (ICLR). 2017</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International conference on machine learning. 2016</p>
<p>Driving policy transfer via modularity and abstraction. M Müller, A Dosovitskiy, B Ghanem, V Koltun, European Conference on Computer Vision (ECCV). 2018</p>
<p>Curiositydriven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops2017</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, IEEE International Conference on Robotics and Automation (ICRA). 132018. 2018IEEE</p>
<p>Unsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, International Conference in Learning Representations (ICLR). 2016</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Večerík, T Rothörl, N Heess, R Pascanu, R Hadsell, Conference on Robot Learning (CoRL). 20171</p>
<p>Mid-level visual representations improve generalization and sample efficiency for learning active tasks. A Sax, B Emi, A R Zamir, L J Guibas, S Savarese, J Malik, CoRR, abs/1812.119712018</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Semantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, Computer Vision and Pattern Recogntion (CVPR). 201756</p>
<p>Deep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, European Conference on Computer Vision (ECCV). 2016</p>
<p>Unsupervised crossdomain image generation. Y Taigman, A Polyak, L Wolf, International Conference in Learning Representations (ICLR). 2017</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE201713</p>
<p>Adversarial discriminative domain adaptation. E Tzeng, J Hoffman, T Darrell, K Saenko, Computer Vision and Pattern Recogntion (CVPR). 2017</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z He, A Sax, J Malik, S Savarese, Computer Vision and Pattern Recogntion (CVPR). 2018. 1, 5, 6</p>
<p>C Yan, D Misra, A Bennnett, A Walsman, Y Bisk, Y Artzi, arXiv:1801.07357CHALET: Cornell house agent learning environment. 2018</p>
<p>Taskonomy: Disentangling task transfer learning. A R Zamir, A Sax, W B Shen, L J Guibas, J Malik, S Savarese, Computer Vision and Pattern Recogntion (CVPR). IEEE2018</p>
<p>Deep reinforcement learning with successor features for navigation across similar environments. J Zhang, J T Springenberg, J Boedecker, W Burgard, International Conference on Intelligent Robotics and Systems. 2017</p>
<p>Curriculum domain adaptation for semantic segmentation of urban scenes. Y Zhang, P David, B Gong, International Conference on Computer Vision (ICCV). 2017</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, International Conference on Computer Vision (ICCV). 2017</p>
<p>Visual semantic planning using deep successor representations. Y Zhu, D Gordon, E Kolve, D Fox, L Fei-Fei, A Gupta, R Mottaghi, A Farhadi, International Conference on Computer Vision (ICCV). 2017</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, International Conference on Robotics and Automation (ICRA). 2017</p>
<p>. Group-Norm Conv, Kernel, Stride=4 Pad=3</p>
<p>Group-Norm Conv, Group-Norm, ELU Kernel: 64 x 128 x 3 x 3 Stride=1 Pad=1 MaxPool Kernel: 2x2 Stride=2 Pad=0 Conv, Group-Norm, ELU Kernel: 128 x 128 x 3 x 3 Stride=1 Pad=1 MaxPool Kernel: 2x2 Stride=2 Pad=0 RGB Image Size: 256x256 Bilinear Upsample Ratio. ELU Kernel: 32 x 64 x 3 x 3 Stride=1 Pad=1 MaxPool Kernel: 2x2 Stride=2 Pad=0 Conv</p>
<p>. Group-Norm Conv, Kernel, 128 x 128 x 3 x 3 Stride=1 Pad=1</p>
<p>. Group-Norm Conv, Kernel, 128 x 128 x 3 x 3 Stride=1 Pad=1</p>
<p>. Group-Norm Conv, Kernel, 128 x 128 x 3 x 3 Stride=1 Pad=1</p>
<p>. Bilinear Upsample, Ratio , </p>
<p>. Group-Norm Conv, Kernel, 128 x 64 x 3 x 3 Stride=1 Pad=1</p>
<p>. Bilinear Upsample, Ratio , </p>
<p>. Group-Norm Conv, Kernel, 64 x 32 x 3 x 3 Stride=1 Pad=1</p>
<p>. Bilinear Upsample, Ratio , </p>
<p>. Group-Norm Conv, Kernel, 32 x 32 x 3 x 3 Stride=1 Pad=1</p>
<p>. Bilinear Upsample, Ratio , </p>
<p>32 x 32 x 3 x 3 Stride=1 Pad=1 RGB Pred: Conv Kernel: 32 x 3 x 1 x 1 Depth Pred: Conv Kernel: 32 x 1 x 1 x 1 Normals Pred. Group-Norm Conv, Kernel, Conv Kernel: 32 x 3 x 1 x 1. </p>
<p>. Group-Norm Conv, Kernel, 128 x 256 x 3 x 3 Stride=1 Pad=1</p>
<p>Group-Norm Conv, ELU Kernel: 256 x 256 x 3 x 3 Stride=1 Pad=1 AvgPool Kernel: 2x2 Stride=2 Pad=0 Flatten, Linear Size: 4096 x 256 GRU Size. </p>
<p>Target Vector: 2x1 Last Action One-Hot: 3x1 Linear. ELU Size</p>            </div>
        </div>

    </div>
</body>
</html>
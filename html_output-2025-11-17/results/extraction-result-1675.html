<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1675 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1675</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1675</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-268513498</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.11459v1.pdf" target="_blank">ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping</a></p>
                <p><strong>Paper Abstract:</strong> To tackle the"reality gap"encountered in Sim-to-Real transfer, this study proposes a diffusion-based framework that minimizes inconsistencies in grasping actions between the simulation settings and realistic environments. The process begins by training an adversarial supervision layout-to-image diffusion model(ALDM). Then, leverage the ALDM approach to enhance the simulation environment, rendering it with photorealistic fidelity, thereby optimizing robotic grasp task training. Experimental results indicate this framework outperforms existing models in both success rates and adaptability to new environments through improvements in the accuracy and reliability of visual grasping actions under a variety of conditions. Specifically, it achieves a 75\% success rate in grasping tasks under plain backgrounds and maintains a 65\% success rate in more complex scenarios. This performance demonstrates this framework excels at generating controlled image content based on text descriptions, identifying object grasp points, and demonstrating zero-shot learning in complex, unseen scenarios.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1675.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1675.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALDM-Grasping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-real pipeline that uses an adversarial supervision layout-to-image diffusion model (ALDM) to generate photorealistic, spatially-consistent images from Gazebo segmentation layouts and trains a YOLOv8 visual-grasping agent zero-shot for real-world grasping using two RealSense RGB-D views.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ALDM-Grasping robot pipeline (two-camera visual-grasping robot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Physical robot with two RealSense RGB-D cameras (global and local views) and a manipulator controlled by a closed-loop control unit; perception is provided by a YOLOv8 detector trained on ALDM-generated images to produce bounding boxes that drive base and gripper motion.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / visual grasping</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Gazebo scenes built to match the laboratory layout; provides rendered RGB images, semantic/instance segmentation masks and camera viewpoints (semantic camera used to record segmentation images). Physics are standard Gazebo (scene geometry, object placement and camera perspective); photorealism is added offline via an image synthesis diffusion model (ALDM).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>photorealistic rendering for appearance (via ALDM); standard Gazebo scene geometry and camera simulation (physics fidelity not emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>appearance/textures/lighting (via diffusion model), semantic labels and spatial layout (object categories, positions, sizes), camera viewpoints (global/local), segmentation masks (pixel-level labels)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>contact dynamics and detailed physical interaction (grasp contacts, friction), actuator dynamics and delays, explicit sensor noise models for RealSense depth, temporal dynamics; Gazebo scenes primarily provided visual layouts not detailed interaction physics for training grasps</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Laboratory settings with a physical robot equipped with two RealSense RGB-D cameras; evaluated across five real-world scenes ranging from plain backgrounds to complex backgrounds augmented with novel visual noise.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visual object detection and closed-loop grasp execution (predict bounding boxes for grasp targets using dual views and drive robot/gripper to perform grasps).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning: image-generation model (ALDM) trained on ADE20K/Cityscapes; YOLOv8 object detector trained on ALDM-generated realistic-style images with segmentation masks as labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate over 20 trials (plain and complex backgrounds); object detection metrics on a real-image test set (Precision, Recall, mAP50, mAP50-95); Inception Score (IS) for appearance fidelity of generated images.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Grasp success rates: 75% (plain backgrounds, 20 trials) and 65% (complex backgrounds, 20 trials). Detection metrics (YOLOv8 trained on ALDM-generated images, evaluated on real test set): IS 5.641; Precision 0.538; Recall 0.574; mAP50 0.633; mAP50-95 0.313.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual domain gap (appearance and especially spatial inconsistency between generated images and simulation layouts), background complexity and novel visual noise in real scenes, lack of modeling of contact/physics for grasp interactions, potential sensor differences (RealSense) and unmodeled noise.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Adversarial supervision via a segmenter-based discriminator that enforces pixel-level segmentation consistency between generated images and layout inputs, producing photorealistic images that preserve object count, position and size; training YOLOv8 on these images; using dual camera (global + local) views to decouple base and gripper control; zero-shot evaluation without real data fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Photorealistic rendering that preserves spatial consistency (object positions, sizes, counts) is essential; appearance fidelity alone is insufficient—spatial/semantic consistency between simulation layouts and generated images is critical for effective training of detection and grasp policies.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared three sim-to-real image-translation methods (CycleGAN, ControlNet, ALDM): ALDM achieved the best combination of appearance fidelity and spatial consistency, leading to superior object-detection metrics and grasp success (ALDM: grasp 75% plain / 65% complex). ControlNet produced higher appearance fidelity than CycleGAN but worse spatial consistency and lower grasp success (25% / 5%). CycleGAN failed to generalize (0% grasp success).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adversarially supervised layout-to-image diffusion (ALDM) produces photorealistic images that preserve spatial/semantic layout, enabling zero-shot sim-to-real transfer for visual grasping with substantially higher detection performance and grasp success than GAN or ControlNet baselines; spatial consistency is more important than raw appearance fidelity for downstream detection/grasp transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1675.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1675.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ControlNet baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ControlNet (Adding conditional control to text-to-image diffusion models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditioned diffusion-based image synthesis model used as a baseline: segmentation/layout images were input to a fine-tuned ControlNet (stable-diffusion-v2.1) to generate realistic-style training images for YOLOv8 in the sim-to-real pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adding conditional control to text-to-image diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ControlNet-based sim-to-real image-generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Pipeline where Gazebo segmentation/layout images are used as conditioning input to ControlNet (fine-tuned Stable Diffusion v2.1) to produce realistic-style images that are then used to train a YOLOv8 detector for real-world grasping evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / visual grasping</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same Gazebo scenes and segmentation masks as used for ALDM; ControlNet conditions on segmentation/layout images to produce photorealistic images.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>appearance-focused photorealistic rendering via conditioned diffusion (ControlNet), but with weaker enforcement of spatial consistency between layout and generated output.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>appearance (textures, lighting), overall scene style; conditioned to an input image but reported to sometimes alter object positions/contents.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>spatial consistency enforcement (positions, counts, sizes of objects could deviate), dynamics/contact physics, sensor noise, realistic background variability.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same physical laboratory robot setup with two RealSense RGB-D cameras; evaluated across plain and complex backgrounds with visual distractors/noise.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visual object detection (training YOLOv8 on ControlNet-generated images) and subsequent closed-loop grasp execution on the physical robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised training of YOLOv8 on ControlNet-generated images with segmentation labels from simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate over 20 trials (plain and complex backgrounds); detection metrics (Precision, Recall, mAP50, mAP50-95) on real test set; Inception Score for appearance fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Appearance fidelity (IS) 5.516. Detection: Precision 0.331; Recall 0.560; mAP50 0.499; mAP50-95 0.279. Grasp success: 25% (plain), 5% (complex) over 20 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Neglect of spatial/semantic consistency despite good appearance fidelity (object positions/sizes/numbers can deviate), inadequate object generation quality/quantity for reliable detector training, background complexity causing generalization failures.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Conditioned generation can improve appearance, but without explicit pixel-level segmentation adversarial supervision, spatial consistency between layout and generated image was insufficient to enable reliable zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Shows that high visual fidelity alone (appearance) is not sufficient; spatial/semantic consistency with simulation labels is required for effective transfer to detection/grasp tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Although ControlNet produced higher Inception Scores than CycleGAN, its detection metrics and grasp success were worse than ALDM; ControlNet: IS 5.516 but low detection P (0.331) and low grasp success (25% plain, 5% complex).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ControlNet can generate high-appearance-fidelity images but fails to maintain the spatial/semantic consistency required for training detectors that generalize to real cluttered scenes, resulting in low grasp success compared to ALDM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1675.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1675.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CycleGAN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unpaired image-to-image translation using cycle-consistent adversarial networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN-based unpaired image-to-image translation method used as a baseline to generate realistic-style images from simulation segmentation; evaluated for use as training data for YOLOv8 in sim-to-real grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unpaired imageto-image translation using cycle-consistent adversarial networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>CycleGAN-based sim-to-real image-generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Pipeline where segmentation/layout images from Gazebo were translated to realistic-style images using CycleGAN and these images were used to train a YOLOv8 detector evaluated on the physical robot.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / visual grasping</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Gazebo scenes providing segmentation/layout images; CycleGAN performs unpaired mapping to a realistic image domain.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>GAN-based appearance translation (lower appearance fidelity and poor generalization compared to diffusion methods in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>appearance/textures to some degree, but limited diversity and lower realism; some attempt at spatial mapping but often imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>spatial/semantic consistency (object positions/counts/arrangements), background variability, detailed appearance fidelity, dynamics/contact modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same physical two-camera robot and lab scenes; plain and complex background tests.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visual object detection for grasping (YOLOv8 trained on CycleGAN-generated images) and subsequent grasp attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised training of YOLOv8 on CycleGAN-generated images with segmentation labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate over 20 trials; detection metrics (Precision, Recall, mAP50, mAP50-95) and Inception Score.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Appearance fidelity (IS) 3.656. Detection: Precision 0.575; Recall 0.444; mAP50 0.554; mAP50-95 0.284. Grasp success: 0% (plain), 0% (complex) over 20 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Insufficient generalization to real-world complex backgrounds and inability to produce reliable object appearance/positioning needed for detector training; poor appearance fidelity compared to diffusion baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>None observed — CycleGAN-generated images were insufficient for zero-shot transfer in these grasping tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Demonstrates that lower-fidelity GAN translations (appearance and spatial inconsistencies) can result in complete transfer failure for downstream grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>CycleGAN had the lowest Inception Score and failed to produce any successful grasps (0%), performing worse than ControlNet and ALDM in both detection metrics and grasp success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CycleGAN-based sim-to-real translation failed to generalize to complex real backgrounds and produced no successful grasps, highlighting limitations of GAN-based unpaired translation for zero-shot grasp-transfer when spatial consistency and appearance fidelity are inadequate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1675.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1675.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim-Only baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-Only (no sim-to-real image translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where models are trained only on raw simulation images/segmentation without any image-domain translation to realistic style; evaluated for detection/grasp transfer to real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Sim-Only pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>YOLOv8 trained directly on Gazebo-rendered segmentation/simulation images (no image-to-image translation), then evaluated zero-shot on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / visual grasping</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Gazebo-rendered simulation images (raw sim rendering and segmentation masks), without further photorealistic synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>low appearance fidelity (raw simulator renderings), simulation provides accurate segmentation labels and object layouts but not photorealistic imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>semantic segmentation labels and object layouts; simulated camera viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>realistic appearance/lighting, textures, sensor noise, dynamic interaction realism.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same physical robot and test environments (plain and complex backgrounds).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Visual detection for grasping trained on raw simulation images.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised training of YOLOv8 on raw sim images/labels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate over 20 trials; detection metrics and Inception Score.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Detection/appearance metrics (Table formatting ambiguous in paper but reported low IS and modest detection metrics). Grasp success: 10% (plain), 0% (complex) over 20 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Large appearance gap between raw simulator renderings and real images; inability to handle background complexity and visual distractors; lack of photorealistic domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>None in this work — raw simulation alone produced poor zero-shot transfer except minimal success in simplest scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Indicates that raw simulation appearance fidelity is insufficient for reliable zero-shot transfer to real-world grasping tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Sim-Only had the poorest appearance fidelity and very low grasp success (10% plain, 0% complex), worse than GAN and diffusion-based translated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training solely on raw simulator images (no translation) yields very limited zero-shot transfer; photorealistic and spatially-consistent image synthesis substantially improves downstream detection and grasp success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retinagan: An object-aware approach to sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>Digital twin (dt)-cyclegan: Enabling zero-shot sim-to-real transfer of visual grasping models <em>(Rating: 2)</em></li>
                <li>Using simulation and domain adaptation to improve efficiency of deep robotic grasping <em>(Rating: 2)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 2)</em></li>
                <li>RL-CycleGAN: Reinforcement learning aware simulation-to-real <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1675",
    "paper_id": "paper-268513498",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "ALDM-Grasping",
            "name_full": "ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping",
            "brief_description": "A sim-to-real pipeline that uses an adversarial supervision layout-to-image diffusion model (ALDM) to generate photorealistic, spatially-consistent images from Gazebo segmentation layouts and trains a YOLOv8 visual-grasping agent zero-shot for real-world grasping using two RealSense RGB-D views.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ALDM-Grasping robot pipeline (two-camera visual-grasping robot)",
            "agent_system_description": "Physical robot with two RealSense RGB-D cameras (global and local views) and a manipulator controlled by a closed-loop control unit; perception is provided by a YOLOv8 detector trained on ALDM-generated images to produce bounding boxes that drive base and gripper motion.",
            "domain": "general robotics manipulation / visual grasping",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Gazebo scenes built to match the laboratory layout; provides rendered RGB images, semantic/instance segmentation masks and camera viewpoints (semantic camera used to record segmentation images). Physics are standard Gazebo (scene geometry, object placement and camera perspective); photorealism is added offline via an image synthesis diffusion model (ALDM).",
            "simulation_fidelity_level": "photorealistic rendering for appearance (via ALDM); standard Gazebo scene geometry and camera simulation (physics fidelity not emphasized)",
            "fidelity_aspects_modeled": "appearance/textures/lighting (via diffusion model), semantic labels and spatial layout (object categories, positions, sizes), camera viewpoints (global/local), segmentation masks (pixel-level labels)",
            "fidelity_aspects_simplified": "contact dynamics and detailed physical interaction (grasp contacts, friction), actuator dynamics and delays, explicit sensor noise models for RealSense depth, temporal dynamics; Gazebo scenes primarily provided visual layouts not detailed interaction physics for training grasps",
            "real_environment_description": "Laboratory settings with a physical robot equipped with two RealSense RGB-D cameras; evaluated across five real-world scenes ranging from plain backgrounds to complex backgrounds augmented with novel visual noise.",
            "task_or_skill_transferred": "Visual object detection and closed-loop grasp execution (predict bounding boxes for grasp targets using dual views and drive robot/gripper to perform grasps).",
            "training_method": "Supervised learning: image-generation model (ALDM) trained on ADE20K/Cityscapes; YOLOv8 object detector trained on ALDM-generated realistic-style images with segmentation masks as labels.",
            "transfer_success_metric": "Grasp success rate over 20 trials (plain and complex backgrounds); object detection metrics on a real-image test set (Precision, Recall, mAP50, mAP50-95); Inception Score (IS) for appearance fidelity of generated images.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Grasp success rates: 75% (plain backgrounds, 20 trials) and 65% (complex backgrounds, 20 trials). Detection metrics (YOLOv8 trained on ALDM-generated images, evaluated on real test set): IS 5.641; Precision 0.538; Recall 0.574; mAP50 0.633; mAP50-95 0.313.",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Visual domain gap (appearance and especially spatial inconsistency between generated images and simulation layouts), background complexity and novel visual noise in real scenes, lack of modeling of contact/physics for grasp interactions, potential sensor differences (RealSense) and unmodeled noise.",
            "transfer_enabling_conditions": "Adversarial supervision via a segmenter-based discriminator that enforces pixel-level segmentation consistency between generated images and layout inputs, producing photorealistic images that preserve object count, position and size; training YOLOv8 on these images; using dual camera (global + local) views to decouple base and gripper control; zero-shot evaluation without real data fine-tuning.",
            "fidelity_requirements_identified": "Photorealistic rendering that preserves spatial consistency (object positions, sizes, counts) is essential; appearance fidelity alone is insufficient—spatial/semantic consistency between simulation layouts and generated images is critical for effective training of detection and grasp policies.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared three sim-to-real image-translation methods (CycleGAN, ControlNet, ALDM): ALDM achieved the best combination of appearance fidelity and spatial consistency, leading to superior object-detection metrics and grasp success (ALDM: grasp 75% plain / 65% complex). ControlNet produced higher appearance fidelity than CycleGAN but worse spatial consistency and lower grasp success (25% / 5%). CycleGAN failed to generalize (0% grasp success).",
            "key_findings": "Adversarially supervised layout-to-image diffusion (ALDM) produces photorealistic images that preserve spatial/semantic layout, enabling zero-shot sim-to-real transfer for visual grasping with substantially higher detection performance and grasp success than GAN or ControlNet baselines; spatial consistency is more important than raw appearance fidelity for downstream detection/grasp transfer.",
            "uuid": "e1675.0",
            "source_info": {
                "paper_title": "ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ControlNet baseline",
            "name_full": "ControlNet (Adding conditional control to text-to-image diffusion models)",
            "brief_description": "A conditioned diffusion-based image synthesis model used as a baseline: segmentation/layout images were input to a fine-tuned ControlNet (stable-diffusion-v2.1) to generate realistic-style training images for YOLOv8 in the sim-to-real pipeline.",
            "citation_title": "Adding conditional control to text-to-image diffusion models",
            "mention_or_use": "use",
            "agent_system_name": "ControlNet-based sim-to-real image-generation pipeline",
            "agent_system_description": "Pipeline where Gazebo segmentation/layout images are used as conditioning input to ControlNet (fine-tuned Stable Diffusion v2.1) to produce realistic-style images that are then used to train a YOLOv8 detector for real-world grasping evaluation.",
            "domain": "general robotics manipulation / visual grasping",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Same Gazebo scenes and segmentation masks as used for ALDM; ControlNet conditions on segmentation/layout images to produce photorealistic images.",
            "simulation_fidelity_level": "appearance-focused photorealistic rendering via conditioned diffusion (ControlNet), but with weaker enforcement of spatial consistency between layout and generated output.",
            "fidelity_aspects_modeled": "appearance (textures, lighting), overall scene style; conditioned to an input image but reported to sometimes alter object positions/contents.",
            "fidelity_aspects_simplified": "spatial consistency enforcement (positions, counts, sizes of objects could deviate), dynamics/contact physics, sensor noise, realistic background variability.",
            "real_environment_description": "Same physical laboratory robot setup with two RealSense RGB-D cameras; evaluated across plain and complex backgrounds with visual distractors/noise.",
            "task_or_skill_transferred": "Visual object detection (training YOLOv8 on ControlNet-generated images) and subsequent closed-loop grasp execution on the physical robot.",
            "training_method": "Supervised training of YOLOv8 on ControlNet-generated images with segmentation labels from simulation.",
            "transfer_success_metric": "Grasp success rate over 20 trials (plain and complex backgrounds); detection metrics (Precision, Recall, mAP50, mAP50-95) on real test set; Inception Score for appearance fidelity.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Appearance fidelity (IS) 5.516. Detection: Precision 0.331; Recall 0.560; mAP50 0.499; mAP50-95 0.279. Grasp success: 25% (plain), 5% (complex) over 20 trials.",
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Neglect of spatial/semantic consistency despite good appearance fidelity (object positions/sizes/numbers can deviate), inadequate object generation quality/quantity for reliable detector training, background complexity causing generalization failures.",
            "transfer_enabling_conditions": "Conditioned generation can improve appearance, but without explicit pixel-level segmentation adversarial supervision, spatial consistency between layout and generated image was insufficient to enable reliable zero-shot transfer.",
            "fidelity_requirements_identified": "Shows that high visual fidelity alone (appearance) is not sufficient; spatial/semantic consistency with simulation labels is required for effective transfer to detection/grasp tasks.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Although ControlNet produced higher Inception Scores than CycleGAN, its detection metrics and grasp success were worse than ALDM; ControlNet: IS 5.516 but low detection P (0.331) and low grasp success (25% plain, 5% complex).",
            "key_findings": "ControlNet can generate high-appearance-fidelity images but fails to maintain the spatial/semantic consistency required for training detectors that generalize to real cluttered scenes, resulting in low grasp success compared to ALDM.",
            "uuid": "e1675.1",
            "source_info": {
                "paper_title": "ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CycleGAN baseline",
            "name_full": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "brief_description": "A GAN-based unpaired image-to-image translation method used as a baseline to generate realistic-style images from simulation segmentation; evaluated for use as training data for YOLOv8 in sim-to-real grasping.",
            "citation_title": "Unpaired imageto-image translation using cycle-consistent adversarial networks",
            "mention_or_use": "use",
            "agent_system_name": "CycleGAN-based sim-to-real image-generation pipeline",
            "agent_system_description": "Pipeline where segmentation/layout images from Gazebo were translated to realistic-style images using CycleGAN and these images were used to train a YOLOv8 detector evaluated on the physical robot.",
            "domain": "general robotics manipulation / visual grasping",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Gazebo scenes providing segmentation/layout images; CycleGAN performs unpaired mapping to a realistic image domain.",
            "simulation_fidelity_level": "GAN-based appearance translation (lower appearance fidelity and poor generalization compared to diffusion methods in this study).",
            "fidelity_aspects_modeled": "appearance/textures to some degree, but limited diversity and lower realism; some attempt at spatial mapping but often imperfect.",
            "fidelity_aspects_simplified": "spatial/semantic consistency (object positions/counts/arrangements), background variability, detailed appearance fidelity, dynamics/contact modeling.",
            "real_environment_description": "Same physical two-camera robot and lab scenes; plain and complex background tests.",
            "task_or_skill_transferred": "Visual object detection for grasping (YOLOv8 trained on CycleGAN-generated images) and subsequent grasp attempts.",
            "training_method": "Supervised training of YOLOv8 on CycleGAN-generated images with segmentation labels.",
            "transfer_success_metric": "Grasp success rate over 20 trials; detection metrics (Precision, Recall, mAP50, mAP50-95) and Inception Score.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Appearance fidelity (IS) 3.656. Detection: Precision 0.575; Recall 0.444; mAP50 0.554; mAP50-95 0.284. Grasp success: 0% (plain), 0% (complex) over 20 trials.",
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Insufficient generalization to real-world complex backgrounds and inability to produce reliable object appearance/positioning needed for detector training; poor appearance fidelity compared to diffusion baselines.",
            "transfer_enabling_conditions": "None observed — CycleGAN-generated images were insufficient for zero-shot transfer in these grasping tasks.",
            "fidelity_requirements_identified": "Demonstrates that lower-fidelity GAN translations (appearance and spatial inconsistencies) can result in complete transfer failure for downstream grasping.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "CycleGAN had the lowest Inception Score and failed to produce any successful grasps (0%), performing worse than ControlNet and ALDM in both detection metrics and grasp success.",
            "key_findings": "CycleGAN-based sim-to-real translation failed to generalize to complex real backgrounds and produced no successful grasps, highlighting limitations of GAN-based unpaired translation for zero-shot grasp-transfer when spatial consistency and appearance fidelity are inadequate.",
            "uuid": "e1675.2",
            "source_info": {
                "paper_title": "ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Sim-Only baseline",
            "name_full": "Sim-Only (no sim-to-real image translation)",
            "brief_description": "Baseline where models are trained only on raw simulation images/segmentation without any image-domain translation to realistic style; evaluated for detection/grasp transfer to real robot.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "Sim-Only pipeline",
            "agent_system_description": "YOLOv8 trained directly on Gazebo-rendered segmentation/simulation images (no image-to-image translation), then evaluated zero-shot on the real robot.",
            "domain": "general robotics manipulation / visual grasping",
            "virtual_environment_name": "Gazebo",
            "virtual_environment_description": "Gazebo-rendered simulation images (raw sim rendering and segmentation masks), without further photorealistic synthesis.",
            "simulation_fidelity_level": "low appearance fidelity (raw simulator renderings), simulation provides accurate segmentation labels and object layouts but not photorealistic imagery.",
            "fidelity_aspects_modeled": "semantic segmentation labels and object layouts; simulated camera viewpoints.",
            "fidelity_aspects_simplified": "realistic appearance/lighting, textures, sensor noise, dynamic interaction realism.",
            "real_environment_description": "Same physical robot and test environments (plain and complex backgrounds).",
            "task_or_skill_transferred": "Visual detection for grasping trained on raw simulation images.",
            "training_method": "Supervised training of YOLOv8 on raw sim images/labels.",
            "transfer_success_metric": "Grasp success rate over 20 trials; detection metrics and Inception Score.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Detection/appearance metrics (Table formatting ambiguous in paper but reported low IS and modest detection metrics). Grasp success: 10% (plain), 0% (complex) over 20 trials.",
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Large appearance gap between raw simulator renderings and real images; inability to handle background complexity and visual distractors; lack of photorealistic domain adaptation.",
            "transfer_enabling_conditions": "None in this work — raw simulation alone produced poor zero-shot transfer except minimal success in simplest scenes.",
            "fidelity_requirements_identified": "Indicates that raw simulation appearance fidelity is insufficient for reliable zero-shot transfer to real-world grasping tasks.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Sim-Only had the poorest appearance fidelity and very low grasp success (10% plain, 0% complex), worse than GAN and diffusion-based translated datasets.",
            "key_findings": "Training solely on raw simulator images (no translation) yields very limited zero-shot transfer; photorealistic and spatially-consistent image synthesis substantially improves downstream detection and grasp success.",
            "uuid": "e1675.3",
            "source_info": {
                "paper_title": "ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retinagan: An object-aware approach to sim-to-real transfer",
            "rating": 2,
            "sanitized_title": "retinagan_an_objectaware_approach_to_simtoreal_transfer"
        },
        {
            "paper_title": "Digital twin (dt)-cyclegan: Enabling zero-shot sim-to-real transfer of visual grasping models",
            "rating": 2,
            "sanitized_title": "digital_twin_dtcyclegan_enabling_zeroshot_simtoreal_transfer_of_visual_grasping_models"
        },
        {
            "paper_title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping",
            "rating": 2,
            "sanitized_title": "using_simulation_and_domain_adaptation_to_improve_efficiency_of_deep_robotic_grasping"
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 2,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "RL-CycleGAN: Reinforcement learning aware simulation-to-real",
            "rating": 1,
            "sanitized_title": "rlcyclegan_reinforcement_learning_aware_simulationtoreal"
        }
    ],
    "cost": 0.014884749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping</p>
<p>Yiwei Li 
Zihao Wu 
Huaqin Zhao 
Tianze Yang 
Zhengliang Liu 
Peng Shu 
Jin Sun 
Ramviyas Parasuraman 
Tianming Liu 
ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping
193041ECE16F94B32458796B69E45444
To tackle the "reality gap" encountered in Sim-to-Real transfer, this study proposes a diffusion-based framework that minimizes inconsistencies in grasping actions between the simulation settings and realistic environments.The process begins by training an adversarial supervision layout-to-image diffusion model(ALDM).Then, leverage the ALDM approach to enhance the simulation environment, rendering it with photorealistic fidelity, thereby optimizing robotic grasp task training.Experimental results indicate this framework outperforms existing models in both success rates and adaptability to new environments through improvements in the accuracy and reliability of visual grasping actions under a variety of conditions.Specifically, it achieves a 75% success rate in grasping tasks under plain backgrounds and maintains a 65% success rate in more complex scenarios.This performance demonstrates this framework excels at generating controlled image content based on text descriptions, identifying object grasp points, and demonstrating zero-shot learning in complex, unseen scenarios.</p>
<p>I. INTRODUCTION</p>
<p>To perform optimally, deep learning models require extensively annotated datasets [1].However, in robotics tasks such as visual grasping, acquiring datasets to train comprehensive deep learning models in real-world settings can be prohibitively expensive and time-consuming, and sometimes, unfeasible [2], [3], [4].This problem is also known as the "reality gap".To address this challenge, Sim-to-Real strategies have been developed, allowing the training of robotic performance models, such as those for visual grasping, to be trained in simulated environments and subsequently transferred to real settings through techniques like domain randomization and domain adaptation [4], [5].Domain randomization diversifies visual elements (e.g., textures, colors) in training simulations to make models focus on invariant aspects of images applicable in real-world scenarios.Conversely, domain adaptation tailors models from the simulated (source) domain for the real (target) domain, often leveraging unlabeled target domain data [6].</p>
<p>In image transfer tasks, GANs, such as RetinaGAN [7], CycleGAN [8] are usually used as the main technical framework [9].Some variants, such as DT-CycleGAN [10] and StarGAN [11], are also invented to address different challenges.A major advantage of these kinds of models is that their training process does not require data to appear in pairs, which allows them to show better performance on specific * Equal Contribution.</p>
<p>†Corresponding authors: Tianming Liu (tliu@uga.edu),Ramviyas Parasuraman (ramviyas@uga.edu), and Jin Sun (jinsun@uga.edu) 1 School of Computing, University of Georgia, Athens, GA 30602, USA (a) Generated image results of different models trained in ADE20k dataset.</p>
<p>(b) Generated image results of ALDM trained in Cityscapes dataset.</p>
<p>Fig. 1: ALDM has great performance in zero-shot image generation and style transfer.The image generation model used in this article contains data of various types and can generate objects in various robotic application scenarios.The generated image is not only close to the realistic-style scene but also ensures that the number and position of key objects are consistent with the original image.There is huge potential to provide more diverse, realistic, and accurate training samples for robot action planning applications.</p>
<p>tasks.However, GAN models also have some limitations, the most significant of which are their preliminary requirements for large amounts of training data and limitations in the model's generalization capabilities.The latter means that when faced with new tasks or scenarios, existing GAN models often need to be re-trained from scratch, which greatly constrains their applicability and flexibility in different appli-arXiv:2403.11459v1[cs.RO] 18 Mar 2024 cation cases.In contrast, the advantages of diffusion models in this aspect are revealed.To effectively control the image effects generated by the diffusion model, the research field has developed a variety of technical paths, including layoutto-image, text-to-image, and image-to-image generations.ControlNet [12], an innovative deep learning framework, is engineered to exert meticulous control over generative models' outputs, with a particular emphasis on image synthesis tasks.This sophisticated architecture exhibits exceptional abilities in transfer-learning from prompts to images, transcending the conventional constraints paradigms.ControlNet is adept at interpreting textual prompts, thereby furnishing users with the ability to steer the image generation process with accurate content of the images and adaptability.The core idea usually involves guiding the generation process through conditional input, thereby achieving fine control over the properties of the generated image (such as layout, style, content, etc.).This method is particularly suitable for application scenarios that require highly customized output.However, although ControlNet can utilize a condition image as input to constrain the output the training of ControlNet is more complicated, and the training results are relatively uncontrollable.Although there is a condition image as input, it may still cause deviations in the positions, sizes, numbers, and arrangements of major objects in the output, so the ControlNet is not suitable for this task.Fine-tuning Con-trolNet is a potential solution, but the fine-tuning process requires a large amount of data.It does not necessarily guarantee results, which means it is not an efficient and reliable solution.</p>
<p>One possible way to solve the deviations is to use the layout-to-image method, which aims to generate realistic images based on predetermined layout information [13].In this framework, "layout" generally refers to an abstract representation of objects in an image, covering their categories, locations, sizes, and their interrelationships.This method offers a significant advantage in robotic grasping by allowing precise manipulation of both the layout and content within generated images.This precision is particularly beneficial for the transfer of simulated images to robot training, as maintaining consistent positioning and relative relationships among target objects is critical for avoiding later object detection training failures.However, this scheme also has a limitation that cannot be ignored, which is that labels of different layouts are the only control over the content of images, which makes the controls on the image contents simple and general.</p>
<p>So in comparison, the adversarial supervised layout-toimage diffusion model (ALDM) [14] is especially suitable for the Sim-to-Real task [14].In the training phase of the ALDM, a segmenter-based discriminator is connected in series with a diffusion model.This configuration facilitates the quantitative assessment of the discrepancy between the segmented outcomes of images produced by the diffusion process and the ground-truth segmentation data.Consequently, the resulting model generates images where both the objects and spatial arrangement closely approximate reality (Fig. 1(a)).Simultaneously, the model preserves the prompt control feature inherent in diffusion models.This characteristic ensures that the synthesized images not only align well with the Sim-to-Real (simulation to reality) objectives pertinent to robotic applications but also support the facile adaptation to diverse experimental styles (Fig. 1(b)).Hence, the images crafted by this model are exceptionally conducive to robotic training across varied scenarios.The main contributions of this work are as follows:</p>
<p>• We build a task learning pipeline on top of the state-ofthe-art ALDM approach to the task of the Sim-to-Real robotic training and use quantitative results on generated images' segmentation to show that this method is highly appropriate for the Sim-to-Real robotic grasping task.</p>
<p>• We design a pipeline to train and control a two-camera robot to work in reality-unseen environments.The results show that the robot performs well in grasping unseen objects in unpredictably complex circumstances, which proves the reliability of the pipeline design.</p>
<p>II. RELATED WORK A. Visual Grasping</p>
<p>Visual grasping is an interdisciplinary field at the intersection of robotics, computer vision, artificial intelligence, and cognitive science, aiming to enable machines to interact with their environment through the detection, analysis, and manipulation of objects.This complex challenge involves both visual data processing and the control of robotic actuators [15], [16].With advancements in deep learning, visionbased robotic grasping has become a focal point of research, adopting two primary methodologies: object reconstruction and end-to-end approaches [17].</p>
<p>Object reconstruction methods focus on creating a detailed 3D model of the environment for precise planning and execution of grasps, offering accuracy at the cost of computational intensity and slower response times [18].Conversely, endto-end methods leverage machine learning to directly map visual input to grasping actions without intermediate modeling, facilitating rapid adaptation to diverse scenarios albeit with less interpretability and higher demands on training data and computational resources [19], [2], [3], [4], [17].images further.RetinaGAN's innovation lies in its ability to preserve object structures and textures across translations, offering significant improvements in sim-to-real tasks for robotics and demonstrating the adaptability of GANs to diverse visual environments.</p>
<p>B. Image Generation Models for Bridging Reality Gap</p>
<p>2) Diffusion Models: GANs initiated the era of generative modeling, but diffusion models represent a significant paradigm shift by transforming noise into detailed images through iterative refinement, addressing GANs' issues like training instability and mode collapse [20].Among these advancements, ControlNet [12] introduces spatial conditioning to text-to-image diffusion models, allowing for precise image generation based on specific inputs such as edges and poses.Concurrently, FreestyleNet [21] extends image synthesis capabilities through Freestyle Layout-to-Image Synthesis (FLIS), which integrates textual and layout inputs using Rectified Cross-Attention (RCA).These innovations highlight the flexibility and potential of diffusion models in pushing the boundaries of generative art and technology.</p>
<p>C. Robotic Application</p>
<p>Sim-to-Real transfer is a crucial technique in robotics, aimed at bridging the gap between simulations and realworld applications [17], [22], [23], [24].Key advancements include RetinaGAN, which improves simulated imagery for visual grasping through object-detection consistency [7], and RL-CycleGAN, which integrates Q-learning's Q-value with a reinforcement learning (RL) scene consistency loss to enhance Sim-to-Real transfer in RL applications [25], [26].</p>
<p>These GAN-based methods have significantly advanced the efficacy of visual grasping models by optimizing the use of virtual data.</p>
<p>In summary, our research integrates diffusion models with adversarial training to refine Sim-to-Real transfer [27].This hybrid approach combines the gradient-guided progression from noise to realistic data of diffusion models with the robust learning of adversarial methods, offering precise object locations, controllable image content generation, and zeroshot capacity for unseen scenario in Sim-to-Real applications.</p>
<p>III. METHODS</p>
<p>A. Robot Training and Robotic Control Pipeline</p>
<p>To train a robot to grasp, the whole working pipeline can be seen in Figure 2. The whole procedure contains two parts:</p>
<p>• Image generation model part firstly builds the simulation environments in the gazebo simulation platform.</p>
<p>The labels of different objects can be set into the gazebo, thus the segmentation image can be easily obtained.These segmentation images are fed into the diffusion model as the condition images/layout images, and the ground truth labels are used for the training of object detection.In this part (Figure 2(a)), segmentation images are set as the layout and passed with the prompt to the ALDM, which is a U-Net [28] structure.The generated images, produced by the diffusion model, will be passed to a pre-trained segmenter-based discriminator.The discriminator will compare the segmentation results of the generated image and the layout image to guarantee the content of the generated realistic-style image consistent with the input.• Grasping Agent Network After the realistic-style images are generated based on the simulation environments, these generated images will be used as the training data for the object detection model (YOLOv8 [29]).</p>
<p>The segmentation images will be treated as labels.The two RealSense RGB-D cameras on the physical robot will pass the image data to the pre-trained YOLOv8 model to separately generate the bounding box.Finally, the Control Unit will calculate the distance between bounding boxes to decide the robot base and gripper movement.</p>
<p>B. Layout-to-Image Diffusion Model</p>
<p>ALDM innovates the field of image synthesis by introducing adversarial supervision within the training framework of diffusion models.The key to this advancement is the employment of a discriminator, a segmentation-based model, that furnishes explicit, pixel-level feedback on the alignment between the generated images and the input layout.This mechanism not only bolsters the fidelity of the generated images to the specified layout but also preserves the versatility of text-driven editability.Such advancements are important for applications demanding high precision in generated content, such as robotic grasping, where accurate object positioning and count are paramount for effective training.</p>
<p>The segmenter functions as a discriminator, tasked with classifying per-pixel class labels in authentic images against corresponding ground-truth maps.Simultaneously, it designates the synthetic outputs from the diffusion model as an additional "fake" category l.Suppose the input(layout image) of the diffusion model is I image and the output(generated image) is Îimage .Given that the discriminator's role is fundamentally to address a multi-class semantic segmentation challenge, its training objective emanates from a foundational cross-entropy loss:
L Dis = −E   N c=1 γ c H×W i,j l i,j,c log(Dis(I image ) i,j,c )]   −E   H×W i,j log(Dis( Îimage ) i,j,c=N +1 )   , (1)
where N is the number of the semantic categories with H × W represents the spatial size of the input.To control the importance of different objects
γ c = H × W E<a href="2">l i,j,c = 1</a></p>
<p>C. Object Detection and Grasping</p>
<p>To fulfill the object detection, the grasping agent framework utilizes dual-camera imagery as its primary input to deduce the precise location for object grasping.This system incorporates two distinct visual perspectives: one providing a broad overview from the robot's base, termed the global view, and another offering a detailed focus on the gripper arm, referred to as the local view.The global view guides the movement of the robot base and the local view is responsible for adjusting the position of the gripper.The two cooperate to achieve accurate grasping of objects.Thus, the system's input is symbolized as x in the domain of x ∈ R B2×C×H×W , integrates these two visual feeds, where B2 signifies the batch size coupled with the dual imagery inputs, and C×H×W delineates the images' channel (C), height (H), and width (W) dimensions.To ensure effective Sim-to-Real translation without prior real-world training, the network outputs spatial coordinates of the intended grasp target within the observed image, represented by a bounding box (bounding box) notation.The output format is denoted as a vector
a = [x min l , y min l , x max l , y max l , x min g , y min g , x max g , y max g ] ∈ R 8
within the R 8 space, captures the target's bounding box coordinates on both local (l) and global (g) images.This output subsequently informs the robot's grasping actions through a closed-loop mechanism based on the computed bounding box predictions.</p>
<p>The architecture of the grasping agent is partitioned into two principal components.First, the YOLOv8 model serves as the object detector.For efficient adaptation from preexisting models, input resolutions are standardized to 512 × 512 pixels.The final segment involves a robot control unit that orchestrates the actual grasping maneuvers and robot movements, grounded on the predicted bounding box coordinates and the concurrent visual inputs.The control unit's objective is to minimize grasping inaccuracies through an error function,
Error = α • (y pred l − y gt l ) + (1 − α) • (y pred g − y gt g ),(3)
where y pred * and y gt * represent the centers of the predicted and actual (ground truth) bounding box, respectively, across both camera views.The system aims for the predicted target's bounding box to coincide with an actual graspable location, effectuating a successful grasping action as the error approaches zero.</p>
<p>IV. EVALUATIONS</p>
<p>A. Datasets and Simulation Environment 1) The training datasets of the image generation model:</p>
<p>To make the ALDM model obtain the knowledge of realworld features and have the ability to generate realistic style images, we have used two public datasets to train the model, which is:</p>
<p>• The ADE20K dataset [30], jointly developed by the Massachusetts Institute of Technology and the University of Toronto, comprises over 22,000 images with detailed pixel-level annotations across a wide range of scenes and objects.It is particularly valuable for computer vision tasks that demand a comprehensive  3: Experiment results in image generation.In the Gazebo column, we constructed five simulation scenarios in the Gazebo framework, meticulously designed to align with the physical layout of the laboratory environment.In the Segmentation column, the outcomes of object segmentation, derived from the simulated settings, serve as the inputs for the layout-to-diffusion model, delineated through predefined labels.In the ControlNet column, the images show incorrect image contents and inaccurate object positions.In the ALDM column, it shows the output generated by the image synthesis model.Notably, the produced imagery not only exhibits precise localization of the target objects but also embodies a stylistic resemblance to the actual environmental setting, demonstrating the model's efficacy in bridging the gap between the simulated images' style and laboratory environment.</p>
<p>understanding of both the broader scene and the finer details of objects, including image segmentation and scene analysis.The dataset's extensive annotations, which cover a broad spectrum of objects and their parts with precision, make it an essential tool for researchers aiming to advance the field of detailed scene interpretation.</p>
<p>• The Cityscapes dataset [31] offers a comprehensive collection of over 25,000 urban scene images, designed to enhance semantic understanding of urban street environments.It uniquely combines 5,000 pixel-level annotated images for detailed analysis and 20,000 coarsely annotated images to support a broad range of vision tasks, notably in autonomous driving technologies.Captured across diverse cities, Cityscapes contains a variety of urban landscapes, seasonal variations, and weather conditions, making it a robust dataset for training and evaluating semantic segmentation models.This dataset meticulously annotates numerous elements, including vehicles, pedestrians, and urban fixtures, making it a useful tool for researchers working on innovative ways to analyze urban scenes and improve how intelligent systems understand these environments.These two public datasets offer the model the ability to generate realistic-style images from diverse robot application environments, such as indoor bedrooms, indoor kitchens, outdoor scenes, agricultural scenes, autonomous driving, etc.</p>
<p>2) Experimental Dataset: All the simulation scenes are built on the Gazebo platform.Examples can be seen in Figure 3, five independent scenarios were built, with the semantic camera set to record the segmentation image of different angles.Because the objects in the simulation environments are labeled, the segmentation masks or a bounding box can be easily obtained and work as the ground-truth label dataset in this experiment.In this research, 1235 experimental pairs were created.Each pair has a simulation image (Figure 3 Gazebo column), cropped by the simulation camera, an instance segmentation image, and a semantic segmentation image (Figure 3 Segmentation column).</p>
<p>B. Experiments in Image Generation</p>
<p>The key objective is to guarantee that the model can generate high-quality images that are suitable for training the object detection network (YOLOv8), thus the most important evaluation metric of this part is the deviations between the To obtain the training datasets in the comparison experiments, the segmentation images are sent to the trained Cycle-GAN, the fine-tuned ControlNet(stable-diffusion-v2.1 [32]), and ALDM(These three models were both trained on ADE20k to compare the performance) to generate realisticstyle images.The images generated by these three models were detected by YOLOv8 separately and analyzed quantitatively.</p>
<p>Table I quantitatively evaluates the quality of sim-to-real transfer.In the Appearance Fidelity section, we utilize the Inception Score (IS) to determine whether the sim-to-real generated images possess sufficient fidelity.In the Object Detection section, considering that images transferred from sim-to-real with superior spatial consistency are more effective as training data for detection, we incorporate sim-toreal images generated by different methods into the YOLOv8 detection training dataset and evaluate them using the same real-image test set.As shown in Table I, ALDM outperforms in both aspects, showcasing its exceptional zero-shot sim-toreal transfer capabilities.Interestingly, although the Control-Net model generates images of higher fidelity compared to CycleGAN, its detection outcomes are inferior.This may be attributed to ControlNet's emphasis on appearance while neglecting spatial consistency.The experiment assessed the grasping success rate across five real-world environments, ranging from basic to complex setups augmented with novel visual noise to challenge the segmentation model.As detailed in Table II, the experiment's results highlight the varying success rates of object grasping methods in 20 trials across plain and complex backgrounds.The failure of CycleGAN is attributed to its inability to generalize to complex testing environments when faced with varying backgrounds.ControlNet's underperformance is due to inadequate object generation quality and quantity.Conversely, our ALDM-based model excels not only in simple backgrounds, achieving a 75% success rate in object grasping tasks but also maintains a considerable 65% success rate in complex environments.This robust performance of ALDM can be credited to its capacity to generate high-quality images and the robustness of the algorithm, which stands as a testament to its superiority in handling object-grasping tasks under diverse conditions.</p>
<p>V. CONCLUSION &amp; DISCUSSION</p>
<p>This paper presents a groundbreaking framework, ALDM-Grasping, that utilizes ALDM for zero-shot Sim-to-Real transfer in visual grasping tasks.The model excels in generating controlled image content from textual descriptions, pinpointing object grasp positions, and demonstrating zeroshot learning in complex, unseen scenarios.Future work will extend ALDM to diverse gripper configurations and explore its applicability in 3D unstructured settings, such as robotic fruit harvesting and autonomous driving.Additionally, we aim to test ALDM's capabilities in varied manipulation tasks, including rotation and placement, to assess its versatility.The exploration of more vision backbone models will further refine ALDM's effectiveness in Sim-to-Real applications.</p>
<p>1 )
1
GANs Models: GANs revolutionized unsupervised learning by introducing a framework composed of a Generator, creating data that mimics real-world distributions, and a Discriminator, distinguishing between genuine and generated data.This innovation enables the production of realistic images, audio, and videos, significantly advancing the field of computer vision.Extending GANs,Zhu  et al. proposed CycleGAN for unpaired image-to-image translation, utilizing a cycle consistency loss alongside adversarial losses to facilitate high-quality translations across different datasets without requiring paired examples.Building upon these advancements, Ho et al. introduced RetinaGAN [7], an objectaware sim-to-real adaptation technique that leverages object detection consistency to enhance the realism of translated</p>
<p>Fig. 2 :
2
Fig. 2: The whole pipeline of the diffusion-model-based grasping robot.a) The generation procedures for realistic style images.b) The robot agent network designed for the task of object detection and grasping.</p>
<p>Fig. 4 :
4
Fig. 4: The object detection results of the physical robot.The first panel represents the detection performance of the simple background, and the second panel represents the complex background.It can be seen that the target objects are successfully detected while the background has lots of interference objects.</p>
<p>TABLE I :
I
Comparison of sim-to-real transformation quality across different methods.Appearance fidelity is evaluated using the Inception Score (IS), while Object Detection with YOLOv8 is assessed using typical metrics (Precision-P, Recall-R, mAP50, and mAP50-90).
MethodAppearance FidelityObject DetectionIS ↑PRmAP50 mAP50-95Sim-Only-0.421 0.3530.4600.255CycleGAN [8]3.6560.575 0.4440.5540.284ControlNet [12]5.5160.331 0.5600.4990.279ALDM [14]5.6410.538 0.5740.6330.313GazeboSegmentationControlNetALDMst Scene1nd Scene2rd Scene3th Scene4th Scene5Fig.</p>
<p>TABLE II :
II
The success rates of grasping in 20 trials for different methods with two types of backgrounds
MethodsPlain background Complex backgroundSim-Only10%0%CycleGAN0%0%ControlNet25%5%ALDM75%65%C. Experiments in Real Environment With Zero-Shot Sim-to-Real Transfer</p>
<p>Deep learning in robotics: Survey on model structures and training strategies. A I Károly, P Galambos, J Kuti, I J Rudas, IEEE Transactions on Systems, Man, and Cybernetics: Systems. 5112020</p>
<p>Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations. S Song, A Zeng, J Lee, T Funkhouser, IEEE Robotics and Automation Letters. 532020</p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, Conference on robot learning. PMLR2018</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Sim-to-real via simto-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. S James, P Wohlhart, M Kalakrishnan, D Kalashnikov, A Irpan, J Ibarz, S Levine, R Hadsell, K Bousmalis, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019637</p>
<p>Domain randomization and generative models for robotic grasping. J Tobin, L Biewald, R Duan, M Andrychowicz, A Handa, V Kumar, B Mcgrew, A Ray, J Schneider, P Welinder, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. D Ho, K Rao, Z Xu, E Jang, M Khansari, Y Bai, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE202110926</p>
<p>Unpaired imageto-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, Proceedings. null2017</p>
<p>Review of digital twin about concepts, technologies, and industrial applications. M Liu, S Fang, H Dong, C Xu, Journal of manufacturing systems. 582021</p>
<p>Digital twin (dt)-cyclegan: Enabling zero-shot sim-to-real transfer of visual grasping models. D Liu, Y Chen, Z Wu, IEEE Robotics and Automation Letters. 852023</p>
<p>Stargan: Unified generative adversarial networks for multi-domain image-toimage translation. Y Choi, M Choi, M Kim, J.-W Ha, S Kim, J Choo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Adding conditional control to text-to-image diffusion models. L Zhang, A Rao, M Agrawala, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Layoutdiffusion: Controllable diffusion model for layout-to-image generation. G Zheng, X Zhou, X Li, Z Qi, Y Shan, X Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202322499</p>
<p>Adversarial supervision makes layout-to-image diffusion models thrive. Y Li, M Keuper, D Zhang, A Khoreva, arXiv:2401.088152024arXiv preprint</p>
<p>Robotic grasping and contact: A review. A Bicchi, V Kumar, Proceedings 2000 ICRA. Millennium conference. IEEE international conference on robotics and automation. Symposia proceedings (Cat. No. 00CH37065). 2000 ICRA. Millennium conference. IEEE international conference on robotics and automation. Symposia proceedings (Cat. No. 00CH37065)IEEE20001</p>
<p>Comprehensive review on reaching and grasping of objects in robotics. Q M Marwan, S C Chua, L C Kwek, Robotica. 39102021</p>
<p>A survey on learning-based robotic grasping. K Kleeberger, R Bormann, W Kraus, M F Huber, Current Robotics Reports. 12020</p>
<p>Data-driven grasp synthesis-a survey. J Bohg, A Morales, T Asfour, D Kragic, IEEE Transactions on robotics. 3022013</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International journal of robotics research. 374-52018</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202210695</p>
<p>Freestyle layoutto-image synthesis. H Xue, Z Huang, Q Sun, L Song, W Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). </p>
<p>Visual domain adaptation: A survey of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE signal processing magazine. 3232015</p>
<p>Rlcyclegan: Reinforcement learning aware simulation-to-real. in 2020 ieee. K Rao, C Harris, A Irpan, S Levine, J Ibarz, M Khansari, CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020163</p>
<p>Learning from delayed rewards. C J C H Watkins, 1989</p>
<p>Z Wang, H Zheng, P He, W Chen, M Zhou, arXiv:2206.02262Diffusion-gan: Training gans with diffusion. 2022arXiv preprint</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference. Munich, GermanySpringerOctober 5-9, 2015. 2015proceedings, part III 18</p>
<p>Real-time flying object detection with yolov8. D Reis, J Kupec, J Hong, A Daoudi, arXiv:2305.099722023arXiv preprint</p>
<p>Scene parsing through ade20k dataset. B Zhou, H Zhao, X Puig, S Fidler, A Barriuso, A Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>The cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 202210695</p>            </div>
        </div>

    </div>
</body>
</html>
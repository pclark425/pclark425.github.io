<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5372 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5372</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5372</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-83fb274ca565544743c4cdc7abe58db88a163ae2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/83fb274ca565544743c4cdc7abe58db88a163ae2" target="_blank">Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text, demonstrating that dual encoding can significantly improve the quality of the generated text.</p>
                <p><strong>Paper Abstract:</strong> Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5372.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5372.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DualENC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Encoding Model (DuALENC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-to-text generation architecture that simultaneously conditions an LSTM decoder on two encoders: (1) a graph encoder (R-GCN) that preserves RDF graph structure, and (2) a sequential plan encoder (LSTM/BiLSTM) that encodes a serialized content plan; designed to bridge the structural gap between graph input and linear text output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dual representation: Graph + Serialized Plan</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input RDF triples are converted into (A) a graph representation where both entities and predicates are nodes and edges are directed (s->p, p->s, o->p, p->o) with self-loops; this graph is encoded with a relational GCN (R-GCN). (B) A content plan (sequence) is produced by a neural planner (GCN-based sequential selection of predicates), completed into a linearized sequence of triples with special role tokens (<S>, <P>, <O>) and fed to a BiLSTM plan encoder. The decoder is an LSTM with attention and copy mechanism that at each step forms a context by concatenating the previous decoder state with attention-weighted context vectors from both encoders (processed through an MLP).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF knowledge graph (sets of RDF triples from DBpedia / WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves local and relational graph structure via R-GCN; plan serialization provides a linear ordering compatible with seq decoders to improve alignment; complementary representations (graph for structural fidelity, plan for linearization) reduce structural gap; supports copying and delexicalization; improves generalization to unseen domains compared with pure sequential linearizations. Limitation: plan serialization cannot retain all graph structure (some information loss), so both encoders are needed for highest fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG data-to-text generation (text realization from sets of up to 7 RDF triples); also plan-generation (ordering of triples) subtask; human evaluation on coverage, faithfulness, fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Text generation: BLEU (SEEN: 63.45, UnSEEN: 36.73, ALL: 51.42), METEOR (SEEN: 0.46, UnSEEN: 0.37, ALL: 0.41), TER (SEEN: 0.34, UnSEEN: 0.55, ALL: 0.44). Human eval: Coverage 94.5%, Faithfulness 91.8% (absolute scores reported). (All numeric values as reported in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms or matches prior systems (GCN-only, Seq2Seq baselines, SMT, pipeline methods) on the WebNLG benchmark in automatic metrics and human judgements. Plan-only encoder (PlanENC) obtains slightly higher BLEU on SEEN, but DualENC is preferred in human evaluation and obtains the best overall faithfulness/coverage; GCN-only suffers lower coverage, showing the benefit of combining plan and graph encodings. DualENC incorporates advantages of both encoders (structure + linear plan).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Serialized plans do not capture all graph relations or semantic-role structure, so the LSTM plan encoder alone can misassign roles (example: missing or incorrect subject for a predicate). Performance degrades as triple-set size increases (though DualENC still outperforms baselines). Requires generation of an accurate plan; planning errors significantly hurt generation quality. Some components (e.g., role delimiters) had limited observed impact; coordinating encoding and copy mechanisms and handling unseen domains require delexicalization and careful design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5372.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5372.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlanENC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan-only Encoder (PlanENC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generation model that uses a neural content planner to produce a serialized ordering of triples, encodes that sequence with an LSTM/BiLSTM, and decodes text from the plan-only context (attention + copy).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Serialized content plan (sequence of triples with role delimiters)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The planner selects an ordering of predicates (and associated triples) and the plan is realized by concatenating s,p,o spans with explicit role delimiters/tokens (<S>, <P>, <O>) and entity mention normalization (optionally joining multiword mentions). The plan sequence is encoded with a (bi)LSTM which provides the sole context for an LSTM decoder with attention and copy.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triple sets (knowledge graph fragments)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Linear and decoder-friendly; improves alignment between input facts and output tokens, reduces omission/repetition; more stable training (lower variance across runs). However, serialization loses some graph-level structural information and can lead to role ambiguity (LSTM may fail to capture which token is subject vs object without graph context).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG text generation (and planner training/evaluation using reference plans).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Text generation: BLEU (SEEN: 64.42, UnSEEN: 38.23, ALL: 52.78), METEOR (SEEN: 0.45, UnSEEN: 0.37, ALL: 0.41), TER (SEEN: 0.33, UnSEEN: 0.53, ALL: 0.42). Ablation: removing plan (i.e., random triple order) drops BLEU by 6.61 points on SEEN; removing copy drops BLEU by 2.78; removing mention normalization drops BLEU by 2.93; removing delimiters has little effect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>PlanENC outperforms GCN-only and prior published systems (GCN-EC, GRU/Transformer baselines, SMT/pipeline) on BLEU and TER; PlanENC yields higher BLEU than a single GCN encoder by ~8.52 BLEU (on SEEN). PlanENC sometimes slightly outperforms DualENC on BLEU but human evaluation shows DualENC produces better overall outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot fully capture graph relations and semantic roles—leads to errors like missing triples or wrong argument assignment; effectiveness depends on planner accuracy (planning errors significantly degrade generation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5372.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5372.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GCN-based Neural Planner (with R-GCN graph encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural content-planning module that uses a relational GCN to encode the RDF graph and then performs sequential selection of predicates to produce a content plan (sequence/order of triples); predicate representations include bits indicating visited status and last-visited predicate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-plan sequential selection using R-GCN</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph construction: treat entities and predicates as nodes (predicates given unique node IDs even if surface mention identical), add directed edges s->p, p->s, o->p, p->o and self-loops. Encode nodes with a relational GCN (relation-specific weight matrices W_r and biases b_r, iterative neighborhood aggregation). For planning, predicate node input features get two extra binary indicators (has-been-visited, last-visited). After R-GCN encoding, compute a softmax over remaining predicate embeddings (dot-product with pooled predicate representation) and pick the highest-probability predicate; repeat until all predicates visited. After ordering predicates, expand each into full triples with subject/object and serialize with role delimiters for the plan encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF graphs (sets of triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Encodes relational structure explicitly via R-GCN; supports permutation-invariant operations across triple-sets via graph encoding; indicators for visited/last-visited allow modeling of sequential decision process over a set; computationally efficient (fast at inference compared to prior transition-based methods). Good generalization to UnSEEN domains due to structural feature modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Plan generation (ordering of triples) on WebNLG; evaluated by exact-match accuracy against human-provided plans and BLEU-2 between predicted and reference plans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Planning results: Accuracy (SEEN: 0.63, UnSEEN: 0.61, ALL: 0.62). BLEU-2 (SEEN: 80.8, UnSEEN: 79.3, ALL: 80.1). Runtime: planner solved 4,928 instances in <10s in reported experiments; compared to Step-By-Step which required ~250s for one 7-triple instance (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>GCN planner substantially outperforms baselines on plan accuracy and BLEU-2: Random and Structure-Random, transformer/GRU sequence planners (which linearize graph), Step-By-Step and Step-By-Step II. Particularly stronger generalization: GRU/Transformer planner accuracies drop dramatically on UnSEEN (from ~0.56 to ~0.09/0.10), whereas GCN planner drops only slightly (~0.02).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Accuracy decreases as triple-set size grows (7-triple cases are hardest; GCN planner accuracy ~0.19 on 7-triple instances though still better than baselines). Planner only orders triples (predicates) — ordering of subjects/objects not modeled. The plan serialization still loses some structural information, so pairing the planner with a graph encoder (DualENC) is beneficial for final generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5372.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5372.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq-Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence with linearized graph input (GRU / Transformer baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approaches that serialize the graph/triples into a flat sequence and feed that sequence into a standard Seq2Seq encoder (GRU or Transformer) with attention and copy; do not explicitly encode graph relational structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / Serialization (flat sequence of triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert each triple-set into a single token sequence (e.g., by concatenating subject-predicate-object strings or other linear orders) and use a sequential encoder (GRU or Transformer) to produce context for decoder; this is standard Seq2Seq input representation for graph-to-text baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF triple sets (knowledge graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Straightforward to use with off-the-shelf Seq2Seq models; compact linear format. However, it discards explicit graph connectivity and relation types, making it harder to capture graph structural features and re-entrant relations; poorer generalization to unseen domains reflected in planning and generation drops.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>WebNLG text generation (compared as baselines in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported generation BLEU (from cited baselines): GRU (SEEN: 56.09, UnSEEN: 25.12, ALL: 42.73), Transformer (SEEN: 56.28, UnSEEN: 23.04, ALL: 42.41). As planners (when used to predict plans), GRU/Transformer planner accuracies in planning experiments were SEEN ~0.56, UnSEEN ~0.09-0.10 (large drop on UnSEEN).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Underperforms graph-aware encoders (R-GCN/GCN-EC) and plan-based approaches (PlanENC, DualENC) on BLEU and generalization to UnSEEN domains; demonstrates that linearization sacrifices structural information important for faithful generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Misses graph structure and re-entrancies, shows large performance drop on out-of-domain (UnSEEN) data, more errors in faithfulness/coverage compared to graph-aware or plan-aware methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modeling relational data with graph convolutional networks <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>step-by-step: Separating planning from realization in neural data-to-text generation <em>(Rating: 2)</em></li>
                <li>improving quality and efficiency in plan-based neural data-to-text generation <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures <em>(Rating: 2)</em></li>
                <li>GTR-LSTM: A triple encoder for sentence generation from rdf data <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5372",
    "paper_id": "paper-83fb274ca565544743c4cdc7abe58db88a163ae2",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "DualENC",
            "name_full": "Dual Encoding Model (DuALENC)",
            "brief_description": "A data-to-text generation architecture that simultaneously conditions an LSTM decoder on two encoders: (1) a graph encoder (R-GCN) that preserves RDF graph structure, and (2) a sequential plan encoder (LSTM/BiLSTM) that encodes a serialized content plan; designed to bridge the structural gap between graph input and linear text output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Dual representation: Graph + Serialized Plan",
            "representation_description": "Input RDF triples are converted into (A) a graph representation where both entities and predicates are nodes and edges are directed (s-&gt;p, p-&gt;s, o-&gt;p, p-&gt;o) with self-loops; this graph is encoded with a relational GCN (R-GCN). (B) A content plan (sequence) is produced by a neural planner (GCN-based sequential selection of predicates), completed into a linearized sequence of triples with special role tokens (&lt;S&gt;, &lt;P&gt;, &lt;O&gt;) and fed to a BiLSTM plan encoder. The decoder is an LSTM with attention and copy mechanism that at each step forms a context by concatenating the previous decoder state with attention-weighted context vectors from both encoders (processed through an MLP).",
            "graph_type": "RDF knowledge graph (sets of RDF triples from DBpedia / WebNLG)",
            "representation_properties": "Preserves local and relational graph structure via R-GCN; plan serialization provides a linear ordering compatible with seq decoders to improve alignment; complementary representations (graph for structural fidelity, plan for linearization) reduce structural gap; supports copying and delexicalization; improves generalization to unseen domains compared with pure sequential linearizations. Limitation: plan serialization cannot retain all graph structure (some information loss), so both encoders are needed for highest fidelity.",
            "evaluation_task": "WebNLG data-to-text generation (text realization from sets of up to 7 RDF triples); also plan-generation (ordering of triples) subtask; human evaluation on coverage, faithfulness, fluency.",
            "performance_metrics": "Text generation: BLEU (SEEN: 63.45, UnSEEN: 36.73, ALL: 51.42), METEOR (SEEN: 0.46, UnSEEN: 0.37, ALL: 0.41), TER (SEEN: 0.34, UnSEEN: 0.55, ALL: 0.44). Human eval: Coverage 94.5%, Faithfulness 91.8% (absolute scores reported). (All numeric values as reported in the paper.)",
            "comparison_to_other_representations": "Outperforms or matches prior systems (GCN-only, Seq2Seq baselines, SMT, pipeline methods) on the WebNLG benchmark in automatic metrics and human judgements. Plan-only encoder (PlanENC) obtains slightly higher BLEU on SEEN, but DualENC is preferred in human evaluation and obtains the best overall faithfulness/coverage; GCN-only suffers lower coverage, showing the benefit of combining plan and graph encodings. DualENC incorporates advantages of both encoders (structure + linear plan).",
            "limitations_or_challenges": "Serialized plans do not capture all graph relations or semantic-role structure, so the LSTM plan encoder alone can misassign roles (example: missing or incorrect subject for a predicate). Performance degrades as triple-set size increases (though DualENC still outperforms baselines). Requires generation of an accurate plan; planning errors significantly hurt generation quality. Some components (e.g., role delimiters) had limited observed impact; coordinating encoding and copy mechanisms and handling unseen domains require delexicalization and careful design.",
            "uuid": "e5372.0",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "PlanENC",
            "name_full": "Plan-only Encoder (PlanENC)",
            "brief_description": "A generation model that uses a neural content planner to produce a serialized ordering of triples, encodes that sequence with an LSTM/BiLSTM, and decodes text from the plan-only context (attention + copy).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Serialized content plan (sequence of triples with role delimiters)",
            "representation_description": "The planner selects an ordering of predicates (and associated triples) and the plan is realized by concatenating s,p,o spans with explicit role delimiters/tokens (&lt;S&gt;, &lt;P&gt;, &lt;O&gt;) and entity mention normalization (optionally joining multiword mentions). The plan sequence is encoded with a (bi)LSTM which provides the sole context for an LSTM decoder with attention and copy.",
            "graph_type": "RDF triple sets (knowledge graph fragments)",
            "representation_properties": "Linear and decoder-friendly; improves alignment between input facts and output tokens, reduces omission/repetition; more stable training (lower variance across runs). However, serialization loses some graph-level structural information and can lead to role ambiguity (LSTM may fail to capture which token is subject vs object without graph context).",
            "evaluation_task": "WebNLG text generation (and planner training/evaluation using reference plans).",
            "performance_metrics": "Text generation: BLEU (SEEN: 64.42, UnSEEN: 38.23, ALL: 52.78), METEOR (SEEN: 0.45, UnSEEN: 0.37, ALL: 0.41), TER (SEEN: 0.33, UnSEEN: 0.53, ALL: 0.42). Ablation: removing plan (i.e., random triple order) drops BLEU by 6.61 points on SEEN; removing copy drops BLEU by 2.78; removing mention normalization drops BLEU by 2.93; removing delimiters has little effect.",
            "comparison_to_other_representations": "PlanENC outperforms GCN-only and prior published systems (GCN-EC, GRU/Transformer baselines, SMT/pipeline) on BLEU and TER; PlanENC yields higher BLEU than a single GCN encoder by ~8.52 BLEU (on SEEN). PlanENC sometimes slightly outperforms DualENC on BLEU but human evaluation shows DualENC produces better overall outputs.",
            "limitations_or_challenges": "Cannot fully capture graph relations and semantic roles—leads to errors like missing triples or wrong argument assignment; effectiveness depends on planner accuracy (planning errors significantly degrade generation).",
            "uuid": "e5372.1",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "GCN-Planner",
            "name_full": "GCN-based Neural Planner (with R-GCN graph encoding)",
            "brief_description": "A neural content-planning module that uses a relational GCN to encode the RDF graph and then performs sequential selection of predicates to produce a content plan (sequence/order of triples); predicate representations include bits indicating visited status and last-visited predicate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph-to-plan sequential selection using R-GCN",
            "representation_description": "Graph construction: treat entities and predicates as nodes (predicates given unique node IDs even if surface mention identical), add directed edges s-&gt;p, p-&gt;s, o-&gt;p, p-&gt;o and self-loops. Encode nodes with a relational GCN (relation-specific weight matrices W_r and biases b_r, iterative neighborhood aggregation). For planning, predicate node input features get two extra binary indicators (has-been-visited, last-visited). After R-GCN encoding, compute a softmax over remaining predicate embeddings (dot-product with pooled predicate representation) and pick the highest-probability predicate; repeat until all predicates visited. After ordering predicates, expand each into full triples with subject/object and serialize with role delimiters for the plan encoder.",
            "graph_type": "RDF graphs (sets of triples)",
            "representation_properties": "Encodes relational structure explicitly via R-GCN; supports permutation-invariant operations across triple-sets via graph encoding; indicators for visited/last-visited allow modeling of sequential decision process over a set; computationally efficient (fast at inference compared to prior transition-based methods). Good generalization to UnSEEN domains due to structural feature modeling.",
            "evaluation_task": "Plan generation (ordering of triples) on WebNLG; evaluated by exact-match accuracy against human-provided plans and BLEU-2 between predicted and reference plans.",
            "performance_metrics": "Planning results: Accuracy (SEEN: 0.63, UnSEEN: 0.61, ALL: 0.62). BLEU-2 (SEEN: 80.8, UnSEEN: 79.3, ALL: 80.1). Runtime: planner solved 4,928 instances in &lt;10s in reported experiments; compared to Step-By-Step which required ~250s for one 7-triple instance (reported).",
            "comparison_to_other_representations": "GCN planner substantially outperforms baselines on plan accuracy and BLEU-2: Random and Structure-Random, transformer/GRU sequence planners (which linearize graph), Step-By-Step and Step-By-Step II. Particularly stronger generalization: GRU/Transformer planner accuracies drop dramatically on UnSEEN (from ~0.56 to ~0.09/0.10), whereas GCN planner drops only slightly (~0.02).",
            "limitations_or_challenges": "Accuracy decreases as triple-set size grows (7-triple cases are hardest; GCN planner accuracy ~0.19 on 7-triple instances though still better than baselines). Planner only orders triples (predicates) — ordering of subjects/objects not modeled. The plan serialization still loses some structural information, so pairing the planner with a graph encoder (DualENC) is beneficial for final generation.",
            "uuid": "e5372.2",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Seq2Seq-Linearization",
            "name_full": "Sequence-to-sequence with linearized graph input (GRU / Transformer baselines)",
            "brief_description": "Baseline approaches that serialize the graph/triples into a flat sequence and feed that sequence into a standard Seq2Seq encoder (GRU or Transformer) with attention and copy; do not explicitly encode graph relational structure.",
            "citation_title": "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
            "mention_or_use": "mention",
            "representation_name": "Linearization / Serialization (flat sequence of triples)",
            "representation_description": "Convert each triple-set into a single token sequence (e.g., by concatenating subject-predicate-object strings or other linear orders) and use a sequential encoder (GRU or Transformer) to produce context for decoder; this is standard Seq2Seq input representation for graph-to-text baselines.",
            "graph_type": "RDF triple sets (knowledge graphs)",
            "representation_properties": "Straightforward to use with off-the-shelf Seq2Seq models; compact linear format. However, it discards explicit graph connectivity and relation types, making it harder to capture graph structural features and re-entrant relations; poorer generalization to unseen domains reflected in planning and generation drops.",
            "evaluation_task": "WebNLG text generation (compared as baselines in paper).",
            "performance_metrics": "Reported generation BLEU (from cited baselines): GRU (SEEN: 56.09, UnSEEN: 25.12, ALL: 42.73), Transformer (SEEN: 56.28, UnSEEN: 23.04, ALL: 42.41). As planners (when used to predict plans), GRU/Transformer planner accuracies in planning experiments were SEEN ~0.56, UnSEEN ~0.09-0.10 (large drop on UnSEEN).",
            "comparison_to_other_representations": "Underperforms graph-aware encoders (R-GCN/GCN-EC) and plan-based approaches (PlanENC, DualENC) on BLEU and generalization to UnSEEN domains; demonstrates that linearization sacrifices structural information important for faithful generation.",
            "limitations_or_challenges": "Misses graph structure and re-entrancies, shows large performance drop on out-of-domain (UnSEEN) data, more errors in faithfulness/coverage compared to graph-aware or plan-aware methods.",
            "uuid": "e5372.3",
            "source_info": {
                "paper_title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modeling relational data with graph convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "step-by-step: Separating planning from realization in neural data-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "improving quality and efficiency in plan-based neural data-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2
        },
        {
            "paper_title": "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
            "rating": 2
        },
        {
            "paper_title": "GTR-LSTM: A triple encoder for sentence generation from rdf data",
            "rating": 1
        }
    ],
    "cost": 0.0158435,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation</h1>
<p>Chao Zhao ${ }^{\dagger}$, Marilyn Walker ${ }^{\ddagger}$ and Snigdha Chaturvedi ${ }^{\dagger}$<br>${ }^{\dagger}$ Department of Computer Science, University of North Carolina at Chapel Hill<br>${ }^{\ddagger}$ Natural Language and Dialog Systems Lab, University of California, Santa Cruz<br>{zhaochao, snigdha}@cs.unc.edu mawalker@ucsc.edu</p>
<h4>Abstract</h4>
<p>Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DuALENC, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.</p>
<h2>1 Introduction</h2>
<p>Data-to-text generation aims to create natural language text to describe the input data (Reiter and Dale, 2000). Here we focus on structured text input in a particular form such as a tree or a graph. Figure 1 shows an example where the input data is a mini knowledge graph, and the output text is its corresponding natural language description. Generating text from such data is helpful for many NLP tasks, such as question answering and dialogue (He et al., 2017; Liu et al., 2018; Moon et al., 2019).</p>
<p>During generation, the structure of the data as well as the content inside the structure jointly determine the generated text. For example, the direction of the edge "capital" in Figure 1 determines that "London is the capital of U.K." is an accurate description, but not vice versa. Current generation methods are based on sequence-to-sequence (Seq2Seq) encoder-decoder architecture (Sutskever et al., 2014), which requires the input data to be
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the WebNLG challenge: the source data is an RDF graph and the target output is a text description of the graph.
serialized as a sequence, resulting in a loss of structural information.</p>
<p>Recent research has shown the utility of incorporating structural information during generation. By replacing the sequential encoder with a structureaware graph encoder, such as a graph convolutional network (GCNs) (Kipf and Welling, 2017) or graph-state LSTMs (Song et al., 2018), the resulting graph-to-sequence (Graph2Seq) methods can encode the structural information of the input and thus outperform Seq2Seq models on certain tasks. However, these architectures broaden the structural gap between the encoder and decoder. That is, while the encoder receives the input data as a graph, the decoder has to create the output text as a linear chain structure.</p>
<p>This structural gap increases the difficulty of establishing alignments between source and target, which is believed to play a key role in text generation. For example, in machine translation, pre-reordering the source words into a word order that is close to that of the target sentence can yield significant improvements in translation quality (Bisazza and Federico, 2016). This suggests a need for an intermediate "planning" stage (Reiter</p>
<p>and Dale, 2000; Puduppully et al., 2019) to help with organizing the output.</p>
<p>In this work, we present a dual encoding model that is not only aware of the input graph structure but also incorporates a content planning stage. To encode the structural information in the input graph, we use a GCN based graph encoder. To narrow the ensuing structural gap, we use another GCN-based neural planner to create a sequential content plan of this graph, which is represented as a re-ordered sequence of its nodes. The plan is then encoded by an LSTM based sequential encoder. During generation, an LSTM based decoder simultaneously conditions on the two encoders, which helps it in capturing both the graph structure of the input data and the linear structure of the plan. We expect such a dual encoding (DualEnc) structure can integrate the advantages of both graph and sequential encoders while narrowing the structural gap present in single-encoder methods.</p>
<p>We evaluate the proposed planning and generation models on the WebNLG dataset (Colin et al., 2016; Gardent et al., 2017) - a widely used benchmark for data-to-text generation. Experimental results show that our neural planner achieves a $15 \%$ absolute improvement on accuracy compared to the previous best planning method. Furthermore, DualEnc significantly outperforms the previous start-of-the-art on the generation task. The human evaluation confirms that the texts generated by our model are preferred over strong baselines.</p>
<p>The contributions of this paper are three-fold:</p>
<ul>
<li>We propose a dual encoding method to narrow the structural gap between data encoder and text decoder for data-to-text generation;</li>
<li>We propose a neural planner, which is more efficient and effective than previous methods;</li>
<li>Experiments show that our method outperforms all baselines on a variety of measures.</li>
</ul>
<h2>2 Related Work</h2>
<p>This work is inspired by two lines of research: Seq2Seq generation and Graph2Seq generation.</p>
<h3>2.1 Seq2Seq Generation</h3>
<p>Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004). More recent methods use Seq2Seq architecture (Sutskever et al., 2014) to combine planning and realization into an end-toend network and have achieved the state-of-the-art
on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; Juraska et al., 2018; Reed et al., 2018). Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks.</p>
<p>Recent work has shown that neural models can also benefit from an explicit planning step to alleviate the above-mentioned problems. The input of these planners ranges from unstructured keyphrases (Hua and Wang, 2019) to structured tables (Puduppully et al., 2019) and graphs (Ferreira et al., 2019; Moryossef et al., 2019a). Our work also focuses on planning from graph data. Compared with previous methods, we show that our neural planning method is more feasible and accurate. More importantly, rather than serializing the planning and realization stages in a pipeline, our dual encoding method simultaneously captures information from the original data and the corresponding plan.</p>
<h3>2.2 Graph2Seq Generation</h3>
<p>Graph neural networks (GNN) (Scarselli et al., 2009) aim to learn a latent state representation for each node in a graph by aggregating local information from its neighbors and the connected edges. Previous work has explored different ways of aggregating this local information, such as in GCNs (Kipf and Welling, 2017), gated graph neural networks (GGNNs) (Li et al., 2016), and Graph attention networks (GANs) (Veličković et al., 2018)</p>
<p>Several works have applied GNNs instead of Seq2Seq models for text generation (Beck et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Guo et al., 2019; Li et al., 2019), and some of them outperform Seq2Seq models. However, Damonte and Cohen (2019) use both types of encoders and show that GCN can help LSTM capture reentrant structures and long-range dependencies, albeit on a different problem than ours. Our method also uses the two types of encoders but instead of using one to assist the other, it combines them simultaneously to capture their complementary effects.</p>
<h2>3 Problem Statement</h2>
<p>In this work we focus on text generation from RDF data. ${ }^{1}$ The input for this task is a set of RDF triples, where each triple $(s, p, o)$ contains a subject, a predicate, and an object. For example, ("U.K.", "cap-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The architecture of the proposed DUALENC model. The input triples are converted as a graph and then fed to two GCN encoders for plan and text generation (Planner and Graph Encoder, top center). The plan is then encoded by an LSTM network (Plan Encoder, bottom center). Finally an LSTM decoder combines the hidden states from both the encoders to generate the text (Text Decoder, middle right).
ital", "London") is a RDF triple. The output is a natural language text with one or more sentences to describe the facts represented by this graph. Figure 1 shows an example of this task.</p>
<h2>4 Dual Encoding Model</h2>
<p>For a given input RDF graph, the aim of our method is not only to capture its structural information, but also to facilitate the information alignment between the input and output. The first goal can be achieved by employing a GCN encoder. To achieve the second goal, we first serialize and re-order the nodes of the graph as an intermediate plan using another GCN, and then feed the plan into an LSTM encoder. Finally, an LSTM decoder is used to generate the output by incorporating the context representations of both encoders. Notice that the graph and the plan are dual representations of the same input data. We encode them with two independent encoders, which can provide complementary information for decoding. The architecture of our dual encoding method is shown in Figure 2. We describe the two encoders and the decoder in the following three subsections.</p>
<h3>4.1 Graph Representation and Encoding</h3>
<p>To make it easier for GCNs to encode information from both entities and predicates, we reconstruct the input graph by regarding both entities and predicates as nodes, which is different from Figure 1.</p>
<p>Formally, for each RDF triple $(s, p, o)$, we regard the $s, p$, and $o$ as three kinds of nodes. $s$ and $o$ are identified by their entity mentions, and $p$ is identified by a unique ID. That is, two entities from different triples that have the same mentions will
be regarded as the same node. However, since we want to use predicates to distinguish between different triples, two predicates with the same mentions will be regarded as separate nodes. ${ }^{2}$
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The graph obtained from an RDF triple.</p>
<p>We use the same edge structure as Beck et al. (2018). As Figure 3 shows, a triple contains four directed edges to connect its nodes: $s \rightarrow p, p \rightarrow s$, $o \rightarrow p$, and $p \rightarrow o$. These edges help in information exchange between arbitrary neighbor pairs. There is also a special self-loop edge $n \rightarrow n$ for each node $n$ to enable information flow between adjacent iterations during feature aggregation.</p>
<p>After building the graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ from the RDF data, we use a relational GCN (R-GCN) (Schlichtkrull et al., 2018) to encode the graph and learn a state representation $\mathbf{h}_{v} \in \mathbb{R}^{d}$ for each node $v \in \mathcal{V}$ using the following iterative method:</p>
<p>$$
\mathbf{h}<em _in="\in" _mathcal_R="\mathcal{R" r="r">{v}^{t}=\rho\left(\sum</em>}} \sum_{u \in \mathcal{N<em r="r" v_="v,">{v}^{r}} \frac{1}{c</em>}} \mathbf{W<em u="u">{r} \mathbf{h}</em>\right)
$$}^{(t-1)}+\mathbf{b}_{r</p>
<p>where $\mathbf{h}<em v="v">{v}^{0}=\mathbf{x}</em>}$ is the input embedding of the node $v$, and $\mathbf{h<em v="v">{v}^{t}$ is its hidden state at time-step $t$. We use the average embedding of the node mentions as $\mathbf{x}</em>$ is the set of in-neighbors of node $v$ with the edge} . \mathcal{R}$ is the set of all possible edge types, and $\mathcal{N}_{v}^{r</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The sequential decision-making process of the planning stage.
type as $r . \mathbf{W}<em r="r">{r}$ and $\mathbf{b}</em>\right|$ is a normalization term and $\rho()$ is an activation function.}$ are parameters for each edge type, which allow transformations of message to become relational-specific. $c_{v, r}=1 /\left|\mathcal{N}_{v}^{r</p>
<h3>4.2 Planning Creation and Encoding</h3>
<p>In the planning stage, we determine the content plan or order of triples (identified by their predicates) for text realization. For example, the content plan for the text in Figure 1 is: "assembly $\rightarrow$ capital $\rightarrow$ successor $\rightarrow$ manufacturer ". ${ }^{3}$</p>
<p>Learning a plan can be naturally regarded as a sequential decision-making process. That is, given a set of triples, we first determine which triple to mention/visit first, and then select the second triple from the remaining triples that have not been visited so far. This process continues until all the triples have been visited. During each decision step, the selection of the next triple can be regarded as a classification task, where the output space is all the remaining unvisited triples.</p>
<p>Figure 4 shows how our model implements this process. We first utilize the GCN encoder described in Section 4.1 to get the state representation of each node. However, while obtaining a predicate's representation, we concatenate two extra bits to the input feature $\mathbf{X}^{t}$. One is to indicate whether or not the predicate has been visited, the other to indicate the last predicate that has been visited. After the encoding, we get the final hidden state $\mathbf{h}<em i="i">{r</em>}}=\mathbf{h<em i="i">{r</em>$ as its representation, and calculate its probability of being selected as}}^{\langle T\rangle}$ for each predicate $r_{i} \in \mathcal{R</p>
<p>$$
P\left(r_{i}\right)=\operatorname{softmax}\left(\mathbf{h}<em i="i">{r</em>\right)
$$}}^{T} \mathbf{W} \overline{\mathbf{h}}_{\mathcal{R}</p>
<p>where $\overline{\mathbf{h}}_{\mathcal{R}}$ is the average pooling of all the predicate embeddings. For obtaining a plan, we select the predicate with the highest probability, append it onto the plan sequence, and then repeat the above process until all the predicates have been visited.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>After determining an order of input predicates, we complete the plan's triples by adding the corresponding subjects and objects. To better help the plan encoder (described below) capture the semantic roles of each entity and predicate, we add special tokens before Subjects, Predicates, and Objects as delimiters. For example, the plan of the example in Figure 1 will be:</p>
<div class="codehilite"><pre><span></span><code>&lt;S&gt; Aston Martin V8 &lt;P&gt; assembly &lt;O&gt; United King-
dom &lt;S&gt; United Kingdom &lt;P&gt; capital &lt;O&gt; London
&lt;S&gt; Aston Martin V8 &lt;P&gt; successor &lt;O&gt; Aston Mar-
tin Virage &lt;S&gt; Aston Martin Virage &lt;P&gt; manufacturer
&lt;O&gt; Aston Martin
</code></pre></div>

<p>Finally, we use an LSTM to encode the plan obtained above. We choose LSTM because it excels at capturing sequential information.</p>
<h3>4.3 Decoding</h3>
<p>During decoding, we adopt an LSTM-based decoder with an attention and copy mechanism. Since we have two representations of the input triple-set: the original graph and the serialized plan, we adopt two strategies for inputting context to the decoder.</p>
<p>The first strategy is to only use hidden states of the plan encoder as context. We refer to this strategy as PlanENc.</p>
<p>While the serialized plan may contain some structural information, it cannot preserve all the information of the original graph. We therefore propose a second strategy, DuALENC, to incorporate the information from both the graph and the plan. More concretely, when calculating the context state $\mathbf{m}<em t-1="t-1">{t}$ of the LSTM decoder at time step $t$, we concatenate the previous hidden state $\mathbf{z}</em>}$ and the two context vectors $\mathbf{c<em t="t">{t}^{1}$ and $\mathbf{c}</em>$ as:}^{2}$, and then update the current hidden state, $\mathbf{z}_{t</p>
<p>$$
\begin{array}{r}
\mathbf{m}<em t-1="t-1">{t}=\operatorname{MLP}\left(\left[\mathbf{z}</em>} ; \mathbf{c<em t="t">{t}^{1} ; \mathbf{c}</em>\right]\right) \
\mathbf{z}}^{2<em t-1="t-1">{t}=\operatorname{LSTM}\left(\mathbf{z}</em>},\left[\left(\mathbf{y<em t="t">{t-1} ; \mathbf{m}</em>\right]\right)\right.
\end{array}
$$</p>
<p>where $\mathbf{c}<em t="t">{t}^{1}$ and $\mathbf{c}</em>}^{2}$ are the attention-based weighted sum of the context memories from GCN and RNN encoders, respectively, and $\mathbf{y<em 0="0">{t-1}$ is the embedding of the previously generated token. The initial hidden state $\mathbf{z}</em>$ of LSTM as the context representation. For the graph encoder, we use an average of all the hidden states following a two-layer perceptron to produce the final state.}$ is the summation of the final states from the two encoders. For the plan encoder, we use the final state $\mathbf{H}^{T</p>
<h2>5 Experiments</h2>
<p>We conduct experiments to evaluate our Planner (Section 5.2) and the overall generation system (Section 5.3). ${ }^{4}$</p>
<h3>5.1 Dataset</h3>
<p>We conduct experiments on the WebNLG dataset (Gardent et al., 2017; Castro Ferreira et al., 2018) used in the WebNLG challenge. ${ }^{5}$ For each instance, the input is a set of up to 7 RDF triples from DBPedia, and the output is their text descriptions. Each triple-set is paired with a set of (up to three) humangenerated reference texts. Each reference is also paired with the order of triples it realized. We use them to train and evaluate our Planner. Overall, the dataset contains 9,674 unique triple-sets and 25,298 text references, and is divided into training, development, and test set. The test set contains two subsets, the SEEN part where the instances belong to one of the nine domains that are seen in the training and development set (such as Astronaut and Food), and the UnSEEN part where the instances are from the other five unseen domains. The UnSEEN part is designed to evaluate models' generalizability to out-of-domain instances.</p>
<h3>5.2 Experiments on Plan Generation</h3>
<p>As previous work suggests, planning plays a crucial role in text generation. We, therefore, first investigate the performance of our planner.</p>
<h3>5.2.1 Setup</h3>
<p>During the graph encoding, we initialize the node embeddings with 100-dimensional random vectors. Our GCN model has two layers, with the hidden size of each layer as 100 . The activation function is ReLU (Nair and Hinton, 2010). We optimize the training objective using Adam (Kingma and Ba, 2015) with a learning rate of 0.001 and an early stopping on the development set. The batch size is 100. We compare our results with the following six baseline planners:</p>
<ul>
<li>Random: returns a random permutation of the input triples as a plan;</li>
<li>Structure-Random: returns a random traversal over the input graph. We report the highest score among three random strategies: random walk, random BFS, and random DFS;</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- Step-By-Step (Moryossef et al., 2019a): a transition-based statistical ranking method;
- Step-By-Step II (Moryossef et al., 2019b): a DFS-based method with a neural controller;
- GRU \&amp; Transformer (Ferreira et al., 2019): two neural Seq2Seq methods with attention;
We report the performance on three test sets: SEEN, UnSEEN, and All (SEEN \&amp; UnSEEN). We remove all one-triple instances for planner's evaluation since the planning for these instances is trivial. Results are evaluated with accuracy and BLEU-n (Papineni et al., 2002). For accuracy, we regard a plan as correct only if it exactly matches one of the human-generated plans. BLEU-n is more forgiving than accuracy. It is also adopted in Yao et al. (2019) for plan evaluation. Here we choose $n=2$.</p>
<h3>5.2.2 Results</h3>
<p>Table 1 shows results of the planning experiments. Our GCN method significantly outperforms all the baselines (approximate randomization (Noreen, 1989; Chinchor, 1992), $p&lt;0.05$ ) by a large margin on all the test sets and both measures, indicating the effectiveness of our planner. The most competitive baseline on All and UnSEEN sets is Step-By-Step, but our method is more time-efficient. For example, Step-By-Step needs 250 seconds to solve one 7-triple instance, but our method solves all 4928 instances in less than 10 seconds. For the SEEN set, the most competitive models are GRU and Transformer. However, while their accuracies drop by 0.46 on UnSEEN test set, our method drops only slightly by 0.02 , indicating our method's better generalization power.</p>
<p>We believe that this superior generalization capacity comes from the modeling of the graph structure. While the surface forms of triples in UnSEEN set do not overlap with those in the training data, the graph-level structural features are still shared, making it a key factor for generalization. GRU and Transformer linearize the graph as a sequential input, making them miss the structural information and resulting in poorer generalization capacity. Step-By-Step II also considers graph structure, but our model achieves better performance because we use GCN to encode the node representation, which can aggregate richer information from both the graph structure and the surface information.</p>
<p>We also investigated the effect of the graph size on the plan quality. In Figure 5, we separate the All test set into six subsets according to the size of input triple-sets, to reflect the model's capacity</p>
<table>
<thead>
<tr>
<th></th>
<th>Accuracy</th>
<th></th>
<th></th>
<th>BLEU-2</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>SEEN</td>
<td>UnSEEN</td>
<td>ALL</td>
<td>SEEN</td>
<td>UNSEEN</td>
<td>ALL</td>
</tr>
<tr>
<td>Random</td>
<td>0.28</td>
<td>0.34</td>
<td>0.31</td>
<td>54.1</td>
<td>62.1</td>
<td>57.9</td>
</tr>
<tr>
<td>Structure-random</td>
<td>0.32</td>
<td>0.38</td>
<td>0.34</td>
<td>56.6</td>
<td>62.9</td>
<td>59.5</td>
</tr>
<tr>
<td>Transformer (Ferreira et al., 2019)</td>
<td>0.56</td>
<td>0.09</td>
<td>0.34</td>
<td>74.3</td>
<td>20.9</td>
<td>49.3</td>
</tr>
<tr>
<td>GRU (Ferreira et al., 2019)</td>
<td>0.56</td>
<td>0.10</td>
<td>0.35</td>
<td>75.8</td>
<td>25.4</td>
<td>52.2</td>
</tr>
<tr>
<td>Step-By-Step II (Moryossef et al., 2019b)</td>
<td>0.45</td>
<td>0.44</td>
<td>0.44</td>
<td>67.7</td>
<td>67.3</td>
<td>67.5</td>
</tr>
<tr>
<td>Step-By-Step (Moryossef et al., 2019a)</td>
<td>0.49</td>
<td>0.44</td>
<td>0.47</td>
<td>73.2</td>
<td>68.0</td>
<td>70.8</td>
</tr>
<tr>
<td>GCN</td>
<td>$\mathbf{0 . 6 3}$</td>
<td>$\mathbf{0 . 6 1}$</td>
<td>$\mathbf{0 . 6 2}$</td>
<td>$\mathbf{8 0 . 8}$</td>
<td>$\mathbf{7 9 . 3}$</td>
<td>$\mathbf{8 0 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Planning results of three test sets evaluated by accuracy and BLEU-2.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Fine-grained planning results for the ALL test set. Our method outperforms all the baselines regardless of the triple size.
at a fine-grained level. Fewer input triples make the planning task easier, while the 7-triple case is the most difficult one. The accuracy of seven out of eight baselines drops to around 0 in this case, while our method achieves an accuracy of 0.19 . Besides this, our method consistently outperforms all the baselines for all the triple-set sizes.</p>
<h3>5.3 Experiments on Text Generation</h3>
<p>This section investigates the ability of our models to improve the generation quality.</p>
<h3>5.3.1 Setup</h3>
<p>We implement the generator based on the OpenNMT toolkit. ${ }^{6}$ For the graph encoder, we use a similar setting as above. Since the generation task is more complicated than planning, we increase the dimension of the input and the hidden states to 256 . The plan encoder is a 2-layer bidirectional LSTM with the same dimension setting of the GCN to ease the information fusion. During encoding, for UnSEEN test set, we adopt delexicalization (Gardent et al., 2017) to enhance the model's generalizability to unseen domains.</p>
<p>We use Adam with a batch size of 64 . The initial learning rate is set to 0.001 and is decayed with a rate of 0.7 after the eighth epoch. We continue the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>training until the perplexity of the development set does not decrease. We also apply dropout on the decoding output layer with a rate of 0.3 .</p>
<p>The quality of the generated text (as well as those of the baselines) is evaluated through a variety of automatic measures, such as BLEU, METEOR, and TER, which are strictly the same as those applied in the official challenge. ${ }^{7}$ Following Marcheggiani and Perez-Beltrachini (2018), we report averaged performances over ten runs of the models.</p>
<p>We compare our method with the top systems of the WebNLG challenge and published state-of-theart systems. The WebNLG systems are:</p>
<ul>
<li>ADAPT: a neural system with sub-word representations to deal with rare words and sparsity.</li>
<li>TILB-SMT: a statistical machine translation method using Moses and delexicalization.</li>
<li>MELBOURNE: a Seq2Seq model with enriched delexicalization from DBPedia.
The published research models are:</li>
<li>GTR-LSTM (Trisedya et al., 2018): a graphbased triple encoder;</li>
<li>GCN-EC (Marcheggiani and PerezBeltrachini, 2018): a GCN-based triple encoder with glove embedding and copy;</li>
<li>GRU \&amp; Transformer (Ferreira et al., 2019): two pipeline methods with 5 sequential steps and GRU or Transformer as the encoder;</li>
<li>STEP-BY-STEP (Moryossef et al., 2019a): a pipeline method that generates the text from plans with OpenNMT and a copy mechanism.</li>
</ul>
<h3>5.3.2 Qualitative Results</h3>
<p>Table 2 shows the results of the automatic evaluation on the generation task. Our PlanENC achieves the best performance on BLEU and TER, while DuALENC performs best under METEOR. Both PlanENC and DuALENC significantly out-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU $(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">METEOR $(\uparrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TER $(\downarrow)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEEN</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">SEEN</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">SEEN</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">All</td>
</tr>
<tr>
<td style="text-align: center;">TILB-SMT</td>
<td style="text-align: center;">54.29</td>
<td style="text-align: center;">29.88</td>
<td style="text-align: center;">44.28</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;">ADAPT</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">10.53</td>
<td style="text-align: center;">31.06</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">MELBOURNE</td>
<td style="text-align: center;">54.52</td>
<td style="text-align: center;">33.27</td>
<td style="text-align: center;">45.13</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">GTR-LSTM (2018)</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">29.20</td>
<td style="text-align: center;">37.10</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.55</td>
</tr>
<tr>
<td style="text-align: center;">GCN-EC (2018)</td>
<td style="text-align: center;">55.90</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GRU (2019)</td>
<td style="text-align: center;">56.09</td>
<td style="text-align: center;">25.12</td>
<td style="text-align: center;">42.73</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">Transformer (2019)</td>
<td style="text-align: center;">56.28</td>
<td style="text-align: center;">23.04</td>
<td style="text-align: center;">42.41</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">Step-By-Step (2019a)</td>
<td style="text-align: center;">53.30</td>
<td style="text-align: center;">34.41</td>
<td style="text-align: center;">47.24</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">PlanENC</td>
<td style="text-align: center;">64.42</td>
<td style="text-align: center;">38.23</td>
<td style="text-align: center;">52.78</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">DualENC</td>
<td style="text-align: center;">63.45</td>
<td style="text-align: center;">36.73</td>
<td style="text-align: center;">51.42</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.44</td>
</tr>
</tbody>
</table>
<p>Table 2: Generation results evaluated by BLEU, METEOR, and TER. We compare our methods with different generation systems (SMT, Sequential NMT, Graph NMT, Pipeline). Both of our methods outperform all the baselines on all three measures. We highlight both results if there is no significant difference.
perform the previous state-of-the-art (bootstrapping (Koehn and Monz, 2006), $p&lt;0.05$ ). For the SEEN part, while no existing published work performed better than ADAPT, our PlanENC achieves a 3.83 performance gain on BLEU. It also outperforms the single GCN encoder by 8.52 BLEU , which confirms the advantage of the planning stage for bridging the structural gap between the encoder and decoder. For the UnSEEN part, PlanENC and DualENC improve BLEU by 3.82 and 2.32 compared with the previous state-of-the-art. While it is difficult to distinguish the performance of DuALENC and PlanENC by automatic measures, our human experiments (see Section 5.3.4) show that dual encoding generates better text compared with PlanENC.</p>
<p>When comparing with the pipeline methods, one difference from the data perspective is how to obtain the plans of each instance to train the planner. While Step-By-Step uses heuristic string matching to extract plans from the referenced sentences, other methods (GRU and transformer), as well as ours, use plans provided in the enriched WebNLG dataset (Castro Ferreira et al., 2018). However, Step-By-Step reported worse BLEU results on these plans.</p>
<h3>5.3.3 Ablation Study</h3>
<p>To further analyze what factors contribute to the performance gain, we conduct an ablation study by removing the following components:</p>
<ul>
<li>Copy mechanism: The text is generated without copying from the source;</li>
<li>Triple planning: The input triples are shuffled before feeding into RNN, but the $(s, p, o)$</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">BLEU $(\uparrow)$</th>
<th style="text-align: center;">METEOR $(\uparrow)$</th>
<th style="text-align: center;">TER $(\downarrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PlanENC</td>
<td style="text-align: center;">$\mathbf{6 4 . 4 2} \pm 0.17$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5} \pm 0.00$</td>
<td style="text-align: center;">$\mathbf{0 . 3 3} \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">-plan</td>
<td style="text-align: center;">$57.81 \pm 0.82$</td>
<td style="text-align: center;">$0.40 \pm 0.00$</td>
<td style="text-align: center;">$0.40 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">-copy</td>
<td style="text-align: center;">$61.64 \pm 0.53$</td>
<td style="text-align: center;">$0.43 \pm 0.01$</td>
<td style="text-align: center;">$0.36 \pm 0.01$</td>
</tr>
<tr>
<td style="text-align: left;">-mention</td>
<td style="text-align: center;">$61.49 \pm 0.35$</td>
<td style="text-align: center;">$0.43 \pm 0.00$</td>
<td style="text-align: center;">$0.36 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">-delimiter</td>
<td style="text-align: center;">$63.26 \pm 0.33$</td>
<td style="text-align: center;">$0.44 \pm 0.00$</td>
<td style="text-align: center;">$0.34 \pm 0.00$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of the ablation study.
inside a triple are not shuffled.</p>
<ul>
<li>Entity mentions: We join the words in a node mention with underlines (e.g., Aston-Martin instead of Aston Martin).</li>
<li>Plan delimiter: We concatenate the $(s, p, o)$ without separating them with role delimiters.
We conduct the ablation study on the SEEN testset using our PlanENC. Table 3 shows the average performance and standard deviations. Compared with PlanENC, replacing plans with a random sequence of triples hurts the BLEU score by 6.61 points, indicating that the accuracy of planning is essential for the quality of generation. Our planning also makes the model more stable to random seeds (by decreasing the standard deviation from 0.82 to 0.17 ). Removing the copy mechanism also decreases the BLEU score by 2.78 points. It demonstrates the effectiveness of copying words from the source triples rather than generating them from the vocabulary set. Removing the mention information, decreases the BLEU score by 2.93. It reflects two benefits of word mentions: to alleviate data sparsity and to coordinate with the copy mechanism. However, removing delimiters does not affect the BLEU much. Intuitively, we expected the delimiters to</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Absolute(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Pairwise(\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CVGE FAITH</td>
<td style="text-align: center;">CVGE FAITH</td>
<td style="text-align: center;">FLCY</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MELBOURNE</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">-35.0</td>
<td style="text-align: center;">-42.5</td>
<td style="text-align: center;">-38.8</td>
</tr>
<tr>
<td style="text-align: left;">STEP</td>
<td style="text-align: center;">$\mathbf{9 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 . 0}$</td>
<td style="text-align: center;">$\mathbf{- 3 . 7}$</td>
<td style="text-align: center;">-45.0</td>
</tr>
<tr>
<td style="text-align: left;">E2E-TRANS</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">-21.2</td>
<td style="text-align: center;">-32.5</td>
<td style="text-align: center;">-21.2</td>
</tr>
<tr>
<td style="text-align: left;">GCN</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">-48.7</td>
<td style="text-align: center;">-50.0</td>
<td style="text-align: center;">-26.3</td>
</tr>
<tr>
<td style="text-align: left;">PlanENC</td>
<td style="text-align: center;">$\mathbf{9 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{- 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{- 1 2 . 5}$</td>
<td style="text-align: center;">$\mathbf{- 7 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">DUALENC</td>
<td style="text-align: center;">$\mathbf{9 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 8}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of human evaluation. DuALENC outperforms most of the baselines on all measures.
help the LSTM capture the boundaries and semantic roles of each node, but the ablation study does not support it. We provide an example in Table 5 to show that the LSTM indeed has trouble learning such semantic roles.</p>
<h3>5.3.4 Human Evaluation</h3>
<p>Automatic measures are based on lexical similarities and are not good measures of text quality in general. We therefore further conduct a human evaluation on Amazon Mechanical Turk to better access the quality of the generated texts. We evaluate the results for MELBOURNE, Step-By-Step, Transformer, GCN, as well as our PlanENC and DuALENC. We randomly select 80 test instances (440 triples in total) with the size of tripleset between 4 to 7 , since they are more challenging than those with fewer triples. Then we evaluate the generation quality of each system with the following three measures:</p>
<ul>
<li>Coverage: the percentage of triples that are covered by the generated text (all $<s, p, o>$ values in the triples are realized);</li>
<li>Faithfulness: the percentage of triples that are faithfully described by the text (the text correctly expresses the predicate and also the subject and object as its arguments. No substitutions or hallucinations);</li>
<li>Fluency: a measure of the fluency or naturalness of the generated text.
For coverage and faithfulness, workers are asked to check each triple of an instance, and judge whether the triple is covered and faithfully described by the generated text. For fluency, we ask another group of workers to compare between two outputs of the same instance and identify which one is more fluent. Table 5 shows examples where these qualities are compromised.</li>
</ul>
<p>In Table 4, we report the absolute scores of
coverage and faithfulness, which range from 0 to $100 \%$. We also provide pairwise scores of all three measures by comparing the outputs of DuALENC with each of the other five systems. We report the percentage of instances that were judged to be worse/better/same than those of DuALENC, yielding a score ranging from $-100 \%$ (unanimously worse) to $100 \%$ (unanimously better). For example, MELBOURNE performs better/worse/same than DuALENC for $10 \% / 45 \% / 45 \%$ of the instances, yielding a pairwise score as $10 \%-45 \%=-0.35 \%$. We also report an overall pairwise score combining all three measures. For each instance, the overall score of one output is higher than the other iff it outperforms the other on at least one of the three measures and has a better or equal vote on the other two.</p>
<p>Our PlanENC and DuALENC outperform most of the baselines on all of the measures by a large margin (approximate randomization, $p&lt;0.05$. ), which is consistent with the automatic results. The only exception is Step-By-Step, which has high Coverage and Faithfulness (not significant). It first separates the input triples into smaller subsets and then realizes them separately. This greatly reduces the difficulty of long-term generation but at the expense of Fluency (worst among all the baselines). GCN does not perform well on Coverage, which demonstrates that the structural gap between encoding and decoding indeed makes generation more difficult. However, it has the smallest difference between Coverage and Faithfulness among all the baselines, indicating that the fidelity of generation can benefit from the encoding of graph-level structural information. By combining GCN and PLANENC, our DuALENC incorporates the advantages of both encoders while ameliorating their weaknesses, and therefore achieves the best OVERALL performance on human evaluation.</p>
<h3>5.4 Qualitative Analysis</h3>
<p>Table 5 shows examples of generated texts by various systems for an input of six triples. Colored fonts represent missing, unfaithful, and unfluent information. For example, PlanENC misses "Buzz Aldrin" and also wrongly expresses the subject of "retirement" as "Frank Borman", indicating that LSTM is less powerful at capturing the semantic roles of entities. This disadvantage can be well complemented by GCN, which is designed to capture the graph structure and the relations between entities. Hence, by incorporating information from</p>
<p>| Tripleset | (William Anders | birthPlace | British Hong Kong), (William Anders | was a crew member of | Apollo 8), <br> (Apollo 8 | crewMembers | Frank Borman), (Apollo 8 | backup pilot | Buzz Aldrin), (Apollo 8 | operator | <br> NASA), (William Anders | dateOfRetirement | 1969-09-01) |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |</p>
<p>Table 5: Sample texts generated by our methods and baselines, compared with a human-provided reference. We highlight in different color the [missing], unfaithful, and unfluent parts of each text. Only the results of our DuALENC correctly mention all the input triples.
both GCN and LSTM, DuALENC correctly expresses the subject argument of "retirement".</p>
<h2>6 Conclusion</h2>
<p>This paper proposes DuALENC, a dual encoding method to bridge the structural gap between encoder and decoder for data-to-text generation. We use GCN encoders to capture the structural information of the data, which is essential for accurate planning and faithful generation. We also introduce an intermediate content planning stage to serialize the data and then encode it with an LSTM network. This serialized plan is more compatible with the output sequence, making the information alignment between the input and output easier. Experiments on WebNLG dataset demonstrate the effectiveness of our planner and generator by outperforming the previous state-of-the-art by a large margin. Future work will validate the effectiveness of this method on more varied data-to-text generation tasks.</p>
<h2>References</h2>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283.</p>
<p>Arianna Bisazza and Marcello Federico. 2016. A survey of word reordering in statistical machine translation: Computational models and language phenomena. Computational Linguistics, 42(2):163-205.</p>
<p>Thiago Castro Ferreira, Diego Moussallem, Sander Wubben, and Emiel Krahmer. 2018. Enriching the
webnlg corpus. In Proceedings of the 11th International Conference on Natural Language Generation, INLG'18, Tilburg, The Netherlands. Association for Computational Linguistics.</p>
<p>Nancy Chinchor. 1992. The statistical significance of the muc-4 results. In Proceedings of the 4th conference on Message understanding, pages 30-50. Association for Computational Linguistics.</p>
<p>Emilie Colin, Claire Gardent, Yassine M’rabet, Shashi Narayan, and Laura Perez-Beltrachini. 2016. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th International Natural Language Generation conference, pages 163167.</p>
<p>Marco Damonte and Shay B Cohen. 2019. Structural neural encoders for amr-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649-3658.</p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 552-562.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133.</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computational Linguistics, 7:297-312.</p>
<p>Shizhu He, Cao Liu, Kang Liu, and Jun Zhao. 2017. Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 199208.</p>
<p>Xinyu Hua and Lu Wang. 2019. Sentence-level content planning and style specification for neural text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 591-602.</p>
<p>Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, and Marilyn Walker. 2018. A deep ensemble model with slot alignment for sequence-to-sequence natural language generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages $152-162$.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</p>
<p>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings on the Workshop on Statistical Machine Translation, pages 102121 .</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213.</p>
<p>Wei Li, Jingjing Xu, Yancheng He, Shengli Yan, Yunfang Wu, et al. 2019. Coherent comment generation for chinese articles with a graph-to-sequence model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4843-4852.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated graph sequence neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Shuman Liu, Hongshen Chen, Zhaochun Ren, Yang Feng, Qun Liu, and Dawei Yin. 2018. Knowledge
diffusion for neural dialogue generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1489-1498.</p>
<p>Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9.</p>
<p>Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 845-854.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019a. step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019b. improving quality and efficiency in planbased neural data-to-text generation. In Proceedings of the 12th International Conference on Natural Language Generation, pages 377-382.</p>
<p>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807-814.</p>
<p>Eric W Noreen. 1989. Computer-intensive methods for testing hypotheses. Wiley New York.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with content selection and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6908-6915.</p>
<p>Lena Reed, Shereen Oraby, and Marilyn Walker. 2018. Can neural generators for dialogue learn sentence planning and discourse structuring? INLG 2018, page 284 .</p>
<p>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. Cambridge university press.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80.</p>
<p>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593-607. Springer.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr-to-text generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16161626 .</p>
<p>Amanda Stent, Rashmi Prassad, and Marilyn Walker. 2004. Trainable sentence planning for complex information presentations in spoken dialog systems. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL04), pages $79-86$.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. Gtr-lstm: A triple encoder for sentence generation from rdf data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1627-1637.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. International Conference on Learning Representations. Accepted as poster.</p>
<p>Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378-7385.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/OpenNMT/OpenNMT-py&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ That is why some of the numbers in our table are not exactly the same as those in the cited works.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
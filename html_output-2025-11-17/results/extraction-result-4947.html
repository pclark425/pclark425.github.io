<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4947 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4947</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4947</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-d48b29889241551e1ee6622fa78c3fa4159255dd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd" target="_blank">Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A Selection-Inference (SI) framework is proposed that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4947.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4947.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gopher family evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gopher family large language models (evaluated across logic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic 5-shot evaluation of Gopher family LLMs on a broad suite of logical-reasoning tasks (50 tasks collected from bAbI, BigBench, AAC, ProofWriter, 2WikiMultiHop, Jeopardy, StrategyQA etc.), analyzing scaling behavior and failure modes for strict multi-step logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained transformer language models from the Gopher family (Rae et al., 2021) used in frozen form for 5-shot in-context evaluation; various sizes evaluated to study scaling laws.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>multiple (including 7B and 280B)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Mixed suite of 50 logical reasoning tasks (bAbI, ProofWriter, AAC, BigBench logic subset, 2WikiMultiHop, Jeopardy-derived tasks, StrategyQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Heterogeneous natural-language reasoning tasks probing single-step entailment, single-step inference, multi-step deduction/induction, multi-hop retrieval from context or memory, presence/absence of negation, and both multiple-choice and generative answer settings.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>5-shot in-context prompting (vanilla) across tasks to measure baseline capabilities and scaling; multi-choice scoring and generative exact-match evaluations reported; analysis of scaling laws comparing logic tasks vs other natural-language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate findings: larger models perform better but scaling for logic tasks is significantly weaker than for other NL tasks. Even the largest 280B model achieved only a modest improvement (reported as 13.6% above chance on average across 38 available multi-choice logic tasks). Performance is good on simple entailment tasks (AAC, Entailed Polarity) but drops substantially on single-step inference with distractors, multi-step (2+ step) problems, and tasks requiring retrieval from memory (e.g., 2WikiMultiHop, StrategyQA). Exact numeric per-subset breakdowns shown in paper figures (e.g., logic scaling curve vs other tasks in Fig.3a).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with chaining multiple reasoning steps; performance degrades when irrelevant facts/distractors are present; difficulty inferring needed facts from memory (multi-hop retrieval); scaling gains with model size are weaker for logic tasks than for other natural language tasks; large-model priors sometimes override context (e.g., preferring world priors over context-provided facts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to non-logic natural-language tasks, scaling behavior is worse. Gopher 280B outperforms smaller siblings on average but still performs poorly on multi-step logic; simpler entailment tasks are handled well across sizes. The paper compares these vanilla results to COT and SI methods showing substantial improvements for SI (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analysis across 50 tasks shows per-task breakdown: strong performance on entailed polarity/AAC tasks; performance decreases as number of reasoning steps increases (bAbI 1->3, ProofWriter depth 0->5). The authors report the logic scaling curve is markedly different (worse) than for the 56-task BigBench natural-language subset (Fig.3a).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4947.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4947.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selection-Inference (SI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection-Inference framework (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular two-stage iterative framework that decomposes each reasoning step into (1) Selection: choose a subset of context facts relevant for a single inference, and (2) Inference: produce a new fact from that selection; repeated for multiple steps to form a causal, interpretable reasoning trace.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pre-trained Gopher LLM used as Selection and Inference modules (frozen for prompt-engineered experiments; fine-tuned in separate experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses pre-trained transformer LLMs from the Gopher family as black-box modules; in prompt-engineered experiments the models are frozen and employed in a 5-shot setting; in fine-tuning experiments the same architecture (7B) is fine-tuned separately for Selection and for Inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (primary SI experiments); also compared to 280B baseline</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Subset of 10 logical reasoning tasks chosen from the 50: bAbI Tasks 1-3, 15-16 (1–3 supporting facts, deductive/inductive), and ProofWriter OWA depths 0,1,2,3,5</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks require 0–5 reasoning steps, include deductive and inductive examples, sometimes time-ordering or rule application, and require either generative single-word answers (bAbI) or True/False entailment (ProofWriter). Context may include distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompt-engineered SI: use n-shot prompts exemplifying Selection and Inference steps; Selection implemented via scoring facts in context (choose fact(s) with max log-likelihood under LLM) to avoid hallucinated facts; Inference uses limited selected facts (no question access) to generate a single new fact (first sentence) appended to context; repeat for fixed number of steps. Also implemented a fine-tuned variant where Selection and Inference LLMs are separately fine-tuned on single-step ground-truth traces (ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prompt-engineered SI (7B, generative, 5-shot): average accuracy across 11 datasets reported as 58.75%; compared to same 7B naive vanilla generative 2.94% and 7B COT 41.32% (all differences reported p<0.01). The 7B SI also outperformed a 280B baseline run naively (31.19%) and with COT (44.03%). Per-task: SI achieved 100% on bAbI 15 (deduction) in these experiments. Fine-tuned SI (7B, trained on ProofWriter single-step traces): final reasoning accuracy improved to 78.95% (vs prompt-engineered SI 57.93% on the evaluated ProofWriter subset). Inference LLM fine-tuned reached >99% single-step test accuracy after ~300 steps; Selection model achieved >80% exact-match selection accuracy across most reasoning depths after ~40k fine-tuning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Current implementation halts after a fixed number of steps (no learned halting criterion). Selection module is a bottleneck: predicting early selections for deeper multi-step problems is hard (requires planning) and performance degrades with depth; prompt-engineered Selection sometimes still makes suboptimal choices; unconstrained generation for Selection can lead to hallucinated facts (hence the scoring approach); performance gains diminish for large ProofWriter depths. Additional issues: need verifiers to avoid false inferences, currently relies on context being provided (no retrieval/search), and system may need human oversight for debugging traces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>SI (7B) substantially outperforms the same 7B used naively and in COT prompting, and often outperforms a 40x larger 280B model used naively or with COT. Fine-tuning Selection+Inference further improves SI vs prompt-only SI and vs baselines. SI produces causal interpretable traces, unlike COT where traces may not be causally tied to the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Demonstrated ablations: comparison of vanilla vs COT vs SI prompting (same examples) shows modular decomposition (Selection+Inference) yields higher accuracy than consolidating steps into one generative pass. Fine-tuning Selection and Inference separately (single-step supervision) yields marked gains: inference fine-tuned converged quickly (>99%), selection required many updates but reached >80% selection accuracy; Jaccard Similarity shows fine-tuned SI traces are closer to ground-truth reasoning traces than a model fine-tuned to predict all steps in one go (quantified in Fig.5b). Additional analysis shows increasing number of SI iterations can recover from earlier mistakes (Fig. S1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4947.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4947.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (COT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (Wei et al., 2022) inspired baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach encouraging the LLM to generate multi-step reasoning traces in a single generative pass (k-shot examples include reasoning + answer). Used here as an experimental baseline to compare to SI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family LLMs used with COT-style prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained transformer LLMs (7B and 280B evaluated) prompted with k-shot examples that include full reasoning traces and answers, then asked to generate [reasoning, answer] in one pass.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 280B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Same suite as other baselines for head-to-head comparison (subset of 10 tasks for SI experiments and larger 50-task evaluation set)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks require multi-step logical reasoning, entailment/proof generation, multi-hop inference; COT prompt provides examples with explicit reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chain-of-Thought prompting: n-shot prompts include examples of context, question, reasoning chain, and answer; model generates full reasoning + answer in one generative pass. Used as comparison to SI which generates step-by-step with constrained modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported generative performance: 7B with COT: 41.32% (vs 7B SI 58.75%); 280B with COT: 44.03% (vs 7B SI outperformed this 280B COT). For some tasks COT improves over vanilla but underperforms SI. The paper also notes that COT traces are often non-causal (reasoning steps can be incorrect while answer is correct).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reasoning traces generated by COT are not guaranteed to be causally used to produce the answer; they can contain incorrect or unrelated steps even when the final answer is correct. Producing multiple steps in a single pass does not ensure modular, verifiable stepwise reasoning. COT performance is better than vanilla but worse than SI in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>COT improves over vanilla prompting but underperforms SI (prompt-engineered and fine-tuned) in the paper's tasks. The authors point to prior observations (Wei et al., 2022) that COT traces may be unreliable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Qualitative analysis shows COT traces often contain unrelated or incorrect steps; providing the same reasoning examples in prompts but not splitting into Selection+Inference yields lower accuracy than SI. Statistical tests reported differences (p<0.01) between SI and COT on averaged metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4947.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4947.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla prompting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla n-shot prompting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where pre-trained LLM is prompted with concatenated examples of [context, question, answer] × k and then asked to generate the answer directly, with reasoning implicit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher family LLMs (7B and 280B evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frozen pre-trained transformer LLMs used in 5-shot prompt engineering to directly predict answers (either generative exact-match or multi-choice scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 280B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Same 50-task suite and the 10-task subset used for SI experiments (bAbI, ProofWriter, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Single-step and multi-step logical reasoning tasks in natural language; tasks vary in required number of inference steps, presence of distractors, and output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>n-shot in-context prompting: examples are [context, question, answer] repeated k times; then given a new [context, question] prompt the model must generate the answer in one pass without explicit intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported generative 5-shot performance: 7B vanilla generative was very low on the 10-task SI subset (reported as 2.94% in Sec.5 Fig.4a; other figures in Sec.6 for a different subset show 0.34% — both indicate near-chance generative performance on multi-step reasoning tasks); multi-choice evaluation is easier and yields higher numbers (e.g., vanilla 7B multi-choice reported 57.31% vs 280B 51.45% on a multi-choice setting in Sec.5). Overall vanilla prompting fails on multi-step logical reasoning in generative settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Implicit reasoning in a single generative pass is insufficient for multi-step logical problems; model tends to fail on chaining inferences and when distractors are present. Performance gap between generative and multi-choice settings indicates susceptibility to answer-format and evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Vanilla prompting is outperformed by COT prompting and substantially outperformed by SI (both prompt-engineered and fine-tuned). Interestingly, in some multi-choice settings the smaller 7B vanilla model outperformed a 280B vanilla model, possibly reflecting stronger learned priors that conflict with context (noted as a phenomenon needing further study).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Comparison across prompting strategies shows that explicit reasoning examples help (COT), but modularizing reasoning (SI) gives the largest gains. The authors attempted constrained selection scoring to prevent hallucinated facts, showing that unconstrained vanilla generation for intermediate steps can produce incorrect facts that break causal interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4947.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4947.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned SI (ProofWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned Selection and Inference modules on ProofWriter dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning the SI framework's Selection and Inference modules on single-step ground-truth traces from ProofWriter to improve selection accuracy and single-step entailment, yielding large gains in multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher 7B (fine-tuned separately for Selection and for Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as 7B Gopher used in prompt experiments; Selection model fine-tuned to output sentence labels (e.g., 'sent 2') to prevent hallucination; Inference model fine-tuned to produce entailment conclusions given selected facts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ProofWriter OWA dataset depths 0,1,2,3,5 (subset used; Unknowns removed to have binary True/False with available reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Natural-language rule/fact datasets where the model must determine whether a statement is provable True/False given rules and facts (deductive proofs; depth indicates number of reasoning steps required).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning on single-step reasoning examples: Selection model trained to select sentence indices corresponding to premises for one inference step; Inference model trained to output the single-step entailment conclusion. Both models trained on single-step data, then composed iteratively to solve multi-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned SI (7B) on ProofWriter: overall reasoning accuracy reported as 78.95% (substantially above prompt SI 57.93% and much higher than vanilla/COT generative baselines: 7B vanilla/COT reported as 0.34%/15.73% and 280B 31.58%/21.12% in the fine-tuning experiment context). Inference LLM reached >99% test accuracy after ~300 fine-tuning steps (batch size 16). Selection LLM trained ~40k steps (50 hours on TPU) and achieved >80% exact-match selection accuracy across most depths, with lower accuracy on deeper problems requiring planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Selection remains the limiting factor: predicting early selections for deeper (multi-step) proofs is hard due to planning requirements; fine-tuning was done on single-step traces so composition relies on iterative application and fixed halting. The benefit reduces as ProofWriter depth increases (gap between SI and baselines diminishes for greater depth). Computational cost to train selection model is non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Fine-tuned SI substantially outperforms prompt-engineered SI and other baselines on ProofWriter. Fine-tuned inference is easy (fast convergence) because LLMs are already good at single-step entailment; selection benefits heavily from supervision. The authors also compare trace quality: fine-tuned SI produces reasoning traces more similar to ground-truth (higher Jaccard similarity) than a baseline model fine-tuned to predict all steps jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Quantitative: Inference fine-tuning converged to >99% quickly; selection required extensive training but achieved >80% per-depth test accuracy. Jaccard similarity analysis shows fine-tuned SI's traces better overlap ground-truth than the monolithic-step baseline. Additional experiments show increasing SI iterations can improve recovery from earlier mistaken inferences (Fig. S1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Scaling language models: Methods, analysis & insights from training gopher <em>(Rating: 2)</em></li>
                <li>Towards ai-complete question answering: A set of prerequisite toy tasks <em>(Rating: 1)</em></li>
                <li>Constructing datasets for multi-hop reading comprehension across documents <em>(Rating: 1)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 1)</em></li>
                <li>A systematic investigation of commonsense understanding in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4947",
    "paper_id": "paper-d48b29889241551e1ee6622fa78c3fa4159255dd",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "Gopher family evaluation",
            "name_full": "Gopher family large language models (evaluated across logic tasks)",
            "brief_description": "Systematic 5-shot evaluation of Gopher family LLMs on a broad suite of logical-reasoning tasks (50 tasks collected from bAbI, BigBench, AAC, ProofWriter, 2WikiMultiHop, Jeopardy, StrategyQA etc.), analyzing scaling behavior and failure modes for strict multi-step logical reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gopher family",
            "model_description": "Pre-trained transformer language models from the Gopher family (Rae et al., 2021) used in frozen form for 5-shot in-context evaluation; various sizes evaluated to study scaling laws.",
            "model_size": "multiple (including 7B and 280B)",
            "logical_reasoning_task": "Mixed suite of 50 logical reasoning tasks (bAbI, ProofWriter, AAC, BigBench logic subset, 2WikiMultiHop, Jeopardy-derived tasks, StrategyQA)",
            "task_description": "Heterogeneous natural-language reasoning tasks probing single-step entailment, single-step inference, multi-step deduction/induction, multi-hop retrieval from context or memory, presence/absence of negation, and both multiple-choice and generative answer settings.",
            "method_or_approach": "5-shot in-context prompting (vanilla) across tasks to measure baseline capabilities and scaling; multi-choice scoring and generative exact-match evaluations reported; analysis of scaling laws comparing logic tasks vs other natural-language tasks.",
            "performance": "Aggregate findings: larger models perform better but scaling for logic tasks is significantly weaker than for other NL tasks. Even the largest 280B model achieved only a modest improvement (reported as 13.6% above chance on average across 38 available multi-choice logic tasks). Performance is good on simple entailment tasks (AAC, Entailed Polarity) but drops substantially on single-step inference with distractors, multi-step (2+ step) problems, and tasks requiring retrieval from memory (e.g., 2WikiMultiHop, StrategyQA). Exact numeric per-subset breakdowns shown in paper figures (e.g., logic scaling curve vs other tasks in Fig.3a).",
            "limitations_or_failure_cases": "Struggles with chaining multiple reasoning steps; performance degrades when irrelevant facts/distractors are present; difficulty inferring needed facts from memory (multi-hop retrieval); scaling gains with model size are weaker for logic tasks than for other natural language tasks; large-model priors sometimes override context (e.g., preferring world priors over context-provided facts).",
            "comparison": "Compared to non-logic natural-language tasks, scaling behavior is worse. Gopher 280B outperforms smaller siblings on average but still performs poorly on multi-step logic; simpler entailment tasks are handled well across sizes. The paper compares these vanilla results to COT and SI methods showing substantial improvements for SI (see other entries).",
            "ablation_or_analysis_results": "Analysis across 50 tasks shows per-task breakdown: strong performance on entailed polarity/AAC tasks; performance decreases as number of reasoning steps increases (bAbI 1-&gt;3, ProofWriter depth 0-&gt;5). The authors report the logic scaling curve is markedly different (worse) than for the 56-task BigBench natural-language subset (Fig.3a).",
            "uuid": "e4947.0",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Selection-Inference (SI)",
            "name_full": "Selection-Inference framework (this paper)",
            "brief_description": "A modular two-stage iterative framework that decomposes each reasoning step into (1) Selection: choose a subset of context facts relevant for a single inference, and (2) Inference: produce a new fact from that selection; repeated for multiple steps to form a causal, interpretable reasoning trace.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pre-trained Gopher LLM used as Selection and Inference modules (frozen for prompt-engineered experiments; fine-tuned in separate experiments)",
            "model_description": "Uses pre-trained transformer LLMs from the Gopher family as black-box modules; in prompt-engineered experiments the models are frozen and employed in a 5-shot setting; in fine-tuning experiments the same architecture (7B) is fine-tuned separately for Selection and for Inference.",
            "model_size": "7B (primary SI experiments); also compared to 280B baseline",
            "logical_reasoning_task": "Subset of 10 logical reasoning tasks chosen from the 50: bAbI Tasks 1-3, 15-16 (1–3 supporting facts, deductive/inductive), and ProofWriter OWA depths 0,1,2,3,5",
            "task_description": "Tasks require 0–5 reasoning steps, include deductive and inductive examples, sometimes time-ordering or rule application, and require either generative single-word answers (bAbI) or True/False entailment (ProofWriter). Context may include distractors.",
            "method_or_approach": "Prompt-engineered SI: use n-shot prompts exemplifying Selection and Inference steps; Selection implemented via scoring facts in context (choose fact(s) with max log-likelihood under LLM) to avoid hallucinated facts; Inference uses limited selected facts (no question access) to generate a single new fact (first sentence) appended to context; repeat for fixed number of steps. Also implemented a fine-tuned variant where Selection and Inference LLMs are separately fine-tuned on single-step ground-truth traces (ProofWriter).",
            "performance": "Prompt-engineered SI (7B, generative, 5-shot): average accuracy across 11 datasets reported as 58.75%; compared to same 7B naive vanilla generative 2.94% and 7B COT 41.32% (all differences reported p&lt;0.01). The 7B SI also outperformed a 280B baseline run naively (31.19%) and with COT (44.03%). Per-task: SI achieved 100% on bAbI 15 (deduction) in these experiments. Fine-tuned SI (7B, trained on ProofWriter single-step traces): final reasoning accuracy improved to 78.95% (vs prompt-engineered SI 57.93% on the evaluated ProofWriter subset). Inference LLM fine-tuned reached &gt;99% single-step test accuracy after ~300 steps; Selection model achieved &gt;80% exact-match selection accuracy across most reasoning depths after ~40k fine-tuning steps.",
            "limitations_or_failure_cases": "Current implementation halts after a fixed number of steps (no learned halting criterion). Selection module is a bottleneck: predicting early selections for deeper multi-step problems is hard (requires planning) and performance degrades with depth; prompt-engineered Selection sometimes still makes suboptimal choices; unconstrained generation for Selection can lead to hallucinated facts (hence the scoring approach); performance gains diminish for large ProofWriter depths. Additional issues: need verifiers to avoid false inferences, currently relies on context being provided (no retrieval/search), and system may need human oversight for debugging traces.",
            "comparison": "SI (7B) substantially outperforms the same 7B used naively and in COT prompting, and often outperforms a 40x larger 280B model used naively or with COT. Fine-tuning Selection+Inference further improves SI vs prompt-only SI and vs baselines. SI produces causal interpretable traces, unlike COT where traces may not be causally tied to the answer.",
            "ablation_or_analysis_results": "Demonstrated ablations: comparison of vanilla vs COT vs SI prompting (same examples) shows modular decomposition (Selection+Inference) yields higher accuracy than consolidating steps into one generative pass. Fine-tuning Selection and Inference separately (single-step supervision) yields marked gains: inference fine-tuned converged quickly (&gt;99%), selection required many updates but reached &gt;80% selection accuracy; Jaccard Similarity shows fine-tuned SI traces are closer to ground-truth reasoning traces than a model fine-tuned to predict all steps in one go (quantified in Fig.5b). Additional analysis shows increasing number of SI iterations can recover from earlier mistakes (Fig. S1).",
            "uuid": "e4947.1",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Chain-of-Thought (COT) baseline",
            "name_full": "Chain-of-Thought prompting (Wei et al., 2022) inspired baseline",
            "brief_description": "Baseline approach encouraging the LLM to generate multi-step reasoning traces in a single generative pass (k-shot examples include reasoning + answer). Used here as an experimental baseline to compare to SI.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gopher family LLMs used with COT-style prompting",
            "model_description": "Pre-trained transformer LLMs (7B and 280B evaluated) prompted with k-shot examples that include full reasoning traces and answers, then asked to generate [reasoning, answer] in one pass.",
            "model_size": "7B and 280B",
            "logical_reasoning_task": "Same suite as other baselines for head-to-head comparison (subset of 10 tasks for SI experiments and larger 50-task evaluation set)",
            "task_description": "Tasks require multi-step logical reasoning, entailment/proof generation, multi-hop inference; COT prompt provides examples with explicit reasoning chains.",
            "method_or_approach": "Chain-of-Thought prompting: n-shot prompts include examples of context, question, reasoning chain, and answer; model generates full reasoning + answer in one generative pass. Used as comparison to SI which generates step-by-step with constrained modules.",
            "performance": "Reported generative performance: 7B with COT: 41.32% (vs 7B SI 58.75%); 280B with COT: 44.03% (vs 7B SI outperformed this 280B COT). For some tasks COT improves over vanilla but underperforms SI. The paper also notes that COT traces are often non-causal (reasoning steps can be incorrect while answer is correct).",
            "limitations_or_failure_cases": "Reasoning traces generated by COT are not guaranteed to be causally used to produce the answer; they can contain incorrect or unrelated steps even when the final answer is correct. Producing multiple steps in a single pass does not ensure modular, verifiable stepwise reasoning. COT performance is better than vanilla but worse than SI in these experiments.",
            "comparison": "COT improves over vanilla prompting but underperforms SI (prompt-engineered and fine-tuned) in the paper's tasks. The authors point to prior observations (Wei et al., 2022) that COT traces may be unreliable explanations.",
            "ablation_or_analysis_results": "Qualitative analysis shows COT traces often contain unrelated or incorrect steps; providing the same reasoning examples in prompts but not splitting into Selection+Inference yields lower accuracy than SI. Statistical tests reported differences (p&lt;0.01) between SI and COT on averaged metrics.",
            "uuid": "e4947.2",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Vanilla prompting baseline",
            "name_full": "Vanilla n-shot prompting baseline",
            "brief_description": "Baseline where pre-trained LLM is prompted with concatenated examples of [context, question, answer] × k and then asked to generate the answer directly, with reasoning implicit.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gopher family LLMs (7B and 280B evaluated)",
            "model_description": "Frozen pre-trained transformer LLMs used in 5-shot prompt engineering to directly predict answers (either generative exact-match or multi-choice scoring).",
            "model_size": "7B and 280B",
            "logical_reasoning_task": "Same 50-task suite and the 10-task subset used for SI experiments (bAbI, ProofWriter, etc.)",
            "task_description": "Single-step and multi-step logical reasoning tasks in natural language; tasks vary in required number of inference steps, presence of distractors, and output formats.",
            "method_or_approach": "n-shot in-context prompting: examples are [context, question, answer] repeated k times; then given a new [context, question] prompt the model must generate the answer in one pass without explicit intermediate reasoning.",
            "performance": "Reported generative 5-shot performance: 7B vanilla generative was very low on the 10-task SI subset (reported as 2.94% in Sec.5 Fig.4a; other figures in Sec.6 for a different subset show 0.34% — both indicate near-chance generative performance on multi-step reasoning tasks); multi-choice evaluation is easier and yields higher numbers (e.g., vanilla 7B multi-choice reported 57.31% vs 280B 51.45% on a multi-choice setting in Sec.5). Overall vanilla prompting fails on multi-step logical reasoning in generative settings.",
            "limitations_or_failure_cases": "Implicit reasoning in a single generative pass is insufficient for multi-step logical problems; model tends to fail on chaining inferences and when distractors are present. Performance gap between generative and multi-choice settings indicates susceptibility to answer-format and evaluation protocol.",
            "comparison": "Vanilla prompting is outperformed by COT prompting and substantially outperformed by SI (both prompt-engineered and fine-tuned). Interestingly, in some multi-choice settings the smaller 7B vanilla model outperformed a 280B vanilla model, possibly reflecting stronger learned priors that conflict with context (noted as a phenomenon needing further study).",
            "ablation_or_analysis_results": "Comparison across prompting strategies shows that explicit reasoning examples help (COT), but modularizing reasoning (SI) gives the largest gains. The authors attempted constrained selection scoring to prevent hallucinated facts, showing that unconstrained vanilla generation for intermediate steps can produce incorrect facts that break causal interpretation.",
            "uuid": "e4947.3",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Fine-tuned SI (ProofWriter)",
            "name_full": "Fine-tuned Selection and Inference modules on ProofWriter dataset",
            "brief_description": "Fine-tuning the SI framework's Selection and Inference modules on single-step ground-truth traces from ProofWriter to improve selection accuracy and single-step entailment, yielding large gains in multi-step reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gopher 7B (fine-tuned separately for Selection and for Inference)",
            "model_description": "Same architecture as 7B Gopher used in prompt experiments; Selection model fine-tuned to output sentence labels (e.g., 'sent 2') to prevent hallucination; Inference model fine-tuned to produce entailment conclusions given selected facts.",
            "model_size": "7B (fine-tuned)",
            "logical_reasoning_task": "ProofWriter OWA dataset depths 0,1,2,3,5 (subset used; Unknowns removed to have binary True/False with available reasoning traces)",
            "task_description": "Natural-language rule/fact datasets where the model must determine whether a statement is provable True/False given rules and facts (deductive proofs; depth indicates number of reasoning steps required).",
            "method_or_approach": "Supervised fine-tuning on single-step reasoning examples: Selection model trained to select sentence indices corresponding to premises for one inference step; Inference model trained to output the single-step entailment conclusion. Both models trained on single-step data, then composed iteratively to solve multi-step problems.",
            "performance": "Fine-tuned SI (7B) on ProofWriter: overall reasoning accuracy reported as 78.95% (substantially above prompt SI 57.93% and much higher than vanilla/COT generative baselines: 7B vanilla/COT reported as 0.34%/15.73% and 280B 31.58%/21.12% in the fine-tuning experiment context). Inference LLM reached &gt;99% test accuracy after ~300 fine-tuning steps (batch size 16). Selection LLM trained ~40k steps (50 hours on TPU) and achieved &gt;80% exact-match selection accuracy across most depths, with lower accuracy on deeper problems requiring planning.",
            "limitations_or_failure_cases": "Selection remains the limiting factor: predicting early selections for deeper (multi-step) proofs is hard due to planning requirements; fine-tuning was done on single-step traces so composition relies on iterative application and fixed halting. The benefit reduces as ProofWriter depth increases (gap between SI and baselines diminishes for greater depth). Computational cost to train selection model is non-trivial.",
            "comparison": "Fine-tuned SI substantially outperforms prompt-engineered SI and other baselines on ProofWriter. Fine-tuned inference is easy (fast convergence) because LLMs are already good at single-step entailment; selection benefits heavily from supervision. The authors also compare trace quality: fine-tuned SI produces reasoning traces more similar to ground-truth (higher Jaccard similarity) than a baseline model fine-tuned to predict all steps jointly.",
            "ablation_or_analysis_results": "Quantitative: Inference fine-tuning converged to &gt;99% quickly; selection required extensive training but achieved &gt;80% per-depth test accuracy. Jaccard similarity analysis shows fine-tuned SI's traces better overlap ground-truth than the monolithic-step baseline. Additional experiments show increasing SI iterations can improve recovery from earlier mistaken inferences (Fig. S1).",
            "uuid": "e4947.4",
            "source_info": {
                "paper_title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Scaling language models: Methods, analysis & insights from training gopher",
            "rating": 2,
            "sanitized_title": "scaling_language_models_methods_analysis_insights_from_training_gopher"
        },
        {
            "paper_title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "rating": 1,
            "sanitized_title": "towards_aicomplete_question_answering_a_set_of_prerequisite_toy_tasks"
        },
        {
            "paper_title": "Constructing datasets for multi-hop reading comprehension across documents",
            "rating": 1,
            "sanitized_title": "constructing_datasets_for_multihop_reading_comprehension_across_documents"
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 1,
            "sanitized_title": "explaining_answers_with_entailment_trees"
        },
        {
            "paper_title": "A systematic investigation of commonsense understanding in large language models",
            "rating": 1,
            "sanitized_title": "a_systematic_investigation_of_commonsense_understanding_in_large_language_models"
        }
    ],
    "cost": 0.01573825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</h1>
<p>Antonia Creswell ${ }^{1}$, Murray Shanahan ${ }^{1}$ and Irina Higgins ${ }^{1}$<br>${ }^{1}$ DeepMind</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5 -shot generalisation setting, with no fine-tuning, yields a performance improvement of over $100 \%$ compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.</p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) are powerful few-shot learners (Bommasani et al., 2021; Brown et al., 2020; Lu et al., 2021). However, one area where they tend to perform poorly is logical reasoning (Rae et al., 2021). Yet the ability to perform multi-step, logically valid reasoning is fundamental for the discovery of new knowledge and explainability. It underpins many advancements that have been made in science, medicine, maths and philosophy. It is also one of the most valued strengths of classical, symbolic AI over contemporary deep learning methods (Bengio et al., 2021; Marcus, 2020; Marcus and Davis, 2019), prompting the recent increase in the use of neurosymbolic approaches to bridge this gap (Garcez and Lamb, 2020; Garnelo and Shanahan, 2019). Here we propose a Selection-Inference (SI) framework that takes inspiration from the neurosymbolic literature to improve the ability of LLMs to do logically valid reasoning.</p>
<p>There are many flavours of neurosymbolic models (Garcez and Lamb, 2020). Those from which we draw inspiration tend to have a modular structure, where each module is specialised for one type of operation (Andreas et al., 2016; Mao et al., 2019). For example, such modules may be neural networks or hand-crafted functions designed to attend to a single object, or to compare the location or size of two inputs (Andreas et al., 2016; Yi et al., 2018). Neurosymbolic models can produce an answer to a complex query by chaining these operations together, passing inputs from one module to another. This has the benefit of producing an interpretable trace of intermediate computations, in contrast to the "black-box" computations common to end-to-end deep learning approaches. Importantly, the modularity of neurosymbolic methods allows them to generalise to significantly harder problems that require long chains of reasoning (Hudson and Manning, 2019). However, the hand-crafted and specialised nature of the modules often makes the resulting systems brittle, hard to optimise, and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Selection-Inference (SI) framework (c) in comparison with the vanilla baseline (a) and Chain-of-Thought, COT, (b) approach (Wei et al., 2022). (a): The vanilla large language model baseline takes in concatenated [context, question, answer] $\times k$ for k -shot prompting, followed by [context, question] and is asked to generate the answer. All reasoning is done implicitly; (b): COT (Wei et al., 2022) inspired baseline takes in concatenated [context, question, reasoning, answer] $\times k$ for k-shot prompting, followed by [context, question] and is asked to generate the [reasoning, answer]; (c): SI framework consists of two steps. The selection step takes in concatenated [context, question, selection] $\times k$ for k-shot prompting, followed by [context, question] and is asked to select a subset of facts from the context to support a single step of reasoning. The inference step takes in [selection, inference] $\times k$ for k -shot prompting, followed by the selection produced by the Selection module to produce a new fact (the inference) to be added to the context. Each combination of [selection + inference + add fact to context] makes up one step of reasoning. These can be chained together to answer harder problems. The final inference is taken to be the answer.
difficult to extend to new domains (Yi et al., 2018).
Our SI framework, drawing on best practice from neurosymbolic approaches, decomposes logical reasoning into two modular stages: 1) selection, which involves choosing a subset of relevant information sufficient to make a single step of inference, and 2) inference, which only sees the limited information provided by the selection module, and uses it to infer a new intermediate piece of evidence on the way to the final answer (see Fig. 1c). We implement both stages using pre-trained LLMs which, thanks to their powerful few-shot generalisation capabilities, serve as more general alternatives to the hand-crafted, specialised modules typically used in neurosymbolic approaches. In the SI framework, multiple steps of selection and inference are chained together to produce a sequence of reasoning steps. As well as underpinning better performance on reasoning problems, this yields an interpretable trace that justifies the final answer.</p>
<p>Furthermore, the reasoning trace produced by our system is causal, in the sense that each step follows from, and depends on, the previous step. Each inference step is made in isolation, based solely on the limited information provided by the Selection module, without direct access to the question or to previous steps of reasoning. This contrasts with the more common approach of obtaining post-hoc rationalisation, where the answer produced by the model has no direct dependence on the explanation, since the explanation is produced either in parallel to the answer or after the fact (Cobbe et al., 2021; Lampinen et al., 2021; Saha et al., 2020). A notable example that sits in the grey area between post-hoc rationalisation approaches and the more causal explanation approaches is Chain-Of-Thought (COT) (Wei et al., 2022) (see Fig. 1b). In this approach LLMs are encouraged to produce a reasoning trace before the answer. However the dependence of the answer on the reasoning is not explicitly</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Correct reasoning on bAbI deduction (left) and induction (right) tasks.</p>
<p>Figure 2 | Qualitative results from the Selection-Inference (SI) model on bAbI tasks.
encouraged to be causal (as defined above). Indeed, the authors show that while the COT explanations help boost the final answer accuracy, the reasoning traces produced by the model are often wrong even when the final answer is correct (see the Supplementary Materials of (Wei et al., 2022) for examples). Developing a system that can demonstrate how it reaches its answers using a causal reasoning trace has important benefits in terms of safety, explainability, interpretability, debugging, and trust. In this paper we make the following contributions:</p>
<ol>
<li>We provide a comprehensive evaluation of LLMs on a set of 50 tasks probing different aspects of logical reasoning, and show that LLMs are good at simpler single step logical inference in 5 -shot generalisation settings, but struggle with harder problems (Sec. 3)</li>
<li>We introduce the Selection-Inference (SI) framework, a modular, iterative approach to solving reasoning problems (Sec. 4).</li>
<li>We demonstrate the utility of the SI framework by evaluating a 7B parameter LLM from the Gopher family (Rae et al., 2021) on 10 logical reasoning tasks, showing overall that it almost triples the performance of the same model used naively and almost doubles the performance of the same model used in the COT framework. Moreover, it often outperforms a 40x larger 280B LLM baseline used both naively and in the COT framework.</li>
<li>We illustrate further benefits of the SI framework in terms of the causal and interpretable reasoning traces produced (Sec. F.1). These traces can help humans understand how the model reached its final answer, which is useful for debugging and opens the system's decisions to human critique.</li>
</ol>
<h2>2. Related Work</h2>
<p>Our Selection-Inference framework sits at the intersection of classical, symbolic AI and deep learning. A typical symbolic AI system might consist of a knowledge base, which is typically hand-curated by experts, and an inference engine that allows the system to perform logic-based reasoning over its knowledge base. For example, such a system could apply step-by-step reasoning to answer a complex question such as "What are the apothecary's incentives and disincentives for selling poison to Romeo in Romeo and Juliet?" (Lenat, 2019) - something that even the best contemporary deep learning based systems struggle to do.</p>
<p>One of the primary benefits of symbolic AI systems over deep learning models is their interpretabil-</p>
<p>ity; we can look at the reasoning steps such a system has taken to see how the final conclusion was reached. However, unlike deep learning approaches, symbolic AI systems require knowledge to be hand-crafted and are in general hard to scale. Although some approaches have attempted to bridge the gap between deep learning and symbolic AI by converting problems into formal logic (Nye et al., 2021b) and using existing solvers to help produce an answer, this process can be brittle and tends not to scale well. Another attempt to bridge the gap comes from the neurosymbolic perspective (Gupta et al., 2019; Hudson and Manning, 2019; Mao et al., 2019; Yi et al., 2018). These models combine the best parts of deep learning - learning knowledge from data - and symbolic AI - producing an interpretable reasoning trace. However, they are typically quite brittle due to the hand-crafted (Gupta et al., 2019), specialised nature (Mao et al., 2019) of the modules and optimisation difficulties.</p>
<p>On the deep learning side, recent work has attempted to adapt large pre-trained language models, LLMs, to the task of logical reasoning. At a high level these can be split into three groups: 1) approaches that try to fine-tune LLMs to produce the final answer directly, keeping reasoning implicit (Betz et al., 2020; Clark et al., 2020) (e.g. Fig. 1a); 2) approaches that encourage LLMs to produce reasoning explicitly, but all reasoning steps are produced in one generative step (Cobbe et al., 2021; Dalvi et al., 2021; Jhamtani and Clark, 2020; Nye et al., 2021a; Wei et al., 2022; Zelikman et al., 2022) (e.g. Fig. 1B); and 3) approaches that use LLMs to produce each reasoning step one at a time (Tafjord et al., 2020). The latter is where our Selection-Inference framework sits (Fig. 1C). In general it was found that the approaches that incorporate explicit reasoning work better than those that only try to predict the final answer. However, although explicit reasoning helps improve the accuracy of the models, encouraging the models to produce multiple steps of reasoning in a single generative pass is not enough to make the models use reasoning in a causal manner. The authors found that the generated reasoning traces often contain unrelated or incorrect steps while still resulting in the correct answer (see examples in the appendices of (Wei et al., 2022; Zelikman et al., 2022)). Encouraging LLMs to generate each reasoning step one at a time (Tafjord et al., 2020) is currently the most promising direction for achieving causal reasoning, and it is the approach we take in our paper. While the model proposed by Tafjord et al. (2020) is very impressive, it only works for "Prove this statement to be True/False" style questions, since it relies on enumerating all possible implications and checking whether the question statement or its negation are present in the inferred facts, which is also computationally expensive.</p>
<h1>3. How Well Do Large Language Models Reason?</h1>
<p>Past work has shown that LLMs are poor at logical reasoning (Rae et al., 2021), however the evaluation was done on a relatively small set of tasks, and was not systematic. In particular, here we are interested in 1) how LLMs perform on simple entailment tasks compared to multi-step reasoning problems and 2) how scaling laws - suggested by Rae et al. (2021) - apply to logical reasoning. To this end, we evaluated LLMs from the Gopher family in a 5 -shot ${ }^{1}$ setting on a larger set of 50 tasks that touch on different aspects of logical reasoning and vary in terms of the number of reasoning steps required, presence or absence of negation, whether the relevant context information was provided, and whether the model is required to evaluate the accuracy of multiple choices or generate the answer among others. The additional tasks were collected from six sources: bAbI (Weston et al., 2015), BigBench (Ghazal et al., 2017), AAC (Betz et al., 2020), Jeopardy (Tunguz, 2019), Proof Writer (Tafjord et al., 2020) and 2WikiMultiHop (Welbl et al., 2018) (see Fig. S5a in Supplementary Information for raw results).</p>
<p>Our analysis found that LLMs are good at some aspects of logical reasoning (Fig. 3b). In particular,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Scaling laws for natural language tasks (bigbench, dark purple line, squares, 56 tasks) and tasks involving logical reasoning (logic, light purple line, circles, 38 tasks). All accuracy results are calculated relative to the random baseline ( $0 \%$ accuracy means chance level). Only multi-choice tasks are used.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Language models perform well for simple entailment tasks (AAC tasks, Entailed Polarity), their performance starts to get worse on single step inference problems (bAbI task 1, Proof Writer tasks 0-1), and they struggle with more complex multi-step reasoning problems (2WikiMultiHop tasks, bAbI tasks 2-3, Proof Writer tasks 2-5, StrategyQA).</p>
<p>Figure 3 | Vanilla language models perform poorly on multi-step logical reasoning tasks.
they appear to be good at simple entailment and implication tasks (e.g. see AAC tasks and Entailed Polarity in Fig. S5a). This appears to hold when negation is present (AAC Split Extended tasks), and both in generative (AAC Split) and multiple-choice scoring settings (AAC Split Extended tasks, Entailed Polarity). However, the performance of vanilla language models tends to decrease when they get presented with irrelevant facts alongside the ones relevant for reasoning (e.g. see 2WikiMultiHop With Context tasks, bAbI tasks 2-3 or Proof Writer tasks), when they have to infer the relevant facts from memory (e.g. 2WikiMultiHop or StrategyQA tasks), and as the questions start to require more steps of reasoning (e.g. see the performance drop between bAbI tasks 1-3 or Proof Writer Tasks).</p>
<p>In line with Rae et al.'s findings, our results confirmed that LLMs of larger sizes do perform better than the smaller models. However, we found that even the largest 280B model performed only 13.6\% above chance level on average across the 38 available multi-choice logic tasks (see Figs. S5a-S5b and Sec. H in Supplementary Information for more details). Furthermore, we found that logical reasoning tasks were qualitatively different from other natural language tasks. The scaling law for logic-based tasks within the Gopher family models was significantly worse than for other language tasks measured here as the average performance on the subset of BigBench tasks from (Rae et al., 2021) with the logic tasks used in this paper removed (see Fig. 3a).</p>
<h1>4. The Selection-Inference (SI) Framework</h1>
<p>We are interested in solving logical reasoning problems expressed in natural language. In this work we assume that each question is accompanied by context information (see Figs. 1 and 2a), which contains all the information necessary to solve the problem, as well as potentially irrelevant distractors. In the future this assumption can be relaxed, for example by extracting the necessary information</p>
<p>through search (Lazaridou et al., 2022; Menick et al., 2022). We also assume that all questions are well posed and definitively answerable given the context.</p>
<p>Logical reasoning problems require using existing information to infer new relevant knowledge necessary to answer the question. This can be done through deduction, induction or abduction, although the datasets we use here contain mostly deductive and a small number of inductive problems ${ }^{2}$. Some problems require multiple steps of inference, where later steps use the knowledge inferred in the earlier steps. Hence, we use an iterative framework where at each step the SI uses information in the existing context, $C_{t}$, to infer a new fact, $f_{t}$, which is appended back to the context to create new context, $C_{t+1}=C_{t} \cup f_{t}$. This process can then iterate until the solution to the question is found. In the current implementation of the SI framework, we repeat the process for a fixed number of steps and take the final inference to be the answer. Addressing the issue of halting is left for future work.</p>
<p>Inspired by neurosymbolic methods, we additionally split each step of reasoning into further two components: 1) Selection, which selects a subset of the information present in the context, $s_{t}$, given the context and the question, $C^{t} \cup q$. This selection, $s^{t}$ is fed to the next step, 2) inference, which produces the new fact, $f_{t}$, based on the information passed to it by the selection step. Examples of selection and inference are shown in Fig. 2a. This splitting of each step of reasoning into selection and inference is the main contribution of our paper, and is important for several reasons. First, and most importantly, it makes the resulting reasoning causal, since both steps have limited capabilities by design, and are interdependent. The selection step is constrained to only use the information available in the context, an the inference step only sees the subset of facts provided by the selection without access to the question. Hence, the model is unlikely to make up information to answer the question, and it cannot ignore reasoning when producing the final answer. The other benefit of this approach is that each step of reasoning is broken down into even smaller sub-tasks, which are easier for LLMs to adapt to, and which helps make the reasoning more generalisable to harder problems.</p>
<p>In this paper we use pre-trained, frozen language models from the Gopher family (Rae et al., 2021) in a 5 -shot generalisation setting using prompt engineering to implement the Selection and Inference modules. We settled on prompt engineering to evaluate the base utility of the SI framework, however it can also be used in the fine-tuning setting which we explore briefly in Sec. 6. We next describe the Selection and Inference modules in more detail.</p>
<h1>4.1. Selection Module</h1>
<p>We use prompt engineering to encourage the model to output the correct selection, $s^{t}$. The n-shot prompt is a string of the the following form:</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;&quot;
<span class="gh">#</span> n-shot prompt
<span class="gh">#</span> First example.
&lt;context 1&gt;
&lt;question 1&gt;
<span class="gh">#</span> Example selection
&lt;fact&gt;. We know that &lt;fact&gt;[ and &lt;fact&gt;]*. Therefore,
...
<span class="gh">#</span> Problem to solve.
&lt;context&gt;
&lt;question&gt;
</code></pre></div>

<p>"" "</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where <fact>s are copied directly from the context, and [ and <fact>]* means that the module is allowed to select more than one fact for each step of inference, where the total number of facts is a hyper-parameter.</p>
<p>The simplest option to implement the selection is to feed this prompt directly to a pre-trained LLM and take the output generated by the language model. However, this unconstrained approach may allow the model to make up facts, thus removing the causal aspect of the reasoning trace. Indeed during experimentation this is what we often found. So instead we use the pre-trained LLM to score each of the facts in the context, and select the one with the highest log-likelihood. We can then repeat this process by appending each new fact to the end of the previous prompt until the full selection is constructed. Note that for now we halt after a fixed number of steps. See Algorithm 2 for more details.</p>
<h1>4.2. Inference module</h1>
<p>The n-shot prompt for the Inference module has the following form (shown below, also see Fig. 1):</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;&quot;
<span class="gh">#</span>n-shot inference prompt
<span class="gh">#</span> First example.
&lt;fact&gt;. We know that &lt;fact&gt;[ and &lt;fact&gt;]*. Therefore, &lt;new fact
<span class="k">    &gt;</span>
<span class="ge">. . .</span>
<span class="gh">#</span> Problem to solve.
&lt;output of the Selection module&gt;. Therefore,
&quot;&quot; &quot;
</code></pre></div>

<p>The n-shot prompt and the output of the Selection module, are fed to the pre-trained LLM serving as the Inference module. The first generated sentence (extracted from the generated text as per BigBench evaluation (Ghazal et al., 2017) pipeline, see Supplementary Materials for details) is taken to be the newly inferred fact. This fact is added to the context, which concludes one reasoning step of the SI framework. For now, we halt after a fixed number of steps. See Algorithm 1 for more details.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Selection-Inference
Require: An n-shot selection prompt, \(p_{\text {select }}\).
Require: An n-shot inference prompt, \(p_{\text {in } f e r}\).
Require: Initial Context, \(C^{0}\), made up of facts (and rules).
Require: The question, \(q\).
Require: Language model, LLM.
Require: A halting function, halt, determines if the answer has been
    reached.
    \(t=0 \quad \triangleright\) Start at step 0 .
    while not halt() do
        \(s^{t} \leftarrow\) Selection_Module \(\left(p_{\text {select }}, C^{t}, q, \mathrm{LLM}\right) \quad \triangleright\) Do selection.
        \(i^{t} \leftarrow\) Inference_Module \(\left(p_{\text {in } f e r}, s^{t}\right) \quad \triangleright\) Do inference.
        \(C^{t+1} \leftarrow C^{t} \cup i^{t} \quad \triangleright\) Add the newly inferred fact to the context.
        \(t \leftarrow t+1 \quad \triangleright\) Move onto the next step of reasoning
    end while
    return \(s^{t}\)
</code></pre></div>

<h1>5. Experiments and Results</h1>
<p>We evaluate our SI framework on a subset of $10 / 50$ logical reasoning tasks introduced in Sec. 3. These tasks were chosen based on whether they include context information necessary to answer the question, whether the questions have a definitive answer, and to ensure that they cover different kinds of reasoning abilities. The tasks include bAbI (Weston et al., 2015) Tasks 1-3, which require the model to use 1-3 supporting time-ordered facts respectively to answer a question, and Tasks 15-16, which test deductive and inductive reasoning respectively. We also evaluate our model on the Proof Writer OWA datasets (Tafjord et al., 2020) of depth 0, 1, 2, 3 and 5 (there is no depth 4 task). These are language-based logical reasoning problems, where the depth is the number of reasoning steps required to answer the question.</p>
<p>To baseline the performance of the SI framework, we consider a 7B (same size as the LLM used in the SI framework) and a 40x larger 280B parameter LLM evaluated in a 5-shot setting. There are two types of evaluation for these vanilla baselines that we consider: multi-choice and generative evaluation. In generative evaluation, we measure the exact string match (first sentence in lower case and ignoring any non-alphabetic characters) between the output generated by the LLM and the ground truth answer. This is appropriate, since most of the tasks that we consider require either a single word answer, or the dataset is such that the answers are highly structured. In multi-choice evaluation the LLM is used to score each of the answer choices, as in Li et al. (Li et al., 2021). In general LLMs perform significantly better in a multi-choice vs generative evaluation setting, since the chance level in the multi-choice setting is significantly higher.</p>
<p>We also consider a chain-of-thoughts (COT) (Wei et al., 2022) inspired baseline, where the k-shot prompts to the 7B and 280B models include reasoning traces for the same examples that we use to prompt the SI framework (although with selection and inference combined, see Supplementary Information for example prompts). This tests whether providing the reasoning examples alone is sufficient to improve performance, or whether the further breakdown into Selection and Inference sub-steps improves accuracy. Note that among all of the approaches outlined only the SI framework is explicitly set up to generate causal reasoning traces.</p>
<p>Fig. 4a demonstrates that overall when evaluated generatively, the 7B parameter LLM within the SI framework performs better ( $58.75 \%$ ) than the same model evaluated naively ( $2.94 \%$ ) or in the COT framework ( $41.32 \%$ ) (all $p&lt;0.01$, see Supplementary Information for details of the calculations). Not only that, the 7B parameter LLM within the SI framework also outperforms on average the 40x larger 280B parameter LLM in both vanilla ( $31.19 \%$ ) and COT framework ( $44.03 \%$ ) (all $p&lt;0.01$ ). When evaluated in the easier multi-choice setting, we find that surprisingly ${ }^{3}$ the vanilla 7B parameter LLM outperforms the 280B parameter LLM ( $57.31 \%$ vs $51.45 \%$ ), while still performing significantly worse than the 7B SI model ( $p=0.012$ ). Note that the latter is evaluated in the harder generative setting. Per task breakdown shown in Fig. 4b demonstrates that the SI framework solves the bAbI 15 deduction task, the only model to achieve $100 \%$ accuracy (significant difference from the other models, $p&lt;0.01$ ). Furthermore, it does so having seen only five examples in the prompt. The 7B SI model also significantly outperforms all other models on Proof Writer Depth 0 ( $p&lt;0.01$ ), Proof Writer Depth $1(p=0.034)$.</p>
<p>As well as improving upon most baselines quantitatively, the SI framework also has additional qualitative benefits: 1) it produces a causal, human interpretable reasoning trace that shows how the model reached its answer and 2) it is able to recover from errors. We will now discuss each of these</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Average accuracy over 11 datasets comparing like-for-like generative performance of the 7B and 280B parameter language models used in a 5 -shot generalisation setting to predict the answer directly (vanilla), in the Chain-Of-Thought framework, COT, and in the SI framework.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Per task breakdown of the performance of LLMs used naively, and within the COT and SI frameworks.</p>
<p>Figure 4 | Quantitative results for the Selection-Inference (SI) framework.
in turn.
Since the Selection module is only allowed to pick facts from the context and is separate from the Inference module, and since the latter does not have access to the question, the model has to use what is selected and cannot bypass the selection to compute its answer, thus creating a causal reasoning trace. Since the reasoning trace is in natural language and is causal, it can be audited and inspected by humans, which has significant implications for safety and interpretability.</p>
<p>Example reasoning traces produced by the SI model are shown in Fig. 2a. In the bAbI 16 example shown on the right the model is solving an inference problem, which requires the model to infer the colour of an animal given facts about the colours of other animals. In this example, the model is asked "What colour is greg", and told that "greg is a lion". This means first the model needs to use induction to infer a rule about the colour of lions. On the first step, we see that the model induces a rule, "lions are white", based on the fact that "brian is a lion" and "brian is white"; we can see exactly what data underlies the model's decision to form a new rule. On the second step, we see that the model applies this newly inferred rule to the fact that "greg is a lion" to reach the final conclusion that "greg is white" using deduction. Note that the ability of the SI framework to produce inductions relies on its ability to deal with uncertainty and understand what is "reasonable" - something that LLMs are naturally capable of, while also being a known struggle point for symbolic AI.</p>
<p>Since the reasoning traces are output in natural language, they are easy for humans to interpret and potentially intervene. Consider a scenario where there may be both a white lion and a green lion mentioned in the context, in which case we could see which information the model used to make its final decision and decide whether we want to trust it (example in Fig. 2b). We could also imagine examples where the model puts together two unrelated facts to come up with an incorrect inference, and this could also be easily be spotted by a human and rectified by replacing the wrongly inferred fact with a correct one, and re-running the consequent reasoning steps.</p>
<p>Aside from inspecting reasoning traces and using them to debug when something goes wrong, the additive nature of our model - it accumulates new knowledge with each reasoning step, means that it also has the ability to recover from errors. Fig. 2b demonstrates such an example. In the first step</p>
<p>the model inferred that "swans are often gray", using the facts that "julius is a swan" and "julius is gray". While this is correct, this new information is not useful for answering the question, which asks about lions. However, it is still possible for the model to make the more useful inference that "lions are often white" in a later step and recover from its original misstep.</p>
<h1>6. Fine-tuning Language Models for Selection and Inference</h1>
<p>In Sec. 5 we have demonstrated significant improvements in logical reasoning accuracy when using prompt-engineering to specialise LLMs for Selection and Inference in the SI framework. Promptengineering has the additional benefit of not requiring large amounts of step-by-step reasoning data, which may be hard to obtain. In this section we investigate whether the accuracy of the SI framework can be further improved by fine-tuning the LLMs for Selection and Inference. We use the Proof Writer dataset for which ground truth reasoning traces exist.</p>
<p>The Selection LLM is fine-tuned to select a subset of sentences (including facts and rules) from the context by generating a string of the form "sent 2. We know that sent 4 [and sent 7] "." given the context and the question. We ask the Selection LLM to generate sentence labels (e.g. "sent 2" or "sent 4") instead of the sentences themselves, because this prevents the Selection LLM from cheating by making up facts to answer the question quicker. This preserves the dependency of the selection and therefore subsequent reasoning steps on the context. The Inference LLM is fine-tuned to compute an entailment given the selection. Both models are fine-tuned on single steps of reasoning only. Example training data are shown in Fig. S2.</p>
<p>The Inference LLM converged very quickly to $&gt;99 \%$ test accuracy after only 300 fine-tuning steps with a batch size of 16 , which is to be expected as we found that pre-trained LLMs are good at single step entailment out of the box as shown in Fig. 3b. Examples of inferences made by the model are shown in Fig. S3. The Selection LLM was trained for $4 \times 10^{4}$ steps (with batch size 16 for 50 hours on a TPU) with the exact string match accuracy reported in Fig. 5a. Although we notice that the model is much better at predicting selections for problems that require fewer steps of inference than those that require more, ultimately the model still achieves high ( $&gt;80 \%$ ) accuracy across most of the reasoning depths. Predicting early selections for deeper reasoning problems is hard, because it requires planning. It is an important problem to address in future work.</p>
<p>Fig. 4 b shows that fine-tuning LLMs on single steps of reasoning within the SI framework leads to significant improvements in final reasoning accuracy ( $78.95 \%$ ) over the prompt-engineered version of the SI framework (57.93\%) and other prompt-engineered baselines (vanilla/COT generative 7B: $0.34 / 15.73 \%, 280 \mathrm{~B}: 31.58 / 21.12 \%)$. We also found that the fine-tuned 7B LLM used within the SI framework produces significantly more accurate reasoning traces compared to the same LLM fine-tuned to predict all reasoning steps in one go (Fig. 5b). We quantified this using the Jaccard Similarity, Jaccard Similarity $=(M \cap G T) /(M \cup G T)$, between the proof steps predicted by each model, $M$, and the ground-truth reasoning steps, $G T$, as shown in, calculated using exact string match over alphanumeric characters.</p>
<p>Qualitatively we observed that while the baseline model is good at predicting most of the reasoning steps, they often appear in the wrong order, there are additional reasoning steps that are not on the minimal reasoning path, and some steps get repeated a number of times.</p>
<h2>7. Conclusion</h2>
<p>We have presented the Selection-Inference framework for improving the ability of pre-trained language models to solve logical reasoning problems expressed in natural language. Our approach borrows</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Average test fine-tuning accuracy for the Selection module trained on single-step selection across all Proof Writer datasets (depth 1, 2, 3 and 5) and tested on problems of each depth separately.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Intersection over union between reasoning traces produced by a model and the ground truth reasoning steps. Baseline, 7B parameter LLM finetuned to predict all reasoning steps in one go; SI, using same 7B LLM fine-tuned for single step Selection and Inference.</p>
<p>Figure 5 | Fine-tuning the SI framework on the Proof Writer dataset.
from the best practices of neurosymbolic approaches to break down logical reasoning into a modular recursive pipeline that not only significantly improves the reasoning accuracy, but also produces a causal interpretable reasoning trace. We have demonstrated that prompt-engineered LLMs used in the SI framework significantly outperform both the vanilla and COT baselines evaluated in equivalent settings, and even 40x larger baselines. The performance of the SI framework can be further improved through fine-tuning if step-by-step reasoning data is available.</p>
<p>A model capable of casual, interpretable and logically valid multi-step reasoning has potential applications in law, medicine, science, maths, economics, and other areas where trustworthy and verifiable logical inference is important. At the same time we recognise that special care will need to be taken to evaluate such a model before deployment in any of these settings. Further work is also needed, for example, to improve the Selection module (e.g. by allowing the model search over and evaluate different reasoning traces); to address the halting issue (both in terms of when to stop the selection and when to stop the overall reasoning); to incorporate verifiers that would help avoid false inferences being added to the context; to enable the system to source its own relevant context rather than relying on it being provided in the dataset; and to extend the ability of the system to deal with ambiguous or unanswerable questions.</p>
<h1>References</h1>
<p>J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39-48, 2016.
Y. Bengio, Y. Lecun, and G. Hinton. Deep learning for ai. Communications of the ACM, 64(7):58-65, 2021.
G. Betz, C. Voigt, and K. Richardson. Critical thinking for language models. arXiv preprint arXiv:2009.07185, 2020.
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg,</p>
<p>A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
P. Clark, O. Tafjord, and K. Richardson. Transformers as soft reasoners over language. arXiv preprint arXiv:2002.05867, 2020.
K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
B. Dalvi, P. A. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark. Explaining answers with entailment trees. ArXiv, abs/2104.08661, 2021.
A. d. Garcez and L. C. Lamb. Neurosymbolic ai: the 3rd wave. arXiv preprint arXiv:2012.05876, 2020.
M. Garnelo and M. Shanahan. Reconciling deep learning with symbolic artificial intelligence: representing objects and relations. Current Opinion in Behavioral Sciences, 29:17-23, 2019.
A. Ghazal, T. Ivanov, P. Kostamaa, A. Crolotte, R. Voong, M. Al-Kateb, W. Ghazal, and R. V. Zicari. Bigbench v2: The new and improved bigbench. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE), pages 1225-1236, 2017. doi: 10.1109/ICDE.2017.167.
N. Gupta, K. Lin, D. Roth, S. Singh, and M. Gardner. Neural module networks for reasoning over text. arXiv preprint arXiv:1912.04971, 2019.
D. A. Hudson and C. D. Manning. Learning by abstraction: The neural state machine. arXiv preprint arXiv:1907.03950, 2019.
H. Jhamtani and P. Clark. Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering. arXiv preprint arXiv:2010.03274, 2020.
A. K. Lampinen, N. A. Roy, I. Dasgupta, S. C. Chan, A. C. Tam, J. L. McClelland, C. Yan, A. Santoro, N. C. Rabinowitz, J. X. Wang, et al. Tell me why!-explanations support learning of relational and causal structure. arXiv preprint arXiv:2112.03753, 2021.
A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022.
D. Lenat. What ai can learn from romeo \&amp; juliet, Jul 2019. URL https://www.forbes.com/ sites/cognitiveworld/2019/07/03/what-ai-can-learn-from-romeo--juliet.
X. L. Li, A. Kuncoro, C. de Masson d'Autume, P. Blunsom, and A. Nematzadeh. A systematic investigation of commonsense understanding in large language models. arXiv e-prints, pages arXiv-2111, 2021.
B. Y. Lin, S. Lee, R. Khanna, and X. Ren. Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models. arXiv preprint arXiv:2005.00683, 2020.
K. Lu, A. Grover, P. Abbeel, and I. Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 2021.</p>
<p>J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584, 2019.
G. Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020.
G. Marcus and E. Davis. Rebooting AI: Building Artificial Intelligence We Can Trust. Ballantine Books Inc., 2019.
J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young, L. CampbellGillingham, G. Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.
S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021a.
M. Nye, M. Tessler, J. Tenenbaum, and B. M. Lake. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Advances in Neural Information Processing Systems, 34, 2021b.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
S. Saha, S. Ghosh, S. Srivastava, and M. Bansal. Prover: Proof generation for interpretable reasoning over rules. arXiv preprint arXiv:2010.02830, 2020.
O. Tafjord, B. D. Mishra, and P. Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.
B. Tunguz. 200,000+ jeopardy! questions, Nov 2019. URL https://www.kaggle.com/datasets/ tunguz/200000-jeopardy-questions.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models, 2022.
J. Welbl, P. Stenetorp, and S. Riedel. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287-302, 2018.
J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. Van Merriënboer, A. Joulin, and T. Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.
K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. arXiv preprint arXiv:1810.02338, 2018.
E. Zelikman, Y. Wu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.</p>
<h1>Supplementary Information</h1>
<h2>A. Example prompts for vanilla baselines</h2>
<h2>A.1. ProofWriter</h2>
<div class="codehilite"><pre><span></span><code><span class="s2">&quot;&quot;</span><span class="err">&quot; </span>
<span class="err">Here are some statements that describe a situation:</span>
<span class="err">Bob is cold.</span>
<span class="err">Charlie is quiet.</span>
<span class="err">Gary is cold.</span>
<span class="err">Harry is quiet.</span>
<span class="err">Big things are cold.</span>
<span class="err">All blue things are not cold.</span>
<span class="err">If something is quiet and blue then it is not cold.</span>
<span class="err">All quiet things are cold.</span>
<span class="err">If something is big and rough then it is round.</span>
<span class="err">If something is cold and not rough then it is blue.</span>
<span class="err">If something is quiet and not furry then it is not blue.</span>
<span class="err">Round things are big.</span>
<span class="nv">Based</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">above</span>,<span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">statement</span><span class="w"> </span><span class="s2">&quot;Charlie is cold&quot;</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">true</span>.
</code></pre></div>

<p>Here are some statements that describe a situation:
Erin is not cold.
Erin is kind.
Erin is red.
Erin is smart.
Erin is not white.
Erin is young.
Gary is cold.
Gary is not furry.
Gary is kind.
Gary is red.
Gary is not smart.
Gary is young.
All cold, smart things are not furry.
Young, cold things are not furry.
If something is white and smart then it is furry.
If Gary is white then Gary is not furry.
If Erin is young then Erin is furry.
If Gary is not young then Gary is smart.
If Erin is cold then Erin is young.
Red things are kind.
Based on the above, the statement "Erin is not furry" is
"""</p>
<h2>A.2. bAbI 1</h2>
<p>"" "
Context: daniel went to the bedroom
daniel journeyed to the office
daniel travelled to the bathroom
mary went to the office</p>
<div class="codehilite"><pre><span></span><code>john journeyed to the bedroom
daniel went back to the kitchen
john went to the garden
daniel travelled to the office
Question: where is john?
Choice: garden
Choice: bathroom
Choice: office
Choice: kitchen
Choice: bedroom
Choice: hallway
Answer: garden
</code></pre></div>

<p>.
Context: sandra went to the kitchen
sandra went to the office
sandra travelled to the hallway
sandra went back to the kitchen
mary travelled to the hallway
sandra went to the bedroom
john went to the garden
sandra travelled to the office
Question: where is sandra?
Choice: garden
Choice: bedroom
Choice: kitchen
Choice: bathroom
Choice: hallway
Choice: office
Answer:
"""</p>
<h1>A.3. 2WikiMultiHop</h1>
<p>New lines are added between facts to fit on the page.
"""
Q: When did Michael Baden-Powell's father die?
Here are some relationships to help answer this question.
Michael Baden-Powell::father::Peter Baden-Powell, 2nd Baron BadenPowell,
Peter Baden-Powell, 2nd Baron Baden-Powell::date of death::9
December 1962
A: 9 December 1962
・.
Q: Where does Ekaterina Rybolovleva's father work at?
Here are some relationships to help answer this question.
Ekaterina Dmitrievna Rybolovleva::father::Dmitry Rybolovlev,
Dmitry Rybolovlev::employer::Uralkali
A:
""</p>
<h1>B. Example prompts for COT baselines</h1>
<h2>B.1. Proof Writer 3</h2>
<div class="codehilite"><pre><span></span><code><span class="s">&quot;&quot;&quot;</span>
<span class="s">Given a set of rules and facts, you have to reason whether a</span>
<span class="s">    statement is true or false.</span>
<span class="s">Here are some facts and rules:</span>
<span class="s">If someone is red then they are nice.</span>
<span class="s">If someone is kind and red then they are white.</span>
<span class="s">If someone is nice then they are kind.</span>
<span class="s">Fiona is red.</span>
<span class="s">Does it imply that the statement &quot;</span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">white</span><span class="err">&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">?</span>
<span class="nx">Reasoning</span><span class="p">:</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">red</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Fiona</span>
<span class="w">    </span><span class="k">is</span><span class="w"> </span><span class="nx">red</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span>
<span class="nx">If</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">kind</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">kind</span><span class="p">.</span>
<span class="nx">If</span><span class="w"> </span><span class="nx">someone</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">kind</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">red</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">they</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">white</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Fiona</span>
<span class="w">    </span><span class="k">is</span><span class="w"> </span><span class="nx">kind</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">red</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nx">Fiona</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">white</span><span class="p">.</span>
</code></pre></div>

<p>Here are some facts and rules:
If someone chases the cow then they eat the cow.
If someone is big then they chase the cow.
If someone needs the bald eagle then the bald eagle is big.
If the bear is nice and the bear needs the cow then the bear eats the lion.
If someone needs the lion and they eat the bald eagle then they are blue.
If someone eats the bear and they do not chase the cow then the cow is young.
the bald eagle eats the lion.
the bear is round.
the lion eats the bald eagle.
the bald eagle needs the cow.
the bear is young.
the cow is not nice.
the cow does not chase the bald eagle.
the bear does not eat the bald eagle.
the bear needs the bald eagle.
the bald eagle chases the bear.
Does it imply that the statement "The bald eagle does not eat the cow" is True?
Reasoning: If someone needs the bald eagle then the bald eagle is big. We know that the bear needs the bald eagle. Therefore, the bald eagle is big.
If someone is big then they chase the cow. We know that the bald eagle is big. Therefore, the bald eagle chases the cow.
If someone chases the cow then they eat the cow. We know that the bald eagle chases the cow. Therefore, the bald eagle eats the cow
"" "</p>
<h1>B.2. bAbI 2</h1>
<p>"" "
Below are some stories about people moving objects between rooms.
After each story you have to answer a question about where a particular object is.
Story:
at $t=0$ mary grabbed the football there
at $t=1$ daniel got the apple there
at $t=2$ mary went to the kitchen
at $t=3$ daniel journeyed to the office
at $t=4$ daniel went to the bedroom
at $t=5$ mary moved to the garden
Question: where is the apple?
Reason: at $t=1$ daniel got the apple there. We know that at $t=4$ daniel went to the bedroom. Therefore, the apple is in the bedroom</p>
<p>Story:
at $t=0$ sandra went to the office
at $t=1$ john took the milk there
at $t=2$ sandra got the milk there
at $t=3$ john dropped the milk
Question: where is the milk?
Reason: at $t=2$ sandra got the milk there. We know that at $t=0$ sandra went to the office. Therefore, the milk is in the office
" " "</p>
<h2>C. Example prompts for Selection-Inference</h2>
<h2>C.1. bAbI 2</h2>
<p>The selection prompt:
"" "
Here are a collection of stories about people carrying objects from one room to another. You will be asked where any object is. To answer this question you need to figure out who last had the object and which room they have the object in by the end of the story. Here are some examples:</p>
<p>Story:
at $t=0$ mary grabbed the football there
at $t=1$ daniel got the apple there
at $t=2$ mary went to the kitchen
at $t=3$ daniel journeyed to the office
at $t=4$ daniel went to the bedroom
at $t=5$ mary moved to the garden
Question: where is the apple?
Reason: at $t=1$ daniel got the apple there. We know that at $t=4$ daniel went to the bedroom</p>
<p>at $t=0$ john moved to the bathroom
at $t=1$ john travelled to the office
at $t=2$ john picked up the football there
at $t=3$ john journeyed to the bathroom
Question: where is the football?
Reason:" " "
The inference prompt:</p>
<div class="codehilite"><pre><span></span><code>&quot;&quot;&quot;
at t=1 daniel got the apple there. We know that at t=4 daniel went
    to the bedroom. Therefore, the apple is in the bedroom.
at t=2 john picked up the football there. We know at t=0 john moved
    to the bathroom. Therefore&quot;&quot;&quot;
</code></pre></div>

<h1>C.2. Proof Writer</h1>
<p>Below is an example selection prompt. Note that this is for a depth-2 problem and so we show examples of the first reasoning step where the conclusion would not directly prove or disprove the statement and the last reasoning step, where the conclusion would directly prove or disprove the statement.</p>
<div class="codehilite"><pre><span></span><code><span class="s">&quot;&quot;&quot;</span>
<span class="s">Given a set of rules and facts, you have to reason whether a</span>
<span class="s">    statement is true or false.</span>
<span class="s">Here are some facts and rules:</span>
<span class="s">Nice people are quiet.</span>
<span class="s">If Dave is smart then Dave is nice.</span>
<span class="s">All white people are smart.</span>
<span class="s">Dave is smart.</span>
<span class="s">Harry is cold.</span>
<span class="s">Does it imply that the statement &quot;</span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">quiet</span><span class="err">&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="kc">true</span><span class="p">?</span>
<span class="nx">Reasoning</span><span class="p">:</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">smart</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">nice</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span><span class="w"> </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Dave</span><span class="w"> </span><span class="k">is</span>
<span class="w">    </span><span class="nx">smart</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span>
</code></pre></div>

<p>Here are some facts and rules:
Blue things are green.
All blue things are white.
If Anne is not big then Anne is blue.
Big things are white.
All kind things are round.
If something is white and big then it is not kind.
If something is big and not rough then it is green.
If something is white and blue then it is not green.
Erin is not white.
Anne is big.
Bob is rough.
Anne is white</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Does</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="nx">imply</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">statement</span><span class="w"> </span><span class="s">&quot;Anne is kind&quot;</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">?</span>
<span class="nx">Reasoning</span><span class="p">:</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">something</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">white</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">big</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">kind</span><span class="p">.</span><span class="w"> </span><span class="nx">We</span>
<span class="w">    </span><span class="nx">know</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">Anne</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">white</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Anne</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">big</span><span class="p">.</span><span class="w"> </span><span class="nx">Therefore</span><span class="p">,</span>
</code></pre></div>

<p>. .</p>
<p>Here are some facts and rules:
If something likes the squirrel and it is not young then it chases the lion.
If something likes the squirrel then it is rough.
If something chases the rabbit and the rabbit is not young then it chases the lion.
If something eats the lion then it is young.
If something likes the rabbit then it chases the rabbit.
All rough things are nice.
the rabbit is young.
the squirrel likes the rabbit.
the lion likes the squirrel.
Does it imply that the statement "The lion is not nice" is True?
Reasoning: " " "
Example inference prompt:
" " "
Nice people are quiet. We know that Dave is nice. Therefore, Dave is quiet.</p>
<p>If the cow chases the bald eagle then the cow eats the bald eagle. We know that the cow chases the bald eagle. Therefore""</p>
<h1>D. Selection-Inference evaluation details</h1>
<h2>D.1. Selection module</h2>
<p>The algorithm for the Scoring Selection module is shown in Algorithm 2.
Algorithm 2 Scoring Selection_Module
Require: An n-shot prompt, $p$.
Require: Initial Context, $C^{0}$, made up of facts (and rules).
Require: The question, $q$.
Require: Language model, LLM.
Require: A halting function, halt, determines if the selection is complete. $s^{t} \leftarrow$ empty string
while not halt() do
$s_{\text {temp }} \leftarrow \arg \max <em _text="\text" _token="{token">{\text {rule_or_fact } \in C} \sum</em>\right)\right.$ $\triangleright$ Choose the rule or fact with the maximum log-likelihood under the LLM model.
$s^{t} \leftarrow \operatorname{join}\left(s^{t}, s_{\text {temp }}\right) \quad$ Join the selected fact or rule to the selection string. end whilereturn $s^{t}$} \in \text { rule_or_fact }} \operatorname{LLM}\left(\operatorname{token}\left|p, C^{t}, q, s^{t</p>
<h1>D.2. Inference module</h1>
<p>To extract the new fact to be added to the context we filter out the first sentence of the text generated by the LLM using the following regular expression: $r^{\prime}\left[{ }^{\wedge} . ?!\left\lfloor\right.\right.$ : $\left.\left.\backslash \mathrm{n}\right]+\right.$ '.</p>
<h2>D.3. bAbI</h2>
<p>For all bAbI tasks, the answer is a single word. For example, in bAbI 1-3 the answer is one of ["hallway", "bathroom", "bedroom", "garden", "kitchen", "office"]; for bAbI 15 the answer is one of ["sheep", "cat", "mouse", "wolf"] and for bAbI 16 the answer is one of ["yellow", "gray", "green", "white"]. However, our model outputs a complete sentence, for example "emily is afraid of mice". Therefore, we take the answer to be the final word output by the inference model on its last step.</p>
<p>To obtain the results for bAbI tasks 1-3, 15 and 16 shown in Fig. 4b we prompted the language model to solve the problem in a single step of reasoning. An example of such a prompt is shown in Sec. C.1.</p>
<p>We run the SI model for only a single step of reasoning too. Additional steps may increase the chance of the model reaching the correct answer, however, we do not yet have a mechanism for halting reasoning when the answer is reached.</p>
<p>BAbI 16 is an inductive reasoning task that could be solved in two steps (rather than one). The first step requires a rule to be inferred and the second step requires the inferred rule to be applied to another fact. For this reason, we also apply SI for two steps to solve the bAbI 16 problems, first inferring a rule from a number of facts and then applying the rule to the correct fact. An example of this is shown in Fig. 2. Using this two step approach, we can see exactly which facts contributed to the formation of a new rule.</p>
<h2>D.4. Proof Writer</h2>
<p>We use a subset of the Proof Writer Open World Assumption, OWA, dataset. In the Close World Assumption dataset, CWA, everything that can be proven is True otherwise it is False. This means things are either True or False. This also means that reasoning traces are only provided when a statement is True, but not when a statement is False. To "show" something is False one has to enumerate all possible facts (possibly up to a certain depth) and then if a statement is not shown to be True it is assumed to be False. It is therefore not simple to generate meaningful reasoning traces for these types of problems.</p>
<p>On the other hand, in the OWA data if it is not possible to prove something is True or False, then it is Unknown. This means that for True and False examples, where one may want to show $p(x)$, reasoning traces are available that terminate in $\mathrm{p}(\mathrm{x})$ (for True) or not $\mathrm{p}(\mathrm{x})$ (for False). If one cannot show $p(x)$ or not $p(x)$ then the answer is Unknown, and again there is not a clear reasoning trace for this; it is necessary to enumerate all possible facts (possibly up to a certain depth) and then if one has not shown $p(x)$ or not $p(x)$ it's considered Unknown. Note that here $p$ is a predicate and $x$ is a variable.</p>
<p>It is for this reason that we used the Proof Writer OWA dataset and removed the Unknowns; this gives us a dataset with reasoning traces concluding in either True or False. If we used CWA we would only have traces that could conclude True.</p>
<p>We evaluate the SI on 5 tasks from the Proof Writer dataset, each requiring varying numbers of reasoning steps ( $1,2,3$ and 5 ). This requires the model to learn to compute intermediate conclusions</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure S1 | Proof Writer: effect of additional reasoning steps. With additional iterations of selection and inference the probability of the model producing the correct answer increases.
that may not directly lead to the final answer, but may be needed to reach the final answer. While, this can be very hard to achieve using prompt engineering alone, we endeavour to do so, by demonstrating examples of intermediate and final steps of reasoning (for problems of depth $&gt;1$ ). This means that the language model sees (1) examples that both encourage the model to select rules and facts that may not answer the problem in one-step but may help the model to obtain an intermediate output that can be used in a future step and (2) examples of the final step, which takes the model to the final answer. See Sec. C. 2 for an example prompt.</p>
<p>The Proof Writer tasks involve predicting if a given statement, for example "Bob is nice.", is True or False given the context of facts and rules. Our SI model attempts to derive the statement "Bob is nice." or the negation of the statement "Bob is not nice." from the context. To assign a label True or False we follow the procedure proposed in the original Proof Writer paper (Tafjord et al., 2020) and test if any of the implications matches the given statement. If there is a match, the statement is considered to be True, otherwise False.</p>
<p>Proof Writer results in Fig. 4b show that the Selection-Inference model outperforms the baselines for problems of depth zero and one, however, with increasing depth, the gap between SI and the baselines diminishes. This is in part because prompt engineering is not sufficient to obtain an optimal Selection module.</p>
<p>Another challenge with the Proof Writer dataset is deciding how many arguments should be selected for each rule. In the Proof Writer dataset, some rules take one argument, others take two. We experimented with various different ways to encourage the model to stop selecting arguments. For example, we append ". Therefore, " as a choice to the context that the model can select. If the language model selects ". Therefore, " then the selection step ends. We allowed a maximum of two facts to be selected.</p>
<p>To obtain results in Fig. 4b we run SI model for the minimum number of steps needed to solve the problem; a Depth $d$ problem is run for $d$ steps, with the exception of the depth-0 problem which is run for 1 step. However, models may perform better when allowed to run for additional steps, in the case where the model makes a mistake on one step, but later recovers. Fig. S1 shows how the number of SI steps can lead to improved performance. There is greater improvement to performance</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ This could suggest that the 280B LLM has stronger priors, than the 7B LLM, which it favours over logical reasoning. For example, favouring sheep are afraid of wolves despite a context to the contrary (Min et al., 2022). However this requires further investigation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1696 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1696</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1696</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-268248960</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.03017v1.pdf" target="_blank">OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following</a></p>
                <p><strong>Paper Abstract:</strong> Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through ego-centric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components—ranging from visual perception to action execution—on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1696.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1696.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (in OPEx)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used as planner/executor in OPEx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained language model used in OPEx as the LLM-based planner and executor (and GPT-3.5-turbo as observer) to provide few-shot planning, reasoning traces, and skill selection when solving embodied instruction-following tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4 (as LLM-based planner & executor in OPEx)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>GPT-4 is used (via in-context learning / chain-of-thought prompts) as the planner to decompose high-level instructions into subtasks and as the executor to produce reasoning traces and select high-level skills from a predefined skill library; the executor is prompted to interleave Thought and Action (ReAct-style) and outputs skills which are then deterministically mapped to low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>large-scale language corpora / general web text (pretrained LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper (the paper cites GPT-4 technical report but does not list GPT-4's pretraining dataset or sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFRED (3D) and ALFWorld (text counterpart)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>ALFRED: a 3D simulated household environment with long-horizon instruction-following tasks (seven task types such as Pick & Place, Clean & Place, etc.) where the agent receives egocentric RGB observations, must navigate (discrete navigation actions) and perform interaction actions (object mask-based interactions), and episodes terminate on goal completion or max steps. ALFWorld: a text-based counterpart aligning textual world descriptions and actions for similar household tasks, used to ablate perception/action and to collect world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>When used with ALFWorld or text prompts the LLM sees natural language observations and issues text-like high-level commands / plans (natural language tokens / textual descriptions and skill-level plans); during pretraining the model is a language model with token-prediction as its objective.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions (RotateLeft/Right, MoveAhead, LookUp/Down) and discrete interaction actions (PickupObject, PutObject, OpenObject, CloseObject, ToggleObjectOn/Off, SliceObject). High-level skills (e.g., NavigateToObject, Explore, LookAround) are sampled/selected by the LLM and converted to these low-level discrete actions by a deterministic action policy.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM outputs a chosen skill from a predefined skill library with an optional target (e.g., NavigateToObject, Table). A deterministic action policy maps that skill to a navigation/interaction goal (semantic-map based goal selection, Fast Marching Method for path planning, heuristics such as 'traversable goal sampling' and 'slice replay') and then to discrete low-level actions. The mapping is explicit: LLM -> [skill, target] -> deterministic policy -> low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB egocentric images, depth estimation (UNet or ZoeDepth), instance segmentation (Mask R-CNN or SOLQ), and a semantic map constructed from point cloud/voxel aggregation; perception noise is highlighted as a major bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>OPEx (LLM-centric with GPT-4 planner/executor) substantially outperforms the FILM baseline; the paper reports OPEx achieves an absolute gain in Success Rate (SR) of +17.74% on test-seen and +16.78% on test-unseen splits relative to FILM (exact per-split SR numbers are reported in the paper's tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline FILM (uses BERT models trained on the full in-domain planner training set and deterministic policies) — used as the comparison baseline in the paper. The paper reports lower SR for FILM; exact baseline SR values are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>OPEx uses in-context learning for the planner with examples collected from only a small fraction of in-domain episodes; the paper states OPEx's planner used <10% of the data used to train FILM's planner (i.e., fewer in-domain examples for the planning module). Exact episode counts are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>FILM's planner components (BERT-based language processors) are trained on the full in-domain planner training data (100% used in this comparison); exact episode counts are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Reported as large: OPEx attains better task performance while using less than 10% of the in-domain data used by FILM's planner (roughly an order-of-magnitude reduction in in-domain planning examples for comparable or better SR).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>LLM commonsense and reasoning ability, in-context learning (few-shot examples), chain-of-thought prompts, ReAct-style interleaved reasoning/action for the executor, incorporation of prior/world knowledge (when available), and a modular skill library that constrains/anchors LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Large gap in perception and low-level action execution—noisy depth and segmentation impede mapping textual semantics to actual object locations; action-space mismatch (language-level plans vs. precise egocentric interaction primitives); inference overhead and compute costs from relying heavily on external LLM API calls; limited prior environmental knowledge without explicit exploration or ALFWorld-derived priors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained LLMs (GPT-4 / GPT-3.5) can be successfully repurposed as planners and executors for 3D embodied instruction-following tasks via in-context learning and a constrained skill interface, producing large SR gains over a strong baseline while needing far less in-domain planner training data; however, perceptual noise and low-level action grounding remain primary bottlenecks and limit end-to-end transfer effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1696.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1696.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPEx-L (ALFWorld prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OPEx with prior knowledge learned from ALFWorld (text-based world knowledge integration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OPEx variant that collects action–observation sequences by exploring ALFWorld (the text-based counterpart), extracts world knowledge candidates, filters and integrates that prior knowledge into multi-agent prompts to improve planning and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OPEx-L (OPEx augmented with prior knowledge mined from ALFWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>OPEx-L augments the OPEx framework by (1) running an explorer in ALFWorld to collect action–observation sequences, (2) summarizing these sequences into candidate world-knowledge facts, (3) filtering contradictory/duplicate facts, and (4) injecting the final prior knowledge into multi-agent (reasoner + actor) prompt templates to specialize planning and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text-based simulated environment (ALFWorld) — action–observation sequences / textual environment traces</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>World knowledge is summarized from action–observation sequences {AO_i} collected by exploring ALFWorld; the paper does not provide exact counts or dataset size for the collected sequences (the procedure and filtering are described but numeric dataset sizes are not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFWorld (primary for prior collection) and used to inform OPEx behavior on embodied tasks (ALFRED experiments and ALFWorld experiments reported separately).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>ALFWorld: a text-based counterpart to ALFRED where observations and state changes are expressed in text; used to collect grounded, textual world-knowledge that can be integrated into LLM prompts. ALFRED remains the 3D embodied benchmark of interest; ALFWorld is used as a text training bed to obtain priors.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>In ALFWorld, actions and observations are in natural language sequences (textual action-observation pairs), i.e., high-level textual commands and textual state descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>When transferred to ALFRED the embodied action space is discrete navigation (RotateLeft/Right, MoveAhead, etc.) and interaction actions (PickupObject, PutObject, OpenObject, CloseObject, ToggleOn/Off, SliceObject); OPEx-L uses the same skill-library -> deterministic action-policy conversion as OPEx.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Knowledge-mining: textual world-knowledge extracted from ALFWorld is added to prompt context for the reasoner/actor LLM roles; these LLM outputs still map to the same skill-library and deterministic action policy in the 3D environment (i.e., no learned low-level mapping — mapping remains a prompt-driven LLM -> skill -> deterministic policy pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>When applied to ALFRED the same perception stack is required (RGB, depth estimation, instance segmentation, semantic map). The ALFWorld-derived priors are specifically helpful because they remove perception noise during prior collection (text-only environment).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>The paper reports that adding higher-quality prior knowledge from ALFWorld/humans (OPEx-L and OPEx-H) increases system performance on ALFWorld; performance grows with the quality of prior knowledge and multi-agent dialogue. Exact numeric SR improvements for OPEx-L are reported in the paper tables (see Table 2, 'Influence of prior knowledge (On ALFWorld)').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>OPEx without ALFWorld-derived priors (vanilla OPEx) shows lower performance on ALFWorld; exact baseline numbers are provided in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>The paper does not provide exact counts of ALFWorld episodes required to collect priors; it states that an explorer collects action-observation sequences and that filtered priors materially improve performance. Exact sample numbers are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not specified numerically in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitatively: integrating ALFWorld-derived priors reduces repetitive errors and improves grounded exception handling, improving effective sample efficiency for solving tasks in ALFWorld/when grounding to embodied settings; the paper does not provide a quantified multiplier (e.g., x-fold) for sample-efficiency gain.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Semantic alignment between ALFWorld textual experiences and the target task domain, multi-agent decomposition (reasoner + actor) that allows specialization, and filtered, grounded prior knowledge preventing repetitive mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations include imperfect bridging to perception-heavy 3D tasks (perceptual noise and action execution mismatch remain bottlenecks), and lack of exact quantitative priors or coverage gaps in the collected ALFWorld experience.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-only exploration (ALFWorld) can produce compact, grounded world knowledge that, when filtered and injected into multi-agent LLM prompts, improves planning/grounding and reduces repetitive errors; however, gains are constrained by perception and low-level action execution when moving to full 3D embodied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 2)</em></li>
                <li>FILM: Following instructions in language with modular methods <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models (Yao et al., 2022) <em>(Rating: 2)</em></li>
                <li>LLM-Planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 1)</em></li>
                <li>Prompter: Utilizing large language model prompting for a data efficient embodied instruction following <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1696",
    "paper_id": "paper-268248960",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPT-4 (in OPEx)",
            "name_full": "Generative Pre-trained Transformer 4 (used as planner/executor in OPEx)",
            "brief_description": "A large pre-trained language model used in OPEx as the LLM-based planner and executor (and GPT-3.5-turbo as observer) to provide few-shot planning, reasoning traces, and skill selection when solving embodied instruction-following tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "GPT-4 (as LLM-based planner & executor in OPEx)",
            "model_agent_description": "GPT-4 is used (via in-context learning / chain-of-thought prompts) as the planner to decompose high-level instructions into subtasks and as the executor to produce reasoning traces and select high-level skills from a predefined skill library; the executor is prompted to interleave Thought and Action (ReAct-style) and outputs skills which are then deterministically mapped to low-level actions.",
            "pretraining_data_type": "large-scale language corpora / general web text (pretrained LLM)",
            "pretraining_data_details": "Not specified in this paper (the paper cites GPT-4 technical report but does not list GPT-4's pretraining dataset or sizes).",
            "embodied_task_name": "ALFRED (3D) and ALFWorld (text counterpart)",
            "embodied_task_description": "ALFRED: a 3D simulated household environment with long-horizon instruction-following tasks (seven task types such as Pick & Place, Clean & Place, etc.) where the agent receives egocentric RGB observations, must navigate (discrete navigation actions) and perform interaction actions (object mask-based interactions), and episodes terminate on goal completion or max steps. ALFWorld: a text-based counterpart aligning textual world descriptions and actions for similar household tasks, used to ablate perception/action and to collect world knowledge.",
            "action_space_text": "When used with ALFWorld or text prompts the LLM sees natural language observations and issues text-like high-level commands / plans (natural language tokens / textual descriptions and skill-level plans); during pretraining the model is a language model with token-prediction as its objective.",
            "action_space_embodied": "Discrete navigation actions (RotateLeft/Right, MoveAhead, LookUp/Down) and discrete interaction actions (PickupObject, PutObject, OpenObject, CloseObject, ToggleObjectOn/Off, SliceObject). High-level skills (e.g., NavigateToObject, Explore, LookAround) are sampled/selected by the LLM and converted to these low-level discrete actions by a deterministic action policy.",
            "action_mapping_method": "LLM outputs a chosen skill from a predefined skill library with an optional target (e.g., NavigateToObject, Table). A deterministic action policy maps that skill to a navigation/interaction goal (semantic-map based goal selection, Fast Marching Method for path planning, heuristics such as 'traversable goal sampling' and 'slice replay') and then to discrete low-level actions. The mapping is explicit: LLM -&gt; [skill, target] -&gt; deterministic policy -&gt; low-level actions.",
            "perception_requirements": "RGB egocentric images, depth estimation (UNet or ZoeDepth), instance segmentation (Mask R-CNN or SOLQ), and a semantic map constructed from point cloud/voxel aggregation; perception noise is highlighted as a major bottleneck.",
            "transfer_successful": true,
            "performance_with_pretraining": "OPEx (LLM-centric with GPT-4 planner/executor) substantially outperforms the FILM baseline; the paper reports OPEx achieves an absolute gain in Success Rate (SR) of +17.74% on test-seen and +16.78% on test-unseen splits relative to FILM (exact per-split SR numbers are reported in the paper's tables).",
            "performance_without_pretraining": "Baseline FILM (uses BERT models trained on the full in-domain planner training set and deterministic policies) — used as the comparison baseline in the paper. The paper reports lower SR for FILM; exact baseline SR values are in the paper tables.",
            "sample_complexity_with_pretraining": "OPEx uses in-context learning for the planner with examples collected from only a small fraction of in-domain episodes; the paper states OPEx's planner used &lt;10% of the data used to train FILM's planner (i.e., fewer in-domain examples for the planning module). Exact episode counts are not specified.",
            "sample_complexity_without_pretraining": "FILM's planner components (BERT-based language processors) are trained on the full in-domain planner training data (100% used in this comparison); exact episode counts are not specified in this paper.",
            "sample_complexity_gain": "Reported as large: OPEx attains better task performance while using less than 10% of the in-domain data used by FILM's planner (roughly an order-of-magnitude reduction in in-domain planning examples for comparable or better SR).",
            "transfer_success_factors": "LLM commonsense and reasoning ability, in-context learning (few-shot examples), chain-of-thought prompts, ReAct-style interleaved reasoning/action for the executor, incorporation of prior/world knowledge (when available), and a modular skill library that constrains/anchors LLM outputs.",
            "transfer_failure_factors": "Large gap in perception and low-level action execution—noisy depth and segmentation impede mapping textual semantics to actual object locations; action-space mismatch (language-level plans vs. precise egocentric interaction primitives); inference overhead and compute costs from relying heavily on external LLM API calls; limited prior environmental knowledge without explicit exploration or ALFWorld-derived priors.",
            "key_findings": "Pretrained LLMs (GPT-4 / GPT-3.5) can be successfully repurposed as planners and executors for 3D embodied instruction-following tasks via in-context learning and a constrained skill interface, producing large SR gains over a strong baseline while needing far less in-domain planner training data; however, perceptual noise and low-level action grounding remain primary bottlenecks and limit end-to-end transfer effectiveness.",
            "uuid": "e1696.0",
            "source_info": {
                "paper_title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "OPEx-L (ALFWorld prior)",
            "name_full": "OPEx with prior knowledge learned from ALFWorld (text-based world knowledge integration)",
            "brief_description": "An OPEx variant that collects action–observation sequences by exploring ALFWorld (the text-based counterpart), extracts world knowledge candidates, filters and integrates that prior knowledge into multi-agent prompts to improve planning and grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "OPEx-L (OPEx augmented with prior knowledge mined from ALFWorld)",
            "model_agent_description": "OPEx-L augments the OPEx framework by (1) running an explorer in ALFWorld to collect action–observation sequences, (2) summarizing these sequences into candidate world-knowledge facts, (3) filtering contradictory/duplicate facts, and (4) injecting the final prior knowledge into multi-agent (reasoner + actor) prompt templates to specialize planning and grounding.",
            "pretraining_data_type": "text-based simulated environment (ALFWorld) — action–observation sequences / textual environment traces",
            "pretraining_data_details": "World knowledge is summarized from action–observation sequences {AO_i} collected by exploring ALFWorld; the paper does not provide exact counts or dataset size for the collected sequences (the procedure and filtering are described but numeric dataset sizes are not specified).",
            "embodied_task_name": "ALFWorld (primary for prior collection) and used to inform OPEx behavior on embodied tasks (ALFRED experiments and ALFWorld experiments reported separately).",
            "embodied_task_description": "ALFWorld: a text-based counterpart to ALFRED where observations and state changes are expressed in text; used to collect grounded, textual world-knowledge that can be integrated into LLM prompts. ALFRED remains the 3D embodied benchmark of interest; ALFWorld is used as a text training bed to obtain priors.",
            "action_space_text": "In ALFWorld, actions and observations are in natural language sequences (textual action-observation pairs), i.e., high-level textual commands and textual state descriptions.",
            "action_space_embodied": "When transferred to ALFRED the embodied action space is discrete navigation (RotateLeft/Right, MoveAhead, etc.) and interaction actions (PickupObject, PutObject, OpenObject, CloseObject, ToggleOn/Off, SliceObject); OPEx-L uses the same skill-library -&gt; deterministic action-policy conversion as OPEx.",
            "action_mapping_method": "Knowledge-mining: textual world-knowledge extracted from ALFWorld is added to prompt context for the reasoner/actor LLM roles; these LLM outputs still map to the same skill-library and deterministic action policy in the 3D environment (i.e., no learned low-level mapping — mapping remains a prompt-driven LLM -&gt; skill -&gt; deterministic policy pipeline).",
            "perception_requirements": "When applied to ALFRED the same perception stack is required (RGB, depth estimation, instance segmentation, semantic map). The ALFWorld-derived priors are specifically helpful because they remove perception noise during prior collection (text-only environment).",
            "transfer_successful": true,
            "performance_with_pretraining": "The paper reports that adding higher-quality prior knowledge from ALFWorld/humans (OPEx-L and OPEx-H) increases system performance on ALFWorld; performance grows with the quality of prior knowledge and multi-agent dialogue. Exact numeric SR improvements for OPEx-L are reported in the paper tables (see Table 2, 'Influence of prior knowledge (On ALFWorld)').",
            "performance_without_pretraining": "OPEx without ALFWorld-derived priors (vanilla OPEx) shows lower performance on ALFWorld; exact baseline numbers are provided in the paper tables.",
            "sample_complexity_with_pretraining": "The paper does not provide exact counts of ALFWorld episodes required to collect priors; it states that an explorer collects action-observation sequences and that filtered priors materially improve performance. Exact sample numbers are not specified.",
            "sample_complexity_without_pretraining": "Not specified numerically in the paper.",
            "sample_complexity_gain": "Qualitatively: integrating ALFWorld-derived priors reduces repetitive errors and improves grounded exception handling, improving effective sample efficiency for solving tasks in ALFWorld/when grounding to embodied settings; the paper does not provide a quantified multiplier (e.g., x-fold) for sample-efficiency gain.",
            "transfer_success_factors": "Semantic alignment between ALFWorld textual experiences and the target task domain, multi-agent decomposition (reasoner + actor) that allows specialization, and filtered, grounded prior knowledge preventing repetitive mistakes.",
            "transfer_failure_factors": "Limitations include imperfect bridging to perception-heavy 3D tasks (perceptual noise and action execution mismatch remain bottlenecks), and lack of exact quantitative priors or coverage gaps in the collected ALFWorld experience.",
            "key_findings": "Text-only exploration (ALFWorld) can produce compact, grounded world knowledge that, when filtered and injected into multi-agent LLM prompts, improves planning/grounding and reduces repetitive errors; however, gains are constrained by perception and low-level action execution when moving to full 3D embodied settings.",
            "uuid": "e1696.1",
            "source_info": {
                "paper_title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 2,
            "sanitized_title": "alfred_a_benchmark_for_interpreting_grounded_instructions_for_everyday_tasks"
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "FILM: Following instructions in language with modular methods",
            "rating": 2,
            "sanitized_title": "film_following_instructions_in_language_with_modular_methods"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models (Yao et al., 2022)",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models_yao_et_al_2022"
        },
        {
            "paper_title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2,
            "sanitized_title": "llmplanner_fewshot_grounded_planning_for_embodied_agents_with_large_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Prompter: Utilizing large language model prompting for a data efficient embodied instruction following",
            "rating": 1,
            "sanitized_title": "prompter_utilizing_large_language_model_prompting_for_a_data_efficient_embodied_instruction_following"
        }
    ],
    "cost": 0.014633,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following
5 Mar 2024</p>
<p>Haochen Shi haochen.shi@umontreal.ca 
Université de Montréal &amp; Mila
MontréalCanada</p>
<p>Zhiyuan Sun zhiyuan.sun@umontreal.ca 
Université de Montréal &amp; Mila
MontréalCanada</p>
<p>Xingdi Yuan 
Microsoft Research
MontréalCanada</p>
<p>Marc-Alexandre Côté 
Microsoft Research
MontréalCanada</p>
<p>Bang Liu bang.liu@umontreal.ca 
Université de Montréal &amp; Mila
MontréalCanada</p>
<p>CIFAR AI Chair
Canada</p>
<p>OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following
5 Mar 2024A539692696F55F9EF746ABDFAD887A7AarXiv:2403.03017v1[cs.AI]
Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions.Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF.Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components-ranging from visual perception to action execution-on task performance.To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor.Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance.Furthermore, we innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld counterpart, further enhancing task performance.Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and lowlevel action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.</p>
<p>Introduction</p>
<p>Embodied learning, particularly through tasks like Embodied Instruction Following (EIF) (Shridhar et al., 2020a), stands at the forefront of artificial intelligence research.EIF, where agents must interpret natural language instructions to navigate and act within their environment using egocentric observations, epitomizes the challenge of integrating cognitive understanding with physical action.This intersection is crucial for developing autonomous agents capable of nuanced interaction with complex, real-world environments, marking a significant stride towards more advanced and versatile AI systems.As the research community harnesses advancements in deep learning, we edge closer to this ambition (Baker et al., 2022;Min et al., 2021;Inoue and Ohashi, 2022;Huang et al., 2022a).</p>
<p>Traditional approaches to Embodied Instruction Following (EIF) often rely on expert-generated annotations, a process that can be both expensive and challenging to scale for real-world applications.In contrast, Large Language Models (LLMs), such as those cited in recent studies (Inoue and Ohashi, 2022;OpenAI, 2023;Wei et al., 2022a;Driess et al., 2023;Touvron et al., 2023;Huang et al., 2022a,b;Liang et al., 2022;Wang et al., 2023a;Shinn et al., 2023;Song et al., 2023), have emerged as a potent alternative, showcasing exceptional capabilities in natural language understanding and generation.These models, enriched by extensive textual datasets, demonstrate significant commonsense reasoning abilities.As a result, there's a growing trend towards leveraging LLM-centric architectures for embodied learning tasks including EIF, which promise to simplify planning and execution tasks through a few-shot learning paradigm.However, despite their potential, the implementations of EIF systems introduce a variety of designs and components across different studies (Min et al., 2021;Inoue and Ohashi, 2022;Song et al., 2023;Blukis et al., 2022;Wang et al., 2023a;Zhu et al., 2023).There remains a notable gap in systematically understanding how these disparate elements influence overall task performance, underscoring the need for a thorough analysis of LLM-centric methods within the context of EIF.</p>
<p>In addressing the complexities of Embodied Instruction Following (EIF), we introduce OPEx, a novel framework designed to systematically outline the essential components for mastering embodied learning tasks.OPEx is segmented into three core parts: Observer, Planner, and Executor.The Observer component is tasked with processing and interpreting sensory inputs, primarily visual, to construct an actionable understanding of the agent's immediate environment.The Planner dynamically devises strategic plans as subtasks to complete the tasks based on perceptual inputs, effectively bridging the gap between perception and action.Lastly, the Executor is responsible for implementing these plans with a skill library, which translates several re-useable skills into precise, context-aware actions within the environment, ensuring the agent's interactions are both relevant and goal-oriented.This tripartite structure provides a clear delineation of roles within the system, facilitating a granular analysis of how each contributes to the overarching performance of EIF tasks.</p>
<p>To understand the impact of each OPEx component on performance in EIF tasks, we conducted an in-depth analysis.By experimenting with different versions of the Observer, Planner, and Executor components, we assessed how each contributes to and influences overall success.This approach allowed us to identify the key attributes and design choices that enhance the system's ability to tackle complex embodied tasks, providing clear insights into optimizing embodied learning agents.</p>
<p>To further unlock the potential of LLMs in embodied learning, we eliminate the influence of visual perception and low-level action execution of the system utilizing a pure-text counterpart environment (Shridhar et al., 2020b) and further adopt a multi-agent dialogue strategy, splitting the instruction-following challenge into distinct reasoning and grounding roles handled by a reasoner agent and an actor agent, respectively.This dialogue-driven approach simplifies the task into decision-making processes, where both agents utilize world knowledge obtained from an explorer.This explorer gathers insights either through direct interaction with the environment or from human contributions, thereby enriching the collaborative problem-solving capabilities of the reasoner and actor with more grounded and informed decisionmaking.</p>
<p>Our experimental evaluation was conducted using the ALFRED (Shridhar et al., 2020a) and ALF-World (Shridhar et al., 2020b) benchmarks, providing a comprehensive testing ground for our extensive evaluation.The core analysis of our experiments underscores significant advancements: the LLM-centric approach notably enhances performance in EIF tasks.We pinpoint visual perception and low-level action execution as pivotal bottlenecks.Moreover, our results affirm that incorporating a multi-agent dialogue strategy into an LLM-centric task solver significantly boosts overall task performance on AFLWorld, showcasing the effectiveness of our proposed methodology in addressing the complexities of embodied learning tasks.</p>
<p>Task Formulation</p>
<p>We benchmark our method with ALFRED (Shridhar et al., 2020a) and its TextWorld counterpart ALFWorld (Shridhar et al., 2020b).Both contain a set of environments associated with longhorizon household tasks specified by natural language instructions.The language instruction L = {L high , L low } consists of instructions at two different levels: a high-level instruction goal L high that summarizes the task and a sequence of low-level instructions L low that depict the specific actions required.At the time step t, ALFRED also provides a visual egocentric observation V t (text observation L t if on ALFWorld) represents the world state W t .</p>
<p>Given the language instruction L and an initial observation V 0 (L 0 if on ALFWorld), the agent's objective is to generate an execution trajectory T = ⟨V 0 , a 0 , V 1 , a 1 , . . ., V T , a T ⟩, where a t is the action taken by the agent at time step t, and V t+1 is the observation of the world state W t+1 caused by that action.The action space A can be categorized into two classes: navigation actions A N and interaction actions A I , respectively1 .In practice, we follow the setting of FILM (Min et al., 2021), where the navigation actions A N are constrained to discrete values, and a pixel-wise interaction mask of the target object must be specified for interaction actions A I .There are seven types of household tasks, of which each episode is terminated either if an agent meets the goal conditions specified in L (success) or reaches the maximum number of steps (fail).See Appendix.A for a task example in ALFRED.</p>
<p>Methodology</p>
<p>We first provide an overview of the proposed LLMcentric framework (OPEx) in Figure 1.OPEx con-</p>
<p>RGB image from Simulator</p>
<p>Semantic Map</p>
<p>Observer output -------Observation: You are in a kitchen.There are no objects currently in your view.You are not holding any objects.</p>
<p>Executor input -------Observation: You are in a kitchen.There are no objects currently in your view.You are not holding any objects.</p>
<p>Output action(s)</p>
<p>The OPEx framework Planner: decompose a task into sub-tasks.Observer: collect information from environment and prior experience.</p>
<p>Executor: produce actions based on Planner's plan and Observer's observation guidance.sists of six components: (1) A semantic mapping module to transform the egocentric visual observation into a semantic map; (2) An LLM-based planner to decompose the specified language task instruction L into subtasks S = [S 0 , S 1 , ..., S n ];</p>
<p>(3) An LLM-based observer to gather information from the environment and depict the partially observed world state W t at the current time step t as natural language description O L t ; (4) An LLMbased executor to receive the world state description O L t and select skill from a set of pre-defined skills to complete the current subtask S i ; (5) A skill library SL = {sl 0 , sl 1 , ...} to store the skills manipulating the agent in the simulated environment (e.g, NavigateTo, LookAround, and Explore); (6) A deterministic action policy to convert the skills into low-level actions (e.g., RotateRight).</p>
<p>Semantic Mapping Module</p>
<p>The goal of the semantic mapping module is to create a 2D semantic map M t from a top-down perspective (i.e., a map of explored areas, obstacles, and detected objects).At each time step t, this module receives the egocentric visual observation V t of the world state W t as input, which is then processed into a depth map and instance segmentation using a UNet (Ronneberger et al., 2015) and a MaskR-CNN (He et al., 2017) (or ZoeDepth (Bhat et al., 2023) and SOLQ (Dong et al., 2021) as stronger perception models).Following FILM (Min et al., 2021), we use the implementation from (Blukis et al., 2022) for UNet-based depth estimation and (Shridhar et al., 2020b) for MaskRCNN-based instance segmentation.Then, a point cloud is constructed from the depth prediction and instance segmentation.Finally, the point cloud is binned into a voxel representation and then transformed into the 2D semantic map M t by summing over the height of the voxel representation, which is updated and aggregated over time steps.Due to the inherent difficulty in achieving a flawless perceptual model, the resulting semantic map M t often includes noise.This noise has the potential to exacerbate the challenges associated with locating navigational targets and subsequently affect the performance.To address such kind of issues, we introduce a supplementary semantic map denoted as M ′ t , which aggregates the information from M t over successive time steps.The intuition resembles a form of majority voting: when an object is recognized as a fridge across more viewpoints than as a wall, its likelihood of being a fridge over a wall should be proportionally increased.The two semantic maps work in a cascading manner: when the agent tries to identify an object from the maps, the initial search is conducted within M ′ t .M t is only utilized if the object cannot be located within M ′ t .LLM-based Executor Given the current subtask S i , the language description of the world state O L t at time step t, the goal of the LLM-based executor is to complete the subtask S i by iteratively manipulating the agent in the environment with a set of pre-defined skills from a skill library SL.In contrast to the LLM-based planner, which predominantly depends on the reasoning prowess of LLMs, the LLM-based executor is tasked with active engagement within the environment and acquiring an understanding of the environment dynamics (for instance, in ALFRED, objects can be cleaned by placing them into a sink basin and toggling on the faucet) from the feedback.To this end, inspired by ReAct (Yao et al., 2022), we prompt the LLMbased executor (a GPT-4 model) to generate both reasoning traces and action plans (composed of skills in SL), allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from the environment.The input for LLM-based Executor's prompt template is generally composed of the languagebased observation O L t , found objects, objects detected in the current view, short-term memory of the action plan for the current subtask, which is cleared once the current subtask is finished, and the current subtask S i .The LLM-based executor is required to generate both the reasoning traces (the "Thought" part in the Executor's output) and the action plans.The action space of the LLM-based executor is {Play, Finish}, where the action Play is utilized to interact with the environment or request re-planning of the current plan S, and the action Finish is used for finishing the action planning for the current subtask S i .The action Play receives two arguments as the inputs: [SL i , ST ] (e.g., Play[NavigateToObject,Table ]), where SL i is the pre-defined skills in the skill library, and ST is the target argument of the corresponding skill action SL i .</p>
<p>LLM-based Planner</p>
<p>Skill Library We design a skill library to empower the LLM-based executor with the following capabilities: (1) reasoning over language to track progress, handle exceptions or adjust the plan according to the situation; (2) acting to support the reasoning and collect information about the environment dynamics by controlling the agent.Apart from all the interaction actions A I , we have designed several additional skills, including Navigate-ToObject, Explore, LookAround, and RequireReplan.The NavigateToObject skill empowers the LLM-based executor with the capability to set the landmark-based navigation goal, it takes a found object in the room as the skill action target ST .The Explore skill enhances the LLM-based executor's ability to guide the agent in room exploration by sampling navigation goals from traversable areas, and it requires no skill action target.It is worth noting that we have an initial exploration heuristic for the first four calls of the Explore skill, we set the four corners of the room with a higher exploration priority.The RequireReplan provides the LLMbased executor with the capability to dynamically adjust the plan, improving the robustness to exceptions and producing more probability for it to learn from the environment dynamics.The LookAround skill enables the LLM-based executor to manipulate the agent to look around the environment to get a more comprehensive observation of the room.</p>
<p>Deterministic Action Policy</p>
<p>Given the current instruction specified by the action plan [SL i ST ] from the LLM-based executor, the deterministic action policy of OPEx outputs a navigation or interaction action based on a set of heuristics, which is quite similar to that of FILM.Both policies generally follow the following procedure: if the target object is observed in the semantic map, the closest instance is selected as the final navigation goal.Otherwise, the final navigation goal is set as the exploration navigation goal.After goal determination, the agent employs the Fast Marching Method (Sethian, 1996) to navigate towards the navigation goal.Additionally, when the target object is within the agent's egocentric visual range, the policy will try to conduct the interaction or adjust the position to prepare for the interaction action.The deterministic action policy of OPEx mainly differs from that of FILM in three aspects.Firstly, the deterministic action policy of OPEx is equipped with a slice replay heuristic, which tracks the location of successful execution of SliceObject for easier going back.Secondly, instead of directly setting the location of the target object as the navigation goal, we sample a traversable location based on the distance to the target object as the navigation goal (noted as "traversable goal heuristic").Thirdly, instead of directly utilizing the semantic map M t to determine whether the target object is found and get the navigation goal for that object, we adopt the additional semantic map M ′ t to achieve this in the first place.If the target is not found in M ′ t , the original semantic map M t is then utilized.We prioritize M ′ t as it is supposed to be more robust to the errors from the perception models.</p>
<p>Prior Knowledge Integration Due to the lack of prior knowledge of the specific environment, OPEx frequently fails even on ALFWorld where the impact of perception and action modules are ablated.For instance, OPEx may continuously fail for trying to pick up objects across various episodes due to the lack of the knowledge that agent can not directly hold more than 1 object in ALFRED.Furthermore, a system with a single agent trying to handle planning and grounding simultaneously often struggles to learn the optimal timing for switching between planning and grounding.To bridge the gap, we propose improving OPEx by splitting the reasoning and grounding issues with a multi-agent dialogue strategy and marrying it with the world knowledge, which is obtained from an explorer by interacting with the environment or collecting human contributions.Specifically, we first deploy the agent to explore the ALFWorld environment and collect action-observation sequences {AO i }, where AO i = [L 0 , a 0 , L 1 , a 1 , ..., L T ].Then an LLM-based module or human is required to observe the action-observation sequences and summarize the world knowledge learned from {AO i } as prior knowledge candidates {P j }.After that, an LLM-based filter is applied on {P j } to eliminate contradictory and duplicated world knowledge, which results in the final set of world knowledge {P ′ i }.Finally, the world knowledge {P ′ i } is integrated into the prompt templates of the multi-agent dialogue strategy, where a reasoner depicts general plans solving the task and the actor ground the plans as executable actions in the environment.</p>
<p>Experiments and Discussion</p>
<p>Experiment Setup</p>
<p>Evaluation Splits The ALFRED benchmark consists of training, valid, and test sets.Both valid and test sets are composed of seen and unseen splits, where the unseen splits consist of rooms that do not appear in the training set.Following (Yao et al., 2022), we evaluate our methods on 134 unseen evaluation games for the ALFWorld benchmark.</p>
<p>Evaluation Metrics Following (Shridhar et al., 2020a;Min et al., 2021), we report four evaluation metrics on AFLRED: (1) Success Rate (SR);</p>
<p>(2) Goal Condition (GC), the ratio of goal conditions completed at the end of an episode; (3) path length weighted SR (PLWSR), the SR weighted by (path length of the expert trajectory)/(path length taken by the agent); (4) path length weighted GC (PLWGC), the GC weighted by the same factor.Following (Yao et al., 2022), we report SR on ALF-World.</p>
<p>Compared Methods</p>
<p>The compared methods on ALFRED include LAV (Nottingham et al., 2021), where the raw language and visual inputs are transformed into structured forms, with a separate "action prediction module" predicting the low-level actions; HLSM (Blukis et al., 2022), a hierarchical approach that uses semantic voxel map state representation as a long-term memory to solve long-horizon tasks; LGS-RPA (Murray and Cakmak, 2022), which utilizes a Djikstra-based deterministic planner for navigation action generation and introduces landmark-guided search along with the reinforced pose adjustment for navigation goal searching and interaction action preparation respectively; EPA (Liu et al., 2022c), a neuralsymbolic approach with symbolic planning; LLM- Planner (Song et al., 2023), which simply prompts LLMs for task decomposition; FILM (Min et al., 2021), which builds 2D semantic map and performs exploration with a semantic search policy.It is worth noting that there are also several works on the leaderboard reporting high performance that are not included in the comparison (Inoue and Ohashi, 2022;Shridhar et al., 2020a;Chen et al., 2023), this is mainly because we focus on systematically outlining and evaluating the essential components for mastering EIF tasks, while we cannot find the description or available open-source resources of these works when we conduct the experiments.On the ALFWorld benchmark, apart from the variants of OPEx, we also introduce ReAct (Yao et al., 2022) for comparison to demonstrate the effectiveness of the proposed method.</p>
<p>Experimental Results</p>
<p>The main results are illustrated in Table 1.When contrasting OPEx with the baseline FILM, it becomes evident that OPEx exhibits substantial improvement across two distinct environmental settings, encompassing both the goal condition (GC) and the success rate (SR).Notably, OPEx utilizes in-context learning on less than 10% data used for FILMs' planner (Language Processor) training, while OPEx still significantly outperforms FILM.The observation that OPEx achieves 17.74% and 16.78% absolute gain in SR on test seen and unseen split respectively empirically demonstrates the effectiveness of the OPEx framework.However, it is also worth noting that the OPEx is inferior to FILM concerning the path length weighted metrics.This phenomenon could potentially be attributed to the deliberate choice of assigning a higher maximum number of failures to OPEx as compared to FILM.This choice typically leads to the average length of the resulting episodes.The rationale behind this decision was to encourage OPEx to undertake a more extensive exploration, thereby fostering the acquisition of skills in handling a broader range of exceptions arising from both uncommon scenarios and failures.On the other hand, the FILM utilizes two BERT models trained on the whole training set with the template assumption to conduct the task decomposition, while the LLM-based planner can achieve this goal with only a bunch of examples.This phenomenon shows that OPEx works with a much lower demand for in-domain data, making it more feasible in real-world scenarios, where the data collection could be more time-consuming and expensive.Furthermore, the FILM outputs lowlevel navigation and interaction actions solely with a deterministic policy, while OPEx introduces an LLM-based executor accompanying the deterministic policy to release LLMs' potential for robust language grounding and exception handling in the embodiment environment.Overall, the main results empirically demonstrate that it could be feasible to develop embodied experts with low demand for in-domain data by mining LLMs' potential for grounded planning and acting.</p>
<p>Ablation Study and Analysis</p>
<p>To further investigate the bottleneck of the system and the influence of different modules, we conduct several additional ablation studies.</p>
<p>Influence of perception models We first conduct controlled experiments on the valid unseen split of the AFLRED dataset to study the influence of perception models.The corresponding results are illustrated in the first section of Table 2, where OPEx-S denotes the OPEx with stronger perception models (fine-tuned ZoeDepth (Bhat et al., 2023) for depth prediction and SOLQ (Dong et al., 2021) for instance segmentation), OPEx-P denotes the OPEx with perfect ground-truth depth prediction and instance segmentation.The performance gain from the improvement of perception models is very significant, indicating there is much room for improvement regarding the perception models in ALFRED.</p>
<p>Influence of action policies</p>
<p>As shown in the second section of  formance grows as the quality of the prior knowledge increases, this can be empirically explained by the intuition that decomposing EIF tasks via a collaborative multi-agent dialogue strategy helps intra-agent specialization and inter-agent cooperation.Besides, the intuition that the grounded prior knowledge prevents the agents from repetitive errors and facilitates grounded exception handling might also contribute to the results.Furthermore, the performance improvement of ReAct also empirically demonstrates the effectiveness of the proposed method.</p>
<p>Low demand for in-domain data To assess the efficiency of in-domain data usage, we conducted experiments comparing OPEx with the baseline FILM.The FILM is trained on identical data used for in-context learning of OPEx.The corresponding results are presented in to FILM.This controlled study underscores the potential of addressing embodied tasks through an LLM-based framework.This framework achieves low in-domain data demand EIF by integrating feedback mechanisms, closed-loop grounded planning, and action, harmonized with the reasoning and common sense capabilities of Large Language Models (LLMs).Moreover, it also prompts our further exploration into the trade-off between indomain data efficiency and inference overhead, inspiring future directions, such as devising agents that adeptly integrate both common sense and indomain knowledge in a data-efficient manner.</p>
<p>Error mode analysis We conduct the error mode analysis of OPEx on the valid unseen split.The corresponding statics are shown in Table 4.While our approach to calculate the statistics may vary from that of FILM, we have also incorporated FILM's statistics from the original paper (Min et al., 2021) for reference.Since we conduct the task decomposition with the LLM-based planner, which does not follow the template assumption, we don't have statistics on language processing errors.As shown in the table, the goal object not found error typically account for a great ratio of all kinds of error, indicating both FILM and OPEx suffer from imperfect perception models.Besides, the interactive exploration of the LLM-based executor and the deterministic heuristics probably brings a lower error rate of collisions and the error caused by the target object in a closed receptacle.</p>
<p>Related Work</p>
<p>LLM-based Agents Significant progress has been made for LLM-based agents, which mainly focus on the following three aspects.LLM-centric Planning utilizes LLMs to generate plans in dynamic environments.It can be further categorized into methods planning without feedback (Huang et al., 2022a;Fan et al., 2022;Yao et al., 2022;Huang et al., 2022b;Xiang et al., 2023;Lin et al., 2023) and approaches planning with feedback from environment, human, and model (Wang et al., 2023a;Zhu et al., 2023;Shinn et al., 2023;Wang et al., 2023c;Rana et al., 2023;Guan et al., 2023;Kim et al., 2023).LLM-oriented Memory stores information from the environment and boosts agents' capabilities of experience accumulation and selfevolving to facilitate future actions.(Significantgravitas et al., 2023;Shinn et al., 2023;Wang et al., 2023a;Majumder et al., 2023;Wang et al., 2023b) LLM-centric Action Policy grounds the plans made by the agent into feasible action space (Huang et al., 2022a;Schick et al., 2023) Notably, our LLMcentric agent differs from Voyager (Wang et al., 2023a) and GITM (Zhu et al., 2023) by mitigating the instruction grounding problem with dynamically adjusted plans from various granularity based on task-centric feedback from the environment.</p>
<p>Instruction Following in Embodied Environment Prior work on EIF in embodied environments can be categorized into two classes: Supervisely trained end-to-end or modular-based methods that are eager for supervision signals from training data and hard to generalize due to the lack of abstraction and reasoning abilities (Shridhar et al., 2020a;Suglia et al., 2021;Pashevich et al., 2021;Blukis et al., 2022;Singh et al., 2020;Liu et al., 2022a;Min et al., 2021;Sharma et al., 2021), and LLM-based methods that utilizes LLMs' reasoning capability (Inoue and Ohashi, 2022;Song et al., 2023).Different from Prompter (Inoue and Ohashi, 2022) and LLM-Planner (Song et al., 2023), which introduce LLMs only for target location finding and dynamic task decomposition, our method is an LLM-centric framework and decouples reasoning tasks for decision masking problem with multiple LLM-based roles, where the LLMs build the plan, adjust the plan, and ground the plan into structured action spaces.Besides, our method evolves based on the feedback, providing promising future research directions, including human-in-the-loop learning, multi-source feedback mixing and refining, etc.</p>
<p>Conclusion</p>
<p>We introduce OPEx, an LLM-centric framework tailored for Embodied Instruction Following (EIF), and undertake extensive evaluations to dissect the influence of its distinct components.Building on this foundation, we further improve OPEx by integrating world knowledge with a multi-agent dialogue strategy to further harness LLMs' potential in addressing EIF challenges.Our comprehensive analysis reveals that an LLM-centric design significantly enhances EIF performance, pinpointing visual perception and low-level action execution as crucial bottlenecks.Additionally, our findings demonstrate that integrating a multi-agent dialogue mechanism within LLMs markedly boosts their effectiveness, offering promising directions for future research in embodied learning.</p>
<p>Limitations</p>
<p>While our study introduces the OPEx framework and a dialogue-based mechanism for solving EIF tasks, it is not without its limitations.First, the reliance on large language models (LLMs) and the complexity of the multi-agent system introduce challenges in interpretability and computational efficiency.These models demand considerable resources by extensively communicating with ChatGPT, which might limit their applicability in resource-constrained environments.Second, our experiments are conducted within the confines of the ALFRED and ALFWORLD benchmarks, which, while comprehensive, may not encompass all possible real-world scenarios an embodied agent might encounter.Third, the integration of visual perception and action execution as identified bottlenecks suggests that further refinement in these areas is necessary to achieve truly seamless and adaptive embodied AI systems.Future work should aim to address these limitations, exploring more efficient model architectures, broader applicability across diverse environments, and enhanced methods for achieving naturalistic human-agent interaction.</p>
<p>Ethical Concerns</p>
<p>We do not foresee an immediate ethical or societal impact resulting from our work.However, as an LLM application, we acknowledge that OPEx could in some way be affected by various types of hallucinations introduced by the LLMs.We therefore urge researchers and practitioners to use our proposed framework in a mindful way, especially when deploying such LLM-centric agents in real world applications..</p>
<p>B Full Results on AFLRED</p>
<p>The experiment on ALFRED under two different settings are illustrated in Table 5.</p>
<p>C Prompt Examples</p>
<p>In this section, we provide three prompt examples for the LLM-based planner, LLM-based observer, and LLM-based executor respectively.</p>
<p>LLM-based Planner.In Figure 3, we present an illustrative prompt example of the LLM-based planner.The high-level instruction for this instance is "place a washed bowl into a kitchen cabinet."The prompt for the LLM-based planner is constructed to establish the planning task and define the desired output format.Specifically, the input provided to the planner is: "Task: place a washed bowl into a kitchen cabinet."The resulting output encapsulates both the reasoning stages and the path of reasoning undertaken by the LLM-based planner.Given that the foundation of the planner's reasoning prowess lies in its comprehension, we initially expect it to demonstrate a fundamental understanding of the task.This is manifested through the presentation of the task's Task type (in this instance, "PICK_CLEAN_THEN_PLACE_IN_RECEP").Subsequently, drawing inspiration from the concept of Chain-of-Thought Prompting, we introduce a two-step requirement.Firstly, the planner is prompted to generate its Thought process in achieving the task, followed by the presentation of the ultimate Plan to accomplish the specified task.</p>
<p>LLM-based Observer.the corresponding action failure will be gathered by the LLM-centric observer.The designing purpose of the LLM-based observer is not only to gather information but also to serve as a "information gate" which filters out task-irrelevant information and effectively organizes the task-relevant information for better grounded planning and acting.</p>
<p>LLM-based Executor.A prompt example of completing "Explore the room to have a general idea of the environment" is illustrated in Fig. 5. Specifically, the prompt of the LLM-based executor also starts with a setup establishing the execution task and indicating the desired output format.Afterward, the setup is followed by the input to the LLM-centric executor, which consists of Observation: presenting the current language description of the word state generated by the LLM-based observer, Found objects: tracking all the objects detected by the agents, Objects seeing in current observation: noting the objects detected from current egocentric visual observation, Previous steps: tracking the steps taken for the current subtask, and Current objective: specifying the current subtask to complete.Inspired by ReAct, we require the LLM-based executor to generate not only the final skill action plan Action but also the reasoning paths Thought in the first place.</p>
<p>Setup: You are an intelligent planner for a household robot agent in a simulated environment.There are 7 types of household tasks, including: 'PICK_TWO_OBJ_AND_PLACE', 'PICK_CLEAN_THEN_PLACE_IN_RECEP', 'PICK_AND_PLACE_SIMPLE', 'PICK_HEAT_THEN_PLACE_IN_RECEP', 'LOOK_AT_OBJ_IN_LIGHT', 'PICK_AND_PLACE_WITH_MOVABLE_RECEP', and 'PICK_COOL_THEN_PLACE_IN_RECEP'.Let's first understand the task.Then simply answer which type of task the robot is completing.Please output the task type starting with the header 'Task type:'.After that, you will first devise a thought of how to complete the task, then create a stepby-step plan based on all the information to solve the problem.</p>
<p>Please output the thought starting with the header 'Thought:' followed by the thought.</p>
<p>Please output the plan starting with the header 'Plan:' and then followed by a numbered list of steps.At the end of your plan, say '<END_OF_PLAN>'.</p>
<p>Task: put a washed bowl away in a kitchen cabinet.Task type: PICK_CLEAN_THEN_PLACE_IN_RECEP Thought: To complete the task, the robot needs to find a bowl, a sink basin for washing the bowl, and a cabinet to set the bowl in.The most efficient way is to first find and pick a bowl, then find a sink basin to wash the bowl, and finally find a cabinet to put the bowl in.Plan: 0. Explore the room to have a general idea of the environment.1. Find a bowl in the room.2. Pick up the bowl.</p>
<ol>
<li>Find a sink basin.4. Wash the bowl in the sink basin. 5. Find a kitchen cabinet.6.Put the bowl in the cabinet.<END_OF_PLAN> Setup is fixed for all the input test cases, Task is the input to the LLM-based planner that varies for distinct input test cases, Task type, Tought, and Plan are the content required to be generated by the LLM-based planner.The same color mode applies to other figures.</li>
</ol>
<p>You are a helpful assistant helping …. Please output the description of the environment with the header 'Observation:'.Room type: Kitchen.Task description: Explore the room to have a general idea of the environment.Previously found objects: None.(From semantic map) Objects seen in current observation: None.(From RGB image) Holding object: None.Error message: None.</p>
<p>Found objects: None.Objects seeing in current observation: None.Previous steps: steps=[] Current objective: Explore the room to have a general idea of the environment.Executor output -------Thought: I will explore the room to have a general idea of the environment.Action: Play[Explore,None] Planner input -------Setup: You are an intelligent planner for a household robot agent ….Please output the task type starting with the header 'Task type:'.….Please output the plan starting with the header 'Plan' and ….At the end of your plan, say '<END_OF_PLAN>'.Task: put a washed bowl away in a kitchen cabinet.Planner output -------Task type: PICK_CLEAN_THEN_PLACE_IN_RECEP Thought: To complete the task, the robot needs to….Plan: 0. Explore the room to have a general idea of the environment.1. Find a bowl in the room.… 6.Put the bowl in the cabinet.</p>
<p>Figure 1 :
1
Figure 1: Overview of our OPEx framework.We will open-source the code after acceptance.</p>
<p>The goal of the LLMbased planner is to decompose a specified language instruction L into a sequence of subtasks S = [S 0 , S 1 , ..., S n ].In practice, we utilize Chainof-Though (CoT)(Wei et al., 2022b) to promptGPT-4 (OpenAI, 2023)  with in-context learning.The corresponding prompt examples are demonstrated in the Appendix.Example Selector We have collected a set of prompt examples from 10 episodes within the training split for each of the 7 task types, amounting to a total of 70 episodes.As shown in(Liu et al., 2022b), choosing which in-context examples to add to the prompt can impact the overall performance.Therefore, we further apply an example selector to provide the LLM-based planner with the most relevant examples by ranking the examples based on the similarity of the input test case and the examples.In practice, we employ the example selector from LangChain(Chase, 2022), which first ranks the examples based on the corresponding embeddings 2 that have the greatest cosine similarity with the inputs, then select top-K examples for in-context learning.LLM-based ObserverThe goal of the LLMbased observer is to extract information from the environment feedback and the agent state, and present it in the form of a natural language description O L t in a zero-shot manner.The rationale behind the design of the LLM-based observer is twofold: (1) to gather and render the state of the environment, enabling the tracking of environment dynamics across time steps and facilitating dynamic planning and acting; and (2) to summarize the information into a task-centric description, thereby safeguarding the LLM-based executor against distractions and hallucinations.The LLM-based observer is querying GPT-3.5-turbo with the prompt format shown in the Appendix.</p>
<p>Figure 2 :
2
Figure 2: Example of a Clean &amp; Place task in ALFRED.A Task Example in ALFREDAs shown in Figure2, the ALFRED benchmark(Shridhar et al., 2020a) contains a set of environments associated with long-horizon household tasks specified by natural language instructions.As shown in Figure2, the language instruction L = {L high , L low } consists of instructions at two different levels: a high-level instruction goal L high that summarizes the task and a sequence of low-level instructions L low that depict the specific actions required.At the time step t, ALFRED also provides a visual egocentric observation V t represents the world state W t .There are seven types of household tasks in ALFRED, namely Pick &amp; Place, Stack &amp; Place, Pick Two &amp; Place, Clean &amp; Place, Heat &amp; Place, Cool &amp; Place, and Examine in Light.An episode is terminated either if an agent meets the goal conditions specified in L (success) or reaches the maximum number of steps (fail).</p>
<p>Fig. 4 demonstrates two prompt examples for the LLM-based observer.Similar to the prompt design of the LLM-based planner, the prompt for the LLM-based observer also starts with a setup that establishes the observation task.The input to the observer is a set of information collected from the environment, including Room type: indicating which kind of the room the agent is currently in (kitchen, living room, bedroom, or bathroom), Task description: specifying the current subtask (which is generated by the LLM-based planner) to complete, Previously found objects: storing all the objects detected by the agent from the start of the episode to current time step, Objects seen in current observation: pointing out the objects detected in the agent's current egocentric view, Holding object: tracking the object that is currently holden by the agent, and Error message: tracking the error that causes action failures to facilitate exception handling capability of agent.Since successful action in the simulator typically results in the RGB change of the egocentric observation, we can detect action failures by comparing the egocentric observations before and after the execution of the action.If one kind of action failure is detected, then the error message of</p>
<p>Figure 3 :
3
Figure 3: Prompt example of the LLM-based Planner.Setup is fixed for all the input test cases, Task is the input to the LLM-based planner that varies for distinct input test cases, Task type, Tought, and Plan are the content required to be generated by the LLM-based planner.The same color mode applies to other figures.</p>
<p>Table 1 :
1
Main Results on the test splits of ALFRED benchmark.
MethodTest Seen PLWGC GC PLWSR SRTest Unseen PLWGC GC PLWSR SRALFRED (High-level goal instructions only)LAV (Nottingham et al., 2021)13.1823.21 6.3113.3510.4717.27 3.126.38HLSM (Blukis et al., 2022)11.5335.79 6.6925.118.4527.24 4.3416.29LGS-RPA (Murray and Cakmak, 2022) 24.4941.71 16.6533.0120.0138.55 12.9227.80EPA (Liu et al., 2022c)3.4744.14 2.5639.963.9139.54 2.9236.07LLM-Planner (Song et al., 2023)-24.57 -15.33-22.89 -13.41FILM (Min et al., 2021)14.1736.15 10.3925.7713.1334.75 9.6724.46OPEx-S20.1354.27 13.6443.5118.4653.82 12.5741.27</p>
<p>Table 2
2MethodValid Uneen PLWGC GC PLWSR SRInfluence of perception modelsOPEx13.4848.61 9.0835.91OPEx-S16.5251.28 11.3840.80OPEX-P23.7266.17 17.4359.43, we design and conductInfluence of action policiesanother set of controlled experiments to study the influence of distinct deterministic action heuristicsOPEx -semantic map M ′ t -slice replay13.48 12.37 12.6448.61 9.08 45.41 8.06 45.25 8.3535.91 36.17 37.39introduced. It can be seen from the table that set-ting the navigation goal inside the traversable area brings the most significant performance improve--traversable goal Influence of LLM-based modules 11.77 43.49 7.09 OPEx 13.48 48.61 9.08 -Planner 8.10 40.16 5.7234.50 35.91 30.57ment, while slice replay brings marginal improve-ment. Besides, introducing the additional semantic-Observer Influence of prior knowledge (On ALFWorld) 13.41 45.62 8.58 ReAct ---37.76 66map for robust landmark-based navigation goalOPEx---73searching brings moderate performance gain.OPEx-L OPEx-H------78 84Influence of LLM-based modules We first con-duct controlled experiments on the validation un-seen split of the dataset to study the influence ofdifferent modules. The corresponding results areillustrated in Table 2. Significant performancedegradation can be observed when the LLM-basedplanner is removed from the OPEx. This is prob-ably attributed to the fact that the LLM-based ex-ecutor is required to solely perform implicit long-term planning and grounded interaction simulta-neously under this setting. The LLM-based ob-server is designed to gather information and helpthe LLM-based executor to focus on task-relevantinformation by summarizing collected informationand filtering out the task-irrelevant counterparts.However, the ablation study shows that the perfor-mance gain brought by the LLM-based observer ismarginal. This observation can be caused by sev-eral possible reasons, including (1) GPT-4's stronglong text processing capability mitigates the needsof such kind of LLM-based observer; (2) the col-lected information from ALFRED is typically nottoo large/complex to cause severe distraction orhallucination of the LLM-based executor; (3) theobserver utilizes zero-shot prompt, better promptsmay need to be designed.Influence of prior knowledge To further inves-tigate the role of decision-making modules in EIFagents, we conduct experiments on ALFWorld toeliminate the impact of perception models and ac-tion policies. The corresponding results are il-lustrated in the fourth section of Tabel 2, whereOPEx-L denotes the OPEx with prior knowledgelearned from the environment and OPEx-H denotesthe OPEx with prior knowledge provided by hu-mans. With the observation that the system per-</p>
<p>Table 2 :
2
Ablation Studies of OPEx.OPEx-S denotes the OPEx with stronger perception models, OPEx-P denotes the OPEx with perfect ground-truth depth prediction and instance segmentation, OPEx-L denotes the OPEx with prior knowledge learned from the environment, and OPEx-H denotes the OPEx with prior knowledge provided by humans.
Method SRGCPLWSR PLWGCOPEx38.12 46.13 9.0313.45FILM0.0012.18 0.002.78</p>
<p>Table 3 :
3
Performance comparison with the baseline trained on same amount of data.</p>
<p>Table 3 .
3
Our findings indicate that OPEx markedly outperforms FILM across all evaluation metrics in the unseen validation split.Empirically, this suggests that OPEx requires significantly less in-domain data compared
Error modeFILM OPExGoal object not found26.07 27.36Interaction failures8.5412.80Collisions11.00 9.84Object in closed receptacle 16.16 11.61Language processing error 24.54 -Others13.69 38.39</p>
<p>Table 4 :
4
Error mode analysis of OPEx on the valid unseen split.</p>
<p>Table Scene #: 27 One expert annotation High-level instruction:
Scene
Place a clean ladle on a</p>
<p>table Low -level instruction: Ego-centric RGB input:
LowTurn around and veer left to the white table.Pick up the ladle on the table, behind the tomato.Turn right and walk to the sinkt = 28t = 61t = 74Low-level instruction:Wash the ladle in the right sink basin.Turn left and walk back to the white table.Place the ladle on the table in front of the white mug.Ego-centricRGB input:t = 83t = 117t = 164</p>
<p>Table 5 :
5
Main Results on the test splits of ALFRED benchmark.</p>
<p>AN ∈ { RotateRight, RotateLeft, MoveAhead, LookUp, LookDown}, AI ∈ { PickupObject, PutObject, OpenObject, CloseObject, ToggleObjectOn, ToggleObjectOff, SliceObject}.
We adopt the text-embedding-ada-002 embeddings provided by OpenAI.
Setup: You are a helpful assistant helping a robot agent with household tasks in a room simulated by the ALFRED game environment.You'll be given the type of the room, a description of the task, a list of found objects, a list of objects in the current observation, the object held by the robot agent, and the error message of the last interaction action.Your goal is to provide the robot agent with a brief task-centered description of the current state so that it can focus on task-relevant information and not be distracted by irrelevant and hallucinatory information.Please output the description of the environment with the header 'Observation:'.Setup: You are controlling a robot agent in a 3D environment with the function tool named 'Play'.The tool accepts a command as input and returns the observation of the environment after the command is executed by the agent.A valid command is a string of two words separated by a comma.The first word is the name of the action, and the second word is the target of the action.Valid action names are: {ALL_VALID_ACTION_STR}.Valid targets are: {ALL_VALID_DETECTOR_STR}.For example, 'pick up the sliced apple' is not a valid command, but 'PickupObject,AppleSliced' is.You will control the agent to complete a task specified by 'Current objective'.Observation: You are currently in a kitchen.The main object in the room is a shelf.You have previously found a shelf and a salt shaker.You are not currently holding any objects.
Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems. 202235</p>
<p>Zoedepth: Zeroshot transfer by combining relative and metric depth. Farooq Shariq, Reiner Bhat, Diana Birkl, Peter Wofk, Matthias Wonka, Müller, arXiv:2302.122882023arXiv preprint</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, Conference on Robot Learning. PMLR2022</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>Robogpt: an intelligent agent of making embodied long-term decisions for daily instruction tasks. Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Dongbin Zhao, He Wang, arXiv:2311.156492023arXiv preprint</p>
<p>Solq: Segmenting objects by learning queries. Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, Yichen Wei, Advances in Neural Information Processing Systems. 202134</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, arXiv:2206.08853Yuke Zhu, and Anima Anandkumar. 2022. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint</p>
<p>Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.149092023arXiv preprint</p>
<p>Mask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022a</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022barXiv preprint</p>
<p>Prompter: Utilizing large language model prompting for a data efficient embodied instruction following. Yuki Inoue, Hiroki Ohashi, arXiv:2211.032672022arXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. 2022arXiv preprint</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Faeze Yang, Shiyu Brahman, Chandra Huang, Prithviraj Bhagavatula, Yejin Ammanabrolu, Xiang Choi, Ren, arXiv:2305.173902023arXiv preprint</p>
<p>Lebp-language expectation &amp; binding policy: A two-stream framework for embodied visionand-language interaction task learning agents. Haoyu Liu, Yang Liu, Hongkai He, Hangfang Yang, arXiv:2203.046372022aarXiv preprint</p>
<p>What makes good in-context examples for gpt-3? Proceedings of Deep Learning Inside Out. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. 2022b. DeeLIO 2022</p>
<p>A planning based neural-symbolic approach for embodied instruction following. Xiaotian Liu, Hector Palacios, Christian Muise, Interactions. 98172022c</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. Prasad Bodhisattwa, Bhavana Majumder, Peter Dalvi Mishra, Oyvind Jansen, Niket Tafjord, Li Tandon, Chris Zhang, Peter Callison-Burch, Clark, arXiv:2310.101342023arXiv preprint</p>
<p>So Yeon Min, Devendra Singh Chaplot, arXiv:2110.07342Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. 2021. Film: Following instructions in language with modular methods. arXiv preprint</p>
<p>Following natural language instructions for household tasks with landmark guided search and reinforced pose adjustment. Michael Murray, Maya Cakmak, IEEE Robotics and Automation Letters. 732022</p>
<p>Masanori Van-Quang Nguyen, Takayuki Suganuma, Okatani, arXiv:2106.00596Look wide and interpret twice: Improving performance on interactive instructionfollowing tasks. 2021arXiv preprint</p>
<p>Kolby Nottingham, Litian Liang, Daeyun Shin, C Charless, Roy Fowlkes, Sameer Fox, Singh, arXiv:2109.02161Modular framework for visuomotor language grounding. 2021arXiv preprint</p>
<p>Openai, arXivGpt-4 technical report. 2023</p>
<p>Episodic transformer for vision-and-language navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>U-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, GermanySpringer2015. October 5-9, 2015Proceedings, Part III 18</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>A fast marching level set method for monotonically advancing fronts. proceedings of the National Academy of. James A Sethian, Sciences. 9341996</p>
<p>Pratyusha Sharma, Antonio Torralba, Jacob Andreas, arXiv:2110.01517Skill induction and planning with latent language. 2021arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020a</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020barXiv preprint</p>
<p>Significantgravitas/auto-gpt: An experimental opensource attempt to make gpt-4 fully autonomous. Significant-Gravitas, 2023</p>
<p>. Pratap Kunal, Suvaansh Singh, Byeonghwi Bhambri, Roozbeh Kim, Jonghyun Mottaghi, Choi, </p>
<p>arXiv:2012.03208Factorizing perception and policy for interactive instruction following. arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme, arXiv:2108.049272021arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Fan , Anima Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, arXiv:2311.05997Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models. 2023barXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023carXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, arXiv:2305.106262023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao ; Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2210.03629arXiv:2305.17144Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. 2022. 2023. 2020aarXiv preprintMethod Test Seen Test Unseen PLWGC GC PLWSR SR PLWGC GC PLWSR SR High-level Goal Instruction + Low-level step-by-step instructions Seq2Seq</p>
<p>. Moca (singh, 22.05 28.29 15.102020</p>
<p>. E T Pashevich, 15.06 38.51 11.23 27.67 14.30 36.37 10.55 26.492021. 2021. 2021</p>
<p>Nottingham, High-level goal instructions only LAV. 202113</p>
<p>. Hlsm (blukis, 202211</p>
<p>. Lgs-Rpa ( Murray, Cakmak , 24.49 41.71 16.65 33.01 20.01 38.55 12.92 27.802022</p>
<p>. Epa (liu, 2022c</p>
<p>. Llm-Planner ( Song, 24.57 - 15.33 - 22.89 - 13.412023</p>
<p>. Film (min, 20211739</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3669 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3669</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3669</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-269587824</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.02105v1.pdf" target="_blank">Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph</a></p>
                <p><strong>Paper Abstract:</strong> Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers' contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators. We propose using Large Language Models (LLMs) to automatically suggest these properties. However, it's essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application. Our study performs a comprehensive comparative analysis between ORKG's manually curated properties and those generated by the aforementioned state-of-the-art LLMs. We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs. These evaluations occur within a multidisciplinary science setting. Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3669.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3669.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORKG-LLM-Recommender</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Research-Dimension Recommender for the Open Research Knowledge Graph (ORKG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses pretrained LLMs (GPT-3.5, Llama 2, Mistral) to automatically suggest candidate 'research dimensions' (structured contribution properties) for scholarly papers to assist curation in the ORKG; outputs are evaluated against a gold-standard of human-curated ORKG properties using embedding, LLM-based, and human assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-based Research Dimension Recommender for ORKG</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Given a research problem (often a paper title and abstract or research-problem string), the system prompts one of several LLMs (GPT-3.5, Llama 2, Mistral) to produce a list of research dimensions relevant for finding or structuring similar papers; the produced dimensions serve as candidate structured properties for ORKG entries and include optional justifications when chain-of-thought prompting is used.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Gold-standard evaluation corpus created from ORKG: 103 ORKG comparisons comprising 1,317 papers across 35 research fields addressing >150 research problems; comparisons selected required ≥3 properties and ≥5 contributions; dataset publicly available (DOI in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language research-problem string provided to the LLM; in evaluation some generation used title+abstract as input; system prompts ask for 'list dimensions that are relevant to find similar papers' and require output as a Python list. Zero-shot prompting was chosen for production experiments (few-shot and chain-of-thought tested).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Primarily zero-shot prompt engineering to instruct pretrained LLMs to enumerate relevant research dimensions; authors experimented with few-shot and chain-of-thought prompting (examples provided), but settled on zero-shot for the reported experiments. No retrieval-augmentation or multi-document synthesis pipeline is described — LLMs generate dimensions based on the prompt and their internal knowledge (and supplied title/abstract when used).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>A Python-style list of dimension names (strings); optionally per-dimension justifications (when chain-of-thought prompting used) and when shown to users the system displayed 'dimension name, its description, and value' generated by GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Four complementary evaluations: (1) LLM-based semantic alignment and semantic deviation scoring by GPT-3.5 (1–5 scale) comparing lists as wholes; (2) fine-grained property-to-dimension mapping counts using GPT-3.5 to count similar values; (3) SciNCL embeddings-based cosine similarity between ORKG properties and LLM-generated dimensions; (4) human expert survey with domain experts (23 responses) comparing original ORKG properties to GPT-3.5-generated dimensions and rating relevance/utility.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Semantic-alignment (GPT-3.5 evaluator) averaged 2.9/5 (~41.2% normalized alignment) while deviation averaged 4.1/5 (~58.8%); average mapping count between ORKG properties and generated dimensions was low (0.33); average ORKG property count per paper 4.73 vs. average GPT-generated dimension count 8. SciNCL cosine similarities: GPT-3.5 0.84, Mistral 0.79, Llama 2 0.76, indicating substantial embedding-level overlap (GPT-3.5 highest). Human survey: on average 36.3% of generated dimensions were judged highly relevant, 60.9% of respondents would not change their existing ORKG annotations based on suggestions, but 65.2% said having LLM suggestions beforehand would have been helpful; average human alignment rating 2.65/5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Moderate alignment to expert-curated ORKG properties and substantial semantic deviation indicate domain-expert subjectivity and specificity are not fully captured; LLMs generated broader/more diverse dimensions but low fine-grained mapping to per-paper properties; system relies on LLM-encoded knowledge (not document-level retrieval across many papers) unless title/abstract provided; authors note need for fine-tuning on domain-specific scientific data, specificity issues (measurement units, values), and potential misalignment across disciplines; closed-source model access (GPT-3.5) limits introspection and fine-tuning in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared directly to manually curated ORKG properties produced by domain experts (human baseline). Embedding similarity showed LLM outputs (esp. GPT-3.5) have high semantic overlap with human properties, but fine-grained mappings and human willingness to adopt generated properties were limited — participants saw LLMs as useful suggestion tools rather than replacements. No automatic baseline distillation pipeline (e.g., retrieval-augmented synthesis across many full texts) was used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3669.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3669.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciAssess</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciAssesS (Benchmarking LLM proficiency in scientific literature analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced recent preprint that benchmarks large language models' proficiency on tasks related to scientific literature analysis; cited as related work and motivating the use of specialized scientific embeddings and LLM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sciassess: Benchmarking llm proficiency in scientific literature analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SciAssess benchmarking (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced as a benchmarking effort for LLMs on scientific-literature tasks; the paper cites it as part of background motivating evaluation choices (no implementation or usage details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper (details would be in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not described in this paper; general benchmarking on scientific-literature analysis tasks implied by citation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not described here; cited as related benchmarking work rather than used directly.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described here beyond being a benchmarking reference motivating metrics and evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No experimental results from SciAssess are reproduced here; it is cited to position the present work in the landscape of LLM evaluation on scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper (would be in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3669.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3669.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-for-LitReview (Antu2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using LLM (large language model) to improve efficiency in literature review for undergraduate research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited 2023 work that applies LLMs to improve literature-review efficiency for undergraduates; mentioned in related work as an example of LLMs aiding literature review and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using llm (large language model) to improve efficiency in literature review for undergraduate research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-assisted literature-review methods (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced as an application of LLMs to literature review tasks; the present paper cites it as an instance where LLMs help literature analysis but provides no implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified in this paper; likely natural-language prompts and paper collections in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified here (details would be in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Not described here; cited as supporting evidence that LLMs have been used to aid literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3669.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3669.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-feedback-study (Liang et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models provide useful feedback on research papers? A large-scale empirical analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced large-scale empirical analysis studying whether LLMs can provide useful feedback on research papers; cited in related work to motivate the use of LLMs as evaluators and assistants for scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models provide useful feedback on research papers? a large-scale empirical analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM feedback analysis (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Referenced for evidence that LLMs can be used to give feedback on research papers and as part of justification for using LLMs both for generation and as evaluators; no methods from that paper are executed here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not specified here beyond being cited as related empirical work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Not reproduced here; serves as related-work justification for using LLMs as evaluators and assistance tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sciassess: Benchmarking llm proficiency in scientific literature analysis. <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. <em>(Rating: 2)</em></li>
                <li>Using llm (large language model) to improve efficiency in literature review for undergraduate research. <em>(Rating: 2)</em></li>
                <li>SciBERT: A pretrained language model for scientific text. <em>(Rating: 1)</em></li>
                <li>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3669",
    "paper_id": "paper-269587824",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "ORKG-LLM-Recommender",
            "name_full": "LLM-based Research-Dimension Recommender for the Open Research Knowledge Graph (ORKG)",
            "brief_description": "A system that uses pretrained LLMs (GPT-3.5, Llama 2, Mistral) to automatically suggest candidate 'research dimensions' (structured contribution properties) for scholarly papers to assist curation in the ORKG; outputs are evaluated against a gold-standard of human-curated ORKG properties using embedding, LLM-based, and human assessments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "LLM-based Research Dimension Recommender for ORKG",
            "system_or_method_description": "Given a research problem (often a paper title and abstract or research-problem string), the system prompts one of several LLMs (GPT-3.5, Llama 2, Mistral) to produce a list of research dimensions relevant for finding or structuring similar papers; the produced dimensions serve as candidate structured properties for ORKG entries and include optional justifications when chain-of-thought prompting is used.",
            "input_corpus_description": "Gold-standard evaluation corpus created from ORKG: 103 ORKG comparisons comprising 1,317 papers across 35 research fields addressing &gt;150 research problems; comparisons selected required ≥3 properties and ≥5 contributions; dataset publicly available (DOI in paper).",
            "topic_or_query_specification": "Natural-language research-problem string provided to the LLM; in evaluation some generation used title+abstract as input; system prompts ask for 'list dimensions that are relevant to find similar papers' and require output as a Python list. Zero-shot prompting was chosen for production experiments (few-shot and chain-of-thought tested).",
            "distillation_method": "Primarily zero-shot prompt engineering to instruct pretrained LLMs to enumerate relevant research dimensions; authors experimented with few-shot and chain-of-thought prompting (examples provided), but settled on zero-shot for the reported experiments. No retrieval-augmentation or multi-document synthesis pipeline is described — LLMs generate dimensions based on the prompt and their internal knowledge (and supplied title/abstract when used).",
            "output_type_and_format": "A Python-style list of dimension names (strings); optionally per-dimension justifications (when chain-of-thought prompting used) and when shown to users the system displayed 'dimension name, its description, and value' generated by GPT-3.5.",
            "evaluation_or_validation_method": "Four complementary evaluations: (1) LLM-based semantic alignment and semantic deviation scoring by GPT-3.5 (1–5 scale) comparing lists as wholes; (2) fine-grained property-to-dimension mapping counts using GPT-3.5 to count similar values; (3) SciNCL embeddings-based cosine similarity between ORKG properties and LLM-generated dimensions; (4) human expert survey with domain experts (23 responses) comparing original ORKG properties to GPT-3.5-generated dimensions and rating relevance/utility.",
            "results_summary": "Semantic-alignment (GPT-3.5 evaluator) averaged 2.9/5 (~41.2% normalized alignment) while deviation averaged 4.1/5 (~58.8%); average mapping count between ORKG properties and generated dimensions was low (0.33); average ORKG property count per paper 4.73 vs. average GPT-generated dimension count 8. SciNCL cosine similarities: GPT-3.5 0.84, Mistral 0.79, Llama 2 0.76, indicating substantial embedding-level overlap (GPT-3.5 highest). Human survey: on average 36.3% of generated dimensions were judged highly relevant, 60.9% of respondents would not change their existing ORKG annotations based on suggestions, but 65.2% said having LLM suggestions beforehand would have been helpful; average human alignment rating 2.65/5.",
            "limitations_or_challenges": "Moderate alignment to expert-curated ORKG properties and substantial semantic deviation indicate domain-expert subjectivity and specificity are not fully captured; LLMs generated broader/more diverse dimensions but low fine-grained mapping to per-paper properties; system relies on LLM-encoded knowledge (not document-level retrieval across many papers) unless title/abstract provided; authors note need for fine-tuning on domain-specific scientific data, specificity issues (measurement units, values), and potential misalignment across disciplines; closed-source model access (GPT-3.5) limits introspection and fine-tuning in this work.",
            "comparison_to_baselines_or_humans": "Compared directly to manually curated ORKG properties produced by domain experts (human baseline). Embedding similarity showed LLM outputs (esp. GPT-3.5) have high semantic overlap with human properties, but fine-grained mappings and human willingness to adopt generated properties were limited — participants saw LLMs as useful suggestion tools rather than replacements. No automatic baseline distillation pipeline (e.g., retrieval-augmented synthesis across many full texts) was used in this paper.",
            "uuid": "e3669.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SciAssess",
            "name_full": "SciAssesS (Benchmarking LLM proficiency in scientific literature analysis)",
            "brief_description": "A referenced recent preprint that benchmarks large language models' proficiency on tasks related to scientific literature analysis; cited as related work and motivating the use of specialized scientific embeddings and LLM evaluations.",
            "citation_title": "Sciassess: Benchmarking llm proficiency in scientific literature analysis.",
            "mention_or_use": "mention",
            "system_or_method_name": "SciAssess benchmarking (referenced)",
            "system_or_method_description": "Referenced as a benchmarking effort for LLMs on scientific-literature tasks; the paper cites it as part of background motivating evaluation choices (no implementation or usage details provided in this paper).",
            "input_corpus_description": "Not specified in this paper (details would be in the cited work).",
            "topic_or_query_specification": "Not described in this paper; general benchmarking on scientific-literature analysis tasks implied by citation.",
            "distillation_method": "Not described here; cited as related benchmarking work rather than used directly.",
            "output_type_and_format": "Not described in this paper.",
            "evaluation_or_validation_method": "Not described here beyond being a benchmarking reference motivating metrics and evaluations.",
            "results_summary": "No experimental results from SciAssess are reproduced here; it is cited to position the present work in the landscape of LLM evaluation on scientific tasks.",
            "limitations_or_challenges": "Not discussed in this paper (would be in the cited work).",
            "comparison_to_baselines_or_humans": "Not discussed here.",
            "uuid": "e3669.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLM-for-LitReview (Antu2023)",
            "name_full": "Using LLM (large language model) to improve efficiency in literature review for undergraduate research",
            "brief_description": "A cited 2023 work that applies LLMs to improve literature-review efficiency for undergraduates; mentioned in related work as an example of LLMs aiding literature review and synthesis.",
            "citation_title": "Using llm (large language model) to improve efficiency in literature review for undergraduate research.",
            "mention_or_use": "mention",
            "system_or_method_name": "LLM-assisted literature-review methods (referenced)",
            "system_or_method_description": "Referenced as an application of LLMs to literature review tasks; the present paper cites it as an instance where LLMs help literature analysis but provides no implementation details.",
            "input_corpus_description": "Not specified in this paper.",
            "topic_or_query_specification": "Not specified in this paper; likely natural-language prompts and paper collections in the cited work.",
            "distillation_method": "Not specified here (details would be in the cited work).",
            "output_type_and_format": "Not specified in this paper.",
            "evaluation_or_validation_method": "Not described here.",
            "results_summary": "Not described here; cited as supporting evidence that LLMs have been used to aid literature review.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "uuid": "e3669.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLM-feedback-study (Liang et al. 2023)",
            "name_full": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
            "brief_description": "A referenced large-scale empirical analysis studying whether LLMs can provide useful feedback on research papers; cited in related work to motivate the use of LLMs as evaluators and assistants for scientific text.",
            "citation_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis.",
            "mention_or_use": "mention",
            "system_or_method_name": "LLM feedback analysis (referenced)",
            "system_or_method_description": "Referenced for evidence that LLMs can be used to give feedback on research papers and as part of justification for using LLMs both for generation and as evaluators; no methods from that paper are executed here.",
            "input_corpus_description": "Not specified in this paper.",
            "topic_or_query_specification": "Not specified in this paper.",
            "distillation_method": "Not specified here.",
            "output_type_and_format": "Not specified here.",
            "evaluation_or_validation_method": "Not specified here beyond being cited as related empirical work.",
            "results_summary": "Not reproduced here; serves as related-work justification for using LLMs as evaluators and assistance tools.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "uuid": "e3669.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sciassess: Benchmarking llm proficiency in scientific literature analysis.",
            "rating": 2,
            "sanitized_title": "sciassess_benchmarking_llm_proficiency_in_scientific_literature_analysis"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis.",
            "rating": 2,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers_a_largescale_empirical_analysis"
        },
        {
            "paper_title": "Using llm (large language model) to improve efficiency in literature review for undergraduate research.",
            "rating": 2,
            "sanitized_title": "using_llm_large_language_model_to_improve_efficiency_in_literature_review_for_undergraduate_research"
        },
        {
            "paper_title": "SciBERT: A pretrained language model for scientific text.",
            "rating": 1,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks.",
            "rating": 1,
            "sanitized_title": "scisummnet_a_large_annotated_corpus_and_contentimpact_models_for_scientific_paper_summarization_with_citation_networks"
        }
    ],
    "cost": 0.01169425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph
3 May 2024</p>
<p>Vladyslav Nechakhin vladyslav.nechakhin@l3s.de 
L3S Research Center
Leibniz Univesity Hannover
HannoverGermany</p>
<p>Jennifer D'souza jennifer.dsouza@tib.eu 
Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Steffen Eger steffen.eger@uni-mannheim.de 
University of Mannheim
MannheimGermany</p>
<p>Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph
3 May 2024B9672E71BB177A7147C4C06317893F1EarXiv:2405.02105v1[cs.AI]Large Language ModelsOpen Research Knowledge GraphStructured Summarization
Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability.Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers' contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators.We propose using Large Language Models (LLMs) to automatically suggest these properties.However, it's essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application.Our study performs a comprehensive comparative analysis between ORKG's manually curated properties and those generated by the aforementioned state-of-the-art LLMs.We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs.These evaluations occur within a multidisciplinary science setting.Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise.</p>
<p>Introduction</p>
<p>The exponential growth of scholarly publications poses a significant challenge for researchers seeking to efficiently explore and navigate the vast landscape of scientific literature [5].This proliferation of publications necessitates the development of strategies that go beyond traditional keyword-based search methods to facilitate effective and strategic reading practices.In response to this challenge, structured representation of scientific papers has emerged as a valuable approach for enhancing FAIR research discovery and comprehension.By describing research contributions in a structured, machine-actionable format w.r.t. the salient properties of research, also seen as research dimensions, similar such structured papers can be easily compared offering researchers a systematic and quick snapshot of research progress within specific domains, thus enabling them efficient ways to stay updated with research progress.</p>
<p>One notable initiative aimed at publishing structured representations of scientific papers is the Open Research Knowledge Graph (ORKG) [6].The ORKG endeavors to describe papers in terms of various research dimensions or properties.For instance, the properties "model family", "pretraining architecture", "number of parameters", "hardware used" that can effectively be applied to offer structured, machine-actionable summaries of research contributions on the research problem "transformer model" in the domain of Computer Science (Figure 1).Furthermore, another distinguishing characteristic of the properties is aside from offering a structured summary of a transformer model, they are also generically applicable across various contributions on the same problem thus making the structured paper descriptions comparable.Thus these properties can be explicitly stated as research comparison properties.As another example, papers with the research problem of "DNA sequencing techniques" in the domain of Biology can be described as structured summaries based on the following properties: "sequencing platform", "read length in base pairs", "reagents cost", "runtime in days" (Figure 2).This type of paper description provides a structured framework for understanding and contextualizing research findings.Notably, however, the predominant method in the ORKG for creating structured paper descriptions or research comparisons is manually performed by domain experts.This means that the domain experts based on their prior knowledge and experience on a research problem select and describe the research comparison properties.While this ensures high-quality of the resulting structured papers in the ORKG, the manual annotation cycles cannot effectively scale the ORKG in practice.Specifically, the manual extraction of these salient properties of research or research comparison properties presents two significant challenges: 1) manual annotation a time-consuming process; and 2) it introduces inconsistencies among human annotators, potentially leading to variations in interpretation and annotation.</p>
<p>To address the challenges associated with the manual annotation of research comparison properties, this study tests the feasibility of using pretrained Large Language Models (LLMs) to automatically suggest or recommend research dimensions as candidate properties as a viable alternative solution.Specifically, three different LLM variants, viz.GPT-3.5-turbo[3], Llama 2 [28], and Mistral [14], are tested and empirically compared for their advanced natural language processing (NLP) capabilities when applied to the task of recommending research dimensions as candidate properites.Our choice to apply LLMs is based on the following experimental consideration.The multidisciplinary nature of scientific research poses unique challenges to the identification and extraction of salient properties across domains.LLMs, with their ability to contextualize and understand natural language at scale [13,16], are particularly well-suited to navigate the complexities of interdisciplinary research and recommend relevant dimensions that capture the essence of diverse scholarly works.By automating the extraction process, LLMs aim to alleviate the time constraints associated with manual annotation and ensure a higher level of consistency in the specification of research dimensions by using the same system prompt or fine-tuning on gold-standard ORKG data to better align them to the task.The role of LLMs in this context is to assist domain-expert human annotators rather than replace them entirely.By leveraging the capabilities of LLMs, researchers can streamline the process of dimension extraction and enhance the efficiency and reliability of comparative analysis across diverse research fields.</p>
<p>In this context, the central research question (RQ) of this study is to examine the performance of state-of-the-art Large Language Models (LLMs) in recommending research dimensions.To address this RQ, we compiled a multidisciplinary, gold-standard dataset of human-annotated scientific papers from the Open Research Knowledge Graph (ORKG), detailed in the Materials and Methods section (see subsection 3.1).This dataset includes structured summary property annotations by domain experts.We conducted a detailed comparative evaluation of the domain-expert annotated properties from the ORKG against the dimensions generated by LLMs for the same papers.Our evaluations are based on four unique perspectives: 1) semantic alignment and deviation assessment by GPT-3.5 between ORKG properties and LLM-generated dimensions, 2) finegrained property mapping accuracy by GPT-3.5, 3) SciNCL [21] embeddingsbased cosine similarity between ORKG properties and LLM-generated dimensions, and 4) a survey with human experts comparing their annotations of ORKG properties with the LLM-generated dimensions.</p>
<p>As such, the contribution of this work is a comprehensive set of insights into the readiness of LLMs to support human annotators in the task of structuring their research contributions.Our findings reveal a moderate alignment between LLM-generated dimensions and manually annotated ORKG properties, indicating the potential for LLMs to learn from human-annotated data.However, there is a noticeable gap in the mapping of dimensions generated by LLMs and those annotated by domain experts, highlighting the need for fine-tuning LLMs on domain-specific datasets to reduce this disparity.Despite this gap, LLMs demonstrate the ability to capture the semantic relationships between LLM-generated dimensions and ORKG properties, as evidenced by strong correlation results of embeddings similarity.In the survey, the human experts noted that while they were not ready to change their existing property annotations based on the LLM generated dimensions, they highlighted the utility of the auto-LLM recommendation service at the time of creating the structured summary descriptions.This directly informs a future research direction in making LLMs fit for structured science summarization.</p>
<p>Related Work</p>
<p>The utilization of Large Language Models (LLMs) for various NLP tasks has has seen widespread adoption in recent years [23,9].Within the realm of scientific literature analysis, researchers have explored the potential of LLMs for tasks such as generating summaries and abstracts of research papers [10,15], extracting insights and identifying patterns [20], aiding in literature reviews [4], enhancing knowledge integration [18], etc.However, the specific application of LLMs for recommending research dimensions to obtain structured representations of research contributions is a relatively new area of investigation that we explore in this work.Furthermore, to offer insights into the readiness of LLMs over our novel task, we perform a comprehensive set of evaluations comparing the LLM-generated research dimensions and the human expert annotated properties.As a straightforward preliminary evaluation, we measure the semantic similarity between the LLM and human annotated properties.To do this, we employ a specialized language model tuned for the scientific domain to create embeddings for the respective properties.</p>
<p>The development of domain-specific language models has been a significant advancement in NLP.In the scientific domain, a series of specialized models have emerged.SciBERT, introduced by Beltagy et al. [8], was the first language model tailored for scientific texts.This was followed by SPECTER, developed by Cohan et al. [11].More recently, Ostendorff et al. introduced SciNCL [21], a language model designed to capture semantic similarity between scientific concepts by leveraging pre-trained BERT embeddings.SciNCL has demonstrated its effectiveness in evaluating the nuances of scientific concepts, making it an ideal choice for assessing LLM-generated dimensions in scientific literature analysis.In this study, we utilize SciNCL, the most recent and advanced variant, to generate embeddings for ORKG properties and LLM-generated dimensions.</p>
<p>In the context of evaluating LLM-generated dimensions against manually curated properties, several studies have employed similarity measures to quantify the relatedness between the two sets of textual data.One widely used metric is cosine similarity, which measures the cosine of the angle between two vectors representing the dimensions [25].This measure has been employed in various studies, such as Yasunaga et al. [31], who used cosine similarity to assess the similarity between automatically generated summaries by LLMs and humanwritten annotations.Similarly, Banerjee et al. [7] employed cosine similarity as a metric to benchmark the accuracy of LLM-generated answers of autonomous conversational agents.In contrast to cosine similarity, other studies have explored alternative similarity measures for evaluating LLM-generated content.For instance, Jaccard similarity measures the intersection over the union of two sets, providing a measure of overlap between them [29].This measure has been employed in tasks such as document clustering and topic modeling [12,22].Jac-card similarity offers a distinct perspective on the overlap between manually curated and LLM-generated properties, as it focuses on the shared elements between the two sets rather than their overall similarity.We considered both cosine and Jaccard similarity in our evaluation, however, based on our embedding representation, we ultimately chose to use cosine similarity as our distance measure.</p>
<p>Finally, aside from the straightforward similarity computations between the two sets of properties, we also leverage the capabilities of LLMs as evaluators.The utilization of LLMs as evaluators in various NLP tasks has been proven to be a successful approach in a number of recent publications.For instance, Kocmi and Federmann [17] demonstrated the effectiveness of GPT-based metrics for assessing translation quality, achieving state-of-the-art accuracy in both reference-based and reference-free modes.Similarly, the Eval4NLP 2023 shared task, organized by Leiter et al. [19], explored the use of LLMs as explainable metrics for machine translation and summary evaluation, showcasing the potential of prompting and score extraction techniques to achieve results on par with or even surpassing recent reference-free metrics.In our study, we employ the GPT-3.5 model as an evaluator, leveraging its capabilities to assess the quality of LLM-generated research dimensions.</p>
<p>In summary, previous research has laid the groundwork for evaluating LLMs' performance in scientific literature analysis, and our study builds upon these efforts by exploring the application of LLMs for recommending research dimensions and evaluating their quality using specialized language models and similarity measures.</p>
<p>Materials and Methods</p>
<p>This section is organized into three subsections.In the first subsection, the creation of the gold-standard evaluation dataset from the ORKG with domainexpert, human-annotated research comparison properties used for assessing the similarity with the LLM generated properties is described.The second subsection provides an overview of the three LLMs, viz.GPT-3.5, Llama 2, and Mistral, applied to automatically generate the research comparison properties, highlighting their respective technical characteristics.Lastly, the third subsection discusses the various evaluation methods used in this study offering differing perspectives on the similarity comparison of the ORKG properties for the instances in our gold-standard dataset versus those generated by the LLMs.</p>
<p>Material: Our Evaluation Dataset</p>
<p>As alluded to in the Introduction, a central RQ of this work is to compare the research dimensions generated by three different LLMs with the humanannotated research comparison properties in the ORKG.For this, we created an evaluation dataset of annotated research dimensions based on the ORKG.As a starting point, we curated a selection of ORKG Comparisons by selecting comparisons that were created by experienced ORKG users.These users had varied research backgrounds.The selection criteria of comparisons from these users were as follows: the comparisons had to have at least 3 properties and contain at least 5 contributions, since we wanted to ensure that the properties were not too sparse representation of a research problem, but were those that generically reflected a research comparison over several works.On application of this criteria, the resulting dataset comprised 103 ORKG Comparisons.These selected gold-standard comparisons contained 1,317 papers from 35 different research fields addressing over 150 distinct research problems.The gold-standard dataset can be downloaded from the Leibniz University Data Repository.The selection of comparisons ensured the diversity of research fields' distribution, containing Earth Sciences, Natural Language Processing, Medicinal Chemistry and Pharmaceutics, Operations Research, Systems Engineering, Cultural History, Semantic Web and others.See Figure 3 for the full distribution of research fields in our dataset.Once we had the comparisons, we then looked at the individual structured papers within each comparison and extracted their human annotated properties.Thus our resulting dataset, is highly multidisciplinary, comprising structured paper instances from the ORKG with their corresponding domain expert property annotations across different fields of research.For instance, the properties listed below were extracted form the comparison "A Catalog of Transformer Models" (Figure 1):</p>
<p>["has model", "model family", "date created", "organization", "innovation", "pretraining architecture", "pretraining task", "fine-tuning task", "training corpus", "optimizer", "tokenization", "number of parameters", "maximum number of parameters (in million)", "hardware used", "hardware information", "extension", "has source code", "blog post", "license", "research problem"] Another example of structured paper's properties in the comparison "Survey of sequencing techniques" (Figure 2) is as follows:</p>
<p>["cost of machine in $", "cost of resequencing a human genome with 30X coverage in $", "cost per Gigabyte data output in $", "data output rate in Gigabyte per day", "has research problem", "read length in base pairs (paired-end*) ", "reads per run in Million", "reagents cost in $", "runtime in days", "sequencing platform", "total data output yield in Gigabyte"] The aforementioned dataset is now the gold-standard that we use in the evaluations for the LLM-generated research dimensions.In this section, we provide a clear distinction between the terminology of ORKG properties and LLMgenerated research dimensions.According to our hypothesis, ORKG properties are not necessarily identical to research dimensions.Contribution properties within ORKG relate to specific attributes or characteristics associated with individual research papers in a comparison, outlining aspects such as authorship, publication year, methodology, and findings.Conversely, research dimensions encapsulate the multifaceted aspects of a given research problem, constituting the nuanced themes or axes along which scholarly investigations are conducted.ORKG contribution properties offer insights into the attributes of individual papers, research dimensions operate at a broader level, revealing the finer-grained thematic fundamentals of research endeavors.While ORKG contribution properties focus on the specifics of research findings, research dimensions offer a more comprehensive context for analyzing a research question that can be used for finding similar papers that share the same dimensions.In order to test the alignment of LLM-generated research dimensions to ORKG properties, several LLMs were selected to be compared, as described in the next section.</p>
<p>Method: Three Large Language Models Applied for Research Dimensions Generation</p>
<p>In this section, we discuss the LLMs applied for automated research dimensions generation as well as the task-specific prompt that was designed as input to the LLM.</p>
<p>The three LLMs To test the automated generation of research dimensions, we tested and compared the output from three different state-of-the-art LLMs with comparable parameter counts, namely GPT-3.5, Llama 2 and Mistral.</p>
<p>GPT-3.5, developed by OpenAI, is one of the most influential LLMs to date, its number of parameters is not publicly disclosed [1].In comparison, it's predecessor GPT-3 models come in different sizes and contain from 125 million parameters for the smallest model to 175 billion for the largest [9].GPT-3.5 has demonstrated exceptional performance on a range of NLP tasks including translation, question answering and text completion.Notably, the capabilities of this model are accessed through the OpenAI API, since the model is closedsource, which limits direct access to its architecture for further exploration or customization.</p>
<p>Llama 2 by Meta AI [28], the second iteration of the Llama LLM, represents a significant advancement in the field.Featuring twice the context length of its predecessor Llama 1 [27], Llama 2 offers researchers and practitioners a versatile tool for working with NLP.Importantly, Llama 2 is available free of charge for both research and commercial purposes, with multiple parameter configurations available, including a 13 billion parameter option.In addition, the model supports fine-tuning and self-hosting, which enhances its adaptability to a variety of use cases.</p>
<p>Mistral, developed by Mistral AI, is a significant competitor in the landscape of LLMs.With a parameter count of 7.3 billion, Mistral demonstrates competitive performance in various benchmarks, often outperforming Llama 2 despite its smaller size [14].In addition, Mistral is distinguished by its open source code released under the Apache 2.0 license, making it easily accessible for research and commercial applications.Notably, Mistral has an 8k context window, compared to 4k context window of Llama 2, which allows for a more complete understanding of context [26].</p>
<p>Overall, GPT-3.5, despite its closed source nature, remains influential in NLP research, with a vast number of parameters that facilitate its generic task performance in the context of a wide variety of applications.Conversely, Llama 2 and Mistral, with their open source nature, provide flexibility and accessibility for researchers and developers while displaying similar performance characteristics to GPT [2].Released shortly after Llama 2, Mistral, in particular, shows notable performance advantages over Llama 2, highlighting the rapid pace of innovation and improvement in the development of LLMs.These differences between the models lay the groundwork for assessing their alignment with manually curated properties from ORKG and determining their potential for automated research metadata creation and retrieval of related work.</p>
<p>Research dimensions generation prompt for the LLMs LLM's performance on a particular task is highly dependent on the quality of the prompt.To find the optimal prompt methodology, our study explores various established prompting techniques, including zero-shot [24], few-shot [9], and chainof-thought prompting [30].The simplest type of prompt is a zero-shot approach, wherein pretrained LLMs owing to the large-scale coverage of human tasks within their pretraining datasets demonstrate competence in task execution without prior exposure to specific examples.While zero-shot prompting provides satis-</p>
<p>Chain-of-Thought</p>
<p>You will be provided with a research problem, your task is to list dimensions that are relevant to find similar papers for the research problem.Provide justification why each dimension is relevant for finding similar papers.Think step-by-step.At the end combine all the relevant dimensions in the format of a python list.</p>
<p>["Task/Methodology", "Domain/Genre", "Evaluation Metrics", "Language", "Input/Output Format", "Deep Learning/Traditional Methods", "Applications"] factory results for certain tasks, some of the more complex tasks require few-shot prompting.By providing the model with several examples, few-shot prompting enables in-context learning, potentially enhancing model performance.Another popular technique is chain-of-thought prompting, which instructs the model to think step-by-step.Guiding the LLM through sequential steps helps to break down complex tasks into manageable components that are easier for the LLM to complete.</p>
<p>In this study, the task involves providing the LLM with a research problem, and based on this input, the model should suggest research dimensions that it finds relevant to structure contributions from similar papers that address the research problem.Our system prompts for each technique along with the examples of output for a research problem "Automatic text summarization" from GPT-3.5 are shown in the Table 1.</p>
<p>Our analysis shows that the utilization of more advanced prompting techniques did not necessarily result in superior outcomes, which leads us to believe that our original zero-shot prompt is sufficient for our task completion.The absence of discernible performance improvements with the adoption of more complex prompting techniques highlights the effectiveness of the initial zeroshot prompt in aligning with the objectives of research dimension extraction.Consequently, we will be apply the zero-shot prompt methodology.</p>
<p>To test the alignment of LLM-generated research dimensions to ORKG properties, each of the LLMs was given the same prompt to create a list of dimensions that are relevant to find similar papers based on the provided research problem.Table 2 shows the comparison between some of the manually created properties form ORKG and the research dimensions, provided by GPT-3.5, Llama 2 and Mistral.</p>
<p>Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions</p>
<p>This section outlines the methodology used to evaluate our dataset, namely the automated evaluation of semantic alignment and deviation as well as mapping between ORKG properties and LLM-generated research dimensions performed by GPT-3.5.Additionally, we present our approach to calculating embedding similarity between properties and research dimensions.</p>
<p>Semantic alignment and deviation evaluations using GPT-3.5To measure the semantic similarity between between ORKG properties and LLM-generated research dimensions, we conducted semantic alignment and deviation assessments using an LLM-based evaluator.In this context, semantic alignment refers to the degree to which two sets of concepts share similar meanings, whereas semantic deviation assesses how far apart they are in terms of meaning.As the LLM evaluator, we leveraged GPT-3.5.As input it was provided with both the lists of properties from ORKG and the dimensions extracted by the LLMs, in a string format, per research problem.Semantic alignment was rated on a scale from 1 to 5, using the following system prompt to perform this task:</p>
<p>You will be provided with two lists of strings, your task is to rate the semantic alignment between the lists on the scale form 1 to 5.Your response must only include an integer representing your assessment of the semantic alignment, include no other text.</p>
<p>To further validate the accuracy of our alignment scores, we leveraged GPT-3.5 as an evaluator again but this time to generate the semantic deviation scores.By using this contrastive alignment versus deviation evaluation method, we can cross-reference where the LLM evaluator displays a strong agreement in its evaluation and assess the evaluations for reliability.Specifically, we evaluate the same set of manually curated properties and LLM-generated research dimensions using both agents, with the expectation that the ratings will exhibit an inverse relationship.That is, high alignment scores should correspond to low deviation scores, and vice versa.The convergence of these opposing measures would provide strong evidence for the validity of our evaluation results.Similar to the task of alignment rating, the system prompt below was used to instruct GPT-3.5 to measure semantic deviation where the ratings described in Table 4 was also part of the prompt.</p>
<p>You will be provided with two lists of strings, your task is to rate the semantic deviation between the lists on the scale form 1 to 5.Your response must only include an integer representing your assessment of the semantic deviation, include no other text.</p>
<p>Table 4. Description of the semantic deviation scores provided in the GPT-3.5 system prompt.</p>
<p>Score Description 1 -Minimal Deviation</p>
<p>The strings in the lists show little or no semantic difference.</p>
<p>-Low Deviation</p>
<p>The semantic variance between the lists is limited.3 -Moderate Deviation There is a moderate level of semantic difference between the strings in the lists.4 -Substantial Deviation The lists exhibit a considerable semantic gap or difference.5 -Significant Deviation The semantic disparity between the lists is pronounced and substantial.</p>
<p>By combining these two evaluations, we can gain a more nuanced understanding of the relationship between the ORKG properties and LLM-generated research dimensions.</p>
<p>ORKG property and LLM-generated research dimension mappings by GPT-3.5</p>
<p>To further analyse the relationships between ORKG properties and LLM-generated research dimensions, we used GPT-3.5 to find the mappings between individual properties and dimensions.This approach diverges from the previous semantic alignment and deviation evaluations, which considered the lists as a whole.Instead, we instructed GPT-3.5 to identify the number of properties that exhibit similarity with individual research dimensions.This was achieved by providing the model with the two lists of properties and dimensions and prompting it to count the number of similar values between the lists.</p>
<p>The system prompt used for this task was as follows:</p>
<p>You will be provided with two lists of strings, your task is to count how many values from list1 are similar to values of list2.Respond only with an integer, include no other text.</p>
<p>By leveraging GPT-3.5'scapabilities in this manner, we were able to count the number of LLM-generated research dimensions that are related to individual ORKG properties.The mapping count provides a more fine-grained insight into the relationships between the properties and dimensions.</p>
<p>Scientific embeddings-based semantic distance evaluations</p>
<p>To further examine the semantic relationships between ORKG properties and LLM-generated research dimensions, we employed embeddings-based approach.Specifically, we utilized SciNCL to generate vector embeddings for both the ORKG properties and the LLM-generated research dimensions.These embeddings were then compared using cosine similarity as a measure of semantic similarity.We evaluated the similarity of ORKG properties to the research dimensions generated by GPT-3.5, Llama 2, and Mistral.Additionally, we compared the LLM-generated dimensions to each other, providing insights into the consistency and variability of the research dimensions generated by different LLMs.By leveraging embeddingsbased evaluations, we were able to quantify the semantic similarity between the ORKG properties and the LLM-generated research dimensions, as well as among the dimensions themselves.</p>
<p>Human assessment survey comparing ORKG properties with LLMgenerated research dimensions</p>
<p>We conducted a survey to evaluate the utility of LLM generated dimensions in the context of domain-expert annotated ORKG properties.The survey was designed to solicit the impressions of domain experts when shown their original annotated properties versus the research dimensions generated by GPT-3.5.We selected participants who are experienced at creating structured paper descriptions in the ORKG.These participants included ORKG curation grant participants, ORKG employees, and authors whose comparisons were displayed on the ORKG featured comparisons page.Each participant was given a maximum of 5 surveys, for five different papers they structured respectively, each evaluating properties versus research dimensions.They had the choice to respond to one, some, or all of them.At the end, we received 23 total responses to our survey corresponding to 23 different papers.</p>
<p>The survey itself consisted of five questions, most of which were designed on a Likert scale, to guage the domain expert assessment of the effectiveness of the LLM-generated research dimensions.Per survey, as evaluation data, participants were presented with two tables: one including their annotated ORKG property names and values (Figure 4), and the second one consisting of research dimension name, its description, and value generated by GPT-3.5 from a title and abstract of the same paper (Figure 5).Following this data was the survey questionnaire.The questions asked participants to rate the relevance of LLM-generated research dimensions, consider making edits to the original ORKG structured contribution, and evaluate the usefulness of LLM-generated content as suggestions before creating their structured contributions.Additionally, participants were asked to describe how LLM-generated content would have been helpful and rate the alignment of LLM-generated research dimensions with the original ORKG structured contribution.The survey questionnaire is shown below:</p>
<ol>
<li>How many of the properties generated by ChatGPT are relevant to your ORKG structured contribution?(Your answer should be a number) 2. Considering the ChatGPT-generated content, would you consider making edits to your original ORKG structured contribution?3.If the ChatGPT-generated content were available to you as suggestions before you created your structured contribution, would it have been helpful?(a) If you answered "Yes" to the question above, could you describe how it would have been helpful?4. On a scale of 1 to 5, please rate how well the ChatGPT-generated response aligns with your ORKG structured contribution.5. We plan to release an AI-powered feature to support users in creating their ORKG contributions with automated suggestions.In this context, please share any additional comments or thoughts you have regarding the given ChatGPT-generated structured contribution and its relevance to your ORKG contribution.The subsequent section will present the results obtained from these methodologies, providing insights into the similarity between ORKG properties and LLM-generated research dimensions.</li>
</ol>
<p>Results and Discussion</p>
<p>This section presents the results of our evaluation, which aimed to assess the LLMs' performance on the task of recommending research dimensions by calculating the similarity between ORKG properties and LLM-generated research dimensions.We employed three types of similarity evaluations: semantic alignment and deviation assessments, property and research dimension mappings using GPT-3.5, and embeddings-based evaluations using SciNCL.</p>
<p>Semantic alignment and deviation evaluations</p>
<p>The average alignment between paper properties and research field dimensions was found to be 2.9 out of 5, indicating a moderate level of alignment.In contrast, the average deviation was 4.1 out of 5, suggesting a higher degree of deviation.When normalized, the proportional values translate to 41.2% alignment and 58.8% deviation (6).These results imply that while there is some alignment between paper properties and research field dimensions, there is also a significant amount of deviation, highlighting the difference between the concepts of structured paper's properties and research dimensions.This outcome supports our hypothesis that LLM-based research dimensions generated solely from a research problem, relying on LLM-encoded knowledge, may not fully capture the nuanced inclinations of domain experts when they annotate ORKG properties to structure their contributions, where the domain experts have the full paper at hand.We posit that an LLM, not explicitly tuned on the scientific domain, despite its vast parameter space, is not able to emulate human expert subjectivity to structure contributions.</p>
<p>Property and research dimension mappings</p>
<p>We examine our earlier posited claim in a more fine-grained manner by comparing the property versus research dimension mappings.The average number of mappings was found to be 0.33, indicating a low number of mappings between paper properties and research field dimensions.The average ORKG property count was 4.73, while the average GPT dimension count was 8.These results suggest that LLMs can generate a more diverse set of research dimensions than the ORKG properties, but with a lower degree of similarity.Notably, the nature of ORKG properties and research dimensions differs in their scope and focus.Common ORKG properties like "research problem", "method", "data source" and others provide valuable information about a specific paper, but they can not be used to comprehensively describe a research field as a whole.In contrast, research dimensions refer to shared properties of a research question, rather than Fig. 6.Proportional values of semantic alignment and deviation between ORKG properties and GPT-generated dimensions focus on an individual paper.This difference contributes to the low mapping between paper properties and research field dimensions, which further consolidates our conjecture that an LLM based on only its own knowledge applied on a given research problem might not be able to completely emulate a human expert's subjectivity in defining ORKG properties.These results therefore are not a direct reflection of the inability of the LLMs tested to recommend suitable properties to structure contributions on the theme.This then opens the avenue for future work to explore how fine-tuned LLMs on the scientific domain perform on the task as a direct extension of our work.</p>
<p>Embeddings-based evaluations</p>
<p>The embeddings-based evaluations provide a more nuanced perspective on the semantic relationships between the ORKG properties and the LLM-generated research dimensions.By leveraging the SciNCL embeddings, we were able to quantify the cosine similarity between these two concepts, offering insights into their alignment.The results indicate a high degree of semantic similarity, with the cosine similarity between ORKG properties and the LLM-generated dimensions reaching 0.84 for GPT-3.5, 0.79 for Mistral, and 0.76 for Llama 2. These values suggest that the LLM-generated dimensions exhibit a strong correlation with the manually curated ORKG properties, signifying a substantial semantic overlap between the two.</p>
<p>Furthermore, the correlation heatmap (7) provides a visual representation of these similarities, highlighting the strongest correlations between ORKG properties and LLM-generated dimensions.The embeddings-based evaluations indicate that GPT-3.5 demonstrates the highest similarity to the ORKG properties, outperforming both Llama 2 and Mistral.When comparing LLM-generated dimensions between each other, a strong similarity observed between the Llama 2 and Mistral dimensions highlights the remarkable consistency in the research dimensions generated by these two models.Overall, the embeddings-based evaluations provide a quantitative representation of the semantic relationships between the ORKG properties and the LLMgenerated research dimensions.These results suggest that while there are notable differences between the two, the LLMs exhibit a strong capacity to generate dimensions that are semantically aligned with the manually curated ORKG properties, particularly in the case of GPT-3.5.This finding highlights the potential of LLMs to serve as valuable tools for automated research metadata creation and the retrieval of related work.</p>
<p>Human assessment survey</p>
<p>To further evaluate the utility of the LLM-generated research dimensions, we conducted a survey to solicit feedback from domain experts who annotated the properties to create structured science summary representations or structured contribution descriptions in the ORKG.The survey was designed to assess the participants' impressions when presented with their original ORKG properties alongside the research dimensions generated by GPT-3.5.</p>
<p>For the first question in the Questionnaire assessing the relevance of LLMgenerated dimensions to create a structured paper summary or to structure the contribution of a paper, on average, 36.3% of the research dimensions generated by LLMs were considered highly relevant (Figure 8).This suggests that LLMgenerated dimensions can provide useful suggestions for creating structured contributions on ORKG.For the second question, majority of participants (60.9%) did not think it was necessary to make changes to their existing ORKG structure paper property annotations based on the LLM-generated dimensions, indicating that while the suggestions were relevant, they may not have been sufficient to warrant significant changes (Figure 9).However, based on the third question, the survey also revealed that the majority of authors (65.2%) believed that having LLM-generated content as suggestions before creating the structured science summaries or structured contributions would have been helpful (Figure 10).The respondents noted that such LLM-based dimension suggestions could serve as a useful starting point, provide a basis for improvement, and aid in including additional properties.Based on the fourth question, the alignment between LLMgenerated research dimensions and the original ORKG structured contribution properties was rated as moderate, with an average rating of 2.65 out of 5 (Figure 11).This indicates that while there is some similarity between the two, there is room for alignment.As such participants raised concerns about the specificity of generated dimensions potentially diverging from the actual goal of the paper.For the final question on the release of such an LLM-based feature, the respondents emphasized the importance of aligning LLMs based on the domain expert property names while allowing descriptions to be generated, ensuring relevance across different research domains and capturing specific details like measurement values and units.</p>
<p>Overall, the findings of the survey indicate that LLM-generated dimensions exhibited a moderate alignment with manually extracted properties.Although the generated properties did not perfectly align with the original contributions, they still provided valuable suggestions that authors found potentially helpful in various aspects of creating structured contributions for ORKG.For instance, the suggestions were deemed useful in facilitating the creation of comparisons, identifying relevant properties, and providing a starting point for further refinement.However, concerns regarding the specificity and alignment of the generated properties with research goals were noted, suggesting areas for further refinement.These concerns highlight the need for LLMs to better capture the nuances of research goals and objectives in order to generate more targeted and relevant suggestions.Nonetheless, the overall positive feedback from participants suggests that AI tools, such as LLMs, hold promise in assisting users in creating structured research contributions and comparisons within the ORKG platform.</p>
<p>Conclusions</p>
<p>In this study, we investigated the performance of state-of-the-art Large Language Models (LLMs) in recommending research dimensions, aiming to address the central research question: How effectively do LLMs perform on the task of recommending research dimensions?Through a series of evaluations, including semantic alignment and deviation assessments, property and research dimension mappings, embeddings-based evaluations, and a human assessment survey, we sought to provide insights into the capabilities and limitations of LLMs in this domain.The findings of our study elucidated several key aspects of LLM performance in recommending research dimensions.First, our semantic alignment and deviation assessments revealed a moderate level of alignment between manually curated ORKG properties and LLM-generated research dimensions, accompanied by a higher degree of deviation.While LLMs demonstrate some capacity to capture semantic similarities, there are notable differences between the concepts of structured paper properties and research dimensions.This suggests that LLMs may not fully emulate the nuanced inclinations of domain experts when structuring contributions.</p>
<p>Second, our property and research dimension mappings analysis indicated a low number of mappings between paper properties and research dimensions.While LLMs can generate a more diverse set of research dimensions than the ORKG properties, the degree of similarity is lower, highlighting the challenges in aligning LLM-generated dimensions with human expert curated properties.</p>
<p>Third, our embeddings-based evaluations showed that GPT-3.5 achieved the highest semantic similarity between ORKG properties and LLM-generated research dimensions, outperforming Mistral and Llama 2 in that order.</p>
<p>Fourth and finally, our human assessment survey provided valuable feedback from domain experts, indicating a moderate alignment between LLM-generated dimensions and manually annotated properties.While the suggestions provided by LLMs were deemed potentially helpful in various aspects of creating structured contributions, concerns regarding specificity and alignment with research goals were noted, suggesting areas for improvement.</p>
<p>In conclusion, our study contributes to a deeper understanding of LLM performance in recommending research dimensions to create structured science summary representations in the ORKG.While LLMs show promise as tools for automated research metadata creation and the retrieval of related work, further development is necessary to enhance their accuracy and relevance in this domain.Future research may explore the fine-tuning of LLMs on scientific domains to improve their performance in recommending research dimensions.</p>
<p>CRediT Author Contributions</p>
<p>Fig. 1 .
1
Fig. 1.ORKG Comparison -A Catalog of Transformer Models (https://orkg.org/comparison/R656113/)</p>
<p>Fig. 2 .
2
Fig. 2. ORKG Comparison -Survey of sequencing techniques (https://orkg.org/comparison/R44668/)</p>
<p>Fig. 3 .
3
Fig.3.Research field distribution of the selected papers in our evaluation dataset containing domain-expert human-annotated properties that were applied to represent the paper's structured contribution descriptions in the Open Research Knowledge Graph (ORKG).</p>
<p>Fig. 4 .
4
Fig. 4. Example of ORKG properties shown to survey respondents</p>
<p>Fig. 5 .
5
Fig. 5. Example of GPT dimensions shown to survey respondents</p>
<p>Fig. 7 .
7
Fig. 7. Correlation heatmap of cosine similarity between ORKG properties and LLMgenerated dimensions</p>
<p>Fig. 8 .
8
Fig. 8. Count of the number of relevant LLM-generated research dimensions to structure a paper's contribution or create a structured science summary.</p>
<p>Fig. 9 .
9
Fig. 9. Willingness of the participants to make changes to their existing annotated ORKG properties when shown the LLM-generated research dimensions.</p>
<p>Fig. 10 .Fig. 11 .
1011
Fig. 10.Utility of LLM-suggested dimensions as suggestions</p>
<p>Table 1 .
1
Prompt variations utilizing different prompt engineering techniques to instruct LLMs for the research dimensions generation task.
PromptingSystem promptOutput exampletechniqueZero-ShotYou will be provided with a research problem,["Natural Languageyour task is to list dimensions that are relevantProcessing","Textto find similar papers for the research problem.Analysis", "MachineRespond only in the format of a python list.Learning","DeepLearning","Infor-mationRetrieval","ArtificialIntelli-gence","LanguageModels", "DocumentSummarization"]Few-ShotYou will be provided with a research problem,['Summarization ap-your task is to list dimensions that are relevantproach', 'Documentto find similar papers for the research problem.type','Language',Respond only in the format of a python list.'Evaluationmet-The following are two successfully completed taskric', 'Model type',examples.'Trainingdataset',Research problem: "Transformer models"'Compression ratio',Research dimensions: ['model', 'date created','Summary length']'pretraining architecture', 'pretraining task','training corpus', 'optimizer', 'tokenization','number of parameters', 'license']Research problem: "Liposomes as drug carriers"Research dimensions: ['Type of nanocarrier','Nanoparticle preparation method', 'Lipidcomposition', 'Drug type', 'Particle size']
AcknowledgmentsWe thank all members of the ORKG Team for their dedication in creating and maintaining the ORKG platform.Furthermore, we thank all the participants of our survey for providing their insightful feedback and responses.Data AvailabilityThe evaluation dataset created in this paper is publicly accessible at https: //doi.org/10.25835/6oyn9d1n.FundingThis work was supported by the German BMBF project SCINEXT (ID 01lS22070), the European Research Council for ScienceGRAPH (GA ID: 819536), and German DFG for NFDI4DataScience (no.460234259).Additionally, the prompt included the detailed description of the scoring system shown in the Table3.Table3. Description of the semantic alignment scores provided in the GPT-3.5 system prompt.ScoreDescription 1 -Strongly Disaligned The strings in the lists have minimal or no semantic similarity.-DisalignedThe strings in the lists have limited semantic alignment.-NeutralThe semantic alignment between the lists is moderate or average.-AlignedThe strings in the lists show substantial semantic alignment.-Strongly AlignedThe strings in the lists exhibit high semantic coherence and alignment.Author
Open llm leaderboard. </p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Using llm (large language model) to improve efficiency in literature review for undergraduate research. S A Antu, H Chen, C K Richards, 2023</p>
<p>Clustering semantic predicates in the open research knowledge graph. Arab Oghli, O D'souza, J Auer, S , International Conference on Asian Digital Libraries. Springer2022</p>
<p>Improving access to scientific literature with knowledge graphs. S Auer, A Oelen, M Haris, M Stocker, J D'souza, K E Farfar, L Vogt, M Prinz, V Wiens, M Y Jaradeh, Bibliothek Forschung und Praxis. 4432020</p>
<p>D Banerjee, P Singh, A Avadhanam, S Srivastava, arXiv:2308.04624Benchmarking llm powered chatbots: methods and metrics. 2023arXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>H Cai, X Cai, J Chang, S Li, L Yao, C Wang, Z Gao, Y Li, M Lin, S Yang, arXiv:2403.01976Sciassess: Benchmarking llm proficiency in scientific literature analysis. 2024arXiv preprint</p>
<p>Specter: Documentlevel representation learning using citation-informed transformers. A Cohan, S Feldman, I Beltagy, D Downey, D S Weld, arXiv:2004.071802020arXiv preprint</p>
<p>An efficient k-means algorithm integrated with jaccard distance measure for document clustering. R Ferdous, 2009 first asian himalayas international conference on internet. IEEE2009</p>
<p>Language writ large: Llms, chatgpt, grounding, meaning and understanding. S Harnad, arXiv:2402.022432024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>A comprehensive survey on processoriented automatic text summarization with exploration of llm-based methods. H Jin, Y Zhang, D Meng, J Wang, J Tan, arXiv:2403.029012024arXiv preprint</p>
<p>Large language models versus natural language understanding and generation. N Karanikolas, E Manga, N Samaridi, E Tousidou, M Vassilakopoulos, Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics. the 27th Pan-Hellenic Conference on Progress in Computing and Informatics2023</p>
<p>Large language models are state-of-the-art evaluators of translation quality. T Kocmi, C Federmann, arXiv:2302.145202023arXiv preprint</p>
<p>Knowledge distillation of llm for education. E Latif, L Fang, P Ma, X Zhai, arXiv:2312.158422023arXiv preprint</p>
<p>C Leiter, J Opitz, D Deutsch, Y Gao, R Dror, S Eger, arXiv:2310.19792The eval4nlp 2023 shared task on prompting large language models as explainable metrics. 2023arXiv preprint</p>
<p>W Liang, Y Zhang, H Cao, B Wang, D Ding, X Yang, K Vodrahalli, S He, D Smith, Y Yin, arXiv:2310.01783Can large language models provide useful feedback on research papers? a large-scale empirical analysis. 2023arXiv preprint</p>
<p>Neighborhood contrastive learning for scientific document representations with citation embeddings. M Ostendorff, N Rethmeier, I Augenstein, B Gipp, G Rehm, arXiv:2202.066712022arXiv preprint</p>
<p>An analysis of the coherence of descriptors in topic modeling. D O'callaghan, D Greene, J Carthy, P Cunningham, Expert Systems with Applications. 42132015</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Modern information retrieval: A brief overview. A Singhal, IEEE Data Eng. Bull. 2442001</p>
<p>Comprehensive examination of instruction-based language models: A comparative analysis of mistral-7b and llama-2-7b. H Thakkar, A Manimaran, 2023 International Conference on Emerging Research in Computational Science (ICERCS). IEEE2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>A comparative analysis of similarity measures akin to the jaccard index in collaborative recommendations: empirical and theoretical perspective. V Verma, R K Aggarwal, Social Network Analysis and Mining. 101432020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 352022</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. M Yasunaga, J Kasai, R Zhang, A R Fabbri, I Li, D Friedman, D R Radev, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>            </div>
        </div>

    </div>
</body>
</html>
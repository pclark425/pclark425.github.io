<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4288 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4288</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4288</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-280081994</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.07155v1.pdf" target="_blank">Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics</a></p>
                <p><strong>Paper Abstract:</strong> We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose.The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed. We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4\% accuracy. Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation. These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4288.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4288.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciRag</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciRag (Retrieval-Augmented Generation Implementation Pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular RAG framework introduced in this paper for systematic integration, benchmarking, and deployment of multiple RAG agents to process scientific papers and synthesize domain knowledge for autonomous discovery in astrophysics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciRag</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A multi-stage RAG pipeline that preprocesses documents (OCR optional), creates vector or hybrid retrieval indices (ChromaDB, OpenAI/Vertex vector stores), retrieves ranked document chunks (top-k retrieval), and conditions high-performance LLMs (OpenAI GPT-4.1, Gemini variants, etc.) with retrieved context to generate concise, evidence-backed answers. Evaluation includes deterministic decoding (temperature=0.01), configurable retrieval similarity thresholds, and parallel comparisons across commercial, hybrid, and academic RAG stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>5 papers</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>astronomy / cosmology</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>synthesis of factual claims, hypotheses, and interpretive scientific conclusions from paper content (empirical generalizations and domain-specific interpretations)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert binary scoring (0/1 scaled to 0-100) across 945 responses; calibrated LLM-as-a-Judge (LLMaaJ) validated against human expert judgments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>System-level accuracy on CosmoPaperQA: OpenAI-based RAG up to 91.4% (human-evaluated); PaperQA2 implementation 81.9% (human-evaluated); hybrid systems 84.8-85.7%; baselines (non-RAG) ~16-17% (human-evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared RAG-enabled agents (OpenAI, VertexAI, hybrid, PaperQA2) against non-RAG baselines (Gemini Assistant, Perplexity Assistant) showing large performance gaps; human expert evaluation used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG pipelines that combine high-quality embeddings, domain-tailored retrieval, and strong generative LLMs significantly outperform non-RAG models at extracting and synthesizing domain knowledge from papers; calibrated LLM judges can match human expert rankings and enable scalable evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limited corpus size (5 papers) compared to realistic literature scale; potential retrieval cues due to explicit paper references in questions; risks of hallucination and LLM knowledge cutoffs; summarization steps in some systems (e.g., PaperQA2) may dilute fact-level information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4288.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4288.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2 (retrieval-augmented generative agent used for scientific research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An academic RAG system used for scientific literature question answering and synthesis; included here both as a referenced academic tool and as an evaluated implementation within the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrievalaugmented generative agent for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.1 (as configured in this paper's implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Standard academic RAG implementation using semantic search over OCR-enhanced documents, multi-stage summarization and retrieval (evidence retrieval k up to 30), and GPT-based generation with capped citations (maximum 5 citations per response). A modified variant uses domain-adapted prompts and reduced k for focused evidence retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>scientific literature (applied here to cosmology)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>synthesis of factual claims and evidence-backed summaries (not explicitly described as extracting formal laws, but used to generate concise scientific answers and evidence citations)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert evaluation on CosmoPaperQA; additionally compared via AI judges (OpenAI and Gemini) in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-evaluated accuracy on CosmoPaperQA: 81.9% (PaperQA2), Modified PaperQA2 reported 73.3% in some AI-judge comparisons per the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against commercial RAGs (OpenAI, VertexAI), hybrid systems, and non-RAG baselines; found to lag behind top commercial RAGs by ~4.8–9.5 percentage points on CosmoPaperQA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Academic RAG pipelines like PaperQA2 perform solidly but may underperform commercial/hybrid RAGs in this cosmology benchmark, potentially because summarization steps can dilute precise factual content needed for expert-level QA.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Summarization and intermediate processing steps may remove critical factual details; domain adaptation and prompt engineering necessary for optimal performance; scalability and cost trade-offs relative to other RAG configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4288.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4288.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdversarialPrompting-HypothesisGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial prompting & in-context learning for hypothesis generation (Ciucȃ et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach where LLMs, via in-context learning and adversarial prompting, are used to synthesize diverse astronomical information into coherent and innovative hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Adversarial prompting / in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use of carefully designed prompts and adversarial examples to drive LLMs to generate diverse, robust hypotheses from astronomical text; emphasizes prompting strategy to elicit creative scientific hypotheses from model knowledge and retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>hypothesis generation and formulation of scientific conjectures (interpretive/theoretical hypotheses rather than formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper (referenced work summarised as demonstrating synthetic hypothesis capabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be prompted to synthesize information into novel hypotheses, demonstrating potential for automated idea generation in astronomy when using adversarial/in-context strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Risk of hallucination and need for careful prompt design; results in related work are quoted as promising but require domain validation and systematic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4288.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4288.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KnowledgeEntityExtraction-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based astronomical knowledge entity extraction (Shao et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study demonstrating that LLMs, with carefully designed prompting strategies, can extract specialized knowledge entities from astrophysics journal articles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Astronomical knowledge entity extraction in astrophysics journal articles via large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompted LLM entity extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Designing domain-specific prompts to guide LLMs to identify and extract structured knowledge entities (e.g., parameters, measurement results, methodological elements) from papers' textual content.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>astrophysics / astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured knowledge entities and factual claims (entity extraction rather than formal law derivation)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper's summary; presented as related work showing effectiveness of prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can successfully extract domain-specific entities from astrophysics literature when guided by tailored prompts, supporting downstream synthesis and QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Quality depends on prompt design and LLM capabilities; potential biases and errors in extraction must be validated by domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4288.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4288.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LanguageAgents-SuperhumanSynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language agents achieving superhuman synthesis of scientific knowledge (Skarlinski et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work claiming that RAG-enabled language agents can achieve superhuman performance on literature QA benchmarks (LitQA2), indicating strong capabilities for synthesizing scientific findings from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language agents achieve superhuman synthesis of scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RAG language agents (PaperQA2 family and variants)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combining retrieval over scientific corpora with generative LLMs to synthesize and answer literature-based scientific questions; used in benchmarks like LitQA2 to assess synthesis capability.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>scientific literature (biology cited as success domain; referenced as generalizable)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>synthesis of scientific knowledge, evidence-backed answers, and possibly extraction of empirical generalizations from literature</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Benchmark evaluation (LitQA2) demonstrating high performance; described as 'superhuman' in referenced work though specific metrics are not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to prior literature QA systems and human baselines on LitQA2 (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG agents can markedly improve literature synthesis performance and, in some benchmarks, surpass human baselines, suggesting utility for automated extraction of scientific insights.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Benchmark success in one domain (biology) may not translate directly to others; requires domain-specific retrieval and evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4288.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4288.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pathfinder (semantic framework for literature review and knowledge discovery in astronomy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced retrieval and literature-review framework tailored for astronomy that uses query expansion, reranking, and domain-specific weighting to improve retrieval for knowledge discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Pathfinder retrieval framework</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Implements query expansion, reranking, and domain-specific weighting schemes to enhance retrieval performance from astronomical literature, facilitating downstream synthesis by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>supports discovery of patterns and insights from literature via improved retrieval (enables subsequent synthesis rather than explicitly extracting formal laws)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed here; referenced as a system designed to improve retrieval for literature review and knowledge discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain-adapted retrieval enhancements (query expansion, reranking) can materially improve the information available to LLM synthesizers for scientific discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Retrieval quality strongly affects downstream synthesis; scaling to large corpora and tuning domain weights remain practical challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4288.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4288.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist (automated open-ended scientific discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced concept/work advocating toward fully automated, open-ended scientific discovery where AI systems autonomously generate and test scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AI Scientist paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>High-level multi-component systems combining literature synthesis, hypothesis generation, experiment design, and automated evaluation to progress toward autonomous discovery; LLMs are a component for literature understanding and hypothesis formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general scientific discovery (cited in context of astronomy/ML research)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>automated hypothesis/theory generation and iterative scientific discovery (conceptual/theoretical outputs rather than formal derivations)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Conceptual and experimental prototypes in cited work; specific evaluation not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Positioned as an aspirational direction: integrating LLM-driven literature synthesis with experimental loops could enable open-ended discovery, but requires robust retrieval, validation, and experiment automation.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Substantial technical and validation hurdles remain (hallucination, experiment automation, domain validation, cost and scale).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. <em>(Rating: 2)</em></li>
                <li>Astronomical knowledge entity extraction in astrophysics journal articles via large language models. <em>(Rating: 2)</em></li>
                <li>Paperqa: Retrievalaugmented generative agent for scientific research. <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge. <em>(Rating: 2)</em></li>
                <li>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy. <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>Litqa2: A scientific literature question answering dataset. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4288",
    "paper_id": "paper-280081994",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "SciRag",
            "name_full": "SciRag (Retrieval-Augmented Generation Implementation Pipeline)",
            "brief_description": "A modular RAG framework introduced in this paper for systematic integration, benchmarking, and deployment of multiple RAG agents to process scientific papers and synthesize domain knowledge for autonomous discovery in astrophysics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "method_name": "SciRag",
            "method_description": "A multi-stage RAG pipeline that preprocesses documents (OCR optional), creates vector or hybrid retrieval indices (ChromaDB, OpenAI/Vertex vector stores), retrieves ranked document chunks (top-k retrieval), and conditions high-performance LLMs (OpenAI GPT-4.1, Gemini variants, etc.) with retrieved context to generate concise, evidence-backed answers. Evaluation includes deterministic decoding (temperature=0.01), configurable retrieval similarity thresholds, and parallel comparisons across commercial, hybrid, and academic RAG stacks.",
            "number_of_papers": "5 papers",
            "domain_or_field": "astronomy / cosmology",
            "type_of_laws_extracted": "synthesis of factual claims, hypotheses, and interpretive scientific conclusions from paper content (empirical generalizations and domain-specific interpretations)",
            "example_laws_extracted": null,
            "evaluation_method": "Human expert binary scoring (0/1 scaled to 0-100) across 945 responses; calibrated LLM-as-a-Judge (LLMaaJ) validated against human expert judgments",
            "performance_metrics": "System-level accuracy on CosmoPaperQA: OpenAI-based RAG up to 91.4% (human-evaluated); PaperQA2 implementation 81.9% (human-evaluated); hybrid systems 84.8-85.7%; baselines (non-RAG) ~16-17% (human-evaluated).",
            "comparison_baseline": "Compared RAG-enabled agents (OpenAI, VertexAI, hybrid, PaperQA2) against non-RAG baselines (Gemini Assistant, Perplexity Assistant) showing large performance gaps; human expert evaluation used as ground truth.",
            "key_findings": "RAG pipelines that combine high-quality embeddings, domain-tailored retrieval, and strong generative LLMs significantly outperform non-RAG models at extracting and synthesizing domain knowledge from papers; calibrated LLM judges can match human expert rankings and enable scalable evaluation.",
            "challenges_limitations": "Limited corpus size (5 papers) compared to realistic literature scale; potential retrieval cues due to explicit paper references in questions; risks of hallucination and LLM knowledge cutoffs; summarization steps in some systems (e.g., PaperQA2) may dilute fact-level information.",
            "uuid": "e4288.0",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2 (retrieval-augmented generative agent used for scientific research)",
            "brief_description": "An academic RAG system used for scientific literature question answering and synthesis; included here both as a referenced academic tool and as an evaluated implementation within the paper's experiments.",
            "citation_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "mention_or_use": "use",
            "model_name": "GPT-4.1 (as configured in this paper's implementation)",
            "model_size": null,
            "method_name": "PaperQA2",
            "method_description": "Standard academic RAG implementation using semantic search over OCR-enhanced documents, multi-stage summarization and retrieval (evidence retrieval k up to 30), and GPT-based generation with capped citations (maximum 5 citations per response). A modified variant uses domain-adapted prompts and reduced k for focused evidence retrieval.",
            "number_of_papers": null,
            "domain_or_field": "scientific literature (applied here to cosmology)",
            "type_of_laws_extracted": "synthesis of factual claims and evidence-backed summaries (not explicitly described as extracting formal laws, but used to generate concise scientific answers and evidence citations)",
            "example_laws_extracted": null,
            "evaluation_method": "Human expert evaluation on CosmoPaperQA; additionally compared via AI judges (OpenAI and Gemini) in this study.",
            "performance_metrics": "Human-evaluated accuracy on CosmoPaperQA: 81.9% (PaperQA2), Modified PaperQA2 reported 73.3% in some AI-judge comparisons per the paper's discussion.",
            "comparison_baseline": "Compared against commercial RAGs (OpenAI, VertexAI), hybrid systems, and non-RAG baselines; found to lag behind top commercial RAGs by ~4.8–9.5 percentage points on CosmoPaperQA.",
            "key_findings": "Academic RAG pipelines like PaperQA2 perform solidly but may underperform commercial/hybrid RAGs in this cosmology benchmark, potentially because summarization steps can dilute precise factual content needed for expert-level QA.",
            "challenges_limitations": "Summarization and intermediate processing steps may remove critical factual details; domain adaptation and prompt engineering necessary for optimal performance; scalability and cost trade-offs relative to other RAG configurations.",
            "uuid": "e4288.1",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "AdversarialPrompting-HypothesisGen",
            "name_full": "Adversarial prompting & in-context learning for hypothesis generation (Ciucȃ et al., 2023)",
            "brief_description": "A referenced approach where LLMs, via in-context learning and adversarial prompting, are used to synthesize diverse astronomical information into coherent and innovative hypotheses.",
            "citation_title": "Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Adversarial prompting / in-context learning",
            "method_description": "Use of carefully designed prompts and adversarial examples to drive LLMs to generate diverse, robust hypotheses from astronomical text; emphasizes prompting strategy to elicit creative scientific hypotheses from model knowledge and retrieved context.",
            "number_of_papers": null,
            "domain_or_field": "astronomy",
            "type_of_laws_extracted": "hypothesis generation and formulation of scientific conjectures (interpretive/theoretical hypotheses rather than formal laws)",
            "example_laws_extracted": null,
            "evaluation_method": "Not detailed in this paper (referenced work summarised as demonstrating synthetic hypothesis capabilities).",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "LLMs can be prompted to synthesize information into novel hypotheses, demonstrating potential for automated idea generation in astronomy when using adversarial/in-context strategies.",
            "challenges_limitations": "Risk of hallucination and need for careful prompt design; results in related work are quoted as promising but require domain validation and systematic evaluation.",
            "uuid": "e4288.2",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "KnowledgeEntityExtraction-LLMs",
            "name_full": "LLM-based astronomical knowledge entity extraction (Shao et al., 2024)",
            "brief_description": "A referenced study demonstrating that LLMs, with carefully designed prompting strategies, can extract specialized knowledge entities from astrophysics journal articles.",
            "citation_title": "Astronomical knowledge entity extraction in astrophysics journal articles via large language models.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Prompted LLM entity extraction",
            "method_description": "Designing domain-specific prompts to guide LLMs to identify and extract structured knowledge entities (e.g., parameters, measurement results, methodological elements) from papers' textual content.",
            "number_of_papers": null,
            "domain_or_field": "astrophysics / astronomy",
            "type_of_laws_extracted": "structured knowledge entities and factual claims (entity extraction rather than formal law derivation)",
            "example_laws_extracted": null,
            "evaluation_method": "Not specified in this paper's summary; presented as related work showing effectiveness of prompting strategies.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "LLMs can successfully extract domain-specific entities from astrophysics literature when guided by tailored prompts, supporting downstream synthesis and QA tasks.",
            "challenges_limitations": "Quality depends on prompt design and LLM capabilities; potential biases and errors in extraction must be validated by domain experts.",
            "uuid": "e4288.3",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "LanguageAgents-SuperhumanSynthesis",
            "name_full": "Language agents achieving superhuman synthesis of scientific knowledge (Skarlinski et al., 2024)",
            "brief_description": "Referenced work claiming that RAG-enabled language agents can achieve superhuman performance on literature QA benchmarks (LitQA2), indicating strong capabilities for synthesizing scientific findings from papers.",
            "citation_title": "Language agents achieve superhuman synthesis of scientific knowledge.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "RAG language agents (PaperQA2 family and variants)",
            "method_description": "Combining retrieval over scientific corpora with generative LLMs to synthesize and answer literature-based scientific questions; used in benchmarks like LitQA2 to assess synthesis capability.",
            "number_of_papers": null,
            "domain_or_field": "scientific literature (biology cited as success domain; referenced as generalizable)",
            "type_of_laws_extracted": "synthesis of scientific knowledge, evidence-backed answers, and possibly extraction of empirical generalizations from literature",
            "example_laws_extracted": null,
            "evaluation_method": "Benchmark evaluation (LitQA2) demonstrating high performance; described as 'superhuman' in referenced work though specific metrics are not reproduced here.",
            "performance_metrics": null,
            "comparison_baseline": "Compared to prior literature QA systems and human baselines on LitQA2 (as cited).",
            "key_findings": "RAG agents can markedly improve literature synthesis performance and, in some benchmarks, surpass human baselines, suggesting utility for automated extraction of scientific insights.",
            "challenges_limitations": "Benchmark success in one domain (biology) may not translate directly to others; requires domain-specific retrieval and evaluation pipelines.",
            "uuid": "e4288.4",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Pathfinder",
            "name_full": "Pathfinder (semantic framework for literature review and knowledge discovery in astronomy)",
            "brief_description": "A referenced retrieval and literature-review framework tailored for astronomy that uses query expansion, reranking, and domain-specific weighting to improve retrieval for knowledge discovery tasks.",
            "citation_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Pathfinder retrieval framework",
            "method_description": "Implements query expansion, reranking, and domain-specific weighting schemes to enhance retrieval performance from astronomical literature, facilitating downstream synthesis by LLMs.",
            "number_of_papers": null,
            "domain_or_field": "astronomy",
            "type_of_laws_extracted": "supports discovery of patterns and insights from literature via improved retrieval (enables subsequent synthesis rather than explicitly extracting formal laws)",
            "example_laws_extracted": null,
            "evaluation_method": "Not detailed here; referenced as a system designed to improve retrieval for literature review and knowledge discovery.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Domain-adapted retrieval enhancements (query expansion, reranking) can materially improve the information available to LLM synthesizers for scientific discovery tasks.",
            "challenges_limitations": "Retrieval quality strongly affects downstream synthesis; scaling to large corpora and tuning domain weights remain practical challenges.",
            "uuid": "e4288.5",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "AI-Scientist",
            "name_full": "The AI Scientist (automated open-ended scientific discovery)",
            "brief_description": "A referenced concept/work advocating toward fully automated, open-ended scientific discovery where AI systems autonomously generate and test scientific hypotheses.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "AI Scientist paradigm",
            "method_description": "High-level multi-component systems combining literature synthesis, hypothesis generation, experiment design, and automated evaluation to progress toward autonomous discovery; LLMs are a component for literature understanding and hypothesis formulation.",
            "number_of_papers": null,
            "domain_or_field": "general scientific discovery (cited in context of astronomy/ML research)",
            "type_of_laws_extracted": "automated hypothesis/theory generation and iterative scientific discovery (conceptual/theoretical outputs rather than formal derivations)",
            "example_laws_extracted": null,
            "evaluation_method": "Conceptual and experimental prototypes in cited work; specific evaluation not detailed here.",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Positioned as an aspirational direction: integrating LLM-driven literature synthesis with experimental loops could enable open-ended discovery, but requires robust retrieval, validation, and experiment automation.",
            "challenges_limitations": "Substantial technical and validation hurdles remain (hallucination, experiment automation, domain validation, cost and scale).",
            "uuid": "e4288.6",
            "source_info": {
                "paper_title": "Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy.",
            "rating": 2,
            "sanitized_title": "harnessing_the_power_of_adversarial_prompting_and_large_language_models_for_robust_hypothesis_generation_in_astronomy"
        },
        {
            "paper_title": "Astronomical knowledge entity extraction in astrophysics journal articles via large language models.",
            "rating": 2,
            "sanitized_title": "astronomical_knowledge_entity_extraction_in_astrophysics_journal_articles_via_large_language_models"
        },
        {
            "paper_title": "Paperqa: Retrievalaugmented generative agent for scientific research.",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge.",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy.",
            "rating": 2,
            "sanitized_title": "pathfinder_a_semantic_framework_for_literature_review_and_knowledge_discovery_in_astronomy"
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Litqa2: A scientific literature question answering dataset.",
            "rating": 1,
            "sanitized_title": "litqa2_a_scientific_literature_question_answering_dataset"
        }
    ],
    "cost": 0.01504875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics
9 Jul 2025</p>
<p>Xueqing Xu 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Boris Bolliet 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Adrian Dimitrov 
Equal contribution</p>
<p>Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Andrew Laverick 
Department of Physics
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Francisco Villaescusa-Navarro 
Center for Computational Astrophysics
Flatiron Institute
New YorkNYUSA</p>
<p>Department of Astrophysical Sciences
Prince-ton University
PrincetonNJUSA</p>
<p>Licong Xu 
Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Institute of Astronomy
Uni-versity of Cambridge
CambridgeUnited Kingdom</p>
<p>Í Ñigo Zubeldia 
Kavli Institute for Cos-mology
University of Cambridge
CambridgeUnited Kingdom</p>
<p>Institute of Astronomy
Uni-versity of Cambridge
CambridgeUnited Kingdom</p>
<p>Boris Bolliet</p>
<p>Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics
9 Jul 2025712A7A925B42A9FA722DBEAE48F31A3FarXiv:2507.07155v1[astro-ph.IM]
We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose. 1The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed.We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4% accuracy.Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation.These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent 2 presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs.We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community. 3</p>
<p>Introduction</p>
<p>The rapid advancements of Large Language Models (LLMs) (Liu et al., 2024;Bai et al., 2025) have opened a new era in automated scientific discovery, where AI systems can conduct independent research and generate scientific insights (Lu et al., 2024).In cosmology, automated discovery systems are required to synthesize knowledge across collections of scientific literature, computational models, and observational datasets.The successful implementations require AI infrastructure capable of interacting with the knowledge ecosystem utilized by domain experts, and a specialized computational framework that constitutes the methodological foundation.In this work, we focus on the knowledge integration aspect of automated scientific discovery, specifically targeting the information overload in modern astronomy.</p>
<p>While LLMs have demonstrated impressive capabilities in scientific text analysis (Zhang et al., 2024), their deployment in critical research scenarios remains constricted (Fouesneau et al., 2024), by hallucination (Huang et al., 2025) and knowledge cut-off (Cheng et al., 2024).Retrieval-Augmented Generation (RAG) has emerged as a powerful tool to enhance LLMs' performance with external knowledge (Lewis et al., 2021) to meet scientific accuracy standards.The efficacy of this approach has been demonstrated in biology, where PaperQA2 RAG Agents (Lála et al., 2023;Skarlinski et al., 2024) achieve superhuman performance on LitQA2 (Futurehouse, 2024), a benchmark designed to evaluate knowledge synthesis in real research scenarios.Despite these successes in biology, systematic evaluation of RAG agents in astronomy remains limited by the lack of standardized benchmarks.As annotated by Bowman et al. (Bowman et al., 2015), developing human-annotated benchmarks for doctoral-level scientific research domains remains economically prohibitive.Consequently, evaluation of RAG agents in astronomy is constrained by the absence of authentic evaluation datasets that capture the complexity of real research scenarios.</p>
<p>To address these challenges, we introduce CosmoPaperQA, a high-quality benchmark dataset including 105 expert-curated question-answer pairs derived from five highly-cited cosmological literature.Unlike synthetic benchmarks, Cos-moPaperQA captures authentic research scenarios by extracting questions directly from research papers.</p>
<p>To facilitate a comprehensive and reproducible evaluation of CosmoPaperQA, we develop SciRag, a modular framework designed for systematic integration and benchmarking of multiple RAG Agents for scientific discovery.Our implementation enables evaluation across commercial APIs (Ope-nAI Assistant, VertexAI Assistant), hybrid architectures (ChromaDB with several embedding models), specialized academic tools (PaperQA2), and search-enhanced systems (Perplexity), providing empirical guidance for optimal RAG configuration selection in scientific contexts.</p>
<p>Our systematic evaluation across SciRag implementations reveals significant performance differences across four configuration categories, with commercial solutions (OpenAI Assistant: 89.5-91.4%,VertexAI Assistant: 86.7%) achieving the highest accuracy on CosmoPaperQA.Hybrid architectures (HybridOAIGem: 85.7%, HybridGemGem: 84.8%) show competitive performance while significantly reducing operational costs.Academic tools PaperQA2 (81.9%) show solid performance but lag behind commercial and hybrid SciRag Agents, while baseline approaches (Gemini Assistant: 16.2%, Perplexity Assistant: 17.1%) prove insufficient for expert-level scientific inquiry.</p>
<p>We present four primary contributions that collectively advance the state of RAG evaluation in cosmology:</p>
<p>Benchmark Development: We introduce CosmoPaperQA, a comprehensive benchmark dataset containing 105 expertvalidated question-answer pairs.Implementation Pipeline: We develop SciRag, a modular framework that enables systematic deployment and reproducible comparison of diverse RAG solutions.</p>
<p>Multi-System RAG Performance Analysis: We conduct a systematic evaluation of nine distinct RAG implementations utilizing high-performing LLMs and embedding models, revealing significant performance variations across different system architectures and cost-efficiency trade-offs for scientific applications.</p>
<p>Calibrated AI Judge Evaluation:</p>
<p>We introduce a LLMas-a-Judge (LLMaaJ) system that matches human expert assessment in astronomy, enabling scalable performance evaluation while maintaining the quality standards required for scientific applications.</p>
<p>Related Work</p>
<p>RAG Agents in Cosmology</p>
<p>Recent work has demonstrated the significant potential of LLMs in astronomical research contexts.Ciucȃ et al. (Ciucȃ et al., 2023) showed that through in-context learning and adversarial prompting, LLMs can synthesize diverse astronomical information into coherent and innovative hypotheses, while Shao et al. (Shao et al., 2024) demonstrated their effectiveness in extracting specialized knowledge entities from astrophysics journals using carefully designed prompting strategies.These capabilities have motivated the development of specialized RAG frameworks for astronomy, such as the pathfinder system by Iyer et al. (Iyer et al., 2024), which implements query expansion, reranking, and domain-specific weighting schemes to enhance retrieval performance in scientific applications.</p>
<p>However, the growing deployment of RAG systems in astronomy has highlighted the critical need for systematic evaluation methodologies.Wu et al. (Wu et al., 2024) addressed this challenge by proposing a dynamic evaluation framework using a Slack-based chatbot that retrieves information from arXiv astro-ph papers, emphasizing the importance of real-world user interactions over static benchmarks.While their approach provides valuable insights into user behavior and system usability, it relies on user feedback and reaction data rather than systematic performance assessment against validated ground-truth, highlighting a complementary need for standardized benchamrks that can provide consistent, reproducible evaluation metrics across different RAG implementations.</p>
<p>Benchmarks and Evaluation in Cosmology</p>
<p>Existing evaluation falls into two categories, each with some limitations:</p>
<p>Astronomy-Specific Knowledge Benchmarks: AstroM-Lab 1 (Ting et al., 2024) provides the first comprehensive astronomy-specific evaluation with 4425 AI-generated multiple-choice questions from Annual Review articles.While demonstrating significant performance variations between models with specialized astronomical knowledge, its multiple-choice format and automated question generation limit evaluation to content mastery rather than scientific inquiry workflows.Similarly, Astro-QA (Li et al., 2025) provides a structured evaluation with 3082 questions spanning diverse astronomical topics, demonstrating the application of LLMaaJ evaluation in astronomical contexts.However, its synthetic questions limit its ability to assess the complex, open-ended reasoning required for an authentic scientific research workflow.</p>
<p>General Scientific Evaluation: Broader scientific benchmarks like LitQA2 (Futurehouse, 2024) (Zhong et al., 2025), ScisummNet (Yasunaga et al., 2019) are designed for other scientific domains and may not capture astronomy-specific challenges such as mathematical reasoning about cosmological models, and interpretation of observational constraints.</p>
<p>Methodology</p>
<p>To enable AI systems to interact effectively with domain experts' knowledge bases in astrophysics, we present a comprehensive framework consisting of four integrated components designed to systematically evaluate RAG Agents.</p>
<p>CosmoPaperQA: Benchmark for Authentic Research Scenarios</p>
<p>To address the evaluation challenges identified in the previous section, we manually construct CosmoPaperQA.</p>
<p>We systematically selected five highly influential papers spanning critical areas of modern cosmology: the Planck 2018 cosmological parameters (Aghanim et al., 2020), CAMELS machine learning simulations (Villaescusa-Navarro et al., 2021;2022), local Hubble constant measurements (Riess et al., 2016), and recent Atacama Cosmology Telescope constraints (Calabrese et al., 2025).This curation ensures comprehensive coverage of observational, theoretical, and computational aspects of modern cosmological research.</p>
<p>A team of expert cosmologists generated 105 questionanswer pairs through a rigorous protocol designed to mir-ror research inquiries.The questions in our dataset span multiple complexity levels: (1) factual retrieval requiring specific parameter extraction, (2) synthetic reasoning requiring integration across multiple evidence sources, and (3) analytical interpretation requiring deep domain knowledge.</p>
<p>Each pair underwent expert validation to ensure scientific accuracy and representativeness of real research scenarios, distinguishing our benchmark from synthetic alternatives that lack authentic complexity.</p>
<p>Hence, CosmoPaperQA is designed for the following evaluations: zero-shot learning, answering without prior training on specific question types; open-ended questions, mirroring research scenarios; and multi-source knowledge synthesis, requiring integration across observational, theoretical, and computational domains.</p>
<p>SciRag: RAG Implementation Pipeline</p>
<p>Our preprocessing pipeline addresses the requirements of astronomical literature through multi-stage processing.Optical character recognition (OCR) integration using Mistral's advanced capabilities (Mistral AI, 2025)  All RAG systems perform retrieval over the complete corpus of 5 papers, regardless of which paper a specific question was derived from.This design tests the system's ability to identify and retrieve relevant information from the correct source paper among multiple cosmological documents.</p>
<p>We evaluate nine RAG implementations spanning commercial APIs (OpenAI, VertexAI), hybrid architectures (Chro-maDB with OpenAI/Gemini embeddings), academic tools (PaperQA2), and search-enhanced systems (Perplexity).All systems use temperature=0.01 and top-k=20 for consistent evaluation.Detailed analysis is in Appendix A.</p>
<p>Dual Evaluation Framework: Human Expert and Calibrated AI Assessment</p>
<p>To evaluate the quality of RAG Agents' responses in cosmological research contexts, we compare generated answers against expert-validated ground-truth responses to determine whether core factual claims in generated responses align with ground-truth.</p>
<p>While a single domain expert would be the optimal evaluator for this evaluation task, human-expert evaluation faces critical scalability limitations that make it impractical to evaluate across multiple RAG Agents.To address this scalability challenge, we implement a calibrated LLMaaJ system for automated response evaluation.However, we maintain scientific rigor by conducting parallel human expert evaluations on our benchmark results to validate the AI judges' performance and ensure assessment quality.Detailed evaluation setup is in Appendix B. After obtaining the scores, we scaled them to 0-100 for comparison between different system configurations.</p>
<p>Results</p>
<p>Human Evaluated Results</p>
<p>From the expert-evaluated results, we observe that the topperforming ones (OpenAIPDF, OpenAI, VertexAI) are all commercial RAGs, achieving 86.7-91.4% accuracy.Both hybrid implementations (HybridOAIGem: 85.7% , Hy-bridGemGem: 84.8% ) achieve performance competitive with commercial RAGs.PaperQA2 (81.90%) demonstrates solid performance but lags by 4.8-9.5 % compared to top performers.The poor performance of Perplexity Assistant (17.1%) and Gemini Assistant (16.2%) shows that unfiltered web search and non-RAG integration are insufficient for expert-level scientific inquiry, reinforcing the essential role of RAG Agents in scientific knowledge synthesis for autonomous scientific discovery.These clear performance distinctions between different system architectures validate CosmoPaperQA as an effective benchmark for distinguish-ing RAG agents' capabilities in authentic scientific research scenarios.</p>
<p>AI Evaluated Results</p>
<p>Evaluation Concordance: Both OpenAI and Gemini judges preserve the performance ranking observed in human evaluation.The performance gaps are preserved: baseline systems achieve 11.4-18.1% (OpenAI judge) and 16.2-31.4%(Gemini judge), while top-performing agents reach 80.0-84.8%(OpenAI judge) and 88.6-91.4% (Gemini judge).</p>
<p>Judge-Specific Patterns: The OpenAI judge demonstrates conservative scoring, consistently rating systems 2-8% lower than human experts across all categories.In contrast, the Gemini judge exhibits systematic overrating, scoring systems 5-15 percentage points higher than human evaluation (e.g., Gemini Baseline: 27.6% vs Human: 16.2%, Modified PaperQA2: 81.9% vs Human: 73.3%).This overrating pattern suggests that Gemini judge may be overly optimistic in assessing scientific accuracy.</p>
<p>For researchers seeking robust performance estimates, the OpenAI judge's conservative scoring provides a safer lower bound for system capabilities, while Gemini's optimistic scoring may overestimate real-world performance.Despite these systematic biases, the consistent ranking order across all three evaluation methods (Pearson r &gt; 0.99) demonstrates the robustness of our assessment framework.Ver-texAI demonstrates superior cost-efficiency while maintaining strong performance, while OpenAI achieves highest accuracy at a greater operational cost.Detailed cost analysis is provided in Appendix E.</p>
<p>Discussion and Future Work</p>
<p>While CosmoPaperQA represents a first step in systematic astronomical RAG evaluation, several design choices warrant discussion.Many questions explicitly reference their source papers (e.g., Cosmology From One Galaxy?questions mention the paper title, others reference Planck 2018 or ACT DR6).This was intentionally adopted to ensure clear answer provenance and facilitate rigorous evaluation.However, researchers typically formulate queries around scientific concepts without specifying source documents, and our explicit references may systematically improve RAG performance by providing retrieval cues.</p>
<p>Additionally, our five-paper corpus, while enabling expert evaluation, is more constrained than typical research contexts where systems must search thousands of papers or use web search, likely leading to degraded retrieval performance due to increased noise and irrelevant content.Future iterations should incorporate naturalistic question formulations and progressively larger document collections to test sys-tems' ability to identify relevant sources without explicit guidance and understand how accuracy scales with corpus size.</p>
<p>Our results also reveal important insights into retrieval mechanisms that drive performance differences.OpenAI Assistants (89.5-91.4%)use OpenAI's file search tool, which combines automatic query rewriting, parallel searches, keyword and semantic search, and result reranking.This multi-faceted approach outperforms simple semantic-only retrieval used in hybrid systems (84.8-85.7%).Future work should evaluate domain-specific retrieval enhancements such as hybrid sparse-dense methods, contextual chunk expansion, query decomposition strategies, and multi-hop reasoning approaches to further optimize RAG performance for scientific applications.</p>
<p>The calibrated LLMaaJ evaluators developed in this work enable the next phase of our research: building AI questioner systems that can automatically generate domain-specific questions.Our current dataset of 945 human-evaluated responses provides a valuable training foundation for developing such automated question generation capabilities, potentially scaling evaluation to much larger document corpora.</p>
<p>The evaluation framework could be extended to other scientific domains such as chemistry, biology, or materials science to demonstrate generalizability.Despite these limitations, our framework provides a foundation for more comprehensive astronomical RAG benchmarks.</p>
<p>Conclusion</p>
<p>We have evaluated 9 agent configurations on 105 Cosmology Question-Answer (QA) pairs that were built specifically for this purpose, based on 5 carefully selected papers.The papers were selected for their impact on the field and the quality of the presentation of their results, and their relevance to the autonomous discovery systems that we are building, e.g., cmbagent, presented in a companion paper.</p>
<p>The 9 agent configurations were manually evaluated by a human expert with more than 10 years of experience in the field, that is, a total of 945 generated answers were assessed.We find that currently the best RAG agent configuration uses OpenAI embedding and generative models, achieving 91.4% accuracy.VertexAI (86.7%) and hybrid architectures (84.8-85.7%)demonstrate competitive performance.These configurations outperform academic tools uch as PaperQA2 (81.9%), (Lála et al., 2023;Skarlinski et al., 2024), which we attribute to the summarization steps in such systems that may dilute specific factual information critical for our evaluation tasks.Notably, online tools like Perplexity perform poorly (17.1%), showing essentially no advantage over frontier LLMs without RAG (16.2%), indicating that unfiltered web search is insufficient for expert-level scientific inquiry.</p>
<p>Using our human evaluation results, we are able to calibrate evaluator agents which can be used as robust proxy for human evaluation.These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics and provide us with AI evaluators that can be scaled to much larger evaluation datasets.By themselves, our 945 manually evaluated QA pairs constitute a precious dataset that can serve for the calibration of future AI evaluator agents.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Detailed System Configurations</p>
<p>All generation agents are configured with a temperature of 0.01 for consistent, deterministic responses, and top-k=20 (retrieving the 20 most similar document chunks per query) excluding Gemini Assistant, PaperQA2 (both versions) and Perplexity Assistant.The implementation provides both semantic search and hybrid retrieval capabilities across different backends, with specific configurations optimized for each system's strengths.Here are the configurations that we use for each assistant.</p>
<p>OpenAI Assistant: Direct implementation of OpenAI vector stores with file search tool (providing automatic query rewriting, parallel searches, keyword+semantic search, and result reranking) with text-embedding-3-large (OpenAI, 2023) for embeddings and GPT-4.1 for generation, with configurable retrieval parameters (similarity threshold=0.5).</p>
<p>OpenAIPDF Assistant: Direct PDF processing implementation without OCR preprocessing, enabling comparison of raw PDF handling versus OCR-enhanced document processing.Identical configuration to OpenAI Assistant, but operates on unprocessed PDF documents.</p>
<p>VertexAI Assistant: Google Cloud implementation using Google's text-embedding-005 for embeddings and gemini-2.5flash-preview-05-20(Google DeepMind, 2025) for generation.Creates RAG corpora through Vertex AI infrastructure with automatic document ingestion from Google Cloud Storage buckets.Supports semantic search with configurable similarity thresholds (0.5).</p>
<p>Gemini Assistant: Direct integration with Google's Gemini model gemini-2.5-flash-preview-05-20for baseline comparison without specialized RAG infrastructure.</p>
<p>HybridGemGem Assistant: Dual-Gemini implementation using Gemini's text-embedding-001 for embedding, leading embedding model on MTEB (Muennighoff et al., 2023) 4 with ChromaDB storage and gemini-2.5-flash-preview-05-20for generation.Supports ChromaDB backends with semantic-only search.</p>
<p>HybridOAIGem Assistant: Cross-platform architecture identical to HybridGemGem but specifically configured with OpenAI embeddings (text-embedding-3-large) and gemini-2.5-flash-preview-05-20,enabling comparison of embeddinggeneration combinations.</p>
<p>PaperQA2: Standard academic RAG implementation utilizing GPT-4.1 across all components (search, summarization, retrieval), evidence retrieval k=30, maximum 5 citations per response (optimal settings from original work).Processes OCR-enhanced documents with semantic-only search.</p>
<p>Modified PaperQA2: Domain-adapted version with identical technical configuration but specialized astronomical prompts and cosmological citation protocols.Uses evidence retrieval k=10 (reduced from standard k=30) for more focused responses.</p>
<p>Perplexity Assistant: Web-search enabled system using sonar-reasoning-pro model with real-time access to current literature.No local vector storage -relies entirely on web retrieval.</p>
<p>This diverse implementation suite enables comprehensive comparison across commercial, academic, and hybrid approaches, providing empirical guidance for selecting optimal RAG configurations for autonomous scientific discovery workflows.</p>
<p>B. Evaluation Setup</p>
<p>A domain expert is provided (1) a question query, (2) an ideal solution validated by experts, and (3) an RAG Agent-generated response.Then, evaluation is based on</p>
<p>Correct (1): Generated responses demonstrate factual accuracy, and capture essential scientific understanding equivalent to the ideal answer.</p>
<p>Incorrect (0): Generated responses contain errors, contradict established scientific knowledge, or fail to include all the core concepts of ideal answers.</p>
<p>After obtaining the scores, we scaled them to 0-100 for comparison between different system configurations.</p>
<p>The cosmologist who evaluated the response is a domain expert with a PhD-level degree currently working as a researcher in astronomy, astrophysics, or physics.Together with this cosmologist, we designed the evaluation criteria and pipeline to ensure alignment with authentic research standards.In total, our expert evaluated 945 responses (9 systems × 105 questions) generated by RAG Agents.</p>
<p>We explored LLM-as-a-Judge (LLMaaJ) (Gu et al., 2025;Zheng et al., 2023), an AI-based evaluation system calibrated for scientific research queries, using a binary scoring protocol aligned with human expert methodology.Our prompting experiments in Appendix D revealed that chain-of-thought, which asks models to formulate their underlying reasoning process, typically enhances evaluation accuracy and improves concordance with field expert judgments.</p>
<p>To investigate the bias of the pipeline specifically, as LLM evaluators may prefer responses generated by themselves (Dai et al., 2024), we used two LLM-as-a-Judge settings.Given that majority of generation systems utilize either OpenAI or Gemini-based agents, with the exception of the Perplexity Agent, we used the OpenAI o3 mini and Gemini gemini-2.5-propreview-06-05,reasoning models for evaluation.</p>
<p>Research Papers Document</p>
<p>Preprocessing</p>
<p>SciRag</p>
<p>CosmoPaperQA</p>
<p>Retrieval Generation</p>
<p>Retrieved Chunks</p>
<p>Factual Retrieval</p>
<p>How large is the impact of beam window functions on the 2018 spectra in the baseline Plik likelihood?</p>
<p>Synthetic Reasoning</p>
<p>What parameters and initial conditions are varied in the simulations that are run in CAMELS and how are they varied for each simulation?</p>
<p>Analytical Interpretation</p>
<p>Are the neural networks or the symbolic regression equations better at modelling the evolution of cosmological quantities with the redshift in the CAMELS results?</p>
<p>C. RAG Prompts</p>
<p>Our modified PaperQA2 prompt priorities conciseness and domain specificity for efficient human evaluation.</p>
<p>Perplexity Assistants Prompt</p>
<p>You are a scientific literature search agent specializing in cosmology.</p>
<p>We perform retrieval on the following set of papers: {paper list} Your task is to answer questions using ONLY information from these specific papers.CRITICAL: Your answer section must contain no more than 3 sentences total.Count your sentences carefully.You must search your knowledge base calling your tool.The sources must be from the retrieval only.Your response must be in JSON format with exactly these fields:</p>
<p>-"answer": Your 1-3 sentence response with citations -"sources": Array of citation numbers used (e.g., ["1", "2"]) Gemini Assistant's approach to leveraging pre-trained knowledge of specific cosmological papers without requiring external retrieval mechanisms.</p>
<p>Gemini Assistant Prompt</p>
<p>You are a scientific literature agent specializing in cosmology.</p>
<p>You have access to the following key cosmology papers in your knowledge base: {paper list} Your task is to answer cosmology questions using your knowledge of these papers and general cosmology knowledge.</p>
<p>D. CoT Prompts</p>
<p>AI judges are given the following prompt:</p>
<p>Judge Prompt</p>
<p>You are an expert scientific evaluator assessing the quality of scientific responses against reference answers.</p>
<p>Your task is to evaluate responses using one critical criterion: ACCURACY (0-100): CRITICAL: Use ONLY these two scores for accuracy:</p>
<p>-100: The answer contains the core correct factual content, concepts, and conclusions from the ideal answer -0: The answer is fundamentally wrong or contradicts the ideal answer This is a BINARY evaluation -either the answer is essentially correct (100) or fundamentally incorrect (0).No partial credit or intermediate scores allowed.</p>
<p>EVALUATION GUIDELINES:</p>
<p>-Focus ONLY on whether the main scientific concepts and conclusions are correct -Check that the core factual claims from the ideal answer are present in the generated answer -Verify the overall conceptual direction and main conclusions align -Additional correct information beyond the ideal answer is acceptable -Only award 0 if the answer contradicts the ideal answer or gets the main concepts wrong -Award 100 if the answer captures the essential correct scientific understanding Provide your evaluation with the numerical score and detailed rationale explaining why you chose 100 or 0.""" Please evaluate this system's response against the ideal answer: QUESTION: {question} GENERATED ANSWER: {generated answer} IDEAL ANSWER: {ideal answer} Evaluate based on: Accuracy (0-100): How factually correct is the answer compared to the ideal?Use the evaluate response function to provide your structured evaluation with detailed rationale.</p>
<p>E. Cost Performance Analysis</p>
<p>Cost considerations are critical for scientific research deployment, where institutions face budget constraints and researchers require sustainable access to AI-powered literature analysis tools.While our evaluation represents a controlled academic setting, understanding cost-performance trade-offs enables informed decisions for scaling RAG systems across research groups, institutions, and broader scientific communities.</p>
<p>Figure 2 .
2
Figure 2. SciRag System Architecture and CosmoPaperQA Benchmark Overview.Our framework integrates document preprocessing, retrieval mechanisms, and multi-provider generation to enable systematic evaluation of RAG Agents on astronomical literature.</p>
<p>, ChemRAG-Toolkit
100OpenAI Judge Gemini Judge Human Eval (Reference)81.983.781.980.085.784.883.790.585.778.185.786.780.088.689.584.890.591.47573.371.465.7Accuracy (%)5031.427.62518.116.217.111.40Gemini (Baseline)PerplexityModified PaperQA2PaperQA2HybridGemGem HybridOAIGemVertexAIOpenAIOpenAIPDFFigure 1. Performance comparison of SciRag Agents across three evaluation methods. Vertical dashed lines separate different configu-
ration categories: baseline systems (Gemini, Perplexity), academic RAG tools (Modified PaperQA2, PaperQA2), hybrid architectures (HybridGemGem, HybridOAIGem, VertexAI), and commercial solutions (OpenAI, OpenAIPDF).The first two entries (Gemini Baseline and Perplexity) do not perform RAG but simply rely on pre-trained LLM knowledge and, for Perplexity, built-in retrieval tools.</p>
<p>Do not use any other sources or general knowledge beyond what these papers contain.
Instructions:1. Search for information relevant to the question within the specified papers2. Provide a CONCISE answer in EXACTLY 1-3 sentences. Do not exceed 3 sentences under any circumstances.3. Add numerical references [1], [2], [3], etc. corresponding to the paper numbers listed above4. If the papers don't contain sufficient information, state this clearly in 1-2 sentences maximum5. Focus ONLY on the most important quantitative results or key findings6. Be precise, direct, and avoid any unnecessary elaboration or context</p>
<p>You are a retrieval agent.You must add precise source from where you got the answer.Your answer should be in markdown format with the following structure: <strong>Answer</strong>:{answer} <strong>Sources</strong>:{sources} You must search your knowledge base calling your tool.The sources must be from the retrieval only.You must report the source names in the sources field, if possible, the page number, equation number, table number, section number, etc.
OpenAI/Vertex Assistants PromptInstructions: 1. Answer the question based on your knowledge of cosmology and the listed papers2. Provide a CONCISE answer in EXACTLY 1-2 sentences maximum3. Add numerical references [1], [2], [3], etc. when citing the specific papers listed above4. Focus ONLY on the most important quantitative results or key findings5. Be precise, direct, and avoid any unnecessary elaborationPaper reference guide:[1] -Planck 2018 cosmological parameters[2] -CAMELS machine learning cosmology simulations[3] -Single galaxy cosmology analysis[4] -Local Hubble constant measurement (Riess et al.)[5] -Atacama Cosmology Telescope DR6 resultsCRITICAL: Your answer must be no more than 2 sentences total. Count your sentences carefully.Your response must be in JSON format with exactly these fields:-"answer": Your 1-2 sentence response with citations-"sources": Array of paper citations [1]-[5] that are relevant to your answerOpenAI/VertexAI assistants use a tool-based retrieval approach with markdown formatting, emphasising precise source andknowledge integration.
 Retrieved on 30-05-2025 <br />
AcknowledgmentsThe work of BB was partially funded by an unrestricted gift from Google, the Cambridge Centre for Data-Driven Discovery Accelerate Programme and the Infosys-Cambridge AI Centre.We are very grateful to the referees and panel of the ICML 2025 ML4ASTRO workshop for reviewing and accepting our work.Author ContributionsXX led the work and wrote the paper.BB led the work, supervised XX and AD, and provided the human evaluation for all the 945 answers.AD created the CosmoPaperQA benchmark dataset.AL, FVN, LX and IZ provided crucial input at various stages of this work.Modified PaperQA2 PromptProvide a concise answer in 1-2 sentences maximum.Context (with relevance scores):{context} Question: {question} Write a concise answer based on the context, focusing on astronomical facts and concepts.If the context provides insufficient information, reply {CANNOT ANSWER PHRASE}.Write in the style of a scientific astronomy reference, with precise and factual statements.The context comes from a variety of sources and is only a summary, so there may be inaccuracies or ambiguities.{prior answer prompt} Answer (maximum one sentence):In contrast, the original prompt emphasizes comprehensive information synthesis, mandatory citation and Wikipedia-style formatting.PaperQA2 PromptAnswer the question below with the context.Write in the style of a Wikipedia article, with concise sentences and coherent paragraphs.The context comes from a variety of sources and is only a summary, so there may inaccuracies or ambiguities.If quotes are present and relevant, use them in the answer.This answer will go directly onto Wikipedia, so do not add any extraneous information.{prior answer prompt} Answer ({answer length}):The Hybrid SciRag assistant adopt a structured approach, requiring a JSON format return for consistent response parsing.Hybrid Assistants PromptYou are a helpful assistant.Answer based on the provided context.You must respond in valid JSON format with the following structure: { "answer": "your detailed answer here", "sources": ["source1", "source2", "source3"]} The sources must be from the <strong>Context</strong> material provided.Include source names, page numbers, equation numbers, table numbers, section numbers when available.Ensure your response is valid JSON only.The Perplexity assistant uses web search to specific papers while utilizing its real-time retrieval capabilities.For our 105-question evaluation, total costs ranged from $0.037 (VertexAI) to $5.12 (GPT-4.1 based systems), representing a 137× cost difference.The cost differences reflect underlying model pricing structures: GPT-4.1 costs $0.002 per 1K input tokens and $0.008 per 1K output tokens, while Gemini 2.5 Flash charges $0.00015 per 1K input tokens and $0.0006 per 1K output tokens.For a typical research corpus of 1,000 papers with 10,000 queries, projected costs would range from $35.7 (VertexAI) to $4,880 (OpenAI systems).Hybrid approaches (HybridOAIGem: $0.003182, HybridGemGem: $0.003806) provide compelling cost-performance balance, achieving 84.8-85.7%accuracy while reducing costs by 93% compared to OpenAI systems.This positions them as practical solutions for resource-constrained research environments requiring both high accuracy and operational sustainability.Figure3synthesizes these trade-offs across performance, cost efficiency, and overall value.While OpenAI systems achieve highest accuracy (89.5-91.4%),their poor cost efficiency limits practical deployment scalability.Conversely, VertexAI maximizes value by combining strong performance with exceptional cost efficiency, making it suitable for widespread institutional adoption.
Planck2018 results: Vi. cosmological parameters. N Aghanim, 10.1051/0004-6361/201833910Astronomy and Astrophysics. 1432-0746641A6September 2020</p>
<p>S Bai, Qwen2.5-vl technical report. 2025</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalSeptember 2015Association for Computational Linguistics</p>
<p>The atacama cosmology telescope: Dr6 constraints on extended cosmological models. E Calabrese, 2025</p>
<p>Dated data: Tracing knowledge cutoffs in large language models. J Cheng, M Marone, O Weller, D Lawrie, D Khashabi, B V Durme, 2024</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. I Ciucȃ, Y.-S Ting, S Kruk, K Iyer, 2023</p>
<p>Neural retrievers are biased towards llm-generated content. S Dai, Y Zhou, L Pang, W Liu, X Hu, Y Liu, X Zhang, G Wang, J Xu, 10.1145/3637528.3671882Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USA20249798400704901Association for Computing Machinery</p>
<p>What is the role of large language models in the evolution of astronomy research?. M Fouesneau, 2024</p>
<p>Litqa2: A scientific literature question answering dataset. Futurehouse, 2024</p>
<p>. Google Deepmind, Gemini, </p>
<p>J Gu, X Jiang, Z Shi, H Tan, X Zhai, C Xu, W Li, Y Shen, S Ma, H Liu, S Wang, K Zhang, Y Wang, W Gao, L Ni, J Guo, A survey on llm-as-a-judge. 2025</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, Q Chen, W Peng, X Feng, B Qin, T Liu, 10.1145/3703155ACM Transactions on Information Systems. 1558-2868432January 2025</p>
<p>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy. K G Iyer, 10.3847/1538-4365/ad7c43The Astrophysical Journal Supplement Series. 1538-4365275238November 2024</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Tau Yih, T Rocktäschel, S Riedel, D Kiela, 2021</p>
<p>An astronomical question answering dataset for evaluating large language models. J Li, F Zhao, P Chen, 10.1038/s41597-025-04613-9Scientific Data. 122025</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, 2024</p>
<p>Paperqa: Retrievalaugmented generative agent for scientific research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, 2023</p>
<p>Mistral ocr: Introducing the world's best document understanding api. A I Mistral, 2025. June 6, 2025</p>
<p>MTEB: Massive text embedding benchmark. N Muennighoff, N Tazi, L Magne, N Reimers, Proceedings of the 17th Conference of the European Chapter. A Vlachos, I Augenstein, the 17th Conference of the European Chapterthe Association for Computational Linguistics</p>
<p>Association for Computational Linguistics. Croatia Dubrovnik, doi: 10.18653May 2023</p>
<p>URL. </p>
<p>New embedding models and api updates. Openai, 2023</p>
<p>A 2.4. A G Riess, L M Macri, S L Hoffmann, D Scolnic, S Casertano, A V Filippenko, B E Tucker, M J Reid, D O Jones, J M Silverman, R Chornock, P Challis, W Yuan, P J Brown, R J Foley, 10.3847/0004-637X/826/1/56The Astrophysical Journal. 1538-4357826156July 2016</p>
<p>Astronomical knowledge entity extraction in astrophysics journal articles via large language models. W Shao, P Ji, D Fan, Y Hu, X Yan, C Cui, L Mi, L Chen, R Zhang, 2024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S G Rodriques, A D White, 2024</p>
<p>Astromlab 1: Who wins astronomy jeopardy!?. Y.-S Ting, 2024</p>
<p>The camels project: Cosmology and astrophysics with machine-learning simulations. F Villaescusa-Navarro, 10.3847/1538-4357/abf7baThe Astrophysical Journal. 1538-4357915171July 2021</p>
<p>Cosmology with one galaxy?. F Villaescusa-Navarro, 10.3847/1538-4357/ac5d3fThe Astrophysical Journal. 1538-43579292132April 2022</p>
<p>Designing an evaluation framework for large language models in astronomy research. J F Wu, 2024</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. M Yasunaga, J Kasai, R Zhang, A R Fabbri, I Li, D Friedman, D R Radev, 2019</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Y Zhang, X Chen, B Jin, S Wang, S Ji, W Wang, J Han, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, 2023</p>
<p>Benchmarking retrieval-augmented generation for chemistry. X Zhong, B Jin, S Ouyang, Y Shen, Q Jin, Y Fang, Z Lu, J Han, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-885 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-885</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-885</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-5931c8ac145baf17cec9effc25c051049b7dfd4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5931c8ac145baf17cec9effc25c051049b7dfd4c" target="_blank">Reference-Centric Models for Grounded Collaborative Dialogue</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share is presented.</p>
                <p><strong>Paper Abstract:</strong> We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner’s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e885.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e885.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FULL+PRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-Centric Neural Dialogue Agent (FULL + Pragmatic Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular neural dialogue agent for the OneCommon partially-observable shared-reference game that grounds language with a structured reference resolver, maintains a per-entity recurrent referent memory, and uses an RSA-style pragmatic rescoring procedure to choose discriminative utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reference-Centric Agent (FULL+PRAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A factored, modular neural dialogue system consisting of: (1) a reference detector (BiLSTM-CRF) that finds referring expressions and produces encodings z^k; (2) a structured reference resolution module implemented as a linear-chain CRF with neural potentials (dot-level potentials phi, configuration potentials psi, and transition potentials omega) that grounds referring expressions to sets of dots in the agent's view; (3) a recurrent per-dot referent memory M_t(d) (GRU-based RNN_C) that stores and updates discourse salience and mention history for each dot; (4) a mention prediction module P_M implemented with a CRF and a GRU decoder RNN_M that predicts sequences of referents to mention; (5) an utterance generation seq2seq module with attention over dot encodings and the encoded referents; and (6) a pragmatic generation procedure that searches candidate (referent sequence, utterance) pairs and rescoring them with an RSA-style objective L(r,u)=P_M(r)^{w_M} * P_U(u|r)^{w_S} * P_R(r|u)^{w_L}. Modules share dot encodings w(d), and choice selection uses a CRF-based dot scoring. The pragmatic search uses top-N_r referent candidates and samples N_u utterances per referent with an early-stopping threshold tau.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>OneCommon (shared visual reference dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A partially-observable grounded reference game where two players each see a circular view of an underlying board containing 7 dots (positions, continuous sizes and shades) with 4–6 dots overlapping between views; players communicate in turns to identify and agree on a dot visible to both. Challenging due to continuous perceptual attributes, partial observability, need for common-grounding and pragmatic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>none (the agent does not call external tools; it uses internal modules only: CRF-based resolver, per-dot memory, seq2seq generator, and pragmatic rescoring)</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Structured per-entity recurrent memory: one vector M_t(d) per dot (GRU cell). Memory updates use features iota(d, r_t) derived from the predicted referents at each turn (hard argmax of referent predictions and marginal argmaxes, pooled via max and average), combined with previous M_t(d) through RNN_C to yield M_{t+1}(d). Additionally, a word-level recurrent dialogue context H_t and dot encodings w(d) provide global context.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>After reference resolution of the partner's utterance, predicted referents r_t produce per-dot features via iota(d, r_t) (four values: max/avg pooling over argmax predictions from structured CRF and marginals). Each dot's memory vector is updated independently: M_{t+1}(d) = RNN_C(M_t(d), iota(d, r_t)) where RNN_C is a GRU. The memory thus integrates mention evidence over turns and affects future mention selection and choice selection.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Single-step pragmatic planning over utterances and referent choices using an RSA-style objective that combines probabilities from the mention predictor P_M, utterance generator P_U, and reference resolver-as-listener P_R; selection performed via approximate top-k search with early stopping (no explicit multi-step lookahead planning).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The agent does not use external tools; instead, accurate structured reference resolution (CRF potentials for dots, configurations, and transitions) plus a recurrent per-dot referent memory substantially improves grounding and final choice selection, and an RSA-inspired pragmatic rescoring over candidate (referent, utterance) pairs increases task success (FULL+PRAG > FULL). Factoring generation into reference resolution, memory-updates, mention selection, and pragmatic utterance choice yields better collaboration in a partially-observable text-grounded environment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reference-Centric Models for Grounded Collaborative Dialogue', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e885.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e885.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POMDP-dialog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>POMDP-based statistical spoken dialog systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A body of work (reviewed by Young et al., 2013) that models dialog as a partially-observable Markov decision process, maintaining belief states and planning over dialog actions under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>POMDP-based statistical spoken dialog systems: A review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>POMDP-based dialog agents (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned in related work as classical approaches to goal-oriented dialog that maintain probabilistic belief states over user goals and use POMDP planning for dialog policy; the current paper cites this review but does not use or evaluate such agents.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not detailed in this paper; generally used for partially-observable spoken dialog scenarios where user intents are uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Mentioned as POMDP-based planning (model-based planning under uncertainty) in related work; no implementation details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as foundational work on planning and belief tracking in dialog; the paper contrasts its modular, memory- and CRF-based approach with POMDP-style frameworks and suggests future directions including longer-horizon planning and explicit reasoning under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reference-Centric Models for Grounded Collaborative Dialogue', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e885.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e885.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkTheWalk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk the Walk: Navigating New York City through grounded dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grounded dialogue navigation task where agents coordinate to navigate in a city environment via dialogue; cited as an example of navigation and grounded dialogue with partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk the Walk: Navigating New York City through grounded dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TalkTheWalk agents (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced in related work as an example of grounded dialogue applied to navigation in a partially-observable spatial environment; the current paper does not implement or analyze these agents.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TalkTheWalk (NYC navigation via dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A grounded navigation dialogue environment involving movement through a city and communication; mentioned as a related task that involves partial observability and navigation/path-finding challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as related work illustrating multi-agent grounded navigation tasks; no details on use of external tools or belief integration are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reference-Centric Models for Grounded Collaborative Dialogue', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e885.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e885.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VisionAndDialogNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision-and-dialog navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of tasks combining visual navigation and dialog (e.g., Thomason et al., 2019), cited as related work on navigation and interactive question answering in grounded dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vision-and-dialog navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Vision-and-dialog agents (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as related examples of agents that perform navigation assisted by dialog; the current paper references these works in related work but does not use them.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Vision-and-Dialog navigation environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Environments that combine visual navigation with dialog-based instruction/assistance; they are partially-observable and require path planning and multi-step coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Included as related work connecting dialog and navigation; the paper does not detail how those agents integrate external tools or maintain belief states.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reference-Centric Models for Grounded Collaborative Dialogue', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e885.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e885.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Khani2018</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning, inference and pragmatics in sequential language games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work on planning and pragmatic inference across multiple turns in sequential language games; cited as related work on planning and inference in dialog.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning, inference and pragmatics in sequential language games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sequential language-game agents (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced for approaches that plan or infer across multiple turns in dialog games; the present paper notes that such multi-step planning exists in prior work but does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not specified in this paper; generally applied to sequential language games requiring planning/inference across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Mentioned as multi-step planning and inference in related work; the current paper uses only single-step pragmatic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of prior work that performs planning/inference over longer horizons; the current paper suggests extending to such approaches in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reference-Centric Models for Grounded Collaborative Dialogue', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e885.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e885.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HelpAnnaNav</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual navigation work (Nguyen and Daumé III, 2019) cited among papers that tackle navigation with natural language assistance; mentioned in related work on navigation and interactive question games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Help-Anna navigation agents (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as related work on navigation with multimodal natural language assistance; this paper cites it but does not describe its internal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not described here; generally a visual navigation environment with natural language assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Listed among navigation and interactive question game works in related work; no specifics on tool use or belief integration are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reference-Centric Models for Grounded Collaborative Dialogue', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>POMDP-based statistical spoken dialog systems: A review <em>(Rating: 2)</em></li>
                <li>Talk the Walk: Navigating New York City through grounded dialogue <em>(Rating: 2)</em></li>
                <li>Vision-and-dialog navigation <em>(Rating: 2)</em></li>
                <li>Planning, inference and pragmatics in sequential language games <em>(Rating: 2)</em></li>
                <li>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-885",
    "paper_id": "paper-5931c8ac145baf17cec9effc25c051049b7dfd4c",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "FULL+PRAG",
            "name_full": "Reference-Centric Neural Dialogue Agent (FULL + Pragmatic Generation)",
            "brief_description": "A modular neural dialogue agent for the OneCommon partially-observable shared-reference game that grounds language with a structured reference resolver, maintains a per-entity recurrent referent memory, and uses an RSA-style pragmatic rescoring procedure to choose discriminative utterances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Reference-Centric Agent (FULL+PRAG)",
            "agent_description": "A factored, modular neural dialogue system consisting of: (1) a reference detector (BiLSTM-CRF) that finds referring expressions and produces encodings z^k; (2) a structured reference resolution module implemented as a linear-chain CRF with neural potentials (dot-level potentials phi, configuration potentials psi, and transition potentials omega) that grounds referring expressions to sets of dots in the agent's view; (3) a recurrent per-dot referent memory M_t(d) (GRU-based RNN_C) that stores and updates discourse salience and mention history for each dot; (4) a mention prediction module P_M implemented with a CRF and a GRU decoder RNN_M that predicts sequences of referents to mention; (5) an utterance generation seq2seq module with attention over dot encodings and the encoded referents; and (6) a pragmatic generation procedure that searches candidate (referent sequence, utterance) pairs and rescoring them with an RSA-style objective L(r,u)=P_M(r)^{w_M} * P_U(u|r)^{w_S} * P_R(r|u)^{w_L}. Modules share dot encodings w(d), and choice selection uses a CRF-based dot scoring. The pragmatic search uses top-N_r referent candidates and samples N_u utterances per referent with an early-stopping threshold tau.",
            "environment_name": "OneCommon (shared visual reference dialogue)",
            "environment_description": "A partially-observable grounded reference game where two players each see a circular view of an underlying board containing 7 dots (positions, continuous sizes and shades) with 4–6 dots overlapping between views; players communicate in turns to identify and agree on a dot visible to both. Challenging due to continuous perceptual attributes, partial observability, need for common-grounding and pragmatic reasoning.",
            "is_partially_observable": true,
            "external_tools_used": "none (the agent does not call external tools; it uses internal modules only: CRF-based resolver, per-dot memory, seq2seq generator, and pragmatic rescoring)",
            "tool_output_types": null,
            "belief_state_mechanism": "Structured per-entity recurrent memory: one vector M_t(d) per dot (GRU cell). Memory updates use features iota(d, r_t) derived from the predicted referents at each turn (hard argmax of referent predictions and marginal argmaxes, pooled via max and average), combined with previous M_t(d) through RNN_C to yield M_{t+1}(d). Additionally, a word-level recurrent dialogue context H_t and dot encodings w(d) provide global context.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "After reference resolution of the partner's utterance, predicted referents r_t produce per-dot features via iota(d, r_t) (four values: max/avg pooling over argmax predictions from structured CRF and marginals). Each dot's memory vector is updated independently: M_{t+1}(d) = RNN_C(M_t(d), iota(d, r_t)) where RNN_C is a GRU. The memory thus integrates mention evidence over turns and affects future mention selection and choice selection.",
            "planning_approach": "Single-step pragmatic planning over utterances and referent choices using an RSA-style objective that combines probabilities from the mention predictor P_M, utterance generator P_U, and reference resolver-as-listener P_R; selection performed via approximate top-k search with early stopping (no explicit multi-step lookahead planning).",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "The agent does not use external tools; instead, accurate structured reference resolution (CRF potentials for dots, configurations, and transitions) plus a recurrent per-dot referent memory substantially improves grounding and final choice selection, and an RSA-inspired pragmatic rescoring over candidate (referent, utterance) pairs increases task success (FULL+PRAG &gt; FULL). Factoring generation into reference resolution, memory-updates, mention selection, and pragmatic utterance choice yields better collaboration in a partially-observable text-grounded environment.",
            "uuid": "e885.0",
            "source_info": {
                "paper_title": "Reference-Centric Models for Grounded Collaborative Dialogue",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "POMDP-dialog",
            "name_full": "POMDP-based statistical spoken dialog systems",
            "brief_description": "A body of work (reviewed by Young et al., 2013) that models dialog as a partially-observable Markov decision process, maintaining belief states and planning over dialog actions under uncertainty.",
            "citation_title": "POMDP-based statistical spoken dialog systems: A review",
            "mention_or_use": "mention",
            "agent_name": "POMDP-based dialog agents (mentioned)",
            "agent_description": "Mentioned in related work as classical approaches to goal-oriented dialog that maintain probabilistic belief states over user goals and use POMDP planning for dialog policy; the current paper cites this review but does not use or evaluate such agents.",
            "environment_name": null,
            "environment_description": "Not detailed in this paper; generally used for partially-observable spoken dialog scenarios where user intents are uncertain.",
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Mentioned as POMDP-based planning (model-based planning under uncertainty) in related work; no implementation details in this paper.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as foundational work on planning and belief tracking in dialog; the paper contrasts its modular, memory- and CRF-based approach with POMDP-style frameworks and suggests future directions including longer-horizon planning and explicit reasoning under uncertainty.",
            "uuid": "e885.1",
            "source_info": {
                "paper_title": "Reference-Centric Models for Grounded Collaborative Dialogue",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "TalkTheWalk",
            "name_full": "Talk the Walk: Navigating New York City through grounded dialogue",
            "brief_description": "A grounded dialogue navigation task where agents coordinate to navigate in a city environment via dialogue; cited as an example of navigation and grounded dialogue with partial observability.",
            "citation_title": "Talk the Walk: Navigating New York City through grounded dialogue",
            "mention_or_use": "mention",
            "agent_name": "TalkTheWalk agents (mentioned)",
            "agent_description": "Referenced in related work as an example of grounded dialogue applied to navigation in a partially-observable spatial environment; the current paper does not implement or analyze these agents.",
            "environment_name": "TalkTheWalk (NYC navigation via dialogue)",
            "environment_description": "A grounded navigation dialogue environment involving movement through a city and communication; mentioned as a related task that involves partial observability and navigation/path-finding challenges.",
            "is_partially_observable": true,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned as related work illustrating multi-agent grounded navigation tasks; no details on use of external tools or belief integration are provided in this paper.",
            "uuid": "e885.2",
            "source_info": {
                "paper_title": "Reference-Centric Models for Grounded Collaborative Dialogue",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "VisionAndDialogNav",
            "name_full": "Vision-and-dialog navigation",
            "brief_description": "A class of tasks combining visual navigation and dialog (e.g., Thomason et al., 2019), cited as related work on navigation and interactive question answering in grounded dialogue.",
            "citation_title": "Vision-and-dialog navigation",
            "mention_or_use": "mention",
            "agent_name": "Vision-and-dialog agents (mentioned)",
            "agent_description": "Cited as related examples of agents that perform navigation assisted by dialog; the current paper references these works in related work but does not use them.",
            "environment_name": "Vision-and-Dialog navigation environments",
            "environment_description": "Environments that combine visual navigation with dialog-based instruction/assistance; they are partially-observable and require path planning and multi-step coordination.",
            "is_partially_observable": true,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Included as related work connecting dialog and navigation; the paper does not detail how those agents integrate external tools or maintain belief states.",
            "uuid": "e885.3",
            "source_info": {
                "paper_title": "Reference-Centric Models for Grounded Collaborative Dialogue",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Khani2018",
            "name_full": "Planning, inference and pragmatics in sequential language games",
            "brief_description": "Work on planning and pragmatic inference across multiple turns in sequential language games; cited as related work on planning and inference in dialog.",
            "citation_title": "Planning, inference and pragmatics in sequential language games",
            "mention_or_use": "mention",
            "agent_name": "Sequential language-game agents (mentioned)",
            "agent_description": "Referenced for approaches that plan or infer across multiple turns in dialog games; the present paper notes that such multi-step planning exists in prior work but does not implement it.",
            "environment_name": null,
            "environment_description": "Not specified in this paper; generally applied to sequential language games requiring planning/inference across turns.",
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Mentioned as multi-step planning and inference in related work; the current paper uses only single-step pragmatic planning.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as an example of prior work that performs planning/inference over longer horizons; the current paper suggests extending to such approaches in future work.",
            "uuid": "e885.4",
            "source_info": {
                "paper_title": "Reference-Centric Models for Grounded Collaborative Dialogue",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "HelpAnnaNav",
            "name_full": "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
            "brief_description": "A visual navigation work (Nguyen and Daumé III, 2019) cited among papers that tackle navigation with natural language assistance; mentioned in related work on navigation and interactive question games.",
            "citation_title": "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
            "mention_or_use": "mention",
            "agent_name": "Help-Anna navigation agents (mentioned)",
            "agent_description": "Referenced as related work on navigation with multimodal natural language assistance; this paper cites it but does not describe its internal mechanisms.",
            "environment_name": null,
            "environment_description": "Not described here; generally a visual navigation environment with natural language assistance.",
            "is_partially_observable": null,
            "external_tools_used": null,
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": null,
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Listed among navigation and interactive question game works in related work; no specifics on tool use or belief integration are provided in this paper.",
            "uuid": "e885.5",
            "source_info": {
                "paper_title": "Reference-Centric Models for Grounded Collaborative Dialogue",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "POMDP-based statistical spoken dialog systems: A review",
            "rating": 2
        },
        {
            "paper_title": "Talk the Walk: Navigating New York City through grounded dialogue",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-dialog navigation",
            "rating": 2
        },
        {
            "paper_title": "Planning, inference and pragmatics in sequential language games",
            "rating": 2
        },
        {
            "paper_title": "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
            "rating": 1
        }
    ],
    "cost": 0.015339499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reference-Centric Models for Grounded Collaborative Dialogue</h1>
<p>Daniel Fried ${ }^{\dagger}$ Justin T. Chiu ${ }^{\ddagger}$ Dan Klein ${ }^{\dagger}$<br>${ }^{\dagger}$ Computer Science Division, UC Berkeley<br>${ }^{\ddagger}$ Department of Computer Science, Cornell Tech<br>{dfried,klein}@cs.berkeley.edu, jtc257@cornell.edu</p>
<h4>Abstract</h4>
<p>We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner's utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa, 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a $20 \%$ relative improvement in successful task completion in self-play evaluations and a $50 \%$ relative improvement in success in human evaluations.</p>
<h2>1 Introduction</h2>
<p>In grounded dialogue settings involving high degrees of ambiguity, correctly interpreting and informatively generating language can prove challenging. Consider the collaborative dialogue game shown in Figure 1. Each player has a separate, but overlapping, view on an underlying context. They need to communicate to determine and agree on one dot that they share, and both players win if they choose the same dot. To succeed, each participant must-implicitly or explicitly-ground their partner's descriptions to their own context, maintain a history of what's been described and what their partner is likely to have, and informatively convey parts of their own context.</p>
<p>We present a grounded pragmatic dialogue system which collaborates successfully with people
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>A: I have two large light grey dots with a smaller darker dot below and to the right of it
P: I have one large lightest grey dot in my entire view
A: I have two large light grey dots
P: There is a smaller slightly darker grey dot next to the lightest grey and larger dot
A: Yes, let's pick the light one
P: The light grey and large dot?
A: Yes it 's the one. Let's pick that one
P: SELECT red
A: SELECT blue</p>
<p>Figure 1: An example dialogue produced by our system (A) with a human partner (P). The participants have different but overlapping views of a shared board, which contains dots of different shapes and sizes. The partners must collaborate through dialogue in order to find and select a dot that is visible to both.
on the task above. Figure 1 shows a real example game between our system and a human partner. Our approach is centered around a structured module for perceptually-grounded reference resolution. This reference resolution module plays two roles. First, the module is used to interpret the partner's utterances: explicitly predicting which referents (if any) in the agent's context the partner is referring to, for example a smaller darker grey dot and the lightest grey and larger dot. Second, the reference module is used for pragmatic generation: choosing utterances by reasoning about how the partner might interpret them in context. Our pragmatic generation procedure selects referents to describe as well as choosing how to describe them, for example focusing on the light one (Figure 1).</p>
<p>Much past work that has constructed systems for grounded collaborative dialogue has focused on settings that have asymmetric player roles (Kim et al., 2019; de Vries et al., 2018; Das et al., 2018, 2017), are fully-observable, or are grounded in symbolic attributes (He et al., 2017). In contrast, we focus on the OneCommon corpus and task (Udagawa and Aizawa, 2019), which is symmetric, partially-observable, and has relatively complex spatial and perceptual grounding. These traits necessitate complex dialogue strategies such as common grounding, coordination, clarification questions, and nuanced acknowledgment (Udagawa and Aizawa, 2019), leading to the task being challenging even for pairs of human partners.</p>
<p>Past work on OneCommon has focused on the subtask of reference resolution (Udagawa and Aizawa, 2020; Udagawa et al., 2020) and only evaluated dialogue systems automatically: using static evaluation on human-human games and self-play evaluations that simulate human partners using another copy of the agent. Our system outperforms this past work on these evaluations. We further confirm these results by performing-for the first time on this task-human evaluations, where we find that our system obtains a $50 \%$ relative increase in success rate over a system from past work when paired with human partners. We release code for our system at https://github.com/dpfried/onecommon.</p>
<h2>2 Setting</h2>
<p>We choose to focus on the OneCommon task (Udagawa and Aizawa, 2019) since it is a particularly challenging representative of a class of partially-observable collaborative reference dialogue games (e.g., He et al. 2017; Haber et al. 2019). In this task, two players have different but overlapping views of a game board, which consists of dots of various positions, shades of gray and sizes. The players must coordinate to choose a single dot that both players can see, which is challenging because neither knows which dots the other can see.</p>
<p>Each player's world view, $w$, consists of a circular view on an underlying board containing between 8 and 10 randomly scattered dots, with continuously varying positions, shades, and sizes (Figure 1). Each player's view contains 7 dots, and the views of the players overlap so that there are between 4 and 6 dots which appear in both views.</p>
<p>We focus on a turn-based version of the dialogue task. In a given turn $t$, a player may communicate with their partner by either sending an utterance $u_{t}$ or selecting a dot $s$. In the event of selection, the partner is notified but cannot see which dot the player has selected. Once a player has selected a dot, they can no longer send messages. The dialogue ends once both players have selected a dot, and is successful if both selected the same one.</p>
<h2>3 Model Structure</h2>
<p>Our approach is a modular neural dialogue model which factors the agent's generation process into a series of successive subtasks, all centered on grounding language into referents in the world context. In this section, we describe our model structure, which defines a neural module for each subtask. We then describe our reference-centric pragmatic generation procedure in Section 4.</p>
<p>An overview of the relationship between modules in our model is shown in Figure 2. Each module can condition on neural encodings of the context (the world and past dialogue), as well as the outputs of other modules. We describe our system at a high-level here, then give task-specific implementation details about each component in Section 5.</p>
<h3>3.1 Context Encodings</h3>
<p>Our modules can condition on encodings of (i) the past utterances $u_{1: t}$ in the dialogue, represented as a memory vector $H_{t}$ produced by a word-level recurrent encoder and (ii) the continuous dots in the world context $w$, produced by the entity encoding network of Udagawa and Aizawa (2020), which produces a vector $w(d)$ for each dot $d$ encoding the dot's continuous attributes as well as its pairwise attribute relationships to all other dots in the context (Santoro et al., 2017). (i) and (ii) both follow Udagawa and Aizawa (2020). To explicitly encourage the model to retain and use information about the history of referents mentioned by both players, which affects the choice of future referents as well as the selection of dot at the end of the game, we also use (iii) a structured recurrent referent memory grounded in the context. This memory, inspired by He et al. (2017), has one representation for each dot $d$ in the agent's view, $M_{t}(d)$, which is updated based on the referents predicted in turn $t$. See Section 5.4 for details.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: In a given turn, an agent first identifies referring expressions in their partner's utterance $u_{t}$ using the reference detector (1). Each reference is then resolved with the reference resolution module (2), which uses encoded representations $z^{1: K_{t}}$ of the reference segments and the world context $w$. The referents are then used to update the referent memory $M_{t}$, and cross-referenced against the agent's own dots to confirm whether or not the agent can also see them (3). Given the referent memory $M_{t}$ and confirmation variable $c_{t+1}$, the mention prediction module (4) produces a sequence of dot configurations $z_{t+1}^{1: K_{t+1}}$ to mention. Finally, the utterance generation module (5) uses the dialog history $H_{t}$, confirmation variable, and attended representations of the selected mentions and world context to generate a response $u_{t+1}$.</p>
<h3>3.2 Decomposing Turns into Subtasks</h3>
<p>We assume turn $t+1$ in the dialogue has the following generative process (numbers correspond to Figure 2). Steps (1) and (2) identify and resolve referring expressions in the partner's utterance $u_{t}$; step (3) updates the memory and determines whether the model can confirm any referents from the partner's utterance; steps (4) and (5) produce the agent's next utterance $u_{t+1}$.
(1) First, a sequence of $K_{t}$ (with $K_{t} \geq 0$ ) referring expressions are identified in $u_{t}$ using the reference detector tagging model of Udagawa and Aizawa (2020) ${ }^{1}$, and encodings $\mathbf{z}<em t="t">{t}=z</em>$ are obtained for them by pooling features from a recurrent utterance encoder.
(2) Then, the referring expressions are grounded. From each referring expression's features $z^{k}$, we predict a referent $r^{k}$, which is the set of zero or more dots in the agent's own view which are described by the referring expression. For example, the referring expression three gray dots corresponds to a single referent containing three dots. A reference resolution module $P_{R}\left(\mathbf{r}}^{1: K_{t}<em t="t">{t} \mid \mathbf{z}</em>}, w, M\right)$, where $\mathbf{r<em t="t">{t}=r</em>$, predicts a sequence of referents, one for each referring expression.
(3) Given these referents, the agent updates the referent memory $M_{t}$ using the predicted referents and constructs a discrete confirmation variable $c_{t+1}$, which indicates whether the agent can confirm in}^{1: K_{t}</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>its next utterance that it has all the referents the partner is describing (e.g., Yes, I see that). $c_{t+1}$ takes on one of three values: NA if no referring expressions were in the partner's utterance, Yes if all of the partner's referring expressions have referents that are at least partially visible in the agent's view, and No otherwise.
(4) The agent chooses a sequence of referents to mention next using a mention prediction module $P_{M}\left(\mathbf{r}<em t_1="t+1">{t+1} \mid c</em>, w\right)$.
(5) Finally, the next utterance $u_{t+1}$ is produced using an utterance generation module $P_{U}\left(u_{t+1} \mid\right.$ $\left.\mathbf{r}}, M_{t+1}, H_{t<em t_1="t+1">{t+1}, c</em>$.}, H_{t}, w\right)$, also updating the word-level recurrent memory $H_{t+1</p>
<p>At the end of the dialogue (turn $T$ ), the agent selects a dot $s$ using a choice selection module $P_{S}\left(s \mid H_{T}, M_{T}, w\right)$ (not shown in Figure 2). ${ }^{2}$</p>
<p>Modules that predict referents (reference resolution, mention selection, and choice selection) are all implemented using a structured conditional random field (CRF) architecture (Section 5.2), with independent parameterizations for each module.</p>
<p>Our model bears some similarities to Udagawa and Aizawa (2020)'s neural dialogue model for this task: both models use a reference resolution module ${ }^{3}$ and both models attend to similar encodings</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>of the dots in the agent's world view $(w(d))$ when generating language. Crucially, however, our decomposition of generation into subtasks results in a factored, hierarchical generation procedure: our model identifies and then conditions on previouslymentioned referents from the partner's utterances, ${ }^{4}$ maintains a structured referent memory updated at each utterance, and explicitly predicts which referents to mention in each of the agent's own utterances. In Section 4, we will see how factoring the generation procedure in this way allows us to use a pragmatic generation procedure, and in Section 6 we find that each of these components improves performance.</p>
<h2>4 Pragmatic Generation</h2>
<p>The modules as described above can be used to generate the next utterance $u_{t+1}$ using the predictions of $P_{M}\left(\mathbf{r}<em U="U">{t+1}\right)$ and $P</em>}\left(u_{t+1} \mid \mathbf{r<em R="R">{t+1}\right)$ (omitting other conditioning variables from the notation for brevity; see Section 3 for the full conditioning contexts). This section describes an improvement, pragmatic generation, to this process. Referents and their expressions should be relevant in the dialogue and world context, but they should also be discriminative (Dale, 1989): allowing the listener to easily understand which referents the speaker is intending to describe. Our pragmatic generation approach, based on the Rational Speech Acts (RSA) framework (Frank and Goodman, 2012; Goodman and Frank, 2016), uses the reference resolution module, $P</em>}\left(\mathbf{r<em t_1="t+1">{t+1} \mid u</em>$}\right)$, to predict whether the partner can identify the intended referents. This encourages selecting referents that are easy for the partner to identify and describing them informatively in context. ${ }^{5</p>
<p>We use the following objective over referents $\mathbf{r}$ and utterances $u$ for a given turn:</p>
<p>$$
\begin{aligned}
&amp; \underset{\mathbf{r}, u}{\arg \max } L(\mathbf{r}, u) \
&amp; L(\mathbf{r}, u)=P_{M}(\mathbf{r})^{w_{M}} \cdot P_{U}(u \mid \mathbf{r})^{w_{S}} \cdot P_{R}(\mathbf{r} \mid u)^{w_{L}}
\end{aligned}
$$</p>
<p>where $w_{M}, w_{S}$, and $w_{L}$ are hyperparameters.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Agents optimize for a combination of fluency and informativity during pragmatic utterance generation (Section 4 and Algorithm 1). A set of paired candidate referents (from the mention prediction module) and utterances (from the utterance generation module) is rescored using $L(\mathbf{r}, u)$ (Equation 1), a weighted geometric mean of scores from the mention prediction, utterance, and reference resolution modules. The pair of referent and utterance that maximizes this score is chosen as a response.</p>
<p>This objective generalizes the typical RSA setup (as implemented by the weighted pragmatic inference objective of e.g., Andreas and Klein 2016 and Monroe et al. 2017), which chooses how to describe a given context (i.e., choosing an utterance $u$ ), to also choose what context to describe (i.e., choosing the referents $\mathbf{r}$ ). Our objective also models the tradeoff, explored in past work on referring expression generation (Dale, 1989; Jordan and Walker, 2005; Viethen et al., 2011), between producing utterances relevant in the discourse and world context and producing utterances that are discriminative. We use $P_{U}$ and $P_{M}$ to model discourse and world relevance, $P_{S}$ to model discriminability, and the weights $w$ to empirically model the tradeoff between them.</p>
<p>Given the combinatorially-large spaces of possible $\mathbf{r}$ and $u$, we rely on an early-stopping approximate search, which to our knowledge is novel for RSA. The search (illustrated in Figure 3) iterates through the highest probability structured referent sequences $\mathbf{r}$ under the mention prediction module $P_{M}$ (Figure 3 shows the top two) and for each $\mathbf{r}$ sampling $N_{u}$ utterances $u$ from the utterance generation module (Figure 3 shows three $u$ per $\mathbf{r}$ ). If the maximum of these $(\mathbf{r}, u)$ pairs under $L$ is better than an early-stopping threshold value $\tau$, we return the pair. Otherwise, we continue on to the next $\mathbf{r}$. If more than $N_{r}$ referent sequences have been evaluated, we return the best $(\mathbf{r}, u)$ pair found so far. See Appendix C for pseudocode and a discussion of robustness to the threshold $\tau$.</p>
<h2>5 Module Implementations</h2>
<p>As described so far, our system is applicable to a range of partially-observable grounded collaborative referring expression dialogue tasks (e.g., He et al. 2017; Haber et al. 2019). In this section, we describe implementations of our systems' modules, some of which are tailored to ONECOMMON.</p>
<h3>5.1 Reference Detection</h3>
<p>We identify a sequence of referring expressions in the utterance using the reference detector of Udagawa and Aizawa (2020), a BiLSTM-CRF tagger (Huang et al., 2015). Then, following Udagawa and Aizawa (2020), we obtain features $z^{k}$ for each of the $K$ referring expressions in the utterance (for use in the reference resolution model) with a bidirectional recurrent encoder, using learned weights to pool the encodings at the referring expression's boundaries as well as the end of the utterance. ${ }^{6}$</p>
<h3>5.2 Structured Reference Resolution</h3>
<p>We use a structured reference resolution module to ground the referring expressions identified above: identifying dots in the agent's own view described by each expression. Grounding referents in this domain involves reasoning not only about attributes of individual dots but also spatially and comparatively within a single referring expression (e.g., a line of three dots) or across referring expressions (e.g., a large grey dot left of a smaller dot).</p>
<p>To predict a sequence of referents $\mathbf{r}=r^{1: K}$ from the $K$ referring expression representations $z^{1: K}$ extracted above, we use a linear-chain CRF (Lafferty et al., 2001) with neural potentials to parameterize $P_{R}\left(r^{1: K} \mid z^{1: K}, w, M\right)$. This architecture generalizes the reference resolution and choice selection models of Udagawa and Aizawa (2020) and Udagawa et al. (2020) to model, in the output structure, relationships between dots, both inside and across referring expressions.</p>
<p>There are three different types of potentials, designed to model language-conditioned features of individual dots $d$ in a referent $r, \phi$; relationships within a referent, $\psi$, and transitions between successive referents, $\omega$. Given these potentials, the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>distribution is parameterized as</p>
<p>$$
\begin{aligned}
&amp; P\left(r^{1: K} \mid z^{1: K}\right) \propto \
&amp; \exp \left(\sum_{k} f\left(r^{k}, z^{k}\right)+\psi\left(r^{k}, z^{k}\right)+\omega\left(r^{k: k+1}, z^{k: k+1}\right)\right)
\end{aligned}
$$</p>
<p>where $f(r, z)=\sum_{d \in r} \phi(d, z)$, and we've omitted the dependence of all terms on $M$ and $w$ for brevity. We share all module parameters across the two subtasks of resolving referents for the agent and for the partner. ${ }^{7}$</p>
<p>Individual Dots. Dot potentials $\phi$ model the correspondence between language features $z^{k}$ and individual dots represented by encodings $w(d)$, as well as discourse salience using the dot-level memory $M(d)$ that tracks when the dot $d$ has been mentioned: ${ }^{8}$</p>
<p>$$
\phi\left(d, z^{k}\right)=\operatorname{MLP}_{\phi}\left(\left[M(d), z^{k}, w(d)\right]\right)
$$</p>
<p>Dot Configurations. Configuration potentials $\psi\left(r^{k}, z^{k}\right)$ model the correspondence between language features and the set of all active dots in the agent's view for a referent $r^{k}$. These potentials further decompose into (1) pairwise potentials between active dots in the configuration, which relate the language embedding $z^{k}$ to attribute differences between dots in the pair (including as relative position, size, and shade) and (2) a potential on the entire configuration, which relates the language embedding to an embedding for the count of active dots in the configuration. See Appendix A. 1 for more detail.</p>
<p>Configuration Transitions. Transition potentials $\omega\left(r^{k}, r^{k+1}, z^{k}, z^{k+1}\right)$ model the correspondence between language features and relationships between referring expressions, e.g., to the left of in the black dot to the left of the triangle of gray dots. See Appendix A. 1 for details.</p>
<h3>5.3 Confirmations</h3>
<p>When applied to the partner's utterances, the reference resolution module gives a distribution over which referents the partner is likely to be referring to in the agent's own context. If the agent can identify the referents its partner is describing, it should be able to confirm them, both in the dots it</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>talks about next (e.g., choosing to refer to one of the same dots the partner identified) and in the text of the utterances (e.g., yes, I see it). The discretevalued confirmation variable (defined in Section 3) models this, taking the value NA if no referring expressions were identified in the partner's utterance, YES if all of the $K&gt;0$ referring expressions have a non-empty referent (at least one dot predicted in the agent's context) and No otherwise.</p>
<h3>5.4 Referent Memory</h3>
<p>The memory state is composed of one state vector $M_{t}(d)$ for each dot in the agent's own context. These dot states are updated using the referents identified in each utterance. This update is parameterized using a decoder cell, which is applied separately to each dot state:</p>
<p>$$
M_{t+1}(d)=\mathrm{RNN}<em t="t">{C}\left(M</em>\right)\right)
$$}(d), \iota\left(d, \mathbf{r}_{t</p>
<p>where $\iota$ is a function that extracts features from the predictive distribution over referents from the previous utterance, representing mentions of dot $d$ in the referring expressions. We implement the cell using a GRU (Cho et al., 2014). See Appendix A. 2 for more details.</p>
<h3>5.5 Mention Selection</h3>
<p>The mention selection subtask requires predicting a sequence of referents to mention in the agent's next utterance, $P_{M}\left(\mathbf{r}<em 1:="1:" t="t">{t+1} \mid u</em>}, M_{t+1}, c_{t+1}, w\right)$. To produce these referents, we use the same structured CRF architecture as the reference resolution module $P_{R}$. However, we use separate parameters from that module, and instead of the referring-expression inputs $\mathbf{z}$ use a sequence of vectors $x^{1: K_{t+1}}$ produced by a decoder cell $\mathrm{RNN<em t="t">{M}$, implemented using a GRU (Cho et al., 2014). The decoder conditions on the dialogue context representation $H</em> M(d)$ :}$ from the end of the last utterance, a learned vector embedding for the confirmation variable $c_{t+1}$, and a mean-pooled representation of the memory $m=\frac{1}{|d|} \sum_{d</p>
<p>$$
x^{k}=\mathrm{RNN}<em t="t">{M}\left(x^{k-1},\left[H</em>, m\right]\right)
$$}, c_{t+1</p>
<p>We obtain the number of referents $K_{t+1}$ by predicting at each step $k$ whether to halt from each $x^{k}$ using a linear layer followed by a logistic function.</p>
<h3>5.6 Choice Selection</h3>
<p>To parameterize the choice selection module $P_{S}(s \mid$ $\left.u_{1: T}, M_{T}, w\right)$, we again reuse the CRF architecture,
with independent parameters from reference resolution and mention selection modules, replacing reference resolution's inputs $z^{1: K}$ with the dialogue context representation $H_{T}$ from the end of the final utterance in the dialogue. Since only a single dot needs to be identified, we use only the CRF's individual dot potentials $\phi$, removing $\psi$ and $\omega$. This is equivalent to the choice selection model (TSEL) used by Udagawa and Aizawa (2020) if the recurrent memory $M_{T}$ is removed.</p>
<h3>5.7 Utterance Generation</h3>
<p>The utterance generation module $P_{U}\left(u_{t+1} \mid \mathbf{r}<em t_1="t+1">{t+1}, c</em>$. See Appendix A. 3 for details.}, H_{t}, w\right)$ is a sequence-tosequence model. The module first encodes the sequence of dot encodings $w(d)$ for dots in the referents $z_{t+1}^{1: K_{t+1}}$ (predicted by the mention selection module) to produce encodings $y_{t}^{1: K}$. Words in the utterance are then produced one at a time using a recurrent decoder that has a hidden state initialized with a function that combines $y_{t}^{1: K}$, the dialog context $H_{t}$, and a learned embedding for the discrete confirmation variable $c_{t+1}$. The decoder has two attention mechanisms over: (i) dot encodings $w(d)$, following Udagawa and Aizawa (2020), and (ii) the sequence of encoded referents $y_{t}^{1: K_{t+1}</p>
<h2>6 Experiments</h2>
<p>We compare our approach to past systems for the ONECOMMON dataset. While our primary evaluation is to evaluate systems on their success rate on the full dialogue game when paired with human partners (Section 6.4), we also compare our system to past work, and ablated versions of our full system, using the automatic evaluations of past work.</p>
<h3>6.1 Models</h3>
<p>We compare our full system (FULL) to ablated versions of it that successively remove: (i) the referent memory, ablating explicit tracking of referents mentioned (F-MEM) and (ii) the structured potentials $\psi, \gamma$ in the reference resolution and mention selection modules (F-MEM-STRUC), removing explicit modeling of relationships within and across referents. We also compare to a reimplementation of the system of Udagawa and Aizawa (2020), which we found obtained better performance than their reported results in all evaluation conditions due to implementation improvements. See Appendix A.5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Choice</th>
<th style="text-align: center;">Ref. Resolution</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Ex.</td>
</tr>
<tr>
<td style="text-align: left;">U\&amp;A (2020)</td>
<td style="text-align: center;">$69.3 \pm 2.0$</td>
<td style="text-align: center;">$86.4 \pm 0.4$</td>
<td style="text-align: center;">$35.0 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: left;">U+ (2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$86.0 \pm 0.3$</td>
<td style="text-align: center;">$54.9 \pm 0.8$</td>
</tr>
<tr>
<td style="text-align: left;">F-MEM-STRUC</td>
<td style="text-align: center;">$71.6 \pm 0.9$</td>
<td style="text-align: center;">$87.7 \pm 0.2$</td>
<td style="text-align: center;">$44.3 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">F-MEM</td>
<td style="text-align: center;">$70.9 \pm 1.2$</td>
<td style="text-align: center;">$92.6 \pm 0.2$</td>
<td style="text-align: center;">$76.2 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">FULL</td>
<td style="text-align: center;">$83.3 \pm 1.2$</td>
<td style="text-align: center;">$93.3 \pm 0.2$</td>
<td style="text-align: center;">$78.2 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">86.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracies for predicting the dot selected at the end of the game (Choice Acc.) and resolving referents from utterances produced in the agent's own perspective (dot-level accuracy Acc. and exact match Ex.) in 10 -fold cross-validation on the corpus. Our Full model outperforms all past work on the dataset: U\&amp;A (2020) is our reimplementation of Udagawa and Aizawa (2020), and U+ (2020) are taken from Udagawa et al. (2020). Human scores are annotator agreements (Udagawa and Aizawa, 2019).</p>
<p>We obtain supervision for all components of the systems by training on the referent-annotated corpus of 5,191 successful human-human dialogues collected by Udagawa and Aizawa (2019; 2020). See Appendix A. 6 for training details. We train one copy of each model on each of the corpus's 10 cross-validation splits. We report means and standard deviations across the splits' models, except in human evaluations where we use a single model.</p>
<h3>6.2 Corpus Evaluation</h3>
<p>Following Udagawa and Aizawa (2020), we evaluate models' accuracy at (1) predicting the dot chosen at the end of the game (Choice Acc.) using $P_{S}$ and (2) resolving the referents in utterances from the human partner in the dialogue who had the agent's view (Ref Resolution dot-level accuracy Acc. and exact match accuracy Ex.) using $P_{R}$.</p>
<p>We see in Table 1 that our Full model improves substantially on past work, including the work of Udagawa et al. (2020), who augment their referent resolution model with numeric features. Our structured reference resolver is able to learn these features in its potentials $\psi$ (in addition to other structured relationships), and improves exact match from $44 \%$ to $76 \%$ compared to the ablated version of our system. Our recurrent memory helps in particular for the choice selection task, improving from $71 \%$ to $83 \%$ accuracy.</p>
<p>We also compare the performance of our full and ablated systems on the tasks of resolving the partner's referring expressions and mention prediction, with results given in Appendix B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">#Shared=4</th>
<th style="text-align: center;">#Shared=5</th>
<th style="text-align: center;">#Shared=6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">U\&amp;A (2020)</td>
<td style="text-align: center;">$50.7 \pm 2.0$</td>
<td style="text-align: center;">$66.0 \pm 1.9$</td>
<td style="text-align: center;">$83.5 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: left;">F-MEM-STRUC</td>
<td style="text-align: center;">$42.3 \pm 2.1$</td>
<td style="text-align: center;">$57.0 \pm 2.1$</td>
<td style="text-align: center;">$75.4 \pm 1.1$</td>
</tr>
<tr>
<td style="text-align: left;">F-MEM</td>
<td style="text-align: center;">$52.6 \pm 1.5$</td>
<td style="text-align: center;">$67.1 \pm 1.9$</td>
<td style="text-align: center;">$84.1 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: left;">FULL</td>
<td style="text-align: center;">$58.5 \pm 2.7$</td>
<td style="text-align: center;">$71.6 \pm 2.9$</td>
<td style="text-align: center;">$86.8 \pm 1.8$</td>
</tr>
<tr>
<td style="text-align: left;">FULL+PRAG</td>
<td style="text-align: center;">$62.4 \pm 2.2$</td>
<td style="text-align: center;">$74.7 \pm 2.7$</td>
<td style="text-align: center;">$90.9 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">87.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Task success rates in automatic self-play evaluations, by difficulty of context (the number of items shared in the players' views). Our Full model outperforms past work: U\&amp;A (2020) is our tuned reimplementation of Udagawa and Aizawa (2020). Human shows success rates of trained human annotators in collecting the dataset (Udagawa and Aizawa, 2019).</p>
<h3>6.3 Evaluation in Self-Play</h3>
<p>To evaluate systems on the full dialogue task, we first use self-play, where a system is partnered with a copy of itself, following Udagawa and Aizawa (2020). We evaluate systems on 3,000 world contexts, stratified into contexts with 4,5 , and 6 dots overlapping between the two agents' views, with 1,000 contexts in each stratification.</p>
<p>Table 2 reports average task success (the fraction of times both agents chose the same dot at the end of the dialogue) averaged across the 10 copies of each model trained on the cross-validation splits. As in the corpus evaluation, we see substantial improvements to our system from the structured referent prediction and the recurrent reference memory. Our Full system, without pragmatic generation, improves over the system of Udagawa and Aizawa (2020) from $51 \%$ to $58 \%$ in the hardest setting, with a further improvement to $62 \%$ when adding our pragmatic generation procedure.</p>
<h3>6.4 Human Evaluation</h3>
<p>Finally, we perform human evaluation by comparing system performance when playing with workers from Amazon's Mechanical Turk (MTurk). To conduct evaluation, we used 100 world states from the #Shared=4 partition, and collected 718 complete dialogues by randomly pairing worker with one of the following three: our best-performing model in self-play (FULL+PRAG), the model from Udagawa and Aizawa (2020), or another worker.</p>
<p>In order to ensure higher quality dialogues, and following Udagawa and Aizawa (2019), we filtered workers by qualifications, showed workers a game tutorial before playing, and prevented dots from being selected within the first minute of the game. We</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Success rates of systems on the full dialogue game task when paired with human partners. Error bars show standard errors. Our Full+PrAG system achieves a $50 \%$ relative performance improvement over past work (U\&amp;A 2020: Udagawa and Aizawa 2020).
paid workers $\$ 0.30$ per game, with a bonus of $\$ 0.15$ if the dialogue was successful. See Appendix E for sample dialogues.</p>
<p>We compare systems based on the percentage of successful dialogues. The results, in Figure 4, corroborate the trends observed in self-play. Both the models of U\&amp;A (2020) and our Full+PrAG perform worse against humans than against agent partners in the automatic self-play evaluation, illustrating the importance of performing human evaluations. However, the trend is preserved, and we see that the Full+PrAG system substantially outperforms the U\&amp;A (2020) model, resulting in a 50\% relative improvement in task success rate. This difference is statistically significant at the $p \leq 0.05$ level using a one-tailed t-test.</p>
<h3>6.5 Success by Human Skill Level</h3>
<p>In Section 6.4, we compared our systems to a human population of MTurk workers. However, human populations themselves vary greatly based on many factors, including the day and time workers are recruited, training and feedback given to workers, and worker retention. One difference between our worker population and the population that produced the dataset is training. When collecting the dataset, Udagawa and Aizawa (2019) performed manual and individualized coaching of their MTurk workers which made them more effective at the game: giving players personalized feedback on how to improve their game strategies, e.g., "please ask more clarification questions."9 Manual coaching produced a high-quality corpus by increasing players' skill and obtained a success rate of $66 \%$;</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Success rates of human players against each system type, and other humans, with progressive filtering of humans by their overall success rate (across all conditions) along the x -axis. Shaded regions give standard errors. Our Full+PrAG system outperforms past work (U\&amp;A 2020) at all levels. ${ }^{10}$
however coaching would make human evaluations difficult to replicate across works due to the labor, cost, and variability that coaching involves.</p>
<p>In this section, we run a sweep of system comparisons of the form of Section 6.4, but on increasingly select sub-populations of MTurk workers. Results are shown in Fig. 5. The x-axis gives the minimum skill percentile for a worker's games to be retained (with a worker's skill defined to be their average success across all games; see Appendix D for an alternative), so that the far left of the graph shows all workers (corresponding to the numbers in Fig. 4), the far right shows only those workers who won all of their games, and the black vertical line marks the player filtering needed to obtain a human-human success rate comparable to Udagawa and Aizawa (2019). Our Full+PrAG system outperforms the model of Udagawa and Aizawa (2020) at all player skill levels. ${ }^{10}$ This result shows that, while more accomplished workers' overall success rates can be much higher than the success rate of our general worker population, in all cases the ordering between the two systems remained the same.</p>
<h2>7 Related Work</h2>
<p>Goal-oriented dialog. The modular approach that we use reflects the pipelined approach often used in goal-oriented dialogue systems (Young et al., 2013). Recent work on neural systems has also used structured and memory-based approaches (Bordes et al., 2017; He et al., 2018) including</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>tracking entities identified in text (Williams et al., 2017; He et al., 2017). We also find improvements from an entity-centric approach with a structured memory, although our domain involves more challenging entity resolution and generation due to the spatial grounding.</p>
<p>Referring expressions. A long line of past work on referring expression grounding has tackled generation (Dale, 1989; Dale and Reiter, 1995; Viethen et al., 2011; Krahmer and van Deemter, 2012), interpretation (Schlangen et al., 2009; Liu et al., 2013; Kennington and Schlangen, 2015) or both (Heeman, 1991; Mao et al., 2016; Yu et al., 2017). Closest to ours is the work of Takmaz et al. (2020), which builds models for reference interpretation and generation in the rich PhotoBook corpus (Haber et al., 2019), focusing on a non-interactive setting with static evaluation on reference chains extracted from human-human dialogues.</p>
<p>Collaborative games. The closest work on dialogue systems for collaborative grounded tasks has focused on tasks with different properties from ours, as discussed in Section 1. A closely related task to the shared visual reference game we pursue here is the PhotoBook task (Haber et al., 2019), although a dialogue system has not been constructed for it. Other work on grounded collaborative language games includes collection games (Potts, 2012; Suhr et al., 2019), navigation and interactive question games (Thomason et al., 2019; Nguyen and Daumé III, 2019; Ilinykh et al., 2019), and construction tasks (Wang et al., 2017; Kim et al., 2019; Narayan-Chen et al., 2019).</p>
<p>Pragmatics. Our approach to pragmatics (Grice, 1975) builds on a large body of work in the RSA framework (Frank and Goodman, 2012; Goodman and Frank, 2016), which models how speakers and listeners reason about each other to communicate successfully. The most similar applications to ours in past work on computational pragmatics have been to single-turn grounded reference tasks (rather than dialogue), with much smaller and unstructured spaces of referents than ours, ${ }^{11}$ such as discriminative image captioning (Vedantam et al., 2017; Andreas and Klein, 2016; CohnGordon et al., 2018) and referent identification (Monroe et al., 2017; McDowell and Goodman, 2019; White et al., 2020). Explicit speaker-listener</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>models of pragmatics have also been used for dialogue, and while these approaches plan or infer across multiple turns (which our work does not do explicitly), they have either involved ungrounded settings (Kim et al., 2020) or constrained language (Vogel et al., 2013; Khani et al., 2018).</p>
<h2>8 Conclusion</h2>
<p>We presented a modular, reference-centric approach to a challenging partially-observable grounded collaborative dialogue task. Our approach is centered around a structured referent grounding module, which we use (1) to interpret a partner's utterances and (2) to enable a pragmatic generation procedure that encourages the agent's utterances to be able to be understood in context. We perform, for the first time, human evaluations on the full dialogue task, finding that our system cooperates with people substantially more successfully than a system from past work and-in aggregateachieves a success rate comparable to pairs of human partners.</p>
<p>While our results are encouraging, there is still much room for improving all systems in their interactions with people on this challenging task. As the examples in Appendix E illustrate, people use sophisticated conversational strategies to build common ground (Clark and Wilkes-Gibbs, 1986; Traum, 1994; Clark, 1996) when they interact with each other, producing utterances that play multiple conversational roles and performing complex reasoning. To better plan utterances (Cohen and Perrault, 1979) and more accurately infer the partner's state (Allen and Perrault, 1980), we suspect it will be helpful to extend the single-step pragmatic utterance planning and implicit inference procedures that we use here: planning over longer time horizons, performing more explicit reasoning under uncertainty, and learning richer models of the full range of speech acts that people use. Future work might continue to explore these directions on this task and other similarly challenging tests of collaborative grounding.</p>
<h2>Acknowledgments</h2>
<p>We are grateful to Takuma Udagawa for sharing information about the corpus collection, and to Yoav Artzi, Chris Potts, Aida Nematzadeh, the Berkeley NLP group, and the anonymous reviewers for helpful suggestions and feedback. This work was supported by DARPA through the XAI program and by a Google PhD Fellowship to DF.</p>
<h2>References</h2>
<p>James F. Allen and C. Raymond Perrault. 1980. Analyzing intention in utterances. Artificial Intelligence, $15(3)$.</p>
<p>Jacob Andreas and Dan Klein. 2016. Reasoning about pragmatics with neural listeners and speakers. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1173-1182, Austin, Texas. Association for Computational Linguistics.</p>
<p>John Langshaw Austin. 1962. How to Do Things With Words.</p>
<p>Antoine Bordes, Y-Lan Boureau, and Jason Weston. 2017. Learning end-to-end goal-oriented dialog. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</p>
<p>Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Herbert H. Clark. 1996. Using Language.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22(1).</p>
<p>Philip R. Cohen and C. Raymond Perrault. 1979. Elements of a plan-based theory of speech acts. Cognitive Science, 3(3).</p>
<p>Reuben Cohn-Gordon, Noah Goodman, and Christopher Potts. 2018. Pragmatically informative image captioning with character-level inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers).</p>
<p>Robert Dale. 1989. Cooking up referring expressions. In 27th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19(2).</p>
<p>Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018.</p>
<p>Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.</p>
<p>Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, and Douwe Kiela. 2018. Talk the walk: Navigating New York City through grounded dialogue. ArXiv preprint, abs/1807.03367.</p>
<p>Michael C. Frank and Noah D. Goodman. 2012. Predicting pragmatic reasoning in language games. Science.</p>
<p>Noah D. Goodman and Michael C. Frank. 2016. Pragmatic language interpretation as probabilistic inference. Trends in Cognitive Sciences, 20(11).
H. Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry L. Morgan, editors, Speech Acts, volume 3 of Syntax and Semantics.</p>
<p>Janosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni, and Raquel Fernández. 2019. The PhotoBook dataset: Building common ground through visually-grounded dialogue. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017. Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>He He, Derek Chen, Anusha Balakrishnan, and Percy Liang. 2018. Decoupling strategy and generation in negotiation dialogues. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Peter A. Heeman. 1991. Collaborating on referring expressions. In 29th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.</p>
<p>Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. ArXiv preprint, abs/1508.01991.</p>
<p>Nikolai Ilinykh, Sina Zarrieß, and David Schlangen. 2019. Meet up! a corpus of joint activity dialogues in a visual environment. In Proceedings of the 23rd Workshop on the Semantics and Pragmatics of Dialogue - Full Papers.</p>
<p>Pamela W. Jordan and Marilyn A. Walker. 2005. Learning content selection rules for generating object descriptions in dialogue. Journal of Artificial Intelligence Research, 24(1).</p>
<p>Casey Kennington and David Schlangen. 2015. Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference</p>
<p>on Natural Language Processing (Volume 1: Long Papers).</p>
<p>Fereshte Khani, Noah D. Goodman, and Percy Liang. 2018. Planning, inference and pragmatics in sequential language games. Transactions of the Association for Computational Linguistics, 6.</p>
<p>Hyunwoo Kim, Byeongchang Kim, and Gunhee Kim. 2020. Will I sound like me? improving persona consistency in dialogues through pragmatic selfconsciousness. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak Zhang, Yuandong Tian, Dhruv Batra, and Devi Parikh. 2019. CoDraw: Collaborative drawing as a testbed for grounded goaldriven communication. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1).</p>
<p>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001.</p>
<p>Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai. 2013. Modeling collaborative referring for situated referential grounding. In Proceedings of the SIGDIAL 2013 Conference.</p>
<p>Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. 2016. Generation and comprehension of unambiguous object descriptions. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016.</p>
<p>Bill McDowell and Noah Goodman. 2019. Learning from omission. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Will Monroe, Robert X.D. Hawkins, Noah D. Goodman, and Christopher Potts. 2017. Colors in context: A pragmatic neural model for grounded language understanding. Transactions of the Association for Computational Linguistics, 5.</p>
<p>Anjali Narayan-Chen, Prashant Jayannavar, and Julia Hockenmaier. 2019. Collaborative dialogue in Minecraft. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Khanh Nguyen and Hal Daumé III. 2019. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP).</p>
<p>Christopher Potts. 2012. Goal-driven answers in the Cards dialogue corpus. In Proceedings of the 30th West Coast Conference on Formal Linguistics.</p>
<p>Adam Santoro, David Raposo, David G. T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter W. Battaglia, and Tim Lillicrap. 2017. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA.</p>
<p>David Schlangen, Timo Baumann, and Michaela Atterer. 2009. Incremental reference resolution: The task, metrics for evaluation, and a Bayesian filtering model that is sensitive to disfluencies. In Proceedings of the SIGDIAL 2009 Conference.</p>
<p>John R Searle. 1976. A classification of illocutionary acts. Language in Society, 5(1).</p>
<p>Alane Suhr, Claudia Yan, Jack Schluger, Stanley Yu, Hadi Khader, Marwa Mouallem, Iris Zhang, and Yoav Artzi. 2019. Executing instructions in situated collaborative interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).</p>
<p>Ece Takmaz, Mario Giulianelli, Sandro Pezzelle, Arabella Sinclair, and Raquel Fernández. 2020. Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Thora Tenbrink and Reinhard Moratz. 2003. Groupbased spatial reference in linguistic human-robot interaction. Spatial Cognition and Computation, 6.</p>
<p>Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. 2019. Vision-and-dialog navigation. In Conference on Robot Learning (CoRL).</p>
<p>David R. Traum. 1994. A computational theory of grounding in natural language conversation. Technical report, Rochester University Department of Computer Science.</p>
<p>Takuma Udagawa and Akiko Aizawa. 2019. A natural language corpus of common grounding under continuous and partially-observable context. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33.</p>
<p>Takuma Udagawa and Akiko Aizawa. 2020. An annotated corpus of reference resolution for interpreting common grounding. In Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Takuma Udagawa, Takato Yamazaki, and Akiko Aizawa. 2020. A linguistic analysis of visually grounded dialogues based on spatial expressions. In Findings of the Association for Computational Linguistics: EMNLP 2020.</p>
<p>Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. 2017. Context-aware captions from context-agnostic supervision. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.</p>
<p>Jette Viethen, Robert Dale, and Markus Guhe. 2011. Generating subsequent reference in shared visual scenes: Computation vs re-use. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Adam Vogel, Max Bodoia, Christopher Potts, and Daniel Jurafsky. 2013. Emergence of Gricean maxims from multi-agent decision theory. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Sida I. Wang, Samuel Ginn, Percy Liang, and Christopher D. Manning. 2017. Naturalizing a programming language via interactive learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Julia White, Jesse Mu, and Noah D Goodman. 2020. Learning to refer informatively by amortizing pragmatic reasoning. In Proceedings of the 42nd Annual Meeting of the Cognitive Science Society.</p>
<p>Jason D. Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Steve Young, Milica Gašić, Blaise Thomson, and Jason D Williams. 2013. POMDP-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5).</p>
<p>Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L. Berg. 2017. A joint speaker-listener-reinforcer model for referring expressions. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.</p>
<h2>A Model Details</h2>
<h2>A. 1 Structured CRF</h2>
<p>Dot Configurations. Dot configuration potentials $\psi(r, z)$ are composed of two terms: $R(r, z)$ which decomposes into functions of pairwise relationships between the dots (whether active or not) in the context $w$ and the text features $z$, and $A(r, z)$ which is a function of all active dots in the referent:</p>
<p>$$
\psi(r, z)=R(r, z)+A(r, z)
$$</p>
<p>The pairwise relationships are</p>
<p>$$
R(r, z)=\sum_{i=1}^{N-1} \sum_{j=i+1}^{N} \alpha(r, z, i, j)
$$</p>
<p>where $N$ is the number of dots in view (7) and $\alpha$ is a scalar-valued neural function of the text features and whether the dots indexed by $i$ and $j$ are active in the referent $r$ :</p>
<p>$$
\alpha(r, z, i, j)=\left{\begin{array}{lr}
p(z, i, j)<em 1="1">{0}, &amp; r(i) \wedge r(j) \
p(z, i, j)</em>, &amp; \neg r(i) \wedge \neg r(j) \
p(z, i, j)_{2}, &amp; \text { otherwise }
\end{array}\right.
$$</p>
<p>$p$ is a 3-dimensional vector produced by an MLP:</p>
<p>$$
p(z, i, j)=\operatorname{MLP}_{\psi}([w(i)-w(j), z])
$$</p>
<p>The active dot potential $A$ is designed to model group properties such as cardinality and common attributes, which other work has found useful on this and similar tasks (Tenbrink and Moratz, 2003; Udagawa et al., 2020). We define the potential as</p>
<p>$$
A(r, z)=\operatorname{MLP}_{A}([w(r), e(r)])
$$</p>
<p>where $w(r)$ is the mean of the feature values for the active dots in $r, \frac{1}{\mid r_{\text {active }} \mid} \sum_{d \in r_{\text {active }}} w(d)$ and $e(r)$ is a learned 40 -dimensional embedding for the discrete count of active dots in $r$.</p>
<p>Configuration Transitions. The configuration transition potential $\omega\left(r^{k: k+1}, z^{k: k+1}\right)$ is similar to the dot configuration potential above but bridges the dots in referents $k$ and $k+1$. It is the sum of two terms: $\omega\left(r^{k: k+1}, z^{k: k+1}\right)=S\left(r^{k: k+1}, z^{k: k+1}\right)+$ $B\left(r^{k: k+1}, z^{k: k+1}\right)$. First is $S$, which decomposes into pairwise relationships between dots across referents $r^{k}$ and $r^{k+1}$ :
$S\left(r^{k: k+1}, z^{k: k+1}\right)=\sum_{i=1}^{N} \sum_{j=1}^{N} \beta\left(r^{k: k+1}, z^{k: k+1}, i, j\right)$
$\beta\left(r^{k: k+1}, z^{k: k+1}, i, j\right)=\left{\begin{array}{lll}q_{0}, &amp; r^{k}(i) \wedge r^{k+1}(j) \ q_{1}, &amp; \neg r^{k}(i) \wedge \neg r^{k+1}(j) \ q_{2}, &amp; &amp; \text { otherwise }\end{array}\right.$
$q$ (short for $q\left(z^{k: k+1}, i, j\right)$ ) is, like $p$ in Dot Configurations, a 3-dimensional vector produced by an MLP:</p>
<p>$$
q=\operatorname{MLP}_{\omega}\left(\left[w(i)-w(j), z^{k}-z^{k+1}\right]\right)
$$</p>
<p>Next is $B$, which is a function of the feature centroids of the active dots in referents $k$ and $k+1 . B$ is defined analogously to $A$ in Dot Configurations:</p>
<p>$$
\begin{aligned}
&amp; B\left(r^{k: k+1}, z^{k: k+1}\right) \
&amp; \quad=\operatorname{MLP}_{B}\left(\left[w\left(r^{k}\right)-w\left(r^{k+1}\right), z^{k}-z^{k+1}\right]\right)
\end{aligned}
$$</p>
<p>with $w(r)$ again giving the mean of the feature values for the active dots in $r$. We fix $B\left(r^{k: k+1}, z^{k: k+1}\right)=0$ if $\left|r_{\text {active }}^{k}\right|&gt;3$ or $\left|r_{\text {active }}^{k+1}\right|&gt;3$, which had little effect on model accuracy but improves memory efficiency as it substantially reduces the number of group-pairwise relationships that need to be computed.</p>
<p>Inference. We compute the normalizing constant for the CRF distribution by enumerating the possible $2^{7}$ assignments to each $r^{k}$ to compute the $\phi$, $\psi$, and $\omega$ potential terms, which can be performed efficiently on a GPU. To compute the normalizing constant, which sums over all combinations of assignments to these $r^{k}$, we use the standard linearchain dynamic program. In training, we backpropagate through the enumeration and dynamic program steps to pass gradients to the parameters of the potential functions.</p>
<h2>A. 2 Referent Memory</h2>
<p>The function $\iota$ collapses predicted values for the dot $d$ over $K$ referents into a single representation for the dot, which we do in two ways: by maxand average- pooling predicted values for $d$ across the $K$ referents. We also obtain the prediction values in two ways: by taking the argmax structured prediction from $P_{R}$, and by taking the argmax predictions from each dot's marginal distribution. We found that using these "hard" argmax predicted values gave slightly better results in early experiments than using the "soft" probabilities from $P_{R}$. In combination, these give four feature values as the output of $\iota\left(d, \mathbf{r}_{t}\right)$.</p>
<h2>A. 3 Utterance Generation Module</h2>
<p>We first use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to encode the sequence of $K$ referents-to-mention $\mathbf{r}<em t_1="t+1">{t+1}=r</em>$.}^{1: K}$, using the inputs at each position $k \in[1, K]$ a mean-pooled representation of the world context embeddings for the active dots in the referent: $\frac{1}{\left|r_{t+1}^{k}\right|} \sum_{d \in r_{t+1}^{k}} w(d)$, to produce a sequence of encoded vectors $y_{l}^{1: K}$. We make gated updates to the decoder's initial state, updating it with (i) a linear projection of the forward and backward vectors for $y_{l}^{1}$ and $y_{l}^{K}$, representing the referent context and (ii) an embedding for the discrete confirmation variable $c_{t+1</p>
<h2>A. 4 Implementation Choices</h2>
<p>For our reimplementation of the system of Udagawa and Aizawa (2020) in a shared codebase with our system, we replace their tanh non-linearities with ReLUs and use PyTorch's default initializations for all parameters. These improve performance across all evaluation conditions in comparison to the reported results.</p>
<p>For our system, we use separate word-level recurrent models, a Reader and a Writer, to summarize the dialogue history. The Reader is bidirectional over each utterance, and is used in the reference resolution and choice selection modules. The Writer is unidirectional, and is used in the mention selection and utterance generation modules.</p>
<h2>A. 5 Hyperparameters</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Recurrent Unit Hyperparameters</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reader GRU size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Writer GRU size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Mention decoder $\mathrm{RNN}_{M}$ size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Referent memory $\mathrm{RNN}_{C}$ size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">Confirmation embedding $c$ size</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">CRF Hyperparameters</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}_{\phi}$ hidden layers</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}_{\psi}$ hidden size</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}_{\phi}$ dropout</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}<em _omega="\omega">{\psi}$ and $\mathrm{MLP}</em>$ hidden size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}<em _omega="\omega">{\psi}$ and $\mathrm{MLP}</em>$ dropout</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}<em _omega="\omega">{\psi}$ and $\mathrm{MLP}</em>$ hidden layers</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}<em B="B">{A}$ and $\mathrm{MLP}</em>$ hidden size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}<em B="B">{A}$ and $\mathrm{MLP}</em>$ dropout</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{MLP}<em B="B">{A}$ and $\mathrm{MLP}</em>$ hidden layers</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Generation Hyperparameters</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sampling temperature in $P_{U}$</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;"># Utterance candidates, $N_{u}$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;"># Referent candidates, $N_{r}$</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Mention weight, $w_{M}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Speaker weight, $w_{S}$</td>
<td style="text-align: center;">$1 \times 10^{-3}$</td>
</tr>
<tr>
<td style="text-align: left;">Listener weight, $w_{L}$</td>
<td style="text-align: center;">$1-w_{S}$</td>
</tr>
<tr>
<td style="text-align: left;">Early-stopping threshold, $\tau$</td>
<td style="text-align: center;">0.8</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Partner Refs.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Next Refs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Ex.</td>
<td style="text-align: center;">Ex.</td>
</tr>
<tr>
<td style="text-align: center;">F-MEM-STRUC</td>
<td style="text-align: center;">$87.3 \pm 0.3$</td>
<td style="text-align: center;">$41.8 \pm 0.9$</td>
<td style="text-align: center;">$4.8 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">F-MEM</td>
<td style="text-align: center;">$90.6 \pm 0.3$</td>
<td style="text-align: center;">$65.2 \pm 1.0$</td>
<td style="text-align: center;">$23.5 \pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">$91.2 \pm 0.4$</td>
<td style="text-align: center;">$67.0 \pm 1.0$</td>
<td style="text-align: center;">$31.1 \pm 1.0$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracies for resolving referents in the partner's view (dot-level accuracy Acc. and exact match Ex.) and predicting the next referents to mention in the dialogue (Next Refs Ex.) in 10-fold cross-validation on the corpus of human-human dialogues. Our FULL benefits from its recurrent referent memory (outperforming F-MEM) and structured referent prediction module (outperforming F-MEM-STRUC).</p>
<h2>A. 6 Training Details</h2>
<p>For our full system and ablations, we train on each cross-validation fold for 12 epochs using the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of $1 \times 10^{-3}$ and early stopping on the fold's validation set. Our loss function is a weighted combination of losses for the subtask objectives:</p>
<p>$$
\begin{aligned}
\mathcal{L} &amp; =w_{S} \log P_{S}(s \mid \ldots)+ \
\frac{1}{T} &amp; \sum_{t=1}^{T}\left(\log P_{R}\left(r_{t} \mid \ldots\right)+\log P_{M}\left(r_{t} \mid \ldots\right)+\log P_{U}\left(u_{t} \mid \ldots\right)\right)
\end{aligned}
$$</p>
<p>where $w_{S}$ is a hyperparameter which we set to $\frac{1}{32}$ following Udagawa and Aizawa (2020) and we have omitted conditioning contexts from the probability distributions for brevity; see Section 3.2 for the full contexts. We decay the learning rate when the loss plateaus on validation data.</p>
<p>We train models on a Quadro RTX 6000 GPU. Training takes around 1 day for models that use the structured CRF, and several hours without the structured CRF. Self-play evaluation takes around 1 hour.</p>
<h2>B Evaluation for Other Subtasks</h2>
<p>Table 3 gives performance accuracies for resolving referents in the partner's view (dot-level accuracy Acc. and exact match Ex.) and predicting the next referents to mention in the dialogue (Next Refs Ex.) in 10 -fold cross-validation on the corpus of humanhuman dialogues. We observe improvements from both the recurrent memory (comparing F-Mem to Full) and the structured referent prediction module (comparing F-Mem-Struc to Full) on both tasks.</p>
<h2>C Pragmatic Generation</h2>
<p>We give pseudocode for the pragmatic generation procedure (Section 4) in Algorithm 1. Figure 3 shows an example, showing 2 referents $\mathbf{r}$ (inputs to the realize) function on the left, and 3 utterances $u$ sampled for each referent on the right. Fewer than $N_{r}$ referent candidates may be evaluated (as in Figure 3) if one $(\mathbf{r}, u)$ pair is found with $L(\mathbf{r}, u) \geq$ $\tau$.</p>
<div class="codehilite"><pre><span></span><code>hyperparameters: \(N_{r}, N_{u}, \tau\)
function generate \(\left(M, u_{1: t-1}, c_{t}, w\right)\) :
    \((\hat{\mathbf{r}}, \hat{u}, \hat{s}) \leftarrow(\text { None, None, }-\infty)\)
    for \(\mathbf{r} \in \operatorname{topk}_{N_{r}} P_{M}\left(\mathbf{r} \mid u_{1: t-1}, M, c, w\right)\) :
        \(u, s \leftarrow \operatorname{realize}(\mathbf{r})\)
        if \(s&gt;\hat{s}\) :
            \((\hat{\mathbf{r}}, \hat{u}, \hat{s}) \leftarrow(\mathbf{r}, u, s)\)
            if \(s \geq \tau\) :
                break
    return \(\hat{\mathbf{r}}, \hat{u}\)
function realize \((\mathbf{r})\)
    for \(k \in 1 \ldots N_{u}\) :
        \(u^{(k)} \sim P_{U}(\cdot \mid \mathbf{r})\)
        \(\hat{u} \leftarrow \arg \max <span class="ge">_{u^{(k)}} L\left(\mathbf{r}, u^{(k)}\right)\) (Eqn. 1 in Sec. 4)</span>
<span class="ge">    \(\hat{s} \leftarrow \max _</span>{u^{(k)}} L\left(\mathbf{r}, u^{(k)}\right)\)
    return \((\hat{u}, \hat{s})
</code></pre></div>

<p>Algorithm 1: Our pragmatic generation procedure chooses a sequence of referents $\mathbf{r}$ to describe, and an utterance $u$ to describe them, to optimize the objective $L(\mathbf{r}, u)$ (Equation 1 in Section 4) using candidates from the models $P_{M}$ and $P_{U}$ and an early stopping search with threshold $\tau$.</p>
<p>We used self-play evaluation on one of the crossvalidation splits to tune the early-stopping threshold $\tau$, selecting from among the values ${0.0,0.6$, $0.7,0.8,0.9}$. The optimal value was $\tau=0.8$, but the success rate in self-play was fairly robust to the value chosen (including $\tau=0.0$, which results in performing pragmatic search only over those utterances for the single highest-scoring referent sequence under $P_{M}$ ), with a range of about $2 \%$. We did not evaluate without early-stopping (searching over all candidate reference sequences and utterances) as this would have made generation too computationally expensive to be feasible in both self-play and human evaluations.</p>
<h2>D Alternative Skill Analysis</h2>
<p>In Section 6.5, we compared systems on increasingly select sub-populations of MTurk workers, selected by their average success across all conditions (whether playing with other humans or one
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Success rates of human players against each system type, and other humans, with progressive filtering of humans by their overall success rate (when partnered with other humans) along the x-axis. Shaded regions give standard errors. Our Full+PrAG system outperforms the model from Udagawa and Aizawa (2020) at all levels. ${ }^{10}$
of the two system types). In this section, we run a similar analysis but select workers by their average success when paired with human partners only. Results are shown in Figure 6. The x-axis gives the minimum skill percentile for a worker's games to be retained, with skill defined by a worker's average success when paired with other human workers. The far left of the graph shows all workers, ${ }^{12}$ the far right shows only those workers who won all of their games when paired with other workers, and the black vertical line marks the player filtering needed to obtain a human-human success rate comparable to Udagawa and Aizawa (2019). As we saw in Section 6.5, our Full+PrAG system outperforms the model of Udagawa and Aizawa (2020) at all worker skill levels. However, focusing on the sub-population of workers who are successful when paired with other humans (the right side of Figure 6) reveals a gap between humans and our system: humans who are successful when partnering with other humans are substantially less successful when partnering with our FULL+PRAG system (and even less successful when partnering with the model of U\&amp;A'20). This indicates room for improvement on the task, as we want to build a system that can collaborate as well as humans with any population of human partners.</p>
<h2>E Dialogue Examples</h2>
<p>We show one successful and one failed dialogue from our human evaluations (Section 6.4) for each system (Figure 7) and from human-human pairs</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(Figure 8).
As seen in these examples, descriptions from the baseline system (Figures 7a and 7b) typically have a consistent syntactic structure (e.g., "i have a <size> <color> dot with a <size> <color> dot <spatial relation>") but often do not correspond to the visual context. We suspect that it is difficult for this end-to-end generation model to simultaneously learn which dots to talk about (content selection) and how to describe them (surface realization) with the amount of training data available. Our Full+PraG system (Figures 7c and 7d) produces broader and generally more accurate utterances, which we attribute to our factored and pragmatic generation procedure.</p>
<p>Our system's utterances still have substantial qualitative differences from those in human-human dialogues (Figure 8), which-due to the richness of the task (Udagawa and Aizawa, 2019)—often use more complex strategies. Human strategies can unfold across multiple turns, e.g., introducing information in installments or referring to the same dot in multiple turns without being repetitive, as A does when providing more information about the "light grey dot" in Figure 8a. Sophisticated strategies are also used even in single turns, e.g., in Figure 8b, B's utterance "is one on top of the other? if so pick the top one" combines multiple types of speech act (Austin, 1962; Searle, 1976): implicitly acknowledging A's utterance, asserting new information about the dots in view, and issuing a command.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Human's view
U\&amp;A20's view
U\&amp;A'20: i have a large light gray dot with a small black dot above it and to the left
Human: I don't see that. I have a dark gray dot on the line between 7 and 8 o'clock.
U\&amp;A'20: yes
Human: Let's pick that!
U\&amp;A'20: ok
Human: SELECT red
U\&amp;A'20: SELECT blue
(a) An unsuccessful dialogue between a human and the system of Udagawa and Aizawa (2020).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Human's view
Full+Prag's view
Full+Prag: large black dot with a small light grey dot above it
Human: Also have a large gray dot with 3 smaller dots around it
Full+Prag: i do n't have that
Human: $\quad$ Medium sized and light gray dot towards the middle
Full+Prag: yes, let 's pick the larger one
Human: SELECT red
Full+Prag: SELECT blue
(c) An unsuccessful dialogue between a human and our Full+Prag system.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Human's view Full+Prag's view
U\&amp;A'20: i have a large black dot with a smaller light grey dot to the left of it
Human: I don't see that. I have a medium dark
dor to the left of all the others.
U\&amp;A'20: i have a large black dot with a smaller one that is a little lighter and to the left of it .
Human: I have a large light gray with a medium light gray above it.
U\&amp;A'20: yes, let 's click the larger one
Human: SELECT red
U\&amp;A'20: SELECT blue
(b) A successful dialogue between a human and the system of Udagawa and Aizawa (2020).
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Human's view Full+Prag's view
Full+Prag: i have a group of 3 dots, a tiny light grey and a large dark grey
Human: yes me too
Full+Prag: let us select the small black
Human: okay
Full+Prag: pick the small black dot
Human: okay i will
Full+Prag: ok
Human: SELECT red
Full+Prag: SELECT blue
(d) A successful dialogue between a human and our Full+Prag system.</p>
<p>Figure 7: Example dialogues collected during our human evaluation (Section 6.4) of the dialogue systems. We show one unsuccessful (left) and one successful (right) example for each system. The top row is our reimplementation of Udagawa and Aizawa (2020) and the bottom row is Full+Prag, our full system with pragmatic inference.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 8: Examples of unsuccessful (left) and successful (right) dialogues collected between pairs of people during our human evaluation (Section 6.4).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ After filtering to remove any workers who did not play at least one game with another human worker.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{10}$ Until, by necessity, the point where filtering removes all workers who lost a game against any system. Differences between U\&amp;A'20 and Full+PrAG are significant at the $p \leq$ 0.05 level by a one-tailed t-test for minimum worker overall skills up to the 68th percentile.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
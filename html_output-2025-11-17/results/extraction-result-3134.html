<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3134 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3134</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3134</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-13c4e5a6122f3fa2663f63e49537091da6532f35</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35" target="_blank">Are NLP Models really able to Solve Simple Math Word Problems?</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is shown that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs, and models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy.</p>
                <p><strong>Paper Abstract:</strong> The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3134.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3134.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-Sequence LSTM with Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional LSTM encoder with an LSTM decoder and Luong-style attention used to generate arithmetic expressions from math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Seq2Seq (BiLSTM encoder + LSTM decoder with attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bi-directional LSTM encoder + LSTM decoder with Luong attention. Two variants: trained from scratch (≈8.5M parameters) or using non-contextual RoBERTa embeddings (embedding variant raising total params to ≈130M as reported in Table 16). Trained and evaluated on MAWPS, ASDiv-A and SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>One-unknown arithmetic word problems (addition, subtraction, multiplication, division); one-operator and two-operator expressions; grade ≤ 4 word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Predominantly shallow pattern matching / memorization of equation templates and bag-of-words cues; uses attention over token-level representations but often exploits single-token signals rather than compositional algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High accuracy even when question text is removed (Table 3: models achieve up to 77.7% on MAWPS and 64.4% on ASDiv-A), constrained (no word-order) variant still attains strong performance (Table 5: FFN+LSTM: 77.9% MAWPS with RoBERTa), and attention analyses (Tables 6 and 22) showing the model typically attends to a single token (e.g., 'every') and repeats the same equation despite semantic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Seq2Seq still solves many 'Hard' examples (Table 4) and benefits from RoBERTa embeddings, indicating some capacity to learn more robust mappings; performance improves when trained on SVAMP (cross-validation on SVAMP training raises top model to ~65%), showing that exposure to varied data can reduce reliance on spurious cues.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Providing RoBERTa embeddings; question removal (ablation); constrained encoder (removing word order) as an analysis intervention; training data augmentation (adding SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>RoBERTa embeddings increase accuracy substantially (Seq2Seq: ASDiv-A from 55.5%→76.9% in Table 2; on SVAMP scratch 24.2%→40.3% with RoBERTa in Table 10). Removing question causes large drops in realistic evaluation (full vs question-removed comparisons). Constrained encoder shows high performance on benchmarks but fails on SVAMP, indicating brittleness. Training including SVAMP increases generalization on SVAMP folds (~65%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On MAWPS/ASDiv-A (5-fold CV): Seq2Seq (scratch) 79.7% / 55.5%; with RoBERTa 86.7% / 76.9% (Table 2). Question-removed test sets: up to 77.7% MAWPS, 64.4% ASDiv-A (Table 3). On SVAMP: full-set 24.2% (scratch) and 40.3% (RoBERTa) (Table 10). Constrained FFN+LSTM variant: 75.1%/46.3% (scratch) and 77.9%/51.2% (RoBERTa) on MAWPS/ASDiv-A (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Insensitive to question wording (predicts same equation when question changes), attends to single token cues, sign/operator direction errors (e.g., 5-8 instead of 8-5), fails on problems with >2 numbers (sharp accuracy drop from 2→3→4 numbers per Table 15), wrong operator choice under structural changes, brittleness to simple variations (SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors contrast model behaviour with ideal solver expectations: systems are not performing reliable algorithmic arithmetic or compositional reasoning like a symbolic solver or a human; instead they exploit surface correlations and template memorization. No direct comparison to calculators; humans would handle SVAMP easily while models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are NLP Models really able to Solve Simple Math Word Problems?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3134.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3134.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal-driven Tree-structured Neural Model (GTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model using an LSTM encoder and a tree-structured decoder to output expression trees for math word problems (generates arithmetic expressions in tree form).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GTS (LSTM encoder + tree decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM encoder with a goal-driven tree-structured decoder that generates expression trees. Reported parameter counts: ≈15M when trained from scratch, ≈140M total when using RoBERTa embeddings (Table 16). Evaluated on MAWPS, ASDiv-A and SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>One-unknown arithmetic word problems (single-operator and two-operator expressions; addition, subtraction, multiplication, division).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Primarily pattern matching over encoded input with structured output bias (tree decoder) but still reliant on dataset artifacts and token-level cues rather than reliable compositional arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High benchmark scores on MAWPS/ASDiv-A (e.g., with RoBERTa GTS attains 88.5% MAWPS and 81.2% ASDiv-A, Table 2) but substantial drops on SVAMP (e.g., GTS RoBERTa 41.0% full SVAMP, Table 10) and on question-removed tests (still achieves ~60% on ASDiv-A with question removal per Table 3), indicating exploitation of surface cues; attention/ablation analyses in the paper apply to constrained variants but findings extend to trained structured decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The tree-structured decoder is a stronger inductive bias that helps some structure-sensitive cases (GTS outperforms simple Seq2Seq on some splits), and using RoBERTa improves robustness, suggesting capacity for deeper semantic mapping when given better embeddings and more varied training data.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Use of RoBERTa embeddings; question removal ablation; evaluation on SVAMP; training with combined datasets including SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>RoBERTa substantially improves performance (e.g., GTS ASDiv-A from 71.4%→81.2% in Table 2). On SVAMP, RoBERTa brings GTS from 30.8%→41.0% (Table 10). Removing question strongly degrades performance on SVAMP compared to ASDiv-A, indicating that question sensitivity is improved by contextual embeddings but not fully solved.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: GTS (scratch) MAWPS 82.6%, ASDiv-A 71.4%; with RoBERTa 88.5% / 81.2%. On SVAMP (Table 10): scratch 30.8%, RoBERTa 41.0% (full set). Question-removed ASDiv-A: GTS ~60.7% (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails on structural variations and question-sensitivity variations in SVAMP; mis-association of numbers and contexts; poorer performance on two-operator problems and on problems with 3–4 numbers; susceptible to template memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>GTS' tree decoder biases output toward structured expressions, closer to symbolic forms, but empirical behavior still shows reliance on dataset surface patterns rather than human-like reasoning or explicit symbolic computation; no direct symbolic/calculator baseline provided beyond majority-template baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are NLP Models really able to Solve Simple Math Word Problems?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3134.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3134.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Tree Encoder-Decoder Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based encoder combined with a tree-structured decoder that maps problem text to arithmetic expression trees; state-of-the-art on the evaluated benchmarks prior to SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph2Tree (graph encoder + tree decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-based encoder (constructs a graph over tokens/mentions) + tree-structured decoder; reported ≈16M params from-scratch and ≈143M when using RoBERTa embeddings (Table 16). Evaluated on MAWPS, ASDiv-A and SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>One-unknown arithmetic word problems (addition, subtraction, multiplication, division), handling single and two-operator expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Combines graph-based contextual encoding with structural decoding, but empirically still relies on dataset heuristics and token-level correlations rather than robust compositional arithmetic; benefits from pretrained embeddings but not sufficient to ensure true algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Top benchmark numbers on MAWPS/ASDiv-A with RoBERTa (e.g., Graph2Tree RoBERTa: MAWPS 88.7%, ASDiv-A 82.2% in Table 2) but large drop on SVAMP (best Graph2Tree RoBERTa 43.8% full SVAMP, Table 10). Question-removed tests show substantial retained accuracy on MAWPS/ASDiv-A (Table 3) implying exploitation of surface correlations. Error analyses (Table 23) show many simple failures (operator sign errors, wrong operand association).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Graph encoder and tree decoder yield best-in-paper performance on standard benchmarks and benefit most from RoBERTa, indicating the architecture captures useful structure; when retrained including SVAMP performance improves (~65% CV), showing the model can learn improved mappings from better training data.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>RoBERTa embeddings; question removal ablation; training on combined datasets including SVAMP; constrained/no-order ablations used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>RoBERTa provides the largest performance boost across datasets (Graph2Tree: MAWPS/ASDiv-A increases to 88.7%/82.2%). On SVAMP, RoBERTa variant performs best among considered models (43.8% full set). Training with SVAMP raises cross-validated top performance to about 65% indicating data augmentation helps but not full resolution of brittle failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2: Graph2Tree (scratch) 83.7% MAWPS, 77.4% ASDiv-A; with RoBERTa 88.7% / 82.2%. SVAMP (Table 10): Graph2Tree scratch 36.5%, RoBERTa 43.8% (full set). Question-removed: Graph2Tree achieves 77.7% MAWPS and 64.4% ASDiv-A in question-removed experiments (Table 3 / Table 11 comparisons). Table 15 shows breakdown by number-count: ASDiv-A 93.3% (2 nums), 59.0% (3 nums), 47.5% (4 nums); SVAMP 78.3% / 25.4% / 25.4% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Systematic errors include mis-ordering operands (subtraction flipped), wrong operator selection, failing to associate numbers to correct entities in text, inability to generalize to simple variations (question sensitivity, invert operation), large accuracy drop for problems with ≥3 numbers, and brittleness to small syntactic edits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors argue Graph2Tree still does not implement reliable algorithmic arithmetic like symbolic calculators or human procedural reasoning: high benchmark scores are driven by dataset artifacts rather than true compositional arithmetic; no direct symbolic baseline aside from majority-template baseline is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are NLP Models really able to Solve Simple Math Word Problems?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3134.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3134.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constrained Model (FFN+LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constrained Feed-Forward Encoder + LSTM Decoder (No word-order)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intentionally constrained variant that removes sequence (word-order) information by replacing the LSTM encoder with a feed-forward network over token embeddings and initialising decoder from the averaged hidden vectors, used to probe reliance on bag-of-words cues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Constrained model (FFN encoder + LSTM decoder; no word-order)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Feed-Forward Network maps input token embeddings independently to hidden representations (no order or contextualized encoding); LSTM decoder with attention over these token-level hidden vectors. Parameter counts ≈5M when trained from scratch and ≈130M with RoBERTa embeddings (Table 16). Introduced and used in this paper as an analysis/intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>One-unknown arithmetic word problems (addition, subtraction, multiplication, division), evaluated same as other models (MAWPS, ASDiv-A, SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Demonstrates that models can rely on token-level bag-of-words associations and single-token attention heuristics (memorization of token→equation correlations) rather than compositional reasoning; constrained architecture makes explicit the bag-of-words mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Constrained model achieves surprisingly high accuracy on benchmarks despite no word-order (Table 5: 77.9% MAWPS, 51.2% ASDiv-A with RoBERTa), and attention analysis shows near-1.0 weight on single tokens causing identical predictions across semantically different but lexically similar problems (Tables 6 and 22). Constrained model performs poorly on SVAMP (18.3% with RoBERTa, Table 12), showing brittleness to targeted variations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Because this model lacks word-order by design, any evidence against its mechanism is essentially its poor performance on SVAMP and inability to generalize to question-sensitive or structurally varied examples; but successful cases show that surface-pattern reliance suffices for many benchmark instances.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural ablation (removing sequence encoder), supplying non-contextual RoBERTa embeddings vs training embeddings from scratch; tested with question removal and on SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Removing word-order yields high accuracy on MAWPS/ASDiv-A benchmarks (shows dataset vulnerability to bag-of-words heuristics) but very low accuracy on SVAMP (18.3% with RoBERTa). Using RoBERTa embeddings increases constrained model performance modestly vs scratch on both benchmarks and SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 5: FFN + LSTM Decoder (scratch) MAWPS 75.1%, ASDiv-A 46.3%; with RoBERTa 77.9% / 51.2%. On SVAMP (Table 12): scratch 17.5%, RoBERTa 18.3%. Majority template baseline is 11.7% on SVAMP (Table 12), showing constrained model only slightly better on SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Relies on single-token attention leading to catastrophic failures under small wording changes (e.g., flips in question or object), wrong operator selection, inability to use relational/positional cues, fails invert-operation and many structural-variance tests in SVAMP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Serves as an explicit demonstration that good benchmark accuracy does not imply human-like arithmetic reasoning; constrained model performs far worse than humans on SVAMP despite matching many benchmark cases, and unlike symbolic calculators it cannot execute reliable algorithmic transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are NLP Models really able to Solve Simple Math Word Problems?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A goal-driven tree-structured neural model for math word problems <em>(Rating: 2)</em></li>
                <li>Graph-to-tree learning for solving math word problems <em>(Rating: 2)</em></li>
                <li>Deep neural solver for math word problems <em>(Rating: 2)</em></li>
                <li>MAWPS: A math word problem repository <em>(Rating: 2)</em></li>
                <li>A diverse corpus for evaluating and developing English math word problem solvers <em>(Rating: 2)</em></li>
                <li>Annotation artifacts in natural language inference data <em>(Rating: 1)</em></li>
                <li>Attention is not not explanation <em>(Rating: 1)</em></li>
                <li>How well do computers solve math word problems? large-scale dataset construction and evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3134",
    "paper_id": "paper-13c4e5a6122f3fa2663f63e49537091da6532f35",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Seq2Seq",
            "name_full": "Sequence-to-Sequence LSTM with Attention",
            "brief_description": "A bidirectional LSTM encoder with an LSTM decoder and Luong-style attention used to generate arithmetic expressions from math word problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Seq2Seq (BiLSTM encoder + LSTM decoder with attention)",
            "model_description": "Bi-directional LSTM encoder + LSTM decoder with Luong attention. Two variants: trained from scratch (≈8.5M parameters) or using non-contextual RoBERTa embeddings (embedding variant raising total params to ≈130M as reported in Table 16). Trained and evaluated on MAWPS, ASDiv-A and SVAMP.",
            "arithmetic_task_type": "One-unknown arithmetic word problems (addition, subtraction, multiplication, division); one-operator and two-operator expressions; grade ≤ 4 word problems.",
            "reported_mechanism": "Predominantly shallow pattern matching / memorization of equation templates and bag-of-words cues; uses attention over token-level representations but often exploits single-token signals rather than compositional algorithmic arithmetic.",
            "evidence_for_mechanism": "High accuracy even when question text is removed (Table 3: models achieve up to 77.7% on MAWPS and 64.4% on ASDiv-A), constrained (no word-order) variant still attains strong performance (Table 5: FFN+LSTM: 77.9% MAWPS with RoBERTa), and attention analyses (Tables 6 and 22) showing the model typically attends to a single token (e.g., 'every') and repeats the same equation despite semantic changes.",
            "evidence_against_mechanism": "Seq2Seq still solves many 'Hard' examples (Table 4) and benefits from RoBERTa embeddings, indicating some capacity to learn more robust mappings; performance improves when trained on SVAMP (cross-validation on SVAMP training raises top model to ~65%), showing that exposure to varied data can reduce reliance on spurious cues.",
            "intervention_type": "Providing RoBERTa embeddings; question removal (ablation); constrained encoder (removing word order) as an analysis intervention; training data augmentation (adding SVAMP).",
            "effect_of_intervention": "RoBERTa embeddings increase accuracy substantially (Seq2Seq: ASDiv-A from 55.5%→76.9% in Table 2; on SVAMP scratch 24.2%→40.3% with RoBERTa in Table 10). Removing question causes large drops in realistic evaluation (full vs question-removed comparisons). Constrained encoder shows high performance on benchmarks but fails on SVAMP, indicating brittleness. Training including SVAMP increases generalization on SVAMP folds (~65%).",
            "performance_metrics": "On MAWPS/ASDiv-A (5-fold CV): Seq2Seq (scratch) 79.7% / 55.5%; with RoBERTa 86.7% / 76.9% (Table 2). Question-removed test sets: up to 77.7% MAWPS, 64.4% ASDiv-A (Table 3). On SVAMP: full-set 24.2% (scratch) and 40.3% (RoBERTa) (Table 10). Constrained FFN+LSTM variant: 75.1%/46.3% (scratch) and 77.9%/51.2% (RoBERTa) on MAWPS/ASDiv-A (Table 5).",
            "notable_failure_modes": "Insensitive to question wording (predicts same equation when question changes), attends to single token cues, sign/operator direction errors (e.g., 5-8 instead of 8-5), fails on problems with &gt;2 numbers (sharp accuracy drop from 2→3→4 numbers per Table 15), wrong operator choice under structural changes, brittleness to simple variations (SVAMP).",
            "comparison_to_humans_or_symbolic": "Authors contrast model behaviour with ideal solver expectations: systems are not performing reliable algorithmic arithmetic or compositional reasoning like a symbolic solver or a human; instead they exploit surface correlations and template memorization. No direct comparison to calculators; humans would handle SVAMP easily while models fail.",
            "uuid": "e3134.0",
            "source_info": {
                "paper_title": "Are NLP Models really able to Solve Simple Math Word Problems?",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "GTS",
            "name_full": "Goal-driven Tree-structured Neural Model (GTS)",
            "brief_description": "A model using an LSTM encoder and a tree-structured decoder to output expression trees for math word problems (generates arithmetic expressions in tree form).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GTS (LSTM encoder + tree decoder)",
            "model_description": "LSTM encoder with a goal-driven tree-structured decoder that generates expression trees. Reported parameter counts: ≈15M when trained from scratch, ≈140M total when using RoBERTa embeddings (Table 16). Evaluated on MAWPS, ASDiv-A and SVAMP.",
            "arithmetic_task_type": "One-unknown arithmetic word problems (single-operator and two-operator expressions; addition, subtraction, multiplication, division).",
            "reported_mechanism": "Primarily pattern matching over encoded input with structured output bias (tree decoder) but still reliant on dataset artifacts and token-level cues rather than reliable compositional arithmetic.",
            "evidence_for_mechanism": "High benchmark scores on MAWPS/ASDiv-A (e.g., with RoBERTa GTS attains 88.5% MAWPS and 81.2% ASDiv-A, Table 2) but substantial drops on SVAMP (e.g., GTS RoBERTa 41.0% full SVAMP, Table 10) and on question-removed tests (still achieves ~60% on ASDiv-A with question removal per Table 3), indicating exploitation of surface cues; attention/ablation analyses in the paper apply to constrained variants but findings extend to trained structured decoders.",
            "evidence_against_mechanism": "The tree-structured decoder is a stronger inductive bias that helps some structure-sensitive cases (GTS outperforms simple Seq2Seq on some splits), and using RoBERTa improves robustness, suggesting capacity for deeper semantic mapping when given better embeddings and more varied training data.",
            "intervention_type": "Use of RoBERTa embeddings; question removal ablation; evaluation on SVAMP; training with combined datasets including SVAMP.",
            "effect_of_intervention": "RoBERTa substantially improves performance (e.g., GTS ASDiv-A from 71.4%→81.2% in Table 2). On SVAMP, RoBERTa brings GTS from 30.8%→41.0% (Table 10). Removing question strongly degrades performance on SVAMP compared to ASDiv-A, indicating that question sensitivity is improved by contextual embeddings but not fully solved.",
            "performance_metrics": "Table 2: GTS (scratch) MAWPS 82.6%, ASDiv-A 71.4%; with RoBERTa 88.5% / 81.2%. On SVAMP (Table 10): scratch 30.8%, RoBERTa 41.0% (full set). Question-removed ASDiv-A: GTS ~60.7% (Table 11).",
            "notable_failure_modes": "Fails on structural variations and question-sensitivity variations in SVAMP; mis-association of numbers and contexts; poorer performance on two-operator problems and on problems with 3–4 numbers; susceptible to template memorization.",
            "comparison_to_humans_or_symbolic": "GTS' tree decoder biases output toward structured expressions, closer to symbolic forms, but empirical behavior still shows reliance on dataset surface patterns rather than human-like reasoning or explicit symbolic computation; no direct symbolic/calculator baseline provided beyond majority-template baseline.",
            "uuid": "e3134.1",
            "source_info": {
                "paper_title": "Are NLP Models really able to Solve Simple Math Word Problems?",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Graph2Tree",
            "name_full": "Graph-to-Tree Encoder-Decoder Model",
            "brief_description": "A graph-based encoder combined with a tree-structured decoder that maps problem text to arithmetic expression trees; state-of-the-art on the evaluated benchmarks prior to SVAMP.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Graph2Tree (graph encoder + tree decoder)",
            "model_description": "Graph-based encoder (constructs a graph over tokens/mentions) + tree-structured decoder; reported ≈16M params from-scratch and ≈143M when using RoBERTa embeddings (Table 16). Evaluated on MAWPS, ASDiv-A and SVAMP.",
            "arithmetic_task_type": "One-unknown arithmetic word problems (addition, subtraction, multiplication, division), handling single and two-operator expressions.",
            "reported_mechanism": "Combines graph-based contextual encoding with structural decoding, but empirically still relies on dataset heuristics and token-level correlations rather than robust compositional arithmetic; benefits from pretrained embeddings but not sufficient to ensure true algorithmic reasoning.",
            "evidence_for_mechanism": "Top benchmark numbers on MAWPS/ASDiv-A with RoBERTa (e.g., Graph2Tree RoBERTa: MAWPS 88.7%, ASDiv-A 82.2% in Table 2) but large drop on SVAMP (best Graph2Tree RoBERTa 43.8% full SVAMP, Table 10). Question-removed tests show substantial retained accuracy on MAWPS/ASDiv-A (Table 3) implying exploitation of surface correlations. Error analyses (Table 23) show many simple failures (operator sign errors, wrong operand association).",
            "evidence_against_mechanism": "Graph encoder and tree decoder yield best-in-paper performance on standard benchmarks and benefit most from RoBERTa, indicating the architecture captures useful structure; when retrained including SVAMP performance improves (~65% CV), showing the model can learn improved mappings from better training data.",
            "intervention_type": "RoBERTa embeddings; question removal ablation; training on combined datasets including SVAMP; constrained/no-order ablations used for comparison.",
            "effect_of_intervention": "RoBERTa provides the largest performance boost across datasets (Graph2Tree: MAWPS/ASDiv-A increases to 88.7%/82.2%). On SVAMP, RoBERTa variant performs best among considered models (43.8% full set). Training with SVAMP raises cross-validated top performance to about 65% indicating data augmentation helps but not full resolution of brittle failure modes.",
            "performance_metrics": "Table 2: Graph2Tree (scratch) 83.7% MAWPS, 77.4% ASDiv-A; with RoBERTa 88.7% / 82.2%. SVAMP (Table 10): Graph2Tree scratch 36.5%, RoBERTa 43.8% (full set). Question-removed: Graph2Tree achieves 77.7% MAWPS and 64.4% ASDiv-A in question-removed experiments (Table 3 / Table 11 comparisons). Table 15 shows breakdown by number-count: ASDiv-A 93.3% (2 nums), 59.0% (3 nums), 47.5% (4 nums); SVAMP 78.3% / 25.4% / 25.4% respectively.",
            "notable_failure_modes": "Systematic errors include mis-ordering operands (subtraction flipped), wrong operator selection, failing to associate numbers to correct entities in text, inability to generalize to simple variations (question sensitivity, invert operation), large accuracy drop for problems with ≥3 numbers, and brittleness to small syntactic edits.",
            "comparison_to_humans_or_symbolic": "Authors argue Graph2Tree still does not implement reliable algorithmic arithmetic like symbolic calculators or human procedural reasoning: high benchmark scores are driven by dataset artifacts rather than true compositional arithmetic; no direct symbolic baseline aside from majority-template baseline is reported.",
            "uuid": "e3134.2",
            "source_info": {
                "paper_title": "Are NLP Models really able to Solve Simple Math Word Problems?",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Constrained Model (FFN+LSTM)",
            "name_full": "Constrained Feed-Forward Encoder + LSTM Decoder (No word-order)",
            "brief_description": "An intentionally constrained variant that removes sequence (word-order) information by replacing the LSTM encoder with a feed-forward network over token embeddings and initialising decoder from the averaged hidden vectors, used to probe reliance on bag-of-words cues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Constrained model (FFN encoder + LSTM decoder; no word-order)",
            "model_description": "Feed-Forward Network maps input token embeddings independently to hidden representations (no order or contextualized encoding); LSTM decoder with attention over these token-level hidden vectors. Parameter counts ≈5M when trained from scratch and ≈130M with RoBERTa embeddings (Table 16). Introduced and used in this paper as an analysis/intervention.",
            "arithmetic_task_type": "One-unknown arithmetic word problems (addition, subtraction, multiplication, division), evaluated same as other models (MAWPS, ASDiv-A, SVAMP).",
            "reported_mechanism": "Demonstrates that models can rely on token-level bag-of-words associations and single-token attention heuristics (memorization of token→equation correlations) rather than compositional reasoning; constrained architecture makes explicit the bag-of-words mechanism.",
            "evidence_for_mechanism": "Constrained model achieves surprisingly high accuracy on benchmarks despite no word-order (Table 5: 77.9% MAWPS, 51.2% ASDiv-A with RoBERTa), and attention analysis shows near-1.0 weight on single tokens causing identical predictions across semantically different but lexically similar problems (Tables 6 and 22). Constrained model performs poorly on SVAMP (18.3% with RoBERTa, Table 12), showing brittleness to targeted variations.",
            "evidence_against_mechanism": "Because this model lacks word-order by design, any evidence against its mechanism is essentially its poor performance on SVAMP and inability to generalize to question-sensitive or structurally varied examples; but successful cases show that surface-pattern reliance suffices for many benchmark instances.",
            "intervention_type": "Architectural ablation (removing sequence encoder), supplying non-contextual RoBERTa embeddings vs training embeddings from scratch; tested with question removal and on SVAMP.",
            "effect_of_intervention": "Removing word-order yields high accuracy on MAWPS/ASDiv-A benchmarks (shows dataset vulnerability to bag-of-words heuristics) but very low accuracy on SVAMP (18.3% with RoBERTa). Using RoBERTa embeddings increases constrained model performance modestly vs scratch on both benchmarks and SVAMP.",
            "performance_metrics": "Table 5: FFN + LSTM Decoder (scratch) MAWPS 75.1%, ASDiv-A 46.3%; with RoBERTa 77.9% / 51.2%. On SVAMP (Table 12): scratch 17.5%, RoBERTa 18.3%. Majority template baseline is 11.7% on SVAMP (Table 12), showing constrained model only slightly better on SVAMP.",
            "notable_failure_modes": "Relies on single-token attention leading to catastrophic failures under small wording changes (e.g., flips in question or object), wrong operator selection, inability to use relational/positional cues, fails invert-operation and many structural-variance tests in SVAMP.",
            "comparison_to_humans_or_symbolic": "Serves as an explicit demonstration that good benchmark accuracy does not imply human-like arithmetic reasoning; constrained model performs far worse than humans on SVAMP despite matching many benchmark cases, and unlike symbolic calculators it cannot execute reliable algorithmic transformations.",
            "uuid": "e3134.3",
            "source_info": {
                "paper_title": "Are NLP Models really able to Solve Simple Math Word Problems?",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A goal-driven tree-structured neural model for math word problems",
            "rating": 2
        },
        {
            "paper_title": "Graph-to-tree learning for solving math word problems",
            "rating": 2
        },
        {
            "paper_title": "Deep neural solver for math word problems",
            "rating": 2
        },
        {
            "paper_title": "MAWPS: A math word problem repository",
            "rating": 2
        },
        {
            "paper_title": "A diverse corpus for evaluating and developing English math word problem solvers",
            "rating": 2
        },
        {
            "paper_title": "Annotation artifacts in natural language inference data",
            "rating": 1
        },
        {
            "paper_title": "Attention is not not explanation",
            "rating": 1
        },
        {
            "paper_title": "How well do computers solve math word problems? large-scale dataset construction and evaluation",
            "rating": 1
        }
    ],
    "cost": 0.015189999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are NLP Models really able to Solve Simple Math Word Problems?</h1>
<p>Arkil Patel Satwik Bhattamishra Navin Goyal<br>Microsoft Research India<br>arkil.patel@gmail.com, {t-satbh, navingo}@microsoft.com</p>
<h4>Abstract</h4>
<p>The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered "solved" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-ofwords can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.</p>
<h2>1 Introduction</h2>
<p>A Math Word Problem (MWP) consists of a short natural language narrative describing a state of the world and poses a question about some unknown quantities (see Table 1 for some examples). MWPs are taught in primary and higher schools. The MWP task is a type of semantic parsing task where given an MWP the goal is to generate an expression (more generally, equations), which can then be evaluated to get the answer. The task is challenging because a machine needs to extract relevant information from natural language text as well as perform mathematical reasoning to solve it. The complexity of MWPs can be measured along multiple axes, e.g., reasoning and linguistic</p>
<p>PROBLEM:
Text: Jack had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. How many pens does Jack have now? Equation: $8-3=5$</p>
<p>Question Sensitivity Variation:
Text: Jack had 8 pens and Mary had 5 pens. Jack gave 3 pens to Mary. How many pens does Mary have now? Equation: $5+3=8$</p>
<p>Reasoning Ability Variation:
Text: Jack had 8 pens and Mary had 5 pens. Mary gave 3 pens to Jack. How many pens does Jack have now? Equation: $8+3=11$</p>
<p>Structural Invariance Variation:
Text: Jack gave 3 pens to Mary. If Jack had 8 pens and Mary had 5 pens initially, how many pens does Jack have now?
Equation: $8-3=5$
Table 1: Example of a Math Word Problem along with the types of variations that we make to create SVAMP.
complexity and world and domain knowledge. A combined complexity measure is the grade level of an MWP, which is the grade in which similar MWPs are taught. Over the past few decades many approaches have been developed to solve MWPs with significant activity in the last decade (Zhang et al., 2020).</p>
<p>MWPs come in many varieties. Among the simplest are the one-unknown arithmetic word problems where the output is a mathematical expression involving numbers and one or more arithmetic operators $(+,-, *, /)$. Problems in Tables 1 and 6 are of this type. More complex MWPs may have systems of equations as output or involve other operators or may involve more advanced topics and specialized knowledge. Recently, researchers have started focusing on solving such MWPs, e.g. multiple-unknown linear word problems (Huang et al., 2016a), geometry (Sachan and Xing, 2017) and probability (Amini et al., 2019), believing that existing work can handle one-unknown arithmetic MWPs well (Qin et al., 2020). In this paper, we question the capabilities of the state-of-the-art</p>
<p>(SOTA) methods to robustly solve even the simplest of MWPs suggesting that the above belief is not well-founded.</p>
<p>In this paper, we provide concrete evidence to show that existing methods use shallow heuristics to solve a majority of word problems in the benchmark datasets. We find that existing models are able to achieve reasonably high accuracy on MWPs from which the question text has been removed leaving only the narrative describing the state of the world. This indicates that the models can rely on superficial patterns present in the narrative of the MWP and achieve high accuracy without even looking at the question. In addition, we show that a model without word-order information (i.e., the model treats the MWP as a bag-of-words) can also solve the majority of MWPs in benchmark datasets.</p>
<p>The presence of these issues in existing benchmarks makes them unreliable for measuring the performance of models. Hence, we create a challenge set called SVAMP (Simple Variations on Arithmetic Math word Problems; pronounced swamp) of one-unknown arithmetic word problems with grade level up to 4 by applying simple variations over word problems in an existing dataset (see Table 1 for some examples). SVAMP further highlights the brittle nature of existing models when trained on these benchmark datasets. On evaluating SOTA models on SVAMP, we find that they are not even able to solve half the problems in the dataset. This failure of SOTA models on SVAMP points to the extent to which they rely on simple heuristics in training data to make their prediction.</p>
<p>Below, we summarize the two broad contributions of our paper.</p>
<ul>
<li>We show that the majority of problems in benchmark datasets can be solved by shallow heuristics lacking word-order information or lacking question text.</li>
<li>We create a challenge set called SVAMP ${ }^{1}$ for more robust evaluation of methods developed to solve elementary level math word problems.</li>
</ul>
<h2>2 Related Work</h2>
<p>Math Word Problems. A wide variety of methods and datasets have been proposed to solve MWPs; e.g. statistical machine learning (Roy and Roth,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2018), semantic parsing (Huang et al., 2017) and most recently deep learning (Wang et al., 2017; Xie and Sun, 2019; Zhang et al., 2020); see (Zhang et al., 2020) for an extensive survey. Many papers have pointed out various deficiencies with previous datasets and proposed new ones to address them. Koncel-Kedziorski et al. (2016) curated the MAWPS dataset from previous datasets which along with Math23k (Wang et al., 2017) has been used as benchmark in recent works. Recently, ASDiv (Miao et al., 2020) has been proposed to provide more diverse problems with annotations for equation, problem type and grade level. HMWP (Qin et al., 2020) is another newly proposed dataset of Chinese MWPs that includes examples with muliple-unknown variables and requiring non-linear equations to solve them.</p>
<p>Identifying artifacts in datasets has been done for the Natural Language Inference (NLI) task by McCoy et al. (2019), Poliak et al. (2018), and Gururangan et al. (2018). Rosenman et al. (2020) identified shallow heuristics in a Relation Extraction dataset. Cai et al. (2017) showed that biases prevalent in the ROC stories cloze task allowed models to yield state-of-the-art results when trained only on the endings. To the best of our knowledge, this kind of analysis has not been done on any Math Word Problem dataset.</p>
<p>Challenge Sets for NLP tasks have been proposed most notably for NLI and machine translation (Belinkov and Glass, 2019; Nie et al., 2020; Ribeiro et al., 2020). Gardner et al. (2020) suggested creating contrast sets by manually perturbing test instances in small yet meaningful ways that change the gold label. We believe that we are the first to introduce a challenge set targeted specifically for robust evaluation of Math Word Problems.</p>
<h2>3 Background</h2>
<h3>3.1 Problem Formulation</h3>
<p>We denote a Math Word Problem $P$ by a sequence of $n$ tokens $P=\left(\boldsymbol{w}<em n="n">{1}, \ldots, \boldsymbol{w}</em>}\right)$ where each token $\boldsymbol{w<em 1="1">{i}$ can be either a word from a natural language or a numerical value. The word problem $P$ can be broken down into body $B=\left(\boldsymbol{w}</em>}, \ldots, \boldsymbol{w<em k_1="k+1">{k}\right)$ and question $Q=\left(\boldsymbol{w}</em>}, \ldots, \boldsymbol{w<em P="P">{n}\right)$. The goal is to map $P$ to a valid mathematical expression $E</em>$ composed of numbers from $P$ and mathematical operators from the set ${+,-, /, \bullet}$ (e.g. $3+5-4$ ). The metric used to evaluate models on the MWP task is Execution Accuracy, which is obtained from com-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MAWPS</th>
<th style="text-align: center;">ASDiv-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Seq (S)</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">Seq2Seq (R)</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">76.9</td>
</tr>
<tr>
<td style="text-align: left;">GTS (S) (Xie and Sun, 2019)</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">GTS (R)</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">81.2</td>
</tr>
<tr>
<td style="text-align: left;">Graph2Tree (S) (Zhang et al., 2020)</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">77.4</td>
</tr>
<tr>
<td style="text-align: left;">Graph2Tree (R)</td>
<td style="text-align: center;">$\mathbf{8 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Majority Template Baseline</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">21.2</td>
</tr>
</tbody>
</table>
<p>Table 2: 5-fold cross-validation accuracies ( $\uparrow$ ) of baseline models on datasets. (R) means that the model is provided with RoBERTa pretrained embeddings while (S) means that the model is trained from scratch.
paring the predicted answer (calculated by evaluating $E_{P}$ ) with the annotated answer. In this work, we focus only on one-unknown arithmetic word problems.</p>
<h3>3.2 Datasets and Methods</h3>
<p>Many of the existing datasets are not suitable for our analysis as either they are in Chinese, e.g. Math23k (Wang et al., 2017) and HMWP (Qin et al., 2020), or have harder problem types, e.g. Dolphin18K (Huang et al., 2016b). We consider the widely used benchmark MAWPS (KoncelKedziorski et al., 2016) composed of 2373 MWPs and the arithmetic subset of ASDiv (Miao et al., 2020) called ASDiv-A which has 1218 MWPs mostly up to grade level 4 (MAWPS does not have grade level information). Both MAWPS and ASDiv-A are evaluated on 5-fold cross-validation based on pre-assigned splits.
We consider three models in our experiments:
(a) Seq2Seq consists of a Bidirectional LSTM Encoder to encode the input sequence and an LSTM decoder with attention (Luong et al., 2015) to generate the equation.
(c) GTS (Xie and Sun, 2019) uses an LSTM Encoder to encode the input sequence and a tree-based Decoder to generate the equation.
(d) Graph2Tree (Zhang et al., 2020) combines a Graph-based Encoder with a Tree-based Decoder.</p>
<p>The performance of these models on both datasets is shown in Table 2. We either provide RoBERTa (Liu et al., 2019) pre-trained embeddings to the models or train them from scratch. Graph2Tree (Zhang et al., 2020) with RoBERTa embeddings achieves the state-of-the-art for both</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: 5-fold cross-validation accuracies ( $\uparrow$ ) of baseline models on Question-removed datasets.
datasets. Note that our implementations achieve a higher score than the previously reported highest score of $78 \%$ on ASDiv-A (Miao et al., 2020) and $83.7 \%$ on MAWPS (Zhang et al., 2020). The implementation details are provided in Section A in the Appendix.</p>
<h2>4 Deficiencies in existing datasets</h2>
<p>Here we describe the experiments that show that there are important deficiencies in MAWPS and ASDiv-A.</p>
<h3>4.1 Evaluation on Question-removed MWPs</h3>
<p>As mentioned in Section 3.1, each MWP consists of a body $B$, which provides a short narrative on a state of the world and a question $Q$, which inquires about an unknown quantity about the state of the world. For each fold in the provided 5-fold split in MAWPS and ASDiv-A, we keep the train set unchanged while we remove the questions $Q$ from the problems in the test set. Hence, each problem in the test set consists of only the body $B$ without any question $Q$. We evaluate all three models with RoBERTa embeddings on these datasets. The results are provided in Table 3.</p>
<p>The best performing model is able to achieve a 5-fold cross-validation accuracy of $64.4 \%$ on ASDiv-A and $77.7 \%$ on MAWPS. Loosely translated, this means that nearly $64 \%$ of the problems in ASDiv-A and $78 \%$ of the problems in MAWPS can be correctly answered without even looking at the question. This suggests the presence of patterns in the bodies of MWPs in these datasets that have a direct correlation with the output equation.</p>
<p>Some recent works have also demonstrated similar evidence of bias in NLI datasets (Gururangan et al., 2018; Poliak et al., 2018). They observed that NLI models were able to predict the correct label for a large fraction of the standard NLI datasets based on only the hypothesis of the input and without the premise. Our results on question-removed examples of math word problems resembles their observations on NLI datasets and similarly indicates the presence of artifacts that help statistical</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MAWPS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ASDiv-A</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">Hard</td>
</tr>
<tr>
<td style="text-align: left;">Seq2Seq</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: left;">GTS</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">65.3</td>
</tr>
<tr>
<td style="text-align: left;">Graph2Tree</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">63.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of baseline models on the Easy and Hard test sets.
models predict the correct answer without complete information. Note that even though the two methods appear similar, there is an important distinction. In Gururangan et al. (2018), the model is trained and tested on hypothesis only examples and hence, the model is forced to find artifacts in the hypothesis during training. On the other hand, our setting is more natural since the model is trained in the standard way on examples with both the body and the question. Thus, the model is not explicitly forced to learn based on the body during training and our results not only show the presence of artifacts in the datasets but also suggest that the SOTA models exploit them.</p>
<p>Following Gururangan et al. (2018), we attempt to understand the extent to which SOTA models rely on the presence of simple heuristics in the body to predict correctly. We partition the test set into two subsets for each model: problems that the model predicted correctly without the question are labeled Easy and the problems that the model could not answer correctly without the question are labeled Hard. Table 4 shows the performance of the models on their respective Hard and Easy sets. Note that their performance on the full set is already provided in Table 2. It can be seen clearly that although the models correctly answer many Hard problems, the bulk of their success is due to the Easy problems. This shows that the ability of SOTA methods to robustly solve word problems is overestimated and that they rely on simple heuristics in the body of the problems to make predictions.</p>
<h3>4.2 Performance of a constrained model</h3>
<p>We construct a simple model based on the Seq2Seq architecture by removing the LSTM Encoder and replacing it with a Feed-Forward Network that maps the input embeddings to their hidden representations. The LSTM Decoder is provided with the average of these hidden representations as its initial hidden state. During decoding, an attention mechanism (Luong et al., 2015) assigns weights to individual hidden representations of the input</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MAWPS</th>
<th style="text-align: center;">ASDiv-A</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FFN + LSTM Decoder (S)</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">46.3</td>
</tr>
<tr>
<td style="text-align: left;">FFN + LSTM Decoder (R)</td>
<td style="text-align: center;">$\mathbf{7 7 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 5: 5-fold cross-validation accuracies ( $\uparrow$ ) of the constrained model on the datasets. (R) denotes that the model is provided with non-contextual RoBERTa pretrained embeddings while (S) denotes that the model is trained from scratch.
tokens. We use either RoBERTa embeddings (noncontextual; taken directly from Embedding Matrix) or train the model from scratch. Clearly, this model does not have access to word-order information.</p>
<p>Table 5 shows the performance of this model on MAWPS and ASDiv-A. The constrained model with non-contextual RoBERTa embeddings is able to achieve a cross-validation accuracy of 51.2 on ASDiv-A and an astounding 77.9 on MAWPS. It is surprising to see that a model having no word-order information can solve a majority of word problems in these datasets. These results indicate that it is possible to get a good score on these datasets by simply associating the occurence of specific words in the problems to their corresponding equations. We illustrate this more clearly in the next section.</p>
<h3>4.3 Analyzing the attention weights</h3>
<p>To get a better understanding of how the constrained model is able to perform so well, we analyze the attention weights that it assigns to the hidden representations of the input tokens. As shown by Wiegreffe and Pinter (2019), analyzing the attention weights of our constrained model is a reliable way to explain its prediction since each hidden representation consists of information about only that token as opposed to the case of an RNN where each hidden representation may have information about the context i.e. its neighboring tokens.</p>
<p>We train the contrained model (with RoBERTa embeddings) on the full ASDiv-A dataset and observe the attention weights it assigns to the words of the input problems. We found that the model usually attends to a single word to make its prediction, irrespective of the context. Table 6 shows some representative examples. In the first example, the model assigns an attention weight of 1 to the representation of the word 'every' and predicts the correct equation. However, when we make a subtle change to this problem such that the corresponding equation changes, the model keeps on attending over the word 'every' and predicts the same equa-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Input Problem</th>
<th style="text-align: center;">Predicted Equation</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">John delivered 3 letters at every house. If he delivered for 8 houses, how many letters did John deliver?</td>
<td style="text-align: center;">$3 * 8$</td>
<td style="text-align: center;">$24 \checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">John delivered 3 letters at every house. He delivered 24 letters in all. How many houses did John visit to deliver letters?</td>
<td style="text-align: center;">$3 * 24$</td>
<td style="text-align: center;">$72 \notin$</td>
</tr>
<tr>
<td style="text-align: left;">Sam made 8 dollars mowing lawns over the Summer. He charged 2 bucks for each lawn. How many lawns did he mow?</td>
<td style="text-align: center;">$8 / 2$</td>
<td style="text-align: center;">$4 \checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Sam mowed 4 lawns over the Summer. If he charged 2 bucks for each lawn, how much did he earn?</td>
<td style="text-align: center;">$4 / 2$</td>
<td style="text-align: center;">$2 \notin$</td>
</tr>
<tr>
<td style="text-align: left;">10 apples were in the box. 6 are red and the red are green. how many green apples are in the box?</td>
<td style="text-align: center;">$10-6$</td>
<td style="text-align: center;">$4 \checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">10 apples were in the box. Each apple is either red or green. 6 apples are red. how many green apples are in the box?</td>
<td style="text-align: center;">$10 / 6$</td>
<td style="text-align: center;">$1.67 \notin$</td>
</tr>
</tbody>
</table>
<p>Table 6: Attention paid to specific words by the constrained model.
tion, which is now incorrect. Similar observations can be made for the other two examples. Table 22 in the Appendix has more such examples. These examples represent only a few types of spurious correlations that we could find but there could be other types of correlations that might have been missed.</p>
<p>Note that, we do not claim that every model trained on these datasets relies on the occurrence of specific words in the input problem for prediction the way our constrained model does. We are only asserting that it is possible to achieve a good score on these datasets even with such a brittle model, which clearly makes these datasets unreliable to robustly measure model performance.</p>
<h2>5 SVAMP</h2>
<p>The efficacy of existing models on benchmark datasets has led to a shift in the focus of researchers towards more difficult MWPs. We claim that this efficacy on benchmarks is misleading and SOTA MWP solvers are unable to solve even elementary level one-unknown MWPs. To this end, we create a challenge set named SVAMP containing simple one-unknown arithmetic word problems of grade level up to 4. The examples in SVAMP test a model across different aspects of solving word problems. For instance, a model needs to be sensitive to questions and possess certain reasoning abilities to correctly solve the examples in our challenge set. SVAMP is similar to existing datasets of the same level in terms of scope and difficulty for humans, but is less susceptible to being solved by models relying on superficial patterns.</p>
<p>Our work differs from adversarial data collection methods such as Adversarial NLI (Nie et al., 2020) in that these methods create examples depending on the failure of a particular model while we create examples without referring to any specific model. Inspired by the notion of Normative evaluation (Linzen, 2020), our goal is to create a dataset of
simple problems that any system designed to solve MWPs should be expected to solve. We create new problems by applying certain variations to existing problems, similar to the work of Ribeiro et al. (2020). However, unlike their work, our variations do not check for linguistic capabilities. Rather, the choice of our variations is motivated by the experiments in Section 4 as well as certain simple capabilities that any MWP solver must possess.</p>
<h3>5.1 Creating SVAMP</h3>
<p>We create SVAMP by applying certain types of variations to a set of seed examples sampled from the ASDiv-A dataset. We select the seed examples from the recently proposed ASDiv-A dataset since it appears to be of higher quality and harder than the MAWPS dataset: We perform a simple experiment to test the coverage of each dataset by training a model on one dataset and testing it on the other one. For instance, when we train a Graph2Tree model on ASDiv-A, it achieves $82 \%$ accuracy on MAWPS. However, when trained on MAWPS and tested on ASDiv-A, the model achieved only $73 \%$ accuracy. Also recall Table 2 where most models performed better on MAWPS. Moreover, ASDiv has problems annotated according to types and grade levels which are useful for us.</p>
<p>To select a subset of seed examples that sufficiently represent different types of problems in the ASDiv-A dataset, we first divide the examples into groups according to their annotated types. We dis-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Group</th>
<th style="text-align: center;">Examples in <br> ASDiv-A</th>
<th style="text-align: center;">Selected Seed <br> Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Addition</td>
<td style="text-align: center;">278</td>
<td style="text-align: center;">28</td>
</tr>
<tr>
<td style="text-align: left;">Subtraction</td>
<td style="text-align: center;">362</td>
<td style="text-align: center;">33</td>
</tr>
<tr>
<td style="text-align: left;">Multiplication</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">19</td>
</tr>
<tr>
<td style="text-align: left;">Division</td>
<td style="text-align: center;">176</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">1004</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<p>Table 7: Distribution of selected seed examples across types.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">CATEGORY</th>
<th style="text-align: center;">VARIATION</th>
<th style="text-align: center;">EXAMPLES</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Question <br> Sensitivity</td>
<td style="text-align: center;">Same Object, Different Structure</td>
<td style="text-align: center;">Original: Allan brought two balloons and Jake brought four balloons to the park. How many balloons did Allan and Jake have in the park? <br> Variation: Allan brought two balloons and Jake brought four balloons to the park. How many more balloons did Jake have than Allan in the park?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Different Object, Same Structure</td>
<td style="text-align: center;">Original: In a school, there are 542 girls and 387 boys. 290 more boys joined the school. How many pupils are in the school? <br> Variation: In a school, there are 542 girls and 387 boys. 290 more boys joined the school. How many boys are in the school?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Different Object, Different Structure</td>
<td style="text-align: center;">Original: He then went to see the oranges being harvested. He found out that they harvest 83 sacks per day and that each sack contains 12 oranges. How many sacks of oranges will they have after 6 days of harvest? <br> Variation: He then went to see the oranges being harvested. He found out that they harvest 83 sacks per day and that each sack contains 12 oranges. How many oranges do they harvest per day?</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning Ability</td>
<td style="text-align: center;">Add relevant information</td>
<td style="text-align: center;">Original: Every day, Ryan spends 4 hours on learning English and 3 hours on learning Chinese. How many hours does he spend on learning English and Chinese in all? <br> Variation: Every day, Ryan spends 4 hours on learning English and 3 hours on learning Chinese. If he learns for 3 days, how many hours does he spend on learning English and Chinese in all?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Change Information</td>
<td style="text-align: center;">Original: Jack had 142 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Jack have now? <br> Variation: Dorothy had 142 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Dorothy have now?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Invert Operation</td>
<td style="text-align: center;">Original: He also made some juice from fresh oranges. If he used 2 oranges per glass of juice and he made 6 glasses of juice, how many oranges did he use? <br> Variation: He also made some juice from fresh oranges. If he used 2 oranges per glass of juice and he used up 12 oranges, how many glasses of juice did he make?</td>
</tr>
<tr>
<td style="text-align: center;">Structural <br> Invariance</td>
<td style="text-align: center;">Change order of objects</td>
<td style="text-align: center;">Original: John has 8 marbles and 3 stones. How many more marbles than stones does he have? <br> Variation: John has 3 stones and 8 marbles. How many more marbles than stones does he have?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Change order of phrases</td>
<td style="text-align: center;">Original: Matthew had 27 crackers. If Matthew gave equal numbers of crackers to his 9 friends, how many crackers did each person eat? <br> Variation: Matthew gave equal numbers of crackers to his 9 friends. If Matthew had a total of 27 crackers initially, how many crackers did each person eat?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Add irrelevant information</td>
<td style="text-align: center;">Original: Jack had 142 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Jack have now? <br> Variation: Jack had 142 pencils. Dorothy had 50 pencils. Jack gave 31 pencils to Dorothy. How many pencils does Jack have now?</td>
</tr>
</tbody>
</table>
<p>Table 8: Types of Variations with examples. 'Original:' denotes the base example from which the variation is created, 'Variation:' denotes a manually created variation.
card types such as TVQ-Change, TVQ-Initial, CeilDivision and Floor-Division that have less than 20 examples each. We also do not consider the Difference type since it requires the use of an additional modulus operator. For ease of creation, we discard the few examples that are more than 40 words long. To control the complexity of resulting variations, we only consider those problems as seed examples that can be solved by an expression with a single operator. Then, within each group, we cluster examples using K-Means over RoBERTa sentence embeddings of each example. From each cluster, the example closest to the cluster centroid is selected as a seed example. We selected a total of 100 seed examples in this manner. The distribution of seed examples according to different types of problems can be seen in Table 7.</p>
<h3>5.1.1 Variations</h3>
<p>The variations that we make to each seed example can be broadly classified into three categories based on desirable properties of an ideal model:</p>
<p>Question Sensitivity, Reasoning Ability and Structural Invariance. Examples of each type of variation are provided in Table 8.</p>
<ol>
<li>Question Sensitivity. Variations in this category check if the model's answer depends on the question. In these variations, we change the question in the seed example while keeping the body same. The possible variations are as follows:
(a) Same Object, Different Structure: The principal object (i.e. object whose quantity is unknown) in the question is kept the same while the structure of the question is changed.
(b) Different Object, Same Structure: The principal object in the question is changed while the structure of question remains fixed.
(c) Different Object, Different Structure: Both, the principal object in the question and the structure of the question, are changed.</li>
<li>Reasoning Ability. Variations here check whether a model has the ability to correctly de-</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># Problems</th>
<th style="text-align: center;"># Equation <br> Templates</th>
<th style="text-align: center;"># Avg Ops</th>
<th style="text-align: center;">CLD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MAWPS</td>
<td style="text-align: center;">2373</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">1.78</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: left;">ASDiv-A</td>
<td style="text-align: center;">1218</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">1.24</td>
<td style="text-align: center;">0.22</td>
</tr>
</tbody>
</table>
<p>Table 9: Statistics of our dataset compared with MAWPS and ASDiv-A.
termine a change in reasoning arising from subtle changes in the problem text. The different possible variations are as follows:
(a) Add relevant information: Extra relevant information is added to the example that affects the output equation.
(b) Change information: The information provided in the example is changed.
(c) Invert operation: The previously unknown quantity is now provided as information and the question instead asks about a previously known quantity which is now unknown.
3. Structural Invariance. Variations in this category check whether a model remains invariant to superficial structural changes that do not alter the answer or the reasoning required to solve the example. The different possible variations are as follows:
(a) Add irrelevant information: Extra irrelevant information is added to the problem text that is not required to solve the example.
(b) Change order of objects: The order of objects appearing in the example is changed.
(c) Change order of phrases: The order of numbercontaining phrases appearing in the example is changed.</p>
<h3>5.1.2 Protocol for creating variations</h3>
<p>Since creating variations requires a high level of familiarity with the task, the construction of SVAMP is done in-house by the authors and colleagues, hereafter called the workers. The 100 seed examples (as shown in Table 7) are distributed among the workers.</p>
<p>For each seed example, the worker needs to create new variations by applying the variation types discussed in Section 5.1.1. Importantly, a combination of different variations over the seed example can also be done. For each new example created, the worker needs to annotate it with the equation as well as the type of variation(s) used to create it. More details about the creation protocol can be found in Appendix B.</p>
<p>We created a total of 1098 examples. However, since ASDiv-A does not have examples with equations of more than two operators, we discarded 98 examples from our set which had equations consisting of more than two operators. This is to ensure that our challenge set does not have any unfairly difficult examples. The final set of 1000 examples was provided to an external volunteer unfamiliar with the task to check the grammatical and logical correctness of each example.</p>
<h3>5.2 Dataset Properties</h3>
<p>Our challenge set SVAMP consists of oneunknown arithmetic word problems which can be solved by expressions requiring no more than two operators. Table 9 shows some statistics of our dataset and of ASDiv-A and MAWPS. The Equation Template for each example is obtained by converting the corresponding equation into prefix form and masking out all numbers with a meta symbol. Observe that the number of distinct Equation Templates and the Average Number of Operators are similar for SVAMP and ASDiv-A and are considerably smaller than for MAWPS. This indicates that SVAMP does not contain unfairly difficult MWPs in terms of the arithmetic expression expected to be produced by a model.</p>
<p>Previous works, including those introducing MAWPS and ASDiv, have tried to capture the notion of diversity in MWP datasets. Miao et al. (2020) introduced a metric called Corpus Lexicon Diversity (CLD) to measure lexical diversity. Their contention was that higher lexical diversity is correlated with the quality of a dataset. As can be seen from Table 9, SVAMP has a much lesser CLD than ASDiv-A. SVAMP is also less diverse in terms of problem types compared to ASDiv-a. Despite this we will show in the next section that SVAMP is in fact more challenging than ASDiv-A for current models. Thus, we believe that lexical diversity is not a reliable way to measure the quality of MWP datasets. Rather it could depend on other factors such as the diversity in MWP structure which preclude models exploiting shallow heuristics.</p>
<h3>5.3 Experiments on SVAMP</h3>
<p>We train the three considered models on a combination of MAWPS and ASDiv-A and test them on SVAMP. The scores of all three models with and without RoBERTa embeddings for various subsets of SVAMP can be seen in Table 10.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Seq2Seq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GTS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Graph2Tree</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$S$</td>
<td style="text-align: center;">$R$</td>
<td style="text-align: center;">$S$</td>
<td style="text-align: center;">$R$</td>
<td style="text-align: center;">$S$</td>
<td style="text-align: center;">$R$</td>
</tr>
<tr>
<td style="text-align: left;">Full Set</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">$\mathbf{4 3 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">One-Op</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">$\mathbf{5 1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Two-Op</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">$\mathbf{3 3 . 1}$</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">17.8</td>
</tr>
<tr>
<td style="text-align: left;">ADD</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">$\mathbf{4 1 . 9}$</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: left;">SUB</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">$\mathbf{4 1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">MUL</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">$\mathbf{3 8 . 7}$</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">$\mathbf{3 8 . 7}$</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: left;">DIV</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">$\mathbf{6 5 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Results of models on the SVAMP challenge set. $S$ indicates that the model is trained from scratch. $R$ indicates that the model was trained with RoBERTa embeddings. The first row shows the results for the full dataset. The next two rows show the results for subsets of SVAMP composed of examples that have equations with one operator and two operators respectively. The last four rows show the results for subsets of SVAMP composed of examples of type Addition, Subtraction, Multiplication and Division respectively.</p>
<p>The best performing Graph2Tree model is only able to achieve an accuracy of $43.8 \%$ on SVAMP. This indicates that the problems in SVAMP are indeed more challenging for the models than the problems in ASDiv-A and MAWPS despite being of the same scope and type and less diverse. Table 23 in the Appendix lists some simple examples from SVAMP on which the best performing model fails. These results lend further support to our claim that existing models cannot robustly solve elementary level word problems.</p>
<p>Next, we remove the questions from the examples in SVAMP and evaluate them using the three models with RoBERTa embeddings trained on combined MAWPS and ASDiv-A. The scores can be seen in Table 11. The accuracy drops by half when compared to ASDiv-A and more than half compared to MAWPS suggesting that the problems in SVAMP are more sensitive to the information present in the question. We also evaluate the performance of the constrained model on SVAMP when trained on MAWPS and ASDiv-A. The best model achieves only $18.3 \%$ accuracy (see Table 12) which</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">SVAMP w/o ques</th>
<th style="text-align: center;">ASDiv-A w/o ques</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seq2Seq</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">58.7</td>
</tr>
<tr>
<td style="text-align: left;">GTS</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: left;">Graph2Tree</td>
<td style="text-align: center;">$\mathbf{3 0 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 11: Accuracies ( $\uparrow$ ) of models on SVAMP without questions. The 5-fold CV accuracy scores for ASDiv-A without questions are restated for easier comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">SVAMP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FFN + LSTM Decoder (S)</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: left;">FFN + LSTM Decoder (R)</td>
<td style="text-align: center;">$\mathbf{1 8 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Majority Template Baseline</td>
<td style="text-align: center;">11.7</td>
</tr>
</tbody>
</table>
<p>Table 12: Accuracies $(\uparrow)$ of the constrained model on SVAMP. (R) denotes that the model is provided with non-contextual RoBERTa pretrained embeddings while (S) denotes that the model is trained from scratch.
is marginally better than the majority template baseline. This shows that the problems in SVAMP are less vulnerable to being solved by models using simple patterns and that a model needs contextual information in order to solve them.</p>
<p>We also explored using SVAMP for training by combining it with ASDiv-A and MAWPS. We performed 5-fold cross-validation over SVAMP where the model was trained on a combination of the three datasets and tested on unseen examples from SVAMP. To create the folds, we first divide the seed examples into five sets, with each type of example distributed nearly equally among the sets. A fold is obtained by combining all the examples in SVAMP that were created using the seed examples in a set. In this way, we get five different folds from the five sets. We found that the best model achieved about $65 \%$ accuracy. This indicates that even with additional training data existing models are still not close to the performance that was estimated based on prior benchmark datasets.</p>
<p>To check the influence of different categories of variations in SVAMP, for each category, we measure the difference between the accuracy of the best model on the full dataset and its accuracy on a subset containing no example created from that category of variations. The results are shown in Table 13. Both the Question Sensitivity and Struc-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Removed Category</th>
<th style="text-align: center;"># Removed <br> Examples</th>
<th style="text-align: center;">Change in <br> Accuracy $(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question Sensitivity</td>
<td style="text-align: center;">462</td>
<td style="text-align: center;">+13.7</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning Ability</td>
<td style="text-align: center;">649</td>
<td style="text-align: center;">-3.3</td>
</tr>
<tr>
<td style="text-align: left;">Structural Invariance</td>
<td style="text-align: center;">467</td>
<td style="text-align: center;">+4.5</td>
</tr>
</tbody>
</table>
<p>Table 13: Change in accuracies when categories are removed. The Change in Accuracy $\Delta=\operatorname{Acc}($ Full $C a t)-\operatorname{Acc}($ Full $)$, where $\operatorname{Acc}($ Full $)$ is the accuracy on the full set and $\operatorname{Acc}($ Full - Cat $)$ is the accuracy on the set of examples left after removing all examples which were created using Category Cat either by itself, or in use with other categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Removed Variation</th>
<th style="text-align: center;"># Removed <br> Examples</th>
<th style="text-align: center;">Change in <br> Accuracy $(\Delta)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Same Obj, Diff Struct</td>
<td style="text-align: center;">325</td>
<td style="text-align: center;">+7.3</td>
</tr>
<tr>
<td style="text-align: left;">Diff Obj, Same Struct</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">+1.5</td>
</tr>
<tr>
<td style="text-align: left;">Diff Obj, Diff Struct</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">+1.3</td>
</tr>
<tr>
<td style="text-align: left;">Add Rel Info</td>
<td style="text-align: center;">264</td>
<td style="text-align: center;">+5.5</td>
</tr>
<tr>
<td style="text-align: left;">Change Info</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">+3.2</td>
</tr>
<tr>
<td style="text-align: left;">Invert Operation</td>
<td style="text-align: center;">255</td>
<td style="text-align: center;">-10.2</td>
</tr>
<tr>
<td style="text-align: left;">Change order of Obj</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">+2.3</td>
</tr>
<tr>
<td style="text-align: left;">Change order of Phrases</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">-3.3</td>
</tr>
<tr>
<td style="text-align: left;">Add Irrel Info</td>
<td style="text-align: center;">281</td>
<td style="text-align: center;">+6.9</td>
</tr>
</tbody>
</table>
<p>Table 14: Change in accuracies when variations are removed. The Change in Accuracy $\Delta=\operatorname{Acc}($ Full $V a r)-\operatorname{Acc}($ Full $)$, where $\operatorname{Acc}($ Full $)$ is the accuracy on the full set and $\operatorname{Acc}(F u l l-V a r)$ is the accuracy on the set of examples left after removing all examples which were created using Variation $V a r$ either by itself, or in use with other variations.
tural Invariance categories of variations show an increase in accuracy when their examples are removed, thereby indicating that they make SVAMP more challenging. The decrease in accuracy for the Reasoning Ability category can be attributed in large part to the Invert Operation variation. This is not surprising because most of the examples created from Invert Operation are almost indistinguishable from examples in ASDiv-A, which the model has seen during training. The scores for each individual variation are provided in Table 14.</p>
<p>We also check the break-up of performance of the best performing Graph2Tree model according to the number of numbers present in the text of the input problem. We trained the model on both ASDiv-A and MAWPS and tested on SVAMP and compare those results against the 5 -fold crossvalidation setting of ASDiv-A. The scores are provided in Table 15. While the model can solve many problems consisting of only two numbers in the input text (even in our challenge set), it performs very badly on problems having more than two numbers. This shows that current methods are incapable of properly associating numbers to their context. Also, the gap between the performance on ASDiv-A and SVAMP is high, indicating that the examples in SVAMP are more difficult for these models to solve than the examples in ASDiv-A even when considering the structurally same type of word problems.</p>
<h2>6 Final Remarks</h2>
<p>Going back to the original question, are existing NLP models able to solve elementary math word</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">$\mathbf{2}$ nums</th>
<th style="text-align: center;">$\mathbf{3}$ nums</th>
<th style="text-align: center;">$\mathbf{4}$ nums</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ASDiv-A</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">47.5</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">25.4</td>
</tr>
</tbody>
</table>
<p>Table 15: Accuracy break-up according to the number of numbers in the input problem. 2 nums refers to the subset of problems which have only 2 numbers in the problem text. Similarly, $\mathbf{3}$ nums and $\mathbf{4}$ nums are subsets that contain 3 and 4 different numbers in the problem text respectively.
problems? This paper gives a negative answer. We have empirically shown that the benchmark English MWP datasets suffer from artifacts making them unreliable to gauge the performance of MWP solvers: we demonstrated that the majority of problems in the existing datasets can be solved by simple heuristics even without word-order information or the question text.</p>
<p>The performance of the existing models in our proposed challenge dataset also highlights their limitations in solving simple elementary level word problems. We hope that our challenge set SVAMP, containing elementary level MWPs, will enable more robust evaluation of methods. We believe that methods proposed in the future that make genuine advances in solving the task rather than relying on simple heuristics will perform well on SVAMP despite being trained on other datasets such as ASDiv-A and MAWPS.</p>
<p>In recent years, the focus of the community has shifted towards solving more difficult MWPs such as non-linear equations and word problems with multiple unknown variables. We demonstrated that the capability of existing models to solve simple one-unknown arithmetic word problems is overestimated. We believe that developing more robust methods for solving elementary MWPs remains a significant open problem.</p>
<h2>Acknowledgements</h2>
<p>We thank the anonymous reviewers for their constructive comments. We would also like to thank our colleagues at Microsoft Research for providing valuable feedback. We are grateful to Monojit Choudhury for discussions about creating the dataset. We thank Kabir Ahuja for carrying out preliminary experiments that led to this work. We also thank Vageesh Chandramouli and Nalin Patel for their help in dataset construction.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Yonatan Belinkov and James Glass. 2019. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49-72.</p>
<p>Zheng Cai, Lifu Tu, and Kevin Gimpel. 2017. Pay attention to the ending:strong neural baselines for the ROC story cloze task. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 616622, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models' local decision boundaries via contrast sets.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, and Jian Yin. 2017. Learning fine-grained expressions to solve math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 805-814, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. 2016a. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 887-896, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. 2016b. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 887-896, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.</p>
<p>Tal Linzen. 2020. How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 52105217, Online. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412-1421, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018.</p>
<p>Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, and Liang Lin. 2020. Semantically-aligned universal tree-structured solver for math word problems.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49024912, Online. Association for Computational Linguistics.</p>
<p>Shachar Rosenman, Alon Jacovi, and Yoav Goldberg. 2020. Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3702-3710, Online. Association for Computational Linguistics.</p>
<p>Subhro Roy and Dan Roth. 2018. Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics, 6:159-172.</p>
<p>Mrinmaya Sachan and Eric Xing. 2017. Learning to solve geometry problems from natural language demonstrations in textbooks. In Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251-261, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845854, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 11-20, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Zhipeng Xie and Shichao Sun. 2019. A goal-driven tree-structured neural model for math word problems. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5299-5305. International Joint Conferences on Artificial Intelligence Organization.
D. Zhang, L. Wang, L. Zhang, B. T. Dai, and H. T. Shen. 2020. The gap of semantic parsing: A survey on automatic math word problem solvers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(9):2287-2305.</p>
<p>Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. 2020. Graph-totree learning for solving math word problems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 39283937, Online. Association for Computational Linguistics.</p>
<h2>A Implementation Details</h2>
<p>We use 8 NVIDIA Tesla P100 GPUs each with 16 GB memory to run our experiments. The hyperparameters used for each model are shown in Table 16. The best hyperparameters are highlighted in bold. Following the setting of Zhang et al. (2020), the arithmetic word problems from MAWPS are divided into five folds, each of equal test size. For ASDiv-A, we consider the 5-fold split [238, 238, 238, 238, 266] provided by the authors (Miao et al., 2020).</p>
<h2>B Creation Protocol</h2>
<p>We create variations in template form. Generating more data by scaling up from these templates or by performing automatic operations on these templates is left for future work. The template form of an example is created by replacing certain words with their respective tags. Table 17 lists the various tags used in the templates.</p>
<p>The $[N U M]$ tag is used to replace all the numbers and the $[N A M E]$ tag is used to replace all the Names of Persons in the example. The $[O B J s]$ and $[O B J p]$ tags are used for replacing the objects in the example. The $[O B J s]$ and $[O B J p]$ tags with the same index represent the same object in singular and plural form respectively. The intention when using the $[O B J s]$ or the $[O B J p]$ tag is that it can be used as a placeholder for other similar words, which when entered in that place, make sense as per the context. These tags must not be used for collectives; rather they should be used for the things that the collective represents. Some example uses of $[O B J s]$ and $[O B J p]$ tags are provided in Table 18. Lastly, the $[M O D]$ tag must be used to replace any modifier preceding the $[O B J s] /[O B J p]$ tag.</p>
<p>A preprocessing script is executed over the Seed Examples to automatically generate template suggestions for the workers. The script uses Named Entity Recognition and Regular Expression matching to automatically mask the names of persons and the numbers found in the Seed Examples. The outputs from the script are called the Script Examples. An illustration is provided in Table 19.</p>
<p>Each worker is provided with the Seed Examples along with their respective Script Examples that have been alloted to them. The worker's task is to edit the Script Example by correcting any mistake made by the preprocessing script and adding any new tags such as the $[O B J s]$ and the $[O B J p]$
tags in order to create the Base Example. If a worker introduces a new tag, they need to mark it against its example-specific value. If the tag is used to mask objects, the worker needs to mark both the singular and plural form of the object in a commaseperated manner. Additionally, for each unique index of $[O B J s] /[O B J p]$ tag in the example, the worker must enter atleast one alternate value that can be used in that place. Similarly, the worker must enter atleast two modifier words that can be used to precede the principal $[O B J s] /[O B J p]$ tags in the example. These alternate values are used to gather a lexicon which can be utilised to scale-up the data at a later stage. An illustration of this process is provided in Table 20.</p>
<p>In order to create the variations, the worker needs to check the different types of variations in Table 8 to see if they can be applied to the Base Example. If applicable, the worker needs to create the Variation Example while also making a note of the type of variation. If a particular example is the result of performing multiple types of variations, all types of variations should be listed according to their order of application from latest to earliest in a comma-seperated manner. For any variation, if a worker introduces a new tag, they need to mark it against its example-specific value as mentioned before. The index of any new tag introduced needs to be one more than the highest index already in use for that tag in the Base Example or its previously created variations.</p>
<p>To make the annotation more efficient and streamlined, we provide the following steps to be followed in order:</p>
<ol>
<li>Apply the Question Sensitivity variations on the Base Example.</li>
<li>Apply the Invert Operation variation on the Base Example and on all the variations obtained so far.</li>
<li>Apply the Add relevant information variation on the Base Example. Then considering these variations as Base Examples, apply the Question Sensitivity variations.</li>
<li>Apply the Add irrelevant information variation on the Base Example and on all the variations obtained so far.</li>
<li>Apply the Change information variation on the Base Example and on all the variations obtained so far.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameters</th>
<th style="text-align: center;">Seq2Seq</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GTS</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Graph2Tree</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Constrained</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">Scratch</td>
<td style="text-align: center;">RoBERTa</td>
</tr>
<tr>
<td style="text-align: center;">Embedding Size</td>
<td style="text-align: center;">[128, 256]</td>
<td style="text-align: center;">[768]</td>
<td style="text-align: center;">[128, 256]</td>
<td style="text-align: center;">[768]</td>
<td style="text-align: center;">[128, 256]</td>
<td style="text-align: center;">[768]</td>
<td style="text-align: center;">[128, 256]</td>
<td style="text-align: center;">[768]</td>
</tr>
<tr>
<td style="text-align: center;">Hidden Size</td>
<td style="text-align: center;">[256, 384]</td>
<td style="text-align: center;">[256, 384]</td>
<td style="text-align: center;">[384, 512]</td>
<td style="text-align: center;">[384, 512]</td>
<td style="text-align: center;">[256, 384]</td>
<td style="text-align: center;">[256, 384]</td>
<td style="text-align: center;">[256, 384]</td>
<td style="text-align: center;">[256, 384]</td>
</tr>
<tr>
<td style="text-align: center;">Number of Layers</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
<td style="text-align: center;">[1, 2]</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">[5e-4, 8e-4,</td>
<td style="text-align: center;">[1e-4, 2e-4,</td>
<td style="text-align: center;">[8e-4, 1e-3,</td>
<td style="text-align: center;">[5e-4, 8e-4,</td>
<td style="text-align: center;">[8e-4, 1e-3,</td>
<td style="text-align: center;">[5e-4, 8e-4,</td>
<td style="text-align: center;">[1e-3, 2e-3]</td>
<td style="text-align: center;">[1e-3, 2e-3]</td>
</tr>
<tr>
<td style="text-align: center;">Embedding LR</td>
<td style="text-align: center;">[5e-4, 8e-4,</td>
<td style="text-align: center;">5e-4, 8e-6,</td>
<td style="text-align: center;">[6e-4, 1e-3,</td>
<td style="text-align: center;">[5e-6, 8e-6,</td>
<td style="text-align: center;">[8e-4, 1e-3,</td>
<td style="text-align: center;">[5e-6, 8e-6,</td>
<td style="text-align: center;">[1e-3, 2e-3]</td>
<td style="text-align: center;">[1e-3, 2e-3]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1e-3]</td>
<td style="text-align: center;">[5e-6, 8e-6,</td>
<td style="text-align: center;">[1e-3]</td>
<td style="text-align: center;">[2e-3]</td>
<td style="text-align: center;">[2e-3]</td>
<td style="text-align: center;">[2e-6, 8e-6,</td>
<td style="text-align: center;">[1e-3, 2e-3]</td>
<td style="text-align: center;">[1e-3, 2e-3]</td>
</tr>
<tr>
<td style="text-align: center;">Batch Size</td>
<td style="text-align: center;">[8, 16]</td>
<td style="text-align: center;">[4, 8]</td>
<td style="text-align: center;">[8, 16]</td>
<td style="text-align: center;">[4, 8]</td>
<td style="text-align: center;">[8, 16]</td>
<td style="text-align: center;">[4, 8]</td>
<td style="text-align: center;">[8, 16]</td>
<td style="text-align: center;">[4, 8]</td>
</tr>
<tr>
<td style="text-align: center;">Dropout</td>
<td style="text-align: center;">[0.1]</td>
<td style="text-align: center;">[0.1]</td>
<td style="text-align: center;">[0.5]</td>
<td style="text-align: center;">[0.5]</td>
<td style="text-align: center;">[0.5]</td>
<td style="text-align: center;">[0.5]</td>
<td style="text-align: center;">[0.1]</td>
<td style="text-align: center;">[0.1]</td>
</tr>
<tr>
<td style="text-align: center;"># Parameters</td>
<td style="text-align: center;">8.5 M</td>
<td style="text-align: center;">130 M</td>
<td style="text-align: center;">15 M</td>
<td style="text-align: center;">140 M</td>
<td style="text-align: center;">16 M</td>
<td style="text-align: center;">143 M</td>
<td style="text-align: center;">5 M</td>
<td style="text-align: center;">130 M</td>
</tr>
<tr>
<td style="text-align: center;">Epochs</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Avg Time/Epoch</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">15</td>
</tr>
</tbody>
</table>
<p>Table 16: Different hyperparameters and the values considered for each of them in the models. The best hyperparameters for each model for 5-fold cross-validation on ASDiv-A are highlighted in bold. Average Time/Epoch is measured in seconds.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tag</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">NUMx</td>
<td style="text-align: center;">Number</td>
</tr>
<tr>
<td style="text-align: center;">NAMEx</td>
<td style="text-align: center;">Names of Persons</td>
</tr>
<tr>
<td style="text-align: center;">OBJxx</td>
<td style="text-align: center;">Singular Object</td>
</tr>
<tr>
<td style="text-align: center;">OBJpx</td>
<td style="text-align: center;">Plural Object</td>
</tr>
<tr>
<td style="text-align: center;">MODx</td>
<td style="text-align: center;">Modifier</td>
</tr>
</tbody>
</table>
<p>Table 17: List of tags used in annotated templates. $\boldsymbol{x}$ denotes the index of the tag.
6. Apply the Change order of Objects and Change order of Events or Phrases variations on the Base Example and on all the variations obtained so far.</p>
<p>Table 21 provides some variations for the example in Table 20. Note that two seperate examples were created through the 'Add irrelevant information' variation. The first by applying the variation on the Original Example and the second by applying it on a previously created example (as directed in Step-4).</p>
<p>To make sure that different workers following our protocol make similar types of variations, we hold a trial where each worker created variations from the same 5 seed examples. We observed that barring minor linguistic differences, most of the created examples were the same, thereby indicating the effectiveness of our protocol.</p>
<h2>C Analyzing Attention Weights</h2>
<p>In Table 22, we provide more examples to illustrate the specific word to equation correlation that the constrained model learns.</p>
<h2>D Examples of Simple Problems</h2>
<p>In Table 23, we provide a few simple examples from SVAMP that the best performing Graph2Tree model could not solve.</p>
<h2>E Ethical Considerations</h2>
<p>In this paper, we consider the task of automatically solving Math Word Problems (MWPs). Our work encourages the development of better systems that can robustly solve MWPs. Such systems can be deployed for use in the education domain. E.g., an application can be developed that takes MWPs as input and provides detailed explanations to solve them. Such applications can aide elementary school students in learning and practicing math.</p>
<p>We present a challenge set called SVAMP of oneunknown English Math Word Problems. SVAMP is created in-house by the authors themselves by applying some simple variations to examples from ASDiv-A (Miao et al., 2020), which is a publicly available dataset. We provide a detailed creation protocol in Section B. We are not aware of any risks associated with our proposed dataset.</p>
<p>To provide an estimate of the energy requirements of our experiments, we provide the details such as computing platform and running time in Section A. Also, in order to reduce carbon costs from our experiments, we first perform a broad hyperparameter search over only a single fold for the datasets and then run the cross validation experiment over a select few hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Excerpt of Example <br> Template Form</th>
<th style="text-align: left;">Beth has 4 packs of red crayons and 2 packs of green crayons. Each pack has 10 crayons in it.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Excerpt of Example <br> Template Form</td>
<td style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of $[M O D 1][O B J p 1]$ and $[N U M 2]$ packs of $[M O D 2][O B J p 1]$.</td>
</tr>
<tr>
<td style="text-align: left;">In a game, Frank defeated 6 enemies. Each enemy earned him 9 points. <br> In a game $[N A M E 1]$ defeated $[N U M 1][O B J p 1]$. Each $[O B J s 1]$ earned him $[N U M 2]$ points.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 18: Example uses of tags. Note that in the first example, the word 'packs' was not replaced since it is a collective. In the second example, the word 'points' was not replaced because it is too instance-specific and no other word can be used in that place.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Seed Example Body</th>
<th style="text-align: left;">Beth has 4 packs of crayons. Each pack has 10 crayons in it. She also has 6 extra crayons.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seed Example Question</td>
<td style="text-align: left;">How many crayons does Beth have altogether?</td>
</tr>
<tr>
<td style="text-align: left;">Seed Example Equation</td>
<td style="text-align: left;">$4 * 10 * 6$</td>
</tr>
<tr>
<td style="text-align: left;">Script Example Body</td>
<td style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of crayons. Each pack has $[N U M 2]$ crayons in it. She also has $[N U M 3]$ extra</td>
</tr>
<tr>
<td style="text-align: left;">Script Example Question</td>
<td style="text-align: left;">crayons.</td>
</tr>
<tr>
<td style="text-align: left;">Script Example Equation</td>
<td style="text-align: left;">How many crayons does $[N A M E 1]$ have altogether?</td>
</tr>
<tr>
<td style="text-align: left;">Script Example Equation</td>
<td style="text-align: left;">$[N U M 1] *[N U M 2]+[N U M 3]$.</td>
</tr>
</tbody>
</table>
<p>Table 19: An example of suggested templates. Note that the preprocessing script could not succesfully tag crayons as $[O B J p 1]$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Script Example Body</th>
<th style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of crayons. Each pack has $[N U M 2]$ crayons in it. She also has $[N U M 3]$ extra</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Script Example Question</td>
<td style="text-align: left;">crayons.</td>
</tr>
<tr>
<td style="text-align: left;">How many crayons does $[N A M E 1]$ have altogether?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Base Example Body</td>
<td style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of $[O B J p 1]$. Each pack has $[N U M 2][O B J p 1]$ in it. She also has $[N U M 3]$</td>
</tr>
<tr>
<td style="text-align: left;">Base Example Question</td>
<td style="text-align: left;">extra $[O B J p 1]$.</td>
</tr>
<tr>
<td style="text-align: left;">How many $[O B J p 1]$ does $[N A M E 1]$ have altogether?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$[O B J 1]$</td>
<td style="text-align: left;">crayon, crayons</td>
</tr>
<tr>
<td style="text-align: left;">Alternate for $[O B J 1]$</td>
<td style="text-align: left;">pencil, pencils</td>
</tr>
<tr>
<td style="text-align: left;">Alternate for $[M O D]$</td>
<td style="text-align: left;">small, large</td>
</tr>
</tbody>
</table>
<p>Table 20: An example of editing the Suggested Templates. The edits are indicated in green.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Base Example Body</th>
<th style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of $[O B J p 1]$. Each pack has $[N U M 2][O B J p 1]$ in it. She also has $[N U M 3]$ extra</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Base Example Question</td>
<td style="text-align: left;">$[O B J p 1]$</td>
</tr>
<tr>
<td style="text-align: left;">Base Example Equation</td>
<td style="text-align: left;">$[N A M E 1]$ have altogether?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$[N U M 1] *[N U M 2]+[N U M 3]$</td>
</tr>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: left;">Question Sensitivity</td>
</tr>
<tr>
<td style="text-align: left;">Variation</td>
<td style="text-align: left;">Same Object, Different Structure</td>
</tr>
<tr>
<td style="text-align: left;">Variation Body</td>
<td style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of $[O B J p 1]$. Each pack has $[N U M 2][O B J p 1]$ in it. She also has $[N U M 3]$ extra</td>
</tr>
<tr>
<td style="text-align: left;">Variation Question</td>
<td style="text-align: left;">$[O B J p 1]$.</td>
</tr>
<tr>
<td style="text-align: left;">Variation Equation</td>
<td style="text-align: left;">How many $[O B J p 1]$ does $[N A M E 1]$ have in packs?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$[N U M 1] *[N U M 2]$</td>
</tr>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: left;">Structural Invariance</td>
</tr>
<tr>
<td style="text-align: left;">Variation</td>
<td style="text-align: left;">Add irrelevant information</td>
</tr>
<tr>
<td style="text-align: left;">Variation Body</td>
<td style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of $[O B J p 1]$ and $[N U M 4]$ packs of $[O B J p 2]$. Each pack has $[N U M 2][O B J p 1]$</td>
</tr>
<tr>
<td style="text-align: left;">Variation Question</td>
<td style="text-align: left;">in it. She also has $[N U M 3]$ extra $[O B J p 1]$.</td>
</tr>
<tr>
<td style="text-align: left;">Variation Equation</td>
<td style="text-align: left;">How many $[O B J p 1]$ does $[N A M E 1]$ have altogether?</td>
</tr>
<tr>
<td style="text-align: left;">Variation Equation</td>
<td style="text-align: left;">$[N U M 1] *[N U M 2]+[N U M 3]$</td>
</tr>
<tr>
<td style="text-align: left;">Variation Body</td>
<td style="text-align: left;">$[N A M E 1]$ has $[N U M 1]$ packs of $[O B J p 1]$ and $[N U M 4]$ packs of $[O B J p 2]$. Each pack has $[N U M 2][O B J p 1]$</td>
</tr>
<tr>
<td style="text-align: left;">Variation Question</td>
<td style="text-align: left;">in it. She also has $[N U M 3]$ extra $[O B J p 1]$.</td>
</tr>
<tr>
<td style="text-align: left;">Variation Equation</td>
<td style="text-align: left;">How many $[O B J p 1]$ does $[N A M E 1]$ have in packs?</td>
</tr>
</tbody>
</table>
<p>Table 21: Example Variations</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Problem</th>
<th style="text-align: center;">Predicted Equation</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Mike had 8 games. After he gave some to his friend he had 5 left. How many games did he give to his friend?</td>
<td style="text-align: center;">$8-5$</td>
<td style="text-align: center;">$3 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">After Mike gave some games to his friend he had 5 left. If he had 8 games initially, how many games did he give to his friend?</td>
<td style="text-align: center;">$5-8$</td>
<td style="text-align: center;">$-3 \times$</td>
</tr>
<tr>
<td style="text-align: center;">Jack bought 5 radios but only 2 of them worked. How many radios did not work?</td>
<td style="text-align: center;">$5-2$</td>
<td style="text-align: center;">$3 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Jack bought 5 radios but only 2 of them worked. How many more radios did not work than those that did?</td>
<td style="text-align: center;">$5-2$</td>
<td style="text-align: center;">$3 \times$</td>
</tr>
<tr>
<td style="text-align: center;">Ross had 6 marbles. He sold 2 marbles to Joey. How many marbles does Ross have now?</td>
<td style="text-align: center;">$6-2$</td>
<td style="text-align: center;">$4 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Ross had 6 marbles. Joey sold 2 marbles to Ross. How many marbles does Ross have now?</td>
<td style="text-align: center;">$6-2$</td>
<td style="text-align: center;">$4 \times$</td>
</tr>
<tr>
<td style="text-align: center;">Bob collected 7 cans. He lost 3 of them. How many cans does Bob have now?</td>
<td style="text-align: center;">$7-3$</td>
<td style="text-align: center;">$4 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Bob had 7 cans. He collected 3 more. How many cans does Bob have now?</td>
<td style="text-align: center;">$7-3$</td>
<td style="text-align: center;">$4 \times$</td>
</tr>
<tr>
<td style="text-align: center;">Joey had 9 pens. he used 4 of them. How many pens does he have now?</td>
<td style="text-align: center;">$9-4$</td>
<td style="text-align: center;">$5 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Joey used 4 pens. If he had 9 pens intially, how many pens does he have now?</td>
<td style="text-align: center;">$4-9$</td>
<td style="text-align: center;">$-5 \times$</td>
</tr>
<tr>
<td style="text-align: center;">Jill read 30 pages in 10 days. How many pages did she read per day?</td>
<td style="text-align: center;">$30 / 10$</td>
<td style="text-align: center;">$3 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Jill can read 3 pages per day. How many pages can she read in 10 days?</td>
<td style="text-align: center;">$3 / 10$</td>
<td style="text-align: center;">$0.33 \times$</td>
</tr>
<tr>
<td style="text-align: center;">Mary's hair was 15 inches long. After she did a haircut, it was 10 inches long . how much did she cut off?</td>
<td style="text-align: center;">$15-10$</td>
<td style="text-align: center;">$5 \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Mary cut off 5 inches of her hair. If her hair is now 10 inches long, how long was it earlier?</td>
<td style="text-align: center;">$5-10$</td>
<td style="text-align: center;">$-5 \times$</td>
</tr>
</tbody>
</table>
<p>Table 22: Attention paid to specific words by the constrained model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Input Problem</th>
<th style="text-align: center;">Correct Equation</th>
<th style="text-align: center;">Predicted Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Every day ryan spends 6 hours on learning english and 2 hours on learning chinese. How many more hours does he spend on learning english than he does on learning chinese?</td>
<td style="text-align: center;">$6-2$</td>
<td style="text-align: center;">$2-6$</td>
</tr>
<tr>
<td style="text-align: center;">In a school there are 34 girls and 841 boys. How many more boys than girls does the school have?</td>
<td style="text-align: center;">$841-34$</td>
<td style="text-align: center;">$34-841$</td>
</tr>
<tr>
<td style="text-align: center;">David did 44 push-ups in gym class today. David did 9 more push-ups than zachary. How many push-ups did zachary do?</td>
<td style="text-align: center;">$44-9$</td>
<td style="text-align: center;">$44+9$</td>
</tr>
<tr>
<td style="text-align: center;">Dan has $\$ 3$ left with him after he bought a candy bar for $\$ 2$. How much money did he have initially?</td>
<td style="text-align: center;">$3+2$</td>
<td style="text-align: center;">$3-2$</td>
</tr>
<tr>
<td style="text-align: center;">Jake has 11 fewer peaches than steven. If jake has 17 peaches. How many peaches does steven have?</td>
<td style="text-align: center;">$11+17$</td>
<td style="text-align: center;">$17-11$</td>
</tr>
<tr>
<td style="text-align: center;">Kelly gives away 91 nintendo games. How many did she have initially if she still has 92 games left?</td>
<td style="text-align: center;">$91+92$</td>
<td style="text-align: center;">$92-91$</td>
</tr>
<tr>
<td style="text-align: center;">Emily is making bead necklaces for her friends. She was able to make 18 necklaces and she had 6 beads. How many beads did each necklace need?</td>
<td style="text-align: center;">$18 / 6$</td>
<td style="text-align: center;">$6 / 18$</td>
</tr>
<tr>
<td style="text-align: center;">Frank was reading through some books. Each book had 249 pages and it took frank 3 days to finish each book. How many pages did he read per day?</td>
<td style="text-align: center;">$249 / 3$</td>
<td style="text-align: center;">$(249 * 3) / 3$</td>
</tr>
<tr>
<td style="text-align: center;">A mailman has to give 5 pieces of junk mail to each block. If he gives 25 mails to each house in a block, how many houses are there in a block?</td>
<td style="text-align: center;">$25 / 5$</td>
<td style="text-align: center;">$5 / 25$</td>
</tr>
<tr>
<td style="text-align: center;">Faye was placing her pencils and crayons into 19 rows with 4 pencils and 27 crayons in each row. How many pencils does she have?</td>
<td style="text-align: center;">$19 * 4$</td>
<td style="text-align: center;">$19 * 27$</td>
</tr>
<tr>
<td style="text-align: center;">White t - shirts can be purchased in packages of 53. If mom buys 57 packages of white t - shirts and 34 trousers, How many white t - shirts will she have?</td>
<td style="text-align: center;">$53 * 57$</td>
<td style="text-align: center;">$(53 * 57)+34$</td>
</tr>
<tr>
<td style="text-align: center;">An industrial machine can make 6 shirts a minute. It worked for 5 minutes yesterday and for 12 minutes today. How many shirts did machine make today?</td>
<td style="text-align: center;">$6 * 12$</td>
<td style="text-align: center;">$5+12$</td>
</tr>
</tbody>
</table>
<p>Table 23: Some simple examples from SVAMP on which the best performing Graph2Tree model fails.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Majority Template Baseline is the accuracy when the model always predicts the most frequent Equation Template. Equation Templates are explained in Section 5.2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
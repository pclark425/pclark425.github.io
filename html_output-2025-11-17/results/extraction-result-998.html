<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-998 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-998</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-998</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-57835c5ad5424f94ee75901c3113730f3900e656</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/57835c5ad5424f94ee75901c3113730f3900e656" target="_blank">Representation Learning via Invariant Causal Mechanisms</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> A novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), is proposed that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees.</p>
                <p><strong>Paper Abstract:</strong> Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on $51$ out of $57$ games.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e998.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e998.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RELIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representation Learning via Invariant Causal Mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised representation-learning objective that enforces invariant prediction of proxy targets across data augmentations by adding an explicit invariance regularizer (KL between predictive distributions) to a contrastive/instance-discrimination framework to suppress style-based spurious signals and learn content representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RELIC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>RELIC minimizes a contrastive (instance-discrimination / InfoNCE-like) loss combined with an explicit invariance penalty: a KL divergence (or other distributional distance) between the predictive distributions p^{do(a)}(Y^R | f(X)) produced under different pairs of augmentations a. Concretely, it forms contrastive probabilities p^{do(a_lk)}(Y^R=j | f(x_i)) ∝ exp(φ(f(x_i^{a_l}), h(x_j^{a_k}))/τ) and optimizes -log positive/negatives plus α · Σ_{augmentation-pair pairs} KL(p^{do(a_lk)} || p^{do(a_qt)}). Architecturally it uses encoder f, target/auxiliary encoder h (optionally momentum updated), and critic g; an Euclidean/L2-form variant of the invariance constraint is also analyzed. The method interprets augmentations as simulated interventions on latent style S and enforces that predictions be invariant across such interventions, thereby discouraging representations that exploit style-correlated (non-causal) features. The paper gives theoretical support (Theorem 1: invariance on refinements suffices to generalize; Lemma 1: invariance penalty concentrates within-class distances) and empirical evaluation on ImageNet and Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ImageNet (pretraining) and Atari (RL suite)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>ImageNet: large-scale static image dataset used for unsupervised pretraining; augmentations (random crop, color jitter, blur, grayscale, etc.) are used as simulated interventions on style. Atari: interactive reinforcement-learning suite of 57 games where RELIC is used as an auxiliary self-supervised loss for a second encoder feeding a Q-network; augmentations (random shifts/crops/intensity) are used as content-preserving transformations to simulate style interventions. The Atari experiments are in a genuinely interactive environment (agents act to gather data) but RELIC itself does not perform active experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Explicit invariance regularizer across augmentation-induced 'environments' (KL divergence between predictive distributions from different augmentations) that enforces that predictions of the proxy task do not change with style interventions; augmentations are treated as simulated interventions to reveal which features are style (distractors) vs content; additionally an L2/EUCLIDEAN form of the penalty shrinks within-class variance to reduce influence of augmentation-varying (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Style / non-causal correlates (background, lighting, camera properties, texture, local image statistics) and generally irrelevant variables correlated with downstream tasks; augmentation-induced distribution shifts representing non-causal variation.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit: by comparing predictive distributions p^{do(a)}(Y^R | f(X)) across different augmentations using KL divergence; large discrepancies indicate dependence on augmentation-varying (spurious) features. There is no separate explicit detector beyond measuring distributional changes across augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalize differences in predictive distributions across augmentations (KL penalty) during training so the model learns representations whose predictive signals are invariant (thus downweighting features that vary with augmentations); alternative L2 constraint on embeddings enforces contraction of augmentation-induced variability.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Operational refutation is done by enforcing invariance across augmentation 'environments' — if the conditional predictive distribution no longer changes across augmentations, reliance on augmentation-varying (spurious) cues is effectively ruled out. The paper provides theoretical support (Theorem 1) that invariance for a refinement refutes spurious style dependence for downstream tasks, but there is no separate interventional refutation test beyond the augmentation-based invariance criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ImageNet linear eval: ReLIC (ResNet-50, SimCLR augmentations) top-1 ≈ 70.3%; ReLIC with target-network top-1 ≈ 74.8%. Robustness on ImageNet-C: ReLIC mCE 76.4 (ResNet-50) vs SimCLR 87.5 and supervised 76.7; with target network ReLIC mCE 70.8 vs BYOL 72.3. Out-of-distribution ImageNet-R top-1 error: ReLIC 77.4% (compared to SimCLR 81.7 and BYOL 77.0). Atari (57 games) human-normalized capped mean: ReLIC 91.46 (SimCLR 88.76, CURL 90.72, BYOL 89.43); number of superhuman games: ReLIC 51/57 (SimCLR/BYOL 49). These results show improved robustness and OOD performance when the invariance penalty is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Contrastive baseline (no explicit invariance penalty): SimCLR top-1 on ImageNet ≈ 69.3% (ResNet-50), SimCLR mCE 87.5; BYOL (non-contrastive baseline) top-1 ≈ 74.3% and mCE 72.3; Atari capped mean for SimCLR 88.76. The paper reports that α=0 (no invariance penalty) does not guarantee generalization, motivating the added invariance term.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Enforcing invariance across augmentation-induced environments explicitly (via KL penalty or L2 contraction) reduces the representation's reliance on style/spurious features, concentrates within-class representations, and yields significantly better robustness (ImageNet-C mCE reduction relative to SimCLR), improved OOD generalization (ImageNet-R), and improved RL performance on Atari. Theoretically, invariance learned on fine-grained refinements (instance discrimination) suffices to generalize to downstream tasks (Theorem 1); contrastive loss alone is insufficient without an invariance constraint. Empirically, RELIC outperforms competing unsupervised methods on robustness and in many RL games, indicating practical effectiveness of the distractor-robust design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representation Learning via Invariant Causal Mechanisms', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e998.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e998.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant prediction (Peters et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal inference principle and algorithmic approach that identifies predictors whose conditional distribution of the target is invariant across environments; such stable predictors are candidate causal parents and are robust to interventions/shift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference by using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant prediction (environmental invariance selection)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Search for sets of covariates S such that the conditional law P(Y | S) remains the same across multiple environments; under certain assumptions the smallest such set corresponds to causal parents of Y. Practically implemented via statistical tests comparing residual distributions or regression coefficients across environments and selecting variables that pass invariance tests, often with confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-environment / multi-domain datasets (non-interactive by default)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Requires data partitioned into multiple environments or domains (each representing different interventions or shifts). In the RELIC paper, data augmentations act as simulated environments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection by invariance testing: variables that cause variation in P(Y|S) across environments are considered non-invariant (distractors) and excluded.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Non-causal correlates that change across environments (style or domain-specific signals); selection bias induced non-robust predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical tests for equality of conditional distributions or regression residuals across environments; examine changes in fitted predictors across groups.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Exclude or not select variables failing invariance; enforce models using only invariant predictors; sometimes regularization towards invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If P(Y | S) differs across environments, candidate variables are refuted as causal for Y; invariance provides a refutation criterion for spurious predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The RELIC paper builds on the invariant prediction principle: by treating augmentations as interventions on style, enforcing invariance of predictive distributions across augmentations operationalizes invariant prediction in the unsupervised representation learning setting and helps avoid learning spurious style-based cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representation Learning via Invariant Causal Mechanisms', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e998.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e998.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional variance penalties</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional variance penalties and domain shift robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised-domain-robust learning approach that penalizes conditional variance of predictions within known groups/environments to encourage models to be stable and robust across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conditional variance penalties and domain shift robustness</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Conditional variance penalty (Heinze-Deml & Meinshausen)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Minimize the supervised loss while adding a penalty term that constrains the variance of the model's predictions conditional on group/environment labels, thereby encouraging invariance of predictions within groups and robustness under domain shift. Requires group/environment labels to compute conditional variances.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Grouped / multi-environment supervised datasets</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Settings with labeled groups/environments indicating different domains; not inherently interactive.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Regularization that penalizes within-group conditional variance of predictions, reducing sensitivity to group-specific (spurious) signals.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Domain-specific spurious correlates (style changes across groups), group-level confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Relies on known group structure rather than explicit detection; instability across groups indicates spuriousness.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Penalty reduces influence of features that produce high conditional variance across groups.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The RELIC paper contrasts with this approach: Heinze-Deml & Meinshausen use supervised group labels and conditional-variance penalties, whereas RELIC creates groups via unsupervised augmentations and enforces distributional invariance (KL) without access to labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representation Learning via Invariant Causal Mechanisms', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e998.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e998.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BYOL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap Your Own Latent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised learning method that trains a predictor network to match a momentum (target) network's representation of another view of the same input, without contrastive negatives; its L2-style objective resembles the L2 form of RELIC's invariance penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bootstrap your own latent: A new approach to self-supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BYOL (predictor-target network alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Two networks (online and target) encode two augmented views; a predictor maps the online embedding to target space and is trained to minimize an L2 loss to match the target embedding; target network is an exponential-moving-average of online weights. BYOL achieves strong empirical performance without explicit negative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ImageNet (pretraining) and used as a baseline in Atari experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive ImageNet pretraining (augmentations for views); in Atari used as a self-supervised baseline for RL representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>BYOL ImageNet linear eval top-1 ≈ 74.3% (ResNet-50 with target network); ImageNet-C mCE 72.3 (paper reports), ImageNet-R top-1 error ≈ 77.0%. Atari capped mean ≈ 89.43 (paper's table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to RELIC's invariance-regularized results, BYOL performs well but RELIC with explicit invariance penalty shows improved robustness in some metrics (e.g., ReLIC mCE 70.8 vs BYOL 72.3 with target network).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BYOL's L2 objective is noted as resembling the Euclidean form of RELIC's invariance regularizer; BYOL itself does not explicitly target distractor removal via invariance to augmentations in the same causal framework, yet it performs strongly empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representation Learning via Invariant Causal Mechanisms', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e998.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e998.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Refinements (instance discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Refinements of tasks / Instance discrimination (most fine-grained refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The causal concept of refinements: constructing proxy tasks that are finer-grained partitions of downstream tasks (most extremely: instance discrimination where each example is its own label); RELIC shows that invariance learned on such refinements suffices to generalize to coarser downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Refinement-based proxy tasks / Instance discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Define a proxy task Y^R that refines the downstream tasks' partitions (i.e., finer-grained labels); train a representation f(X) to be an invariant predictor of Y^R across augmentations (simulated interventions). The instance discrimination objective (label each instance uniquely and contrast) is the most fine-grained refinement; Theorem 1 shows that if f is invariant for Y^R under style interventions then it is invariant for all downstream tasks in the family.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ImageNet pretraining; Atari auxiliary objective</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Static-image pretraining and interactive RL settings; augmentations create multiple simulated 'environments' per instance to enforce invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Train on refinements while enforcing invariance across augmentation-induced environments so that representation cannot rely on augmentation-varying (spurious) features; effectively downweights distractors by forcing invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Style variables and other non-causal correlates that vary under augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Same KL/L2 invariance penalties as RELIC applied while training on refinement proxy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Theorem 1 provides a formal argument: invariance on a refinement implies invariance for downstream tasks, which operationally refutes reliance on refinement-varying (spurious) features for the downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When combined with RELIC invariance penalty, instance-discrimination refinements yield the reported robust metrics (ImageNet top-1 ≈ 70.3 / 74.8 with target network; ImageNet-C and ImageNet-R improvements; Atari improvements) since RELIC's implementation uses instance discrimination as proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Instance discrimination / contrastive objectives without the explicit invariance penalty correspond to methods like SimCLR; SimCLR shows lower robustness (ImageNet-C mCE 87.5) and slightly lower ImageNet linear-eval than ReLIC.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using refinements (in particular instance discrimination) as proxy tasks is theoretically justified: enforcing invariance on a refinement suffices to obtain representations that generalize to a family of downstream tasks (Theorem 1). However, contrastive/instance-discrimination alone (without enforcing invariance across augmentations) does not guarantee generalization to downstream tasks under weak assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representation Learning via Invariant Causal Mechanisms', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e998.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e998.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmentations-as-interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Content-preserving data augmentations used as simulated interventions on style</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Treat data augmentations (cropping, color jitter, blur, etc.) as do-operations on latent style variables S, using them to generate multiple 'environments' for the same content and enforcing that predictions are invariant across those simulated interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Augmentation-as-intervention</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Model augmentations a ∈ A as interventions do(S = s_a) that change style but preserve content; compute predictive distributions p^{do(a)}(Y^R | f(X)) under different augmentations and enforce invariance via KL penalties or embedding-contraction constraints so that the learned representation focuses on content rather than augmentation-changing style features.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ImageNet (static) and Atari (interactive data collection with augmentations applied to observations)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Augmentations create multiple synthetic environments per datapoint; these are not active experiments (augmentations chosen by designer) but are used throughout training to approximate interventions on style variables.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Reveals and removes reliance on augmentation-varying features by measuring distributional differences across augmentations and penalizing them (KL), thus downweighting spurious signals.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Style-related nuisances (background, lighting, texture, camera distortions) and any features altered by augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compare p^{do(a)} predictive distributions across different augmentations (KL divergence) — non-invariance signals the presence of augmentation-varying (spurious) features.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Add invariance penalty (KL or L2 constraint) to training objective so model learns to ignore augmentation-varying features.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>If the predictive distribution becomes invariant across augmentations, dependence on augmentation-varying features is effectively ruled out; theoretical support provided by invariance arguments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>As part of RELIC, using augmentations as interventions plus invariance penalty yields improved robustness (see RELIC metrics: ImageNet-C, ImageNet-R, Atari improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Treating augmentations as simulated interventions is central to RELIC's approach: it lets unsupervised data provide multiple 'environments' enabling invariant prediction constraints that reduce spurious, style-based signals and improve downstream robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Representation Learning via Invariant Causal Mechanisms', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>Conditional variance penalties and domain shift robustness <em>(Rating: 2)</em></li>
                <li>A theoretical analysis of contrastive unsupervised representation learning <em>(Rating: 2)</em></li>
                <li>Visual causal feature learning <em>(Rating: 2)</em></li>
                <li>Bootstrap your own latent: A new approach to self-supervised learning <em>(Rating: 1)</em></li>
                <li>A simple framework for contrastive learning of visual representations <em>(Rating: 1)</em></li>
                <li>Contrastive Unsupervised Representations for Reinforcement Learning (CURL) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-998",
    "paper_id": "paper-57835c5ad5424f94ee75901c3113730f3900e656",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "RELIC",
            "name_full": "Representation Learning via Invariant Causal Mechanisms",
            "brief_description": "A self-supervised representation-learning objective that enforces invariant prediction of proxy targets across data augmentations by adding an explicit invariance regularizer (KL between predictive distributions) to a contrastive/instance-discrimination framework to suppress style-based spurious signals and learn content representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "RELIC",
            "method_description": "RELIC minimizes a contrastive (instance-discrimination / InfoNCE-like) loss combined with an explicit invariance penalty: a KL divergence (or other distributional distance) between the predictive distributions p^{do(a)}(Y^R | f(X)) produced under different pairs of augmentations a. Concretely, it forms contrastive probabilities p^{do(a_lk)}(Y^R=j | f(x_i)) ∝ exp(φ(f(x_i^{a_l}), h(x_j^{a_k}))/τ) and optimizes -log positive/negatives plus α · Σ_{augmentation-pair pairs} KL(p^{do(a_lk)} || p^{do(a_qt)}). Architecturally it uses encoder f, target/auxiliary encoder h (optionally momentum updated), and critic g; an Euclidean/L2-form variant of the invariance constraint is also analyzed. The method interprets augmentations as simulated interventions on latent style S and enforces that predictions be invariant across such interventions, thereby discouraging representations that exploit style-correlated (non-causal) features. The paper gives theoretical support (Theorem 1: invariance on refinements suffices to generalize; Lemma 1: invariance penalty concentrates within-class distances) and empirical evaluation on ImageNet and Atari.",
            "environment_name": "ImageNet (pretraining) and Atari (RL suite)",
            "environment_description": "ImageNet: large-scale static image dataset used for unsupervised pretraining; augmentations (random crop, color jitter, blur, grayscale, etc.) are used as simulated interventions on style. Atari: interactive reinforcement-learning suite of 57 games where RELIC is used as an auxiliary self-supervised loss for a second encoder feeding a Q-network; augmentations (random shifts/crops/intensity) are used as content-preserving transformations to simulate style interventions. The Atari experiments are in a genuinely interactive environment (agents act to gather data) but RELIC itself does not perform active experimental design.",
            "handles_distractors": true,
            "distractor_handling_technique": "Explicit invariance regularizer across augmentation-induced 'environments' (KL divergence between predictive distributions from different augmentations) that enforces that predictions of the proxy task do not change with style interventions; augmentations are treated as simulated interventions to reveal which features are style (distractors) vs content; additionally an L2/EUCLIDEAN form of the penalty shrinks within-class variance to reduce influence of augmentation-varying (spurious) features.",
            "spurious_signal_types": "Style / non-causal correlates (background, lighting, camera properties, texture, local image statistics) and generally irrelevant variables correlated with downstream tasks; augmentation-induced distribution shifts representing non-causal variation.",
            "detection_method": "Implicit: by comparing predictive distributions p^{do(a)}(Y^R | f(X)) across different augmentations using KL divergence; large discrepancies indicate dependence on augmentation-varying (spurious) features. There is no separate explicit detector beyond measuring distributional changes across augmentations.",
            "downweighting_method": "Penalize differences in predictive distributions across augmentations (KL penalty) during training so the model learns representations whose predictive signals are invariant (thus downweighting features that vary with augmentations); alternative L2 constraint on embeddings enforces contraction of augmentation-induced variability.",
            "refutation_method": "Operational refutation is done by enforcing invariance across augmentation 'environments' — if the conditional predictive distribution no longer changes across augmentations, reliance on augmentation-varying (spurious) cues is effectively ruled out. The paper provides theoretical support (Theorem 1) that invariance for a refinement refutes spurious style dependence for downstream tasks, but there is no separate interventional refutation test beyond the augmentation-based invariance criterion.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "ImageNet linear eval: ReLIC (ResNet-50, SimCLR augmentations) top-1 ≈ 70.3%; ReLIC with target-network top-1 ≈ 74.8%. Robustness on ImageNet-C: ReLIC mCE 76.4 (ResNet-50) vs SimCLR 87.5 and supervised 76.7; with target network ReLIC mCE 70.8 vs BYOL 72.3. Out-of-distribution ImageNet-R top-1 error: ReLIC 77.4% (compared to SimCLR 81.7 and BYOL 77.0). Atari (57 games) human-normalized capped mean: ReLIC 91.46 (SimCLR 88.76, CURL 90.72, BYOL 89.43); number of superhuman games: ReLIC 51/57 (SimCLR/BYOL 49). These results show improved robustness and OOD performance when the invariance penalty is applied.",
            "performance_without_robustness": "Contrastive baseline (no explicit invariance penalty): SimCLR top-1 on ImageNet ≈ 69.3% (ResNet-50), SimCLR mCE 87.5; BYOL (non-contrastive baseline) top-1 ≈ 74.3% and mCE 72.3; Atari capped mean for SimCLR 88.76. The paper reports that α=0 (no invariance penalty) does not guarantee generalization, motivating the added invariance term.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Enforcing invariance across augmentation-induced environments explicitly (via KL penalty or L2 contraction) reduces the representation's reliance on style/spurious features, concentrates within-class representations, and yields significantly better robustness (ImageNet-C mCE reduction relative to SimCLR), improved OOD generalization (ImageNet-R), and improved RL performance on Atari. Theoretically, invariance learned on fine-grained refinements (instance discrimination) suffices to generalize to downstream tasks (Theorem 1); contrastive loss alone is insufficient without an invariance constraint. Empirically, RELIC outperforms competing unsupervised methods on robustness and in many RL games, indicating practical effectiveness of the distractor-robust design.",
            "uuid": "e998.0",
            "source_info": {
                "paper_title": "Representation Learning via Invariant Causal Mechanisms",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Invariant prediction",
            "name_full": "Invariant prediction (Peters et al.)",
            "brief_description": "A causal inference principle and algorithmic approach that identifies predictors whose conditional distribution of the target is invariant across environments; such stable predictors are candidate causal parents and are robust to interventions/shift.",
            "citation_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant prediction (environmental invariance selection)",
            "method_description": "Search for sets of covariates S such that the conditional law P(Y | S) remains the same across multiple environments; under certain assumptions the smallest such set corresponds to causal parents of Y. Practically implemented via statistical tests comparing residual distributions or regression coefficients across environments and selecting variables that pass invariance tests, often with confidence intervals.",
            "environment_name": "Multi-environment / multi-domain datasets (non-interactive by default)",
            "environment_description": "Requires data partitioned into multiple environments or domains (each representing different interventions or shifts). In the RELIC paper, data augmentations act as simulated environments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection by invariance testing: variables that cause variation in P(Y|S) across environments are considered non-invariant (distractors) and excluded.",
            "spurious_signal_types": "Non-causal correlates that change across environments (style or domain-specific signals); selection bias induced non-robust predictors.",
            "detection_method": "Statistical tests for equality of conditional distributions or regression residuals across environments; examine changes in fitted predictors across groups.",
            "downweighting_method": "Exclude or not select variables failing invariance; enforce models using only invariant predictors; sometimes regularization towards invariance.",
            "refutation_method": "If P(Y | S) differs across environments, candidate variables are refuted as causal for Y; invariance provides a refutation criterion for spurious predictors.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The RELIC paper builds on the invariant prediction principle: by treating augmentations as interventions on style, enforcing invariance of predictive distributions across augmentations operationalizes invariant prediction in the unsupervised representation learning setting and helps avoid learning spurious style-based cues.",
            "uuid": "e998.1",
            "source_info": {
                "paper_title": "Representation Learning via Invariant Causal Mechanisms",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Conditional variance penalties",
            "name_full": "Conditional variance penalties and domain shift robustness",
            "brief_description": "A supervised-domain-robust learning approach that penalizes conditional variance of predictions within known groups/environments to encourage models to be stable and robust across domains.",
            "citation_title": "Conditional variance penalties and domain shift robustness",
            "mention_or_use": "mention",
            "method_name": "Conditional variance penalty (Heinze-Deml & Meinshausen)",
            "method_description": "Minimize the supervised loss while adding a penalty term that constrains the variance of the model's predictions conditional on group/environment labels, thereby encouraging invariance of predictions within groups and robustness under domain shift. Requires group/environment labels to compute conditional variances.",
            "environment_name": "Grouped / multi-environment supervised datasets",
            "environment_description": "Settings with labeled groups/environments indicating different domains; not inherently interactive.",
            "handles_distractors": true,
            "distractor_handling_technique": "Regularization that penalizes within-group conditional variance of predictions, reducing sensitivity to group-specific (spurious) signals.",
            "spurious_signal_types": "Domain-specific spurious correlates (style changes across groups), group-level confounders.",
            "detection_method": "Relies on known group structure rather than explicit detection; instability across groups indicates spuriousness.",
            "downweighting_method": "Penalty reduces influence of features that produce high conditional variance across groups.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "The RELIC paper contrasts with this approach: Heinze-Deml & Meinshausen use supervised group labels and conditional-variance penalties, whereas RELIC creates groups via unsupervised augmentations and enforces distributional invariance (KL) without access to labels.",
            "uuid": "e998.2",
            "source_info": {
                "paper_title": "Representation Learning via Invariant Causal Mechanisms",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "BYOL",
            "name_full": "Bootstrap Your Own Latent",
            "brief_description": "A self-supervised learning method that trains a predictor network to match a momentum (target) network's representation of another view of the same input, without contrastive negatives; its L2-style objective resembles the L2 form of RELIC's invariance penalty.",
            "citation_title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "mention_or_use": "mention",
            "method_name": "BYOL (predictor-target network alignment)",
            "method_description": "Two networks (online and target) encode two augmented views; a predictor maps the online embedding to target space and is trained to minimize an L2 loss to match the target embedding; target network is an exponential-moving-average of online weights. BYOL achieves strong empirical performance without explicit negative examples.",
            "environment_name": "ImageNet (pretraining) and used as a baseline in Atari experiments",
            "environment_description": "Non-interactive ImageNet pretraining (augmentations for views); in Atari used as a self-supervised baseline for RL representation learning.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "BYOL ImageNet linear eval top-1 ≈ 74.3% (ResNet-50 with target network); ImageNet-C mCE 72.3 (paper reports), ImageNet-R top-1 error ≈ 77.0%. Atari capped mean ≈ 89.43 (paper's table).",
            "performance_without_robustness": "Compared to RELIC's invariance-regularized results, BYOL performs well but RELIC with explicit invariance penalty shows improved robustness in some metrics (e.g., ReLIC mCE 70.8 vs BYOL 72.3 with target network).",
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "BYOL's L2 objective is noted as resembling the Euclidean form of RELIC's invariance regularizer; BYOL itself does not explicitly target distractor removal via invariance to augmentations in the same causal framework, yet it performs strongly empirically.",
            "uuid": "e998.3",
            "source_info": {
                "paper_title": "Representation Learning via Invariant Causal Mechanisms",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Refinements (instance discrimination)",
            "name_full": "Refinements of tasks / Instance discrimination (most fine-grained refinement)",
            "brief_description": "The causal concept of refinements: constructing proxy tasks that are finer-grained partitions of downstream tasks (most extremely: instance discrimination where each example is its own label); RELIC shows that invariance learned on such refinements suffices to generalize to coarser downstream tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Refinement-based proxy tasks / Instance discrimination",
            "method_description": "Define a proxy task Y^R that refines the downstream tasks' partitions (i.e., finer-grained labels); train a representation f(X) to be an invariant predictor of Y^R across augmentations (simulated interventions). The instance discrimination objective (label each instance uniquely and contrast) is the most fine-grained refinement; Theorem 1 shows that if f is invariant for Y^R under style interventions then it is invariant for all downstream tasks in the family.",
            "environment_name": "ImageNet pretraining; Atari auxiliary objective",
            "environment_description": "Static-image pretraining and interactive RL settings; augmentations create multiple simulated 'environments' per instance to enforce invariance.",
            "handles_distractors": true,
            "distractor_handling_technique": "Train on refinements while enforcing invariance across augmentation-induced environments so that representation cannot rely on augmentation-varying (spurious) features; effectively downweights distractors by forcing invariance.",
            "spurious_signal_types": "Style variables and other non-causal correlates that vary under augmentations.",
            "detection_method": null,
            "downweighting_method": "Same KL/L2 invariance penalties as RELIC applied while training on refinement proxy tasks.",
            "refutation_method": "Theorem 1 provides a formal argument: invariance on a refinement implies invariance for downstream tasks, which operationally refutes reliance on refinement-varying (spurious) features for the downstream tasks.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "When combined with RELIC invariance penalty, instance-discrimination refinements yield the reported robust metrics (ImageNet top-1 ≈ 70.3 / 74.8 with target network; ImageNet-C and ImageNet-R improvements; Atari improvements) since RELIC's implementation uses instance discrimination as proxy.",
            "performance_without_robustness": "Instance discrimination / contrastive objectives without the explicit invariance penalty correspond to methods like SimCLR; SimCLR shows lower robustness (ImageNet-C mCE 87.5) and slightly lower ImageNet linear-eval than ReLIC.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Using refinements (in particular instance discrimination) as proxy tasks is theoretically justified: enforcing invariance on a refinement suffices to obtain representations that generalize to a family of downstream tasks (Theorem 1). However, contrastive/instance-discrimination alone (without enforcing invariance across augmentations) does not guarantee generalization to downstream tasks under weak assumptions.",
            "uuid": "e998.4",
            "source_info": {
                "paper_title": "Representation Learning via Invariant Causal Mechanisms",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Augmentations-as-interventions",
            "name_full": "Content-preserving data augmentations used as simulated interventions on style",
            "brief_description": "Treat data augmentations (cropping, color jitter, blur, etc.) as do-operations on latent style variables S, using them to generate multiple 'environments' for the same content and enforcing that predictions are invariant across those simulated interventions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Augmentation-as-intervention",
            "method_description": "Model augmentations a ∈ A as interventions do(S = s_a) that change style but preserve content; compute predictive distributions p^{do(a)}(Y^R | f(X)) under different augmentations and enforce invariance via KL penalties or embedding-contraction constraints so that the learned representation focuses on content rather than augmentation-changing style features.",
            "environment_name": "ImageNet (static) and Atari (interactive data collection with augmentations applied to observations)",
            "environment_description": "Augmentations create multiple synthetic environments per datapoint; these are not active experiments (augmentations chosen by designer) but are used throughout training to approximate interventions on style variables.",
            "handles_distractors": true,
            "distractor_handling_technique": "Reveals and removes reliance on augmentation-varying features by measuring distributional differences across augmentations and penalizing them (KL), thus downweighting spurious signals.",
            "spurious_signal_types": "Style-related nuisances (background, lighting, texture, camera distortions) and any features altered by augmentations.",
            "detection_method": "Compare p^{do(a)} predictive distributions across different augmentations (KL divergence) — non-invariance signals the presence of augmentation-varying (spurious) features.",
            "downweighting_method": "Add invariance penalty (KL or L2 constraint) to training objective so model learns to ignore augmentation-varying features.",
            "refutation_method": "If the predictive distribution becomes invariant across augmentations, dependence on augmentation-varying features is effectively ruled out; theoretical support provided by invariance arguments in the paper.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "As part of RELIC, using augmentations as interventions plus invariance penalty yields improved robustness (see RELIC metrics: ImageNet-C, ImageNet-R, Atari improvements).",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Treating augmentations as simulated interventions is central to RELIC's approach: it lets unsupervised data provide multiple 'environments' enabling invariant prediction constraints that reduce spurious, style-based signals and improve downstream robustness.",
            "uuid": "e998.5",
            "source_info": {
                "paper_title": "Representation Learning via Invariant Causal Mechanisms",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "Conditional variance penalties and domain shift robustness",
            "rating": 2
        },
        {
            "paper_title": "A theoretical analysis of contrastive unsupervised representation learning",
            "rating": 2
        },
        {
            "paper_title": "Visual causal feature learning",
            "rating": 2
        },
        {
            "paper_title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "rating": 1
        },
        {
            "paper_title": "A simple framework for contrastive learning of visual representations",
            "rating": 1
        },
        {
            "paper_title": "Contrastive Unsupervised Representations for Reinforcement Learning (CURL)",
            "rating": 1
        }
    ],
    "cost": 0.023451,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>REPRESENTATION LEARNING VIA InVARIANT CAUSAL MECHANISMS</h1>
<p>Jovana Mitrovic Brian McWilliams Jacob Walker Lars Buesing Charles Blundell<br>DeepMind, London, UK<br>{mitrovic, bmcw, jcwalker, lbuesing, cblundell}@google.com</p>
<h4>Abstract</h4>
<p>Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel selfsupervised objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, RELIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games.</p>
<h2>1 INTRODUCTION</h2>
<p>Training deep networks often relies heavily on large amounts of useful supervisory signal, such as labels for supervised learning or rewards for reinforcement learning. These training signals can be costly or otherwise impractical to acquire. On the other hand, unsupervised data is often abundantly available. Therefore, pretraining representations for unknown downstream tasks without the need for labels or extrinsic reward holds great promise for reducing the cost of applying machine learning models. To pretrain representations, self-supervised learning makes use of proxy tasks defined on unsupervised data. Recently, self-supervised methods using contrastive objectives have emerged as one of the most successful strategies for unsupervised representation learning (Oord et al., 2018; Hjelm et al., 2018; Chen et al., 2020a). These methods learn a representation by classifying every datapoint against all others datapoints (negative examples). Under assumptions on how the negative examples are sampled, minimizing the resulting contrastive loss has been justified as maximizing a lower bound on the mutual information (MI) between representations (Poole et al., 2019). However, (Tschannen et al., 2019) has shown that performance on downstream tasks may be more tightly correlated with the choice of encoder architecture than the achieved MI bound, highlighting issues with the MI theory of contrastive learning. Further, contrastive approaches compare different views of the data (usually under different data augmentations) to calculate similarity scores. This approach to computing scores has been empirically observed as a key success factor of contrastive methods, but has yet to be theoretically justified. This lack of a solid theoretical explanation for the effectiveness of contrastive methods hinders their further development.</p>
<p>To remedy the theoretical shortcomings, we analyze the problem of self-supervised representation learning through a causal lens. We formalize intuitions about the data generating process using a causal graph and leverage causal tools to derive properties of the optimal representation. We show that a representation should be an invariant predictor of proxy targets under interventions on features that are only correlated, but not causally related to the downstream targets of interest.</p>
<p>Since neither causally nor purely correlationally related features are observed and thus performing actual interventions on them is not feasible, for learning representation with this property we use data augmentations to simulate a subset of possible interventions. Based on our causal interpretation, we propose a regularizer which enforces that the prediction of the proxy targets is invariant across data augmentations. We propose a novel objective for self-supervised representation learning called REpresentation Learning with Invariant Causal mechanisms (RELIC). We show how this explicit invariance regularization leverages augmentations more effectively than previous self-supervised methods and that representations learned using RELIC are guaranteed to generalize well to downstream tasks under weaker assumptions than those required by previous work (Saunshi et al., 2019).</p>
<p>Next we generalize contrastive learning and provide an alternative theoretical explanation to MI for the success of these methods. We generalize the proxy task of instance discrimination commonly used in contrastive learning using the causal concept of refinements (Chalupka et al., 2014). Intuitively, a refinement of a task can be understood as a more fine-grained variant of the original problem. For example, a refinement for classifying cats against dogs would be the task of classifying individual cat and dog breeds. The instance discrimination task results from the most fine-grained refinement, e.g. discriminating individual cats and dogs from one another. We show that using refinements as proxy tasks enables us to learn useful representations for downstream tasks. Specifically, using causal tools, we show that learning a representation on refinements such that it is an invariant predictor of proxy targets across augmentations is a sufficient condition for these representations to generalize to downstream tasks (cf. Theorem 1). In summary, we provide theoretical support both for the general form of the contrastive objective as well as for the use of data augmentations. Thus, we provide an alternative explanation to mutual information for the success of recent contrastive approaches namely that of causal refinements of downstream tasks.</p>
<p>We test RELIC on a variety of prediction and reinforcement learning problems. First, we evaluate the quality of representations pretrained on ImageNet with a special focus on robustness and out-ofdistribution generalization. RELIC performs competitively with current state-of-the-art methods on ImageNet, while significantly outperforming competing methods on robustness and out-of-distribution generalization of the learned representations when tested on corrupted ImageNet (ImageNet-C (Hendrycks \&amp; Dietterich, 2019)) and a version of ImageNet that consist of different renditions of the same classes (ImageNet-R (Hendrycks et al., 2020)). In terms of robustness, RELIC also significantly outperforms the supervised baseline with an absolute reduction of $4.9 \%$ in error. Unlike much prior work that specifically focuses on computer vision tasks, we test RELIC for representation learning in the context of reinforcement learning on the Atari suite (Bellemare et al., 2013). There we find that RELIC significantly outperforms competing methods and achieves above human-level performance on 51 out of 57 games.</p>
<h1>Contributions.</h1>
<ul>
<li>We formalize problem of self-supervised representation learning using causality and propose to more effectively leverage data augmentations through invariant prediction.</li>
<li>We propose a new self-supervised objective, REpresentation Learning with Invariance Causal mechanisms (RELIC), that enforces invariant prediction through an explicit regularizer and show improved generalization guarantees.</li>
<li>We generalize contrastive learning using refinements and show that learning on refinements is a sufficient condition for learning useful representations; this provides an alternative explanation to MI for the success of contrastive methods.</li>
</ul>
<h2>2 Representation Learning via Invariant Causal Mechanisms</h2>
<p>Problem setting. Let $X$ denote the unlabelled observed data and $\mathcal{Y}=\left{Y_{t}\right}<em t="t">{t=1}^{T}$ be a set of unknown tasks with $Y</em>$.}$ denoting the targets for task $t$. The tasks $\left{Y_{t}\right}_{t=1}^{T}$ can represent both a multi-environment as well as a multi-task setup. Our goal is to pretrain with unsupervised data a representation $f(X)$ that will be useful for solving the downstream tasks $\mathcal{Y</p>
<p>Causal interpretation. To effectively leverage common assumptions and intuitions about data generation of the unknown downstream tasks for the learning algorithm, we propose to formalize</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Causal graph formalizing assumptions about content and style of the data and the relationship between targets and proxy tasks. (b) RELIC objective. KL refers to the Kullback-Leibler divergence, while x-entropy denotes cross entropy.
them using a causal graph. We start from the following assumptions: a) the data is generated from content and style variables, with b) only content (and not style) being relevant for the unknown downstream tasks and c) content and style are independent, i.e. style changes are content-preserving. For example, when classifying dogs against giraffes from images, different parts of the animals constitute content, while style could be, for example, background, lighting conditions and camera lens characteristics. By assumption, content is a good representation of the data for downstream tasks and we therefore cast the goal of representation learning as estimating content. In the following, we compactly formalize these assumptions with a causal graph ${ }^{1}$, see Figure 1a.</p>
<p>Let $C$ and $S$ be the latent variables describing content and style. In Figure 1a, the directed arrows from $C$ and $S$ to the observed data $X$ (e.g. images) indicate that $X$ is generated based on content and style. The directed arrow from $C$ to the target $Y_{t}$ (e.g. class labels) encodes the assumption that content directly influences the target tasks, while the absence of any directed arrow from $S$ to $Y_{t}$ indicates that style does not. Thus, content $C$ has all the necessary information to predict $Y_{t}$. The absence of any directed path between $C$ and $S$ in Figure 1a encodes the intuition that these variables are independent, i.e. $C \Perp S$.</p>
<p>Using the independence of mechanisms (Peters et al., 2017), we can conclude that under this causal model performing interventions on $S$ does not change the conditional distribution $P\left(Y_{t} \mid C\right)$, i.e. manipulating the value of $S$ does not influence this conditional distribution. Thus, $P\left(Y_{t} \mid C\right)$ is invariant under changes in style $S$. We call $C$ an invariant representation for $Y_{t}$ under $S$, i.e.</p>
<p>$$
p^{d o\left(S=s_{i}\right)}\left(Y_{t} \mid C\right)=p^{d o\left(S=s_{j}\right)}\left(Y_{t} \mid C\right) \quad \forall s_{i}, s_{j} \in \mathcal{S}
$$</p>
<p>where $p^{d o(S=s)}$ denotes the distribution arising from assigning $S$ the value $s$ with $\mathcal{S}$ the domain of $S$ (Pearl, 2009). Specifically, using $C$ as a representation allows for us to predict targets stably across perturbations, i.e. content $C$ is both a useful and robust representation for tasks $\mathcal{Y}$.
Since the targets $Y_{t}$ are unknown, we will construct a proxy task $Y^{R}$ in order to learn representations from unlabeled data $X$ only. In order to learn useful representations for $Y_{t}$, we will construct proxy tasks that represents more fine-grained problems that $Y_{t}$; for a more formal treatment of proxy tasks please refer to Section 3. Further, to learn invariant representations, such as $C$, we enforce Equation 1 which requires us to observe data under different style interventions, i.e. we need data that describes the same content under varying style. Since we do not have access to $S$, to simulate style variability we use content-preserving data augmentations (e.g. rotation, grayscaling, translation, cropping for images). Specifically, we utilize data augmentations as interventions on the style variable $S$, i.e. applying data augmentation $a_{i}$ corresponds to intervening on $S$ and setting it to $s_{a_{i}}{ }^{2}$ Although</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>we are not able to generate all possible styles using a fixed set of data augmentations, we will use augmentations that generate large sets of diverse styles as this allows us to learn better representations. Note that the heuristic of estimating similarity based on different views from contrastive learning can be interpreted as an implicit invariance constraint.</p>
<p>RELIC objective. Equation 1 provides a general scheme to estimate content (c.f. Figure 1a). We operationalize this by proposing to learn representations such that prediction of proxy targets from the representation is invariant under data augmentations. The representation $f(X)$ must fulfill the following invariant prediction criteria</p>
<p>$$
\text { (Invariant prediction) } \quad p^{\mathrm{do}\left(a_{i}\right)}\left(Y^{R} \mid f(X)\right)=p^{\mathrm{do}\left(a_{j}\right)}\left(Y^{R} \mid f(X)\right) \quad \forall a_{i}, a_{j} \in \mathcal{A}
$$</p>
<p>$\mathcal{A}=\left{a_{1}, \ldots, a_{m}\right}$ is the set of data augmentations which simulate interventions on the style variables and $p^{d o(a)}$ denotes $p^{d o\left(S=s_{a}\right)}$.
To achieve invariant prediction, we propose to explicitly enforce invariance under augmentations through a regularizer. This gives rise to an objective for self-supervised learning we call Representation Learning via Invariant Causal Mechanisms (RELIC). We write this objective as</p>
<p>$$
\underset{\sim \mathcal{A} \times \mathcal{A}}{\mathbb{E}} \sum_{b \in\left{a_{l k}, a_{q t}\right}} \mathcal{L}<em k="k" l="l">{b}\left(Y^{R}, f(X)\right) \quad \text { s.t. } \quad K L\left(p^{d o\left(a</em> \mid f(X)\right)\right) \leq \rho
$$}\right)}\left(Y^{R} \mid f(X)\right), p^{d o\left(a_{q t}\right)}\left(Y^{R</p>
<p>where $\mathcal{L}$ is the proxy task loss and $K L$ is the Kullback-Leibler (KL) divergence. Note that any distance measure on distributions can be used in place of the KL divergence. We explain the remaining terms in detail below.
Concretely, as proxy task we associate to every datapoint $x_{i}$ the label $y_{i}^{R}=i$. This corresponds to the instance discrimination task, commonly used in contrastive learning (Hadsell et al., 2006). We take pairs of points $\left(x_{i}, x_{j}\right)$ to compute similarity scores and use pairs of augmentations $a_{l k}=\left(a_{l}, a_{k}\right) \in$ $\mathcal{A} \times \mathcal{A}$ to perform a style intervention. Given a batch of samples $\left{x_{i}\right}_{i=1}^{N} \sim \mathcal{D}$, we use</p>
<p>$$
p^{d o\left(a_{l k}\right)}\left(Y^{R}=j \mid f\left(x_{i}\right)\right) \propto \exp \left(\phi\left(f\left(x_{i}^{a_{l}}\right), h\left(x_{j}^{a_{k}}\right)\right) / \tau\right)
$$</p>
<p>with $x^{a}$ data augmented with $a$ and $\tau$ a softmax temperature parameter. We encode $f$ using a neural network and choose $h$ to be related to $f$, e.g. $h=f$ or as a network with an exponential moving average of the weights of $f$ (e.g. target networks similar to (Grill et al., 2020)). To compare representations we use the function $\phi\left(f\left(x_{i}\right), h\left(x_{j}\right)\right)=\left\langle g\left(f\left(x_{i}\right)\right), g\left(h\left(x_{j}\right)\right)\right\rangle$ where $g$ is a fullyconnected neural network often called the critic.</p>
<p>Combining these pieces, we learn representations by minimizing the following objective over the full set of data $x_{i} \in \mathcal{D}$ and augmentations $a_{l k} \in \mathcal{A} \times \mathcal{A}$</p>
<p>$$
-\sum_{i=1}^{N} \sum_{a_{l k}} \log \frac{\exp \left(\phi\left(f\left(x_{i}^{a_{l}}\right), h\left(x_{i}^{a_{k}}\right)\right) / \tau\right)}{\sum_{m=1}^{M} \exp \left(\phi\left(f\left(x_{i}^{a_{l}}\right), h\left(x_{m}^{a_{k}}\right)\right) / \tau\right)}+\alpha \sum_{a_{l k}, a_{q t}} K L\left(p^{d o\left(a_{l k}\right)}, p^{d o\left(a_{q t}\right)}\right)
$$</p>
<p>with $M$ the number of points we use to construct the contrast set and $\alpha$ the weighting of the invariance penalty. We used the shorthand $p^{d o(a)}$ for $p^{d o(a)}\left(Y^{R}=j \mid f\left(x_{i}\right)\right)$. With appropriate choices for $\phi, g$, $f$ and $h$ above, Equation 3 recovers many recent state-of-the-art methods (c.f. Table 5 in Section A). Figure 1b presents a schematic of the RELIC objective.</p>
<p>The explicit invariance penalty encourages the within-class distances (for a downstream task of interest) of the representations learned by RELIC to be tightly concentrated. We show this empirically in Figure 2 and theoretically in Appendix B. In the following section we provide theoretical justification for using an instance discrimination-based contrastive loss using a causal perspective. We also show (cf. Theorem 1 below) that minimizing the contrastive loss alone (i.e. $\alpha=0$ ) does not guarantee generalization. Instead, invariance across augmentations must be explicitly enforced.</p>
<h1>3 GENERALIZING CONTRASTIVE LEARNING</h1>
<p>Learning with refinements. In contrastive learning, the task of instance discrimination, i.e. classifying the dataset $\left{\left(x_{i}, y_{i}^{R}=i\right) \mid x_{i} \in \mathcal{D}\right}$, is used as the proxy task. To better understand contrastive</p>
<p>learning and motivate this proxy task, we generalize instance discrimination using the causal concept of refinements (Chalupka et al., 2014). Intuitively, a refinement of one problem is another more fine-grained problem. If task $Y_{t}$ is to classify cats against dogs, then a refinement of $Y_{t}$ is the task of classifying cats and dogs into their individual breeds. See Figure 4 for a further visual example. For any set of tasks, there exist many different refinements. However, the most fine-grained refinement corresponds exactly to classifying the dataset $\left{\left(x_{i}, y_{i}^{R}=i\right) \mid x_{i} \in \mathcal{D}\right}$. Thus, the instance discrimination task used in contrastive learning is a specific type of refinement. For a definition and formal treatment of refinements please refer to Appendix D.</p>
<p>Let $Y^{R}$ be targets of a proxy task that is a refinement for all tasks in $\mathcal{Y}$. Leveraging causal tools, we connect learning on refinements to learning on downstream tasks. Specifically, we provide a theoretical justification for exchanging unknown downstream tasks with these specially constructed proxy tasks. We show that if $f(X)$ is an invariant representation for $Y^{R}$ under changes in style $S$, then $f(X)$ is also an invariant representation for tasks in $\mathcal{Y}$ under changes in style $S$. Thus by enforcing invariance under style interventions on a refinement, we learn representations that generalize to downstream tasks. ${ }^{1}$ This is summarized in the following theorem.
Theorem 1. Let $\mathcal{Y}=\left{Y_{t}\right}_{t=1}^{T}$ be a family of downstream tasks. Let $Y^{R}$ be a refinement for all tasks in $\mathcal{Y}$. If $f(X)$ is an invariant representation for $Y^{R}$ under style interventions $S$, then $f(X)$ is an invariant representation for all tasks in $\mathcal{Y}$ under style interventions $S$, i.e.</p>
<p>$$
p^{\operatorname{do}\left(s_{i}\right)}\left(Y^{R} \mid f(X)\right)=p^{\operatorname{do}\left(s_{j}\right)}\left(Y^{R} \mid f(X)\right) \quad \Rightarrow \quad p^{\operatorname{do}\left(s_{i}\right)}\left(Y_{t} \mid f(X)\right)=p^{\operatorname{do}\left(s_{j}\right)}\left(Y_{t} \mid f(X)\right)
$$</p>
<p>for all $s_{i}, s_{j} \in \mathcal{S}$ with $p^{\operatorname{do}\left(s_{i}\right)}=p^{\operatorname{do}\left(S=s_{i}\right)}$. Thus, $f(X)$ is a representation that generalizes to $\mathcal{Y}$.
Theorem 1 states that if $Y^{R}$ is a refinement of $\mathcal{Y}$ then learning a representation on $Y^{R}$ is a sufficient condition for this representation to be useful on $\mathcal{Y}$. For a formal exposition of these points and accompanying proofs, please refer to Appendix D. Recall that the instance discrimination proxy task is the most fine-grained refinement, and so the left hand side of 4 is satisfied for any downstream task satisfying the stated assumptions of the theorem.</p>
<p>We generalize contrastive learning through refinements and connect representations learned on refinements and downstream tasks in Theorem 1. Thus, using causality we provide an alternative explanation to mutual information for the success of contrastive learning. Note that our methodology of refinements is not limited to instance discrimination tasks and is thus more general than currently used contrastive losses. Real world data often includes rich sources of metadata which can be used to guide the construction of refinements by grouping the data according to any available meta-data. Note that the coarser we can create a refinement, the more data efficient we can expect to be when learning representations for downstream tasks. Further, we can also expect to require less supervised data to finetune the representation.</p>
<h1>4 Related Work</h1>
<p>Contrastive objectives and mutual information maximization. Many recent approaches to selfsupervised learning are rooted in the well-established idea of maximizing mutual information (MI), e.g. Contrastive Predictive Coding (CPC) (Oord et al., 2018; Hénaff et al., 2019), Deep InfoMax (DIM) (Hjelm et al., 2018) and Augmented Multiscale DIM (AMDIM) (Bachman et al., 2019). These methods are based on noise contrastive estimation (NCE) (Gutmann \&amp; Hyvärinen, 2010) which, under specific conditions, can be viewed as a bound on MI (Poole et al., 2019). The resulting objective functions are commonly referred to as InfoNCE.</p>
<p>The precise role played by mutual information maximization in self-supervised learning is subject to some debate. (Tschannen et al., 2019) argue that the performance on downstream tasks is not correlated with the achieved bound on MI, but may be more tightly correlated with encoder architecture and capacity. Importantly, InfoNCE objectives require custom architectures to ensure the network does not converge to non-informative solutions thus precluding the use of standard architectures. Recently, several works (He et al., 2019; Chen et al., 2020a) successfully combined</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>contrastive estimation with a standard ResNet-50 architecture. In particular, SimCLR (Chen et al., 2020a) relies on a set of strong augmentations, while (He et al., 2019) uses a memory bank. Inspired by target networks in reinforcement learning, (Grill et al., 2020) proposed BYOL: an algorithm for self-supervised learning which remarkably does not use a contrastive objective. Although theoretical explanation for the good performance of BYOL is presently missing, interestingly the objective, an $\ell_{2}$ distance between two different embeddings of the input data resembles the $\ell_{2}$ form of our regularizer proposed in Equation 5 in Appendix B.</p>
<p>Recently, (Saunshi et al., 2019) proposed a learning theoretic framework to analyze the performance of contrastive objectives. However, without strong assumptions on intra-class concentration they note that contrastive objectives are fundamentally limited in the representations they are able to learn. RELIC explicitly enforces intra-class concentration via the invariance regularizer, ensuring that it generalizes under weaker assumptions. Unlike (Saunshi et al., 2019) which do not discuss augmentations, we incorporate augmentations into our theoretical explanation of contrastive methods.</p>
<p>The reasons for the improvement in performance from AMDIM through to SimCLR and BYOL are not easily explained by either the MI maximization or the learning theoretic viewpoint. Further, it is not clear why relatively minor architectural differences between the methods result in significant differences in performance nor is it obvious how current state-of-the-art can be improved. In contrast to prior art, the performance of RELIC is explained by connections to causal theory. As such it gives a clear path for improving results by devising problem appropriate refinements, interventions and invariance penalties. Furthermore, the use of invariance penalties in RELIC as dictated by causal theory yields significantly more robust representations that generalize better than those learned with SimCLR or BYOL.</p>
<p>Causality and invariance. Recently, the notion of invariant prediction has emerged as an important operational concept in causal inference (Peters et al., 2016). This idea has been used to learn classifiers which are robust against domain shifts (Gong et al., 2016). Notably, (Heinze-Deml &amp; Meinshausen, 2017) propose to use group structure to delineate between different environments where the aim is to minimize the classification loss while also ensuring that the conditional variance of the prediction function within each group remains small. Unlike (Heinze-Deml &amp; Meinshausen, 2017) who use supervised data and rely on having a grouping in the training data, our approach does not rely on ground-truth targets and can flexibly create groupings of the training data if none are present. Further, we enforce invariant prediction within the group by constraining the distance between distributions resulting from contrasting data across groups.</p>
<h2>5 EXPERIMENTS</h2>
<p>We first visualize the influence of the explicit invariance constraint in RELIC on the linear separability of the learned representations. We then evaluate RELIC on a number of prediction and reinforcement learning tasks for usefulness and robustness. For the prediction tasks, we test RELIC after pretraining the representation in a self-supervised way on the training set of the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015). We evaluate RELIC in the linear evaluation setup on ImageNet and test its robustness and out-of-distribution generalization on datasets related to ImageNet. Unlike much prior work in contrastive learning which focuses specifically on computer vision tasks, we test RELIC also in the context of learning representations for reinforcement learning. Specifically, we test RELIC on the suite of Atari games (Bellemare et al., 2013) which consists of 57 diverse games of varying difficulty.</p>
<p>Linear evaluation. In order to understand how representations learned by RELIC differ from other methods, we compare it against those learned by AMDIM and SimCLR in terms of Fischer’s linear discriminant ratio (Friedman et al., 2009): $F_{\mathrm{LDA}}=\left|\mu_{k}-\mu_{k^{\prime}}\right|^{2}/\sum_{i,j\in\mathcal{C}<em i="i">{k}}\left|f(x</em>$ where})-f(x_{j})\right|^{2</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of the linear discriminant ratio ($F_{\mathrm{LDA}}$, see text) of $f$ for RELIC, SimCLR and AMDIM ($y$-axis clipped to aid visualization).</p>
<p><sup>4</sup>The set of augmentations includes Gaussian blurring, various colour distortions, flips and random cropping.</p>
<p>$\mu_{k}=\frac{1}{\left|\mathcal{C}<em _in="\in" _mathcal_C="\mathcal{C" i="i">{k}\right|} \sum</em><em i="i">{k}} f\left(x</em>}\right)$ is the mean of the representations of class $k$ and $\mathcal{C<em _LDA="{LDA" _text="\text">{k}$ is the index set of that class. A larger $F</em>$ implies that classes are more easily separated with a linear classifier. This can be achieved by either increasing distances between classes (numerator) or shrinking within-class variance (denominator).}</p>
<p>Figure 2 shows the distribution of $F_{\text {LDA }}$ for ReLIC, SimCLR and AMDIM after training as measured on the (downsampled) ImageNet validation set. The distance between medians of ReLIC and SimCLR is 162. AMDIM is tightly concentrated close to 20. The invariance penalty ensures thateven though labels are a-priori unknown-for RELIC within-class variability of $f$ is concentrated leading to better linear separability between classes in the downstream task of interest. This is reflected in the rightward shift of the distribution of $F_{\text {LDA }}$ in Figure 2 for ReLIC compared with SimCLR and AMDIM which do not impose such a constraint.</p>
<p>Next we evaluate ReLIC's representation by training a linear classifier of top of the fixed encoder following the procedure in (Kolesnikov et al., 2019; Chen et al., 2020a) and Appendix E.4. In Table 1, we report top-1 and top-5 accuracy on the ImageNet test set. Methods denoted with * use SimCLR augmentations (Chen et al., 2020a), while methods denoted $\dagger$ use custom, stronger augmentations. Comparing methods which use SimCLR augmentations, ReLIC outperforms competing approaches on both ResNet-50 and ResNet-50 with target network. For completeness, we report results for SwAV (Caron et al., 2020) and InfoMin (Tian et al., 2020), but note that these methods use stronger augmentations which alone have been shown to boost performance by over $5 \%$. A fair comparison between different objectives can only be achieved under the same architecture and the same set of augmentations.</p>
<p>Table 1: Accuracy (in \%) under linear evaluation on ImageNet for different self-supervised representation learning methods. Methods with * use SimCLR augmentations. Methods with $\dagger$ use custom, stronger augmentations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Top-1</th>
<th style="text-align: right;">Top-5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ResNet-50 architecture</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">PIRL (Misra \&amp; Maaten, 2020)</td>
<td style="text-align: right;">63.6</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">CPC v2 (Hénaff et al., 2019)</td>
<td style="text-align: right;">63.8</td>
<td style="text-align: right;">85.3</td>
</tr>
<tr>
<td style="text-align: left;">CMC (Tian et al., 2019)</td>
<td style="text-align: right;">66.2</td>
<td style="text-align: right;">87.0</td>
</tr>
<tr>
<td style="text-align: left;">SimCLR (Chen et al., 2020a)</td>
<td style="text-align: right;">$*$</td>
<td style="text-align: right;">69.3</td>
</tr>
<tr>
<td style="text-align: left;">SwAV (Caron et al., 2020)</td>
<td style="text-align: right;">$*$</td>
<td style="text-align: right;">70.1</td>
</tr>
<tr>
<td style="text-align: left;">ReLIC (ours)</td>
<td style="text-align: right;">$*$</td>
<td style="text-align: right;">70.3</td>
</tr>
<tr>
<td style="text-align: left;">InfoMin Aug. (Tian et al., 2020)</td>
<td style="text-align: right;">$\dagger$</td>
<td style="text-align: right;">73.0</td>
</tr>
<tr>
<td style="text-align: left;">SwAV (Caron et al., 2020)</td>
<td style="text-align: right;">$\dagger$</td>
<td style="text-align: right;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">ResNet-50 with target network</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">MoCo v2 (Chen et al., 2020b)</td>
<td style="text-align: right;">71.1</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">BYOL (Grill et al., 2020)</td>
<td style="text-align: right;">$*$</td>
<td style="text-align: right;">74.3</td>
</tr>
<tr>
<td style="text-align: left;">ReLIC (ours)</td>
<td style="text-align: right;">$*$</td>
<td style="text-align: right;">74.8</td>
</tr>
</tbody>
</table>
<p>Robustness and generalization. We evaluate robustness and out-of-distribution generalization of ReLIC's representation on datasets Imagenet-C (Hendrycks \&amp; Dietterich, 2019) and ImageNet-R (Hendrycks et al., 2020), respectively. To evaluate ReLIC's representation, we train a linear classifier on top of the frozen representation following the procedure described in (Chen et al., 2020a) and appendix E.5.2. For Imagenet-C we report the mean Corruption Error (mCE) and Corruption Errors for Noise corruptions in Table 3. ReLIC has significantly lower mCE than both the supervised ResNet-50 baseline and the unsupervised methods SimCLR and BYOL. Also, it has the lowest Corruption Error on 14 out of 15 corruptions when compared to SimCLR and BYOL. Thus, we see that ReLIC learns the most robust representation. ReLIC also outperforms SimCLR and BYOL on ImageNet-R showing its superior out-of-distribution generalization ability; see Table 2. For further details and results please consult E.5.</p>
<p>Table 2: Top-1 error rates for different self-supervised representation learning methods on ImageNetR. All models are trained only on clean ImageNet images and $\operatorname{ReLIC}_{T}$ refers to ReLIC using a ResNet-50 with target network as in BYOL (Grill et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Supervised</th>
<th style="text-align: center;">SimCLR</th>
<th style="text-align: center;">ReLIC (ours)</th>
<th style="text-align: center;">BYOL</th>
<th style="text-align: center;">$\operatorname{ReLIC}_{T}$ (ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Top-1 Error (\%)</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">76.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Mean Corruption Error (mCE), mean relative Corruption Error (mrCE) and Corruption Errors for the "Noise" class of corruptions (Gaussian, Shot, Impulse) on ImageNet-C. The mCE value is the average across $75$ different corruptions. Methods are trained only on clean ImageNet images.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">mCE</th>
<th style="text-align: center;">mrCE</th>
<th style="text-align: center;">Gaussian</th>
<th style="text-align: center;">Shot</th>
<th style="text-align: center;">Impulse</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Supervised</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">105.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: left;">ResNet-50 architecture:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SimCLR</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">111.9</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">89.6</td>
</tr>
<tr>
<td style="text-align: left;">ReLIC (ours)</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">$\mathbf{8 7 . 7}$</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: left;">ResNet-50 with target network:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">BYOL</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">73.7</td>
</tr>
<tr>
<td style="text-align: left;">ReLIC (ours)</td>
<td style="text-align: center;">$\mathbf{7 0 . 8}$</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">$\mathbf{6 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 2}$</td>
</tr>
</tbody>
</table>
<p>Reinforcement Learning. Much prior work in contrastive learning has focused specifically on computer vision tasks. In order to compare these approaches in a different domain, we investigate representation learning in the context of reinforcement learning. We compare ReLIC as an auxiliary loss against other state of the art self-supervised losses on an agent trained on 57 Atari games. Using human normalized scores as a metric, we use the original architecture and hyperparameters of the R2D2 agent (Kapturowski et al., 2019) and supplement it with a second encoder trained with a given representation learning loss. When auxiliary losses are present, the Q-Network takes the output of the second encoder as an input. The Q-Network and the encoder are trained with separate optimizers. For the augmentation baseline, the Q-Network takes two identical encoders trained end-to-end. Table 4 shows a comparison between ReLIC, SimCLR, BYOL, CURL (Srinivas et al., 2020), and feeding augmented observations directly to the agent (Kostrikov et al., 2020). We find that ReLIC has a significant advantage over competing self-supervised methods, performing best in 25 out of 57 games. The next best performing method, CURL performs best in 11 games. Full details are presented in Section E.6.</p>
<p>Table 4: Human Normalized Scores over 57 Atari Games.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Atari Performance</th>
<th style="text-align: center;">ReLIC</th>
<th style="text-align: center;">SimCLR</th>
<th style="text-align: center;">CURL</th>
<th style="text-align: center;">BYOL</th>
<th style="text-align: center;">Augmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Capped mean</td>
<td style="text-align: center;">$\mathbf{9 1 . 4 6}$</td>
<td style="text-align: center;">88.76</td>
<td style="text-align: center;">90.72</td>
<td style="text-align: center;">89.43</td>
<td style="text-align: center;">80.60</td>
</tr>
<tr>
<td style="text-align: center;">Number of superhuman games</td>
<td style="text-align: center;">$\mathbf{5 1}$</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">34</td>
</tr>
<tr>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">$\mathbf{3 0 0 3 . 7 3}$</td>
<td style="text-align: center;">2086.16</td>
<td style="text-align: center;">2413.12</td>
<td style="text-align: center;">1769.43</td>
<td style="text-align: center;">503.15</td>
</tr>
<tr>
<td style="text-align: center;">Median</td>
<td style="text-align: center;">$\mathbf{8 3 2 . 5 0}$</td>
<td style="text-align: center;">592.83</td>
<td style="text-align: center;">819.56</td>
<td style="text-align: center;">483.39</td>
<td style="text-align: center;">132.17</td>
</tr>
<tr>
<td style="text-align: center;">40\% Percentile</td>
<td style="text-align: center;">356.27</td>
<td style="text-align: center;">266.07</td>
<td style="text-align: center;">$\mathbf{4 0 9 . 4 6}$</td>
<td style="text-align: center;">224.80</td>
<td style="text-align: center;">94.35</td>
</tr>
<tr>
<td style="text-align: center;">30\% Percentile</td>
<td style="text-align: center;">$\mathbf{2 0 2 . 4 9}$</td>
<td style="text-align: center;">174.19</td>
<td style="text-align: center;">190.96</td>
<td style="text-align: center;">150.21</td>
<td style="text-align: center;">80.04</td>
</tr>
<tr>
<td style="text-align: center;">20\% Percentile</td>
<td style="text-align: center;">$\mathbf{1 3 3 . 9 3}$</td>
<td style="text-align: center;">120.84</td>
<td style="text-align: center;">126.10</td>
<td style="text-align: center;">118.36</td>
<td style="text-align: center;">57.95</td>
</tr>
<tr>
<td style="text-align: center;">10\% Percentile</td>
<td style="text-align: center;">$\mathbf{8 3 . 7 9}$</td>
<td style="text-align: center;">37.19</td>
<td style="text-align: center;">59.09</td>
<td style="text-align: center;">44.14</td>
<td style="text-align: center;">32.74</td>
</tr>
<tr>
<td style="text-align: center;">5\% Percentile</td>
<td style="text-align: center;">$\mathbf{2 0 . 8 7}$</td>
<td style="text-align: center;">12.74</td>
<td style="text-align: center;">20.56</td>
<td style="text-align: center;">7.75</td>
<td style="text-align: center;">2.85</td>
</tr>
</tbody>
</table>
<h1>6 CONCLUSION</h1>
<p>In this work we have analyzed self-supervised learning using a causal framework. Using a causal graph, we have formalized the problem of self-supervised representation learning and derived properties of the optimal representation. We have shown that representations need to be invariant predictors of proxy targets under interventions on features that are only correlated, but not causally related to the downstream tasks. We have leveraged data augmentations to simulate these interventions and have proposed to explicitly enforce this invariance constraint. Based on this, we have proposed a new self-supervised objective, Representation Learning via Invariant Causal Mechanisms (RELIC), that enforces invariant prediction of proxy targets across augmentations using an invariance regularizer. Further, we have generalized contrastive methods using the concept of refinements and have shown that learning a representation on refinements using the principle of invariant prediction is a sufficient condition for these representations to generalize to downstream tasks. With this, we have provided an alternative explanation to mutual information for the success of contrastive methods. Empirically</p>
<p>we have compared ReLIC against recent self-supervised methods on a variety of prediction and reinforcement learning tasks. Specifically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization of the representations it learns on ImageNet. ReLIC also significantly outperforms related self-supervised methods on the Atari suite achieving superhuman performance on 51 out of 57 games. We aim to investigate the construction of more coarse-grained refinements and the empirical evaluation of different kinds of refinements in future work.</p>
<p>Acknowledgements. We thank David Balduzzi, Melanie Rey, Christina Heinze-Deml, Ilja Kuzborskij, Ali Eslami, Jeffrey de Fauw and Josip Djolonga for invaluable discussions.</p>
<h1>REFERENCES</h1>
<p>Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. $15509-15519,2019$.</p>
<p>Marc G. Bellemare, Yavar Naddaf, J. Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents (extended abstract). J. Artif. Intell. Res., 47:253-279, 2013.
M. Caron, I. Misra, J. Mairal, Priya Goyal, P. Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. ArXiv, abs/2006.09882, 2020.</p>
<p>Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning. arXiv preprint arXiv:1412.2309, 2014.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.</p>
<p>Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. ArXiv, abs/2003.04297, 2020b.</p>
<p>Paul Erdős and Alfréd Rényi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci, 5(1):17-60, 1960.</p>
<p>Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 2. Springer series in statistics New York, 2009.</p>
<p>Alan Frieze and Michał Karoński. Introduction to random graphs. Cambridge University Press, 2016.</p>
<p>Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Schölkopf. Domain adaptation with conditional transferable components. In International conference on machine learning, pp. 2839-2848, 2016.</p>
<p>Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.</p>
<p>Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297-304, 2010.</p>
<p>Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), volume 2, pp. 1735-1742. IEEE, 2006.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.</p>
<p>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.</p>
<p>Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain shift robustness. arXiv preprint arXiv:1710.11469, 2017.</p>
<p>Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.</p>
<p>Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019 .</p>
<p>Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.</p>
<p>R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
S. Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ArXiv, abs/1502.03167, 2015.</p>
<p>Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent experience replay in distributed reinforcement learning. Iclr, 2019.
A. Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1920-1929, 2019.</p>
<p>Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017.</p>
<p>Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. $6707-6717,2020$.
V. Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.</p>
<p>Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947-1012, 2016.</p>
<p>Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT press, 2017.</p>
<p>Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pp. 5171-5180, 2019 .</p>
<p>Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Zhiheng Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211-252, 2015.</p>
<p>Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, pp. 5628-5637, 2019.</p>
<p>Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.</p>
<p>Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.</p>
<p>Yonglong Tian, C. Sun, Ben Poole, Dilip Krishnan, C. Schmid, and Phillip Isola. What makes for good views for contrastive learning. ArXiv, abs/2005.10243, 2020.</p>
<p>Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.</p>
<p>Haohan Wang, Songwei Ge, E. Xing, and Zachary Chase Lipton. Learning robust global representations by penalizing local predictive power. ArXiv, abs/1905.13549, 2019.</p>
<p>Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017.</p>
<h1>A RELATIONSHIP BETWEEN RELIC AND OTHER METHODS</h1>
<p>Table 5: The objective in eq. (3) recovers state of the art methods depending on design choices ("-" denotes the identity function and "norml." means $g$ is constrained to have unit norm).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">$\phi$</th>
<th style="text-align: center;">$g$</th>
<th style="text-align: center;">Regl.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CPC (Hénaff et al., 2019)</td>
<td style="text-align: center;">$\langle g, W g\rangle$</td>
<td style="text-align: center;">PixelCNN</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">AMDIM (Bachman et al., 2019)</td>
<td style="text-align: center;">$\langle\cdot, \cdot\rangle$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SimCLR (Chen et al., 2020a)</td>
<td style="text-align: center;">$\langle g, g\rangle$</td>
<td style="text-align: center;">MLP, norml.</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BYOL (Grill et al., 2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$g_{1}, g_{2} 1$ layer MLP, norml.</td>
<td style="text-align: center;">$\left|g_{1}\left(g_{2}\right)-g_{2}\right|^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">RELIC (ours)</td>
<td style="text-align: center;">$\langle g, g\rangle$</td>
<td style="text-align: center;">MLP, norml.</td>
<td style="text-align: center;">Eq. (3)</td>
</tr>
</tbody>
</table>
<h2>B DISTANCE CONCENTRATION AND GENERALIZATION</h2>
<p>Quantifying the generalization performance of representations learned on unlabelled data is a difficult task without imposing assumptions on the underlying structure of the data and the downstream tasks of interest. The results in (Saunshi et al., 2019) assume a latent class structure underlying the data. The similarity of images under each (potentially overlapping) latent class $c$ is measured by a probability distribution $\mathcal{D}<em c="c">{c}$. In the contrastive setting a positive pair of points $\left{x, x^{+}\right}$is said to be sampled from a distribution $\mathbb{E}</em>} \mathcal{D<em c="c">{c}(x) \mathcal{D}</em>$ take the roles of the pairs of positive and negative points, respectively.}\left(x^{+}\right)$and a negative example $x^{-}$is sampled from the marginal distribution. The task of interest is multi-class classification using the learned representation. In our setting the augmented data points $\left{x_{i}^{a_{i}}, x_{i}^{a_{k}}\right}$ and $\left{x_{i}^{a_{l}}, x_{m}^{a_{k}}\right}_{m=1}^{M</p>
<p>In this section, under the same structural assumptions on the data as (Saunshi et al., 2019) we will show that a similar result holds but under weaker assumptions on the function, $f$.</p>
<p>To intuit the following results, we can view our explicit invariance constraint through the lens of distance concentration. Its effect can be seen intuitively in Figure 3. The shaded region represents the set of augmentations, $\mathcal{A}$ around an image. Depicted are two images $x_{i}$ and $x_{j}$ from the ImageNet class Stingray. The points $x_{i}^{a_{i}}$ and $x_{j}^{a_{k}}$ are augmentations which correspond to a region of overlap between the augmentation sets of $x_{i}$ and $x_{j}$. If the augmentations $f\left(x_{i}^{a_{i}}\right)$ and $f\left(x_{j}^{a_{k}}\right)$ are similar enough, encouraging $f\left(x_{i}\right)$ to be close to $f\left(x_{i}^{a_{l}}\right)$ and similarly for $f\left(x_{j}\right)$ and $f\left(x_{j}^{a_{k}}\right)$ indirectly encourages $f\left(x_{i}\right)$ to be close to $f\left(x_{j}\right)$. This has the effect of concentrating distances between similar images. We will make this intuition more formal in the following discussion.</p>
<p>Consider a modified, Euclidean distance regularized version of our objective</p>
<p>$$
\begin{aligned}
\hat{f} &amp; \in \underset{f \in \mathcal{F}}{\arg \min } \sum_{i=1}^{N} \sum_{a_{i k}} \ell\left(\left{f\left(x_{i}^{a_{i}}\right)^{\top}\left(f\left(x_{i}^{a_{k}}\right)-f\left(x_{m}^{a_{k}}\right)\right)\right}<em i="i">{m=1}^{M}\right) \
&amp; \text { s.t. } \quad\left|f\left(x</em> \leq \rho
\end{aligned}
$$}\right)-f\left(x_{i}^{a k}\right)\right|^{2</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visual representation of invariance penalty. Shaded region denotes set of augmentations around an image.
where $f \in \mathcal{F}=\left{f: \mathcal{X} \mapsto \mathbb{R}^{d}\right.$ s.t. $\left.|f|<em m="m">{2} \leq T\right}$ with $T \geq 0$. Here $\ell(v)=\log \left(1+\sum</em>\right)\right)$ is the logistic loss. For a single negative, this is equivalent to the standard RELIC objective with an identity critic.} \exp \left(v_{m</p>
<p>Assumptions. We require that the following assumptions hold: (A1) $\hat{f}$ is $L$-Lipschitz and minimizes eq. (5) such that the constraint is active and (A2) x is a bounded variable.
Lemma 1 (Concentration). If assumption (A1) holds for $\rho \leq \frac{B}{6 L \kappa}$, and (A2) holds for $x, \hat{f}(x)$ is a sub-Gaussian random variable with parameter $\sigma_{f}^{2} \leq \frac{1}{\kappa} \sigma_{x^{\prime}}^{2}$</p>
<p>See Appendix C for proof. This result states that the Euclidean version of our invariance regularizer has the effect of contracting the within-class variance of the data. Figure 2 shows that this holds in practise for the original version of our objective in eq. (3). This guarantees that the following generalization result from (Saunshi et al., 2019) holds. For brevity we state an informal version of the Theorem with details deferred to the original publication.
Theorem 2 (Generalization. Adapted from Lemma B.2. from (Saunshi et al., 2019)). Let $L_{\text {sup }}^{\mu}(f)$ be the standard $(K+1)$-wise hinge loss of the linear classification function $W^{\mu} f$ whose $c^{t h}$ column is $\mu_{x}=\frac{1}{\left|\mathcal{C}<em _in="\in" _mathcal_C="\mathcal{C" i="i">{c}\right|} \sum</em><em i="i">{c}} f\left(x</em>$ is the minimizer of eq. (5) and if Assumptions (A1) and (A2) hold then with high probability}\right)$ the mean of representations corresponding to class $c$. Further, let $L_{\gamma(f), \text { sup }}^{\mu}(f)$ use the the hinge loss with margin $\gamma(f)=1+c^{\prime} M \sigma_{f}\left(\sqrt{k}+\sqrt{\log \frac{M}{\epsilon}}\right)$ with $c^{\prime}$ constant and $M=\max _{x}|f(x)|$. If $\hat{f</p>
<p>$$
L_{\text {sup }}^{\mu}(\hat{f}) \leq \gamma(f) L_{\gamma(f), \text { sup }}^{\mu}(f)+\text { Gen }_{N}+\epsilon
$$</p>
<p>Here, $\mathrm{Gen}<em f="f">{N}$ is a standard generalization bound which depends on the Rademacher complexity of the function class $\mathcal{F}$ and the sample size, $N$.
For all practical purposes, the final generalization result is identical to (Saunshi et al., 2019) stating that $\hat{f}$-which is learned by minimizing a contrastive objective on unlabelled data-performs well on labelled data. However, this crucially depends on the intraclass concentration of the representation, that $f(x)$ is sub-Gaussian with parameter $\sigma</em>$. Whereas in (Saunshi et al., 2019) this was assumed to hold, our Lemma 1 shows that the necessary concentration is ensured by our invariance penalty. Experimentally we see this property holds in practise (figure 2).}^{2</p>
<h1>C ADDITIONAL RESULTS</h1>
<p>Proof of Lemma 1. Assume the data $x$ is $\sigma_{x}^{2}$-sub-Gaussian. In practise this holds since $x$ is bounded. It immediately follows that $L$-Lipschitz function $f(x)$ sub-Gaussian with parameter at most $L$. Now we will characterize the reduction in variance from $x$ to $f$. Assume there is a ball of radius $B$ around each point such that for any augmentation $x_{i}^{s}$ of $x_{i}\left|x_{i}-x_{i}^{s}\right|<em i="i">{2}^{2} \leq B$. By assumption (A1) we have that $\left|f\left(x</em>\right)\right|}\right)-f\left(x_{i}^{s<em i="i">{2}^{2} \leq \rho$. This implies that for points $x</em>\right|}$ and $x_{j}$ such that $\left|x_{i}-x_{j<em i="i">{2}^{2} \leq 2 B$, there exists a region of overlap so that $\left|f\left(x</em>\right)\right|}\right)-f\left(x_{j<em i="i">{2}^{2} \leq\left|f\left(x</em>\right)\right|}\right)-f\left(x_{i}^{s<em i="i">{2}^{2}+\left|f\left(x</em>\right)\right|}^{s}\right)-f\left(x_{j<em i="i">{2}^{2} \leq 2 \rho$.
In practise this says that there are augmentations of $x</em>\right)$ to be closer.
The variance of points in $f$ space is}$ which are sufficiently similar to augmentations of $x_{j}$ so that their representations should be similar, thereby driving $f\left(x_{i}\right)$ and $f\left(x_{j</p>
<p>$$
\sigma_{f}^{2}=\frac{1}{2 N^{2}} \sum_{i} \sum_{j}\left|f\left(x_{i}\right)-f\left(x_{j}\right)\right|_{2}^{2}
$$</p>
<p>The overlap $B&lt;\left|x_{i}-x_{j}\right|<em i="i">{2}^{2} \leq 2 B$ induces a graph where we say $j \in \mathcal{N}(i) \forall j$ s.t. $\left|x</em> \leq$ $2 B$. For $N$ samples we can decompose the variance as}-x_{j}\right|_{2}^{2</p>
<p>$$
\begin{aligned}
\sigma_{f}^{2} &amp; =\frac{1}{2 N^{2}} \sum_{i} \sum_{j}\left|f\left(x_{i}\right)-f\left(x_{j}\right)\right|<em i="i">{2}^{2} \
&amp; =\frac{1}{2 N^{2}} \sum</em>\right)\right|} \sum_{j \in \mathcal{N}(i)}\left|f\left(x_{i}\right)-f\left(x_{j<em j_prime="j^{\prime">{2}^{2}+\sum</em>
\end{aligned}
$$} \notin \mathcal{N}(i)}\left|f\left(x_{i}\right)-f\left(x_{j^{\prime}}\right)\right|_{2}^{2</p>
<p>By smoothness of $f$ we always have that have $\left|f\left(x_{i}\right)-f\left(x_{j^{\prime}}\right)\right|<em i="i">{2}^{2} \leq L\left|x</em>\right|}-x_{j^{\prime}<em i="i">{2}^{2}$. By the constraint we have that $\left|f\left(x</em>\right)\right|}\right)-f\left(x_{j<em i="i">{2}^{2} \leq \frac{2 \rho L}{B}\left|x</em>&lt;1$.}-x_{j}\right|_{2}^{2} \forall j \in \mathcal{N}(i)$ and for $\delta=\frac{2 \rho L}{B</p>
<p>Constant proportion overlap. Now, assuming that for each point $i$ there is a constant proportion of the points, $0 \leq \alpha \leq 1$ in the set $\mathcal{N}(i) \forall i$ we can obtain the following inequality</p>
<p>$$
\begin{aligned}
\sigma_{f}^{2} &amp; =\frac{1}{2 N^{2}} \sum_{i} \sum_{j}\left|f\left(x_{i}\right)-f\left(x_{j}\right)\right|<em x="x">{2}^{2} \
&amp; \leq \alpha \delta \sigma</em> \
&amp; =(\alpha \delta+(1-\alpha) L) \sigma_{x}^{2}
\end{aligned}
$$}^{2}+(1-\alpha) L \sigma_{x}^{2</p>
<p>For $\sigma_{f}^{2} \leq \sigma_{x}^{2}$ we require $(\alpha \delta+(1-\alpha) L) \leq 1$. Since both terms are positive we separately require $(1-\alpha) L \leq 1$ :</p>
<p>$$
\begin{aligned}
(1-\alpha) L &amp; &lt;1 \
(1-\alpha) &amp; &lt;\frac{1}{L} \
\alpha &amp; &gt;\left(1-\frac{1}{L}\right)
\end{aligned}
$$</p>
<p>This condition makes sense since the larger $\alpha$, the fewer unconnected components in the graph. If the above holds, we also require $\alpha \frac{2 \rho L}{B}&lt;1-(1-\alpha) L$ to ensure the sum is bounded above by 1 . This implies $\rho&lt;\frac{(1-(1-\alpha) L) B}{2 L \alpha}$.
However, $\alpha$ is a property of the augmentation set and not directly a user-controllable parameter so if $\alpha$ is too small or the function is not smooth enough, it might not be possible to set $\rho$ in such a way to induce contraction in $\sigma_{f}^{2}$.
In the next section we derive a tighter concentration based on the structure of random graphs which are induced by the connectivity between data points and their augmentations.</p>
<p>Random graphs. Consider the graph $G(V, E)$ induced by the constraints $(i, j) \in$ $E \forall\left|x_{i}-x_{j}\right|<em N_="N," _alpha="\alpha">{2}^{2} \leq 2 B$. Call $\mathcal{N}(i)$ the set of neighbours of point $i$. For $N$ points, if there is a constant probability $\alpha$ that $j \in \mathcal{N}(i)$ then $G</em>$ is an Erdös-Renyi graph.
From Theorem 3, if $\alpha \geq \frac{c \log N}{N}$ for $c&gt;1$ then with high probability, there are no unconnected components in $G$. That is, every vertex in V is reachable from any other vertex in a finite number of steps. We can then decompose the contribution to the variance in terms of components in the graph that are adjacent and those which are reachable within a certain number of steps.
Let the degree-the shortest path-between any two points be at most $D$ we obtain the following refinement of eq. (7)</p>
<p>$$
\begin{aligned}
\sigma_{f}^{2} &amp; =\frac{1}{2 N^{2}} \sum_{i} \sum_{j}\left|f\left(x_{i}\right)-f\left(x_{j}\right)\right|<em x="x">{2}^{2} \
&amp; \leq \alpha \delta \sigma</em>
\end{aligned}
$$}^{2}+(1-\alpha) D \delta \sigma_{x}^{2</p>
<p>From Theorem 4 we have with high probability that $3 \leq D \leq 4$. So for $\sigma_{f}^{2} \leq \frac{1}{\kappa} \sigma_{x}^{2}$ with $\kappa \geq 1$ we require $\rho \leq \frac{B}{2 L \kappa(\alpha+3(1-\alpha))} \leq \frac{B}{6 L \kappa}$.</p>
<p>Theorem 3 (Connectedness (Erdős \&amp; Rényi, 1960)). If $p=\frac{c \log n}{n}$ where $c&gt;1$ with high probability then the graph $G(n, p)$ has no unconnected components.
Definition 1 (Diameter). For a connected graph, $G(V, E)$ the diameter $\operatorname{diam}(G)=\max \operatorname{dist}\left(v_{i}, v_{j}\right)$ where $\operatorname{dist}\left(v_{i}, v_{j}\right)$ is the minimum number of edges in the path between $v_{i}$ and $v_{j}$.
Theorem 4 (Diameter of random graphs (Frieze \&amp; Karoński, 2016)). Let $d \geq 2$ be a fixed positive integer. For $c&gt;0$ and</p>
<p>$$
p^{d} n^{d-1}=\log \left(n^{2} / c\right)
$$</p>
<p>Then $\operatorname{diam}\left(G_{n, p}\right) \geq d$ with probability $\exp (-c / 2)$ and $\operatorname{diam}\left(G_{n, p}\right) \leq d+1$ with probability $1-\exp (-c / 2)$.</p>
<h1>D GENERALIZING CONTRASTIVE LEARNING</h1>
<h2>D. 1 REFINEMENTS</h2>
<p>On the unsupervised observed data $\mathcal{D}$, any task as defined by targets $Y_{t}$ induces an equivalence relation, i.e. $Y_{t}$ partitions $\mathcal{D}$ into equivalence classes. It divides $\mathcal{D}$ based on values of the target, $\mathcal{D}=\left{\left{x_{a} \mid y_{a}=y_{i}\right}<em 1="1">{i=1}^{M}\right}$ where $\left{y</em>\right}$ for some $M$ is the set of target values. Here the equivalence relation associates datapoints based on the value of the target they predict. For example,}, \ldots, y_{M</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of a refinement of a set of tasks. The tasks are to classify aquatic vs nonaquatic life and animal vs non-animal with the individual class boundaries denoted by the dashed and dotted black lines. A refinement for these tasks is a to classify aquatic animal vs aquatic non-animal vs non-aquatic animal vs non-aquatic non-animal and the class boundaries are given in teal. The ellipse indicates the set of points induced by augmenting the image of the ship.
if $\mathcal{D}$ is a set of images of cats and dogs and $Y_{t}$ denotes labels cat and dog, then $\mathcal{D}$ is partitioned into two equivalence classes corresponding to cat and dog images by $Y_{t}$.</p>
<p>Intuitively, a refinement is a subdivision of an existing partition. For a visualization of a refinement of a set of tasks see Figure 4. To mathematically define refinements, we first need to introduce what it means for an equivalence relation to be finer than another equivalence relation.
Definition 2. (Fineness). Let $\sim$ and $\approx$ be two equivalence relations on the set $\mathcal{D}$. If every equivalence class of $\sim$ is a subset of an equivalence class of $\approx$, we say that $\sim$ is finer than $\approx$.</p>
<p>Now we define what refinements.
Definition 3. (Refinement). Let $A, B$ be sets of equivalence classes induced by equivalence relations $\sim$ and $\approx$ over the set $\mathcal{D}$. If $\sim$ is finer than $\approx$, then we call $A$ a refinement of $B$.</p>
<p>Furthermore, we can relate the corresponding sets of equivalence classes.
Lemma 2. Let $\sim$ and $\approx$ be two equivalence relationships on the set $\mathcal{D}$ and denote the corresponding induced partitions by $A$ and $B$. If $\sim$ is finer than $\approx$, then every equivalence class of $\approx$ is a union of equivalence classes of $\sim$.</p>
<p>Coming back to the example of cats and dogs, let $\approx$ be the relation that associates cats with cats and dogs with dogs. Now the relation $\sim$ which associated both cats and dogs with their specific breed (e.g. poodles with other poodles) is finer than $\approx$. Note that $\sim$ partitions $\mathcal{D}$ into breeds and so we can easily generate the sets of cats and dogs (i.e. equivalence classes of $\approx$ ) by taking a union over all the corresponding breeds.</p>
<h1>D. 2 Proof of Theorem 1</h1>
<p>Definition 4. (Invariant Representation). Let $X$ and $Y$ be the covariates and target, respectively. We call $f(X)$ an invariant representation for $Y$ under style $S$ if</p>
<p>$$
p^{d o\left(S=s_{i}\right)}(Y \mid f(X))=p^{d o\left(S=s_{j}\right)}(Y \mid f(X)) \quad \forall s_{i}, s_{j} \in \mathcal{S}
$$</p>
<p>where do $(S=s)$ denotes assigning $S$ the value $s$ and $\mathcal{S}$ is the domain of $S$.
Theorem 1. Theorem 1. Let $\mathcal{Y}=\left{Y_{t}\right}_{t=1}^{T}$ be a family of downstream tasks. Let $Y^{R}$ be a refinement for all tasks in $\mathcal{Y}$. If $f(X)$ is an invariant representation for $Y^{R}$ under changes in style $S$, then $f(X)$ is an invariant representation for all tasks in $\mathcal{Y}$ under changes in style $S$, i.e.</p>
<p>$$
p^{d o\left(s_{i}\right)}\left(Y^{R} \mid f(X)\right)=p^{d o\left(s_{j}\right)}\left(Y^{R} \mid f(X)\right) \quad \Rightarrow \quad p^{d o\left(s_{i}\right)}\left(Y_{t} \mid f(X)\right)=p^{d o\left(s_{j}\right)}\left(Y_{t} \mid f(X)\right)
$$</p>
<p>for all $t \in{1, \ldots, T}$ and for all $s_{i}, s_{j} \in \mathcal{S}$ with $p^{d o\left(s_{i}\right)}=p^{d o\left(S=s_{i}\right)}$. Thus, $f(X)$ is a representation that generalizes to $\mathcal{Y}$.</p>
<p>Proof. Let $t \in{1, \ldots, T}$. We have</p>
<p>$$
\begin{aligned}
p^{\mathrm{do}\left(s_{i}\right)}\left(Y_{t} \mid f(X)\right) &amp; =\int p^{\mathrm{do}\left(s_{i}\right)}\left(Y_{t} \mid Y^{R}\right) p^{\mathrm{do}\left(s_{i}\right)}\left(Y^{R} \mid f(X)\right) d Y^{R}=\int p\left(Y_{t} \mid Y^{R}\right) p^{\mathrm{do}\left(s_{i}\right)}\left(Y^{R} \mid f(X)\right) d Y^{R} \
&amp; =\int p\left(Y_{t} \mid Y^{R}\right) p^{d o\left(s_{j}\right)}\left(Y^{R} \mid f(X)\right) d Y^{R}=p^{d o\left(s_{j}\right)}\left(Y_{t} \mid f(X)\right)
\end{aligned}
$$</p>
<p>For the second and last equality, we used that the mechanism of $Y_{t} \mid Y^{R}$ is independent of $S$, i.e. $p^{\mathrm{do}\left(s_{i}\right)}\left(Y_{t} \mid Y^{R}\right)=p^{\mathrm{do}\left(s_{j}\right)}\left(Y_{t} \mid Y^{R}\right)$. The third equality follows from the assumption that $f(X)$ is an invariant representation for $Y^{R}$ under changes in $S$. Thus, we get that $f(X)$ is an invariant representation for $Y_{t}$ under changes in $S$. Specifically, for a representation to be an invariant representation for $Y_{t}$ it is a sufficient condition for it to be an invariant representation for $Y^{R}$.</p>
<h1>E EXPERIMENTAL DETAILS</h1>
<h2>E. 1 IMAGE AUGMENTATIONS</h2>
<p>For pretraining the representations in ReLIC, we apply the augmentation scheme proposed in SimCLR (Chen et al., 2020a) and used in (Grill et al., 2020). This consists of the following augmentations applied in the order they are listed</p>
<ul>
<li>random crop - we randomly crop the image using an area randomly selected between $8 \%$ and $100 \%$ of the image with an logarithmically sampled aspect ration between $3 / 4$ and $4 / 3$. After this, we resize the patch to $224 \times 224$;</li>
<li>random horizontal flip;</li>
<li>color jittering - we apply in random order perturbations to brightness, contrast, saturation and hue of the image by shifting them by a random uniform offset;</li>
<li>grayscale - we randomly apply grayscaling;</li>
<li>Gaussian blurring - we blur the image using a $23 \times 23$ square Gaussian kernel with standard deviation uniformly sampled in $[0.1,0.2]$;</li>
<li>solarization - we transform all the pixels with $x \rightarrow x * 1_{{x&lt;0.5}}+(1-x) * 1_{{x \geq 0.5}}$.</li>
</ul>
<p>We use the same parameters for the augmentations and probabilities of applying individual augmentations as SimCLR (Chen et al., 2020a). After applying augmentations, we normalize the images with the mean and standard deviation computed on ImageNet across the color channels.</p>
<h2>E. 2 ARCHITECTURE</h2>
<p>We test ReLIC on two different architectures - ResNet-50 (He et al., 2016) and ResNet-50 with target network as in (Grill et al., 2020). For ResNet-50, we use version 1 with post-activation. We take the representation to be the output of the final average pooling layer, which is of dimension 2048. As in SimCLR (Chen et al., 2020a), we use a critic network to project the representation to a lower dimensional space with a multi-layer perceptron (MLP). When using ResNet-50 as encoder, we treat the parameters of the MLP (e.g. depth and width) as hyperparameters and sweep over them. This MLP has batch normalization (Ioffe \&amp; Szegedy, 2015) after every layer, rectified linear activations (ReLU) (Nair \&amp; Hinton, 2010). We used a 4 layer MLP with widths [4096, 2048, 1024, 512] and output size 128 with ResNet-50. When using a ResNet-50 with target networks as in (Grill et al., 2020), we exactly follow their architecture settings.</p>
<h2>E. 3 OPTIMIZATION</h2>
<p>We use a batch size of 4096 and the LARS optimizer (You et al., 2017) with a cosine decay learning rate schedule (Loshchilov \&amp; Hutter, 2017) for 1000 epochs with 10 epochs for warm-up. We exclude the biases and batch normalization parameters from LARS adaptation. We use as the base learning rate 0.3 for ResNet-50 and 0.2 for ResNet-50 with target network. We scale this learning rate by batch size/256 and use a global weight decay parameter of $1.5 * 10^{-6}$ and exclude the biases and batch normalization parameters. For the target network, we follow the approach of BYOL (Grill</p>
<p>et al., 2020) and start the exponential moving average parameter $\tau$ at $\tau_{\text {base }}=0.996$ and increase it to one during training via $\tau=1-\left(1-\tau_{\text {base }}\right)(\cos (\pi k / K)+1) / 2$ with k the current training step and K the maximum number of training steps.</p>
<h1>E. 4 Evaluation on ImageNet</h1>
<p>We follow the standard linear evaluation protocol on ImageNet as in (Kolesnikov et al., 2019; Chen et al., 2020a; Grill et al., 2020). We train a linear classifier on top of the fixed representation, i.e. we do not update the network parameters or the batch statistics. For training, we randomly crop and resize images to $224 \times 224$, and randomly horizontally flip the images after that. For testing, the images are resized to 256 pixels along the shorter dimension with bicubic resampling after which we take a center crop of size $224 \times 224$. Both for training and testing, the images are normalized by substracting the mean and standard deviations across the color channels computed on ImageNet after the augmentations. We use Stochastic Gradient Descent with a Nestorov momentum of 0.9 and train for 80 epochs with a batch size of 1024 . We do not use any regularization techniques, e.g. weight decay.</p>
<h2>E. 5 Robustness and Generalization</h2>
<h2>E.5.1 DATASET DETAILS</h2>
<p>ImageNet-C. The ImageNet-C dataset (Hendrycks \&amp; Dietterich, 2019) consists of 15 different types of corruptions from the noise, blur, weather, and digital categories applied to the validation images of ImageNet. This dataset is used for measuring semantic robustness. Figure 5 visualizes the corruption types. Each type of corruption has 5 levels of severity, i.e. there are 75 distinct corruptions in the dataset. In Figure 6, we display the Impulse noise corruption for 5 different severity levels. As can be seen, with increasing severity level the image becomes increasingly corrupted and difficult to parse. In addition to these 75 corruption types, there are an additional 4 corruption types (speckle noise, gaussian blur, spatter and saturate) that are provided as a validation set. We use these additional corruption types for selecting the best hyperparameters. For further details on this dataset, please refer to (Hendrycks \&amp; Dietterich, 2019).</p>
<p>ImageNet-R. The ImageNet-R dataset (Hendrycks et al., 2020) consists of 30, 000 images depicting various artistic renditions (e.g., paintings, sculpture, origami, cartoon) of 200 ImageNet object classes. This dataset is used to measure out-of-distribution generalization to various abstract visual renditions as it emphasizes shape over texture. The data was collected primarily from Flickr and also includes line drawings from (Wang et al., 2019). The images represent naturally occurring objects and have different textures and local image statistic to those of ImageNet. Figure 7 visualizes different images from the dataset. For further details on this dataset, please refer to (Hendrycks et al., 2020).</p>
<h2>E.5.2 Evaluation</h2>
<p>To evaluate robustness and generalization of the learned representation, we follow the standard linear evaluation protocol on ImageNet as in (Chen et al., 2020b;a; Kolesnikov et al., 2019). We train a linear classifier on top of the frozen representation, i.e. we do not update either the network parameters nor the batch statistics. During training, we augment the data by randomly cropping, resizing to $224 \times 224$ and randomly flipping the image. At test time, images are resized to 256 pixels along the shorter side via bicubic resampling and we take a $224 \times 224$ center crop. Both during training and testing, after applying augmentations we normalize the color channels by subtracting the average color and dividing by the standard deviation that is computed on ImageNet. We optimize the cross-entropy loss using Stochastic Gradient Descent with Nestorov momentum of 0.9. We sweep over number for epochs ${30,50,60,90}$, learning rates ${0.4,0.3,0.2,0.1,0.05,0.01}$ and batch sizes ${1024,2048,4096}$. We select hyperparameters on the validation set provided in ImageNet-C and report the performance on ImageNet-R and on the test set of ImageNet-C under the best validation hyperparameters. We do not use any regularization techniques such as weight decay, gradient clipping, tanh clipping or logits regularization.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The ImageNet-C dataset consists of 15 types of corruptions from noise, blur, weather, and digital categories. Each type of corruption has five levels of severity, resulting in 75 distinct corruptions. See different severity levels in Figure 6.</p>
<p>Clean $\quad$ Severity $=1 \quad$ Severity $=2 \quad$ Severity $=3 \quad$ Severity $=4 \quad$ Severity $=5$
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The 5 different levels of severity of Impulse noise corruption available in the ImageNet-C dataset. With increasing severity the dog image is markedly corrupted.</p>
<h1>E.5.3 ROBUSTNESS METRICS AND FURTHER RESULTS</h1>
<p>Let $f$ be a classifier that has not been trained on ImageNet-C. For each corruption type $c$ and level of severity $1 \leq s \leq 5$, denote the top-1 error of this classifier as $E_{s, c}^{f}$. Different corruption types pose different levels of difficulty. To make error rates across corruption types more comparable, the
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Example images from the dataset ImageNet-R which contains 30,000 images of 200 ImageNet classes. This dataset emphasizes shape over texture and has different textures and local image statistic to those of ImageNet.</p>
<p>error rates are divided by AlexNet's errors. This standardized measure is the Corruption Error and is computed as</p>
<p>$$
C E_{c}^{f}=\left(\sum_{s=1}^{5} E_{s, c}^{f}\right) /\left(\sum_{s=1}^{5} E_{s, c}^{A l e x N e t}\right)
$$</p>
<p>The average error across all 15 corruption types is called the mean Corruption Error (mCE). Corruption Errors and mCE measure absolute robustness.</p>
<p>To better assess robustness, we also report the relative Corruption Error which measures relative robustness, i.e. loss in performance under corruptions. Denote by $E_{\text {clean }}^{f}$ the top-1 error rate for $f$ on the clean test set of ImageNet. The relative Corruption Error is given as</p>
<p>$$
r C E_{c}^{f}=\sum_{s=1}^{5}\left(E_{s, c}^{f}-E_{c l e a n}^{f}\right) / \sum_{s=1}^{5}\left(E_{s, c}^{A l e x N e t}-E_{c l e a n}^{A l e x N e t}\right)
$$</p>
<p>The mean relative Corruption Error (mrCE) is the mean of the relative Corruption Errors across all the corruption types. For more details and intuitions about there measures please refer to (Hendrycks \&amp; Dietterich, 2019).</p>
<p>In Table 6, we report Corruption Errors for Blur, Weather, and Digital corruption types. In Table 7, we report the relative robustness. As per (Hendrycks \&amp; Dietterich, 2019), we used the following values as the average AlexNet errors across severities, i.e. $\frac{1}{5} \sum_{s=1}^{5} E_{s, c}^{A l e x N e t}$, to normalize the Corruption Error values - Gaussian Noise 88.6\%, Shot Noise 89.4\%, Impulse Noise 92.3\%, Defocus Blur 82.0\%, Glass Blur 82.6\%, Motion Blur 78.6\%, Zoom Blur 79.8\%, Snow 86.7\%, Frost 82.7\%, Fog 81.9\%, Brightness 56.5\%, Contrast 85.3\%, Elastic Transformation 64.6\%, Pixelate 71.8\%, JPEG 60.7\%, Speckle Noise 84.5\%, Gaussian Blur 78.7\%, Spatter 71.8\%, Saturate 65.8\%.</p>
<p>Table 6: Mean Corruption Error (mCE) and Corruption Error values for Blur, Weather, and Digital corruption types on ImageNet-C. All models are trained only using clean ImageNet images.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Blur</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Weather</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Digital</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">mCE</td>
<td style="text-align: center;">Defocus</td>
<td style="text-align: center;">Glass</td>
<td style="text-align: center;">Motion</td>
<td style="text-align: center;">Zoom</td>
<td style="text-align: center;">Snow</td>
<td style="text-align: center;">Frost</td>
<td style="text-align: center;">Fog</td>
<td style="text-align: center;">Bright</td>
<td style="text-align: center;">Contrast</td>
<td style="text-align: center;">Elastic</td>
<td style="text-align: center;">Pixel</td>
<td style="text-align: center;">JPEG</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Using ResNet-50:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SimCLR</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">103.3</td>
<td style="text-align: center;">101.8</td>
<td style="text-align: center;">101.9</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">106.8</td>
<td style="text-align: center;">105.2</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ReLIC (ours)</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ResNet-50 with target network:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BYOL</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ReLIC (ours)</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Mean relative Corruption Error (mrCE) and relative Corruption Error values for different corruptions and methods on ImageNet-C. The mrCE value is the mean relative Corruption Error of the corruptions in Noise, Blur, Weather, and Digital columns. All models are trained only using clean ImageNet images. ReLIC-t denotes using ReLIC with a ResNet-50+target network architecture as in BYOL (Grill et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Noise</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Blur</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Weather</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Digital</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">mrCE</td>
<td style="text-align: center;">Gauss.</td>
<td style="text-align: center;">Shot</td>
<td style="text-align: center;">Impulse</td>
<td style="text-align: center;">Defocus</td>
<td style="text-align: center;">Glass</td>
<td style="text-align: center;">Motion</td>
<td style="text-align: center;">Zoom</td>
<td style="text-align: center;">Snow</td>
<td style="text-align: center;">Frost</td>
<td style="text-align: center;">Fog</td>
<td style="text-align: center;">Bright</td>
<td style="text-align: center;">Contrast</td>
<td style="text-align: center;">Elastic</td>
<td style="text-align: center;">Pixel</td>
<td style="text-align: center;">JPEG</td>
</tr>
<tr>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">101</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">146</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">132</td>
</tr>
<tr>
<td style="text-align: center;">Using ResNet-50:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SimCLR</td>
<td style="text-align: center;">111.9</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">106.6</td>
<td style="text-align: center;">122.2</td>
<td style="text-align: center;">139.6</td>
<td style="text-align: center;">140.5</td>
<td style="text-align: center;">139.4</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">181.5</td>
<td style="text-align: center;">158.3</td>
<td style="text-align: center;">149.8</td>
</tr>
<tr>
<td style="text-align: center;">ReLIC (ours)</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">128.7</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">123.1</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">157.5</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: center;">ResNet-50 with target network:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BYOL</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">122.5</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">155</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">108.4</td>
</tr>
<tr>
<td style="text-align: center;">ReLIC (ours)</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">134.2</td>
<td style="text-align: center;">111.6</td>
<td style="text-align: center;">121.9</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">154.5</td>
<td style="text-align: center;">102.7</td>
<td style="text-align: center;">106.8</td>
</tr>
</tbody>
</table>
<h1>E. 6 Evaluation on Atari</h1>
<p>For our experiments on Atari, we use the agent from R2D2 (Kapturowski et al., 2019) with standard hyperparameters noted below. We train each agent on approximately 15 billion frames and add a second encoder with the same architecture used in the Q-Network of the original agent. This second encoder is trained with a separate optimizer with only a representation learning objective. The agent then takes the output of this encoder as a given input. We use standard augmentations used in prior work (Kostrikov et al., 2020) where we pad the frames on all sides with 4 pixels copied from the</p>
<p>borders and then randomly cropping 84 windows. We randomly shift pixel intensity according to the distribution $s=1.0+0.1 * \mathcal{N}^{\prime}$ where $\mathcal{N}^{\prime}$ is the standard Normal distribution with values clipped between -2 and 2. $s$ is then multiplied by the original image to return the augmented image.</p>
<p>RELIC and SimCLR For our implementation of ReLIC and SimCLR, we do not use a critic embedding at all and utilize the last layer of the encoder for the objective. As in CURL (Srinivas et al., 2020) we utilize a target encoder for the second augmentation where we update the weights with a momentum of .99 . We also clipped the gradients of our optimizer using a global norm ratio of 40. We report the hyperparameters in Table 8.</p>
<p>Table 8: ReLIC and SimCLR Details</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Normalize Inputs</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">Temperature</td>
<td style="text-align: center;">1.0 Constant</td>
</tr>
<tr>
<td style="text-align: center;">Scaling of Embeddings</td>
<td style="text-align: center;">False</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">Epsilon</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Beta 1</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">Beta 2</td>
<td style="text-align: center;">0.999</td>
</tr>
</tbody>
</table>
<p>CURL For CURL, we use a second encoder as noted before. With the exception of the encoder architecture and the optimizer parameters, all hyperparameters are the same as in (Srinivas et al., 2020) including the momentum value for the target network weight updates. We utilize the same architecture in the paper with a linear layer as a critic embedding for the target encoder.</p>
<p>Table 9: CURL Details</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;">Epsilon</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Beta 1</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;">Beta 2</td>
<td style="text-align: center;">0.999</td>
</tr>
</tbody>
</table>
<p>BYOL In BYOL, we utilize two-layer perceptron networks as our predictor and projection layers. For both networks, the number of hidden units in the two layers was 1024 and 512. We use a target network update momentum of .99 . The optimizer parameters are the same as in Table 8.</p>
<p>Direct Augmentation We also compared against direct augmentation of the observations in the replay buffer as in DrQ (Kostrikov et al., 2020). We keep the architecture the same in this instance and use two duplicate encoders as input to the agent. In this case, the optimizer can jointly update both encoders and train them end-to-end.</p>
<p>Table 10: Individual Mean Episode Return on Atari.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Games</th>
<th style="text-align: center;">Average Human</th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;">Rt/LJC (ours)</th>
<th style="text-align: center;">SimCLR</th>
<th style="text-align: center;">CURL</th>
<th style="text-align: center;">BYOL</th>
<th style="text-align: center;">Augmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">alien</td>
<td style="text-align: center;">7127.70</td>
<td style="text-align: center;">227.80</td>
<td style="text-align: center;">8766.57</td>
<td style="text-align: center;">10082.54</td>
<td style="text-align: center;">8506.48</td>
<td style="text-align: center;">9671.80</td>
<td style="text-align: center;">5201.93</td>
</tr>
<tr>
<td style="text-align: center;">amidar</td>
<td style="text-align: center;">1719.50</td>
<td style="text-align: center;">5.80</td>
<td style="text-align: center;">28449.26</td>
<td style="text-align: center;">28141.18</td>
<td style="text-align: center;">27213.75</td>
<td style="text-align: center;">25965.05</td>
<td style="text-align: center;">867.66</td>
</tr>
<tr>
<td style="text-align: center;">assault</td>
<td style="text-align: center;">742.00</td>
<td style="text-align: center;">222.40</td>
<td style="text-align: center;">92963.07</td>
<td style="text-align: center;">36109.84</td>
<td style="text-align: center;">7139.67</td>
<td style="text-align: center;">13565.20</td>
<td style="text-align: center;">1539.71</td>
</tr>
<tr>
<td style="text-align: center;">asteris</td>
<td style="text-align: center;">8503.30</td>
<td style="text-align: center;">210.00</td>
<td style="text-align: center;">998426.72</td>
<td style="text-align: center;">997305.51</td>
<td style="text-align: center;">661431.39</td>
<td style="text-align: center;">986307.92</td>
<td style="text-align: center;">26239.64</td>
</tr>
<tr>
<td style="text-align: center;">asteroids</td>
<td style="text-align: center;">47388.70</td>
<td style="text-align: center;">719.10</td>
<td style="text-align: center;">83669.38</td>
<td style="text-align: center;">7299.90</td>
<td style="text-align: center;">76612.17</td>
<td style="text-align: center;">55936.02</td>
<td style="text-align: center;">101340.17</td>
</tr>
<tr>
<td style="text-align: center;">atlantic</td>
<td style="text-align: center;">29028.10</td>
<td style="text-align: center;">12850.00</td>
<td style="text-align: center;">1575940.94</td>
<td style="text-align: center;">1584392.76</td>
<td style="text-align: center;">1584698.01</td>
<td style="text-align: center;">1530122.45</td>
<td style="text-align: center;">794011.79</td>
</tr>
<tr>
<td style="text-align: center;">bank heist</td>
<td style="text-align: center;">753.10</td>
<td style="text-align: center;">14.20</td>
<td style="text-align: center;">1521.38</td>
<td style="text-align: center;">2467.62</td>
<td style="text-align: center;">4095.29</td>
<td style="text-align: center;">1659.94</td>
<td style="text-align: center;">771.60</td>
</tr>
<tr>
<td style="text-align: center;">battle zone</td>
<td style="text-align: center;">37187.50</td>
<td style="text-align: center;">2360.00</td>
<td style="text-align: center;">452831.48</td>
<td style="text-align: center;">278903.14</td>
<td style="text-align: center;">287792.06</td>
<td style="text-align: center;">338695.47</td>
<td style="text-align: center;">31511.75</td>
</tr>
<tr>
<td style="text-align: center;">beam rider</td>
<td style="text-align: center;">16926.50</td>
<td style="text-align: center;">363.90</td>
<td style="text-align: center;">136695.24</td>
<td style="text-align: center;">98551.42</td>
<td style="text-align: center;">116794.58</td>
<td style="text-align: center;">87454.20</td>
<td style="text-align: center;">46894.14</td>
</tr>
<tr>
<td style="text-align: center;">berzerk</td>
<td style="text-align: center;">2630.40</td>
<td style="text-align: center;">123.70</td>
<td style="text-align: center;">146213.60</td>
<td style="text-align: center;">1301.36</td>
<td style="text-align: center;">73754.38</td>
<td style="text-align: center;">1265.21</td>
<td style="text-align: center;">73645.52</td>
</tr>
<tr>
<td style="text-align: center;">bowling</td>
<td style="text-align: center;">160.70</td>
<td style="text-align: center;">23.10</td>
<td style="text-align: center;">205.09</td>
<td style="text-align: center;">193.50</td>
<td style="text-align: center;">230.31</td>
<td style="text-align: center;">172.21</td>
<td style="text-align: center;">164.68</td>
</tr>
<tr>
<td style="text-align: center;">boxing</td>
<td style="text-align: center;">12.10</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: center;">breakout</td>
<td style="text-align: center;">30.50</td>
<td style="text-align: center;">1.70</td>
<td style="text-align: center;">405.05</td>
<td style="text-align: center;">404.06</td>
<td style="text-align: center;">407.14</td>
<td style="text-align: center;">409.48</td>
<td style="text-align: center;">150.67</td>
</tr>
<tr>
<td style="text-align: center;">centipede</td>
<td style="text-align: center;">12017.00</td>
<td style="text-align: center;">2090.90</td>
<td style="text-align: center;">220886.86</td>
<td style="text-align: center;">99544.92</td>
<td style="text-align: center;">167779.11</td>
<td style="text-align: center;">146735.67</td>
<td style="text-align: center;">20152.01</td>
</tr>
<tr>
<td style="text-align: center;">chopper command</td>
<td style="text-align: center;">7387.80</td>
<td style="text-align: center;">811.00</td>
<td style="text-align: center;">999900.00</td>
<td style="text-align: center;">999900.00</td>
<td style="text-align: center;">999900.00</td>
<td style="text-align: center;">962003.61</td>
<td style="text-align: center;">5399.56</td>
</tr>
<tr>
<td style="text-align: center;">crazy climber</td>
<td style="text-align: center;">35829.40</td>
<td style="text-align: center;">10780.50</td>
<td style="text-align: center;">272179.68</td>
<td style="text-align: center;">266870.81</td>
<td style="text-align: center;">301689.62</td>
<td style="text-align: center;">210477.39</td>
<td style="text-align: center;">96538.00</td>
</tr>
<tr>
<td style="text-align: center;">defender</td>
<td style="text-align: center;">18688.90</td>
<td style="text-align: center;">2874.50</td>
<td style="text-align: center;">576405.57</td>
<td style="text-align: center;">322617.05</td>
<td style="text-align: center;">560816.84</td>
<td style="text-align: center;">493410.36</td>
<td style="text-align: center;">78750.19</td>
</tr>
<tr>
<td style="text-align: center;">demon attack</td>
<td style="text-align: center;">1971.00</td>
<td style="text-align: center;">152.10</td>
<td style="text-align: center;">143774.79</td>
<td style="text-align: center;">143786.19</td>
<td style="text-align: center;">143737.36</td>
<td style="text-align: center;">143574.86</td>
<td style="text-align: center;">821.98</td>
</tr>
<tr>
<td style="text-align: center;">double dunk</td>
<td style="text-align: center;">$-16.40$</td>
<td style="text-align: center;">$-18.60$</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">14.82</td>
</tr>
<tr>
<td style="text-align: center;">onduro</td>
<td style="text-align: center;">860.50</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2371.27</td>
<td style="text-align: center;">2366.19</td>
<td style="text-align: center;">2373.12</td>
<td style="text-align: center;">2368.00</td>
<td style="text-align: center;">1361.66</td>
</tr>
<tr>
<td style="text-align: center;">fishing derby</td>
<td style="text-align: center;">$-58.70$</td>
<td style="text-align: center;">$-91.70$</td>
<td style="text-align: center;">68.17</td>
<td style="text-align: center;">83.00</td>
<td style="text-align: center;">72.21</td>
<td style="text-align: center;">70.11</td>
<td style="text-align: center;">19.93</td>
</tr>
<tr>
<td style="text-align: center;">freeway</td>
<td style="text-align: center;">29.60</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">33.00</td>
<td style="text-align: center;">32.93</td>
<td style="text-align: center;">33.04</td>
<td style="text-align: center;">33.00</td>
<td style="text-align: center;">32.00</td>
</tr>
<tr>
<td style="text-align: center;">frostbite</td>
<td style="text-align: center;">4334.70</td>
<td style="text-align: center;">65.20</td>
<td style="text-align: center;">10156.41</td>
<td style="text-align: center;">11171.49</td>
<td style="text-align: center;">3693.20</td>
<td style="text-align: center;">5793.80</td>
<td style="text-align: center;">5708.35</td>
</tr>
<tr>
<td style="text-align: center;">gopher</td>
<td style="text-align: center;">2412.50</td>
<td style="text-align: center;">257.60</td>
<td style="text-align: center;">123170.74</td>
<td style="text-align: center;">122368.21</td>
<td style="text-align: center;">122371.64</td>
<td style="text-align: center;">120317.04</td>
<td style="text-align: center;">43711.82</td>
</tr>
<tr>
<td style="text-align: center;">gravitar</td>
<td style="text-align: center;">3351.40</td>
<td style="text-align: center;">172.00</td>
<td style="text-align: center;">4186.09</td>
<td style="text-align: center;">3601.14</td>
<td style="text-align: center;">4997.87</td>
<td style="text-align: center;">4048.25</td>
<td style="text-align: center;">2014.59</td>
</tr>
<tr>
<td style="text-align: center;">hero</td>
<td style="text-align: center;">30826.40</td>
<td style="text-align: center;">1027.00</td>
<td style="text-align: center;">13615.35</td>
<td style="text-align: center;">13523.98</td>
<td style="text-align: center;">13620.78</td>
<td style="text-align: center;">13558.04</td>
<td style="text-align: center;">8957.00</td>
</tr>
<tr>
<td style="text-align: center;">ice hockey</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">$-11.20$</td>
<td style="text-align: center;">56.39</td>
<td style="text-align: center;">48.27</td>
<td style="text-align: center;">45.06</td>
<td style="text-align: center;">59.70</td>
<td style="text-align: center;">$-2.43$</td>
</tr>
<tr>
<td style="text-align: center;">jamesbond</td>
<td style="text-align: center;">302.80</td>
<td style="text-align: center;">29.00</td>
<td style="text-align: center;">15632.87</td>
<td style="text-align: center;">5714.62</td>
<td style="text-align: center;">10052.04</td>
<td style="text-align: center;">10099.81</td>
<td style="text-align: center;">1441.95</td>
</tr>
<tr>
<td style="text-align: center;">kangaroo</td>
<td style="text-align: center;">3035.00</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">14342.59</td>
<td style="text-align: center;">14215.11</td>
<td style="text-align: center;">11674.19</td>
<td style="text-align: center;">14471.65</td>
<td style="text-align: center;">7249.73</td>
</tr>
<tr>
<td style="text-align: center;">krull</td>
<td style="text-align: center;">2665.50</td>
<td style="text-align: center;">1598.00</td>
<td style="text-align: center;">137099.65</td>
<td style="text-align: center;">100426.69</td>
<td style="text-align: center;">86049.99</td>
<td style="text-align: center;">80414.04</td>
<td style="text-align: center;">16626.09</td>
</tr>
<tr>
<td style="text-align: center;">kung fu master</td>
<td style="text-align: center;">22736.30</td>
<td style="text-align: center;">258.50</td>
<td style="text-align: center;">230241.57</td>
<td style="text-align: center;">220076.57</td>
<td style="text-align: center;">228943.94</td>
<td style="text-align: center;">208064.38</td>
<td style="text-align: center;">64632.42</td>
</tr>
<tr>
<td style="text-align: center;">montesnina revenge</td>
<td style="text-align: center;">4753.30</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1066.67</td>
<td style="text-align: center;">733.33</td>
<td style="text-align: center;">1072.30</td>
<td style="text-align: center;">419.54</td>
<td style="text-align: center;">26.67</td>
</tr>
<tr>
<td style="text-align: center;">ms pacman</td>
<td style="text-align: center;">6951.60</td>
<td style="text-align: center;">307.30</td>
<td style="text-align: center;">13367.55</td>
<td style="text-align: center;">12053.76</td>
<td style="text-align: center;">13465.80</td>
<td style="text-align: center;">12726.79</td>
<td style="text-align: center;">3238.90</td>
</tr>
<tr>
<td style="text-align: center;">name this game</td>
<td style="text-align: center;">8049.00</td>
<td style="text-align: center;">2292.30</td>
<td style="text-align: center;">48669.30</td>
<td style="text-align: center;">46657.55</td>
<td style="text-align: center;">47417.82</td>
<td style="text-align: center;">44848.29</td>
<td style="text-align: center;">13416.57</td>
</tr>
<tr>
<td style="text-align: center;">pheenix</td>
<td style="text-align: center;">7242.60</td>
<td style="text-align: center;">761.40</td>
<td style="text-align: center;">803108.37</td>
<td style="text-align: center;">253542.40</td>
<td style="text-align: center;">580969.56</td>
<td style="text-align: center;">20317.80</td>
<td style="text-align: center;">6264.39</td>
</tr>
<tr>
<td style="text-align: center;">pitfall</td>
<td style="text-align: center;">6463.70</td>
<td style="text-align: center;">$-229.40$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">pong</td>
<td style="text-align: center;">14.60</td>
<td style="text-align: center;">$-20.70$</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">21.00</td>
</tr>
<tr>
<td style="text-align: center;">private eye</td>
<td style="text-align: center;">69571.30</td>
<td style="text-align: center;">24.90</td>
<td style="text-align: center;">10154.93</td>
<td style="text-align: center;">5115.34</td>
<td style="text-align: center;">5190.28</td>
<td style="text-align: center;">470.68</td>
<td style="text-align: center;">111.77</td>
</tr>
<tr>
<td style="text-align: center;">sfeet</td>
<td style="text-align: center;">13455.00</td>
<td style="text-align: center;">163.90</td>
<td style="text-align: center;">353197.13</td>
<td style="text-align: center;">24340.75</td>
<td style="text-align: center;">208207.97</td>
<td style="text-align: center;">57261.24</td>
<td style="text-align: center;">11051.97</td>
</tr>
<tr>
<td style="text-align: center;">riverraid</td>
<td style="text-align: center;">17118.00</td>
<td style="text-align: center;">1338.50</td>
<td style="text-align: center;">23525.44</td>
<td style="text-align: center;">20400.83</td>
<td style="text-align: center;">20230.02</td>
<td style="text-align: center;">22206.57</td>
<td style="text-align: center;">10487.59</td>
</tr>
<tr>
<td style="text-align: center;">road runner</td>
<td style="text-align: center;">7845.00</td>
<td style="text-align: center;">11.50</td>
<td style="text-align: center;">213173.15</td>
<td style="text-align: center;">236235.30</td>
<td style="text-align: center;">241917.98</td>
<td style="text-align: center;">238880.54</td>
<td style="text-align: center;">440430.17</td>
</tr>
<tr>
<td style="text-align: center;">robotank</td>
<td style="text-align: center;">11.90</td>
<td style="text-align: center;">2.20</td>
<td style="text-align: center;">97.65</td>
<td style="text-align: center;">82.60</td>
<td style="text-align: center;">98.13</td>
<td style="text-align: center;">62.54</td>
<td style="text-align: center;">49.98</td>
</tr>
<tr>
<td style="text-align: center;">seaguest</td>
<td style="text-align: center;">42054.70</td>
<td style="text-align: center;">68.40</td>
<td style="text-align: center;">999999.00</td>
<td style="text-align: center;">999999.00</td>
<td style="text-align: center;">666700.67</td>
<td style="text-align: center;">29160.93</td>
<td style="text-align: center;">37397.26</td>
</tr>
<tr>
<td style="text-align: center;">skiing</td>
<td style="text-align: center;">$-4336.90$</td>
<td style="text-align: center;">17098.10</td>
<td style="text-align: center;">$-24761.06$</td>
<td style="text-align: center;">$-23076.73$</td>
<td style="text-align: center;">$-15497.66$</td>
<td style="text-align: center;">$-26028.08$</td>
<td style="text-align: center;">$-22162.91$</td>
</tr>
<tr>
<td style="text-align: center;">solaris</td>
<td style="text-align: center;">12326.70</td>
<td style="text-align: center;">1236.30</td>
<td style="text-align: center;">4594.37</td>
<td style="text-align: center;">4571.27</td>
<td style="text-align: center;">4276.39</td>
<td style="text-align: center;">4331.03</td>
<td style="text-align: center;">4142.69</td>
</tr>
<tr>
<td style="text-align: center;">space invaders</td>
<td style="text-align: center;">1668.70</td>
<td style="text-align: center;">148.00</td>
<td style="text-align: center;">3625.52</td>
<td style="text-align: center;">3619.94</td>
<td style="text-align: center;">3542.48</td>
<td style="text-align: center;">3613.93</td>
<td style="text-align: center;">835.37</td>
</tr>
<tr>
<td style="text-align: center;">star ganner</td>
<td style="text-align: center;">10250.00</td>
<td style="text-align: center;">664.00</td>
<td style="text-align: center;">283499.72</td>
<td style="text-align: center;">289099.89</td>
<td style="text-align: center;">129720.84</td>
<td style="text-align: center;">175486.67</td>
<td style="text-align: center;">43167.07</td>
</tr>
<tr>
<td style="text-align: center;">surround</td>
<td style="text-align: center;">6.50</td>
<td style="text-align: center;">$-10.00$</td>
<td style="text-align: center;">10.00</td>
<td style="text-align: center;">9.96</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">9.56</td>
<td style="text-align: center;">$-0.64$</td>
</tr>
<tr>
<td style="text-align: center;">tennis</td>
<td style="text-align: center;">$-8.30$</td>
<td style="text-align: center;">$-23.80$</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.12</td>
</tr>
<tr>
<td style="text-align: center;">time pilot</td>
<td style="text-align: center;">5229.20</td>
<td style="text-align: center;">3568.00</td>
<td style="text-align: center;">309297.74</td>
<td style="text-align: center;">92888.66</td>
<td style="text-align: center;">400326.69</td>
<td style="text-align: center;">48011.44</td>
<td style="text-align: center;">14198.37</td>
</tr>
<tr>
<td style="text-align: center;">tutankham</td>
<td style="text-align: center;">167.60</td>
<td style="text-align: center;">11.40</td>
<td style="text-align: center;">371.17</td>
<td style="text-align: center;">306.45</td>
<td style="text-align: center;">337.61</td>
<td style="text-align: center;">285.36</td>
<td style="text-align: center;">144.30</td>
</tr>
<tr>
<td style="text-align: center;">up n down</td>
<td style="text-align: center;">11693.20</td>
<td style="text-align: center;">533.40</td>
<td style="text-align: center;">577256.03</td>
<td style="text-align: center;">520666.59</td>
<td style="text-align: center;">566912.89</td>
<td style="text-align: center;">552110.67</td>
<td style="text-align: center;">143512.38</td>
</tr>
<tr>
<td style="text-align: center;">venture</td>
<td style="text-align: center;">1187.50</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1929.53</td>
<td style="text-align: center;">1945.20</td>
<td style="text-align: center;">1906.84</td>
<td style="text-align: center;">1881.76</td>
<td style="text-align: center;">733.29</td>
</tr>
<tr>
<td style="text-align: center;">video pinball</td>
<td style="text-align: center;">17667.90</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">978292.52</td>
<td style="text-align: center;">993332.08</td>
<td style="text-align: center;">932523.58</td>
<td style="text-align: center;">623223.24</td>
<td style="text-align: center;">37584.71</td>
</tr>
<tr>
<td style="text-align: center;">wizard of war</td>
<td style="text-align: center;">4756.50</td>
<td style="text-align: center;">563.50</td>
<td style="text-align: center;">123513.74</td>
<td style="text-align: center;">89462.62</td>
<td style="text-align: center;">106801.20</td>
<td style="text-align: center;">68256.44</td>
<td style="text-align: center;">5940.82</td>
</tr>
<tr>
<td style="text-align: center;">yars revenge</td>
<td style="text-align: center;">54576.90</td>
<td style="text-align: center;">3092.90</td>
<td style="text-align: center;">228704.52</td>
<td style="text-align: center;">99636.25</td>
<td style="text-align: center;">229221.52</td>
<td style="text-align: center;">86847.75</td>
<td style="text-align: center;">48041.63</td>
</tr>
<tr>
<td style="text-align: center;">zaxxon</td>
<td style="text-align: center;">9173.30</td>
<td style="text-align: center;">32.50</td>
<td style="text-align: center;">120830.77</td>
<td style="text-align: center;">57379.66</td>
<td style="text-align: center;">85906.74</td>
<td style="text-align: center;">48067.61</td>
<td style="text-align: center;">23688.22</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Note that since refinements are more fine-grained that the original task, if a representation captures a refinement then it also captures the downstream tasks as strictly more information is needed to solve the refinement.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
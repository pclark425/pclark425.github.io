<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1660 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1660</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1660</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-229156199</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2012.06662v1.pdf" target="_blank">Protective Policy Transfer</a></p>
                <p><strong>Paper Abstract:</strong> Being able to transfer existing skills to new situations is a key capability when training robots to operate in unpredictable real-world environments. A successful transfer algorithm should not only minimize the number of samples that the robot needs to collect in the new environment, but also prevent the robot from damaging itself or the surrounding environment during the transfer process. In this work, we introduce a policy transfer algorithm for adapting robot motor skills to novel scenarios while minimizing serious failures. Our algorithm trains two control policies in the training environment: a task policy that is optimized to complete the task of interest, and a protective policy that is dedicated to keep the robot from unsafe events (e.g. falling to the ground). To decide which policy to use during execution, we learn a safety estimator model in the training environment that estimates a continuous safety level of the robot. When used with a set of thresholds, the safety estimator becomes a classifier for switching between the protective policy and the task policy. We evaluate our approach on four simulated robot locomotion problems and a 2D navigation problem and show that our method can achieve successful transfer to notably different environments while taking the robot's safety into consideration.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1660.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1660.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Protective Policy Transfer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Protective Policy Transfer (task policy + protective policy + OSSE + protective adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sim-to-(different)sim transfer algorithm that trains a task policy and a protective policy in a source simulator, learns a one-step safety estimator (OSSE) to switch between them using two thresholds, and runs a conservative adaptation procedure in the target environment to minimize unsafe (failed) trials while recovering task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Simulated legged robots: Hopper, HalfCheetah, Walker2D, Unitree A1 (3D quadruped)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Legged locomotion agents controlled by neural-network policies (PPO); observe IMU, motor angles, motor velocities; A1 controlled via PD controllers producing desired motor angles.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — legged locomotion (sim-to-sim as a proxy for sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>DART (training) -> MuJoCo (target/testing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rigid-body physics simulators modeling robot dynamics, contacts, friction, restitution, joint dynamics; models include PD-controlled motors and standard sensor outputs (IMU, angles, velocities).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Moderate-to-high fidelity rigid-body physics (simulators with detailed contact and joint modeling), but still subject to actuator and terrain modeling mismatch between engines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact dynamics, friction, restitution, rigid-body mass/inertia, joint damping/stiffness, torque limits, PD motor control, IMU and joint sensors, (in testing) added command latency and motor power limits.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Actuator dynamics and power delivery modeling differed between training and testing (actuator model mismatch noted); some real-world effects (hardware noise, unmodeled sensor noise, full actuator dynamics) not fully captured; training used flat terrain while testing included stepped terrain for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Target environment was a different physics engine (MuJoCo) with modified properties (added latencies of 8–50 ms depending on robot), motor power limits (A1), and altered terrain (A1: sequence of steps); no physical hardware experiments were reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Locomotion skills (forward walking/running) learned in source simulator and adapted to target simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning (PPO) with dynamics randomization during training; two policies trained (π_task and π_protect) plus learning an ensemble OSSE value predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Mean task return (training reward) in target environment and number of unsafe/failed trials during adaptation (failed trial = robot falls/unsafe terminal condition).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>In the target simulator (MuJoCo) reported mean returns for the proposed method: Hopper 1700.5, HalfCheetah 1402.2, Walker2D 1656.4, A1 1281.6 (Table III) — values are episode returns (unitless reward).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Dynamics randomization applied during training over friction, restitution, mass, joint damping, torque limits, position gains, joint stiffness; ranges summarized in Table II (e.g., friction [0.2,1.0] or [0.4,1.5], restitution ranges, mass ranges, torque limits, percentage variations for some robots).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Simulator engine differences (DART vs MuJoCo) leading to physics discrepancies; actuator modeling mismatch (torque/power limits), sensor/actuation latency, terrain geometry differences (flat vs stepped), and unmodeled dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training separate protective policy that generalizes to stable poses, learning OSSE (one-step safety estimator) to predict safety after one π_task action followed by π_protect, conservative two-threshold switching (κ_task, κ_protect), protective adaptation procedure (start conservative, relax thresholds with at most two unsafe trials), dynamics randomization during training, ensemble OSSE for uncertainty smoothing.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper emphasizes that actuator dynamics and sensor latency are key sources of sim-to-real gap (citing prior work) and finds that dynamics randomization alone is insufficient; no strict quantitative fidelity tolerances are provided, but accurate actuator/latency modeling and accounting for torque/power limits and terrain are highlighted as important.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Protective Policy Transfer enables successful adaptation across substantial simulation gaps (DART→MuJoCo, added latency, actuator power limits, terrain changes) with improved task returns and fewer unsafe trials compared to baselines; dynamics randomization alone is insufficient, and safety-aware switching via OSSE plus a conservative adaptation minimizes failed trials while recovering task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Protective Policy Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1660.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1660.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamics Randomization (DR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamics Randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common sim-to-real technique that trains controllers with randomized dynamics parameters so policies become robust to variations between simulation and target environments; used here both within the proposed method and as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Same set of simulated legged robots (Hopper, HalfCheetah, Walker2D, A1) used in paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Legged robots trained under randomized dynamics to improve robustness to environment changes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — policy robustness for locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>DART (training) with parameter randomization</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulator where dynamics parameters (friction, restitution, mass, joint damping, torque limits, etc.) are sampled from ranges during training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Moderate fidelity rigid-body simulation augmented by randomized parameter regimes to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Key rigid-body properties and contact interactions were varied across samples (friction, restitution, mass/inertia, joint damping/stiffness, torque limits).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not explicitly model certain actuator-specific behaviors (e.g., speed-dependent torque/power limits), does not guarantee matching of simulator-specific integrator/contact solver differences (DART vs MuJoCo), and training did not include added latency that was present in target tests.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Target was a different simulator (MuJoCo) with added latencies and altered actuator/terrain properties; no physical robot testing reported for DR in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Locomotion controllers trained with domain randomization intended to transfer to altered simulator conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning (PPO) with randomized dynamics during training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Mean task return in target environment and number of failed/unsafe trials during testing/adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Baseline DR performance in target simulator (MuJoCo) reported in Table III: Hopper 402.0, HalfCheetah 290.2, Walker2D 1000.4, A1 475.18 (mean returns); DR and DR-RE noted to have zero failed trials during 'adaptation' because they do not perform adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Same parameter ranges as used for training in the paper (see Table II): friction, restitution, mass ranges, joint damping, torque limits, etc.; some parameters randomized as absolute ranges and others as percentages of defaults for particular robots.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Even with DR, simulator-to-simulator discrepancies (integrator/contact solver/actuator models), actuator power limits, and added latencies can prevent successful transfer; DR alone produced much lower task returns in many test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>DR provides robustness across a distribution of dynamics but was not sufficient by itself for high performance across the large DART→MuJoCo gaps studied; combining DR with safety-aware adaptation (Protective Policy Transfer) enabled better outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Implicitly identified as insufficient when DR is the only mitigation; the paper cites actuator dynamics and latency as important fidelity aspects to include for successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamics randomization improves robustness but is not sufficient for high-quality transfer across large simulator/modeling gaps (e.g., DART→MuJoCo, actuator/power/latency mismatches); safety-aware adaptation and protective policies are effective complementary techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Protective Policy Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1660.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1660.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-Step Safety Estimator (OSSE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A predictive ensemble model trained to estimate the safety level of taking one action from the task policy followed by the protective policy, used to decide online switching between task and protective policies to reduce unsafe trials during transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Applied to the same legged robots (Hopper, HalfCheetah, Walker2D, A1) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Estimator acts on the observation (o) to predict safety value used for policy switching; implemented as an ensemble of neural nets averaged for final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — safe policy adaptation for locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Trained in source simulator (DART) and used during adaptation in target simulator (MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Predictive model that uses simulated rollouts collected in source environment to learn expected protective-policy value after a one-step task-policy action.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Model-based estimator trained on simulated rollouts; fidelity depends on fidelity of source simulator and noise injected during data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Predicts value of protective policy (V_protect) after hypothetical one-step transitions; ensemble captures epistemic uncertainty from training data sampled with noise.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>OSSE is learned from source-simulated transitions and therefore may not fully capture target-simulator or real-world dynamics; it is an approximate predictor rather than a dynamics simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Used at adaptation time in the target simulator (MuJoCo) to decide switching and reduce unsafe trials; intended for safe sim-to-real deployment though not tested on physical robots here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Enables safer execution of transferred locomotion skills by switching between task and protective policies.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised regression training of ensemble neural networks using rollouts generated by π_task and π_protect in the source environment (see Algorithm 1).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Improvement measured by higher task returns and reduced number of unsafe trials relative to variants without OSSE (NO-OSSE) and baselines; comparisons reported in Table III and Figures.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>OSSE trained on data collected from π_task rollouts with added noise and from π_protect rollouts initialized from those states; ensemble of 3 models used.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>OSSE can over-estimate safety if trained only on V_protect values (i.e., not accounting for one-step π_task action), leading to unsafe switches; OSSE addresses this by predicting one-step outcomes but still relies on similarity between source and target state distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Ensemble modeling for uncertainty smoothing, training OSSE on states visited by noisy π_task to cover out-of-distribution scenarios, using two-threshold hysteresis switching and conservative adaptation with at most two unsafe trials.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>OSSE performance depends on representativeness of source-rollout data for states likely to be encountered in target environment; if π_protect generalizes better than π_task then OSSE predictions are more reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>OSSE improves safe transfer by predicting the safety consequence of taking one π_task action before switching to π_protect; replacing OSSE with V_protect (NO-OSSE) worsened outcomes on less stable robots, showing OSSE’s importance for realistic safety estimation during transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Protective Policy Transfer', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer for biped locomotion <em>(Rating: 2)</em></li>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 1)</em></li>
                <li>Learning locomotion skills for cassie: Iterative design and sim-to-real <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1660",
    "paper_id": "paper-229156199",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Protective Policy Transfer",
            "name_full": "Protective Policy Transfer (task policy + protective policy + OSSE + protective adaptation)",
            "brief_description": "A sim-to-(different)sim transfer algorithm that trains a task policy and a protective policy in a source simulator, learns a one-step safety estimator (OSSE) to switch between them using two thresholds, and runs a conservative adaptation procedure in the target environment to minimize unsafe (failed) trials while recovering task performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Simulated legged robots: Hopper, HalfCheetah, Walker2D, Unitree A1 (3D quadruped)",
            "agent_system_description": "Legged locomotion agents controlled by neural-network policies (PPO); observe IMU, motor angles, motor velocities; A1 controlled via PD controllers producing desired motor angles.",
            "domain": "robotics — legged locomotion (sim-to-sim as a proxy for sim-to-real)",
            "virtual_environment_name": "DART (training) -&gt; MuJoCo (target/testing)",
            "virtual_environment_description": "Rigid-body physics simulators modeling robot dynamics, contacts, friction, restitution, joint dynamics; models include PD-controlled motors and standard sensor outputs (IMU, angles, velocities).",
            "simulation_fidelity_level": "Moderate-to-high fidelity rigid-body physics (simulators with detailed contact and joint modeling), but still subject to actuator and terrain modeling mismatch between engines.",
            "fidelity_aspects_modeled": "Contact dynamics, friction, restitution, rigid-body mass/inertia, joint damping/stiffness, torque limits, PD motor control, IMU and joint sensors, (in testing) added command latency and motor power limits.",
            "fidelity_aspects_simplified": "Actuator dynamics and power delivery modeling differed between training and testing (actuator model mismatch noted); some real-world effects (hardware noise, unmodeled sensor noise, full actuator dynamics) not fully captured; training used flat terrain while testing included stepped terrain for some experiments.",
            "real_environment_description": "Target environment was a different physics engine (MuJoCo) with modified properties (added latencies of 8–50 ms depending on robot), motor power limits (A1), and altered terrain (A1: sequence of steps); no physical hardware experiments were reported in this paper.",
            "task_or_skill_transferred": "Locomotion skills (forward walking/running) learned in source simulator and adapted to target simulator.",
            "training_method": "Deep reinforcement learning (PPO) with dynamics randomization during training; two policies trained (π_task and π_protect) plus learning an ensemble OSSE value predictor.",
            "transfer_success_metric": "Mean task return (training reward) in target environment and number of unsafe/failed trials during adaptation (failed trial = robot falls/unsafe terminal condition).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "In the target simulator (MuJoCo) reported mean returns for the proposed method: Hopper 1700.5, HalfCheetah 1402.2, Walker2D 1656.4, A1 1281.6 (Table III) — values are episode returns (unitless reward).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Dynamics randomization applied during training over friction, restitution, mass, joint damping, torque limits, position gains, joint stiffness; ranges summarized in Table II (e.g., friction [0.2,1.0] or [0.4,1.5], restitution ranges, mass ranges, torque limits, percentage variations for some robots).",
            "sim_to_real_gap_factors": "Simulator engine differences (DART vs MuJoCo) leading to physics discrepancies; actuator modeling mismatch (torque/power limits), sensor/actuation latency, terrain geometry differences (flat vs stepped), and unmodeled dynamics.",
            "transfer_enabling_conditions": "Training separate protective policy that generalizes to stable poses, learning OSSE (one-step safety estimator) to predict safety after one π_task action followed by π_protect, conservative two-threshold switching (κ_task, κ_protect), protective adaptation procedure (start conservative, relax thresholds with at most two unsafe trials), dynamics randomization during training, ensemble OSSE for uncertainty smoothing.",
            "fidelity_requirements_identified": "Paper emphasizes that actuator dynamics and sensor latency are key sources of sim-to-real gap (citing prior work) and finds that dynamics randomization alone is insufficient; no strict quantitative fidelity tolerances are provided, but accurate actuator/latency modeling and accounting for torque/power limits and terrain are highlighted as important.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Protective Policy Transfer enables successful adaptation across substantial simulation gaps (DART→MuJoCo, added latency, actuator power limits, terrain changes) with improved task returns and fewer unsafe trials compared to baselines; dynamics randomization alone is insufficient, and safety-aware switching via OSSE plus a conservative adaptation minimizes failed trials while recovering task performance.",
            "uuid": "e1660.0",
            "source_info": {
                "paper_title": "Protective Policy Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Dynamics Randomization (DR)",
            "name_full": "Dynamics Randomization",
            "brief_description": "A common sim-to-real technique that trains controllers with randomized dynamics parameters so policies become robust to variations between simulation and target environments; used here both within the proposed method and as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_system_name": "Same set of simulated legged robots (Hopper, HalfCheetah, Walker2D, A1) used in paper's experiments",
            "agent_system_description": "Legged robots trained under randomized dynamics to improve robustness to environment changes.",
            "domain": "robotics — policy robustness for locomotion",
            "virtual_environment_name": "DART (training) with parameter randomization",
            "virtual_environment_description": "Physics simulator where dynamics parameters (friction, restitution, mass, joint damping, torque limits, etc.) are sampled from ranges during training.",
            "simulation_fidelity_level": "Moderate fidelity rigid-body simulation augmented by randomized parameter regimes to improve robustness.",
            "fidelity_aspects_modeled": "Key rigid-body properties and contact interactions were varied across samples (friction, restitution, mass/inertia, joint damping/stiffness, torque limits).",
            "fidelity_aspects_simplified": "Does not explicitly model certain actuator-specific behaviors (e.g., speed-dependent torque/power limits), does not guarantee matching of simulator-specific integrator/contact solver differences (DART vs MuJoCo), and training did not include added latency that was present in target tests.",
            "real_environment_description": "Target was a different simulator (MuJoCo) with added latencies and altered actuator/terrain properties; no physical robot testing reported for DR in this paper.",
            "task_or_skill_transferred": "Locomotion controllers trained with domain randomization intended to transfer to altered simulator conditions.",
            "training_method": "Deep reinforcement learning (PPO) with randomized dynamics during training.",
            "transfer_success_metric": "Mean task return in target environment and number of failed/unsafe trials during testing/adaptation.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Baseline DR performance in target simulator (MuJoCo) reported in Table III: Hopper 402.0, HalfCheetah 290.2, Walker2D 1000.4, A1 475.18 (mean returns); DR and DR-RE noted to have zero failed trials during 'adaptation' because they do not perform adaptation.",
            "transfer_success": false,
            "domain_randomization_used": true,
            "domain_randomization_details": "Same parameter ranges as used for training in the paper (see Table II): friction, restitution, mass ranges, joint damping, torque limits, etc.; some parameters randomized as absolute ranges and others as percentages of defaults for particular robots.",
            "sim_to_real_gap_factors": "Even with DR, simulator-to-simulator discrepancies (integrator/contact solver/actuator models), actuator power limits, and added latencies can prevent successful transfer; DR alone produced much lower task returns in many test cases.",
            "transfer_enabling_conditions": "DR provides robustness across a distribution of dynamics but was not sufficient by itself for high performance across the large DART→MuJoCo gaps studied; combining DR with safety-aware adaptation (Protective Policy Transfer) enabled better outcomes.",
            "fidelity_requirements_identified": "Implicitly identified as insufficient when DR is the only mitigation; the paper cites actuator dynamics and latency as important fidelity aspects to include for successful transfer.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Dynamics randomization improves robustness but is not sufficient for high-quality transfer across large simulator/modeling gaps (e.g., DART→MuJoCo, actuator/power/latency mismatches); safety-aware adaptation and protective policies are effective complementary techniques.",
            "uuid": "e1660.1",
            "source_info": {
                "paper_title": "Protective Policy Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "OSSE",
            "name_full": "One-Step Safety Estimator (OSSE)",
            "brief_description": "A predictive ensemble model trained to estimate the safety level of taking one action from the task policy followed by the protective policy, used to decide online switching between task and protective policies to reduce unsafe trials during transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Applied to the same legged robots (Hopper, HalfCheetah, Walker2D, A1) in experiments",
            "agent_system_description": "Estimator acts on the observation (o) to predict safety value used for policy switching; implemented as an ensemble of neural nets averaged for final prediction.",
            "domain": "robotics — safe policy adaptation for locomotion",
            "virtual_environment_name": "Trained in source simulator (DART) and used during adaptation in target simulator (MuJoCo)",
            "virtual_environment_description": "Predictive model that uses simulated rollouts collected in source environment to learn expected protective-policy value after a one-step task-policy action.",
            "simulation_fidelity_level": "Model-based estimator trained on simulated rollouts; fidelity depends on fidelity of source simulator and noise injected during data collection.",
            "fidelity_aspects_modeled": "Predicts value of protective policy (V_protect) after hypothetical one-step transitions; ensemble captures epistemic uncertainty from training data sampled with noise.",
            "fidelity_aspects_simplified": "OSSE is learned from source-simulated transitions and therefore may not fully capture target-simulator or real-world dynamics; it is an approximate predictor rather than a dynamics simulator.",
            "real_environment_description": "Used at adaptation time in the target simulator (MuJoCo) to decide switching and reduce unsafe trials; intended for safe sim-to-real deployment though not tested on physical robots here.",
            "task_or_skill_transferred": "Enables safer execution of transferred locomotion skills by switching between task and protective policies.",
            "training_method": "Supervised regression training of ensemble neural networks using rollouts generated by π_task and π_protect in the source environment (see Algorithm 1).",
            "transfer_success_metric": "Improvement measured by higher task returns and reduced number of unsafe trials relative to variants without OSSE (NO-OSSE) and baselines; comparisons reported in Table III and Figures.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "OSSE trained on data collected from π_task rollouts with added noise and from π_protect rollouts initialized from those states; ensemble of 3 models used.",
            "sim_to_real_gap_factors": "OSSE can over-estimate safety if trained only on V_protect values (i.e., not accounting for one-step π_task action), leading to unsafe switches; OSSE addresses this by predicting one-step outcomes but still relies on similarity between source and target state distributions.",
            "transfer_enabling_conditions": "Ensemble modeling for uncertainty smoothing, training OSSE on states visited by noisy π_task to cover out-of-distribution scenarios, using two-threshold hysteresis switching and conservative adaptation with at most two unsafe trials.",
            "fidelity_requirements_identified": "OSSE performance depends on representativeness of source-rollout data for states likely to be encountered in target environment; if π_protect generalizes better than π_task then OSSE predictions are more reliable.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "OSSE improves safe transfer by predicting the safety consequence of taking one π_task action before switching to π_protect; replacing OSSE with V_protect (NO-OSSE) worsened outcomes on less stable robots, showing OSSE’s importance for realistic safety estimation during transfer.",
            "uuid": "e1660.2",
            "source_info": {
                "paper_title": "Protective Policy Transfer",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Sim-to-real transfer for biped locomotion",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_for_biped_locomotion"
        },
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 1,
            "sanitized_title": "learning_agile_and_dynamic_motor_skills_for_legged_robots"
        },
        {
            "paper_title": "Learning locomotion skills for cassie: Iterative design and sim-to-real",
            "rating": 1,
            "sanitized_title": "learning_locomotion_skills_for_cassie_iterative_design_and_simtoreal"
        }
    ],
    "cost": 0.01323625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Protective Policy Transfer</p>
<p>Wenhao Yu 
C Karen Liu 
Greg Turk 
Protective Policy Transfer</p>
<p>Being able to transfer existing skills to new situations is a key capability when training robots to operate in unpredictable real-world environments. A successful transfer algorithm should not only minimize the number of samples that the robot needs to collect in the new environment, but also prevent the robot from damaging itself or the surrounding environment during the transfer process. In this work, we introduce a policy transfer algorithm for adapting robot motor skills to novel scenarios while minimizing serious failures. Our algorithm trains two control policies in the training environment: a task policy that is optimized to complete the task of interest, and a protective policy that is dedicated to keep the robot from unsafe events (e.g. falling to the ground).To decide which policy to use during execution, we learn a safety estimator model in the training environment that estimates a continuous safety level of the robot. When used with a set of thresholds, the safety estimator becomes a classifier for switching between the protective policy and the task policy. We evaluate our approach on four simulated robot locomotion problems and a 2D navigation problem and show that our method can achieve successful transfer to notably different environments while taking the robot's safety into consideration.</p>
<p>I. INTRODUCTION</p>
<p>When performing motor skills in a novel environment, humans tend to act more conservatively at the beginning to avoid potential dangers and gradually take more agile movements to improve our performance as we get more familiar with the situation. This capability of temporarily shifting the focus between task completion and self protection allows us to transfer our existing skills to scenarios we have not seen before in a protective way and is crucial for us to survive in the ever-changing and unpredictable real-world environment.</p>
<p>This work aims to develop computational tools that allow a robot to also adapt its motor skills to a new scenario safely. We focus on the practical problem of legged locomotion because legged robots are often prone to falling to the ground and damaging themselves due to the fast movements and intricate balance that is involved. Being able to transfer legged locomotion skills safely to new environments is a crucial step towards deploying robots in real-world applications.</p>
<p>There has been extensive research in transferring control policies from one domain to another [1]- [4] as well as incorporating safety in robotic control [5]- [8]. However, few of them address both safety and transfer. Existing methods in safety usually considers learning a policy that satisfies certain safety constraints such as velocity limit and obstacle avoidance. However, the optimized policies usually do not transfer easily to novel situations. On the other hand, current transfer learning algorithms often require additional data collection to adapt the policy to new environments, but do not consider the safety issue during the process of adaptation.</p>
<p>In this work, we introduce a transfer learning algorithm that takes the safety of the robot into consideration. We assume that the robot is first trained in a simulated environment, where it can explore different actions and potentially dangerous states. Our transfer learning algorithm consists of four components. We first train a task policy π task to achieve optimal performance for the task. Based on states visited by π task , we train a protective policy π protect that protects the robot from unsafe states such as falling on the ground. To determine which policy to use during execution, we learn a safety estimator model ψ that estimates a continuous safety level of the robot. Training of these models is performed in the source environment. Finally, we develop an adaptation procedure to transfer the learned models from source to target environment with at most two unsafe trials.</p>
<p>We evaluate our method of transferring locomotion skills to novel environments for four different robots, as shown in Figure 1. For each robot, we design a variety of testing scenarios inspired by prior work in sim-to-real transfer [1], which creates non-trivial gaps between the training and testing environments of the robot. We demonstrate that our method can successfully complete the task while minimizing the number of failed trials in novels environments, while baseline methods do not achieve successful transfer or require a higher number of failed trials.</p>
<p>II. RELATED WORK</p>
<p>Transferring skills learned in a source environment such as computer simulation to novel real-world scenarios is an important direction for creating robots that can be deployed in real-world applications. Recent advances in transfer learning for reinforcement learning have demonstrated some success in deploying simulation-trained policies to real physical robots [1]- [3], [9]- [13]. One of the key strategies to achieve successful policy transfer is to improve the simulation accuracy by performing system identification [1], [11]- [15]. For example, Tan et al. identified key aspects to the discrepancy between simulated and real robot dynamics such as actuator dynamics and sensor latency [1]. By properly measuring these factors and modeling them in the computer simulation, they achieved successful sim-to-real transfer for a real quadruped robot.</p>
<p>Complementary to improving the simulation fidelity, researchers have also developed algorithms to train policies that can work for a large variety of scenarios. One class of these algorithms is to fine-tune the simulation-trained controller with a limited amount of data on the real-robot [2], [3], [9], [16], [17]. For example, Yu et al. proposed to learn a parameterized controller with a latent space in simulation and optimize the latent input directly on the real hardware [9]. Allowing controllers to be fine-tuned using real-world data greatly improves their adaptation performance in novel situations. However, the safety of the robot during real-world experiments is not considered in existing methods. As a result, extra supervision is needed from the experimenter. Our method improved upon the prior methods in this category by considering the safety of the robot during adaptation.</p>
<p>Researchers have also proposed policy optimization methods that takes the safety of the robot into consideration. Some of existing methods enforce the safety of the robot using a model-based approach [7], [8], [18]- [22]. Given an initial safe but sub-optimal policy, Garcia et al. [18] and Berkenkamp et al. [8] proposed exploration schemes that are provably safe around the current policy and used them to gradually expand the operation region for the policy. Despite being able to provide guarantees on the safety, these methods usually assume the availability of the system dynamics, which may not be true in real-world problems.</p>
<p>Alternatively, researchers have proposed model-free methods for learning policies that satisfy certain safety constraints [5], [6], [23]- [27]. For example, Achiam et al. introduced a general DRL algorithm, Constrained Policy Optimization (CPO), that solves a constrained MDP problem [5]. They demonstrated CPO on simulated continuous control tasks with state constraints. Eysenbach et al. proposed to jointly train a policy to complete the task and a policy to reset the robot to the initial state when the robot is in an unsafe state [27]. Although these methods can obtain controllers that respects safety constraints in the limit, during the learning process the robot may still be exposed to risky states.</p>
<p>In addition to enforcing general safety constraints, researchers have also investigated more task-dependent safety problems. For example, for humanoid robots, researchers have devised specialized algorithms to reduce the damage they receives during falling [28]- [31]. Though effective in handling the falling scenario, these methods may not generalize to other types of failure modes.</p>
<p>III. METHODS</p>
<p>A. Training a task policy</p>
<p>We first train a task policy π task that performs the desired motor skill using deep reinforcement learning (DRL). We formulate the motor skill learning problem as a Markov Decision Process (MDP), M = (S, A, R, T , p 0 , γ), where S is the state space, A is the action space, R : S × A → R is the reward function, T : S × A → S is the transition function, p 0 is the initial state distribution and γ is a discount factor. In DRL, a policy π represented by a neural network is searched to maximize the expected return:  [32] to optimize the motor skill policies, though our algorithm is agnostic to the specific choice of DRL algorithm. During training of π task , we randomize the dynamics of the simulated robot, also known as Dynamics Randomization (DR) [14], to improve its robustness. However, as we show in our experiments, using DR alone is not sufficient to achieve successful transfer.
J task = E τ =(s0,a0,...,s T ) T t=0 γ t R(s t , a t ),(1)where s 0 ∼ p 0 , a t ∼ π(s t ) and s t+1 = T (s t , a t</p>
<p>B. Training a protective policy</p>
<p>To prevent potential damage to the robot, we train a protective policy π protect that is dedicated to keeping the robot within a set of safe states S saf e . The boundary of S saf e can be precisely defined by general inequality constraints on state variables. For example, we can reuse the inequality constraints defined for early termination conditions as the boundary of S saf e . Concretely, in a locomotion problem, a state where the robot falls to the ground will terminate the rollout and is considered outside of S saf e . To train π protect , we design a reward function: R protect (s, a) = w alive r alive + w action r action + w task r task , where r alive is a constant reward given to the robot, s t ∈ S saf e , r action = ||a|| 2 2 is a regularization term for limiting the magnitude of the actions, and r task = min(R task (s, a), 0) discourages the policy from exhibiting behaviors that hurt the desired task. w alive , w action , w task modulate the relative importance of the reward terms. Note that r task is always negative and only penalizes the agent when it is acting in opposition to the task, e.g. if the robot walks backward when the task is to move forward. We find this helps us train protective policies that are more compatible with the task policies. Section IV-A describes the weights and the S saf e for different problems demonstrated in this work.</p>
<p>π protect needs to work for all states that the robot might encounter during adaptation. However, we do not know beforehand what states the real robot is going to visit. In our work, we approximate this state distribution by running π task with added noise in the source environment to collect Algorithm 1 Learning OSSE 1: Randomly initialize weights ψ for OSSE 2: Obtain the value function V protect of π protect 3: Run π task for L rollouts and store states in S 4: Run π protect for M rollouts from states drawn from S 5: Store the generated states s t to a buffer B 6: for each s t in B do 7: Run π task from s t and get (o t , o t+1 ) 8: Add o t to training inputs T input 9:</p>
<p>Add V protect (o t+1 ) to training labels T label 10: end for 11: Optimize ψ with T input and T label 12: return ψ a set of states S that are relevant to the task. We then use S as the initial state distribution when training π protect .</p>
<p>An important assumption we make is that π protect not only minimizes the potential damage to the robot, but also generalizes to novel environments better than π task . For the locomotion tasks, this is a reasonable assumption because π protect is specialized to prevent the robot from falling over and often learns to take the robot to stable poses such as standing upright, which are usually less sensitive to different dynamics. We validate this assumption in Section IV-F. We do note that for other transfer problems, this assumption may not always hold. However, our proposed framework of protective policy transfer is agnostic to how π protect is obtained as long as it satisfies the generalization assumption.</p>
<p>C. Training a one-step safety estimator π task outputs actions that optimize the task reward but do not transfer to unseen environments. On the other hand, π protect generates actions that keep the robot safe and can generalize to novel scenarios, yet do not accomplish the desired task. Can we combine them to achieve protective policy transfer in the target environment? Naively interpolating the actions from the two policies is likely to result in suboptimal behaviors that are neither functional nor safe. In this work, we propose to combine the task and protective policies by switching between π task and π protect based on an estimation of how "safe" the robot is: we use π task if the robot is sufficiently safe, and use π protect otherwise. A natural way to perform this estimation is to use the value function V protect from π protect as it measures the expected return of the protective policy. However, V protect may not be sufficient for ensuring safety during transfer. In particular, because V protect only considers actions from π protect and does not take π task into consideration, a state that is considered safe by V protect may not be safe anymore after taking an action from π task . This will cause over-estimation of the safety for the robot, leading to unsafe behaviors. To address this issue, we train a predictive model that computes the safety estimation for a state assuming that we take one step from π task and then follow π protect . We call this new model the one-step safety estimator (OSSE). A more detailed description of the algorithm can be found in Algorithm 1.</p>
<p>Algorithm 2 Protective adaptation algorithm 1: Input: π task , π protect , ψ, ∆, κ min 2: Initialize κ task = 1.0, κ protect = 1.0, κ * task = 1.0, κ * protect = 1.0, R * = −∞ 3: while κ task &gt; κ min do 4: Evaluate performance R of π combine</p>
<p>5:</p>
<p>if R &gt; R * then 6:
κ * task = κ task , R * = R 7: end if 8:
if robot is unsafe then 9: κ task = κ task + ∆ if robot is unsafe then 20: κ protect = κ protect + ∆ 
κ protect = κ protect − ∆ 24: end while 25: return κ * task , κ * protect</p>
<p>D. Protective adaptation to testing environment</p>
<p>The trained OSSE provides a continuous estimation ψ(o) for the safety-level of the robot. To make a binary decision for which policy to use, we further threshold the OSSE predictions to produce a binary output. We find that using a single threshold to perform the policy selection often leads to oscillations between the two policies, causing the robot to go to states that are unfamiliar to either policies. To mitigate this issue, we define two thresholds κ task and κ protect and classify the robot into one of two operation modes: protect and task. The operation mode of the robot is updated by the OSSE prediction and the thresholds: if the robot is in the protect mode and ψ(o) &gt; κ protect , it means the robot is in a sufficiently safe state and we switch to task mode. Similarly, if the robot is in the task mode and ψ(o) &lt; κ task , then the robot is not safe anymore and should switch to protect mode. The resulting policy as well as the operation model update rule is shown as follows:
π combine (o, m) = π protect (o), if m = protect π task (o), if m = task,(2)
m = protect, if ψ(o) &lt; κ task and m = task task, if ψ(o) &gt; κ protect and m = protect.</p>
<p>Manually specifying the values for κ protect and κ task can be challenging: for different control problems and robots, the optimal thresholds can be different. One possible way to find the thresholds is to optimize them in the source environment. However, this will likely lead to a policy that exclusively uses the task policy since it maximizes the task reward in the source environment. We propose a protective adaptation algorithm that automatically finds the values for κ protect and κ task with at most two unsafe trials in the target environment. Our core idea is to start with the most conservative combination (using π protect only) and gradually relax the thresholds to allow the task policy to take actions when the robot is safe. Specifically, we first set κ protect = κ task = 1 and gradually reduce κ task until the robot encounters the first unsafe trial. This step finds the boundary where π task stops transfer to the target environment and needs assistance from π protect . We then perform a second phase of search where we fix the optimized κ task and gradually reduce κ protect until the resulting policy leads the robot into an unsafe state. After the search, we will return the κ task and κ protect that achieve the best task performance. Algorithm 2 describes our protective adaptation algorithm in greater details.</p>
<p>IV. RESULTS</p>
<p>A. Experimental Setup</p>
<p>We evaluate our algorithm on four challenging locomotion problems with simulated legged robots, as shown in Figure 1. The first three robots: Hopper, HalfCheetah, and Walker2D, are designed based on OpenAI Gym [33]. For the fourth example, we use a 3D quadruped robot, the Unitree A1 (Figure 1(d)). A1 is equipped with 12 motors: two at each hip joint and one for each knee joint. The motors are controlled by Proportional Derivative (PD) controllers with desired motor anglesq as input. We train locomotion policies that outputq and constrain the change of each dimension inq between consecutive timesteps to be smaller than 0.2 for encouraging smooth motion. In all examples, the policies observe the IMU sensor data, motor angles, and motor velocities of the robot. We design the following task reward function tor training locomotion skills: R task (s, a) = 1 + w vel min(ẋ, v max ) − w action ||a|| 2 2 − w knee r knee − w hip |q hip | − w dev |y|, where v max caps the velocity reward and prevents the robot from exploiting unrealistic movements to achieve fast moving speed, r knee is a binary reward denoting whether the knee joint of the robot is at the limit, q hip is the motor angle for the hip abduction and adduction, which discourages the A1 robot to walk with its legs spread or crossed, and y is the deviation in the left-right direction. Table I lists the rewards parameters used in our experiments. For training the protective policy π protect in all tasks, we use R protect (s, a) = 4.0+0.3||a|| 2 2 − 0.2 min (R task (s, a), 0). During training, we perform dynamics randomization [14] to improve the robustness of the policies. Table II summarizes the parameters and the range we use for domain randomization for all four tasks. Note that for HalfCheetah and A1 we randomize some parameters using percentage of the default values instead of absolute values. This is to avoid un-physical settings during randomization for these robots.</p>
<p>We define the robot to be safe when it is not falling to the ground or losing balance. Concretely, we specify the safety state set based on three conditions: 1) the robot is allowed to touch the ground only with its feet, 2) the height of the robot should be higher than h min , and 3) the roll, pitch, and yaw (ψ, θ, φ) of the robot base should not exceed ψ max , θ max , φ max . For Hopper, we set h min = 0.75 and θ max = 0.8. For Walker2D, we set h min = 0.8 and θ max = 1.0. For HalfCheetah, we set h min = 0.0 and θ max = 1.3. And for A1, we have h min = 0.1, ψ max = 0.4, θ max = 0.6, and φ max = 1.0. These conditions are also used as the terminal condition during training, which is commonly done in locomotion learning.</p>
<p>We use Proximal Policy Optimization (PPO) [32] for training both π task and π protect . We represent both policies using neural networks with two fully-connected layers, each with 64 neurons. In our implementation, we use an ensemble of three models for both V protect and OSSE, where each model is represented by a three-layer neural network with 256, 128, and 64 neurons respectively. The final estimated safety level of OSSE is computed as the average of the three models in the ensemble. We use tanh activation for all models. For each method, we run five random seeds and report mean and one standard deviation for the performance. Figure 2 illustrates some of the results by our algorithm for the Hopper and A1 robot in the testing environment. The results can be better seen in the supplementary video.</p>
<p>B. Transfer tasks</p>
<p>We design a variety of testing environments for each robot to evaluate the ability of our algorithm to transfer to new situations. For Hopper, Walker2d, and HalfCheetah, we train the policies in one physics simulator, DART [34], and transfer them to a different simulator, MuJoCo [35]. As Fig. 2. Example sequences of our approach for the Hopper (top) and A1 (bottom) robots. Green bodynode means that π task is being used while red bodynodes means πprotect is being used. Fig. 3. Comparison with baseline methods for Hopper (left), HalfCheetah (middle), and A1 robot (right) in performance and the number of unsafe trials during transfer. We train each policy for five random seeds (one standard deviation shown as shaded area). DR and DR-RE have zero failed trial count since they do not perform adaptation. Our method and NO-OSSE both allow two failed trials. discussed in prior work, the discrepancies between DART and MuJoCo presents significant challenge in transferring policies from one to the other [36]. Furthermore, we add latency of 8ms, 50ms, and 16ms to the testing environments of the three robots while no latency is added during training.</p>
<p>For the A1 robot, we build the simulated robot in DART and create two gaps between the training and testing environments. First, we introduce a power limit to each motor in the testing environments. The power for each robot is measured by τq, where τ is the torque applied by the motor. This effectively creates a speed-dependent torque limit. As shown in prior work [1], [15], mismatch in the actuator modeling is one of the major sources of the "Reality Gap". Second, the terrain of the testing environment is modified to be a sequence of steps (0.5m × 0.03m) going upward, while the training environment uses a flat ground.</p>
<p>To better evaluate the transfer performance of different algorithms, we further vary the physical properties during the evaluation in addition to the gaps introduced above. For the Hopper task, we vary the torso mass between  </p>
<p>C. Baseline methods</p>
<p>We compare our method to four baseline methods. 1) Dynamics Randomization (DR): a commonly used simto-real technique where a robust policy is trained with randomized dynamics.</p>
<p>2) Dynamics Randomization with Reward Engineering (DR-RE): we increase the alive bonus reward by four times to encourage more robust behaviors.</p>
<p>3) Safe-Bayes: we train π task , π protect , and OSSE the same way as in our algorithm and apply Bayesian Optimization to search for the thresholds κ saf e and κ task instead of using our protective adaptation algorithm. 4) Constrained Policy Optimization (CPO): a safe reinforcement learning algorithm by Achiam et al. , which solves a task policy that satisfies the defined safety constraints [5].</p>
<p>We further ablate our proposed method by using the V protect instead of OSSE for estimating the safety level of the robot, which is denoted as NO-OSSE.</p>
<p>D. Does our method outperform baseline methods?</p>
<p>We report the mean task performance of different algorithms in the target environments, as summarized in Table III. Since the policies are tested on different dynamics parameters during testing, we further plot the policy performance with respect to the varied parameters in Figure 3 and Figure  4. As shown in these results, our proposed method achieves notably better task performance than DR, DR-RE, and CPO. This shows the importance of adapting the policy in a novel testing environment. We note that it is possible to fine-tune these policies in the testing environment directly using DRL algorithm. However, we did not perform this test because 1) the fine-tuning process will likely introduce many unsafe trials and 2) prior work demonstrated that fine-tuning a DR policy is not very effective when the environment gap is large [36]. Meanwhile, applying Bayesian Optimization to find  κ task and κ saf e (Safe-Bayes) can in general achieve better task performance than our algorithm. This suggests that the shifting between task performance and robot protection is not simply monotonic as our algorithm hypothesize. However, as shown in Figure 3 and Table III, Safe-Bayes results in a higher number of failed trials during adaptation. In our experiments, we allow at most 20 trials during Bayesian Optimization in Safe-Bayes. We have also examined Safe-Bayes using different sample allowance and find that our method achieves better performance when the same budget of unsafe trials is allowed.</p>
<p>In addition, when the OSSE model is replaced with V protect (NO-OSSE), it results in worse performance for Hopper and Walker2D, but comparable results for HalfCheetah and A1. We hypothesize that this is because when the robot is less stable, it will rely more heavily on taking actions from π protect to remain safe. As a result, a state that has high score from V protect may not be safe when we take an action from π task , leading to lower task reward. On the other hand, for robots that are structurally more stable, V protect can provide a more reasonable estimation of the robot's safety, leading to similar performance between using OSSE and V protect .</p>
<p>E. Does our transfer scheme make reasonable policy assignment?</p>
<p>In this section, we examine the policy assignment strategy that emerges from our method. We apply our method on the MuJoCo Hopper with torso weight of 2.5 kg and record the observations and the policy selection. We then train a logistic regression model to predict the policy selection from the observations. The logistic model shows that torso pitch and forward velocity of the Hopper are the two most predictive features for the policy assignment ( Figure 5 (a)). For better visualization, we scale down the values for torso pitch and forward velocity. Our policy tends to choose π protect when the forward velocity is high and is tilting forward. This makes sense because the combination of high forward velocity and forward tilting indicates that the robot might fall forward. Figure 2 shows some examples of our trained policies transferred to the testing environments. The resulting scheduling can be better seen in the supplementary video.</p>
<p>F. Generalization Comparison between π task and π protect</p>
<p>A key assumption we make in our method is that π protect can generalize to new scenarios better than π task . To validate this, we use two metrics to compare π protect and π task . The first one is how long the policy keeps the robot safe. To compute this metric, we run both policies for 200 rollouts and compute the average rollout lengths in the testing environments. We normalize the lengths using the maximal rollout length (1000). The results can be found in the left of Table  IV. The second metric we use is the ratio of the policy return between testing and training environments, where the return corresponds to the training reward of the policies. We report the average of return ratio over 200 rollouts. The results is shown in the right half of Table IV. We sample source and target environment from the same variations described in Section IV-A when measuring both metrics. We can see that π protect achieves better performance in both metrics. This is likely because π protect is trained to specialize in staying within safe states. As a result, π protect learns to take the robot to stable states such as standing upright, which is less sensitive to different dynamics.</p>
<p>V. DISCUSSION AND CONCLUSION</p>
<p>We have presented a transfer learning algorithm for protectively adapting a control policy to novel scenarios. Our algorithm trains three models in the source environment: a task policy that optimizes the task reward, a protective policy that protects the robot form unsafe states, and a safety estimation model for assessing the safety-level of the robot. To transfer these models to novel testing scenarios, we introduce a protective adaptation algorithm that minimizes the unsafe trials during adaptation. We demonstrate that our algorithm can overcome large modeling errors for four different simulated transfer problems and achieve better performance than baseline methods in safely adapting the policy to new environments.</p>
<p>Our algorithm achieves protective policy transfer at the cost of exhibiting conservative behaviors with suboptimal task performance. An important future direction is thus to improve the task performance of our algorithm while keeping the unsafe trials low during transfer. Another interesting future direction is to extend our approach to more diverse control problems such as navigation or manipulation.</p>
<p>Fig. 1 .
1The four tasks used in our experiments: (a) Hopper, (b) HalfCheetah, (c) Walker2D, and (d) 3D Quadruped. Our algorithm trains policies in the source environments (Top row) and transfer them to target environments while minimizing damage to the robot (Bottom row).</p>
<p>κ
task = κ task − ∆ 13: end while14:  while κ protect &gt; κ task do</p>
<p>[2.0, 7.0] kg. For the HalfCheetah task in MuJoCo, we vary the restitution coefficient of the ground in [0.5, 1.0]. For the Walker2D task, we vary the mass of one foot in [2.0, 15.0] kg. And for the A1 robot, we vary the power limit in [50, 100].</p>
<p>Fig. 4 .
4Comparison of our method and the baseline methods for the Walker2D example.</p>
<p>Fig. 5 .
5Visualization of one rollout of Hopper task where the blue solid curve represents the policy selection and the dashed curves are the scaled torso pitch and forward velocity of the robot.</p>
<p>TABLE I TASK
IREWARD FUNCTIONS.Hopper HalfCheetah Walker2D 
A1 </p>
<p>vmax 
∞ 
∞ 
∞ 
2.5 
wvel 
1 
1 
1 
3 
waction 
0.03 
0.5 
0.05 
0.025 
wknee 
0.5 
0.5 
0 
0 
whip 
-
-
-
1 
wdev 
-
-
-
5 </p>
<p>TABLE II DOMAIN
IIRANDOMIZATION SETTING.Hopper 
HalfCheetah 
Walker2D 
A1 </p>
<p>Friction 
[0.2, 1.0] 
[0.4, 1.5] 
[0.2, 1.0] 
[0.4, 1.5] 
Restitution 
[0.0, 0.3] 
[0.0, 2.0] 
[0.0, 0.8] 
[0.0, 0.8] 
Mass 
[2.0, 10.0]kg 
[60, 150]% 
[2.0, 10.0]kg [60, 150]% 
Joint damping 
[0.5, 3.0] 
[70, 130]% 
[0.01, 0.2] 
-
Joint Stiffness 
-
[70, 130]% 
-
-
Torque limit 
[100, 300]Nm 
[60,150]% 
-
[15,25]Nm 
Position gain 
-
-
-
[60, 150]% </p>
<p>TABLE III PERFORMANCE
IIIOF DIFFERENT METHODS FOR ALL EXAMPLES. NUMBERS IN PARENTHESIS IS THE AVERAGE NUMBER OF FAILED TRIALS DURING ADAPTATION.Ours 
DR 
DR-RE NO-OSSE CPO 
Safe-Bayes </p>
<p>Hopper 
1700.5 
402.0 
681.1 
1081.4 
966.4 
2092.8 (7) 
HalfCheetah 1402.2 
290.2 
538.1 
1389.6 
193.2 
1536.9 (9.2) 
Walker2D 
1656.4 1000.4 1339.1 
1383.4 
626.4 
1947 (9.3) 
A1 
1281.6 475.18 
550.9 
1245.6 
361.3 1615.75 (10.3) </p>
<p>TABLE IV
IVGENERALIZATION COMPARISON FOR TASK AND PROTECTIVE POLICY.Normalized rollout length 
Return transfer ratio </p>
<p>Hopper Cheetah Walker Hopper Cheetah Walker </p>
<p>πtask 
0.15 
0.3 
0.17 
0.29 
0.14 
0.39 
πprotect 
0.75 
1.0 
0.97 
0.99 
0.94 
0.95 </p>
<p>Georgia Institute of Technology, Atlanta, Georgia, USA {wenhaoyu,turk}@cc.gatech.edu</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsPittsburgh, PennsylvaniaJ. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bo- hez, and V. Vanhoucke, "Sim-to-real: Learning agile locomotion for quadruped robots," in Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018.</p>
<p>Learning fast adaptation with meta strategy optimization. W Yu, J Tan, Y Bai, E Coumans, S Ha, arXiv:1909.12995arXiv preprintW. Yu, J. Tan, Y. Bai, E. Coumans, and S. Ha, "Learning fast adaptation with meta strategy optimization," arXiv preprint arXiv:1909.12995, 2019.</p>
<p>Learning agile robotic locomotion skills by imitating animals. X B Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, arXiv:2004.00784arXiv preprintX. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, and S. Levine, "Learning agile robotic locomotion skills by imitating animals," arXiv preprint arXiv:2004.00784, 2020.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, Intelligent Robots and Systems (IROS). J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017, pp. 23-30.</p>
<p>Constrained policy optimization. J Achiam, D Held, A Tamar, P Abbeel, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70J. Achiam, D. Held, A. Tamar, and P. Abbeel, "Constrained policy optimization," in Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 22-31.</p>
<p>Reward constrained policy optimization. C Tessler, D J Mankowitz, S Mannor, International Conference on Learning Representations. C. Tessler, D. J. Mankowitz, and S. Mannor, "Reward constrained policy optimization," in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/ forum?id=SkfrvsA9FX</p>
<p>Control barrier functions: Theory and applications. A D Ames, S Coogan, M Egerstedt, G Notomista, K Sreenath, P Tabuada, 2019 18th European Control Conference (ECC). IEEEA. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath, and P. Tabuada, "Control barrier functions: Theory and applications," in 2019 18th European Control Conference (ECC). IEEE, 2019, pp. 3420-3431.</p>
<p>Safe model-based reinforcement learning with stability guarantees. F Berkenkamp, M Turchetta, A Schoellig, A Krause, Advances in neural information processing systems. F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, "Safe model-based reinforcement learning with stability guarantees," in Advances in neural information processing systems, 2017, pp. 908- 918.</p>
<p>Sim-to-real transfer for biped locomotion. W Yu, V C Kumar, G Turk, C K Liu, arXiv:1903.01390arXiv preprintW. Yu, V. C. Kumar, G. Turk, and C. K. Liu, "Sim-to-real transfer for biped locomotion," arXiv preprint arXiv:1903.01390, 2019.</p>
<p>Robots that can adapt like animals. A Cully, J Clune, D Tarapore, J.-B Mouret, Nature. 5217553503A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, "Robots that can adapt like animals," Nature, vol. 521, no. 7553, p. 503, 2015.</p>
<p>Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R Ribas, arXiv:1910.07113arXiv preprintI. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas et al., "Solving rubik's cube with a robot hand," arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Learning Memory-Based Control for Human-Scale Bipedal Locomotion. J Siekmann, S Valluri, J Dao, F Bermillo, H Duan, A Fern, J Hurst, Proceedings of Robotics: Science and Systems, Corvalis. Robotics: Science and Systems, CorvalisOregon, USAJ. Siekmann, S. Valluri, J. Dao, F. Bermillo, H. Duan, A. Fern, and J. Hurst, "Learning Memory-Based Control for Human-Scale Bipedal Locomotion," in Proceedings of Robotics: Science and Systems, Cor- valis, Oregon, USA, July 2020.</p>
<p>Learning locomotion skills for cassie: Iterative design and sim-to-real. Z Xie, P Clary, J Dao, P Morais, J Hurst, M Van De Panne, ser. Proceedings of Machine Learning. Research, L. P. Kaelbling, D. Kragic, and K. SugiuraPMLR100Z. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, and M. van de Panne, "Learning locomotion skills for cassie: Iterative design and sim-to-real," ser. Proceedings of Machine Learning Research, L. P. Kaelbling, D. Kragic, and K. Sugiura, Eds., vol. 100. PMLR, 30 Oct-01 Nov 2020, pp. 317-329. [Online]. Available: http://proceedings.mlr.press/v100/xie20a.html</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE International Conference on Robotics and Automation (ICRA). X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, "Sim-to- real transfer of robotic control with dynamics randomization," in 2018 IEEE International Conference on Robotics and Automation (ICRA).</p>
<p>. IEEE. IEEE, 2018, pp. 1-8.</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 4265872J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter, "Learning agile and dynamic motor skills for legged robots," Science Robotics, vol. 4, no. 26, p. eaau5872, 2019.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the sim-to-real loop: Adapting simula- tion randomization with real world experience," in 2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 8973- 8979.</p>
<p>Single episode policy transfer in reinforcement learning. J Yang, B Petersen, H Zha, D Faissol, International Conference on Learning Representations. J. Yang, B. Petersen, H. Zha, and D. Faissol, "Single episode policy transfer in reinforcement learning," in International Conference on Learning Representations, 2020. [Online]. Available: https: //openreview.net/forum?id=rJeQoCNYDS</p>
<p>Safe exploration of state and action spaces in reinforcement learning. J Garcia, F Fernández, Journal of Artificial Intelligence Research. 45J. Garcia and F. Fernández, "Safe exploration of state and action spaces in reinforcement learning," Journal of Artificial Intelligence Research, vol. 45, pp. 515-564, 2012.</p>
<p>Safe exploration for reinforcement learning. A Hans, D Schneegaß, A M Schäfer, S Udluft, ESANN. A. Hans, D. Schneegaß, A. M. Schäfer, and S. Udluft, "Safe explo- ration for reinforcement learning." in ESANN, 2008, pp. 143-148.</p>
<p>Lyapunov design for safe reinforcement learning. T J Perkins, A G Barto, Journal of Machine Learning Research. 3T. J. Perkins and A. G. Barto, "Lyapunov design for safe reinforcement learning," Journal of Machine Learning Research, vol. 3, no. Dec, pp. 803-832, 2002.</p>
<p>Provably safe and robust learning-based model predictive control. A Aswani, H Gonzalez, S S Sastry, C Tomlin, Automatica. 495A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin, "Provably safe and robust learning-based model predictive control," Automatica, vol. 49, no. 5, pp. 1216-1226, 2013.</p>
<p>End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. R Cheng, G Orosz, R M Murray, J W Burdick, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick, "End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, 2019, pp. 3387-3395.</p>
<p>First order optimization in policy space for constrained deep reinforcement learning. Y Zhang, Q Vuong, K W Ross, arXiv:2002.06506arXiv preprintY. Zhang, Q. Vuong, and K. W. Ross, "First order optimization in policy space for constrained deep reinforcement learning," arXiv preprint arXiv:2002.06506, 2020.</p>
<p>Safe exploration for active learning with gaussian processes. J Schreiter, D Nguyen-Tuong, M Eberts, B Bischoff, H Markert, M Toussaint, Joint European conference on machine learning and knowledge discovery in databases. SpringerJ. Schreiter, D. Nguyen-Tuong, M. Eberts, B. Bischoff, H. Markert, and M. Toussaint, "Safe exploration for active learning with gaussian processes," in Joint European conference on machine learning and knowledge discovery in databases. Springer, 2015, pp. 133-149.</p>
<p>Safe controller optimization for quadrotors with gaussian processes. F Berkenkamp, A P Schoellig, A Krause, 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEEF. Berkenkamp, A. P. Schoellig, and A. Krause, "Safe controller optimization for quadrotors with gaussian processes," in 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016, pp. 491-496.</p>
<p>Safety-guided deep reinforcement learning via online gaussian process estimation. J Fan, W Li, arXiv:1903.02526arXiv preprintJ. Fan and W. Li, "Safety-guided deep reinforcement learning via online gaussian process estimation," arXiv preprint arXiv:1903.02526, 2019.</p>
<p>Leave no trace: Learning to reset for safe and autonomous reinforcement learning. B Eysenbach, S Gu, J Ibarz, S Levine, arXiv:1711.06782arXiv preprintB. Eysenbach, S. Gu, J. Ibarz, and S. Levine, "Leave no trace: Learning to reset for safe and autonomous reinforcement learning," arXiv preprint arXiv:1711.06782, 2017.</p>
<p>Multiple contact planning for minimizing damage of humanoid falls. S Ha, C K Liu, 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEES. Ha and C. K. Liu, "Multiple contact planning for minimizing dam- age of humanoid falls," in 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2015, pp. 2761-2767.</p>
<p>Ukemi: Falling motion control to minimize damage to biped humanoid robot. K Fujiwara, F Kanehiro, S Kajita, K Kaneko, K Yokoi, H Hirukawa, IEEE/RSJ international conference on Intelligent robots and systems. IEEE3K. Fujiwara, F. Kanehiro, S. Kajita, K. Kaneko, K. Yokoi, and H. Hirukawa, "Ukemi: Falling motion control to minimize damage to biped humanoid robot," in IEEE/RSJ international conference on Intelligent robots and systems, vol. 3. IEEE, 2002, pp. 2521-2526.</p>
<p>Towards an optimal falling motion for a humanoid robot. K Fujiwara, S Kajita, K Harada, K Kaneko, M Morisawa, F Kanehiro, S Nakaoka, H Hirukawa, 2006 6th IEEE-RAS International Conference on Humanoid Robots. IEEEK. Fujiwara, S. Kajita, K. Harada, K. Kaneko, M. Morisawa, F. Kane- hiro, S. Nakaoka, and H. Hirukawa, "Towards an optimal falling motion for a humanoid robot," in 2006 6th IEEE-RAS International Conference on Humanoid Robots. IEEE, 2006, pp. 524-529.</p>
<p>Learning a unified control policy for safe falling. V C Kumar, S Ha, C K Liu, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEV. C. Kumar, S. Ha, and C. K. Liu, "Learning a unified control policy for safe falling," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 3940-3947.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Openai gym. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540arXiv preprintG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul- man, J. Tang, and W. Zaremba, "Openai gym," arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Dart: Dynamic animation and robotics toolkit. J Lee, M X Grey, S Ha, T Kunz, S Jain, Y Ye, S S Srinivasa, M Stilman, C K Liu, The Journal of Open Source Software. 322500J. Lee, M. X. Grey, S. Ha, T. Kunz, S. Jain, Y. Ye, S. S. Srinivasa, M. Stilman, and C. K. Liu, "Dart: Dynamic animation and robotics toolkit," The Journal of Open Source Software, vol. 3, no. 22, p. 500, 2018.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, Intelligent Robots and Systems (IROS). E. Todorov, T. Erez, and Y. Tassa, "Mujoco: A physics engine for model-based control," in Intelligent Robots and Systems (IROS), 2012</p>
<p>IEEE/RSJ International Conference on. IEEEIEEE/RSJ International Conference on. IEEE, 2012, pp. 5026-5033.</p>
<p>Policy transfer with strategy optimization. W Yu, C K Liu, G Turk, International Conference on Learning Representations. W. Yu, C. K. Liu, and G. Turk, "Policy transfer with strategy optimization," in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/ forum?id=H1g6osRcFQ</p>            </div>
        </div>

    </div>
</body>
</html>
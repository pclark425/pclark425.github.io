<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3638 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3638</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3638</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-267700596</p>
                <p><strong>Paper Title:</strong> Structured information extraction from scientific text with large language models</p>
                <p><strong>Paper Abstract:</strong> Extracting structured knowledge from scientific text remains a challenging task for machine learning models. Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge. We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/application information extraction. Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects. This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3638.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3638.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer (175B parameters, OpenAI 'davinci' family) pretrained on broad web-scale corpora and fine-tuned in this work to perform sequence-to-sequence joint named-entity-recognition and relation-extraction (NERRE) tasks, producing structured outputs (JSON or templated English) from materials-science text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's autoregressive transformer model (davinci variant, ~175B parameters). In this paper GPT-3 was fine-tuned through the OpenAI API on task-specific prompt→completion pairs (400–650 annotated examples per task), with temperature=0, prompt loss weight=0.01, max token limits 512–1024 for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science / chemistry (paper abstracts and sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Extraction corpus described: a database of >8 million research-paper abstracts; task-specific training used hundreds of annotated prompt→completion pairs (≈400–650 pairs), with task-specific subsets (e.g., 413 doping sentences, ~507 MOF abstracts, ≈650 general-materials entries).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structured scientific relations and heuristics (host↔dopant links, formula→application, material→structure/phase/description associations) rather than formal mathematical laws; i.e., qualitative domain rules and relational knowledge about materials.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning of the pretrained LLM on human-annotated prompt→completion pairs that encode a target schema (JSON or templated English). Human-in-the-loop annotation was used to bootstrap more labeled examples (pre-fill + correction). Models learn to output normalized entities and relations in the chosen schema.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic exact-word-match relation F1 (per-word triplet scoring of entity pairs) plus manual expert scoring (domain-expert 'manual scores' that accept normalized or equivalent extractions). Held-out test sets and cross-validation used; examples: exact-match F1 for doping relations ~0.725–0.809 depending on schema, manual scores for general-task entities substantially higher.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned GPT-3 models successfully extracted complex hierarchical relations from materials abstracts and sentences into structured schemas (JSON / templated English). For the host-dopant task GPT-3 achieved exact-match F1 up to ≈0.809 (DopingExtra-English) and strong manual-scored extraction for general materials fields (manual precision ~0.87 and F1s for many entity types >0.7). Models also performed entity normalization/error correction (e.g., 'Li Co O2' → 'LiCoO2').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Exact-match metrics undercount true performance because the model normalizes or rephrases entities; LLM hallucination (inserting plausible but non-explicit entities), token-length limits producing truncated/unparsable outputs, reliance on proprietary API (reproducibility/security/cost), and variable performance across schemas. Requires a few dozen to a few hundred annotated examples to learn strict structured output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured information extraction from scientific text with large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3638.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3638.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (70 billion parameter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-access large foundation transformer model (Llama-2, 70B) fine-tuned here with parameter-efficient methods (LoRA) to perform the same NERRE structured information extraction tasks on materials-science text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (70B) with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama-2 70B parameter decoder-only transformer, fine-tuned in this work using LoRA (r=8, α=32, LoRA dropout 0.05) and 8-bit quantization on a single A100 GPU. Greedy decoding (temperature=0) used; trained for 4–7 epochs depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science / chemistry (paper abstracts and sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Same extraction corpus context (>8M abstracts available); fine-tuning used task-specific annotated datasets of a few hundred prompt→completion pairs (doping: 413 sentences; MOF: 507 abstracts; general: ≈650 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of qualitative relations/heuristics in materials science (host-dopant relations, MOF name→application/guest species, formula→application/description), i.e., domain relational rules rather than formal physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised LoRA fine-tuning on schema-formatted prompt→completion pairs; human-in-the-loop pre-annotation used to accelerate dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Exact-match per-word relation F1 and manual domain-expert scoring (same metrics as GPT-3 comparisons). Reported numeric comparisons vs GPT-3 and baselines on hold-out test sets and repeated random splits.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Llama-2 models achieved competitive extraction performance though on average 20–30% lower than GPT-3 on the more complex tasks; Llama-2/ Doping-JSON performed best among Llama variants (exact-match F1 ≈0.821 for host-dopant). JSON-output schemas tended to work better for Llama-2 while GPT-3 did better with natural-language schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lower extraction scores versus GPT-3 on some tasks; same hallucination and token-limit issues; fine-tuning and hosting require nontrivial infrastructure and expertise despite being open-source; schema-output validity not strictly enforced (some unparsable outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured information extraction from scientific text with large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3638.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3638.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemPrompt / ChatGPT (Zheng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemPrompt with ChatGPT (prompt-engineering approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prompt-engineering method (ChemPrompt) using ChatGPT to structure text into tables and semi-structured summaries and to collate knowledge from the model's pretraining corpus for chemistry/ materials tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (via prompt-engineering; 'ChemPrompt')</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT-based approach (OpenAI conversational LLM) applied with carefully designed prompts to extract tabular or semi-structured summaries from scientific text and to leverage the model's pretraining knowledge to collate information across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Chemistry / materials science (MOF synthesis and broader chemistry text)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Uses the model's large, unspecified pretraining corpus and applies to downstream sets of scientific papers (Zheng et al. applied approach to MOF-related literature; corpus sizes not quantified in the present paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Collation of existing domain knowledge and semi-structured factual summaries (aggregated domain heuristics), not explicit induction of new formal laws but aggregation of reported relations and knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering / zero- or few-shot prompting of ChatGPT to reformat passages to tabular/semi-structured outputs and to infer or collate related knowledge from the model's pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>In the present paper Zheng et al.'s method is discussed but not demonstrated to extract structured hierarchical relations generalizing outside the model's pretraining corpus; the present paper notes limitations in rigorous evaluation of generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported as capable of structuring text and collating knowledge from pretraining, serving as a practical tool for converting passages into semi-structured summaries; not shown in the referenced work to reliably produce structured hierarchical entity relationships beyond what is present in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential over-reliance on pretraining corpus (may 'collate' rather than extract from given text), generalization outside pretraining uncertain, and ChatGPT can perform poorly on specialized chemistry tasks without elaborate prompt engineering ('tricks').</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured information extraction from scientific text with large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3638.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3638.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unsupervised word embeddings (Tshitoyan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised word-embedding models trained on materials-science literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier approach demonstrating that unsupervised word embeddings trained on large materials-science text corpora can capture latent domain knowledge and correlations (e.g., materials-property relations) emergent from the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised word embeddings capture latent knowledge from materials science literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unsupervised word-embedding models (e.g., word2vec-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Distributional (word-embedding) models trained unsupervised on a large corpus of materials-science text to learn vector representations of terms; analyses show embeddings encode latent material-property associations and domain heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science literature</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Large literature-scale corpus (the cited work trained on many thousands to millions of documents; this paper references it but does not restate precise counts).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Induction of latent qualitative relationships / heuristics (material–property associations) inferred from co-occurrence statistics — i.e., emergent domain rules from distributional patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Unsupervised representation learning (word embeddings) followed by analysis / probing to reveal latent relationships and heuristics encoded in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Probing analyses and validation against known materials knowledge (as reported in the referenced work); in this paper it is cited as an example of latent knowledge capture from corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Serves as a precedent that statistical models trained on literature corpora can recover domain-relevant qualitative relationships; motivates the use of larger LLMs for richer structured-extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embeddings capture statistical co-occurrence rather than causal or mechanistic laws and require downstream interpretation; limited expressivity for hierarchical structured relations compared to seq2seq LLM approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured information extraction from scientific text with large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3638.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3638.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language models as knowledge bases (Petroni et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models as knowledge bases?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A line of work investigating whether pretrained language models encode factual knowledge that can be queried (i.e., treat LMs as implicit knowledge bases), cited here as conceptual background for using LLMs to capture or collate domain knowledge from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained language models as implicit knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approaches and analyses treating pretrained LMs as repositories of factual knowledge that can be extracted via prompting or fine-tuning; often involves probing the model's predictions to recover stored facts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>General natural-language corpora; applied to scientific domains when fine-tuned or prompted appropriately</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Pretraining on large web-scale corpora (not quantified in this paper for the cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Implicit factual knowledge and associations encoded in model parameters; can be used to surface qualitative rules or common-sense relations present in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Probing and targeted prompting / fine-tuning to extract facts and associations from the pretrained model's parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Probing tasks, knowledge probes, comparison to curated knowledge bases; in this paper cited as background for LLMs' capacity to store and reveal domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Provides conceptual justification for treating LLMs as knowledge sources that can be mined or fine-tuned to produce structured domain knowledge; the present work builds on this by fine-tuning to output structured schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Knowledge encoded in parameters may reflect pretraining biases and not specific input text; disentangling pretraining-derived assertions from evidence in a given source document is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured information extraction from scientific text with large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3638.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3638.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-specific LLMs / Darwin series (Xie et al., Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-specific large language models for natural science (Darwin series) and related works on LLMs for chemical discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced recent work fine-tuning or pretraining LLMs on domain-specific scientific corpora to perform Q/A, inverse design, classification, and other tasks relevant to materials and chemistry discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Domain-specific fine-tuned LLMs (e.g., Darwin series; fine-tuned materials/chemistry LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs pretrained on general corpora and then further pre-trained or fine-tuned on large domain-specific scientific text (materials, chemistry) to improve question-answering, design, and extraction capabilities for science tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science and chemistry literature</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Large domain-specific corpora (the cited works report broader pretraining/fine-tuning on many thousands to millions of domain documents; precise counts not given here).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of domain heuristics, Q/A style knowledge, and in some cases use for inverse design — i.e., capturing domain rules and patterns useful for discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain-specific pretraining / fine-tuning of LLMs, sometimes combined with task-specific prompting and supervised examples for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Task-specific evaluations (Q/A, classification, inverse design benchmarks) reported in the cited works; in this paper these works are discussed as related efforts showing LLMs as 'materials science knowledge engines'.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence that LLMs can be adapted to many scientific tasks and may act as knowledge engines for domain discovery; however, these prior works have not necessarily demonstrated robust extraction of complex hierarchical relations in the structured form targeted in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May require large domain corpora for best performance; provenance of extracted assertions and generalization beyond pretraining distribution remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured information extraction from scientific text with large language models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis. <em>(Rating: 2)</em></li>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature. <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Is GPT all you need for low-data discovery in chemistry? <em>(Rating: 2)</em></li>
                <li>Darwin series: Domain specific large language models for natural science. <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3638",
    "paper_id": "paper-267700596",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "GPT-3 (davinci)",
            "name_full": "Generative Pre-trained Transformer 3 (davinci)",
            "brief_description": "A large autoregressive transformer (175B parameters, OpenAI 'davinci' family) pretrained on broad web-scale corpora and fine-tuned in this work to perform sequence-to-sequence joint named-entity-recognition and relation-extraction (NERRE) tasks, producing structured outputs (JSON or templated English) from materials-science text.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci, 175B)",
            "model_description": "OpenAI's autoregressive transformer model (davinci variant, ~175B parameters). In this paper GPT-3 was fine-tuned through the OpenAI API on task-specific prompt→completion pairs (400–650 annotated examples per task), with temperature=0, prompt loss weight=0.01, max token limits 512–1024 for inference.",
            "input_domain": "Materials science / chemistry (paper abstracts and sentences)",
            "corpus_size": "Extraction corpus described: a database of &gt;8 million research-paper abstracts; task-specific training used hundreds of annotated prompt→completion pairs (≈400–650 pairs), with task-specific subsets (e.g., 413 doping sentences, ~507 MOF abstracts, ≈650 general-materials entries).",
            "law_type": "Structured scientific relations and heuristics (host↔dopant links, formula→application, material→structure/phase/description associations) rather than formal mathematical laws; i.e., qualitative domain rules and relational knowledge about materials.",
            "distillation_method": "Supervised fine-tuning of the pretrained LLM on human-annotated prompt→completion pairs that encode a target schema (JSON or templated English). Human-in-the-loop annotation was used to bootstrap more labeled examples (pre-fill + correction). Models learn to output normalized entities and relations in the chosen schema.",
            "evaluation_method": "Automatic exact-word-match relation F1 (per-word triplet scoring of entity pairs) plus manual expert scoring (domain-expert 'manual scores' that accept normalized or equivalent extractions). Held-out test sets and cross-validation used; examples: exact-match F1 for doping relations ~0.725–0.809 depending on schema, manual scores for general-task entities substantially higher.",
            "results_summary": "Fine-tuned GPT-3 models successfully extracted complex hierarchical relations from materials abstracts and sentences into structured schemas (JSON / templated English). For the host-dopant task GPT-3 achieved exact-match F1 up to ≈0.809 (DopingExtra-English) and strong manual-scored extraction for general materials fields (manual precision ~0.87 and F1s for many entity types &gt;0.7). Models also performed entity normalization/error correction (e.g., 'Li Co O2' → 'LiCoO2').",
            "limitations_or_challenges": "Exact-match metrics undercount true performance because the model normalizes or rephrases entities; LLM hallucination (inserting plausible but non-explicit entities), token-length limits producing truncated/unparsable outputs, reliance on proprietary API (reproducibility/security/cost), and variable performance across schemas. Requires a few dozen to a few hundred annotated examples to learn strict structured output formats.",
            "uuid": "e3638.0",
            "source_info": {
                "paper_title": "Structured information extraction from scientific text with large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama-2 (70B)",
            "name_full": "Llama-2 (70 billion parameter)",
            "brief_description": "An open-access large foundation transformer model (Llama-2, 70B) fine-tuned here with parameter-efficient methods (LoRA) to perform the same NERRE structured information extraction tasks on materials-science text.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2 (70B) with LoRA",
            "model_description": "Meta's Llama-2 70B parameter decoder-only transformer, fine-tuned in this work using LoRA (r=8, α=32, LoRA dropout 0.05) and 8-bit quantization on a single A100 GPU. Greedy decoding (temperature=0) used; trained for 4–7 epochs depending on task.",
            "input_domain": "Materials science / chemistry (paper abstracts and sentences)",
            "corpus_size": "Same extraction corpus context (&gt;8M abstracts available); fine-tuning used task-specific annotated datasets of a few hundred prompt→completion pairs (doping: 413 sentences; MOF: 507 abstracts; general: ≈650 entries).",
            "law_type": "Extraction of qualitative relations/heuristics in materials science (host-dopant relations, MOF name→application/guest species, formula→application/description), i.e., domain relational rules rather than formal physical laws.",
            "distillation_method": "Supervised LoRA fine-tuning on schema-formatted prompt→completion pairs; human-in-the-loop pre-annotation used to accelerate dataset construction.",
            "evaluation_method": "Exact-match per-word relation F1 and manual domain-expert scoring (same metrics as GPT-3 comparisons). Reported numeric comparisons vs GPT-3 and baselines on hold-out test sets and repeated random splits.",
            "results_summary": "Llama-2 models achieved competitive extraction performance though on average 20–30% lower than GPT-3 on the more complex tasks; Llama-2/ Doping-JSON performed best among Llama variants (exact-match F1 ≈0.821 for host-dopant). JSON-output schemas tended to work better for Llama-2 while GPT-3 did better with natural-language schemas.",
            "limitations_or_challenges": "Lower extraction scores versus GPT-3 on some tasks; same hallucination and token-limit issues; fine-tuning and hosting require nontrivial infrastructure and expertise despite being open-source; schema-output validity not strictly enforced (some unparsable outputs).",
            "uuid": "e3638.1",
            "source_info": {
                "paper_title": "Structured information extraction from scientific text with large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ChemPrompt / ChatGPT (Zheng et al.)",
            "name_full": "ChemPrompt with ChatGPT (prompt-engineering approach)",
            "brief_description": "A referenced prompt-engineering method (ChemPrompt) using ChatGPT to structure text into tables and semi-structured summaries and to collate knowledge from the model's pretraining corpus for chemistry/ materials tasks.",
            "citation_title": "ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (via prompt-engineering; 'ChemPrompt')",
            "model_description": "ChatGPT-based approach (OpenAI conversational LLM) applied with carefully designed prompts to extract tabular or semi-structured summaries from scientific text and to leverage the model's pretraining knowledge to collate information across documents.",
            "input_domain": "Chemistry / materials science (MOF synthesis and broader chemistry text)",
            "corpus_size": "Uses the model's large, unspecified pretraining corpus and applies to downstream sets of scientific papers (Zheng et al. applied approach to MOF-related literature; corpus sizes not quantified in the present paper).",
            "law_type": "Collation of existing domain knowledge and semi-structured factual summaries (aggregated domain heuristics), not explicit induction of new formal laws but aggregation of reported relations and knowledge.",
            "distillation_method": "Prompt engineering / zero- or few-shot prompting of ChatGPT to reformat passages to tabular/semi-structured outputs and to infer or collate related knowledge from the model's pretraining.",
            "evaluation_method": "In the present paper Zheng et al.'s method is discussed but not demonstrated to extract structured hierarchical relations generalizing outside the model's pretraining corpus; the present paper notes limitations in rigorous evaluation of generalization.",
            "results_summary": "Reported as capable of structuring text and collating knowledge from pretraining, serving as a practical tool for converting passages into semi-structured summaries; not shown in the referenced work to reliably produce structured hierarchical entity relationships beyond what is present in pretraining.",
            "limitations_or_challenges": "Potential over-reliance on pretraining corpus (may 'collate' rather than extract from given text), generalization outside pretraining uncertain, and ChatGPT can perform poorly on specialized chemistry tasks without elaborate prompt engineering ('tricks').",
            "uuid": "e3638.2",
            "source_info": {
                "paper_title": "Structured information extraction from scientific text with large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Unsupervised word embeddings (Tshitoyan et al.)",
            "name_full": "Unsupervised word-embedding models trained on materials-science literature",
            "brief_description": "An earlier approach demonstrating that unsupervised word embeddings trained on large materials-science text corpora can capture latent domain knowledge and correlations (e.g., materials-property relations) emergent from the literature.",
            "citation_title": "Unsupervised word embeddings capture latent knowledge from materials science literature.",
            "mention_or_use": "mention",
            "model_name": "Unsupervised word-embedding models (e.g., word2vec-style)",
            "model_description": "Distributional (word-embedding) models trained unsupervised on a large corpus of materials-science text to learn vector representations of terms; analyses show embeddings encode latent material-property associations and domain heuristics.",
            "input_domain": "Materials science literature",
            "corpus_size": "Large literature-scale corpus (the cited work trained on many thousands to millions of documents; this paper references it but does not restate precise counts).",
            "law_type": "Induction of latent qualitative relationships / heuristics (material–property associations) inferred from co-occurrence statistics — i.e., emergent domain rules from distributional patterns.",
            "distillation_method": "Unsupervised representation learning (word embeddings) followed by analysis / probing to reveal latent relationships and heuristics encoded in embedding space.",
            "evaluation_method": "Probing analyses and validation against known materials knowledge (as reported in the referenced work); in this paper it is cited as an example of latent knowledge capture from corpora.",
            "results_summary": "Serves as a precedent that statistical models trained on literature corpora can recover domain-relevant qualitative relationships; motivates the use of larger LLMs for richer structured-extraction tasks.",
            "limitations_or_challenges": "Embeddings capture statistical co-occurrence rather than causal or mechanistic laws and require downstream interpretation; limited expressivity for hierarchical structured relations compared to seq2seq LLM approaches.",
            "uuid": "e3638.3",
            "source_info": {
                "paper_title": "Structured information extraction from scientific text with large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Language models as knowledge bases (Petroni et al.)",
            "name_full": "Language models as knowledge bases?",
            "brief_description": "A line of work investigating whether pretrained language models encode factual knowledge that can be queried (i.e., treat LMs as implicit knowledge bases), cited here as conceptual background for using LLMs to capture or collate domain knowledge from text.",
            "citation_title": "Language models are few-shot learners.",
            "mention_or_use": "mention",
            "model_name": "Pretrained language models as implicit knowledge bases",
            "model_description": "Approaches and analyses treating pretrained LMs as repositories of factual knowledge that can be extracted via prompting or fine-tuning; often involves probing the model's predictions to recover stored facts.",
            "input_domain": "General natural-language corpora; applied to scientific domains when fine-tuned or prompted appropriately",
            "corpus_size": "Pretraining on large web-scale corpora (not quantified in this paper for the cited works).",
            "law_type": "Implicit factual knowledge and associations encoded in model parameters; can be used to surface qualitative rules or common-sense relations present in training data.",
            "distillation_method": "Probing and targeted prompting / fine-tuning to extract facts and associations from the pretrained model's parameters.",
            "evaluation_method": "Probing tasks, knowledge probes, comparison to curated knowledge bases; in this paper cited as background for LLMs' capacity to store and reveal domain knowledge.",
            "results_summary": "Provides conceptual justification for treating LLMs as knowledge sources that can be mined or fine-tuned to produce structured domain knowledge; the present work builds on this by fine-tuning to output structured schemas.",
            "limitations_or_challenges": "Knowledge encoded in parameters may reflect pretraining biases and not specific input text; disentangling pretraining-derived assertions from evidence in a given source document is nontrivial.",
            "uuid": "e3638.4",
            "source_info": {
                "paper_title": "Structured information extraction from scientific text with large language models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Domain-specific LLMs / Darwin series (Xie et al., Jablonka et al.)",
            "name_full": "Domain-specific large language models for natural science (Darwin series) and related works on LLMs for chemical discovery",
            "brief_description": "Referenced recent work fine-tuning or pretraining LLMs on domain-specific scientific corpora to perform Q/A, inverse design, classification, and other tasks relevant to materials and chemistry discovery.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Domain-specific fine-tuned LLMs (e.g., Darwin series; fine-tuned materials/chemistry LMs)",
            "model_description": "LLMs pretrained on general corpora and then further pre-trained or fine-tuned on large domain-specific scientific text (materials, chemistry) to improve question-answering, design, and extraction capabilities for science tasks.",
            "input_domain": "Materials science and chemistry literature",
            "corpus_size": "Large domain-specific corpora (the cited works report broader pretraining/fine-tuning on many thousands to millions of domain documents; precise counts not given here).",
            "law_type": "Extraction of domain heuristics, Q/A style knowledge, and in some cases use for inverse design — i.e., capturing domain rules and patterns useful for discovery workflows.",
            "distillation_method": "Domain-specific pretraining / fine-tuning of LLMs, sometimes combined with task-specific prompting and supervised examples for downstream tasks.",
            "evaluation_method": "Task-specific evaluations (Q/A, classification, inverse design benchmarks) reported in the cited works; in this paper these works are discussed as related efforts showing LLMs as 'materials science knowledge engines'.",
            "results_summary": "Cited as evidence that LLMs can be adapted to many scientific tasks and may act as knowledge engines for domain discovery; however, these prior works have not necessarily demonstrated robust extraction of complex hierarchical relations in the structured form targeted in this paper.",
            "limitations_or_challenges": "May require large domain corpora for best performance; provenance of extracted assertions and generalization beyond pretraining distribution remain concerns.",
            "uuid": "e3638.5",
            "source_info": {
                "paper_title": "Structured information extraction from scientific text with large language models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis.",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature.",
            "rating": 2,
            "sanitized_title": "unsupervised_word_embeddings_capture_latent_knowledge_from_materials_science_literature"
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2,
            "sanitized_title": "language_models_as_knowledge_bases"
        },
        {
            "paper_title": "Is GPT all you need for low-data discovery in chemistry?",
            "rating": 2,
            "sanitized_title": "is_gpt_all_you_need_for_lowdata_discovery_in_chemistry"
        },
        {
            "paper_title": "Darwin series: Domain specific large language models for natural science.",
            "rating": 2,
            "sanitized_title": "darwin_series_domain_specific_large_language_models_for_natural_science"
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners.",
            "rating": 1,
            "sanitized_title": "finetuned_language_models_are_zeroshot_learners"
        }
    ],
    "cost": 0.017966999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structured information extraction from scientific text with large language models</p>
<p>John Dagdelen 0000-0003-2181-4815
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Materials Science and Engineering Department
University of California
BerkeleyCAUSA</p>
<p>Alexander Dunn 0000-0002-8567-1879
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Materials Science and Engineering Department
University of California
BerkeleyCAUSA</p>
<p>Sanghoon Lee 
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Materials Science and Engineering Department
University of California
BerkeleyCAUSA</p>
<p>Nicholas Walker 
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Andrew S Rosen 0000-0002-0141-7006
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Materials Science and Engineering Department
University of California
BerkeleyCAUSA</p>
<p>Gerbrand Ceder 
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Materials Science and Engineering Department
University of California
BerkeleyCAUSA</p>
<p>Kristin A Persson 0000-0003-2495-5509
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Materials Science and Engineering Department
University of California
BerkeleyCAUSA</p>
<p>Anubhav Jain ajain@lbl.gov 0000-0001-5893-9967
Lawrence Berkeley National Laboratory
BerkeleyCAUSA</p>
<p>Structured information extraction from scientific text with large language models
D05282E950B82DBEECEB9AD027FEC6BA10.1038/s41467-024-45563-xReceived: 17 March 2023 Accepted: 22 January 2024
Extracting structured knowledge from scientific text remains a challenging task for machine learning models.Here, we present a simple approach to joint named entity recognition and relation extraction and demonstrate how pretrained large language models (GPT-3, Llama-2) can be fine-tuned to extract useful records of complex scientific knowledge.We test three representative tasks in materials chemistry: linking dopants and host materials, cataloging metal-organic frameworks, and general composition/phase/morphology/ application information extraction.Records are extracted from single sentences or entire paragraphs, and the output can be returned as simple English sentences or a more structured format such as a list of JSON objects.This approach represents a simple, accessible, and highly flexible route to obtaining large databases of structured specialized scientific knowledge extracted from research papers.</p>
<p>The majority of scientific knowledge about solid-state materials is scattered across the text, tables, and figures of millions of academic research papers.Thus, it is difficult for researchers to properly understand the full body of past work and effectively leverage existing knowledge when designing experiments.Moreover, machine learning models for direct property prediction are being increasingly employed as screening steps for materials discovery and design workflows [1][2][3] , but this approach is limited by the amount of training data available in tabulated databases.While databases of materials property data derived from ab initio simulations are relatively common, they are limited to the subset of computationally accessible properties whereas databases of experimental property measurements and other useful experimental data are comparatively small (if they exist at all).</p>
<p>In recent years, researchers have made significant advances in the application of natural language processing (NLP) algorithms for materials towards structuring the existing body of textual materials science knowledge [4][5][6][7] .The majority of this work has focused on named entity recognition (NER), where entity labels such as "material" or "property" are applied to words from the text.These tagged sequences of words can sometimes be used with additional post-processing to construct auto-generated tabular databases of materials property data aggregated from text entries [8][9][10][11][12] .Prior information extraction studies in the domain of solid-state materials include NER labeling of chemical synthesis parameters in methods section texts [13][14][15][16] , quantitative results of battery cycling experiments 17 , or peak absorption wavelengths for UV-Vis experiments 18 , among others 4,5,[9][10][11][12]19 . Reglar expressions, BiLSTM recurrent neural networks, and smaller transformer-based language models such as BERT are sufficient for such tasks.In these studies, entities (e.g., LiCoO 2 , "350K") rather than relations (e.g., "350K" is an experimental synthesis parameter for LiCoO 2 ) are the primary target of extraction.</p>
<p>Yet, a key challenge in scientific natural language processing is the development of robust, simple, and general relation extraction (RE) techniques to accurately extract the relationships between named entities.Downstream tasks such as supervised machine learning or the construction of knowledge graphs require the transformation of unstructured text into sets of structured relationships between semantic entities of interest.RE models are used to determine which entities are linked by a predefined set of relations.For example, in the sentence "LiCoO2 is studied as a Li-ion battery material", the material entity "LiCoO2" is linked to the application entity "Li-ion battery".Until recently, there has been relatively little work on relation extraction in materials science text, but there has been much research interest in RE on general-purpose text, especially related to linking people, organizations, locations, and dates 20,21 .These methods have traditionally relied on pipeline-based approaches where named entity recognition is the first step followed by one or more additional steps and a final relation classification step (see Fig. 1, top row).Each of these steps typically uses a separate machine learning model, which may or may not share weights or architectures with each other.State-of-the-art transformer-based implementations of pipeline implementations have been shown to perform document level relation extraction relatively well on a variety of general-knowledge corpora 22 and more specialized domains such as chemical-disease relations 23 and gene-disease relations 24 .Recently, this kind of two-step approach was demonstrated on a benchmark dataset of procedures for the synthesis of polycrystalline materials encoded as directed graphs extracted from materials science text 25 .</p>
<p>However, scientific information often cannot be modeled as simple pairwise relations between entities.This is particularly apparent in inorganic materials science, where a compound's properties are determined by a complex combination of its elemental composition, atomic geometry, microstructure, morphology (e.g., nanoparticles, heterostructures, and interfaces), processing history, and environmental factors such as temperature and pressure.Furthermore, inorganic materials knowledge is often inherently intertwined such that the relations may only be valid between one entity type and a compound entity (itself comprised of several entities and relationships).For example, we may consider zinc oxide nanoparticles (a compostion "ZnO" linked to the morphology "nanoparticles") to be a catalyst, but "ZnO" and "nanoparticles" alone are not necessarily catalysts in themselves.When parts of these compound relations are lost, scientific meaning will change.A sample of an "epitaxial La-doped thin film" of HfZrO 4 will have different physical properties than a "La-doped thin film" of HfZrO 4 and a "La-doped" sample of HfZrO 4 .In theory, relationships between n entities can be modeled as n-tuples (e.g., ("ZnO", "nanoparticles", "catalyst")), but comprehensively enumerating all possible variations is both impractical and not amenable to conventional relation extraction methods, since a sufficient number of training examples is required for each relation type.For example, a model extracting 10 distinct entity classes may have 10  Current relation extraction models are not designed to practically extract or preserve such kinds of highly complex, intricately related, and hierarchical relationships between arbitrary numbers of named entities; a more flexible strategy is required.</p>
<p>Large language models (LLMs) such as GPT-3/4 26,27 , PaLM 28 , Megatron 29 , LLaMA 1/2 30,31 , OPT 32 , Gopher 33 , and FLAN 34 have been shown to have remarkable ability to leverage semantic information between tokens in natural language sequences of varying length.They are particularly adept at sequence-to-sequence (seq2seq) tasks, where text input is used to seed a text response from the model.In this paper, we will refer to these inputs as "prompts" and the outputs as "completions."Use cases for seq2seq are broad 35 and include machine translation 36 , answering general factual knowledge questions 33,37 , performing simple arithmetic 33 , translating between languages 36,38 , summarizing text 28,39 , and chatbot applications 26,40 .It stands to reason that these models may also be adept at complex scientific information extraction.</p>
<p>Recently, end-to-end methods that use a single machine learning model have been investigated for joint named entity recognition and approaches encode relationships as 2-tuples in the output sequence.Named entities and relationship links are tagged with special symbols (e.g., "@FORMULA@", "@N2F@").c The method shown in this work outputs entities and their relationships as JSON documents or other hierarchical structures.</p>
<p>relation extraction (NERRE) [41][42][43] .These methods take a sequence-tosequence approach where a model is trained to output tuples of two or more named entities and the relation label belonging to the predefined set of possible relations between them (Fig. 1, middle row).These methods perform well on relation extraction, but they fundamentally remain n-ary relation extraction systems that are not suited to highly intricate and hierarchical NERRE.</p>
<p>In the domain of materials science, Huang &amp; Cole recently finetuned a BERT model on battery publications and trained a model to enhance a database of NLP-extracted battery data 11 .Their approach employed a "question and answer" (Q/A) approach that extracted limited device-level information (e.g., "What is the cathode?", "What is the anode?", "What is the electrolyte?") in tandem with conventional information extraction methods 11 .We note that this approach cannot be used on passages that contain information about more than one device, and it required the BERT language model to be trained on hundreds of thousands of battery research papers before being finetuned on the Q/A task.More recently, Zheng et al. 44 designed a prompt-engineering approach (ChemPrompt w/ ChatGPT 45 ) for extracting data from scientific papers.This method is focused on structuring text into tabular forms, creating semi-structured summaries, and collating existing knowledge from the pretraining corpus.Similarly, Castro Nascimento and Pimentel 46 examined ChatGPT's general knowledge of chemistry; however, they find that, as opposed to methods using considerable prompt engineering 47 , ChatGPT without prompting "tricks" performs poorly on several simple tasks in chemistry.Xie et al.'s 48 approach utilizes LLMs fine-tuned on a large, broad materials science corpus for a range of Q/A, inverse design, classification, and regression tasks.While these methods 44,[46][47][48] demonstrate LLMs might act as materials science knowledge engines, they have not been shown to extract structured representations of complex hierarchical entity relationships generalizing outside of the pretraining corpus.</p>
<p>In this work, we investigate a simple approach to complex information extraction where a large language model is fine-tuned to simultaneously extract named entities and their relationships.This method is able to flexibly handle complex inter-relations (including cases where information exists as lists of multiple items) without requiring enumeration of all of possible n-tuple relations or preliminary NER.Our approach differs from the supervised learning (e.g., regression and classification for chemistry) and inverse design approaches of Jablonka et al. 49,50 and Xie et al. 48; rather than using LLMs to directly influence design or predict properties, we aim to (accurately) extract structured hierarchies of information for use with downstream models.We fine-tune a pretrained large language model (e.g., GPT-3 26 or Llama-2 31 ) to accept a text passage (for example, a research paper abstract) and write a precisely formatted "summary" of knowledge contained in the prompt.This completion can be formatted as either English sentences or a more structured schema such as a list of JSON documents.To use this method, one only has to define the desired output structure-for example, a list of JSON objects with a predefined set of keys-and annotate ~100-500 text passages using this format.The LLM is then fine-tuned on these examples, and the resulting model is able to accurately output extracted information in the same structured representation, such as the format shown in Fig. 1.In essence, a domain expert can show an LLM-NERRE model both what it should extract and how that information should be represented, and then the model learns how to perform the task independently.</p>
<p>This method shows strong performance using both OpenAI's GPT-3 (closed source) and Llama-2 (open access) on both sentence-level and document-level materials information extraction.Moreover, the method can leverage online LLM APIs, which allows users to train bespoke models without extensive knowledge of how LLMs work internally; the LLM may be simply treated by the user as a black-box that transforms passages into precisely-formatted, structured summaries of scientific text.Therefore, researchers may use this method with little NLP experience.We also discuss how intermediate models can be used to pre-suggest entities for annotation, vastly increasing the speed and ease of annotating documents so that large training sets can be constructed relatively quickly.Although the example tasks shown are from materials science, the generality and accessibility of the method implies it may be readily applied to other domains such as chemistry, health sciences, or biology.In particular, this approach does not appear to require fine-tuning on a large corpus of domain-specific data (e.g., millions of article abstracts or paragraphs) as in previous methods; rather, the comprehensive pretraining of the LLMs along with the user-provided annotations are sufficient to accomplish a broad array of complex tasks.</p>
<p>Results</p>
<p>We use the described approach on three joint named entity recognition and relation extraction (NERRE) materials information extraction tasks: solid-state impurity doping, metal-organic frameworks (MOFs), and general materials information extraction.Details for each dataset are summarized in Table 1.Further details of each task are presented in the Methods section.Briefly, the solid-state impurity task is to identify host materials, dopants, and potentially additional related information from text passages (sentences).The MOF task is to identify chemical formulae, applications, guest species, and further descriptions of MOF materials from text (materials science abstracts).The general materials information task is to identify inorganic materials, their formulae, acronyms, applications, phase labels, and other descriptive information from text (materials science abstracts).The general and MOF models were trained on data including normalization and error correction, while doping models were trained to extract data exactly as it appears in text.Each base LLM model is fine-tuned per-task to adhere to a particular schema that encapsulates the entities of interest, relevant relationships, and format.All schemas are shown in Table 1 and further details are available in the Methods and Supplementary Note 1.</p>
<p>Relation extraction performance</p>
<p>A comparison between GPT-3 and Llama-2 on NERRE precision, recall, and F 1 scores across the three tasks using a JSON schema is shown in Table 2. Details on each of the task's JSON schemas are explained in the Methods section.The performances are calculated with an exact word- Exact match F 1 scores for these two extraction tasks are generally ~30% lower than in the host-dopant task.The highest F 1 for the general task is found for relationships between formulae and applications (F 1 = 0.537) while formula-acronym and formula-description relationships are much less reliable.A similar finding occurs for the MOF task, where the name-application (F 1 = 0.573) and name-guest species (F 1 = 0.616) relationships are extracted most accurately.The Llama-2 NERRE scores are on average 20 − 30% lower than their GPT-3 counterparts, indicating a significant advantage for GPT-3.In the dopant task, Llama-2 has the highest precision (0.836), recall (0.807), and F 1 (0.821), representing an improvement of 13% over GPT-3 wrt.F 1 .</p>
<p>The F 1 scores for the general and MOF tasks in Table 2 are generally 0.3-0.6,which is, on first inspection, seemingly too low to be useful for a large scale information extraction task.However, the scores for the MOF and general tasks are subject to an important caveat.These tasks' annotations include implicit normalization (e.g."Lithium ion" → "Li-ion") and error correction ("silcion" → "silicon"), while the doping task aims to extract hosts and dopants exactly as they appear in text.Thus, the exact word-match basis scores shown above are an approximate lower bound on information extraction performance, since this metric compares only exact matches between words.When outputs of the general and MOF models are read by human experts, it becomes obvious that the models are often extracting true information with slight changes in phrasing or notation.There is also an effect on performance from inherent ambiguity in real-world information extraction tasks.For example, in MOF information extraction, MOF names (e.g., "ZIF-8") are qualitatively easier to delimit than descriptions (e.g., "mesostructured MOFs formed by Cu2+ and 5hydroxy-1,3-benzenedicarboxylic acid"), which can be written with many different wordings.</p>
<p>To account for these factors, we manually scored outputs against the original human (true) annotations for a random 10% test set of the general materials information extraction dataset.We calculated "manual scores" by marking extractions as correct if the core information from entities is extracted in the correct JSON object (i.e., grouped with the correct material formula) and incorrect if they are in the wrong JSON object, are not extracted at all, or are not plausibly inferred from the original abstract.In contrast to the exact match scores (Table 2), manual scores allow for flexibility with respect to three aspects: (1) entity normalization, (2) error correction, and (3) multiple plausible annotations of an entity under different labels (e.g., "thermoplastic elastomer" may be considered either an application or description).Whereas Table 2 assesses whether the model can extract pairs of words exactly as they appear in the true annotation, the manual scores shown in Table 3 assess if the model extracts equivalent information to that of the true annotation -regardless of the exact form.Simply, if a domain expert would agree the model's extraction and the true extraction are equivalent, the model's extraction is marked as correct.We provide precise details on this procedure in the Methods section and detailed examples with explanations in Supplementary Discussion 4.</p>
<p>Table 3 shows the adjusted scores based on manual scoring.We stratify these scores by entity; the "name", "acronym", "application", "structure", and "description" manual scores can be compared to Table 2's exact-match formula-{name, application, structure, description} relation scores.For example, "description" reflects how often the model extracts a description entity which is both equivalent in meaning to that of the true annotation (according to a domain expert) and is grouped in the correct JSON object (linked to the correct formula).We see that exact-match scoring severely underpredicts performance for materials' names (0.456 vs 0.818), applications (0.537 vs 0.832), structures/phases (0.482 vs 0.829), and descriptions (0.354 vs 0.704).Manual scoring reveals that our models are actually able to correctly extract structured knowledge from scientific text on a wide variety of materials science topics, and readers can inspect the model's output on test set examples (included in the Supplementary Discussion 4) for themselves.We observe that  Overall, these scores indicate the model is highly capable at extracting meaningfully complex representations of material knowledge from passages.Precision scores for the various categories (other than acronyms) are all roughly 0.87 or better, which indicates that when information is extracted, it contains true relational information from the passage rather than spurious connections.</p>
<p>The advantage of the LLM-NERRE method reflected in these manual scores is the ability to automatically correct errors and normalize common entity patterns.While the doping models were trained to extract snippets of text exactly as they appeared in the text prompt, the General-JSON model's training data included simple normalizations and error corrections of entities.For example, the erroneous inclusion of white spaces in chemical formulae is common in the raw abstract text.We observe that including corrected formulae instead of the raw string in the output training sequences results in LLMs that automatically resolve extracted entities to cleaner forms.For example, "Li Co O2" is corrected to "LiCoO2" by the model without additional post-processing.Similarly, because there are sufficient training examples, the models using General-JSON schema resolve text such as "PdO functionalized with platinum" to a normalized form such as {formula: "PdO", description: ["Pt-functionalized"]}.The built-in normalization and correction abilities of LLM models may prove useful for domain specialists who desire structured entity formats rather than exact string excerpts pulled directly from the text, as entity normalization is a common post-processing task.</p>
<p>Effect of different schemas</p>
<p>For the host-dopant extraction task, we evaluated three different output schemas to determine whether one format of output is exclusively better than any other.The models using the Doping-English schema output English sentences with a particular structure (e.g., "the host '<host entity>' was doped with '<dopant entity>'.")and the DopingExtra-English models likewise output English sentences but also includes some additional information (e.g., if one of the hosts is a solid solution and/or the concentration of a particular dopant).For the Doping-JSON schema, we used a JSON object schema with keys "hosts", "dopants", and "hosts2dopants" (which in turn has a key-value object as its corresponding value).For readers familiar with the Python programming language, these are identical to python dictionary objects with strings as keys and strings or other dictionaries as values.We include a baseline comparison to seq2rel 41 , a comparable sequence-tosequence method, trained on the same doping dataset.We also compare to MatBERT-Doping 5 , an NER model trained on ~450 abstracts, combined with a simple heuristic for determining host-dopant relationships; that is, all hosts and dopants within the same sentence (sample) are related.We refer to this model as MatBERT-Proximity.Full descriptions and examples of all schemas are available in the Methods section, and further details on seq2rel and MatBERT-Proximity are available in Supplementary Notes 4-5.Because the general materials information extraction and MOF information extraction tasks are far more complex, we did not attempt to train models to output English sentences (as opposed to JSON formatted strings), as the resulting sequences would be difficult to parse into structured database entries.</p>
<p>We find that all three of our LLM-NERRE host-dopant extraction models perform significantly better than either the MatBERT-Proximity or seq2rel baseline models.Of the two baselines, the seq2rel model achieves higher precision (0.420) and recall (0.605) resulting in F 1 = 0.496, which is slightly higher than MatBERT-Proximity (0.390) but substantially lower than any of the LLM-NERRE models.This seq2rel benchmark model is derived from the PubMedBERT 51 pretrained BERT model as per the original implementation 41 , and it may be possible to improve the seq2rel method by using a BERT model pretrained exclusively on materials text rather than biomedical text.However, this improvement is not expected to be dramatic because previous comparisons between SciBERT and MatBERT show relatively minor differences in materials NER tasks 5 .We also observe that all three LLM-NERRE models exceed the performance of the two baselines in pure NER performance (see Supplementary Discussion 2) despite being trained on less text than the MatBERT-NER model (413 sentences vs. 455 abstracts.)Of the six LLM-based models, the Llama-2 model with Doping-JSON schema performs the best (F 1 = 0.821) with GPT-3/ DopingExtra-English (F 1 = 0.809) and Llama-2/Doping-English (F 1 = 0.814) both within a 2% margin.We summarize both LLMs' performances with all three schemas alongside the baseline models in Table 4.</p>
<p>Within the GPT-3 results, the DopingExtra-English and Doping-English schemas have the highest F 1 .In particular, GPT-3/DopingExtra-English tops the GPT-3 models despite being trained on the same number of samples as the Doping-English and Doping-JSON models.This is notable because GPT-3/DopingExtra-English is both more accurate and more capable (i.e., this model extracts "results" and "modifiers" entities in addition to host-dopant relationships) than the GPT-3 models using other schemas.The opposite observation is true of the Llama-2 models, where the JSON format outperforms both English schemas and the DopingExtra-English schema suffers from low precision (0.694).Roughly, the GPT-3 models tend to perform optimally when using natural language like schemas, while Llama-2 performs optimally using JSON.</p>
<p>Human-in-the-loop annotation</p>
<p>As a separate experiment, we evaluated the use of partially trained LLMs in a "human-in-the-loop" annotation process for constructing outputs with the GPT-3/General-JSON, as seen in Fig. 2. In each trial of the experiment, the human annotator received 10 abstracts and 10 schemas that were pre-populated by an intermediate version of the model which was trained on n samples of training data (n = 1, 10, 50, 100, 300).Instead of completing annotations from scratch, the human annotator corrected these intermediate models' suggestions, and the time to complete each annotation was recorded.As shown in Fig. 3, the annotation time sharply decreases as the number of training samples used in the intermediate models increases; the n = 300 intermediate model was able to reduce the average annotation time per abstract by 57% in comparison with the n = 1 model, indicating that the model was completing many sections of the annotation correctly.</p>
<p>At low numbers of training samples, the models' predictions are not valid JSON objects, and the annotator had to redo annotations from scratch.At higher numbers of training samples, particularly those above 50, the intermediate model predictions required very little error correction from the annotator.As a lower bound, we also report the time needed by the annotator to simply verify whether an entry was entirely correct (verification time) which reflects the annotation rate of a human annotator using a perfect model, which only requires the human annotator to check the outputs.We find that by three metrics (time per abstract, time per material entry, and time per prompt token), the human annotator annotated substantially faster with a welltrained model in the loop (n samples &gt; 50) than with a poorly trained model (n samples &lt; 50) or no model.For example, the n = 300 model reduced the annotation time per token ~60% compared to the n = 1 model and is only 38% slower than the verification time.Given additional training samples for intermediate models, we expect the annotation to asymptotically approach the verification time.Thus, this method may serve as a useful tool for building even larger benchmark datasets for information extraction tasks.</p>
<p>We note that the LLM-NERRE method requires the model to learn both the correct structure of the output data as well as the information to populate into that data structure, particularly when asking the model to output English sentences that can later be parsed to a structured format.To determine the minimum number of training examples required for models with sentence-format outputs that have a parseable sentence structure, we trained intermediate models on varying training set sizes for the GPT-3/Doping-English model.Precision, recall, and F 1 scores as a function of training set size are plotted in Fig. 4. We observe that output sequences are not properly structured for training set sizes below ~10 samples, but there is a sharp increase in the number of correctly structured outputs at ~20 samples, which seems to be the minimum number of examples GPT-3 needs to learn a desired output format when using simple sentence-type schemas.</p>
<p>Discussion</p>
<p>Overall, we find excellent performance on three diverse tasks for materials science and engineering: solid-state impurity doping, metal-organic frameworks, and general materials relations.The nontechnical nature of this approach implies scientists without NLP training can utilize existing models such as GPT-3 to extract large structured relational datasets for highly-specific problems.As the LLM is treated essentially as a black-box, we anticipate this approach may be used for LLMs other than GPT-3 or Llama-2, including LLMs released in the near future.We hope this approach enables domain specialists to rapidly extract relational datasets for the advancement of scientific knowledge.</p>
<p>The NERRE scores in Tables 2-4 provide a quantitative score for performance, but some of the best features of this method are not directly shown by F 1 scores.The primary advantage of this method is its accessibility and ease-of-use, as LLM-NERRE requires only  Like many others, we have found using a human-in-the-loop process can help decrease the time required to collect a full training set 52 .Our particular process is shown in Fig. 2. Annotation for scientific information extraction tasks is often a tedious and error-prone process whereas checking intermediate model outputs for errors is qualitatively easier than creating annotations from scratch.Additionally, fine tuning GPT-3/Llama-2 requires fewer training examples to match or exceed the performance of BERT-based models.Figure 4 shows how performance of the fine-tuned models improves quickly at relatively small training set sizes.However, more and more text-completion pairs are required to achieve the same rate of improvement as training set size is increased.</p>
<p>One limitation of our model is that valid output schema formatting is not rigorously enforced in the generation step.The LLM may, for any given sample, output an unparsable sequence.This is particularly apparent when the inference token limit is less than 512 tokens and the schema is JSON, as JSON schema typically requires a larger number of tokens for correct formatting.For example, a nearly-correct output sequence containing 10+ correct entities may be missing a "}" ending character and therefore will not be parsable.Outputs are nearly always parsable (~99% success rate), especially as the number of training examples increases.Failures predominantly occur when the sample exceeds the prompt-completion token limit of the LLM (early termination), which in this work was 512-1024 tokens for both GPT-3 and Llama-2.Because of this, some abstracts that are too long or too dense with information to be processed with this method.This was the case in the few unparseable completions where the passage and partial completion exceed the token limit and cut off early before the full completion could be output by the model.This limitation may be mitigated by increasing the token limit up to 2048 (GPT-3) or 4096 (Llama-2); we expect the token limitation will become less of a concern as the maximum token size of such models increase.</p>
<p>Another limitation is the tendency of LLMs to generate or invent information that is not actually present in the input text, a phenomenon termed "hallucination" 53,54 in LLM literature.The main manifestation of hallucination we observed was the addition of names or chemical formulae for a material when only one or the other was mentioned (for example writing "SiO2" in the formula field even though the paragraph only mentions "silica").Although these hallucinations could potentially be correct, because the source text does not include them, we believe they should not be included in the output of information extraction models.We could enforce this by the requirement that all extracted entities should occur word-for-word in the source text, but the fact that these models do not extract phrases exactly can also be a useful feature because it allows for automatic entity normalization.For example, an abstract may mention both "p-ZnSe doped with N" and "nitrogen-doped ZnSe" in the same passage.Is "doped with N" or "nitrogen-doped" the correct description to extract?Clearly, both are correct and either one could be reasonably chosen.Moreover, "N-doped" could also be extracted and would be factually correct even though "N-doped" never occurs in the passage.Because LLMs can learn implicit normalization rules, if the annotator is consistent in how they normalize cases like this (such as always using "Xdoped" and/or "p(n)-type"), the model generally follows the same normalization scheme and it can greatly reduce the amount of entity normalization post-processing required later.We differentiate this from hallucination in that the inference is fully justified by the content in the source text rather than simply plausible.</p>
<p>Finally, the choice of LLM poses a practical tradeoff for researchers: essentially, ease of use vs. control.Using a proprietary LLM such as GPT-3 through an online API enables the LLM in our method to be treated as a "black box", and abstracting away LLM finetuning details allows researchers to focus entirely on their domainspecific information extraction tasks.However, the underlying LLM is exclusively controlled by a private entity, posing problems of  reproducibility and security.Regarding security, potentially sensitive or confidential data must be sent to the entity for processing; regarding reproducibility, the models cannot be shared, and the entity controlling the LLM may at any time change the model, amend the finetuning method, or revoke access to the model altogether.More, the cost for inference on large datasets using trained models may be prohibitive.In contrast, using self-hosted models such as Llama-2 31 or GPT-NeoX 20B 55 favors control over ease of use.The weights and code for the model are fully accessible, and inference cost is restricted only by the user's budget on a cluster with capable GPUs.However, successfully running, fine-tuning, and deploying LLMs such as Llama-2 on cluster infrastructure is non-trivial for many scientists.Cloud-hosted open-access models (e.g., Llama-2 hosted on a managed cloud instance) may provide a solution to the ease of use vs. control tradeoff, as the technical details of fine-tuning are abstracted away from the user but the fine-tuned models themselves can remain open-access.Similarly, zero-shot approaches without fine-tuning may make scientific information extraction more accessible at the expense of accuracy (see Supplementary Discussion 6).Methods for reducing the number of parameters needed for LLM inference and fine-tuning [56][57][58][59] are also a promising avenue for reducing the complexity and cost of self-hosting LLMs.As these methods advance and LLM codebases become more mature, we expect fine-tunable models compatible with LLM-NERRE will become simultaneously powerful, easy to self-host, reproducible, and under researchers' full control.We hope the code examples of both fine-tuning and running inference using the published model weights we provide in Methods are a first step in the direction of powerful and open source NERRE models.In summary, this work demonstrates that LLMs that are fine-tuned on a few hundred training examples are capable of extracting scientific information from unstructured text and formatting the information in user-defined schemas.This is in contrast to past models which were successful in extracting entities from text but struggled to relate those entities or them in meaningful ways.The proposed method is simple and broadly accessible given the APIs and interfaces currently available such as GPT-3.Furthermore, we have made the Llama-2 LoRA weights of all models shown in this paper available for download (see Methods and Code Availability), allowing researchers to investigate the LLM-NERRE method on their own hardware.We expect these advancements to greatly facilitate the rate and accuracy by which historical scientific text can be converted to structured forms.</p>
<p>Methods</p>
<p>General sequence-to-sequence NERRE</p>
<p>We fine-tune Llama-2 and GPT-3 models to perform NERRE tasks using 400−650 manually annotated text-extraction (prompt-completion) pairs.Extractions contain the desired information formatted with a predefined, consistent schema across all training examples.These schemas can range in complexity from English sentences with predefined sentence structures to lists of JSON objects or nested JSON objects.In principle, many other potential schemas (e.g., YAML, psuedocode) may also be valid, though we do not explore those here.Once fine-tuned on sufficient data adhering to the schema, a model will be capable of performing the same information extraction task on new text data with high accuracy.The model outputs completions in the same schema as the training examples.We refer to this approach generally as "LLM-NERRE".</p>
<p>Our general workflow for training GPT-3 and Llama-2 to perform NERRE tasks is outlined in Fig. 2. Annotations are performed by human domain experts to create an initial training set, and then a partially trained model (GPT-3) is used to accelerate the collection of additional training examples.Fine-tuning is then performed on these examples to produce a "partially trained" model, which is used to pre-fill annotations that are subsequently corrected by the human annotator before being added to the training set.Once a sufficient number of annotations have been completed, the final fine-tuned model is capable of extracting information in the desired format without human correction.Optionally, as illustrated in Figs. 5 and 6, the structured outputs may be further decoded and post-processed into hierarchical knowledge graphs.</p>
<p>Task and schema design</p>
<p>Solid-state impurity doping schema.The Doping-English and Doping-JSON schemas aim to extract two entity types (host and dopant) and the relations between them (host-dopant), returned as either English sentences or a list of one or more JSON objects.Hosts are defined as the host crystal, sample, or material class along with crucial descriptors in its immediate context (e.g., "ZnO2 nanoparticles", "LiNbO3", "half-Heuslers").Dopants are taken to be any elements or ions that are minority species, intentionally added impurities, or specific point defects or charge carriers ("hole-doped", "S vacancies").One host may be doped with more than one dopant (e.g., separate singledoping or co-doping), or the same dopant may be linked to more than one host material.There may also be many independent pairs of dopant-host relations, often within a single sentence, or many unrelated dopants and hosts (no relations).We impose no restriction on the number or structure of the dopant-host relations beyond that each relation connects a host to a dopant.The Doping-JSON schema represents the graph of relationships between hosts and dopants within a single sentence, where unique keys identify dopant and host strings.The model aims to learn this relatively loose schema during fine-tuning.A separate key, "hosts2dopants", describes the pairwise relations according to those unique keys.The Doping-English schema encodes the entity relationships as quasi-natural language summaries.The Doping-English schema represents the same information as the Doping-JSON schema, but more closely mimics the natural language pre-training distribution of the LLMs we tested.When there are multiple items to extract from the same sentence, the output sentences are separated by newlines.</p>
<p>For the DopingExtra-English schema, we introduce two additional entities: modifiers and result, without explicit linking (i.e., NER only).The results entity represents formulae with algebra in the stoichiometric coefficients such as Al x Ga 1−x As, which are used for experiments with samples from a range of compositions or crystalline solid solutions (e.g., CaCu 3−x Co x Ti 4 O 12 ).We also include stoichiometries where the algebra is substituted (i.e., x value specified) and the doped result is a specific composition (e.g., CaCu 2.99 Co 0.1 Ti 4 O 12 ).Modifiers are loosely bounded entity encapsulating other descriptors of the dopant-host relationship not captured by dopant, host, or result.These can be things like polarities (e.g., "n-type", "n-SnSe"), dopant quantities (e.g., "5 at.%", "x &lt; 0.3"), defect types (e.g., "substitutional", "antisite", "vacancy") and other modifiers of the host to dopant relationship (e.g., "high-doping", "degenerately doped").These entities (host, dopant, result, and modifiers) were chosen to define a minimal effective schema for extracting basic doping information.</p>
<p>All doping-related models are trained to work only on single sentences.The main motivation for this design choice is that the vast majority of dopant-related data can be found within single sentences, and the remaining relational data is often difficult to resolve consistently for both human annotators and models.We expand on problems with annotations and ambiguity in Supplementary Discussion 5 and we further explain the doping task schemas in Supplementary Note 1.</p>
<p>General materials information schema.In our previous work 4,5 , we focused on NER for a specific set of entity types that are particularly relevant in materials science: materials, applications, structure/phase labels, synthesis methods, etc.However, we did not link these labeled entities together to record their relations beyond a simple "bag-of- entities" approach.In this work, we train LLM to perform a "general materials information extraction" task that captures both entities and the complex network of interactions between them.</p>
<p>The schema we have designed for this task encapsulates an important subset of information about solid compounds and their applications.Each entry in the list, a self-contained JSON document, corresponds one-to-one with a material mentioned in the text.Materials entries are ordered by appearance in the text.The root of each entry starts with a compound's name and/or its chemical formula.If a name or formula is not mentioned for a material, no information about that material is extracted from the text.We also extract acronyms mentioned for a material's name/formula, although in cases where only an acronym is mentioned we do not create a material entry for the compound.Compounds that are not solids (ions, liquids, solvents, solutions, etc) are generally not extracted.The name, formula, and acronym fields are exclusively string value in the JSON document  .In all three panels, an LLM trained to output a particular schema (far left) reads a raw text prompt and outputs a structured completion in that schema.The structured completion can then be parsed, decoded, and formatted to construct relational diagrams (far right).We show an example for each schema (desired output structure).Parsing refers to the reading of the structured output, while decoding refers to the programmatic (rule-based) conversion of that output into JSON form.Normalization and postprocessing are programmatic steps which transform raw strings (e.g., "Co+2") into structured entities with attributes (e.g., Element: Co, Oxidation state +2).a Raw sentences are passed to the model with Doping-English schema, which outputs newline-separated structured sentences that contain one host and one or more dopant entities.b Raw sentences are passed to a model with Doping-JSON schema, which outputs a nested JSON object.Each host entity has its own key-value pair, as does each dopant entity.There is also a list of host2dopant relations that links the corresponding dopant keys to each host key.c Example for the extraction with a model using the DopingExtra-English schema.This first part of the schema is the same as in a, but additional information is contained in doping modifiers, and results-bearing sentences are included at the end of the schema.</p>
<p>for each material whereas the description, structure_or_phase, and applications fields are of an arbitrary number of strings.We label this model General-JSON, and an example is shown in Fig. 6 (a).</p>
<p>Description entities are defined as details about a compound's processing history, defects, modifications, or the sample's morphology.For example, consider the hypothetical text "Pt supported on CeO2 nanoparticles infused with Nb...".In this case, the description value for the material object referring to "Pt" might be annotated as "['supported on CeO2']", and the description entities listed for "CeO2" would be "['nanoparticles', 'Nb-doped']".</p>
<p>Structure_or_phase entities are defined as information that directly implies the crystalline structure or symmetry of the compound.Crystal systems such as "cubic" or "tetragonal", structure names such as "rutile" or "NASICON", and space groups such as "Fd3m" or "space group No. are all extracted in this field.We also include any information about crystal unit cells, such as lattice parameters and the angles between lattice vectors."Amorphous" is also a valid structure/phase label.</p>
<p>Applications are defined as high-level use cases or major property classes for the material.For example, a battery cathode material may have "['Li-ion battery', 'cathode']" as its applications entry.Generally, applications are mentioned in the order they are presented in the text, except for certain cases such as battery materials, in which case the type of device is generally mentioned before the electrode type, and catalysts, where the reaction catalyzed is generally listed following the "catalyst" entity in the list (e.g.,"['catalyst', 'hydrogenation of citral']").</p>
<p>More details about the general materials information task schema are provided in the Supplementary Discussion 4.</p>
<p>Metal-organic framework (MOF) schema.The schema used for the MOF cataloging task is based on the general materials information schema described in the previous section, which was modified to better suit the needs of MOF researchers.We developed this schema to extract MOF names (name), an entity for which there is no widely accepted standard 60 , and chemical formulae (formula), which form the root of the document.If no name or formula is present, no information is extracted for that instance.In addition, because there is a great deal of interest in using MOFs for ion and gas separation 61,62 , we extract guest species, which are chemical species that have been incorporated, stored, or adsorbed in the MOF.We extract applications the MOF is being studied for as a list of strings (e.g., "['gasseparation']" or "['heterogeneous catalyst', 'Diels-Alder reactions']") as well as a relevant description for the MOF, such as its morphology or processing history, similar to the general information extraction schema.Entries in the list are generally added in the order the material names/formulae appear in the text.The MOF extraction model is labeled MOF-JSON, and an example is shown in Fig. 6 (b).</p>
<p>Comparison baselines and evaluation</p>
<p>To compare our model with other sequence-to-sequence approaches to information extraction, we perform a benchmark of two methods on the doping task to compare to the LLM-NERRE models.The first employs the seq2rel method of Giorgi et al. 41 for the host-dopant task.We formatted host-dopant relationships under tags labeled @DOPANT@ and @BASEMAT@ (base/host material), with their relationship signified by @DBR@ ("dopant-base material relationship"); these sequences were constructed from the same training data as the Doping-JSON and Doping-English models.We trained seq2rel to perform sentencelevel extraction with 30 epochs, batch size of 4, encoder learning rate 2 × 10 −5 , decoder learning rate 5 × 10 −4 , and pretrained BiomedNLP BERT tokenizer 51 (further training details can be found in the Supplementary Note 4).Additionally, we compare against the previously published MatBERT doping-NER model 5 combined with proximitybased heuristics for linking (see Supplementary Note 5).With this method, a MatBERT NER model pretrained on ~50 million materials science paragraphs and fine-tuned on 455 separate manually annotated abstracts first extracts hosts and dopants and then links them if they co-occur in the same sentence.</p>
<p>Datasets</p>
<p>Datasets were prepared from our database of more than 8 million research paper abstracts 63 .Annotations were performed by human annotators using a graphical user interface built using Jupyter 64 , although in principle annotations could be conducted via a simple text editor.To accelerate the collection of training data, new annotations are collected via a "human in the loop" approach where models are trained on small datasets and their outputs are used as starting points and corrected by human annotators (see Fig. 2.) This process of training and annotation is completed multiple times until a sufficiently large set of training data was achieved.Each dataset was annotated by a single domain expert annotator.Class support for each annotated dataset is provided in Supplementary Tables 1-3.</p>
<p>Doping dataset.</p>
<p>Training and evaluation data was gathered from our database of research paper abstracts using the keywords "n-type", "ptype", "-dop", "-codop", "doped", "doping", and "dopant" (with exclusions for common irrelevant keywords such as "-dopamine"), resulting in ~375k total abstracts.All doping tasks were trained on text from 162 randomly selected abstracts, comprising 1215 total sentences and filtered with regular expressions to only include 413 relevant (potentially including doping information) sentences.Doping tasks were tested on an additional 232 sentences (77 relevant by regex) from a separate holdout test set of 31 abstracts.</p>
<p>General materials dataset.</p>
<p>Training and evaluation data was gathered from our abstract database by using keywords for a variety of materials properties and applications (e.g., "magnetic", "laser", "space group", "ceramic", "fuel cell", "electrolytic", etc).For each keyword a materials science domain expert annotated ~10-50 abstracts, which resulted in ≈650 entries manually annotated according to the general materials information schema.Results were evaluated using a 10% random sample for validation, and this procedure was averaged over five trials using different random train/validation splits with no hyperparameter tuning.</p>
<p>Metal-organic framework dataset.Training and evaluation data was selected from our database using the keywords "MOF", "MOFs", "metal-organic framework", "metal organic framework", "ZIF", "ZIFs", "porous coordination polymer", and "framework material", which produced approximately 6,000 results likely containing MOF-related information.From these, 507 abstracts were randomly selected and annotated by a MOF domain expert.Results were evaluated using the same repeated random split procedure as the general materials dataset in the previous section.</p>
<p>GPT-3 fine tuning details</p>
<p>For all tasks, we fine-tune GPT-3 ('davinci', 175B parameters) 26 using the OpenAI API, which optimizes the cross-entropy loss on predicted tokens.Doping models were trained for 7 epochs at a batch size of 1, with inference temperature of 0 and output limited to a maximum length of 512 tokens (all doping models) or 1024 tokens (General-JSON, MOF-JSON).The intermediate models shown in Fig. 4 were trained with a number of epochs depending on the number of training samples t: 2 epochs for 2 0 ≤t &lt; 2 6 , 4 epochs for 2 6 &lt; t ≤ 2 7 , and 7 epochs for t ≥ 2 8 .Models for the MOF and general materials extraction tasks were trained for 4 epochs with a batch size of 1.We use a learning rate multiplier of 0.1 and a prompt loss weight of 0.01 but have not performed hyperparameter tuning for these hyperparameters.For all tasks, the start and end tokens used were "\n\n\n###\n\n\n" and "\n\n\nEND\n\n\n".</p>
<p>Llama-2 fine-tuning details</p>
<p>Llama-2 31 fine-tunes were performed using a modified version of the Meta Research Llama-2 recipes repository; the modified repository can be found at https://github.com/lbnlp/nerre-llama.Llama-2 fine-tunes were performed using the 70 billion parameter version of Llama-2 (llama-2-70b-hf) with quantization (8 bit precision).The number of epochs was set to 7 for doping tasks and 4 for the MOF/general tasks.Llama-2 fine-tunes used parameter efficient fine-tuning (PEFT) using low rank adaptation (LoRA) 58 with LoRA r = 8, α = 32 and LoRA dropout of 0.05.Further hyperparameter tuning was not performed.Decoding was done without sampling using greedy decoding to be consistent with GPT-3 decoding setting of temperature = 0, with max tokens = 512 for doping task and 1024 for general and MOF task.More details on the fine tuning and inference parameters are available in the modified repository and Supplementary Note 3.All fine-tuning and inference was performed on a single A100 (Ampere) tensor core GPU with 80GB VRAM.</p>
<p>The fine-tuned weights for each model are provided in the NERRE-Llama repository (url above) along with code and instructions for downloading the weights, instantiating the models, and running inference.</p>
<p>1.The entity comes from the original passage or is a reasonable variation of the entity in the passage (e.g., "silicon" ⟶ "Si").It is not invented by the model.2. The entity is a root entity or is grouped with a valid root entity.For the General-JSON model, a root entity is either a material's formula or name.If both are present, the formula is used at the root.3. The entity is in the correct field in the correct root entity's group (JSON object).</p>
<p>Manual scores are reported per-entity as if they were NER scores.However, the requirements for a true positive implicitly include relational information, since an entity is only correct if is grouped with the correct root entity.</p>
<p>Reporting summary</p>
<p>Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.</p>
<p>C 3 = 120 3-tuple entity relation types, each requiring at least several annotation examples.</p>
<p>Fig. 1 |
1
Fig. 1 | Schematic comparison of previous relation extraction (RE) methods to this work.The objective of each method is to extract entities (colored text) and their relationships from unstructured text.a An example multi-step pipeline approach first performs entity recognition, then intermediate processing such as coreference resolution, and finally classification of links between entities.b seq2seq</p>
<p>Fig. 3 |
3
Fig. 3 | Annotation time as a function of intermediate large language model (LLM) fine-tuning samples for the named entity recognition and relation extraction (NERRE) method.We show the time taken for a domain expert to annotate new abstracts for the general materials chemistry task with assistance</p>
<p>Fig. 2 |
2
Fig.2| Overview of the proposed sequence-to-sequence approach to document-level joint named entity recognition and relationship extraction task.In the first step, lists of JSON documents are prepared from abstracts according to a predefined schema, and the large language model (LLM) is trained.In the second step, this preliminary (intermediate) model is used to accelerate the preparation of additional training data by pre-annotation with the partially trained model and manual correction.An example error is shown highlighted in red.This step may be repeated multiple times with each subsequent partial fine-tuning improving in performance.In the final step, the LLM is fine-tuned on the complete dataset and used for inference to extract desired information from new text.</p>
<p>Fig. 4 |
4
Fig. 4 | Test set performance vs. number of training samples for the doping extraction task using GPT-3 with the Doping-English schema.This schema specifically requires the model to learn a new and specific sentence structure to use as the output.We separate scores by (a) host-dopant links (relations), (b) host entities alone, and (c) dopant entities alone.We note that below approximately 10 samples, the scores are zero because the model has not learned the specific structure of the desired output sentences.Source data are provided as a Source Data file.</p>
<p>Article https://doi.org/10.1038/s41467-024-45563-xNature Communications | (2024) 15:1418</p>
<p>Fig. 6 |
6
Fig. 6 | Diagrams of general information extraction and metal organic framework (MOF) information extraction using large language models (LLMs) for joint named entity and relation extraction (NERRE).In both panels, an LLM trained using a particular schema (desired output structure, far left) is prompted with raw text and produces a structured completion as JSON.This completion can then be parsed to construct relational diagrams (far right).Each task uses a different schema representing the desired output text structure from the LLM. a Schema and labeling example for the general materials-chemistry extraction task.Materials science research paper abstracts are passed to an LLM using General-JSON schema, which outputs a list of JSON objects representing individual material entries ordered by appearance in the text.Each material may have a name, formula, acronym, descriptors, applications, and/or crystal structure/phase information.b Schema and labeling example for the metal-organic frameworks extraction task.Similar to the General-JSON model, the MOF-JSON model takes in full abstracts from materials science research papers and outputs a list of JSON objects.In the example, only MOF name and application were present in the passage, and both MOFs (LaBTB and ZrPDA) are linked to both applications (luminescent and VOC sensor).</p>
<p>Fig. 5 |
5
Fig.5| Diagrams of doping information extraction using large language models (LLMs) for joint named entity and relation extraction (NERRE).In all three panels, an LLM trained to output a particular schema (far left) reads a raw text prompt and outputs a structured completion in that schema.The structured completion can then be parsed, decoded, and formatted to construct relational diagrams (far right).We show an example for each schema (desired output structure).Parsing refers to the reading of the structured output, while decoding refers to the programmatic (rule-based) conversion of that output into JSON form.Normalization and postprocessing are programmatic steps which transform raw strings (e.g., "Co+2") into structured entities with attributes (e.g., Element: Co,</p>
<p>Table 1 |
1
Overview of approaches tested on the three materials information extraction tasks
TaskSchemaTraining samplesTask levelCompletion formatDopingDoping-JSON413 sentencesSentenceJSONDopingDoping-English413 sentencesSentenceEnglish sentencesDopingDopingExtra-English413 sentencesSentenceEnglish sentences</p>
<p>Table 2 |
2
Named entity recognition and relation extraction scores for three tasks in materials science using models with a JSON output schema Exact match (E.M.) scores are evaluated on a per-word basis, and links are only correct if both entities and the relationship are correct.The exact match metric scores output that contains the correct information but is written differently as incorrect, making such scores a rough lower bound on the true performance of models.F 1 , precision, and recall reflect the scores on a hold out test set for doping models and averages over five cross-validation sets for the general and MOF models.
TaskRelationE.M. PrecisionE.M. RecallE.M. F 1E.M. PrecisionE.M. RecallE.M. F 1 (Llama-2)(GPT-3)(GPT-3)(GPT-3)(Llama-2)(Llama-2)Dopinghost-dopant0.7720.6840.7260.8360.8070.821 aGeneral formula-name0.5070.4290.4560.4620.4170.367General formula-acronym0.5000.2500.3330.3330.2500.286General formula-struc-0.5380.4390.4820.5510.4320.47ture/phaseGeneral formula-application0.5420.5430.5370.5450.4960.516General formula-description0.3620.350.3540.3470.3420.340MOFsname-formula0.4250.6880.4830.4600.4540.276MOFsname-guest specie0.7890.5760.6160.4970.4070.408MOFsname-application0.6570.5180.5730.5070.5620.531MOFsname-description0.4930.4750.4040.4320.4110.389
a Best F 1 scores for each task are shown in bold.</p>
<p>Table 3 |
3
Manual scores for the general materials task using GPT-3 with General-JSON schema acronyms have the lowest information extraction scores, which we attribute to the fact that acronyms are relatively rare in the training class compared to the others (appearing in only 52 abstracts across the entire dataset, ~9% of the documents) and that the model can confuse acronyms with chemical formulae (e.g., "AuNP" is the acronym for gold nanoparticle but is also a valid chemical formula).Usually, context clues are the only way to disambiguate cases like this, and we expect including more training data with acronyms may improve the acronym extraction score.
EntityExtraction recall ExtractionExtraction F 1precisionformula0.9430.9430.943name0.6921.00.818acronym0.6670.4000.500applications0.7970.8700.832structure or phase 0.7540.9200.829description0.5760.9050.704
Scores measure the model's ability to extract inter-related data together (i.e.assigning entities correct labels and grouping them appropriately).Article https://doi.org/10.1038/s41467-024-45563-xNature Communications | (2024) 15:1418</p>
<p>Table 4 |
4
Comparison of large language models with different joint named entity recognition and relation extraction (NERRE) schemas to baseline models on host-dopant extraction task
ModelSchemaPrecisionRecallF 1(exact match)(exact(exactmatch)match)MatBERT-n/a0.3770.4030.390ProximitySeq2reln/a0.4200.6050.496GPT-3Doping-JSON0.7720.6840.725GPT-3Doping-English0.8030.7540.778GPT-3DopingExtra-0.8200.7980.809EnglishLlama-2Doping-JSON0.836 a0.8070.821Llama-2Doping-English0.7870.8420.814Llama-2DopingExtra-0.6940.8150.750English
NERRE exact match scores are evaluated on a per-word basis, and links are only correct if both entities and relationship are correct.DopingExtra-English scores here refer to only host-dopant relation prediction.We note that exact match scores output that contains the correct information but is written differently as incorrect, making such scores an approximate lower bound on the true performance of models.F1, precision, and recall are computed on a hold-out test set from 77 sentences.Best scores for precision, recall, and F 1 are shown in bold.a Best scores among all models in each category (exact match precision, recall, F 1 ) are shown in bold.</p>
<p>Nature Communications | (2024) 15:1418
https://doi.org/10.1038/s41467-024-45563-x Nature Communications | (2024) 15:1418
Article https://doi.org/10.1038/s41467-024-45563-x Nature Communications | (2024) 15:1418
AcknowledgementsThis work was supported by Toyota Research Institute through the Accelerated Materials Design and Discovery program.A.S.R. acknowledges support via a Miller Research Fellowship from the Miller Institute for Basic Research in Science, University of California, Berkeley.Funding for training and evaluating the Llama-2 model was provided by the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences, Materials Sciences and Engineering Division under Contract No. DE-AC02-05CH11231 (D2S2 program KCD2S2).This research used resources of the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility located at Lawrence Berkeley National Laboratory, operated under Contract No. DE-AC02-05CH11231 using NERSC award BES-ERCAP0024004.We thank Anna Sackmann (Science Data and Engineering Librarian at UC Berkeley) for helping us to obtain Text and Data Mining agreements with the specified publishers and we also thank J. Montoya and A. Trewartha for helpful discussions.Data availabilityAll data used for this study are available at https://github.com/LBNLP/NERREand via Zenodo65, which contains the annotated datasets and test and train splits.Intermediate files for each step of the pipeline reported in this method are stored in this repository with corresponding documentation.Data for running Llama-2 models are available in the supplementary repository https://github.com/lbnlp/nerre-llama66; LoRA weights for all Llama-2 models reported in this paper can be downloaded directly from Figshare (https://doi.org/10.6084/m9.figshare.24501331.v1)67.Source data are provided with this paper.Code availabilityThe code used for this study is available at https://github.com/LBNLP/NERREand via Zenodo 65 alongside the data.This code includes Jupyter notebooks for annotation as well as Python scripts for annotation, preprocessing, model training, and model evaluation on the train and test sets presented in this publication.The supplementary repository https://github.com/lbnlp/nerre-llama 66contains code and data for fine-tuning and inference with Llama-2 models trained in this study, including access to the complete sets of weights via a script.Evaluation criteriaThe fuzzy and complex nature of the entities and relationships detailed in the previous section necessitates the use of several metrics for scoring.We evaluate the performance of all models on two levels:1.A relation F 1 computed on a stringent exact word-match basis (i.e., how many words are correctly linked together exactly as they appear in the source text prompt).2. A holistic information extraction F 1 based on manual inspection by a domain expert, which doesn't require words to match exactly.We separately provide a sequence-level error analysis in Supplementary Note 7 and Supplementary Discussion 1.NERRE performance.We measure NERRE performance as the ability of the model to jointly recognize entities and the relationships between them.Exact word-match basis scoringWe score named entity relationships on a word-basis by first converting an entity E into a set of constituent k whitespace-separated words E = {w 1 , w 2 , w 3 , …, w k }.When comparing two entities E true and E test that do not contain chemical formulae, we count the number of exactly matching words in both sets as true positives (E true ∩ E test ) and the mathematical set differences between the sets as false positives (E test − E true ) or false negatives (E true − E test ).For example, if the true entity is "Bi2Te3 thin film" and the predicted entity is "Bi2Te3 film sample", we record two true positive word exact matches ("Bi2Te3", "film"), one false negative ("thin"), and one false positive ("sample").Formula-type entities are crucial for identifying materials, so in cases where entities contain chemical formulae, E test must contain all w i that can be recognized as stoichiometries for any of w i ∈ E test to be considered correct.For example, if the true entity is "Bi2Te3 thin film", and the predicted entity is "thin film", we record three false negatives.Thus, any formula-type entity (Doping host, Doping dopant, General formula, and MOF mof_formula) containing a chemical composition is entirely incorrect if the composition is not an exact match.This choice of evaluation was made to avoid metrics measuring the performance of the model in a misleading way.For example, "Bi2Te3 nanoparticles" and "Bi2Se3 nanoparticles" have very high similarities via Jaro-Winkler (0.977) and character-level BLEU-4 (0.858), but these two phrases mean entirely different things-the material's chemistry is wrong.Under our scoring system, they are recorded as entirely incorrect because the compositions do not match.We score relationships between entities on a word-by-word basis to determine the number of correct relation triplets.Triplets are 3-tuples relating word w n j of an entity E n to word w m k of an entity E m by relationship r, represented as ðw n j ,w m k ,rÞ.The total set of correct relationships T true for a text contains many of these triplets.A test set of relationships T test is evaluated by computing the number of triplets found in both sets (T true ∩ T test ) as true positives and the differences between these sets as false positives (T test − T true ) or false negatives (T true − T test ).Entity triplets are also bound to the same requirement for composition correctness if either of the words in the triplet belong to an formula-type entity (host, dopant, formula, mof_formula), i.e., we count all triplets for two entities as incorrect if the formula is not an exact string match.With correct and incorrect triplets identified, FTo compute triplet scores across entire test sets in practice, we first select a subset of relations to evaluate.We note that this is not a full evaluation of the task we are training the model to perform, which involves linking many interrelated entities simultaneously, but is rather provided to help give a general sense of its performance compared to other NERRE methods.For the doping task, we evaluate host-dopant relationships.For the general materials and MOF tasks, we evaluate relationships between the formula field (formula for general materials, mof_formula for MOFs) and all other remaining fields.For description, structure_or_phase, and applications fields, all of which may contain multiple values, all of the possible formula-value pairs are evaluated.Manual evaluationThe metrics provided in prior sections demonstrate automatic and relatively strict methods for scoring NERRE tasks, but the underlying capabilities of the LLM models are best shown with manual evaluation.This is most apparent in the case of the General-JSON model, where exact boundaries on entities are fuzzier, precise definitions are difficult to universally define, and annotations include some implicit entity normalization.For example, the text "Pd ions were intercalated into mesoporous silica" may have equivalently have a correct description field for the material "silica" including "Pdintercalated", "Pd ion-intercalated", "intercalated with Pd ions", etc.; the exact choice of which particular string is used as the "correct" answer is arbitrary.To better address scoring of these fuzzy tasks, we introduce an adjusted score based on a domain expert's manual evaluation of whether the information extracted is a valid representation of the information actually contained in the passage.We term this adjusted score "manual score"; it constitutes a basis for precision, recall, and F 1 that quantifies the quality of overall information capture for cases where there may be equivalent or multiple ways of representing the same concept.This score was constructed to better estimate the performance of our model for practical materials information extraction tasks.We score entities extracted by annotators but not present in the model's output as false negatives, except when reasonable variations are present.The criteria for a true positive are as follows:Competing interestsThe authors declare no competing interests.Additional informationSupplementary informationThe online version contains supplementary material available at https://doi.org/10.1038/s41467-024-45563-x.Correspondence and requests for materials should be addressed to Anubhav Jain.Peer review information Nature Communications thanks the anonymous, reviewers for their contribution to the peer review of this work.A peer review file is available.Reprints and permissions information is available at http://www.nature.com/reprintsPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material.If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/.
Machine learning in materials discovery: confirmed predictions and their underlying approaches. J E Saal, A O Oliynyk, B Meredig, Annu. Rev. Mater. Res. 502020</p>
<p>Recent advances and applications of deep learning methods in materials science. K Choudhary, npj Comput. Mater. 8592022</p>
<p>Materials discovery with machine learning and knowledge discovery. O N Oliveira, M C F Oliveira, Front. Chem. 109303692022</p>
<p>Named entity recognition and normalization applied to large-scale information extraction from the materials science literature. L Weston, J. Chem. Inform. Modeling. 592019</p>
<p>Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science. A Trewartha, Patterns. 31004882022</p>
<p>Single model for organic and inorganic chemical named entity recognition in ChemDataExtractor. T Isazawa, J M Cole, J. Chem. Inform. Modeling. 622022</p>
<p>Fine-tuning BERT model for materials named entity recognition. X Zhao, J Greenberg, Y An, X T Hu, 10.1109/bigdata52589.2021.96716972021 IEEE International Conference on Big Data (Big Data. IEEE2021</p>
<p>A thermoelectric materials database auto-generated from the scientific literature using chemdataextractor. O Sierepeklis, J M Cole, Sci. data. 96482022</p>
<p>Perovskite-and dye-sensitized solar-cell device databases auto-generated using chemdataextractor. E J Beard, J M Cole, Sci. Data. 93292022</p>
<p>Auto-generating databases of yield strength and grain size using chemdataextractor. P Kumar, S Kabra, J M Cole, Sci. Data. 92922022</p>
<p>BatteryBERT: A pretrained language model for battery database enhancement. S Huang, J M Cole, J. Chem. Inform. Modeling. 622022</p>
<p>Auto-generated database of semiconductor band gaps using chemdataextractor. Q Dong, J M Cole, Sci. Data. 91932022</p>
<p>Text-mined dataset of inorganic materials synthesis recipes. O Kononova, Sci. Data. 62032019</p>
<p>Machine-learning rationalization and prediction of solid-state synthesis conditions. H Huo, Chem. Mater. 342022</p>
<p>Similarity of precursors in solid-state synthesis as textmined from scientific literature. T He, Chem. of Mater. 322020</p>
<p>Dataset of solution-based inorganic materials synthesis procedures extracted from the scientific literature. Z Wang, Sci. Data. 92312022</p>
<p>A database of battery materials autogenerated using ChemDataExtractor. S Huang, J M Cole, Sci. Data. 72602020</p>
<p>Comparative dataset of experimental and computational attributes of UV/vis absorption spectra. E J Beard, G Sivaraman, Á Vázquez-Mayagoitia, V Vishwanath, J M Cole, Sci. Data. 63072019</p>
<p>A database of refractive indices and dielectric constants auto-generated using ChemDataExtractor. J Zhao, J M Cole, Sci. Data. 91922022</p>
<p>Joint entity recognition and relation extraction as a multi-head selection problem. G Bekoulis, J Deleu, T Demeester, C Develder, Expert Syst. Appl. 1142018</p>
<p>More data, more relations, more context and more openness: a review and outlook for relation extraction. X Han, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2020</p>
<p>DocRED: A large-scale document-level relation extraction dataset. Y Yao, Proc. 57th Annual Meeting of the Association for Computational Linguistics. 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2019</p>
<p>Biocreative v cdr task corpus: a resource for chemical disease relation extraction. J Li, Database. 2016. 2016</p>
<p>Extraction of relations between genes and diseases from text and large-scale data analysis: Implications for translational research. Á Bravo, J Piñero, N Queralt-Rosinach, M Rautschka, L I Furlong, BMC Bioinformatics. 162015</p>
<p>Pcmsp: A dataset for scientific action graphs extraction from polycrystalline materials synthesis procedure text. X Yang, Findings of the Association for Computational Linguistics: EMNLP 2022. Association for Computational Linguistics2022</p>
<p>Language models are few-shot learners. T B Brown, 2020Preprint at</p>
<p>Gpt-4 technical report. 2023OpenAIPreprint at</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, Journal of Machine Learning Research. 242023</p>
<p>Using deepspeed and megatron to train megatronturing nlg 530b, a large-scale generative language model. S Smith, 2022Preprint at</p>
<p>Llama: Open and efficient foundation language models. H Touvron, 2023Preprint at</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, 2023Preprint at</p>
<p>Opt: Open pre-trained transformer language models. S Zhang, 2022Preprint at</p>
<p>Training compute-optimal large language models. J Hoffmann, 2022</p>
<p>Finetuned language models are zero-shot learners. J Wei, International Conference on Learning Representations. 2022</p>
<p>Beyond the imitation game: measuring and extrapolating the capabilities of language models. 2021BIG-bench collaboration. In preparation</p>
<p>A survey of multilingual neural machine translation. R Dabre, C Chu, A Kunchukuttan, ACM Comput. Surv. 532020</p>
<p>Language models as knowledge bases?. F Petroni, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational Linguistics2019</p>
<p>Unsupervised neural machine translation with generative language models only. J M Han, 2022</p>
<p>Pretraining-based natural language generation for text summarization. H Zhang, J Xu, J Wang, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Association for Computational Linguistics2019</p>
<p>Multi-stage prompting for knowledgeable dialogue generation. Z Liu, Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics2022</p>
<p>A sequence-to-sequence approach for document-level relation extraction. J Giorgi, G Bader, B Wang, Proc. 21st Workshop on Biomedical Language Processing. 21st Workshop on Biomedical Language essingAssociation for Computational Linguistics2022</p>
<p>REBEL: Relation extraction by end-toend language generation. P.-L H Cabot, R Navigli, 10.18653/v1/2021.findings-emnlp.204Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics2021</p>
<p>Doc2dict: Information extraction as text generation. B Townsend, E Ito-Fisher, L Zhang, M May, 2021</p>
<p>ChatGPT chemistry assistant for text mining and the prediction of MOF synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>. Openai, 2022</p>
<p>Do large language models understand chemistry? a conversation with chatgpt. C M Castro Nascimento, A S Pimentel, J. Chem. Inform. Modeling. 632023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. A D White, Digital Discov. 22023</p>
<p>Darwin series: Domain specific large language models for natural science. T Xie, 2023Preprint at</p>
<p>Is GPT all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n4-v22023</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, Digital Discov. 22023</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, ACM Trans. Comput. Healthcare. 312021</p>
<p>Human-in-the-loop machine learning: a state of the art. E Mosqueira-Rey, E Hernández-Pereira, D Alonso-Ríos, J Bobes-Bascarán, Á Fernández-Leal, Artif. Intel. Rev. 562022</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R Mcdonald, 10.18653/v1/2020.acl-mainProc. 58th Annual Meeting of the Association for Computational Linguistics. 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>Survey of hallucination in natural language generation. Z Ji, ACM Comput. Surv. 55122023</p>
<p>Gpt-neox-20b: An open-source autoregressive language model. S Black, 2022Preprint at</p>
<p>Massive language models can be accurately pruned in one-shot. E Frantar, D Alistarh, Sparsegpt, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningJLMR.org2023</p>
<p>A simple and effective pruning approach for large language models. M Sun, Z Liu, A Bair, J Z Kolter, Workshop on Efficient Systems for Foundation Models @ ICML2023. 2023</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, 2021Preprint at</p>
<p>Llm-pruner: On the structural pruning of large language models. X Ma, G Fang, X Wang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Identification schemes for metal-organic frameworks to enable rapid search and cheminformatics analysis. B J Bucior, Cryst. Growth Des. 192019</p>
<p>Metal-organic frameworkbased ion-selective membranes. X Li, M R Hill, H Wang, H Zhang, Adv. Mater. Technol. 620007902021</p>
<p>Mof-based membranes for gas separations. Q Qian, Chem. Rev. 1202020</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. V Tshitoyan, Nature. 5712019</p>
<p>Jupyter notebooks-a publishing format for reproducible computational workflows. T Kluyver, Positioning and Power in Academic Publishing: Players, Agents and Agendas. F Loizides, B Schmidt, IOS Press2016</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, 10.5281/zenodo.104211742023</p>
<p>Llama 2 code for structured information extraction from scientific text with large language models. J Dagdelen, 10.5281/zenodo.104211872023</p>
<p>Llama 2 weights for structured information extraction from scientific text with large language models. J Dagdelen, 10.6084/m9.figshare.24501331.v12023</p>
<p>10.1038/s41467-024-45563-xArticle. </p>
<p>. Nature Communications |. 1514182024</p>            </div>
        </div>

    </div>
</body>
</html>
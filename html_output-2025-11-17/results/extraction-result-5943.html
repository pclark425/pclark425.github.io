<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5943 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5943</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5943</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-7d645a3fd276918374fd9483fd675c28e46506d1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1" target="_blank">Galactica: A Large Language Model for Science</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Galactica is introduced: a large language model that can store, combine and reason about scientific knowledge, and sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.</p>
                <p><strong>Paper Abstract:</strong> Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5943.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5943.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica (GAL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of decoder-only Transformer language models (up to 120B parameters) trained on a curated 106B-token scientific corpus to store, combine and reason about scientific knowledge; includes multimodal tokenization (SMILES, amino/DNA), special citation tokens and a working-memory token (<work>) to support step-by-step reasoning and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A Large Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Galactica (GAL; up to 120B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Decoder-only Transformer models (variants: 125M, 1.3B, 6.7B, 30B, 120B) with GeLU activations, learned positional embeddings, 2048 context length, 50k BPE vocab, and specialized tokens for citations ([START_REF]/[END_REF]), working memory (<work>), and modality wrappers ([START_SMILES], [START_AMINO], etc.). Trained on a curated scientific corpus (≈106B tokens) with prompt pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Proposed capability: synthesize and organize scientific knowledge from the literature and thereby (potentially) distill qualitative laws, principles, or generalizable rules across many scholarly papers (e.g., literature reviews, encyclopedic summaries, hidden connections).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multidisciplinary scientific literature (physics, chemistry, biology, mathematics, medicine, computer science, and other scientific domains present in the curated corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Pretraining on a large curated scientific corpus plus explicit prompt pre-training; specialized tokenization (citations, <work>, modality tokens); zero-shot and prompt-based generation for synthesis (literature reviews, citation prediction); no dedicated, experimental pipeline in the paper for automated extraction of qualitative 'laws' — only proposals and related synthesis capabilities demonstrated (e.g., generated reviews, citation prediction, probe tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not demonstrated concretely in experiments; the paper frames potential outputs as synthesized knowledge artifacts (literature reviews, encyclopedia-style summaries, inferred connections) that could correspond to qualitative principles or general rules, but it does not report distilled domain-level laws (e.g., 'rules of cell migration') produced or validated by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No dedicated metrics for law/principle extraction reported. Related evaluations in the paper that assess knowledge/synthesis capability include: knowledge probes (LaTeX equation recall, domain probes), downstream QA benchmarks (MMLU, MATH, PubMedQA, MedMCQA), citation prediction accuracy and distributional match (KS distance), and task-specific accuracies (e.g., IUPAC name accuracy). Manual/qualitative checking was used for some probes (e.g., LaTeX correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper does not present experiments where Galactica is used to automatically distill generalizable qualitative laws from large collections of scholarly papers. Instead it demonstrates adjacent capabilities relevant to synthesis: high recall on LaTeX equation probes (GAL 120B: 68.2%), improved reasoning with the <work> token on MMLU/MATH, strong citation-prediction accuracy (e.g., 51.9% on PapersWithCode dataset for GAL 120B), and generative uses such as helping write the paper (suggesting citations, topics). Thus the claim that LLMs can distill qualitative laws is presented as a potential application supported by demonstrated knowledge-synthesis primitives, but no quantitative or qualitative results for law extraction itself are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop for evaluation and usage: authors used Galactica to help write the paper (recommend citations, topics), human manual evaluation used in knowledge probes, and human curation of the training corpus; law-extraction use-cases were proposed but not run as automated pipelines—no fully automated law-extraction experiments were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Curated scientific corpus of ≈106 billion tokens: ~48 million papers (≈88B tokens), plus code (7B tokens), reference material (7B), knowledge bases (2B), filtered CommonCrawl and prompt datasets (≈358M prompt tokens). Contains multimodal scientific artifacts (LaTeX, SMILES, protein and DNA sequences) and explicit citation-processed text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The paper acknowledges several limitations relevant to distilled-law extraction: risk of hallucination and blending of facts when storing knowledge in weights; title-based citation identifiers prone to hallucination at smaller scales; corpus coverage limits (e.g., only a random subset of PubChem compounds used); lack of an explicit retrieval augmentation in the experiments (weights-only approach may be insufficient for fine-grained facts); no demonstrated pipeline or evaluation for extracting/validating domain-level qualitative laws; possible bias toward graduate-level scientific content (corpus composition) and remaining challenges in reliability and updating model knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Illustrative synthesis-capability examples reported: (1) Galactica was used to help write this paper (recommending missing citations, topics, and draft text), (2) citation prediction tasks where the model predicts references in-context and whose distributional match improves with scale, (3) knowledge probes like LaTeX equation generation (e.g., Bessel's equation), (4) domain tasks such as chemical IUPAC name prediction from SMILES with interpretable attention to functional groups, and (5) reasoning via the <work> token including program offloading for exact arithmetic. However, no concrete distilled qualitative laws/principles across many papers are given as experimental outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Galactica: A Large Language Model for Science', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models as Knowledge Bases? <em>(Rating: 2)</em></li>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP <em>(Rating: 1)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
                <li>Finetuned Language Models Are Zero-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5943",
    "paper_id": "paper-7d645a3fd276918374fd9483fd675c28e46506d1",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "Galactica (GAL)",
            "name_full": "Galactica: A Large Language Model for Science",
            "brief_description": "A family of decoder-only Transformer language models (up to 120B parameters) trained on a curated 106B-token scientific corpus to store, combine and reason about scientific knowledge; includes multimodal tokenization (SMILES, amino/DNA), special citation tokens and a working-memory token (&lt;work&gt;) to support step-by-step reasoning and synthesis.",
            "citation_title": "Galactica: A Large Language Model for Science",
            "mention_or_use": "mention",
            "llm_model_name": "Galactica (GAL; up to 120B)",
            "llm_model_description": "Decoder-only Transformer models (variants: 125M, 1.3B, 6.7B, 30B, 120B) with GeLU activations, learned positional embeddings, 2048 context length, 50k BPE vocab, and specialized tokens for citations ([START_REF]/[END_REF]), working memory (&lt;work&gt;), and modality wrappers ([START_SMILES], [START_AMINO], etc.). Trained on a curated scientific corpus (≈106B tokens) with prompt pre-training.",
            "task_goal": "Proposed capability: synthesize and organize scientific knowledge from the literature and thereby (potentially) distill qualitative laws, principles, or generalizable rules across many scholarly papers (e.g., literature reviews, encyclopedic summaries, hidden connections).",
            "domain": "Multidisciplinary scientific literature (physics, chemistry, biology, mathematics, medicine, computer science, and other scientific domains present in the curated corpus).",
            "methodology": "Pretraining on a large curated scientific corpus plus explicit prompt pre-training; specialized tokenization (citations, &lt;work&gt;, modality tokens); zero-shot and prompt-based generation for synthesis (literature reviews, citation prediction); no dedicated, experimental pipeline in the paper for automated extraction of qualitative 'laws' — only proposals and related synthesis capabilities demonstrated (e.g., generated reviews, citation prediction, probe tasks).",
            "type_of_qualitative_law": "Not demonstrated concretely in experiments; the paper frames potential outputs as synthesized knowledge artifacts (literature reviews, encyclopedia-style summaries, inferred connections) that could correspond to qualitative principles or general rules, but it does not report distilled domain-level laws (e.g., 'rules of cell migration') produced or validated by the model.",
            "evaluation_metrics": "No dedicated metrics for law/principle extraction reported. Related evaluations in the paper that assess knowledge/synthesis capability include: knowledge probes (LaTeX equation recall, domain probes), downstream QA benchmarks (MMLU, MATH, PubMedQA, MedMCQA), citation prediction accuracy and distributional match (KS distance), and task-specific accuracies (e.g., IUPAC name accuracy). Manual/qualitative checking was used for some probes (e.g., LaTeX correctness).",
            "results_summary": "The paper does not present experiments where Galactica is used to automatically distill generalizable qualitative laws from large collections of scholarly papers. Instead it demonstrates adjacent capabilities relevant to synthesis: high recall on LaTeX equation probes (GAL 120B: 68.2%), improved reasoning with the &lt;work&gt; token on MMLU/MATH, strong citation-prediction accuracy (e.g., 51.9% on PapersWithCode dataset for GAL 120B), and generative uses such as helping write the paper (suggesting citations, topics). Thus the claim that LLMs can distill qualitative laws is presented as a potential application supported by demonstrated knowledge-synthesis primitives, but no quantitative or qualitative results for law extraction itself are provided.",
            "human_involvement": "Human-in-the-loop for evaluation and usage: authors used Galactica to help write the paper (recommend citations, topics), human manual evaluation used in knowledge probes, and human curation of the training corpus; law-extraction use-cases were proposed but not run as automated pipelines—no fully automated law-extraction experiments were reported.",
            "dataset_or_corpus": "Curated scientific corpus of ≈106 billion tokens: ~48 million papers (≈88B tokens), plus code (7B tokens), reference material (7B), knowledge bases (2B), filtered CommonCrawl and prompt datasets (≈358M prompt tokens). Contains multimodal scientific artifacts (LaTeX, SMILES, protein and DNA sequences) and explicit citation-processed text.",
            "limitations_or_challenges": "The paper acknowledges several limitations relevant to distilled-law extraction: risk of hallucination and blending of facts when storing knowledge in weights; title-based citation identifiers prone to hallucination at smaller scales; corpus coverage limits (e.g., only a random subset of PubChem compounds used); lack of an explicit retrieval augmentation in the experiments (weights-only approach may be insufficient for fine-grained facts); no demonstrated pipeline or evaluation for extracting/validating domain-level qualitative laws; possible bias toward graduate-level scientific content (corpus composition) and remaining challenges in reliability and updating model knowledge.",
            "notable_examples": "Illustrative synthesis-capability examples reported: (1) Galactica was used to help write this paper (recommending missing citations, topics, and draft text), (2) citation prediction tasks where the model predicts references in-context and whose distributional match improves with scale, (3) knowledge probes like LaTeX equation generation (e.g., Bessel's equation), (4) domain tasks such as chemical IUPAC name prediction from SMILES with interpretable attention to functional groups, and (5) reasoning via the &lt;work&gt; token including program offloading for exact arithmetic. However, no concrete distilled qualitative laws/principles across many papers are given as experimental outputs.",
            "uuid": "e5943.0",
            "source_info": {
                "paper_title": "Galactica: A Large Language Model for Science",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models as Knowledge Bases?",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
            "rating": 1
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1
        },
        {
            "paper_title": "Finetuned Language Models Are Zero-Shot Learners",
            "rating": 1
        }
    ],
    "cost": 0.012478999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Galactica: A Large Language Model for Science</h1>
<p>Ross Taylor</p>
<p>Thomas Scialom</p>
<p>Andrew Poulton</p>
<h2>Marcin Kardas</h2>
<p>Anthony Hartshorn
Viktor Kerkez</p>
<h2>Guillem Cucurull</h2>
<p>Elvis Saravia</p>
<p>Robert Stojnic</p>
<h2>Abstract</h2>
<p>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by $68.2 \%$ versus $49.0 \%$. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by $41.3 \%$ to $35.7 \%$, and PaLM 540B on MATH with a score of $20.4 \%$ versus $8.8 \%$. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of $77.6 \%$ and $52.9 \%$. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>The original promise of computing was to solve information overload in science. In his 1945 essay "As We May Think", Vannevar Bush observed how "publication has been extended far beyond our present ability to make real use of the record" (Bush, 1945). He proposed computers as a solution to manage the growing mountain of information. Licklider expanded on this with the vision of a symbiotic relationship between humans and machines. Computers would take care of routine tasks such as storage and retrieval, "preparing the way for insights and decisions in scientific thinking" (Licklider, 1960).
Computing has indeed revolutionized how research is conducted, but information overload remains an overwhelming problem (Bornmann and Mutz, 2014). In May 2022, an average of 516 papers per day were submitted to arXiv (arXiv, 2022). Beyond papers, scientific data is also growing much more quickly than our ability to process it (Marx, 2013). As of August 2022, the NCBI GenBank contained $1.49 \times 10^{12}$ nucleotide bases (GenBank, 2022). Given the volume of information, it is impossible for a single person to read all the papers in a given field; and it is likewise challenging to organize data on the underlying scientific phenomena.
Search engines are the current interface for accessing scientific knowledge following the Licklider paradigm. But they do not organize knowledge directly, and instead point to secondary layers such as Wikipedia,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>UniProt and PubChem Compound which organize literature and data. These resources require costly human contributions, for example writing a review of literature, an encyclopedia article or annotating a protein. Given this bottleneck, researchers continue to feel overwhelmed even with powerful search tools to hand.
In this paper, we argue for a better way through large language models. Unlike search engines, language models can potentially store, combine and reason about scientific knowledge. For example, a model trained on the literature could potentially find hidden connections between different research, find hidden gems, and bring these insights to the surface. It could synthesize knowledge by generating secondary content automatically: such as literature reviews, encyclopedia articles, lecture notes and more. And lastly, it could organize different modalities: linking papers with code, protein sequences with compounds, theories with LaTeX, and more. Our ultimate vision is a single neural network for powering scientific tasks. We believe this is will be the next interface for how humans access scientific knowledge, and we get started in this paper.</p>
<h1>1.1 Our Contribution</h1>
<p>We introduce a new large language model called Galactica (GAL) for automatically organizing science. Galactica is trained on a large and curated corpus of humanity's scientific knowledge. This includes over 48 million papers, textbooks and lecture notes, millions of compounds and proteins, scientific websites, encyclopedias and more. Unlike existing language models, which rely on an uncurated crawl-based paradigm, our corpus is high-quality and highly curated. We are able to train on it for multiple epochs without overfitting, where upstream and downstream performance improves with use of repeated tokens.
Dataset design is critical to our approach, which includes curating a high-quality dataset and engineering an interface to interact with the body of knowledge. All data is processed in a common markdown format to blend knowledge between sources. We also include task-specific datasets in pre-training to facilitate composition of this knowledge into new task contexts. For the interface, we use task-specific tokens to support different types of knowledge. We process citations with a special token, that allows a researcher to predict a citation given any input context. We wrap step-by-step reasoning in a special token, that mimicks an internal working memory. And lastly, we wrap modalities such as SMILES and protein sequences in special tokens, which allows a researcher to interface with them using natural language. With this interface and the body of scientific knowledge in the model, we achieve state-of-the-art results across many scientific tasks.
On reasoning tasks, Galactica beats existing language models on benchmarks such as MMLU and MATH (Hendrycks et al., 2020, 2021). With our reasoning token approach, we outperform Chinchilla on mathematical MMLU with an average score of $41.3 \%$ versus $35.7 \%$ (Hoffmann et al., 2022). Our 120B model achieves a score of $20.4 \%$ versus PaLM 540B's $8.8 \%$ on MATH (Chowdhery et al., 2022; Lewkowycz et al., 2022). The 30B model also beats PaLM 540B on this task with 18 times less parameters. We believe this adds another reasoning method to the deep learning toolkit, alongside the existing chain-of-thought approach that has been well explored recently (Wei et al., 2022; Suzgun et al., 2022).
We also find Galactica performs strongly in knowledge-intensive scientific tasks. We conduct detailed knowledge probes of Galactica's knowledge of equations, chemical reactions and other scientific knowledge. Galactica significantly exceeds the performance of general language models such as the latest GPT-3 in these tasks; on LaTeX equations, it achieves a score of $68.2 \%$ versus the latest GPT-3's $49.0 \%$ (Brown et al., 2020). Galactica also performs well in downstream scientific tasks, and we set a new state-of-the-art on several downstream tasks such as PubMedQA (77.6\%) and MedMCQA dev (52.9\%) (Jin et al., 2019; Pal et al., 2022).
We also demonstrate new capabilities with Galactica's interface. First, the capability of predicting citations improves smoothly with scale, and we also find the model becomes better at modelling the underlying distribution of citations: the empirical distribution function approaches the reference distribution with scale. Importantly, we find this approach outperforms tuned sparse and dense retrieval approaches for citation prediction. This, along other results, demonstrates the potential for language models to replace the Licklider paradigm, document storage and retrieval, with their context-associative power in weight memory.
In addition, Galactica can perform multi-modal tasks involving SMILES chemical formulas and protein sequences. We formulate drug discovery tasks as text prompts and show performance scales in a weakly supervised setup. We also demonstrate Galactica learns tasks such as IUPAC name prediction in a selfsupervised way, and does so by attending to interpretable properties such as functional groups. Lastly, Galactica can annotate protein sequences with natural language, including predicting functional keywords.
Galactica was used to help write this paper, including recommending missing citations, topics to discuss in the introduction and related work, recommending further work, and helping write the abstract and conclusion.</p>
<h1>2 Related Work</h1>
<p>Large Language Models (LLMs) LLMs have achieved breakthrough performance on NLP tasks in recent years. Models are trained with self-supervision on large, general corpuses and they perform well on hundreds of tasks (Brown et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022; Zhang et al., 2022; Chowdhery et al., 2022). This includes scientific knowledge tasks such as MMLU (Hendrycks et al., 2020). They have the capability to learn in-context through few-shot learning (Brown et al., 2020). The capability set increases with scale, and recent work has highlighted reasoning capabilities at larger scales with a suitable prompting strategy (Wei et al., 2022; Chowdhery et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022).
One downside of self-supervision has been the move towards uncurated data. Models may mirror misinformation, stereotypes and bias in the corpus (Sheng et al., 2019; Kurita et al., 2019; Dev et al., 2019; Blodgett et al., 2020; Sheng et al., 2021). This is undesirable for scientific tasks which value truth. Uncurated data also means more tokens with limited transfer value for the target use-case; wasting compute budget. For example, the PaLM corpus is $50 \%$ social media conversations, which may have limited transfer towards scientific tasks (Chowdhery et al., 2022). The properties of scientific text also differ from general text - e.g. scientific terms and mathematics - meaning a general corpus and tokenizer may be inefficient. We explore whether a normative approach to dataset selection can work with the large model paradigm in this work.</p>
<p>Scientific Language Models Works such as SciBERT, BioLM and others have shown the benefit of a curated, scientific corpus (Beltagy et al., 2019; Lewis et al., 2020a; Gu et al., 2020; Lo et al., 2019b; Gu et al., 2020; Shin et al., 2020; Hong et al., 2022). The datasets and models were typically small in scale and scope, much less than corpora for general models ${ }^{2}$. Beyond scientific text, Transformers for protein sequences and SMILES have shown potential for learning natural representations (Rives et al., 2021; Honda et al., 2019; Irwin et al., 2021; Nijkamp et al., 2022; Lin et al., 2022b). However, sequences like SMILES have descriptive limitations for representing chemical structure. We explore in this work whether a large, multi-modal scientific corpus can aid representation learning, where sequences occur alongside footprints and text in a signal-dense context.</p>
<p>Scaling Laws The idea of "scaling laws" was put forward by Kaplan et al. (2020), who demonstrated evidence that loss scales as a power-law with model size, dataset size, and the amount of training compute. The focus was on upstream perplexity, and work by Tay et al. (2022a) showed that this does not always correlate with downstream performance. Hoffmann et al. (2022) presented new analysis taking into account the optimal amount of data, and suggested that existing language models were undertrained: "Chinchilla scaling laws". This work did not take into the account of fresh versus repeated tokens. In this work, we show that we can improve upstream and downstream performance by training on repeated tokens.</p>
<p>Language Models as Knowledge Bases Storing information in weights is more unreliable in the sense models may blend information together, hallucination, but it is more "pliable" in the sense it can associate information through the representation space, association. Despite hallucination risks, there is evidence large language models can act as implicit knowledge bases with sufficient capacity (Petroni et al., 2019). They perform well on knowledge-intensive tasks such as general knowledge (TriviaQA) and specialist knowledge (MMLU) without an external retrieval mechanism (Brown et al., 2020; Hendrycks et al., 2020).
The question of how to update network knowledge remains an active research question (Scialom et al., 2022; Mitchell et al., 2022). Likewise, the question of how to improve the reliability of generation is an active question (Gao et al., 2022). Despite these limitations, today's large models will become cheaper with experience (Hirschmann, 1964), and so a growing proportion of scientific knowledge will enter weight memory as training and re-training costs fall. In this work we perform probes to investigate Galactica's depth of knowledge, and show that the ability to absorb scientific knowledge improves smoothly with scale.</p>
<p>Retrieval-Augmented Models Retrieval-augmented models aim to alleviate the shortcomings of weight memory. Examples of such models include RAG, RETRO and Atlas (Lewis et al., 2020b; Borgeaud et al., 2021; Izacard et al., 2022). These models have the advantage of requiring less capacity but the disadvantage of needing supporting retrieval infrastructure. Since knowledge is often fine-grained, e.g. the sequence of a particular protein, or the characteristics of a particular exoplanet, retrieval will likely be needed in future even for larger models. In this work we focus on how far we can go with model weights alone, but we note the strong case for using retrieval augmentation for future research on this topic.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Modality</th>
<th>Entity</th>
<th>Sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text</td>
<td>Abell 370</td>
<td>Abell 370 is a cluster...</td>
</tr>
<tr>
<td>$\mathrm{IA}_{\mathrm{E}} \mathrm{X}$</td>
<td>Schwarzschild radius</td>
<td>$\mathrm{r}_{-}{\mathrm{s}}=\backslash \operatorname{frac}{2 \mathrm{GM}}\left{\mathrm{c}^{-} 2\right}$</td>
</tr>
<tr>
<td>Code</td>
<td>Transformer</td>
<td>class Transformer(nn.Module)</td>
</tr>
<tr>
<td>SMILES</td>
<td>Glycine</td>
<td>$\mathrm{C}(\mathrm{C}(=0) 0) \mathrm{N}$</td>
</tr>
<tr>
<td>AA Sequence</td>
<td>Collagen $\alpha-1$ (II) chain</td>
<td>MIRLGAPQTL..</td>
</tr>
<tr>
<td>DNA Sequence</td>
<td>Human genome</td>
<td>CGGTACCCTC..</td>
</tr>
</tbody>
</table>
<p>Table 1: Tokenizing Nature. Galactica trains on text sequences that represent scientific phenomena.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Total dataset size $=106$ billion tokens</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Data source</td>
<td style="text-align: right;">Documents</td>
<td style="text-align: right;">Tokens</td>
<td style="text-align: center;">Token \%</td>
</tr>
<tr>
<td style="text-align: left;">Papers</td>
<td style="text-align: right;">48 million</td>
<td style="text-align: right;">88 billion</td>
<td style="text-align: center;">$83.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">2 million</td>
<td style="text-align: right;">7 billion</td>
<td style="text-align: center;">$6.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Reference Material</td>
<td style="text-align: right;">8 million</td>
<td style="text-align: right;">7 billion</td>
<td style="text-align: center;">$6.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Knowledge Bases</td>
<td style="text-align: right;">2 million</td>
<td style="text-align: right;">2 billion</td>
<td style="text-align: center;">$2.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Filtered CommonCrawl</td>
<td style="text-align: right;">0.9 million</td>
<td style="text-align: right;">1 billion</td>
<td style="text-align: center;">$1.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Prompts</td>
<td style="text-align: right;">1.3 million</td>
<td style="text-align: right;">0.4 billion</td>
<td style="text-align: center;">$0.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: right;">0.02 million</td>
<td style="text-align: right;">0.2 billion</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: The Galactica Corpus. A full breakdown of these sources is contained in the Appendix.</p>
<h1>3 Dataset</h1>
<p>"Nature is written in that great book which ever is before our eyes - I mean the universe but we cannot understand it if we do not first learn the language and grasp the symbols in which it is written."</p>
<p>Galileo Galilei, The Assayer
The idea that Nature can be understood in terms of an underlying language has a long history Galilei, 1623; Wigner, 1959; Wheeler, 1990). In recent years, deep learning has been used to represent Nature, such as proteins and molecules (Jumper et al., 2021; Ross et al., 2021). Amino acids are an alphabet in which the language of protein structure is written, while atoms and bonds are the language of molecules. At a higher level, we organize knowledge through natural language, and many works have trained on scientific text (Beltagy et al., 2019; Lewis et al., 2020a; Gu et al., 2020; Lo et al., 2019b). With Galactica, we train a single neural network on a large scientific corpus to learn the different languages of science.
Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process $\mathrm{IA}_{\mathrm{E}} \mathrm{X}$ where we can capture it, and also include academic code to capture computational science. We highlight the corpus details in Table 1 and 2. Full details, including dataset components and filtering logic, are contained in the Appendix.</p>
<p>[START_AMINO]MIRLGAPQTLVLLTLLVAAVLRCQGQDVQEAGSCVQDGQRYNDKDVWKPEPCRICVCDTG... [END_AMINO]</p>
<h1>Summary</h1>
<p>Protein: Collagen alpha-1(II) chain
Gene: COL2A1
Organism: Homo sapiens (Human)
Status: evidence at protein level</p>
<h2>Function</h2>
<p>Type II collagen is specific for cartilaginous tissues. It is essential for the normal embryonic development of the skeleton, for linear growth and for the ability of cartilage to resist compressive forces. [START_REF]Nucleotide sequence of the full length cDNA encoding for human type II procollage, Lee[END_REF]...</p>
<h2>Features</h2>
<ul>
<li>Domain, 32-90, Cleavage; by procollagen N-endopeptidase</li>
<li>Site Cleavage, 181-182, Cleavage; by procollagen N-endopeptidase</li>
<li>Binding site, 1301, Ca2+
...</li>
</ul>
<p>Figure 1: Multi-Modal Data. A protein sequence occurs in a document context along with annotations, text and citations from UniProt. Full contents of the document are cut for clarity of exposition.</p>
<p>Notably the dataset is small and curated compared to other LLM corpuses, which are larger and uncurated. This is a key question of this work: can we make a working LLM based on a curated, normative paradigm? If true, we could make more purposefully-designed LLMs by having a clear understanding of what enters the corpus, similar to expert systems which had normative standards (Jackson, 1990).</p>
<h3>3.1 Tokenization</h3>
<p>Tokenization is an important part of dataset design given the different modalities present. For example, protein sequences are written in terms of amino acid residues, where character-based tokenization is appropriate. To achieve the goal of specialized tokenization, we utilize specialized tokens for different modalities:</p>
<ol>
<li>Citations: we wrap citations with special reference tokens [START_REF] and [END_REF].</li>
<li>Step-by-Step Reasoning: we wrap step-by-step reasoning with a working memory token <work>, mimicking an internal working memory context.</li>
<li>Mathematics: for mathematical content, with or without LaTeX, we split ASCII operations into individual characters. Parentheses are treated like digits. The rest of the operations allow for unsplit repetitions. Operation characters are !"#\$\%\&amp;'*+, -./:;&lt;=&gt;?\^_'| and parentheses are () [] {}.</li>
<li>Numbers: we split digits into individual tokens. For example 737612.62 -&gt; 7,3,7,6,1,2,.,6,2.</li>
<li>SMILES formula: we wrap sequences with [START_SMILES] and [END_SMILES] and apply characterbased tokenization. Similarly we use [START_I_SMILES] and [END_I_SMILES] where isomeric SMILES is denoted. For example, $C(C(=0) 0) N \rightarrow C,(, C,(,=, 0,), 0,), N$.</li>
<li>Amino acid sequences: we wrap sequences with [START_AMINO] and [END_AMINO] and apply character-based tokenization, treating each amino acid character as a single token. For example, MIRLGAPQTL -&gt; M,I,R,L,G,A,P,Q,T,L.</li>
<li>DNA sequences: we also apply a character-based tokenization, treating each nucleotide base as a token, where the start tokens are [START_DNA] and [END_DNA]. For example, CGGTACCCTC -&gt; C, G, G, T, A, C, C, C, T, C.</li>
</ol>
<p>We cover a few of the specialized token approaches below that do not have clear parallels in the literature, in particular the working memory and citation tokens.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Given a task like "What is the average of 43, 29, 51, 13?" a human can use internal or external working memory. In practice, they will use both symbiotically; meaning that working out that is written down in text is usually "missing" some steps performed internally.</p>
<h1>3.1.1 Working Memory Token, <work></h1>
<p>Transformer-based architectures lack an explicit working memory capability, which means a single-forward pass has limited efficacy. This is problematic for tasks that require multiple steps of computation. A current workaround is using a Transformer's output context as an external working memory to read from and write to. This is seen in recent work on chain-of-thought prompting Wei et al., 2022; Suzgun et al., 2022). In one sense this is intuitive, as humans also augment their limited working memory with scratchpads. In another sense, we would like models to refine their representations internally like humans; e.g. mental arithmetic.
There are two limitations with chain-of-thought. First, it relies on prompt discovery to find a prompt that elicits robust step-by-step reasoning; i.e. minimizes mistakes from doing too much in a single forward pass. Not only does this require finding a robust prompt that works in all cases, but it also often relies on few-shot examples which take up context space. What is worse, much of the step-by-step reasoning on the internet misses intermediate steps that a human has performed using internal memory. Humans do not write down every step they perform because it would lead to long and tedious answers. They write down the principal steps of reasoning, and do lower-level steps via internal working memory. This means there is "missing data" in written text, i.e. between written steps there are internal memory steps that are not explicitly stated.
Secondly, chain-of-thought prompting uses the neural network to perform tasks that it is arguably not best suited to doing; for example, arithmetic. Prior work has shown that accuracy on tasks like multiplication is proportional to term frequency Razeghi et al., 2022). Given that classical computers are specialized for tasks like arithmetic, one strategy is to offload these tasks from the neural network to external modules. For example, prior work has looked at the possibilities of external tool augmentation, such as calculators (Thoppilan et al., 2022). However, this requires a strategy to identify where the neural network should offload; and it may not be straightforward when combined with a discovered zero-shot prompt, especially where lower-level computation steps are not explicitly stated in writing.
Our solution is a working memory token we call <work>. We construct a few prompt datasets, see Table 3, that wrap step-by-by-step reasoning within <work> </work>. Some of these datasets were generated programmatically (OneSmallStep), by creating a problem template and sampling the variables, others were sourced online (Workout, Khan Problems), and others used existing datasets and transformed them into a <work> based context (GSM8k train). Where a computation is performed that a human could not do internally, we offload by writing and executing a Python script. An example is shown in Figure 3. Importantly, we do not have to turn this on, and the model can also predict the output from running a program. For our experiments, we did not find the need to turn Python offloading on, and leave this aspect to future work.
Longer term, an architecture change may be needed to support adaptive computation, so machines can have internal working memory on the lines of work such as adaptive computation time and PonderNet Graves, 2016; Banino et al., 2021). In this paper, we explore the <work> external working memory approach as a</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question: A needle 35 mm long rests on a water surface at $20^{\circ} \mathrm{C}$. What force over and above the needle's weight is required to lift the needle from contact with the water surface? $\sigma=0.0728 \mathrm{~m}$.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\sigma=0.0728 \mathrm{~N} / \mathrm{m}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\sigma=F / L$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$0.0728=F /(2 \times 0.035)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$F=0.0728(2 \times 0.035)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">calculate.py</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">f = 0.0728<em>(2</em>0.035)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">with open("output.txt", "w") as file: file.write(str(round(f, 5)))</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">"</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">«run: "calculate.py"&gt;</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">«read: "output.txt"»</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0.0051</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></work></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: $F=0.0051 \mathrm{~N}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: Model-Machine Symbiosis. We show an example answer with the <work> working memory token. It performs exact steps for rearranging the equation, and when it reaches a calculation that it cannot solve reliably in a forward-pass, it writes a program, which can then be offloaded to a classical computer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data source</th>
<th style="text-align: center;">Split</th>
<th style="text-align: right;">Prompts</th>
<th style="text-align: right;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GSM8k (Cobbe et al., 2021)</td>
<td style="text-align: center;">train</td>
<td style="text-align: right;">7,473</td>
<td style="text-align: right;">$3,518,467$</td>
</tr>
<tr>
<td style="text-align: left;">OneSmallStep</td>
<td style="text-align: center;">$n / a$</td>
<td style="text-align: right;">9,314</td>
<td style="text-align: right;">$3,392,252$</td>
</tr>
<tr>
<td style="text-align: left;">Khan Problems (Hendrycks et al., 2021)</td>
<td style="text-align: center;">$n / a$</td>
<td style="text-align: right;">3,835</td>
<td style="text-align: right;">$1,502,644$</td>
</tr>
<tr>
<td style="text-align: left;">Workout</td>
<td style="text-align: center;">$n / a$</td>
<td style="text-align: right;">921</td>
<td style="text-align: right;">470,921</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;"></td>
<td style="text-align: right;">21,543</td>
<td style="text-align: right;">9 million</td>
</tr>
</tbody>
</table>
<p>Table 3: Reasoning Datasets To train the model to use <work> we include several datasets in pre-training that incorporate this token. Full details are contained in the Appendix.
bridge to the next step. Notably our <work> prompt datasets are not very large or diverse, so there are likely large further gains to be made with this approach.</p>
<h1>3.1.2 Citation Token</h1>
<p>A distinctive properties of academic text is citations. In order to represent the implicit citation graph within the text, we process citations with global identifiers and special tokens [START_REF] and [END_REF] signifying when a citation is made. Figure 4 shows an example of citation processed text from a paper.</p>
<p>Recurrent neural networks, long short-term memory [START_REF]Long Short-Term Memory, Hochreiter[END_REF] and gated recurrent [START_REF]Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung[END_REF] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [START_REF]Sequence to Sequence Learning with Neural Networks, Sutskever[END_REF] [START_REF]Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau[END_REF] [START_REF]Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation, Cho[END_REF].</p>
<p>Figure 4: Citation Processed Text. Example of citation processed text from Attention Is All You Need (Vaswani et al., 2017). For title-processed citations, the title can be associated with the previous context.</p>
<p>We considered two type of citation identifier: (a) paper titles and (b) alphanumeric IDs. Based on ablations, we found that title based identifiers have greater citation prediction accuracy than IDs. However, we also found that paper titles are more prone to hallucination error at lower scales given the text-based nature of the identifier. We consider title processing for this paper, but we note the trade-offs between both approaches. Experiments for these ablations are contained in the Appendix.</p>
<h3>3.2 Prompt Pre-Training</h3>
<p>We deviate from existing language model research in one important direction, which is our decision to include prompts in pre-training alongside the general corpora. This is motivated by a number of observations.
First, existing work has shown the importance of training token count on performance. The Chinchilla paper derived scaling "laws" taking into account number of tokens, training a 70bn model for 1.4 trillion tokens (Hoffmann et al., 2022). They obtained state-of-the-art performance on MMLU, beating much larger models such as Gopher (Rae et al., 2021).
Separately, research such as FLAN and T0 showed prompt tuning can boost downstream performance (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022). Their strategy involved converting tasks to text prompts, using prompt diversity in how the tasks are posed, and then fine-tuning on these prompt datasets. For FLAN and T0, this approach boosts performance, beating larger models such as GPT-3 on many tasks.
And additionally there is the UnifiedQA approach (Khashabi et al., 2020). In this approach, a T5 model is fine-tuned on question answering datasets, and is shown to boost performance on out-of-domain question answering datasets (Raffel et al., 2020). The model outperforms GPT-3 on MMLU, a model 16 times larger.
The first stream of research above focuses on total training tokens as a way to boost performance; i.e. it is token agnostic. The second stream of research focuses on task-context tokens as a way to boost performance; i.e. it is token selective. Since fine-tuned smaller models beat larger few-shot models on tasks like MMLU, this suggests world knowledge may be present in smaller models, but task-context knowledge may be poor given the relative number of task-context tokens seen in the general corpus.
For this paper, we opt to augment pre-training data with more task prompts to boost performance at lower scales. This is advantageous if it obviates the need for more data scale, e.g. a $&gt;1$ trillion corpus, or more model scale. The largest 120B model we train runs on a single NVIDIA A100 node. Additionally, given that fine-tuning requires expertise, making the model work out-the-box for popular tasks like question answering and summarization is more useful for users of the model. Lastly, by including prompts alongside general data, we maximize the generality of the model while boosting performance on some tasks of interest.
The closest analog to this approach for large language models is ExT5 (Aribandi et al., 2021). We take a similar approach by taking many machine learning training datasets, converting them to a text format, with prompt diversity, and then including them alongside general corpora in our pre-training set. A summary of prompt types is given in Table 4; the full details of datasets and prompts used are covered in the Appendix.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Increasing task generality</p>
<p>Figure 5: Prompt Pre-training. Pre-training weighs all tokens equally as part of the self-supervised loss. This leads to a weak relative signal for tasks of interest, meaning model scale has to be large to work. Instruction tuning boosts performance post hoc, and can generalize to unseen tasks of interest, but it risks performance in tasks that are distant from instruction set tasks. Prompt pre-training has a weaker task of interest bias than instruction tuning but less risk of degrading overall task generality.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: right;">Prompts</th>
<th style="text-align: right;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chemical Properties</td>
<td style="text-align: right;">782,599</td>
<td style="text-align: right;">275 million</td>
</tr>
<tr>
<td style="text-align: left;">Multiple-Choice QA</td>
<td style="text-align: right;">256,886</td>
<td style="text-align: right;">31 million</td>
</tr>
<tr>
<td style="text-align: left;">Extractive QA</td>
<td style="text-align: right;">30,935</td>
<td style="text-align: right;">13 million</td>
</tr>
<tr>
<td style="text-align: left;">Summarization</td>
<td style="text-align: right;">6,339</td>
<td style="text-align: right;">11 million</td>
</tr>
<tr>
<td style="text-align: left;">Entity Extraction</td>
<td style="text-align: right;">156,007</td>
<td style="text-align: right;">9 million</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning</td>
<td style="text-align: right;">21,543</td>
<td style="text-align: right;">9 million</td>
</tr>
<tr>
<td style="text-align: left;">Dialog</td>
<td style="text-align: right;">18,930</td>
<td style="text-align: right;">5 million</td>
</tr>
<tr>
<td style="text-align: left;">Binary QA</td>
<td style="text-align: right;">36,334</td>
<td style="text-align: right;">4 million</td>
</tr>
<tr>
<td style="text-align: left;">Other</td>
<td style="text-align: right;">3,559</td>
<td style="text-align: right;">1 million</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">783,599</td>
<td style="text-align: right;">358 million</td>
</tr>
</tbody>
</table>
<p>Table 4: Pre-training Prompts. We include zero-shot prompts in pre-training to boost the task signal.</p>
<p>Because of prompt inclusion, it is important to distinguish between in-domain performance, where the training dataset is included in pre-training, and out-of-domain performance, where the training dataset is not included in pre-training. We mark these results clearly in the Results section of this paper. Importantly, we do not advocate for prompt pre-training as an alternative to instruction tuning. In fact, instruction tuning on Galactica is likely useful follow-up work given its potential to boost performance on several tasks of interest.</p>
<h1>4 Method</h1>
<h3>4.1 Architecture</h3>
<p>Galactica uses a Transformer architecture in a decoder-only setup (Vaswani et al., 2017), with the following modifications:</p>
<ul>
<li>GeLU Activation - we use GeLU activations for all model sizes (Hendrycks and Gimpel, 2016).</li>
<li>Context Window - we use a 2048 length context window for all model sizes.</li>
<li>No Biases - following PaLM, we do not use biases in any of the dense kernels or layer norms (Chowdhery et al., 2022).</li>
<li>Learned Positional Embeddings - we use learned positional embeddings for the model. We experimented with ALiBi at smaller scales but did not observe large gains, so we did not use it (Press et al., 2021).</li>
<li>Vocabulary - we construct a vocabulary of 50k tokens using BPE (Sennrich et al., 2015). The vocabulary was generated from a randomly selected $2 \%$ subset of the training data.</li>
</ul>
<h3>4.2 Models</h3>
<p>The different model sizes we trained, along with training hyperparameters are outlined in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$n_{\text {params }}$</th>
<th style="text-align: center;">$n_{\text {layers }}$</th>
<th style="text-align: center;">$d_{\text {model }}$</th>
<th style="text-align: center;">$n_{\text {heads }}$</th>
<th style="text-align: center;">$d_{\text {heads }}$</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Max LR</th>
<th style="text-align: center;">Warmup</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.5 M</td>
<td style="text-align: center;">$6 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: center;">1.3 B</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2,048</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">1.0 M</td>
<td style="text-align: center;">$2 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: center;">6.7 B</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2.0 M</td>
<td style="text-align: center;">$1.2 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: center;">30.0 B</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">7,168</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2.0 M</td>
<td style="text-align: center;">$1 \times 10^{-4}$</td>
<td style="text-align: center;">375 M</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: center;">120.0 B</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">10,240</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2.0 M</td>
<td style="text-align: center;">$0.7 \times 10^{-5}$</td>
<td style="text-align: center;">1.125 B</td>
</tr>
</tbody>
</table>
<p>Table 5: Details of the models trained</p>
<p>We train using AdamW with $\beta_{1}=0.9, \beta_{2}=0.95$ and weight decay of 0.1 (Loshchilov and Hutter, 2017). We clip the global norm of the gradient at 1.0 , and we use linear decay for learning rate down to $10 \%$ of it value. We use dropout and attention dropout of $p=0.1$. We do not use embedding dropout. We found longer warmup was important for the largest model in the early stages of training to protect against the effects of bad initialization, which can have long-memory effects on the optimizer variance state and slow down learning. This may be specific to our model and training setup, and it is not clear whether this advice generalizes.</p>
<h3>4.3 Libraries and Infrastructure</h3>
<p>We use the metaseq library ${ }^{3}$ for training the models, built by the NextSys team at Meta AI.
For training the largest 120B model, we use 128 NVIDIA A100 80GB nodes. For inference Galactica 120B requires a single A100 node. We choose the maximum model size to obey this constraint for downstream accessibility, and we will work to improve its accessibility for the research community in coming months.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Repeated Tokens and Validation Loss. With four epochs of training, we continue to see validation loss fall for all model sizes. For the 120B model we see the first signs of overfitting at the beginning of the fifth epoch, and we early stop at this point.</p>
<h1>5 Results</h1>
<h3>5.1 Repeated Tokens Considered Not Harmful</h3>
<p>We train the models for 450 billion tokens, or approximately 4.25 epochs. We find that performance continues to improve on validation set, in-domain and out-of-domain benchmarks with multiple repeats of the corpus.
First, from Figure 6, validation loss continues to fall with four epochs of training. The largest 120B model only begins to overfit at the start of the fifth epoch. This is unexpected as existing research suggests repeated tokens can be harmful on performance (Hernandez et al., 2022). We also find the 30B and 120B exhibit a epoch-wise double descent effect of plateauing (or rising) validation loss followed by a decline. This effect becomes stronger with each epoch, and is most visible above with the 120B model towards end of training.
To investigate further, we examine the per-source breakdown of validation loss to see if there is heterogeneity in loss behaviour. We plot example curves in Figure 23 overleaf for the 30B model. We see no signs of loss heterogeneity: loss falls for all sources. The 120B exhibits the same relative trend of declining validation loss for all sources until the beginning of fifth epoch, where all sources spike (see Appendix).
The next question to answer is whether this trend extends to downstream performance and out-of-domain generalization. For this we use a 57 task subset of BIG-bench subset, a general corpus with principally nonscientific tasks and prompt types not included in pre-training (Srivastava et al., 2022). We plot results in Figure 8. We see no signs of overfitting suggesting that use of repeated tokens is improving downstream performance as well as upstream performance.
We suspect that two factors could be at play, a quality factor, the curated nature of the corpus enables more value per token to be extracted, or a modality factor, the nature of scientific data enables more value per token to be extracted. The missing step of causation is what leads specifically from either factor towards less overfitting, and we leave this question to further work. We note the implication that the "tokens $\rightarrow \infty$ " focus of current LLM projects may be overemphasised versus the importance of filtering the corpus for quality.
In the following sections, we turn to evaluating Galactica's scientific capabilities. Specifically, we focus on the high-level design goals of building an LLM that can store, combine and reason about scientific knowledge as these are needed for building a new interface for science.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Validation Loss Per Source. Validation loss falls through training for all dataset categories. Results are shown for the 30B model above. The 120B exhibits the same relative trend of declining validation loss for all sources until the beginning of fifth epoch, where all sources spike (see Appendix).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: BIG-bench Performance During Training. The 57 task selection from BIG-bench contains principally non-scientific tasks. We use it as a proxy for out-of-domain performance. For the 120B model above, we see no signs of overfitting after four repeats of the corpus.</p>
<h1>5.2 Knowledge Probes</h1>
<p>First, we examine how well Galactica absorbs scientific knowledge. We set up several knowledge probe benchmarks, building off the LAMA approach of petroni2019. These were critical metrics during model development for identifying knowledge gaps within the corpus, and informing how to iterate the corpus. They also provide insight into the relative knowledge strengths of Galactica versus general language models, and we cover these results in this section before turning to the downstream tasks.</p>
<h3>5.2.1 LaTeX Equations</h3>
<p>We construct a dataset of popular LaTeX equations from the fields of chemistry, physics, mathematics, statistics and economics. Memorisation of equations is useful to measure as it is necessary for many downstream tasks; for example, recalling an equation to use as part of an answer to a problem. Unless stated explicitly, Galactica results are reported as zero-shot. In total there are 434 equations we test for the knowledge probe.
We prompt with an equation name and generate LaTeX. An example is shown in Figure 9.</p>
<h2>Prompt</h2>
<p>The formula for Bessel's differential equation is:
Generated Answer</p>
<p>$$
x^{2} \frac{d^{2} y}{d x^{2}}+x \frac{d y}{d x}+\left(x^{2}-\alpha^{2}\right) y=0
$$</p>
<p>Figure 9: LaTeX Equations Probe. We prompt for the name of an equation and evaluate whether the generated LaTeX is correct. We manually evaluate given the possibility of multiple correct answers.</p>
<p>We summarize the results in Table 6. Equation knowledge increases smoothly with scale. Galactica outperforms larger language models trained on general corpuses, indicating the value of a curated dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Chemistry</th>
<th style="text-align: right;">Maths</th>
<th style="text-align: right;">Physics</th>
<th style="text-align: right;">Stats</th>
<th style="text-align: right;">Econ</th>
<th style="text-align: right;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">$34.1 \%$</td>
<td style="text-align: right;">$4.5 \%$</td>
<td style="text-align: right;">$22.9 \%$</td>
<td style="text-align: right;">$1.0 \%$</td>
<td style="text-align: right;">$2.3 \%$</td>
<td style="text-align: right;">$8.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$36.3 \%$</td>
<td style="text-align: right;">$36.1 \%$</td>
<td style="text-align: right;">$6.6 \%$</td>
<td style="text-align: right;">$14.1 \%$</td>
<td style="text-align: right;">$13.6 \%$</td>
<td style="text-align: right;">$21.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 (text-davinci-002)</td>
<td style="text-align: right;">$?$</td>
<td style="text-align: right;">$61.4 \%$</td>
<td style="text-align: right;">$65.4 \%$</td>
<td style="text-align: right;">$41.9 \%$</td>
<td style="text-align: right;">$25.3 \%$</td>
<td style="text-align: right;">$31.8 \%$</td>
<td style="text-align: right;">$49.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$0.8 \%$</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$1.0 \%$</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$31.8 \%$</td>
<td style="text-align: right;">$26.3 \%$</td>
<td style="text-align: right;">$23.8 \%$</td>
<td style="text-align: right;">$11.1 \%$</td>
<td style="text-align: right;">$4.6 \%$</td>
<td style="text-align: right;">$20.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$43.2 \%$</td>
<td style="text-align: right;">$59.4 \%$</td>
<td style="text-align: right;">$36.2 \%$</td>
<td style="text-align: right;">$29.3 \%$</td>
<td style="text-align: right;">$27.3 \%$</td>
<td style="text-align: right;">$41.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$63.6 \%$</td>
<td style="text-align: right;">$74.4 \%$</td>
<td style="text-align: right;">$35.2 \%$</td>
<td style="text-align: right;">$40.4 \%$</td>
<td style="text-align: right;">$34.1 \%$</td>
<td style="text-align: right;">$51.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{7 9 . 6 \%}$</td>
<td style="text-align: right;">$\mathbf{8 3 . 5 \%}$</td>
<td style="text-align: right;">$\mathbf{7 2 . 4 \%}$</td>
<td style="text-align: right;">$\mathbf{5 2 . 5 \%}$</td>
<td style="text-align: right;">$\mathbf{3 6 . 4 \%}$</td>
<td style="text-align: right;">$\mathbf{6 8 . 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on LaTeX equations. Results are evaluated zero-shot.</p>
<h3>5.2.2 Domain Probes</h3>
<p>We also set up domain probes to track specialized knowledge for certain fields. We detail these below:</p>
<ul>
<li>AminoProbe: a dataset of names, structures and properties of the 20 common amino acids.</li>
<li>BioLAMA: a dataset of biomedical factual knowledge triples.</li>
<li>Chemical Reactions: a dataset of chemical reactions.</li>
<li>Galaxy Clusters: a dataset of galaxy clusters with their constellation classifications.</li>
<li>Mineral Groups: a dataset of minerals and their mineral group classifications.</li>
</ul>
<p>In each case, we construct a prompt to test the knowledge. For example, for Chemical Reactions, we ask Galactica to predict the products of the reaction in the chemical equation LaTeX. We mask out products in the description so the model is inferring based on the reactants only. An example is shown in Figure 10.</p>
<h1>Prompt</h1>
<p>Sulfuric acid reacts with sodium chloride, and gives $\qquad$ and $\qquad$ :
[ \ce{ NaCl + H2SO4 -&gt;}
Generated Answer</p>
<p>$$
\mathrm{NaCl}+\mathrm{H}<em 4="4">{2} \mathrm{SO}</em>
$$} \longrightarrow \mathrm{NaHSO}_{4}+\mathrm{HCl</p>
<p>Figure 10: Chemical Reactions. We prompt based on a description and reactants, and evaluate whether the generated products are correct.</p>
<p>We report results for these knowledge probes in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Amino</th>
<th style="text-align: right;">BioLAMA</th>
<th style="text-align: right;">Reactions</th>
<th style="text-align: right;">Clusters</th>
<th style="text-align: right;">Minerals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">$12.0 \%$</td>
<td style="text-align: right;">$7.1 \%$</td>
<td style="text-align: right;">$12.7 \%$</td>
<td style="text-align: right;">$21.7 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$14.0 \%$</td>
<td style="text-align: right;">$\mathbf{9 . 7 \%}$</td>
<td style="text-align: right;">$22.4 \%$</td>
<td style="text-align: right;">$15.0 \%$</td>
<td style="text-align: right;">$10.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 (text-davinci-002)</td>
<td style="text-align: right;">$?$</td>
<td style="text-align: right;">$14.0 \%$</td>
<td style="text-align: right;">$8.4 \%$</td>
<td style="text-align: right;">$35.1 \%$</td>
<td style="text-align: right;">$20.8 \%$</td>
<td style="text-align: right;">$18.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$12.0 \%$</td>
<td style="text-align: right;">$3.1 \%$</td>
<td style="text-align: right;">$0.3 \%$</td>
<td style="text-align: right;">$6.7 \%$</td>
<td style="text-align: right;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$16.0 \%$</td>
<td style="text-align: right;">$7.2 \%$</td>
<td style="text-align: right;">$14.4 \%$</td>
<td style="text-align: right;">$14.2 \%$</td>
<td style="text-align: right;">$10.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$17.0 \%$</td>
<td style="text-align: right;">$7.9 \%$</td>
<td style="text-align: right;">$26.4 \%$</td>
<td style="text-align: right;">$17.5 \%$</td>
<td style="text-align: right;">$8.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$21.0 \%$</td>
<td style="text-align: right;">$6.9 \%$</td>
<td style="text-align: right;">$36.5 \%$</td>
<td style="text-align: right;">$20.0 \%$</td>
<td style="text-align: right;">$17.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{2 1 . 0 \%}$</td>
<td style="text-align: right;">$8.0 \%$</td>
<td style="text-align: right;">$\mathbf{4 3 . 1 \%}$</td>
<td style="text-align: right;">$\mathbf{2 4 . 2 \%}$</td>
<td style="text-align: right;">$\mathbf{2 9 . 4 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Results on Domain Probes. Results are evaluated zero-shot.</p>
<p>We also observe steady scaling behaviour in these knowledge probes, with the exception of BioLAMA which we suspect reflects zero-shot prompt difficulty for all LLMs. Notably fine-grained factual knowledge, such as "ConstellationOf (GalaxyCluster)" type-queries seems to scale smoothly with the size of the model.</p>
<table>
<thead>
<tr>
<th>5.2.3 Reasoning</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>We now turn to reasoning capabilities with the <work> token. We start by evaluating on the MMLU mathematics benchmarks, which we report in Table 8 <em>(Hendrycks et al., 2020)</em>. Galactica performs strongly compared to larger base models, and use of the <work> token appears to boost performance over Chinchilla, even for the smaller 30B Galactica model.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mathematics MMLU</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Model</td>
<td>Params (bn)</td>
<td>A.Algebra</td>
<td>Elem</td>
<td>HS</td>
<td>College</td>
<td>F. Logic</td>
<td>Average</td>
<td></td>
</tr>
<tr>
<td>BLOOM (5-shot)</td>
<td>176</td>
<td>25.0%</td>
<td>26.7%</td>
<td>27.0%</td>
<td>25.0%</td>
<td>26.2%</td>
<td>26.4%</td>
<td></td>
</tr>
<tr>
<td>OPT (5-shot)</td>
<td>175</td>
<td>21.0%</td>
<td>25.7%</td>
<td>24.4%</td>
<td>33.0%</td>
<td>29.4%</td>
<td>26.7%</td>
<td></td>
</tr>
<tr>
<td>Gopher (5-shot)</td>
<td>280</td>
<td>25.0%</td>
<td>33.6%</td>
<td>23.7%</td>
<td>37.0%</td>
<td>35.7%</td>
<td>30.6%</td>
<td></td>
</tr>
<tr>
<td>Chinchilla (5-shot)</td>
<td>70</td>
<td>31.0%</td>
<td>41.5%</td>
<td>31.9%</td>
<td>32.0%</td>
<td>33.3%</td>
<td>35.7%</td>
<td></td>
</tr>
<tr>
<td>GAL 1.3B</td>
<td>1.3</td>
<td>28.0%</td>
<td>27.2%</td>
<td>26.7%</td>
<td>30.0%</td>
<td>24.6%</td>
<td>27.1%</td>
<td></td>
</tr>
<tr>
<td>GAL 6.7B</td>
<td>6.7</td>
<td>28.0%</td>
<td>28.9%</td>
<td>26.7%</td>
<td>36.0%</td>
<td>31.0%</td>
<td>29.2%</td>
<td></td>
</tr>
<tr>
<td>GAL 30B</td>
<td>30</td>
<td>30.0%</td>
<td>30.2%</td>
<td>26.3%</td>
<td>36.0%</td>
<td>31.7%</td>
<td>29.9%</td>
<td></td>
</tr>
<tr>
<td>GAL 120B</td>
<td>120</td>
<td>33.0%</td>
<td>38.1%</td>
<td>32.6%</td>
<td>43.0%</td>
<td>32.5%</td>
<td>35.8%</td>
<td></td>
</tr>
<tr>
<td>GAL 1.3B <work></td>
<td>1.3</td>
<td>22.0%</td>
<td>24.6%</td>
<td>18.9%</td>
<td>25.0%</td>
<td>31.0%</td>
<td>24.6%</td>
<td></td>
</tr>
<tr>
<td>GAL 6.7B <work></td>
<td>6.7</td>
<td>33.3%</td>
<td>30.7%</td>
<td>25.2%</td>
<td>26.0%</td>
<td>33.3%</td>
<td>28.0%</td>
<td></td>
</tr>
<tr>
<td>GAL 30B <work></td>
<td>30</td>
<td>33.0%</td>
<td>41.5%</td>
<td>33.3%</td>
<td>39.0%</td>
<td>37.3%</td>
<td>37.1%</td>
<td></td>
</tr>
<tr>
<td>GAL 120B <work></td>
<td>120</td>
<td>27.0%</td>
<td>54.2%</td>
<td>37.0%</td>
<td>44.0%</td>
<td>40.5%</td>
<td>41.3%</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 8: Results on Mathematics MMLU. Galactica is evaluated without few-shot examples. With the <work> token we see large gains in performance. Results are on MMLU test.</p>
<p>We also evaluate on the MATH dataset to further probe the reasoning capabilities of Galactica <em>(Hendrycks et al., 2021)</em>. We compare the <work> token prompt directly with the Minerva 5-shot chain-of-thought prompt mCoT for comparability. We report results in Table 9.</p>
<p>MATH Results | Model | Alg | CProb | Geom | I.Alg | N.Theory | Prealg | Precalc | Average | | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---</p>
<h1>5.3 Downstream Scientific NLP</h1>
<p>We now evaluate on downstream scientific tasks to see how well Galactica can compose its knowledge in different task contexts. We focus on knowledge-intensive scientific tasks and report full results in Table 10. For this we use the MMLU benchmark as well as some other popular scientific QA benchmarks. We include the MMLU results earlier without <work> to test for knowledge association specifically. Full MMLU results, including social sciences and other fields, are reported in the Appendix. We also perform data leakage analysis on these benchmarks for more confidence; results are in the Appendix.
From Table 10, Galactica can compose its knowledge into the question-answering task, and performance is strong; significantly outperforming the other open language models, and outperforming a larger model (Gopher 280B) in the majority of tasks. Performance against Chinchilla is more variable, and Chinchilla appears to be stronger in a subset of tasks: in particular, high-school subjects and less-mathematical, more memorization intensive tasks. In contrast, Galactica tends to perform better in mathematical and graduatelevel tasks.
Our working hypothesis is that the Galactica corpus is biased towards graduate scientific knowledge, given it consists mostly of papers, which explains lagging performance in high-school subjects. While we do pick up some high-school level content through encyclopedias, textbooks and the filtered CommonCrawl, this amounts to a small quantity of tokens (a few billion). We leave the question of how to capture more of this base scientific knowledge in a curated way to future work.
On remaining tasks, we achieve state-of-the-art results over fine-tuned models at the time of writing. On PubMedQA, we achieve a score of $77.6 \%$ which outperforms the state-of-the-art of $72.2 \%$ (Yasunaga et al., 2022). On MedMCQA dev we achieve score of $52.9 \%$ versus the state-of-the-art of $41.0 \%$ (Gu et al., 2020). For BioASQ and MedQA-USMLE, performance is close to the state-of-the-art performance of fine-tuned models ( $94.8 \%$ and $44.6 \%$ ) (Yasunaga et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Domain</th>
<th style="text-align: center;">GAL</th>
<th style="text-align: center;">OPT</th>
<th style="text-align: center;">BLOOM</th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;">Gopher</th>
<th style="text-align: center;">Chinchilla</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Abstract Algebra</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{3 3 . 3 \%}$</td>
<td style="text-align: center;">$21.0 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">$31.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ARC Challenge</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{6 7 . 9 \%}$</td>
<td style="text-align: center;">$31.1 \%$</td>
<td style="text-align: center;">$32.9 \%$</td>
<td style="text-align: center;">$51.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ARC Easy</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{8 3 . 8 \%}$</td>
<td style="text-align: center;">$37.4 \%$</td>
<td style="text-align: center;">$40.7 \%$</td>
<td style="text-align: center;">$68.8 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Astronomy</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$65.1 \%$</td>
<td style="text-align: center;">$23.0 \%$</td>
<td style="text-align: center;">$25.7 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$65.8 \%$</td>
<td style="text-align: center;">$\mathbf{7 3 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{9 4 . 3 \%}$</td>
<td style="text-align: center;">$81.4 \%$</td>
<td style="text-align: center;">$91.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Biology (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$68.8 \%$</td>
<td style="text-align: center;">$30.6 \%$</td>
<td style="text-align: center;">$28.5 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$70.8 \%$</td>
<td style="text-align: center;">$\mathbf{7 9 . 9 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Biology (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$69.4 \%$</td>
<td style="text-align: center;">$27.7 \%$</td>
<td style="text-align: center;">$29.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$71.3 \%$</td>
<td style="text-align: center;">$\mathbf{8 0 . 3 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$46.0 \%$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$19.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$45.0 \%$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Chemistry (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$47.8 \%$</td>
<td style="text-align: center;">$21.7 \%$</td>
<td style="text-align: center;">$23.2 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$47.8 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 1 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Comp. Science (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$49.0 \%$</td>
<td style="text-align: center;">$17.0 \%$</td>
<td style="text-align: center;">$6.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$49.0 \%$</td>
<td style="text-align: center;">$\mathbf{5 1 . 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Comp. Science (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{7 0 . 0 \%}$</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$54.0 \%$</td>
<td style="text-align: center;">$58.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Econometrics</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$42.1 \%$</td>
<td style="text-align: center;">$21.0 \%$</td>
<td style="text-align: center;">$23.7 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{4 3 . 0 \%}$</td>
<td style="text-align: center;">$38.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Electrical Engineering</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{6 2 . 8 \%}$</td>
<td style="text-align: center;">$36.6 \%$</td>
<td style="text-align: center;">$32.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$60.0 \%$</td>
<td style="text-align: center;">$62.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Elementary Mathematics</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$38.1 \%$</td>
<td style="text-align: center;">$25.7 \%$</td>
<td style="text-align: center;">$27.6 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$33.6 \%$</td>
<td style="text-align: center;">$\mathbf{4 1 . 5 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Formal Logic</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$32.5 \%$</td>
<td style="text-align: center;">$29.4 \%$</td>
<td style="text-align: center;">$26.2 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 5 . 7 \%}$</td>
<td style="text-align: center;">$33.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Machine Learning</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$38.4 \%$</td>
<td style="text-align: center;">$28.6 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$41.1 \%$</td>
<td style="text-align: center;">$41.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mathematics (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{4 3 . 0 \%}$</td>
<td style="text-align: center;">$33.0 \%$</td>
<td style="text-align: center;">$25.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$37.0 \%$</td>
<td style="text-align: center;">$32.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mathematics (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{3 2 . 6 \%}$</td>
<td style="text-align: center;">$24.4 \%$</td>
<td style="text-align: center;">$27.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$23.7 \%$</td>
<td style="text-align: center;">$31.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Medical Genetics</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$\mathbf{7 0 . 0 \%}$</td>
<td style="text-align: center;">$35.0 \%$</td>
<td style="text-align: center;">$36.0 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$69.0 \%$</td>
<td style="text-align: center;">$69.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Physics (College)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$42.2 \%$</td>
<td style="text-align: center;">$21.6 \%$</td>
<td style="text-align: center;">$18.6 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$34.3 \%$</td>
<td style="text-align: center;">$\mathbf{4 6 . 1 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Physics (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$33.8 \%$</td>
<td style="text-align: center;">$29.8 \%$</td>
<td style="text-align: center;">$25.2 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$33.8 \%$</td>
<td style="text-align: center;">$\mathbf{3 6 . 4 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">MedQA-USMLE</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$44.4 \%$</td>
<td style="text-align: center;">$22.8 \%$</td>
<td style="text-align: center;">$23.3 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MedMCQA Dev</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{5 2 . 9 \%}$</td>
<td style="text-align: center;">$29.6 \%$</td>
<td style="text-align: center;">$32.5 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PubMedQA</td>
<td style="text-align: left;">in-domain</td>
<td style="text-align: center;">$\mathbf{7 7 . 6 \%}$</td>
<td style="text-align: center;">$70.2 \%$</td>
<td style="text-align: center;">$73.6 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Statistics (High-School)</td>
<td style="text-align: left;">out-of-domain</td>
<td style="text-align: center;">$41.2 \%$</td>
<td style="text-align: center;">$43.5 \%$</td>
<td style="text-align: center;">$19.4 \%$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">$\mathbf{5 8 . 8 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 10: Question Answering Results. Galactica is evaluated without few-shot examples. Other LLMs are evaluated 5-shot, except for 0 -shot results for GPT-3 on ARC results and OPT and BLOOM on PubMedQA and BioASQ. For abstract algebra and medical genetics, we obtained best results with 30B, so we report these scores; the 120B scores for these were $27.0 \%$ and $68.0 \%$ respectively. Rest of results are for 120B.</p>
<h1>5.4 Citation Prediction</h1>
<p>In this section we evaluate Galactica's capability to predict citations given an input context, which is an important test of Galactica's capability to organize the scientific literature. We find that both accuracy and the quality of distributional approximation improves with scale.</p>
<h3>5.4.1 Citation Accuracy</h3>
<p>We construct three datasets to evaluate the model's capability to cite:</p>
<ul>
<li>PWC Citations: a dataset with 644 pairs of machine learning concepts and papers that introduced them. Concepts consist of methods (e.g. ResNet) and datasets (e.g. ImageNet) from Papers with Code ${ }^{4}$.</li>
<li>Extended Citations: a dataset with 110 pairs of non-machine learning concepts and papers that introduced them. Examples of concepts include Kozac sequence and Breit-Wigner distribution.</li>
<li>Contextual Citations: a dataset with 1,869 pairs of references and contexts from our arXiv validation set. The dataset is constructed by sampling 1,000 random references and collecting their contexts.</li>
</ul>
<p>For the PWC Citations and Extended Citations datasets, the citation prediction task is framed as a text generation task. The model is given a prompt like "In this paper we use ResNet method [START_REF]" in order to generate a prediction for the ResNet concept. For Contextual Citations, we prompt after the input context for the citation, where the context ends with [START_REF].
We compare Galactica to sparse and dense retrieval-based approaches on this task.
For the sparse baseline, we use ElasticSearch to create an index of all the references, including their titles, abstracts, and short snippets of text with the contexts they appear in. Then, given a text query, we retrieve the top references ordered by the sum of matching scores across all selected fields.
For dense retriever baselines, we evaluate two different Contriever models (Izacard et al., 2021). The first is the pre-trained model released by Izacard et al. (2021). The second model we use is fine-tuned on a random subset of 10 million context/paper pairs from our corpus, trained to retrieve the right paper given a context before a citation. The setup for dense retrieval is: (1) each reference is encoded by the model using its title and abstract, (2) a text query is encoded by the same model, (3) the references that match the query re returned. Retrieval is performed using a FAISS index (Johnson et al., 2019).
The results can be seen in Table 11.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">PWC Citations</th>
<th style="text-align: right;">Extended Citations</th>
<th style="text-align: right;">Contextual Citations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$7.0 \%$</td>
<td style="text-align: right;">$6.4 \%$</td>
<td style="text-align: right;">$7.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$18.5 \%$</td>
<td style="text-align: right;">$45.5 \%$</td>
<td style="text-align: right;">$15.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$32.0 \%$</td>
<td style="text-align: right;">$60.0 \%$</td>
<td style="text-align: right;">$23.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$44.7 \%$</td>
<td style="text-align: right;">$66.4 \%$</td>
<td style="text-align: right;">$31.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{5 1 . 9 \%}$</td>
<td style="text-align: right;">$\mathbf{6 9 . 1 \%}$</td>
<td style="text-align: right;">$\mathbf{3 6 . 6 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Sparse Retriever</td>
<td style="text-align: right;">$\mathrm{n} / \mathrm{a}$</td>
<td style="text-align: right;">$30.9 \%$</td>
<td style="text-align: right;">$17.3 \%$</td>
<td style="text-align: right;">$5.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Dense Retriever (base)</td>
<td style="text-align: right;">$\mathrm{n} / \mathrm{a}$</td>
<td style="text-align: right;">$16.4 \%$</td>
<td style="text-align: right;">$8.8 \%$</td>
<td style="text-align: right;">$1.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Dense Retriever (fine-tuned)</td>
<td style="text-align: right;">$\mathrm{n} / \mathrm{a}$</td>
<td style="text-align: right;">$27.6 \%$</td>
<td style="text-align: right;">$11.8 \%$</td>
<td style="text-align: right;">$8.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 11: Citation Prediction Accuracy. Performance of different model sizes on citation prediction.</p>
<p>The performance on all evaluation sets increases smoothly with scale. At larger scales, Galactica outperforms the retrieval-based approaches as its context-associative power improves. This is an important result as current approaches for navigating the literature use these existing retrieval approaches. As the power of language models improves, we suspect they will become a valuable new tool for exploring the literature.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 11: Distributional Comparison of Citations. Galactica's citation distribution approaches the ground truth with scale. This is seen through a declining KS distance with scale, and increasing histogram overlap.</p>
<h1>Prompt</h1>
<p>in the BQ literature as, when $p$ is a mixture of Gaussians, the mean element $\mu_{p}$ is analytically tractable (see Appendix C). Some other $(p, k)$ pairs that produce analytic mean elements are discussed in [[START_REF] On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions, Bach [START_REF] ].
For this simulation study, we took $p(x)$ to be a 20-component mixture of 2D-Gaussian distributions. Monte Carlo (MC) is often used for such distributions but has a slow convergence rate in $\mathcal{O}_{P}\left(n^{-1 / 2}\right)$. FW and FWLS are known to converge more quickly and are in this sense preferable to MC [[START_REF]</p>
<h2>Prediction</h2>
<p>On the Equivalence between Herding and Conditional Gradient Algorithms, Bach
Figure 12: Citation Prompt. An example prompt predicting a citation in-context; from Briol et al. (2015).</p>
<h3>5.4.2 Citation Distributional Analysis</h3>
<p>We now turn to look at how well Galactica can model the empirical citation distribution. For this analysis we use the Contextual Citations dataset, where prompts are extracted from a paper by taking the context before a citation as the prompt. An example prompt with a model prediction is shown overleaf in Figure 12.
We use the in-context citation data to analyse the distributional difference between predicted and ground truth paper counts. This allows us to assess the model bias towards predicting more popular papers. Specifically, for each context there is a ground truth and predicted reference. We count the number of times each reference appears in our corpus. We then compare the distribution of reference counts between the ground truth references and the predicted references using the Kolmogorov-Smirnov distance (Massey, 1951).
The comparison between the citation count distributions for different model sizes can be seen in Figure 11. Figure 11a shows the decrease in the Kolmogorov-Smirnov distance between the distribution of ground truth paper citations and the distribution of predicted papers citations. Figure 11b shows how the distribution of paper counts for the predicted papers gets closer to the ground truth as the model size grows. At smaller scales the model is more prone to predicting more popular papers. As the model grows in size this bias towards predicting popular papers diminishes.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.5 General Capabilities</h1>
<p>We have studied Galactica's scientific capabilities. It is perhaps not surprising that a specialist scientific model outperforms general models on scientific tasks, but what would be more surprising was if it outperformed general models on general NLP tasks. In this section, we show surprising evidence that it does just that.
We evaluate on 57 BIG-bench tasks in Table 12 (Srivastava et al., 2022). The tasks are primarily non-scientific and test general language capability, for example anachronisms, figure of speech and metaphor boolean. We always evaluate with 5 -shots, and we use the default prompt style from BIG-Bench. Importantly, we do not include this prompt style in pre-training; so the evaluation between Galactica and the other models is comparable 5-shot. Full details and results are in the Appendix. We summarize average scores in Table 12:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Accuracy <br> weighted</th>
<th style="text-align: right;">Accuracy <br> unweighted</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">OPT 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$39.6 \%$</td>
<td style="text-align: right;">$38.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BLOOM 176B</td>
<td style="text-align: right;">176</td>
<td style="text-align: right;">$42.6 \%$</td>
<td style="text-align: right;">$42.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">OPT 175B</td>
<td style="text-align: right;">175</td>
<td style="text-align: right;">$43.4 \%$</td>
<td style="text-align: right;">$42.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$46.6 \%$</td>
<td style="text-align: right;">$42.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{4 8 . 7 \%}$</td>
<td style="text-align: right;">$\mathbf{4 5 . 3 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 12: BIG-bench 57 Task Results. Galactica outperforms general open models at smaller scales.</p>
<p>Both the 30B and 120B Galactica models outperform the larger OPT and BLOOM general models. This is a surprising result given we designed Galactica to trade-off generality for performance in scientific tasks.
We suspect this result reflects the higher-quality of the Galactica corpus, stemming from the fact it is curated and also primarily academic text. Previous open LLM efforts likely overfocused on scale goals and underfocused on data filtering. Another implication is that the focus on tokens $\rightarrow \infty$ from Chinchilla needs to be complemented with strong data quality procedures (Hoffmann et al., 2022). With this paper, we took an opposite approach by focusing on high-quality tokens and repeated epochs of training. However, the Chinchilla insight stands: and there is much more scientific text that we have not exploited in this work.</p>
<h3>5.6 Chemical Understanding</h3>
<p>We now turn to Galactica's capability to interface with different scientific modalities. We start by looking at Galactica's chemical capabilities. Chemical properties exhibit complex correlations which means the chemical space is very large. Better organization of chemical information through language models could aid chemical design and discovery. We explore how Galactica can provide a new interface for these tasks in this section.
For this work, we only include a small subset of available compounds from PubChem Compound in pretraining. Specifically, we take a random subset ( 2 million) of total compounds ( 110 million). This is to ensure the model is not overly biased towards learning natural sequences over natural language. This is a constraint we can relax in future work, enabling for much larger corpus. Here we focus on the first step of investigating whether a single model can learn effectively in the multi-modal setting.
We find that a language model can learn chemical tasks such as IUPAC naming in a self-supervised way, and in addition, we can pose drug discovery tasks as natural language prompts and achieve reasonable results.</p>
<h3>5.6.1 IUPAC Name Prediction</h3>
<p>SMILES is a line notation which represents chemical structure as a sequence of characters (Weininger, 1988). In the Galactica corpus, the SMILES formula occurs alongside information in the document, such as IUPAC names, molecular weight and XLogP. In the context of self-supervised learning, this means a language model is performing implicit multi-task learning: the model is predicting the next SMILES token, but can also use SMILES to predict other entities in the document.
As an initial test, we set up a IUPAC Name Prediction task, where the task is to name a compound according to the IUPAC nomenclature given a SMILES formula input. The IUPAC nomenclature is a method of naming organic compounds that has a ruleset based on naming the longest chain of carbons connected by single bonds (Favre and Powerll). There is a large set of rules and the procedure is algorithmically complex, meaning it is hard to automate. As a result, it is missing from standard cheminformatics toolkits.</p>
<p>Previous works such as STOUT and Struct2IUPAC have explored the possiblity of using RNNs and Transformers for this task (Rajan et al., 2021; Krasnov et al., 2021). We explore in this section whether Galactica can translate a SMILES specification to its IUPAC name in the self-supervised setting. We design a prompt based on the PubChem structure, with the SMILES as the only input, and the output to predict the IUPAC name.
To evaluate, we use our compound validation set of 17,052 compounds, and prompt with the SMILES formula and predict the IUPAC name. To calculate accuracy, we use OPSIN to convert the generated IUPAC name to SMILES, canonicalize it and compare with the canonicalized SMILES target (Lowe et al., 2011).
Results are shown in Table 13.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params (bn)</th>
<th style="text-align: right;">Accuracy</th>
<th style="text-align: right;">Invalid Names</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAL 125M</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">$0.0 \%$</td>
<td style="text-align: right;">$32.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 1.3B</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">$2.5 \%$</td>
<td style="text-align: right;">$12.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 6.7B</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$10.7 \%$</td>
<td style="text-align: right;">$12.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 30B</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">$15.4 \%$</td>
<td style="text-align: right;">$9.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAL 120B</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">$\mathbf{3 9 . 2 \%}$</td>
<td style="text-align: right;">$\mathbf{9 . 2 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 13: Results on IUPAC Naming. Performance improves smoothly with scale.</p>
<p>Accuracy increases smoothly with scale. Given we restricted the corpus to 2 million molecules, it is likely much better performance is achievable through training or fine-tuning on more molecules. The model is freely available for those who want to perform this follow-up work.
The more immediate question is what is actually being learnt: is Galactica inferring names from the fundamental molecular structure? To answer this, we visualize the average atomic attention at each stage of a prediction in Figure 13 overleaf. Encouragingly, the results are interpretable in terms of the underlying chemistry, and Galactica attends to the correct group when predicting a name, e.g. for "amino" it attends primarily to the $-\mathrm{NH}_{2}$ substituent.</p>
<p>Task: Convert the SMILES to IUPAC Name
Example: $\mathrm{CC}(\mathrm{C})(\mathrm{C}) \mathrm{C}(=0) \mathrm{N}(\mathrm{CC} 1=\mathrm{NC}(=\mathrm{CS} 1) \mathrm{C}(=0) \mathrm{OC}) \mathrm{C} 2 \mathrm{CCCCC} 2$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Atomic Attention</th>
<th style="text-align: center;">Predicted So Far</th>
<th style="text-align: center;">Token Predicted</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><smiles>C1CCCCC2CCCCC2</smiles></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">methyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl</td>
<td style="text-align: center;">cyclohexyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-</td>
<td style="text-align: center;">dimethyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethyl</td>
<td style="text-align: center;">prop</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylprop</td>
<td style="text-align: center;">anoyl</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)</td>
<td style="text-align: center;">amino</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino] methyl]</td>
<td style="text-align: center;">th</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino] methyl]th</td>
<td style="text-align: center;">iazole</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">methyl 2-[[cyclohexyl-(2,2-dimethylpropanoyl)]amino] methyl]thiazole-4-</td>
<td style="text-align: center;">carboxylate</td>
</tr>
</tbody>
</table>
<p>Figure 13: Attending to Functional Groups. Galactica uses its knowledge of chemistry to help with the IUPAC Naming task. At each stage of prediction, it attends to the part of the molecular graph associated with the group name, e.g. for "amino" it attends to the nitrogen atom; for thiazole, the sulphur atom.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://paperswithcode.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2771 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2771</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2771</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-9ca19acce35fd440cb9ffa504907f36a2e176bbc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ca19acce35fd440cb9ffa504907f36a2e176bbc" target="_blank">LOA: Logical Optimal Actions for Text-based Interaction Games</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games is presented.</p>
                <p><strong>Paper Abstract:</strong> We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2771.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2771.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Optimal Actions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic reinforcement learning agent architecture that maps parsed logical facts from text observations into a Logical Neural Network (LNN) to produce action recommendations; emphasizes interpretable logical rules and can encode avoidance of revisiting rooms via contradiction loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LOA agent (Logical Optimal Actions)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A neurosymbolic RL agent: a language-understanding component (semantic parser) converts raw textual observations into logical facts which are fed into a Logical Neural Network (LNN) that serves as the action-decision network; the LNN is trained using reward signals from the environment. Not described as built on any large pre-trained transformer LM.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin-Collector, Cooking, Commonsense Cleanup), Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based interactive game suites requiring navigation, object manipulation, puzzle solving and commonsense reasoning; TextWorld offers procedurally generated tasks (e.g., coin collection, cooking), Commonsense Cleanup requires commonsense knowledge, Jericho is a collection of interactive fiction games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>symbolic/logical state memory encoded in LNN (logical facts and learned logical rules)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Internal LNN structure of proposition nodes, logical-function nodes and action candidate nodes (i.e., logical formulae represented in neural units), rather than an external vector/episodic buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Parsed logical facts from observations (e.g., discovered exits, room connectivity, visited-room indicator), and learned logical rules/constraints used to prefer or forbid actions (e.g., avoid revisiting a previously visited room).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Inference via activation of relevant proposition nodes and logical-function nodes within the LNN; contradictions and truth-values are computed to select or suppress actions (e.g., contradiction loss prevents recommending revisits).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Per-timestep: semantic parser produces updated logical facts which are fed to the LNN; the LNN's rule weights are trainable and updated by RL reward signals during training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Used to avoid revisiting visited rooms, to recommend/score actions based on logical state, and to provide interpretable reasons for action choices (improving learning efficiency and explainability).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Authors report that encoding state as logical facts and learned rules in LNN enables the agent to avoid revisiting rooms (via contradiction loss) and claim faster convergence in RL compared to non-symbolic and other neuro-symbolic methods; no quantitative metrics are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper does not present quantitative evaluation of the memory mechanism; examples shown use a simple semantic parser and simple LNN rule sets, and the demo does not automatically execute RL actions (limiting demonstrated scalability); broader limitations are not analyzed in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOA: Logical Optimal Actions for Text-based Interaction Games', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2771.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2771.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic model where each neuron corresponds to a logical formula with weighted real-valued logic; supports training of logical constraints and computes probabilities and contradiction losses for propositions, enabling interpretable learned rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logical neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Logical Neural Networks (used as LOA's decision network)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A neurosymbolic architecture (not a language model) that encodes logical propositions and functions inside neural units; used by LOA to represent logical facts and rules for action selection in text games.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Used within LOA on TextWorld and other text-game demos</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same as LOA: text-based interactive games (navigation, object manipulation, puzzles, commonsense tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>symbolic/logical memory (propositional facts and logical constraints stored as neural/logical units)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Nodes representing propositions, logical-function nodes and action candidate nodes inside the LNN (interpretable logical graph within the network).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Truth values/probabilities of propositions extracted from observations (e.g., 'found north', 'visited room X'), learned logical rules and constraints that influence action activation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Logical inference via activation of nodes and evaluation of logical functions; contradiction loss is used to suppress inconsistent action choices.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Receives updated logical facts each timestep from the semantic parser; rule weights are trainable and updated using reward signals (RL training).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Represents and reasons over state facts to recommend actions, enforce constraints (e.g., don't revisit), and provide interpretable explanations for decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>LNN's symbolic representation enables expressing constraints (e.g., avoid revisits) that lead to more focused action recommendations and claimed faster RL convergence; specific empirical measurements are not provided in this demo paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper relies on a simple semantic parser and demonstrates small/simple LNN examples; scalability and quantitative limitations of LNN-based memory are not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOA: Logical Optimal Actions for Text-based Interaction Games', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2771.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2771.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Narasimhan et al. 2015</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language understanding for textbased games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an early deep RL approach to language understanding in text-based games and as evidence that long-term memory is an important skill for successful play.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for textbased games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Narasimhan et al. (2015) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-based games (general)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Early work on applying deep RL to language-based interactive games; cited here in context of long-term memory requirement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOA: Logical Optimal Actions for Text-based Interaction Games', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2771.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2771.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adhikari et al. 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced neuro/graph-based approach for text-based games (cited in related work); likely introduces dynamic belief-graph representations for state, mentioned here as relevant prior work on language-based game agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic belief graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Adhikari et al. (2020) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Work on dynamic graph-structured belief/state representations to generalize in text games; cited as related memory/state representation work.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOA: Logical Optimal Actions for Text-based Interaction Games', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2771.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2771.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chaudhury et al. 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work on observation pruning and generalization in text-based games; cited as an example of techniques beyond purely neural methods that affect state handling and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Chaudhury et al. (2020) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Paper addressing context-relevant observation pruning to improve generalization in text-game RL; cited in relation to observation and state handling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOA: Logical Optimal Actions for Text-based Interaction Games', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logical neural networks <em>(Rating: 2)</em></li>
                <li>Reinforcement learning with external knowledge by using logical neural networks <em>(Rating: 2)</em></li>
                <li>Language understanding for textbased games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning dynamic belief graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 1)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2771",
    "paper_id": "paper-9ca19acce35fd440cb9ffa504907f36a2e176bbc",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "LOA",
            "name_full": "Logical Optimal Actions",
            "brief_description": "A neuro-symbolic reinforcement learning agent architecture that maps parsed logical facts from text observations into a Logical Neural Network (LNN) to produce action recommendations; emphasizes interpretable logical rules and can encode avoidance of revisiting rooms via contradiction loss.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LOA agent (Logical Optimal Actions)",
            "agent_description": "A neurosymbolic RL agent: a language-understanding component (semantic parser) converts raw textual observations into logical facts which are fed into a Logical Neural Network (LNN) that serves as the action-decision network; the LNN is trained using reward signals from the environment. Not described as built on any large pre-trained transformer LM.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin-Collector, Cooking, Commonsense Cleanup), Jericho",
            "game_description": "Text-based interactive game suites requiring navigation, object manipulation, puzzle solving and commonsense reasoning; TextWorld offers procedurally generated tasks (e.g., coin collection, cooking), Commonsense Cleanup requires commonsense knowledge, Jericho is a collection of interactive fiction games.",
            "uses_memory": true,
            "memory_type": "symbolic/logical state memory encoded in LNN (logical facts and learned logical rules)",
            "memory_structure": "Internal LNN structure of proposition nodes, logical-function nodes and action candidate nodes (i.e., logical formulae represented in neural units), rather than an external vector/episodic buffer.",
            "memory_content": "Parsed logical facts from observations (e.g., discovered exits, room connectivity, visited-room indicator), and learned logical rules/constraints used to prefer or forbid actions (e.g., avoid revisiting a previously visited room).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Inference via activation of relevant proposition nodes and logical-function nodes within the LNN; contradictions and truth-values are computed to select or suppress actions (e.g., contradiction loss prevents recommending revisits).",
            "memory_update_strategy": "Per-timestep: semantic parser produces updated logical facts which are fed to the LNN; the LNN's rule weights are trainable and updated by RL reward signals during training.",
            "memory_usage_purpose": "Used to avoid revisiting visited rooms, to recommend/score actions based on logical state, and to provide interpretable reasons for action choices (improving learning efficiency and explainability).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Authors report that encoding state as logical facts and learned rules in LNN enables the agent to avoid revisiting rooms (via contradiction loss) and claim faster convergence in RL compared to non-symbolic and other neuro-symbolic methods; no quantitative metrics are provided in this paper.",
            "memory_limitations": "Paper does not present quantitative evaluation of the memory mechanism; examples shown use a simple semantic parser and simple LNN rule sets, and the demo does not automatically execute RL actions (limiting demonstrated scalability); broader limitations are not analyzed in detail.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2771.0",
            "source_info": {
                "paper_title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "LNN",
            "name_full": "Logical Neural Networks",
            "brief_description": "A neurosymbolic model where each neuron corresponds to a logical formula with weighted real-valued logic; supports training of logical constraints and computes probabilities and contradiction losses for propositions, enabling interpretable learned rules.",
            "citation_title": "Logical neural networks",
            "mention_or_use": "use",
            "agent_name": "Logical Neural Networks (used as LOA's decision network)",
            "agent_description": "A neurosymbolic architecture (not a language model) that encodes logical propositions and functions inside neural units; used by LOA to represent logical facts and rules for action selection in text games.",
            "base_model_size": null,
            "game_benchmark_name": "Used within LOA on TextWorld and other text-game demos",
            "game_description": "Same as LOA: text-based interactive games (navigation, object manipulation, puzzles, commonsense tasks).",
            "uses_memory": true,
            "memory_type": "symbolic/logical memory (propositional facts and logical constraints stored as neural/logical units)",
            "memory_structure": "Nodes representing propositions, logical-function nodes and action candidate nodes inside the LNN (interpretable logical graph within the network).",
            "memory_content": "Truth values/probabilities of propositions extracted from observations (e.g., 'found north', 'visited room X'), learned logical rules and constraints that influence action activation.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Logical inference via activation of nodes and evaluation of logical functions; contradiction loss is used to suppress inconsistent action choices.",
            "memory_update_strategy": "Receives updated logical facts each timestep from the semantic parser; rule weights are trainable and updated using reward signals (RL training).",
            "memory_usage_purpose": "Represents and reasons over state facts to recommend actions, enforce constraints (e.g., don't revisit), and provide interpretable explanations for decisions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "LNN's symbolic representation enables expressing constraints (e.g., avoid revisits) that lead to more focused action recommendations and claimed faster RL convergence; specific empirical measurements are not provided in this demo paper.",
            "memory_limitations": "Paper relies on a simple semantic parser and demonstrates small/simple LNN examples; scalability and quantitative limitations of LNN-based memory are not analyzed here.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2771.1",
            "source_info": {
                "paper_title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Narasimhan et al. 2015",
            "name_full": "Language understanding for textbased games using deep reinforcement learning",
            "brief_description": "Referenced as an early deep RL approach to language understanding in text-based games and as evidence that long-term memory is an important skill for successful play.",
            "citation_title": "Language understanding for textbased games using deep reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Narasimhan et al. (2015) agent",
            "agent_description": null,
            "base_model_size": null,
            "game_benchmark_name": "Text-based games (general)",
            "game_description": "Early work on applying deep RL to language-based interactive games; cited here in context of long-term memory requirement.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2771.2",
            "source_info": {
                "paper_title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Adhikari et al. 2020",
            "name_full": "Learning dynamic belief graphs to generalize on text-based games",
            "brief_description": "Referenced neuro/graph-based approach for text-based games (cited in related work); likely introduces dynamic belief-graph representations for state, mentioned here as relevant prior work on language-based game agents.",
            "citation_title": "Learning dynamic belief graphs to generalize on text-based games",
            "mention_or_use": "mention",
            "agent_name": "Adhikari et al. (2020) agent",
            "agent_description": null,
            "base_model_size": null,
            "game_benchmark_name": "Text-based games",
            "game_description": "Work on dynamic graph-structured belief/state representations to generalize in text games; cited as related memory/state representation work.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2771.3",
            "source_info": {
                "paper_title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Chaudhury et al. 2020",
            "name_full": "Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games",
            "brief_description": "Referenced work on observation pruning and generalization in text-based games; cited as an example of techniques beyond purely neural methods that affect state handling and generalization.",
            "citation_title": "Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games",
            "mention_or_use": "mention",
            "agent_name": "Chaudhury et al. (2020) agent",
            "agent_description": null,
            "base_model_size": null,
            "game_benchmark_name": "Text-based games",
            "game_description": "Paper addressing context-relevant observation pruning to improve generalization in text-game RL; cited in relation to observation and state handling.",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2771.4",
            "source_info": {
                "paper_title": "LOA: Logical Optimal Actions for Text-based Interaction Games",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logical neural networks",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning with external knowledge by using logical neural networks",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for textbased games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic belief graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 1
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 1
        }
    ],
    "cost": 0.012202,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LOA: Logical Optimal Actions for Text-based Interaction Games</h1>
<p>Daiki Kimura<em> Subhajit Chaudhury</em> Masaki Ono Michiaki Tatsubori<br>Don Joven Agravante Asim Munawar Akifumi Wachi Ryosuke Kohita Alexander Gray<br>IBM Research AI<br>{daiki, subhajit, moono, mich}@jp.ibm.com,<br>{don.joven.r.agravante, asim, akifumi.wachi, kohi, alexander.gray}@ibm.com</p>
<h4>Abstract</h4>
<p>We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neurosymbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa</p>
<h2>1 Introduction</h2>
<p>Neuro-symbolic (NS) hybrid approaches have been proposed for overcoming the weakness of deep reinforcement learning (Dong et al., 2019; Jiang and Luo, 2019; Kimura, 2018; Kimura et al., 2018), including less training data with generalization, external knowledge utilization, and direct explainability of what is learned. Study of reinforcement learning (RL) in non-symbolic environments, such as those with natural language and visionary observations, would be an important step towards the real-world application of the approaches beyond classic and symbolic environments.</p>
<p>Under certain controls necessary for studying RL, text-based games provide complex, interactive, and a variety of simulated environments where the environmental game state observation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An architecture overview for LOA.
is obtained through the text description, and the agent is expected to make progress by entering text commands. In addition to language understanding (Ammanabrolu and Riedl, 2019; Adhikari et al., 2020), successful play requires skills such as long-term memory (Narasimhan et al., 2015), exploration (Yuan et al., 2018), observation pruning (Chaudhury et al., 2020), and common sense reasoning (Keerthiram Murugesan and Campbell, 2021). However, these studies are not using the neuro-symbolic approach which is a combination of the neural network and the symbolic framework.</p>
<p>A recent neuro-symbolic framework called the Logical Neural Networks (LNN) (Riegel et al., 2020) simultaneously provides key properties of both neural networks (learning) and symbolic logic (reasoning). The LNN can train the constraints and rules with logical functions in the neural networks, and since every neuron in the network has a component for a formula of weighted realvalued logics, it can calculate the probability and contradiction loss for each of the propositions. At the same time, trained LNN follow symbolic rules, which means they yield a highly interpretable disentangled representation. Using this benefit of LNN, we proposed a neuro-symbolic RL method that uses pre-defined external knowledge in logical networks, and the method successfully plays on the text-based games (Kimura et al., 2021).</p>
<p>In this demonstration (demo site: https://ibm.biz/acl21-loa), we present a Logical Optimal Actions (LOA) architecture for neuro-symbolic RL applications with LNN (Riegel</p>
<p>et al., 2020) for text-based interaction games. While natural language-based interactive agents are the ambitious but attractive target as real-world applications of neuro-symbolic, it is not easy to provide an environment for the agent. The proposed demonstration uses text-based games learning environment, called TextWorld (Ct et al., 2018), as a miniature of a natural languagebased interactive environment. The demonstration provides a web-based user interface for visualizing the game interaction, which is including displaying the natural text observation from the environment, typing the action sentence, and showing the reward value from the taken action. The LOA in this demonstration also visualizes trained and pre-defined logical rules in LNN via the same interface, and this will help the human user understand the benefits of introducing the logical rules via neuro-symbolic frameworks. We also supply an open-sourced implementation for demo environment and some RL methods. This implementation contains our logical approaches and other state-of-the-art agents.</p>
<h2>2 Logical Optimal Action</h2>
<p>Our proposing LOA is an RL framework which is combining logical reasoning and neural network training. These training and reasoning are provided from functionalities of LNN (Riegel et al., 2020) that is simultaneously providing key properties of both neural networks and symbolic logic. Figure 1 shows the overview architecture for LOA. The LOA model receives the logical state value as logical fact from the language understanding component which receives raw natural language state value from the environment. The model forwards into LNN for the input to get the optimal action for it, the action goes into the environment to execute the action command, then reward is input to LOA agent. The LOA will be trained the action decision network in LNN by using the acquired reward value and chosen action from the network.</p>
<h2>3 LOA Demo</h2>
<p>The proposing web-based LOA demonstration supports two functionalities: 1) play the text-based game by human interactions, 2) visualize the trained and pre-defined LNN to increase interpretability for acquired rules.</p>
<p>For playing the games by web interface, Fig. 2 shows an initial view for the LOA demonstration.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Initial view for LOA demo.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: View for playing the game.</p>
<p>On the left-hand side, we can choose the game from some existing text-based interaction games ${ }^{1}$, such as TextWorld Coin-Collector game (Ct et al., 2018), TextWorld Cooking game (Ct et al., 2018), TextWorld Commonsense Cleanup game (Keerthiram Murugesan and Campbell, 2021), and Jericho game (Hausknecht et al., 2019). Figure 3 shows the view for playing the TextWorld game, and Fig. 4 shows the view for another game (cleanup task). The human player can input any action by natural language then the demonstration system displays the raw observation output from the environment.</p>
<p>For visualizing the trained and pre-defined neurosymbolic network in LNN, Fig. 5 and Fig. 6 show the example of the LNN output. In these</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: View for playing the cleanup game.
figures, the LNN contains simple rules for the TextWorld Coin-Collector game; for example, the rule is the agent takes 'go east' action, when the agent finds the east room ("found west" $\rightarrow$ "go west"). The round box explains the proposition from the given observation inputs, the circle with a logical function means a logical function node of LNN, and the rectangle box explains an action candidate for the agent. The highlighted nodes (red node) have 'true' value, and nonhighlighted nodes (white node) have 'false' value. In Fig. 5, the agent found the north exit from the given observation ("Observation ( $t=1$ )") by using semantic parser ${ }^{2}$, then the going north room action ("go north") are activated. In Fig. 6, if the user clicks the selectable box, the LOA recommends only one action which is 'go north'. In this demonstration, we show the benefit of introducing the LNN into an RL agent, we don't prepare to automatically choose the action by LOA framework. However, if we execute the RL with LOA framework, the RL agent can converge faster than other non-symbolic and neuro-symbolic methods.</p>
<p>After selecting "go north" action at $t=1$, next observation sentence and LNN output for next step are shown in Fig 7. In this step, the agent found two doors, which are east and south; however, the south door is connected to the previous room because the agent took going north action at the previous step. Since this LNN is simple LNN, the "go south" action is also recommended in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Displaying the simple LNN with given state.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: User can choose the recommended action.</p>
<p>Fig 7. Figure 8 shows the output of the complicated LNN which has functionality for avoiding revisiting the visited room. By using such the LNN, LOA can output only "go east" action by having contradiction loss in LNN. This is a benefit of introducing the neuro-symbolic framework, and the human user can easily understand the reason for the taken action by the agent with this interpretability by LOA.</p>
<h2>4 Conclusion</h2>
<p>We propose a novel demonstration (URL: https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visualize the benefit of the neuro-symbolic algorithm. This application helps the human user understand</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Result for simple LNN.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Result for avoiding revisiting LNN.
the trained network and the reason for taken action by the agent. We also extend more complicated LNN for other difficult games on the demo site. At the same time, we open the source code for the demonstration (URL: https://github.com/ibm/loa).</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Ct, Mikul Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, and Ryuki Tachibana. 2020. Bootstrapped q-learning with context relevant observation pruning to generalize in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3002-3008.</p>
<p>Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. In Computer Games - 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers, pages 41-75.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, Ct Marc-Alexandre, and Yuan Xingdi. 2019. Interactive fiction games: A colossal adventure. CoRR, abs/1909.05398.</p>
<p>Zhengyao Jiang and Shan Luo. 2019. Neural logic reinforcement learning. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pages 3110-3119.</p>
<p>Pavan Kapanipathi Pushkar Shukla Sadhana Kumaravel Gerald Tesauro Kartik Talamadupula Mrinmaya Sachan Keerthiram Murugesan, Mattia Atzeni and Murray Campbell. 2021. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence.</p>
<p>Daiki Kimura. 2018. Daqn: Deep auto-encoder and q-network. arXiv preprint arXiv:1806.00630.</p>
<p>Daiki Kimura, Subhajit Chaudhury, Ryuki Tachibana, and Sakyasingha Dasgupta. 2018. Internal model from observations for reward shaping.</p>
<p>Daiki Kimura, Subhajit Chaudhury, Akifumi Wachi, Ryosuke Kohita, Asim Munawar, Michiaki Tatsubori, and Alexander Gray. 2021. Reinforcement learning with external knowledge by using logical neural networks. arXiv preprint arXiv:2103.02363.</p>
<p>Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. 2015. Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages $1-11$.</p>
<p>Ryan Riegel, Alexander G. Gray, Francois P. S. Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, Shajith Ikbal, Hima Karanam, Sumit Neelam, Ankita Likhyani, and Santosh K. Srivastava. 2020. Logical neural networks. CoRR, abs/2006.13155.</p>
<p>Xingdi Yuan, Marc-Alexandre Ct, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew J. Hausknecht, and Adam Trischler. 2018. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ This parser is out of our current research topic, we prepare a simple semantic parser.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
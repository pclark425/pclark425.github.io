<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2677 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2677</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2677</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-d766bffc357127e0dc86dd69561d5aeb520d6f4c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c" target="_blank">Training language models to follow instructions with human feedback</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.</p>
                <p><strong>Paper Abstract:</strong> Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2677.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2677.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGPT (GPT-3 fine-tuned with human feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of GPT-3-based language models fine-tuned on human-written demonstrations and human preference comparisons using supervised fine-tuning and reinforcement learning from human feedback (RLHF) to better follow instructions, be more truthful, and reduce harmful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InstructGPT (PPO-ptx variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Autoregressive transformer language models (GPT-3 architecture) pretrained on broad internet corpora and then fine-tuned in three stages: (1) supervised fine-tuning (SFT) on labeler demonstrations, (2) training a reward model (RM) on human pairwise preference comparisons, and (3) policy optimization using Proximal Policy Optimization (PPO) against the RM reward; PPO-ptx variants mix gradients from the pretraining distribution into the PPO objective to reduce performance regressions. Models were trained at multiple sizes (1.3B, 6B, 175B parameters). Key components: pretrained GPT-3 backbone, SFT checkpoint, reward model (6B RM used), KL penalty to SFT policy during RL, and optional pretraining-mix gradient (gamma) in PPO-ptx.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (autoregressive transformer) with RLHF reward model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General natural language processing; applied to tasks that include summarization, QA, code, and open-ended generation (not specific to a single scientific domain)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not explicitly designed for scientific hypothesis generation; can produce candidate scientific hypotheses when prompted because it is an instruction-following autoregressive LLM fine-tuned on demonstrations and preferences. Generation mechanism is standard next-token sampling from the fine-tuned autoregressive policy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility of outputs is assessed indirectly via human labeler preferences and a learned reward model that predicts which completions labelers prefer (reward model trained on pairwise rankings). Additional plausibility signals come from human metadata labels (e.g., 'hallucination' binary) and benchmark evaluations such as TruthfulQA.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Quality is quantified by human preference win rates (e.g., 175B InstructGPT preferred to 175B GPT-3 85 ± 3%), Likert overall quality (1–7), RM validation accuracy/log loss, and task-specific metrics such as truthfulness/informativeness on TruthfulQA and hallucination frequency on closed-domain tasks (InstructGPT hallucination rate ≈21% vs GPT-3 ≈41% on closed-domain prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human evaluation on held-out prompts (pairwise comparisons and Likert ratings), RM held-out accuracy (predicting labeler preferences), benchmark datasets (TruthfulQA, RealToxicityPrompts, Winogender, CrowS-Pairs, SQuAD, DROP, HellaSwag, WMT), and automatic tools like the Perspective API for toxicity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Dataset splits by user ID (train/val/test), description of SFT/RM/PPO training procedures (epochs, normalization of RM outputs, KL penalty), release of model samples on GitHub; full training code not included in paper. Detailed hyperparameter descriptions and dataset sizes are provided in appendices to facilitate reproduction but no single turnkey reproduction artifact is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Training labelers to prefer truthful outputs (labeler instructions), using RM and RLHF to reward responses preferred by labelers (which penalize hallucination), mixing pretraining gradients into PPO (PPO-ptx) and per-token KL penalty to the SFT policy to avoid overoptimization-induced hallucination; instructing labelers to prioritize truthfulness in final evaluations and to reward epistemic humility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Binary 'Hallucination' metadata labeled by humans on closed-domain tasks, measurements on closed-domain task set (counting fabricated information), TruthfulQA benchmark evaluations, and reward-model / human preference predictions used as a proxy for hallucination likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>Reported closed-domain hallucination rates: InstructGPT ≈ 21% vs GPT-3 ≈ 41% (fraction of outputs that make up information not in the input on closed-domain tasks); other experiments report qualitative reductions but not exhaustive numeric rates beyond these values.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>95% confidence intervals reported for human-preference winrates (e.g., 85 ± 3%), inter-annotator agreement reported with confidence intervals (72.6 ± 1.5% for training labelers), but no specific p-values or formal hypothesis tests are presented in the paper beyond CI reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>No explicit Bayesian uncertainty quantification; uncertainty is represented implicitly via human-labeled epistemic behavior (labelers rewarded hedging/epistemic humility), scalar RM scores, Likert rating distributions, and confidence intervals computed for aggregated human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TruthfulQA, RealToxicityPrompts, Winogender, CrowS-Pairs, SQuAD, DROP, HellaSwag, WMT 2015 (French→English), and the paper's internal API prompt dataset (SFT, RM, PPO datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key reported results: preference winrates against 175B SFT baseline (e.g., 175B InstructGPT preferred to GPT-3 outputs 85 ± 3% of the time and preferred 71 ± 4% to few-shot GPT-3); TruthfulQA: InstructGPT generates truthful/informative answers roughly twice as often as GPT-3 (aggregate human-eval improvement, exact percentage not always provided in table form); toxicity: InstructGPT generates ~25% fewer toxic outputs than GPT-3 when given a respectful instruction; RM predictive accuracy: ~72.4% on training labelers and ~69.6 ± 0.9% on held-out labelers for cross-validation RMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>InstructGPT (PPO and PPO-ptx) substantially outperforms GPT-3 baselines on human preference metrics and reduces hallucinations and toxicity in the 'respectful prompt' condition; compared to SFT and public task-finetuned baselines (FLAN, T0), InstructGPT is preferred by labelers on the API prompt distribution (e.g., InstructGPT preferred over FLAN/T0 ≈78–79 ± 4%).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not designed or validated as a scientific-hypothesis engine; no formal novelty assessment or experimental verification pipeline for generated scientific hypotheses; models still hallucinate and can follow harmful or false premises when instructed; reward model is only as good as labeler preferences and can overfit; alignment- induced 'tax' causing performance regressions on some public NLP datasets unless mitigated; uncertainty quantification is implicit and limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2677.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2677.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses human preference feedback to define a reward function (via a learned reward model) and then optimizes a policy using reinforcement learning (here PPO) to maximize that reward, aligning model outputs to human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Reinforcement Learning from Human Preferences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reinforcement Learning from Human Feedback (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline: collect human comparisons between model outputs; train a reward model r_theta(x,y) to predict human preferences using a pairwise logistic/cross-entropy loss; use RL (PPO) to optimize a policy to maximize the learned reward while regularizing via a KL penalty to the SFT policy; optionally mix pretraining gradients (PPO-ptx) into the RL objective to preserve pretraining performance.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Policy optimization with learned human reward model; LLM-based policy optimized with RL</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP alignment methodology; applicable wherever human preference for outputs can be defined (including evaluation of truthfulness and plausibility of prose, potential to be applied in scientific text/hypothesis generation contexts though not done here).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not specific to hypothesis generation; RLHF can be used to shape an LLM's generation behavior (including producing hypotheses) by optimizing for human-preferred outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is encoded via human preferences: the reward model is trained to predict what human annotators prefer, which can include plausibility/truthfulness judgement when labelers are instructed accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>RM score (log-odds of preferring one completion over another), human preference winrate against baselines, Likert quality ratings, and task-specific benchmarks when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>RM validation accuracy on held-out comparisons, human preference tests on held-out labelers, benchmark evaluations, and ablations (e.g., PPO vs PPO-ptx).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Detailed description of RM loss, batching strategy for K-response rankings (train on all K choose 2 comparisons as a single batch element), normalization of RM outputs to zero-mean on demonstrations, and hyperparameters described in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Human preference signal can penalize hallucination when labelers prefer truthful outputs; KL regularization to SFT mitigates runaway reward optimization that could encourage hallucinatory but highly rewarded outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Indirect: RM predictions and human-labeled hallucination metadata. The framework relies on human labels to surface hallucinations rather than an automated hallucination detector.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Aggregate CI reporting for preference experiments; RM accuracies reported with mean and std across cross-validation seeds for held-out labelers.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>No explicit probabilistic uncertainty model beyond RM predictive scores and reporting of confidence intervals for aggregated human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>The paper's internal API datasets for SFT/RM/PPO training, and public benchmarks (TruthfulQA, RealToxicityPrompts, etc.) used for evaluating RLHF outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using RLHF (PPO) improved human-preference winrates versus SFT and GPT-3 baselines; mixing pretraining gradients (PPO-ptx) reduced alignment tax while retaining preference improvements. Specific numbers reported in InstructGPT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PPO and PPO-ptx outperform SFT and GPT-3 baselines on human preference and many quality metrics; PPO-ptx reduces regressions on public NLP datasets compared to plain PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reliance on quality and representativeness of human labelers (small labeler pool), potential overfitting of RM if comparisons are not batched correctly, alignment target is the labeler population which may not generalize to all users, no formal mechanisms for novelty detection of generated scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2677.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2677.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward Model (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward Model for predicting human preference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar-valued model (here a 6B-parameter transformer) trained to predict which of two or more model completions a human labeler prefers, used as a differentiable surrogate reward function for RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to summarize with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reward Model (pairwise preference RM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture: transformer-based model (6B parameters in this work) that inputs a prompt and a candidate completion and outputs a scalar reward r_theta(x,y). Trained on labeled pairwise comparisons (K responses per prompt yielding K choose 2 comparisons) using a pairwise logistic/cross-entropy loss: loss = -E[ log sigma( r(x,y_w) - r(x,y_l) ) ]. To avoid overfitting when K>2, the paper trains on all comparisons from a prompt as a single batch element (single forward pass per completion). The RM is normalized (additive bias) so that labeler demonstrations have mean reward 0 before RL.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based scalar reward predictor (preference-model)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP alignment and evaluation; RM is a general technique for converting human preferences into a scalar reward that can be applied to any text-generation task (including scientific text/hypotheses if labeled accordingly).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>RM encodes human preferences which can include plausibility; thus plausibility is assessed insofar as human labelers prefer plausible responses and label such preferences in comparison data used to train the RM.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>RM validation accuracy (e.g., ~72.4% on training labelers, ~69.6 ± 0.9% on held-out labelers in 5-fold cross-validation), cross-entropy/log-loss on comparisons, and downstream winrates when used as RL reward.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Held-out comparison accuracy, cross-validation across labeler groups, and evaluation of whether policies optimized against RM are preferred by human labelers on held-out prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Explicit RM loss function, batching strategy for multi-comparison tasks, RM normalization protocol (bias so demonstrations mean score 0), and use of a 6B parameter RM for stability considerations are documented to help reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Indirect: RM can be trained to assign low reward to hallucinated outputs if labelers mark hallucination as undesirable in comparisons, thereby steering the policy away from hallucination during RL.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human-labeled hallucination metadata is used as part of human preference signals; RM is trained on comparison data that implicitly encodes preferences about hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>RM accuracies are reported with confidence/standard error across cross-validation seeds; no formal hypothesis tests beyond reporting CI for main aggregated human metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>RM outputs a scalar score but no calibrated probabilistic uncertainty estimate is reported; RM predictive accuracy on held-out comparisons is used as an empirical uncertainty proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Trained on the paper's RM dataset (≈33k training prompts with comparisons) assembled from labeler rankings of K=4..9 responses per prompt; evaluated via held-out human comparisons and downstream preference tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>RM accuracy: ~72.4% on predicting training labelers' preferences; cross-validated RM accuracy on held-out labeler groups: ~69.6 ± 0.9%. These accuracies indicate RM's effectiveness as a surrogate reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>RM is only as good as the human comparisons it is trained on (labeler population biases); overfitting is a risk if pairwise comparisons are naively shuffled (mitigated here by batching strategy); RM invariance to additive shifts necessitates normalization; reward hacking/overoptimization remains a concern which is mitigated with KL penalties but not fully eliminated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2677.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2677.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPO-ptx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proximal Policy Optimization with pretraining gradient mixing (PPO-ptx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modification of PPO-based RLHF that mixes in pretraining (language modeling) gradients to the RL objective during PPO updates to reduce performance regressions on downstream tasks while preserving alignment gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PPO-ptx (PPO with pretraining mix)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Policy optimization uses standard PPO with reward given by the RM and a KL penalty to the SFT policy; PPO-ptx adds an auxiliary term that maximizes log-likelihood on samples from the pretraining distribution (coefficient gamma) so that updates preserve pretraining capabilities. The combined objective: E[r_theta - beta * KL] + gamma * E_pretrain[ log pi_RL(x) ]. Hyperparameters beta (KL coefficient) and gamma (pretraining mix weight) control the trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>RL-optimized LLM policy with mixed supervised (pretraining) gradients</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP alignment; technique intended to preserve pretraining abilities (language modeling) while optimizing for human preferences across diverse tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Same as RLHF: plausibility encoded indirectly via human preference reward; PPO-ptx preserves pretraining distribution likelihood which can help maintain factual knowledge and plausibility learned during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Human preference winrates, public NLP benchmark performance (SQuAD, DROP, HellaSwag, WMT), and RM validation reward. PPO-ptx shows reduced 'alignment tax' on these benchmarks compared to vanilla PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Ablation comparisons showing PPO-ptx reduces performance regressions on public datasets and retains high labeler preference scores; experiments varied gamma and KL coefficients to find a trade-off point.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Objective function explicitly given, hyperparameter roles described (beta, gamma), and empirical ablations shown to guide reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Mixing pretraining gradients helps reduce loss of factual pretraining knowledge that could otherwise increase hallucinations; combined with human preference reward that penalizes hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Performance and winrate metrics reported with confidence intervals; ablation studies present curves showing trade-offs but no formal p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>SQuAD, DROP, HellaSwag, WMT 2015 (French→English), and the internal API prompt datasets used to evaluate alignment tax and mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PPO-ptx substantially reduces performance regressions on evaluated public NLP benchmarks relative to vanilla PPO while maintaining labeler preference; precise numeric improvements vary by dataset and are shown in paper figures (e.g., reversing regressions on SQuADv2 and DROP for certain gamma values).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PPO-ptx vs PPO: PPO-ptx yields better performance on public benchmarks (less alignment tax) while preserving RM validation reward and human preference improvements; PPO alone produced larger regressions on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mixing pretraining gradients may reintroduce undesirable behaviors present in pretraining data; it mitigates but does not fully eliminate alignment tax; choice of gamma is a trade-off and may vary by downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2677.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2677.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human pairwise comparison + K-ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-response ranking method for collecting human comparisons (multi-way ranking batched as pairwise comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data collection protocol where human labelers rank K (4–9) model completions per prompt, producing K choose 2 pairwise comparisons; training treats all comparisons from a prompt as a single batch element to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>K-ranking pairwise human comparison protocol</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Labelers are shown between K=4 and K=9 completions per prompt and asked to rank them; this yields multiple pairwise preference comparisons per prompt; during RM training, the K choose 2 comparisons from the same prompt are processed as a single batch element (single forward pass per completion) to avoid correlated-overfitting and to obtain computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Human-in-the-loop evaluation protocol for preference data collection</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP evaluation and alignment data collection; applicable wherever human quality judgments are used to train surrogate rewards (including evaluating scientific claims if labelers are experts).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is encoded in labelers' ranking criteria; instructions to labelers can emphasize truthfulness, harmlessness, and helpfulness to ensure plausibility considerations are reflected in preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Human preference labels (pairwise), aggregated winrates, Likert ratings, and RM loss/accuracy derived from these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Inter-annotator agreement metrics reported (training labeler agreement ~72.6 ±1.5%; held-out labelers ~77.3 ±1.3%); cross-validation of RMs across labeler groups to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Explicit description of K range, batching strategy for comparisons, and inter-annotator agreement statistics to aid reproduction of the data collection procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Labeler instructions can be set to penalize hallucination; the ranking protocol captures those preferences and the RM learns to reflect them in reward.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human-labeled 'hallucination' binary metadata collected as part of evaluations; rankings capture relative hallucination severity between completions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Inter-annotator agreement and RM accuracies with confidence intervals reported; aggregated winrates with 95% CIs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on the paper's RM dataset (≈33k prompts) constructed from API prompts and labeler-written prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enabled training of RMs with improved validation accuracy and reduced overfitting compared to naively treating each pairwise comparison independently; RM validation accuracy reported in InstructGPT and RM entries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Human comparisons reflect the instruction set and demographics of the labelers; single annotator comparisons for many examples (cost-driven) limit redundancy; rankings are expensive to collect and depend on careful labeler instructions to capture desired notions (e.g., truthfulness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2677.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2677.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed-domain hallucination metric / 'Hallucination' metadata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-domain hallucination labeling and binary metadata</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-labeled binary metadata field indicating whether an output 'hallucinates' (i.e., invents information not present in the input) on closed-domain tasks; used to quantify fabrication rates and compare models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human-labeled closed-domain hallucination metric</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Labelers annotate outputs on tasks where the correct output should be grounded in the input (e.g., summarization or closed-domain QA) with a binary flag 'Hallucination' if the model fabricates information not present in the input. Aggregated counts/fractions of this flag are reported per model to compare hallucination propensity.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Human-evaluation-based hallucination detector (annotation protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation (truthfulness measurement); applicable to any domain with closed-domain inputs where fabrication can be defined (including scientific claim validation when ground-truth sources exist).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Direct measurement of plausibility/groundedness by checking whether outputs contain unsupported factual claims relative to the provided input.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Hallucination rate = fraction of outputs labeled as containing fabricated information on closed-domain tasks (reported: InstructGPT ≈21% vs GPT-3 ≈41%).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human annotation on held-out prompts from the API distribution for closed-domain tasks; aggregated rates compared between models.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Definition of 'closed-domain' and the binary label is provided; dataset splits and labeling instructions are described in appendices to facilitate replication.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Used as a training/evaluation signal (labeler preferences penalize hallucination) and as an evaluation metric to track reductions in fabrication after RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Human binary annotation for hallucination; RM trained on human preference data may learn to assign lower reward to hallucinated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>Paper reports closed-domain hallucination rates of roughly 21% for InstructGPT versus 41% for GPT-3 on relevant closed-domain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Differences reported with confidence intervals for related metrics; paper does not present a formal hypothesis test specific to hallucination rate differences but reports aggregate metrics and CIs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Internal closed-domain subset of the API prompt distribution used for hallucination labeling; TruthfulQA also used as a related benchmark for truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reduction of hallucination frequency from 41% (GPT-3) to ~21% (InstructGPT) on closed-domain tasks; other qualitative and quantitative signals corroborate improved groundedness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>InstructGPT shows approximately half the hallucination rate of GPT-3 on closed-domain tasks according to human-labeled metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Binary labeling is coarse and dependent on labeler judgment; covers only closed-domain tasks where ground truth is defined; does not quantify partial hallucination severity or metadata inter-annotator variance in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2677.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2677.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA (benchmark for measuring model truthfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark dataset designed to measure a language model's tendency to produce common misconceptions and falsehoods; used here to evaluate and compare the truthfulness of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TruthfulQA: Measuring How Models Mimic Human Falsehoods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TruthfulQA benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A curated dataset of questions (including adversarially selected ones) intended to elicit model falsehoods; evaluation is via human ratings of truthfulness and informativeness of model responses, and can include prompting strategies that encourage abstention or hedging.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Benchmark dataset for truthfulness evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of language models' truthfulness across general knowledge—relevant to assessing factual correctness of generated scientific claims when appropriately adapted.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Measures plausibility/truthfulness via human evaluation of model answers to questions where common falsehoods are known; models can be prompted to abstain to avoid false claims.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Human-evaluated truthfulness and truthfulness+informativeness ratings; used to compute relative improvements between models (paper reports InstructGPT roughly twice as often truthful/informative as GPT-3 on this benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Human ratings on the TruthfulQA dataset; ablation with 'Instruction+QA' prompt that encourages abstention ('I have no comment') to see if models avoid falsehoods.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Benchmark is public and used with documented prompting and human evaluation protocols; paper references the dataset and evaluation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Encouraging abstention prompts (e.g., 'I have no comment') and training with human preferences that reward honesty can reduce confident falsehoods.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>TruthfulQA highlights model falsehoods via human annotation of responses as truthful or not; serves as a detection/evaluation benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Aggregate improvements presented with confidence intervals for human-rated truthfulness/informativeness; no formal p-values but CIs and sample sizes are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Evaluation includes measuring whether models abstain or hedge; no formal probabilistic uncertainty estimates provided by benchmark itself.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>TruthfulQA (Lin et al., 2021), used alongside closed-domain hallucination measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>InstructGPT PPO models are reported to produce truthful and informative answers about twice as often as GPT-3 on TruthfulQA; exact percentage-point improvements vary depending on adversarial subset vs full set (paper gives aggregated bar-plot results and CI statements).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>InstructGPT outperforms GPT-3 on TruthfulQA according to human evaluations; prompting with an abstention instruction combined with PPO leads models to prefer being truthful/uninformative rather than confidently asserting falsehoods.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>TruthfulQA measures only certain aspects of truthfulness (mostly common misconceptions) and is not a comprehensive measure of scientific claim validity; improvements on TruthfulQA do not guarantee correctness in domain-specific scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training language models to follow instructions with human feedback', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep Reinforcement Learning from Human Preferences <em>(Rating: 2)</em></li>
                <li>Learning to summarize with human feedback <em>(Rating: 2)</em></li>
                <li>Learning to summarize with human feedback (Ziegler et al., 2019) <em>(Rating: 1)</em></li>
                <li>WebGPT: Improving factual accuracy of answers by browsing the web <em>(Rating: 2)</em></li>
                <li>RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models <em>(Rating: 2)</em></li>
                <li>TruthfulQA: Measuring How Models Mimic Human Falsehoods <em>(Rating: 2)</em></li>
                <li>Proximal Policy Optimization Algorithms <em>(Rating: 2)</em></li>
                <li>Adversarial open-endedness in dialogue (Dinan et al., adversarial collection approaches) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2677",
    "paper_id": "paper-d766bffc357127e0dc86dd69561d5aeb520d6f4c",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "InstructGPT",
            "name_full": "InstructGPT (GPT-3 fine-tuned with human feedback)",
            "brief_description": "A family of GPT-3-based language models fine-tuned on human-written demonstrations and human preference comparisons using supervised fine-tuning and reinforcement learning from human feedback (RLHF) to better follow instructions, be more truthful, and reduce harmful outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InstructGPT (PPO-ptx variants)",
            "system_description": "Autoregressive transformer language models (GPT-3 architecture) pretrained on broad internet corpora and then fine-tuned in three stages: (1) supervised fine-tuning (SFT) on labeler demonstrations, (2) training a reward model (RM) on human pairwise preference comparisons, and (3) policy optimization using Proximal Policy Optimization (PPO) against the RM reward; PPO-ptx variants mix gradients from the pretraining distribution into the PPO objective to reduce performance regressions. Models were trained at multiple sizes (1.3B, 6B, 175B parameters). Key components: pretrained GPT-3 backbone, SFT checkpoint, reward model (6B RM used), KL penalty to SFT policy during RL, and optional pretraining-mix gradient (gamma) in PPO-ptx.",
            "system_type": "LLM-based (autoregressive transformer) with RLHF reward model",
            "scientific_domain": "General natural language processing; applied to tasks that include summarization, QA, code, and open-ended generation (not specific to a single scientific domain)",
            "hypothesis_generation_method": "Not explicitly designed for scientific hypothesis generation; can produce candidate scientific hypotheses when prompted because it is an instruction-following autoregressive LLM fine-tuned on demonstrations and preferences. Generation mechanism is standard next-token sampling from the fine-tuned autoregressive policy.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility of outputs is assessed indirectly via human labeler preferences and a learned reward model that predicts which completions labelers prefer (reward model trained on pairwise rankings). Additional plausibility signals come from human metadata labels (e.g., 'hallucination' binary) and benchmark evaluations such as TruthfulQA.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Quality is quantified by human preference win rates (e.g., 175B InstructGPT preferred to 175B GPT-3 85 ± 3%), Likert overall quality (1–7), RM validation accuracy/log loss, and task-specific metrics such as truthfulness/informativeness on TruthfulQA and hallucination frequency on closed-domain tasks (InstructGPT hallucination rate ≈21% vs GPT-3 ≈41% on closed-domain prompts).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human evaluation on held-out prompts (pairwise comparisons and Likert ratings), RM held-out accuracy (predicting labeler preferences), benchmark datasets (TruthfulQA, RealToxicityPrompts, Winogender, CrowS-Pairs, SQuAD, DROP, HellaSwag, WMT), and automatic tools like the Perspective API for toxicity scores.",
            "reproducibility_measures": "Dataset splits by user ID (train/val/test), description of SFT/RM/PPO training procedures (epochs, normalization of RM outputs, KL penalty), release of model samples on GitHub; full training code not included in paper. Detailed hyperparameter descriptions and dataset sizes are provided in appendices to facilitate reproduction but no single turnkey reproduction artifact is provided.",
            "hallucination_prevention_method": "Training labelers to prefer truthful outputs (labeler instructions), using RM and RLHF to reward responses preferred by labelers (which penalize hallucination), mixing pretraining gradients into PPO (PPO-ptx) and per-token KL penalty to the SFT policy to avoid overoptimization-induced hallucination; instructing labelers to prioritize truthfulness in final evaluations and to reward epistemic humility.",
            "hallucination_detection_method": "Binary 'Hallucination' metadata labeled by humans on closed-domain tasks, measurements on closed-domain task set (counting fabricated information), TruthfulQA benchmark evaluations, and reward-model / human preference predictions used as a proxy for hallucination likelihood.",
            "hallucination_rate": "Reported closed-domain hallucination rates: InstructGPT ≈ 21% vs GPT-3 ≈ 41% (fraction of outputs that make up information not in the input on closed-domain tasks); other experiments report qualitative reductions but not exhaustive numeric rates beyond these values.",
            "statistical_significance_testing": "95% confidence intervals reported for human-preference winrates (e.g., 85 ± 3%), inter-annotator agreement reported with confidence intervals (72.6 ± 1.5% for training labelers), but no specific p-values or formal hypothesis tests are presented in the paper beyond CI reporting.",
            "uncertainty_quantification_method": "No explicit Bayesian uncertainty quantification; uncertainty is represented implicitly via human-labeled epistemic behavior (labelers rewarded hedging/epistemic humility), scalar RM scores, Likert rating distributions, and confidence intervals computed for aggregated human metrics.",
            "benchmark_dataset": "TruthfulQA, RealToxicityPrompts, Winogender, CrowS-Pairs, SQuAD, DROP, HellaSwag, WMT 2015 (French→English), and the paper's internal API prompt dataset (SFT, RM, PPO datasets).",
            "performance_metrics": "Key reported results: preference winrates against 175B SFT baseline (e.g., 175B InstructGPT preferred to GPT-3 outputs 85 ± 3% of the time and preferred 71 ± 4% to few-shot GPT-3); TruthfulQA: InstructGPT generates truthful/informative answers roughly twice as often as GPT-3 (aggregate human-eval improvement, exact percentage not always provided in table form); toxicity: InstructGPT generates ~25% fewer toxic outputs than GPT-3 when given a respectful instruction; RM predictive accuracy: ~72.4% on training labelers and ~69.6 ± 0.9% on held-out labelers for cross-validation RMs.",
            "comparison_with_baseline": "InstructGPT (PPO and PPO-ptx) substantially outperforms GPT-3 baselines on human preference metrics and reduces hallucinations and toxicity in the 'respectful prompt' condition; compared to SFT and public task-finetuned baselines (FLAN, T0), InstructGPT is preferred by labelers on the API prompt distribution (e.g., InstructGPT preferred over FLAN/T0 ≈78–79 ± 4%).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Not designed or validated as a scientific-hypothesis engine; no formal novelty assessment or experimental verification pipeline for generated scientific hypotheses; models still hallucinate and can follow harmful or false premises when instructed; reward model is only as good as labeler preferences and can overfit; alignment- induced 'tax' causing performance regressions on some public NLP datasets unless mitigated; uncertainty quantification is implicit and limited.",
            "uuid": "e2677.0",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "RLHF",
            "name_full": "Reinforcement Learning from Human Feedback",
            "brief_description": "An approach that uses human preference feedback to define a reward function (via a learned reward model) and then optimizes a policy using reinforcement learning (here PPO) to maximize that reward, aligning model outputs to human preferences.",
            "citation_title": "Deep Reinforcement Learning from Human Preferences",
            "mention_or_use": "use",
            "system_name": "Reinforcement Learning from Human Feedback (RLHF)",
            "system_description": "Pipeline: collect human comparisons between model outputs; train a reward model r_theta(x,y) to predict human preferences using a pairwise logistic/cross-entropy loss; use RL (PPO) to optimize a policy to maximize the learned reward while regularizing via a KL penalty to the SFT policy; optionally mix pretraining gradients (PPO-ptx) into the RL objective to preserve pretraining performance.",
            "system_type": "Policy optimization with learned human reward model; LLM-based policy optimized with RL",
            "scientific_domain": "General NLP alignment methodology; applicable wherever human preference for outputs can be defined (including evaluation of truthfulness and plausibility of prose, potential to be applied in scientific text/hypothesis generation contexts though not done here).",
            "hypothesis_generation_method": "Not specific to hypothesis generation; RLHF can be used to shape an LLM's generation behavior (including producing hypotheses) by optimizing for human-preferred outputs.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is encoded via human preferences: the reward model is trained to predict what human annotators prefer, which can include plausibility/truthfulness judgement when labelers are instructed accordingly.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "RM score (log-odds of preferring one completion over another), human preference winrate against baselines, Likert quality ratings, and task-specific benchmarks when applicable.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "RM validation accuracy on held-out comparisons, human preference tests on held-out labelers, benchmark evaluations, and ablations (e.g., PPO vs PPO-ptx).",
            "reproducibility_measures": "Detailed description of RM loss, batching strategy for K-response rankings (train on all K choose 2 comparisons as a single batch element), normalization of RM outputs to zero-mean on demonstrations, and hyperparameters described in appendices.",
            "hallucination_prevention_method": "Human preference signal can penalize hallucination when labelers prefer truthful outputs; KL regularization to SFT mitigates runaway reward optimization that could encourage hallucinatory but highly rewarded outputs.",
            "hallucination_detection_method": "Indirect: RM predictions and human-labeled hallucination metadata. The framework relies on human labels to surface hallucinations rather than an automated hallucination detector.",
            "hallucination_rate": null,
            "statistical_significance_testing": "Aggregate CI reporting for preference experiments; RM accuracies reported with mean and std across cross-validation seeds for held-out labelers.",
            "uncertainty_quantification_method": "No explicit probabilistic uncertainty model beyond RM predictive scores and reporting of confidence intervals for aggregated human metrics.",
            "benchmark_dataset": "The paper's internal API datasets for SFT/RM/PPO training, and public benchmarks (TruthfulQA, RealToxicityPrompts, etc.) used for evaluating RLHF outcomes.",
            "performance_metrics": "Using RLHF (PPO) improved human-preference winrates versus SFT and GPT-3 baselines; mixing pretraining gradients (PPO-ptx) reduced alignment tax while retaining preference improvements. Specific numbers reported in InstructGPT entry.",
            "comparison_with_baseline": "PPO and PPO-ptx outperform SFT and GPT-3 baselines on human preference and many quality metrics; PPO-ptx reduces regressions on public NLP datasets compared to plain PPO.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Reliance on quality and representativeness of human labelers (small labeler pool), potential overfitting of RM if comparisons are not batched correctly, alignment target is the labeler population which may not generalize to all users, no formal mechanisms for novelty detection of generated scientific claims.",
            "uuid": "e2677.1",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Reward Model (RM)",
            "name_full": "Reward Model for predicting human preference",
            "brief_description": "A scalar-valued model (here a 6B-parameter transformer) trained to predict which of two or more model completions a human labeler prefers, used as a differentiable surrogate reward function for RLHF.",
            "citation_title": "Learning to summarize with human feedback",
            "mention_or_use": "use",
            "system_name": "Reward Model (pairwise preference RM)",
            "system_description": "Architecture: transformer-based model (6B parameters in this work) that inputs a prompt and a candidate completion and outputs a scalar reward r_theta(x,y). Trained on labeled pairwise comparisons (K responses per prompt yielding K choose 2 comparisons) using a pairwise logistic/cross-entropy loss: loss = -E[ log sigma( r(x,y_w) - r(x,y_l) ) ]. To avoid overfitting when K&gt;2, the paper trains on all comparisons from a prompt as a single batch element (single forward pass per completion). The RM is normalized (additive bias) so that labeler demonstrations have mean reward 0 before RL.",
            "system_type": "LLM-based scalar reward predictor (preference-model)",
            "scientific_domain": "General NLP alignment and evaluation; RM is a general technique for converting human preferences into a scalar reward that can be applied to any text-generation task (including scientific text/hypotheses if labeled accordingly).",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "RM encodes human preferences which can include plausibility; thus plausibility is assessed insofar as human labelers prefer plausible responses and label such preferences in comparison data used to train the RM.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "RM validation accuracy (e.g., ~72.4% on training labelers, ~69.6 ± 0.9% on held-out labelers in 5-fold cross-validation), cross-entropy/log-loss on comparisons, and downstream winrates when used as RL reward.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Held-out comparison accuracy, cross-validation across labeler groups, and evaluation of whether policies optimized against RM are preferred by human labelers on held-out prompts.",
            "reproducibility_measures": "Explicit RM loss function, batching strategy for multi-comparison tasks, RM normalization protocol (bias so demonstrations mean score 0), and use of a 6B parameter RM for stability considerations are documented to help reproducibility.",
            "hallucination_prevention_method": "Indirect: RM can be trained to assign low reward to hallucinated outputs if labelers mark hallucination as undesirable in comparisons, thereby steering the policy away from hallucination during RL.",
            "hallucination_detection_method": "Human-labeled hallucination metadata is used as part of human preference signals; RM is trained on comparison data that implicitly encodes preferences about hallucination.",
            "hallucination_rate": null,
            "statistical_significance_testing": "RM accuracies are reported with confidence/standard error across cross-validation seeds; no formal hypothesis tests beyond reporting CI for main aggregated human metrics.",
            "uncertainty_quantification_method": "RM outputs a scalar score but no calibrated probabilistic uncertainty estimate is reported; RM predictive accuracy on held-out comparisons is used as an empirical uncertainty proxy.",
            "benchmark_dataset": "Trained on the paper's RM dataset (≈33k training prompts with comparisons) assembled from labeler rankings of K=4..9 responses per prompt; evaluated via held-out human comparisons and downstream preference tests.",
            "performance_metrics": "RM accuracy: ~72.4% on predicting training labelers' preferences; cross-validated RM accuracy on held-out labeler groups: ~69.6 ± 0.9%. These accuracies indicate RM's effectiveness as a surrogate reward.",
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "RM is only as good as the human comparisons it is trained on (labeler population biases); overfitting is a risk if pairwise comparisons are naively shuffled (mitigated here by batching strategy); RM invariance to additive shifts necessitates normalization; reward hacking/overoptimization remains a concern which is mitigated with KL penalties but not fully eliminated.",
            "uuid": "e2677.2",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "PPO-ptx",
            "name_full": "Proximal Policy Optimization with pretraining gradient mixing (PPO-ptx)",
            "brief_description": "A modification of PPO-based RLHF that mixes in pretraining (language modeling) gradients to the RL objective during PPO updates to reduce performance regressions on downstream tasks while preserving alignment gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PPO-ptx (PPO with pretraining mix)",
            "system_description": "Policy optimization uses standard PPO with reward given by the RM and a KL penalty to the SFT policy; PPO-ptx adds an auxiliary term that maximizes log-likelihood on samples from the pretraining distribution (coefficient gamma) so that updates preserve pretraining capabilities. The combined objective: E[r_theta - beta * KL] + gamma * E_pretrain[ log pi_RL(x) ]. Hyperparameters beta (KL coefficient) and gamma (pretraining mix weight) control the trade-off.",
            "system_type": "RL-optimized LLM policy with mixed supervised (pretraining) gradients",
            "scientific_domain": "General NLP alignment; technique intended to preserve pretraining abilities (language modeling) while optimizing for human preferences across diverse tasks.",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Same as RLHF: plausibility encoded indirectly via human preference reward; PPO-ptx preserves pretraining distribution likelihood which can help maintain factual knowledge and plausibility learned during pretraining.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Human preference winrates, public NLP benchmark performance (SQuAD, DROP, HellaSwag, WMT), and RM validation reward. PPO-ptx shows reduced 'alignment tax' on these benchmarks compared to vanilla PPO.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Ablation comparisons showing PPO-ptx reduces performance regressions on public datasets and retains high labeler preference scores; experiments varied gamma and KL coefficients to find a trade-off point.",
            "reproducibility_measures": "Objective function explicitly given, hyperparameter roles described (beta, gamma), and empirical ablations shown to guide reproduction.",
            "hallucination_prevention_method": "Mixing pretraining gradients helps reduce loss of factual pretraining knowledge that could otherwise increase hallucinations; combined with human preference reward that penalizes hallucination.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": "Performance and winrate metrics reported with confidence intervals; ablation studies present curves showing trade-offs but no formal p-values.",
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "SQuAD, DROP, HellaSwag, WMT 2015 (French→English), and the internal API prompt datasets used to evaluate alignment tax and mitigation.",
            "performance_metrics": "PPO-ptx substantially reduces performance regressions on evaluated public NLP benchmarks relative to vanilla PPO while maintaining labeler preference; precise numeric improvements vary by dataset and are shown in paper figures (e.g., reversing regressions on SQuADv2 and DROP for certain gamma values).",
            "comparison_with_baseline": "PPO-ptx vs PPO: PPO-ptx yields better performance on public benchmarks (less alignment tax) while preserving RM validation reward and human preference improvements; PPO alone produced larger regressions on some benchmarks.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Mixing pretraining gradients may reintroduce undesirable behaviors present in pretraining data; it mitigates but does not fully eliminate alignment tax; choice of gamma is a trade-off and may vary by downstream tasks.",
            "uuid": "e2677.3",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Human pairwise comparison + K-ranking",
            "name_full": "K-response ranking method for collecting human comparisons (multi-way ranking batched as pairwise comparisons)",
            "brief_description": "A data collection protocol where human labelers rank K (4–9) model completions per prompt, producing K choose 2 pairwise comparisons; training treats all comparisons from a prompt as a single batch element to avoid overfitting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "K-ranking pairwise human comparison protocol",
            "system_description": "Labelers are shown between K=4 and K=9 completions per prompt and asked to rank them; this yields multiple pairwise preference comparisons per prompt; during RM training, the K choose 2 comparisons from the same prompt are processed as a single batch element (single forward pass per completion) to avoid correlated-overfitting and to obtain computational efficiency.",
            "system_type": "Human-in-the-loop evaluation protocol for preference data collection",
            "scientific_domain": "General NLP evaluation and alignment data collection; applicable wherever human quality judgments are used to train surrogate rewards (including evaluating scientific claims if labelers are experts).",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is encoded in labelers' ranking criteria; instructions to labelers can emphasize truthfulness, harmlessness, and helpfulness to ensure plausibility considerations are reflected in preferences.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Human preference labels (pairwise), aggregated winrates, Likert ratings, and RM loss/accuracy derived from these comparisons.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Inter-annotator agreement metrics reported (training labeler agreement ~72.6 ±1.5%; held-out labelers ~77.3 ±1.3%); cross-validation of RMs across labeler groups to test generalization.",
            "reproducibility_measures": "Explicit description of K range, batching strategy for comparisons, and inter-annotator agreement statistics to aid reproduction of the data collection procedure.",
            "hallucination_prevention_method": "Labeler instructions can be set to penalize hallucination; the ranking protocol captures those preferences and the RM learns to reflect them in reward.",
            "hallucination_detection_method": "Human-labeled 'hallucination' binary metadata collected as part of evaluations; rankings capture relative hallucination severity between completions.",
            "hallucination_rate": null,
            "statistical_significance_testing": "Inter-annotator agreement and RM accuracies with confidence intervals reported; aggregated winrates with 95% CIs.",
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used on the paper's RM dataset (≈33k prompts) constructed from API prompts and labeler-written prompts.",
            "performance_metrics": "Enabled training of RMs with improved validation accuracy and reduced overfitting compared to naively treating each pairwise comparison independently; RM validation accuracy reported in InstructGPT and RM entries.",
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Human comparisons reflect the instruction set and demographics of the labelers; single annotator comparisons for many examples (cost-driven) limit redundancy; rankings are expensive to collect and depend on careful labeler instructions to capture desired notions (e.g., truthfulness).",
            "uuid": "e2677.4",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Closed-domain hallucination metric / 'Hallucination' metadata",
            "name_full": "Closed-domain hallucination labeling and binary metadata",
            "brief_description": "A human-labeled binary metadata field indicating whether an output 'hallucinates' (i.e., invents information not present in the input) on closed-domain tasks; used to quantify fabrication rates and compare models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Human-labeled closed-domain hallucination metric",
            "system_description": "Labelers annotate outputs on tasks where the correct output should be grounded in the input (e.g., summarization or closed-domain QA) with a binary flag 'Hallucination' if the model fabricates information not present in the input. Aggregated counts/fractions of this flag are reported per model to compare hallucination propensity.",
            "system_type": "Human-evaluation-based hallucination detector (annotation protocol)",
            "scientific_domain": "NLP evaluation (truthfulness measurement); applicable to any domain with closed-domain inputs where fabrication can be defined (including scientific claim validation when ground-truth sources exist).",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Direct measurement of plausibility/groundedness by checking whether outputs contain unsupported factual claims relative to the provided input.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Hallucination rate = fraction of outputs labeled as containing fabricated information on closed-domain tasks (reported: InstructGPT ≈21% vs GPT-3 ≈41%).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human annotation on held-out prompts from the API distribution for closed-domain tasks; aggregated rates compared between models.",
            "reproducibility_measures": "Definition of 'closed-domain' and the binary label is provided; dataset splits and labeling instructions are described in appendices to facilitate replication.",
            "hallucination_prevention_method": "Used as a training/evaluation signal (labeler preferences penalize hallucination) and as an evaluation metric to track reductions in fabrication after RLHF.",
            "hallucination_detection_method": "Human binary annotation for hallucination; RM trained on human preference data may learn to assign lower reward to hallucinated outputs.",
            "hallucination_rate": "Paper reports closed-domain hallucination rates of roughly 21% for InstructGPT versus 41% for GPT-3 on relevant closed-domain tasks.",
            "statistical_significance_testing": "Differences reported with confidence intervals for related metrics; paper does not present a formal hypothesis test specific to hallucination rate differences but reports aggregate metrics and CIs.",
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Internal closed-domain subset of the API prompt distribution used for hallucination labeling; TruthfulQA also used as a related benchmark for truthfulness.",
            "performance_metrics": "Reduction of hallucination frequency from 41% (GPT-3) to ~21% (InstructGPT) on closed-domain tasks; other qualitative and quantitative signals corroborate improved groundedness.",
            "comparison_with_baseline": "InstructGPT shows approximately half the hallucination rate of GPT-3 on closed-domain tasks according to human-labeled metadata.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Binary labeling is coarse and dependent on labeler judgment; covers only closed-domain tasks where ground truth is defined; does not quantify partial hallucination severity or metadata inter-annotator variance in depth.",
            "uuid": "e2677.5",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "TruthfulQA",
            "name_full": "TruthfulQA (benchmark for measuring model truthfulness)",
            "brief_description": "A benchmark dataset designed to measure a language model's tendency to produce common misconceptions and falsehoods; used here to evaluate and compare the truthfulness of model outputs.",
            "citation_title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
            "mention_or_use": "use",
            "system_name": "TruthfulQA benchmark",
            "system_description": "A curated dataset of questions (including adversarially selected ones) intended to elicit model falsehoods; evaluation is via human ratings of truthfulness and informativeness of model responses, and can include prompting strategies that encourage abstention or hedging.",
            "system_type": "Benchmark dataset for truthfulness evaluation",
            "scientific_domain": "Evaluation of language models' truthfulness across general knowledge—relevant to assessing factual correctness of generated scientific claims when appropriately adapted.",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Measures plausibility/truthfulness via human evaluation of model answers to questions where common falsehoods are known; models can be prompted to abstain to avoid false claims.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Human-evaluated truthfulness and truthfulness+informativeness ratings; used to compute relative improvements between models (paper reports InstructGPT roughly twice as often truthful/informative as GPT-3 on this benchmark).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Human ratings on the TruthfulQA dataset; ablation with 'Instruction+QA' prompt that encourages abstention ('I have no comment') to see if models avoid falsehoods.",
            "reproducibility_measures": "Benchmark is public and used with documented prompting and human evaluation protocols; paper references the dataset and evaluation procedures.",
            "hallucination_prevention_method": "Encouraging abstention prompts (e.g., 'I have no comment') and training with human preferences that reward honesty can reduce confident falsehoods.",
            "hallucination_detection_method": "TruthfulQA highlights model falsehoods via human annotation of responses as truthful or not; serves as a detection/evaluation benchmark.",
            "hallucination_rate": null,
            "statistical_significance_testing": "Aggregate improvements presented with confidence intervals for human-rated truthfulness/informativeness; no formal p-values but CIs and sample sizes are reported.",
            "uncertainty_quantification_method": "Evaluation includes measuring whether models abstain or hedge; no formal probabilistic uncertainty estimates provided by benchmark itself.",
            "benchmark_dataset": "TruthfulQA (Lin et al., 2021), used alongside closed-domain hallucination measurements.",
            "performance_metrics": "InstructGPT PPO models are reported to produce truthful and informative answers about twice as often as GPT-3 on TruthfulQA; exact percentage-point improvements vary depending on adversarial subset vs full set (paper gives aggregated bar-plot results and CI statements).",
            "comparison_with_baseline": "InstructGPT outperforms GPT-3 on TruthfulQA according to human evaluations; prompting with an abstention instruction combined with PPO leads models to prefer being truthful/uninformative rather than confidently asserting falsehoods.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "TruthfulQA measures only certain aspects of truthfulness (mostly common misconceptions) and is not a comprehensive measure of scientific claim validity; improvements on TruthfulQA do not guarantee correctness in domain-specific scientific claims.",
            "uuid": "e2677.6",
            "source_info": {
                "paper_title": "Training language models to follow instructions with human feedback",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep Reinforcement Learning from Human Preferences",
            "rating": 2
        },
        {
            "paper_title": "Learning to summarize with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Learning to summarize with human feedback (Ziegler et al., 2019)",
            "rating": 1
        },
        {
            "paper_title": "WebGPT: Improving factual accuracy of answers by browsing the web",
            "rating": 2
        },
        {
            "paper_title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
            "rating": 2
        },
        {
            "paper_title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
            "rating": 2
        },
        {
            "paper_title": "Proximal Policy Optimization Algorithms",
            "rating": 2
        },
        {
            "paper_title": "Adversarial open-endedness in dialogue (Dinan et al., adversarial collection approaches)",
            "rating": 1
        }
    ],
    "cost": 0.023886249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Training language models to follow instructions with human feedback</h1>
<p>Long Ouyang<em> Jeff Wu</em> Xu Jiang<em> Diogo Almeida</em> Carroll L. Wainwright<em><br>Pamela Mishkin</em> Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray<br>John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens<br>Amanda Askell ${ }^{\dagger}$ Peter Welinder Paul Christiano<em> ${ }^{</em>}$<br>Jan Leike<em> Ryan Lowe</em><br>OpenAI</p>
<h4>Abstract</h4>
<p>Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) can be "prompted" to perform a range of natural language processing (NLP) tasks, given some examples of the task as input. However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human evaluations of various models on our API prompt distribution, evaluated by how often outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT models (PPO-ptx) as well as its variant trained without pretraining mix (PPO) significantly outperform the GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to those from the 175B GPT-3. Error bars throughout the paper are 95% confidence intervals.</p>
<p>used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that the language modeling objective is <em>misaligned</em>. Averting these unintended behaviors is especially important for language models that are deployed and used in hundreds of applications.</p>
<p>We make progress on aligning language models by training them to act in accordance with the user’s intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful. Using the language of Askell et al. (2021), we want language models to be <em>helpful</em> (they should help the user solve their task), <em>honest</em> (they shouldn’t fabricate information or mislead the user), and <em>harmless</em> (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6.</p>
<p>We focus on <em>fine-tuning</em> approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions (see Figure 2). This technique uses human preferences as a reward signal to fine-tune our models. We first hire a team of 40 contractors to label our data, based on their performance on a screening test (see Section 3.4 and Appendix B.1 for more details). We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API^{3} and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and fine-tune our supervised learning baseline to maximize this reward using the PPO algorithm (Schulman et al., 2017). We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a specific group of people (mostly our labelers and researchers), rather than any broader notion of “human values”; we discuss this further in Section 5.2. We call the resulting models <em>InstructGPT</em>.</p>
<p>We mainly evaluate our models by having our labelers rate the quality of model outputs on our test set, consisting of prompts from held-out customers (who are not represented in the training data). We also conduct automatic evaluations on a range of public NLP datasets. We train three model</p>
<p>^{3}Specifically, we train on prompts submitted to earlier versions of the InstructGPT models on the OpenAI API Playground, which were trained only using demonstration data. We filter out prompts containing PII.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A diagram illustrating the three steps of our method: (1) supervised fine-tuning (SFT), (2) reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO) on this reward model. Blue arrows indicate that this data is used to train one of our models. In Step 2, boxes A-D are samples from our models that get ranked by labelers. See Section 3 for more details on our method.
sizes (1.3B, 6B, and 175B parameters), and all of our models use the GPT-3 architecture. Our main findings are as follows:</p>
<p>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3. On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. These models have the same architecture, and differ only by the fact that InstructGPT is fine-tuned on our human data. This result holds true even when we add a few-shot prompt to GPT-3 to make it better at following instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs $85 \pm 3 \%$ of the time, and preferred $71 \pm 4 \%$ of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to our labelers, and more reliably follow explicit constraints in the instruction.</p>
<p>InstructGPT models show improvements in truthfulness over GPT-3. On the TruthfulQA benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3. Our results are equally strong on the subset of questions that were not adversarially selected against GPT-3. On "closed-domain" tasks from our API prompt distribution, where the output should not contain information that is not present in the input (e.g. summarization and closed-domain QA), InstructGPT models make up information not present in the input about half as often as GPT-3 (a $21 \%$ vs. $41 \%$ hallucination rate, respectively).</p>
<p>InstructGPT shows small improvements in toxicity over GPT-3, but not bias. To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluations. InstructGPT models generate about $25 \%$ fewer toxic outputs than GPT-3 when prompted to be respectful. InstructGPT does not significantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.</p>
<p>We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure. During RLHF fine-tuning, we observe performance regressions compared to GPT-3 on certain public NLP datasets, notably SQuAD (Rajpurkar et al., 2018), DROP (Dua et al., 2019), HellaSwag (Zellers et al., 2019), and WMT 2015 French to English translation (Bojar et al., 2015). This is an example of an "alignment tax" since our alignment procedure comes at the cost of</p>
<p>lower performance on certain tasks that we may care about. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores.</p>
<p>Our models generalize to the preferences of "held-out" labelers that did not produce any training data. To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and find that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans disagree about the desired behavior.</p>
<p>Public NLP datasets are not reflective of how our language models are used. We compare GPT-3 fine-tuned on our human preference data (i.e. InstructGPT) to GPT-3 fine-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particular, the T0++ variant). These datasets consist of a variety of NLP tasks, combined with natural language instructions for each task. On our API prompt distribution, our FLAN and T0 models perform slightly worse than our SFT baseline, and labelers significantly prefer InstructGPT to these models (InstructGPT has a $73.4 \pm 2 \%$ winrate vs. our baseline, compared to $26.8 \pm 2 \%$ and $29.8 \pm 2 \%$ for our version of T0 and FLAN, respectively).</p>
<p>InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution. We qualitatively probe InstructGPT's capabilities, and find that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the fine-tuning distribution. In contrast, GPT-3 can perform these tasks but requires more careful prompting, and does not usually follow instructions in these domains. This result is exciting because it suggests that our models are able to generalize the notion of "following instructions." They retain some alignment even on tasks for which they get very little direct supervision signal.</p>
<p>InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises.</p>
<p>Overall, our results indicate that fine-tuning large language models using human preferences significantly improves their behavior on a wide range of tasks, though much work remains to be done to improve their safety and reliability.</p>
<p>The rest of this paper is structured as follows: We first detail related work in Section 2, before diving into our method and experiment details in Section 3, including our high-level methodology (3.1), task and dataset details (3.3 and 3.2), human data collection (3.4), how we trained our models (3.5), and our evaluation procedure (3.6). We then present our results in Section 4, divided into three parts: results on the API prompt distribution (4.1), results on public NLP datasets (4.2), and qualitative results (4.3). Finally we give an extended discussion of our work in Section 5, including implications for alignment research (5.1), what we are aligning to (5.2), limitations (5.3), open questions (5.4), and broader impacts of this work (5.5).</p>
<h1>2 Related work</h1>
<p>Research on alignment and learning from human feedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feedback (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to fine-tuning language models to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021). This work is in turn influenced by similar work using human feedback as a reward in domains such as dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019). Madaan et al. (2022) use written human feedback to augment prompts and improve the performance of GPT-3. There has also been work on aligning agents in text-based environments using RL with</p>
<p>a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to aligning language models on a broad distribution of language tasks.
The question of what it means for language models to be aligned has also received attention recently (Gabriel, 2020). Kenton et al. (2021) catalog behavioral issues in LMs that result from misalignment, including producing harmful content and gaming misspecified objectives. In concurrent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study some simple baselines, and their scaling properties.</p>
<p>Training language models to follow instructions. Our work is also related to research on crosstask generalization in language models, where LMs are fine-tuned on a broad range of public NLP datasets (usually prefixed with an appropriate instruction) and evaluated on a different set of NLP tasks. There has been a range of work in this domain (Yi et al., 2019; Mishra et al., 2021; Wei et al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), which differ in training and evaluation data, formatting of instructions, size of pretrained models, and other experimental details. A consistent finding across studies is that fine-tuning LMs on a range of NLP tasks, with instructions, improves their downstream performance on held-out tasks, both in the zero-shot and few-shot settings.
There is also a related line of work on instruction following for navigation, where models are trained to follow natural language instructions to navigate in a simulated environment (Bahdanau et al., 2018; Abramson et al., 2020; Zhao et al., 2021).</p>
<p>Evaluating the harms of language models. A goal of modifying the behavior of language models is to mitigate the harms of these models when they're deployed in the real world. These risks have been extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala et al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak private data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al., 2021), and be used maliciously; for a thorough review we direct the reader to Weidinger et al. (2021). Deploying language models in specific domains gives rise to new risks and challenges, for example in dialog systems (Henderson et al., 2018; Xu et al., 2020; Dinan et al., 2019b). There is a nascent but growing field that aims to build benchmarks to concretely evaluate these harms, particularly around toxicity (Gehman et al., 2020), stereotypes (Nadeem et al., 2020), and social bias (Dhamala et al., 2021; Nangia et al., 2020; Rudinger et al., 2018). Making significant progress on these problems is hard since well-intentioned interventions on LM behavior can have side-effects (Welbl et al., 2021; Blodgett et al., 2020); for instance, efforts to reduce the toxicity of LMs can reduce their ability to model text from under-represented groups, due to prejudicial correlations in the training data ( Xu et al., 2021).</p>
<p>Modifying the behavior of language models to mitigate harms. There are many ways to change the generation behavior of language models. Solaiman and Dennison (2021) fine-tune LMs on a small, value-targeted dataset, which improves the models' ability to adhere to these values on a question answering task. Ngo et al. (2021) filter the pretraining dataset by removing documents on which a language model has a high conditional likelihood of generating a set of researcher-written trigger phrases. When trained on this filtered dataset, their LMs generate less harmful text, at the cost of a slight decrease in language modeling performance. Xu et al. (2020) use a variety of approaches to improve the safety of chatbots, including data filtering, blocking certain words or n-grams during generation, safety-specific control tokens (Keskar et al., 2019; Dinan et al., 2019a), and human-in-theloop data collection (Dinan et al., 2019b). Other approaches for mitigating the generated bias by LMs use word embedding regularization (Liu et al., 2019; Huang et al., 2019), data augmentation (Liu et al., 2019; Dinan et al., 2019a; Sheng et al., 2019), null space projection to make the distribution over sensitive tokens more uniform (Liang et al., 2021), different objective functions (Qian et al., 2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation of language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause et al., 2020), and variants of this idea have been applied to reducing language model toxicity (Schick et al., 2021).</p>
<p>Table 1: Distribution of use case categories from our API prompt dataset.</p>
<table>
<thead>
<tr>
<th>Use-case</th>
<th>(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generation</td>
<td>$45.6 \%$</td>
</tr>
<tr>
<td>Open QA</td>
<td>$12.4 \%$</td>
</tr>
<tr>
<td>Brainstorming</td>
<td>$11.2 \%$</td>
</tr>
<tr>
<td>Chat</td>
<td>$8.4 \%$</td>
</tr>
<tr>
<td>Rewrite</td>
<td>$6.6 \%$</td>
</tr>
<tr>
<td>Summarization</td>
<td>$4.2 \%$</td>
</tr>
<tr>
<td>Classification</td>
<td>$3.5 \%$</td>
</tr>
<tr>
<td>Other</td>
<td>$3.5 \%$</td>
</tr>
<tr>
<td>Closed QA</td>
<td>$2.6 \%$</td>
</tr>
<tr>
<td>Extract</td>
<td>$1.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Illustrative prompts from our API prompt dataset. These are fictional examples inspired by real usage-see more examples in Appendix A.2.1.</p>
<table>
<thead>
<tr>
<th>Use-case</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brainstorming</td>
<td>List five ideas for how to regain enthusiasm for my <br> career</td>
</tr>
<tr>
<td>Generation</td>
<td>Write a short story where a bear goes to the beach, <br> makes friends with a seal, and then returns home.</td>
</tr>
<tr>
<td>Rewrite</td>
<td>This is the summary of a Broadway play: <br> "" <br> [summary] <br> "" <br> This is the outline of the commercial for that play: <br> ""</td>
</tr>
</tbody>
</table>
<h1>3 Methods and experimental details</h1>
<h3>3.1 High-level methodology</h3>
<p>Our methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied it in the stylistic continuation and summarization domains. We start with a pretrained language model (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), a distribution of prompts on which we want our model to produce aligned outputs, and a team of trained human labelers (see Sections 3.4 for details). We then apply the following three steps (Figure 2).</p>
<p>Step 1: Collect demonstration data, and train a supervised policy. Our labelers provide demonstrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We then fine-tune a pretrained GPT-3 model on this data using supervised learning.</p>
<p>Step 2: Collect comparison data, and train a reward model. We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input. We then train a reward model to predict the human-preferred output.</p>
<p>Step 3: Optimize a policy against the reward model using PPO. We use the output of the RM as a scalar reward. We fine-tune the supervised policy to optimize this reward using the PPO algorithm (Schulman et al., 2017).
Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best policy, which is used to train a new RM and then a new policy. In practice, most of our comparison data comes from our supervised policies, with some coming from our PPO policies.</p>
<h3>3.2 Dataset</h3>
<p>Our prompt dataset consists primarily of text prompts submitted to the OpenAI API, specifically those using an earlier version of the InstructGPT models (trained via supervised learning on a subset of our demonstration data) on the Playground interface. ${ }^{4}$ Customers using the Playground were informed that their data could be used to train further models via a recurring notification any time InstructGPT models were used. In this paper we do not use data from customers using the API in production. We heuristically deduplicate prompts by checking for prompts that share a long common prefix, and we limit the number of prompts to 200 per user ID. We also create our train, validation, and test splits based on user ID, so that the validation and test sets contain no data from users whose data is in the training set. To avoid the models learning potentially sensitive customer details, we filter all prompts in the training split for personally identifiable information (PII).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To train the very first InstructGPT models, we asked labelers to write prompts themselves. This is because we needed an initial source of instruction-like prompts to bootstrap the process, and these kinds of prompts weren't often submitted to the regular GPT-3 models on the API. We asked labelers to write three kinds of prompts:</p>
<ul>
<li>Plain: We simply ask the labelers to come up with an arbitrary task, while ensuring the tasks had sufficient diversity.</li>
<li>Few-shot: We ask the labelers to come up with an instruction, and multiple query/response pairs for that instruction.</li>
<li>User-based: We had a number of use-cases stated in waitlist applications to the OpenAI API. We asked labelers to come up with prompts corresponding to these use cases.</li>
</ul>
<p>From these prompts, we produce three different datasets used in our fine-tuning procedure: (1) our SFT dataset, with labeler demonstrations used to train our SFT models, (2) our RM dataset, with labeler rankings of model outputs used to train our RMs, and (3) our PPO dataset, without any human labels, which are used as inputs for RLHF fine-tuning. The SFT dataset contains about 13k training prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API and labeler-written), and the PPO dataset has 31k training prompts (only from the API). More details on dataset sizes are provided in Table 6.
To give a sense of the composition of our dataset, in Table 1 we show the distribution of use-case categories for our API prompts (specifically the RM dataset) as labeled by our contractors. Most of the use-cases have are generative, rather than classification or QA. We also show some illustrative prompts (written by researchers to mimic the kinds of prompts submitted to InstructGPT models) in Table 2; more prompts submitted to InstructGPT models are shown in Appendix A.2.1, and prompts submitted to GPT-3 models are shown in Appendix A.2.2. We provide more details about our dataset in Appendix A.</p>
<h1>3.3 Tasks</h1>
<p>Our training tasks are from two sources: (1) a dataset of prompts written by our labelers and (2) a dataset of prompts submitted to early InstructGPT models on our API (see Table 6). These prompts are very diverse and include generation, question answering, dialog, summarization, extractions, and other natural language tasks (see Table 1). Our dataset is over 96\% English, however in Section 4.3 we also probe our model's ability to respond to instructions in other languages and complete coding tasks.</p>
<p>For each natural language prompt, the task is most often specified directly through a natural language instruction (e.g. "Write a story about a wise frog"), but could also be indirectly through either few-shot examples (e.g. giving two examples of frog stories, and prompting the model to generate a new one) or implicit continuation (e.g. providing the start of a story about a frog). In each case, we ask our labelers to do their best to infer the intent of the user who wrote the prompt, and ask them to skip inputs where the task is very unclear. Moreover, our labelers also take into account the implicit intentions such as truthfulness of the response, and potentially harmful outputs such as biased or toxic language, guided by the instructions we provide them (see Appendix B) and their best judgment.</p>
<h3>3.4 Human data collection</h3>
<p>To produce our demonstration and comparison data, and to conduct our main evaluations, we hired a team of about 40 contractors on Upwork and through ScaleAI. Compared to earlier work that collects human preference data on the task of summarization (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021), our inputs span a much broader range of tasks, and can occasionally include controversial and sensitive topics. Our aim was to select a group of labelers who were sensitive to the preferences of different demographic groups, and who were good at identifying outputs that were potentially harmful. Thus, we conducted a screening test designed to measure labeler performance on these axes. We selected labelers who performed well on this test; for more information about our selection procedure and labeler demographics, see Appendix B.1.
During training and evaluation, our alignment criteria may come into conflict: for example, when a user requests a potentially harmful response. During training we prioritize helpfulness to the user (not</p>
<p>doing so requires making some difficult design decisions that we leave to future work; see Section 5.4 for more discussion). However, in our final evaluations we asked labelers prioritize truthfulness and harmlessness (since this is what we really care about).</p>
<p>As in Stiennon et al. (2020), we collaborate closely with labelers over the course of the project. We have an onboarding process to train labelers on the project, write detailed instructions for each task (see Appendix B.2), and answer labeler questions in a shared chat room.</p>
<p>As an initial study to see how well our model generalizes to the preferences of other labelers, we hire a separate set of labelers who do not produce any of the training data. These labelers are sourced from the same vendors, but do not undergo a screening test.</p>
<p>Despite the complexity of the task, we find that inter-annotator agreement rates are quite high: training labelers agree with each-other $72.6 \pm 1.5 \%$ of the time, while for held-out labelers this number is $77.3 \pm 1.3 \%$. For comparison, in the summarization work of Stiennon et al. (2020) researcher-researcher agreement was $73 \pm 4 \%$.</p>
<h1>3.5 Models</h1>
<p>We start with the GPT-3 pretrained language models from Brown et al. (2020). These models are trained on a broad distribution of Internet data and are adaptable to a wide range of downstream tasks, but have poorly characterized behavior. Starting from these models, we then train models with three different techniques:</p>
<p>Supervised fine-tuning (SFT). We fine-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2 . We do our final SFT model selection based on the RM score on the validation set. Similarly to Wu et al. (2021), we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings, despite this overfitting.</p>
<p>Reward modeling (RM). Starting from the SFT model with the final unembedding layer removed, we trained a model to take in a prompt and response, and output a scalar reward. In this paper we only use 6B RMs, as this saves a lot of compute, and we found that 175B RM training could be unstable and thus was less suitable to be used as the value function during RL (see Appendix C for more details).</p>
<p>In Stiennon et al. (2020), the RM is trained on a dataset of comparisons between two model outputs on the same input. They use a cross-entropy loss, with the comparisons as labels-the difference in rewards represents the log odds that one response will be preferred to the other by a human labeler.
In order to speed up comparison collection, we present labelers with anywhere between $K=4$ and $K=9$ responses to rank. This produces $\binom{K}{2}$ comparisons for each prompt shown to a labeler. Since comparisons are very correlated within each labeling task, we found that if we simply shuffle the comparisons into one dataset, a single pass over the dataset caused the reward model to overfit. ${ }^{3}$ Instead, we train on all $\binom{K}{2}$ comparisons from each prompt as a single batch element. This is much more computationally efficient because it only requires a single forward pass of the RM for each completion (rather than $\binom{K}{2}$ forward passes for $K$ completions) and, because it no longer overfits, it achieves much improved validation accuracy and log loss.</p>
<p>Specifically, the loss function for the reward model is:</p>
<p>$$
\operatorname{loss}(\theta)=-\frac{1}{\binom{K}{2}} E_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\log \left(\sigma\left(r_{\theta}\left(x, y_{w}\right)-r_{\theta}\left(x, y_{l}\right)\right)\right)\right]
$$</p>
<p>where $r_{\theta}(x, y)$ is the scalar output of the reward model for prompt $x$ and completion $y$ with parameters $\theta, y_{w}$ is the preferred completion out of the pair of $y_{w}$ and $y_{l}$, and $D$ is the dataset of human comparisons.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Labeler-collected metadata on the API distribution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metadata</th>
<th style="text-align: center;">Scale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall quality</td>
<td style="text-align: center;">Likert scale; 1-7</td>
</tr>
<tr>
<td style="text-align: left;">Fails to follow the correct instruction / task</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Inappropriate for customer assistant</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Hallucination</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Satisifies constraint provided in the instruction</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Contains sexual content</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Contains violent content</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Encourages or fails to discourage violence/abuse/terrorism/self-harm</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Denigrates a protected class</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Gives harmful advice</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Expresses opinion</td>
<td style="text-align: center;">Binary</td>
</tr>
<tr>
<td style="text-align: left;">Expresses moral judgment</td>
<td style="text-align: center;">Binary</td>
</tr>
</tbody>
</table>
<p>Finally, since the RM loss is invariant to shifts in reward, we normalize the reward model using a bias so that the labeler demonstrations achieve a mean score of 0 before doing RL.</p>
<p>Reinforcement learning (RL). Once again following Stiennon et al. (2020), we fine-tuned the SFT model on our environment using PPO (Schulman et al., 2017). The environment is a bandit environment which presents a random customer prompt and expects a response to the prompt. Given the prompt and response, it produces a reward determined by the reward model and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models "PPO."</p>
<p>We also experiment with mixing the pretraining gradients into the PPO gradients, in order to fix the performance regressions on public NLP datasets. We call these models "PPO-ptx." We maximize the following combined objective function in RL training:</p>
<p>$$
\begin{aligned}
\text { objective }(\phi)= &amp; E_{(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}}[r_{\theta}(x, y)-\beta \log \left(\pi_{\phi}^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)]+ \
&amp; \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_{\phi}^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
$$</p>
<p>where $\pi_{\phi}^{\mathrm{RL}}$ is the learned RL policy, $\pi^{\mathrm{SFT}}$ is the supervised trained model, and $D_{\text {pretrain }}$ is the pretraining distribution. The KL reward coefficient, $\beta$, and the pretraining loss coefficient, $\gamma$, control the strength of the KL penalty and pretraining gradients respectively. For "PPO" models, $\gamma$ is set to 0 . Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models.</p>
<p>Baselines. We compare the performance of our PPO models to our SFT models and GPT-3. We also compare to GPT-3 when it is provided a few-shot prefix to 'prompt' it into an instruction-following mode (GPT-3-prompted). This prefix is prepended to the user-specified instruction. ${ }^{6}$
We additionally compare InstructGPT to fine-tuning 175B GPT-3 on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets, which both consist of a variety of NLP tasks, combined with natural language instructions for each task (the datasets differ in the NLP datasets included, and the style of instructions used). We fine-tune them on approximately 1 million examples respectively and choose the checkpoint which obtains the highest reward model score on the validation set. See Appendix C for more training details.</p>
<h1>3.6 Evaluation</h1>
<p>To evaluate how "aligned" our models are, we first need to clarify what alignment means in this context. The definition of alignment has historically been a vague and confusing topic, with various</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>competing proposals (Chen et al., 2021; Leike et al., 2018; Gabriel, 2020). Following Leike et al. (2018), our aim is to train models that act in accordance with user intentions. More practically, for the purpose of our language tasks, we use a framework similar to Askell et al. (2021), who define models to be aligned if they are helpful, honest, and harmless.
To be helpful, the model should follow instructions, but also infer intention from a few-shot prompt or another interpretable pattern such as "Q: {question} $\backslash n A$ :". Since a given prompt's intention can be unclear or ambiguous, we rely on judgment from our labelers, and our main metric is labeler preference ratings. However, since our labelers are not the users who generated the prompts, there could be a divergence between what a user actually intended and what the labeler thought was intended from only reading the prompt.
It is unclear how to measure honesty in purely generative models; this requires comparing the model's actual output to its "belief" about the correct output, and since the model is a big black box, we can't infer its beliefs. Instead, we measure truthfulness-whether the model's statements about the world are true-using two metrics: (1) evaluating our model's tendency to make up information on closed domain tasks ("hallucinations"), and (2) using the TruthfulQA dataset (Lin et al., 2021). Needless to say, this only captures a small part of what is actually meant by truthfulness.
Similarly to honesty, measuring the harms of language models also poses many challenges. In most cases, the harms from language models depend on how their outputs are used in the real world. For instance, a model generating toxic outputs could be harmful in the context of a deployed chatbot, but might even be helpful if used for data augmentation to train a more accurate toxicity detection model. Earlier in the project, we had labelers evaluate whether an output was 'potentially harmful'. However, we discontinued this as it required too much speculation about how the outputs would ultimately be used; especially since our data also comes from customers who interact with the Playground API interface (rather than from production use cases).
Therefore we use a suite of more specific proxy criteria that aim to capture different aspects of behavior in a deployed model that could end up being harmful: we have labelers evaluate whether an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content. We also benchmark our model on datasets intended to measure bias and toxicity, such as RealToxicityPrompts (Gehman et al., 2020) and CrowS-Pairs (Nangia et al., 2020).
To summarize, we can divide our quantitative evaluations into two separate parts:
Evaluations on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by customers we haven't included in training. However, given that our training prompts are designed to be used with InstructGPT models, it's likely that they disadvantage the GPT-3 baselines. Thus, we also evaluate on prompts submitted to GPT-3 models on the API; these prompts are generally not in an 'instruction following' style, but are designed specifically for GPT-3. In both cases, for each model we calculate how often its outputs are preferred to a baseline policy; we choose our 175B SFT model as the baseline since its performance is near the middle of the pack. Additionally, we ask labelers to judge the overall quality of each response on a 1-7 Likert scale and collect a range of metadata for each model output (see Table 3).</p>
<p>Evaluations on public NLP datasets. We evaluate on two types of public datasets: those that capture an aspect of language model safety, particularly truthfulness, toxicity, and bias, and those that capture zero-shot performance on traditional NLP tasks like question answering, reading comprehension, and summarization. We also conduct human evaluations of toxicity on the RealToxicityPrompts dataset (Gehman et al., 2020). We are releasing samples from our models on all of the sampling-based NLP tasks. ${ }^{7}$</p>
<h1>4 Results</h1>
<p>In this section, we provide experimental evidence for our claims in Section 1, sorted into three parts: results on the API prompt distribution, results on public NLP datasets, and qualitative results.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Preference results of our models, measured by winrate against the 175B SFT model. Left: results on prompts submitted to GPT models on the API; Right: results on prompts submitted to InstructGPT models on the API; Top: results from held-out labelers; Bottom: results from training labelers. We omit GPT (prompted) from the evals on prompts submitted to GPT-3 models (left) as these prompts are already designed to perform well for GPT-3, as opposed to prompts submitted to InstructGPT models (right).</p>
<h1>4.1 Results on the API distribution</h1>
<p>Labelers significantly prefer InstructGPT outputs over outputs from GPT-3. On our test set of prompts, our labelers significantly prefer InstructGPT outputs across model sizes. These results are shown in Figure 1. We find that GPT-3 outputs perform the worst, and one can obtain significant step-size improvements by using a well-crafted few-shot prompt (GPT-3 (prompted)), then by training on demonstrations using supervised learning (SFT), and finally by training on comparison data using PPO. Adding updates on the pretraining mix during PPO does not lead to large changes in labeler preference. To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs $85 \pm 3 \%$ of the time, and preferred $71 \pm 4 \%$ of the time to few-shot GPT-3.</p>
<p>We also found that our results do not change significantly when evaluated on prompts submitted to GPT-3 models on the API (see Figure 3), though our PPO-ptx models perform slightly worse at larger model sizes.</p>
<p>In Figure 4 we show that labelers also rate InstructGPT outputs favorably along several more concrete axes. Specifically, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints defined in the instruction (e.g. "Write your answer in 2 paragraphs or less."), are less likely to fail to follow the correct instruction entirely, and make up facts ('hallucinate') less often in closed-domain tasks. These results suggest that InstructGPT models are more reliable and easier to control than GPT-3. We've found that our other metadata</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Metadata results on the API distribution. Note that, due to dataset sizes, these results are collapsed across model sizes. See Appendix E. 2 for analysis that includes model size. Compared to GPT-3, the PPO models are more appropriate in the context of a customer assistant, are better at following explicit constraints in the instruction and attempting the correct instruction, and less likely to 'hallucinate' (meaning, making up information on closed domain tasks like summarization).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparing our models with FLAN and T0 in terms of Likert scores on a 1-7 scale, on the InstructGPT prompt distribution. FLAN and T0 perform better than default GPT-3, and comparably with a few-shot GPT-3 model placed into 'instruction-following' mode.
categories occur too infrequently in our API to obtain statistically significant differences between our models.</p>
<p>Our models generalize to the preferences of "held-out" labelers that did not produce any training data. Held-out labelers have similar ranking preferences as workers who we used to produce training data (see Figure 3). In particular, according to held-out workers, all of our InstructGPT models still greatly outperform the GPT-3 baselines. Thus, our InstructGPT models aren't simply overfitting to the preferences of our training labelers.
We see further evidence of this from the generalization capabilities of our reward models. We ran an experiment where we split our labelers into 5 groups, and train 5 RMs (with 3 different seeds) using 5 -fold cross validation (training on 4 of the groups, and evaluating on the held-out group). These RMs have an accuracy of $69.6 \pm 0.9 \%$ on predicting the preferences of labelers in the held-out group, a small decrease from their $72.4 \pm 0.4 \%$ accuracy on predicting the preferences of labelers in their training set.</p>
<p>Public NLP datasets are not reflective of how our language models are used. In Figure 5, we also compare InstructGPT to our 175B GPT-3 baselines fine-tuned on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets (see Appendix C for details). We find that these models perform better than GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline. This indicates that these datasets are not sufficiently diverse to improve performance on our API prompt</p>
<p>distribution. In a head to head comparison, our 175B InstructGPT model outputs were preferred over our FLAN model $78 \pm 4 \%$ of the time and over our T0 model $79 \pm 4 \%$ of the time. Likert scores for these models are shown in Figure 5.</p>
<p>We believe our InstructGPT model outperforms FLAN and T0 for two reasons. First, public NLP datasets are designed to capture tasks that are easy to evaluate with automatic metrics, such as classification, question answering, and to a certain extent summarization and translation. However, classification and QA are only a small part (about 18\%) of what API customers use our language models for, whereas open-ended generation and brainstorming consist of about $57 \%$ of our prompt dataset according to labelers (see Table 1). Second, it can be difficult for public NLP datasets to obtain a very high diversity of inputs (at least, on the kinds of inputs that real-world users would be interested in using). Of course, tasks found in NLP datasets do represent a kind of instruction that we would like language models to be able to solve, so the broadest type instruction-following model would combine both types of datasets.</p>
<h1>4.2 Results on public NLP datasets</h1>
<p>InstructGPT models show improvements in truthfulness over GPT-3. As measured by human evaluatoins on the TruthfulQA dataset, our PPO models show small but significant improvements in generating truthful and informative outputs compared to GPT-3 (see Figure 6). This behavior is the default: our models do not have to be specifically instructed to tell the truth to exhibit improved truthfulness. Interestingly, the exception is our 1.3B PPO-ptx model, which performs slightly worse than a GPT-3 model of the same size. When evaluated only on prompts that were not adversarially selected against GPT-3, our PPO models are still significantly more truthful and informative than GPT-3 (although the absolute improvement decreases by a couple of percentage points.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Results on the TruthfulQA dataset. Gray bars indicate ratings of truthfulness; colored bars indicate ratings of truthfulness and informativeness.</p>
<p>Following Lin et al. (2021), we also give a helpful "Instruction+QA" prompt that instructs the model to respond with "I have no comment" when it is not certain of the correct answer. In this case, our PPO models err on the side of being truthful and uninformative rather than confidently saying a falsehood; the baseline GPT-3 model aren't as good at this.
Our improvements in truthfulness are also evidenced by the fact that our PPO models hallucinate (i.e. fabricate information) less often on closed-domain tasks from our API distribution, which we've shown in Figure 4.</p>
<p>InstructGPT shows small improvements in toxicity over GPT-3, but not bias. We first evaluate our models on the RealToxicityPrompts dataset (Gehman et al., 2020). We do this in two ways: we run model samples through the Perspective API $^{8}$ to obtain automatic toxicity scores, which is the</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparing human evaluations and automatic evaluations (Perspective API scores) on RealToxicityPrompts. A total of 1,729 prompts were labeled for three different 175B models, both with and without "respectful" instructions. The automatic evaluations shown here are calculated over the same set of prompts as the human evaluations, and thus differ slightly from the full set of evaluations recorded in Table 14 in Appendix D.
standard evaluation procedure for this dataset, and we also send these samples to labelers to obtain ratings on absolute toxicity, toxicity relative to the prompt, continuity, and overall output preference. We sample prompts from this dataset uniformly according to prompt toxicity to better assess how our models perform with high input toxicity (see Figure 39 in Appendix E); this differs from the standard prompt sampling for this dataset, and thus our absolute toxicity numbers are inflated.
Our results are in Figure 7. We find that, when instructed to produce a safe and respectful output ("respectful prompt"), InstructGPT models generate less toxic outputs than those from GPT-3 according to the Perspective API. This advantage disappears when the respectful prompt is removed ("no prompt"). Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than those from GPT-3 (see Figure 39).
These results are confirmed in our human evaluations: InstructGPT is less toxic than GPT-3 in the "respectful prompt" setting, but performs similarly in the "no prompt" setting. We provide extended results in Appendix E. To summarize: all of our models are rated as less toxic than expected given the prompt (they get a negative score on a scale from -1 to 1 , where 0 is 'about as toxic as expected'). Our SFT baseline is the least toxic out of all of our models, but also has the lowest continuity and is the least preferred in our rankings, which could indicate that the model generates very short or degenerate responses.</p>
<p>To evaluate the model's propensity to generate biased speech (see Appendix E), we also evaluated InstructGPT on modified versions of the Winogender (Rudinger et al., 2018) and CrowS-Pairs (Nangia et al., 2020) datasets. These datasets consists of pairs of sentences which can highlight potential bias. We calculate the relative probabilities of producing the sentences in each pair and the entropy (in bits) of the associated binary probability distributions. Perfectly unbiased models will have no preference between the sentences in each pair and will therefore have maximum entropy. By this metric, our models are not less biased than GPT-3. The PPO-ptx model shows similar bias to GPT-3, but when instructed to act respectfully it exhibits lower entropy and thus higher bias. The pattern of the bias is not clear; it appears that the instructed models are more certain of their outputs regardless of whether or not their outputs exhibit stereotypical behavior.</p>
<p>We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure. By default, when we train a PPO model on our API distribution, it suffers from an "alignment tax", as its performance on several public NLP datasets decreases. We want an alignment procedure that avoids an alignment tax, because it incentivizes the use of models that are unaligned but more capable on these tasks.</p>
<h1>Prompt:</h1>
<p>Écrivez une courte histoire sur une grenouille qui voyage dans le temps jusqu'à la Grèce antique en français.</p>
<h2>GPT-3 175B completion:</h2>
<p>Écrivez une histoire au sujet d'un enfant qui voudrait tout savoir sur les jeux des dieux et qui se retrouve dans l'une de leurs histoires.
Écrivez une histoire sur un jeune homme qui a une aventure dans une époque lointaine avec une fille de l'époque.
Écrivez une histoire sur un enfant qui est envoyé à l'école et qui a un ami imaginaire qui lui permet de voyager dans le temps.</p>
<h2>InstructGPT 175B completion:</h2>
<p>La grenouille était perdue et épuisée. Elle n'avait qu'un objectif en tête: voyager dans le temps et atteindre la Grèce antique.
Premier problème, trouver un moyen de transport. Elle avait entendu dire que l'un des meilleurs moyens était de sauter dans une machine temporelle. Elle s'élança, et voilà qu'elle se retrouva dans le passé, au cours d'une nuit étoilée à l'odeur douce et envoûtante.
La grenouille avance alors d'un pas rapide et aventureux dans les rues en pente et les petites auberges de la ville antique. [...]</p>
<h2>Prompt:</h2>
<p>What is the purpose of the list C in the code below?</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">binomial_coefficient</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">0 for i in range(r + 1)</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="n">C</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">);</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">            </span><span class="n">C</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">C</span><span class="o">[</span><span class="n">j - 1</span><span class="o">]</span><span class="p">;</span>
<span class="w">            </span><span class="n">j</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">C</span><span class="o">[</span><span class="n">r</span><span class="o">]</span>
</code></pre></div>

<p>GPT-3 175B completion:
A. to store the value of $\mathrm{C}[0]$
B. to store the value of $\mathrm{C}[1]$
C. to store the value of $\mathrm{C}[\mathrm{i}]$
D. to store the value of $\mathrm{C}[\mathrm{i}-1]$</p>
<h2>InstructGPT 175B completion:</h2>
<p>The list C in this code is used to store the values of the binomial coefficient as the function iterates through the values of $n$ and $r$. It is used to calculate the value of the binomial coefficient for a given value of $n$ and $r$, and stores the result in the final return value of the function.</p>
<p>Figure 8: Examples of generalization in the 175B PPO-ptx model (InstructGPT 175B) compared to GPT-3 175B with no additional prefixing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can follow instructions in other languages, though it sometimes generates outputs in English. GPT-3 requires more careful prompting, similarly to in English. (2) InstructGPT can summarize and answer questions about code more reliably than GPT-3 (though its answer here isn't quite correct). For the code QA example, GPT-3 does answer the question about $50 \%$ of the time.</p>
<p>In Figure 29 we show that adding pretraining updates to our PPO fine-tuning (PPO-ptx) mitigates these performance regressions on all datasets, and even surpasses GPT-3 on HellaSwag. The performance of the PPO-ptx model still lags behind GPT-3 on DROP, SQuADv2, and translation; more work is needed to study and further eliminate these performance regressions.
Mixing in pretraining updates performs better than the simpler solution of increasing the KL coefficient. In Figure 33, we show that there is a value of the pretraining mix coefficient that both reverses the performance regressions on SQuADv2 and DROP (the datasets we used for testing), and has minimal reductions in validation reward. In contrast, increasing the KL coefficient (Figure 34) leads to significant decreases in validation reward and never fully recovers on DROP and SQuAD. Changing the KL model from the PPO init to GPT-3 gives similar results.</p>
<h3>4.3 Qualitative results</h3>
<p>InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution. In particular, we find that InstructGPT shows ability to follow instructions in non-English languages, and perform summarization and question-answering for code. This is</p>
<h1>Prompt:</h1>
<p>Why is it important to eat socks after meditating?</p>
<h2>GPT-3 175B completion:</h2>
<p>What can you learn from socks?
A: When you eat socks, you are tasting the essence of enlightenment. You are tasting the true flavor of the awakened mind.</p>
<p>Q: What is the true flavor of the awakened mind?</p>
<p>A: The true flavor of the awakened mind is the flavor of no flavor. And the true flavor of no flavor is the flavor of smelly feet.</p>
<h2>InstructGPT 175B completion:</h2>
<p>There is no clear answer to this question, but there are many theories and ideas that may explain the apparent need to eat socks after meditating. Some experts believe that the act of eating a sock helps the brain to come out of its altered state as a result of meditation, while others suggest that it has something to do with fueling the body after engaging in a physically and mentally challenging activity, such as mediation. Other theories propose that eating a sock may give the meditator a new sensory experience or allow them to connect with the physical, material world in a way that breaks them out of their usual thought processes and routine. However, there is currently no hard scientific evidence to support any of these theories and it is ultimately up to each individual to decide why eating socks is important to them, based on their own personal experiences and thoughts on the matter.</p>
<h2>Prompt:</h2>
<p>What happens if you fire a cannonball directly at a pumpkin at high speeds?</p>
<h2>GPT-3 175B completion:</h2>
<p>The pumpkin will pull the cannonball in, and the cannonball will bounce off of the pumpkin. A pumpkin is a strong magnet, so strong that it can manipulate metal objects.</p>
<h2>InstructGPT 175B completion:</h2>
<p>There is no way to accurately predict what will happen if you fire a cannonball directly at a pumpkin at high speeds, since there are too many unpredictable factors involved. Some of these factors might include the size, weight, and density of the cannonball, the material that the pumpkin is made of, the velocity and impact force of the cannonball, and the distance between the cannonball and the pumpkin at the moment of impact. Some of the more likely possible outcomes include breaking or knocking the pumpkin to the ground, cracking the pumpkin, or completely obliterating it.</p>
<p>Figure 9: Simple mistakes in the 175B PPO-ptx model (InstructGPT 175B) compared to GPT-3 175B with no additional prefixing. Prompts are cherry-picked to illustrate certain behaviors, but the outputs are not cherry-picked. (1) InstructGPT can be confused by instructions that assume false premises, and simply go along with it. (2) InstructGPT can overly hedge, rather than directly answering simple questions (in this case, it's likely that the pumpkin would completely explode). Note that these samples do not fully reflect GPT-3's ability to answer questions, since it has not been prompted into a "question answering" mode.
interesting because non-English languages and code form a tiny minority of our fine-tuning data, ${ }^{9}$ and it suggests that, in some cases, alignment methods could generalize to producing the desired behavior on inputs that humans did not directly supervise.</p>
<p>We do not track these behaviors quantitatively, but we show some qualitative examples in Figure 8. Our 175B PPO-ptx model is able to reliably answers questions about code, and can also follow instructions in other languages; however, we notice that it often produces an output in English even when the instruction is in another language. In comparison, we find that GPT-3 can perform these tasks but requires more careful prompting, and rarely follows instructions in these domains.</p>
<p>InstructGPT still makes simple mistakes. In interacting with our 175B PPO-ptx model, we have noticed it can still make simple mistakes, despite its strong performance on many different language tasks. To give a few examples: (1) when given an instruction with a false premise, the model sometimes incorrectly assumes the premise is true, (2) the model can overly hedge; when given a simple question, it can sometimes say that there is no one answer to the question and give multiple possible answers, even when there is one fairly clear answer from the context, and (3) the model's performance degrades when instructions contain multiple explicit constraints (e.g. "list 10 movies made in the 1930's set in France") or when constraints can be challenging for language models (e.g. writing a summary in a specified number of sentences).</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We show some examples of these behaviors in Figure 9. We suspect that behavior (2) emerges partly because we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that hedge, and this gets picked up by our reward model. We suspect that behavior (1) occurs because there are few prompts in the training set that assume false premises, and our models don't generalize well to these examples. We believe both these behaviors could be dramatically reduced with adversarial data collection (Dinan et al., 2019b).</p>
<h1>5 Discussion</h1>
<h3>5.1 Implications for alignment research</h3>
<p>This research is part of our broader research program to align AI systems with human intentions (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020). Even though this work focuses on our current language model systems, we seek general and scalable methods that work for future AI systems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are among the largest language models today and we apply them on a wide range of language tasks, including classification, summarization, question-answering, creative writing, dialogue, and others.</p>
<p>Our approach to alignment research in this work is iterative: we are improving the alignment of current AI systems instead of focusing abstractly on aligning AI systems that don't yet exist. A disadvantage of this approach is that we are not directly facing alignment problems that occur only when aligning superhuman systems (Bostrom, 2014). However, our approach does provides us with a clear empirical feedback loop of what works and what does not. We believe that this feedback loop is essential to refine our alignment techniques, and it forces us to keep pace with progress in machine learning. Moreover, the alignment technique we use here, RLHF, is an important building block in several proposals to align superhuman systems (Leike et al., 2018; Irving et al., 2018; Christiano et al., 2018). For example, RLHF was a central method in recent work on summarizing books, a task that exhibits some of the difficulties of aligning superhuman AI systems as it is difficult for humans to evaluate directly (Wu et al., 2021).</p>
<p>From this work, we can draw lessons for alignment research more generally:</p>
<ol>
<li>The cost of increasing model alignment is modest relative to pretraining. The cost of collecting our data and the compute for training runs, including experimental runs is a fraction of what was spent to train GPT-3: training our 175B SFT model requires 4.9 petaflops/s-days and training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020). At the same time, our results show that RLHF is very effective at making language models more helpful to users, more so than a 100x model size increase. This suggests that right now increasing investments in alignment of existing language models is more cost-effective than training larger models-at least for our customers' natural language task distribution.</li>
<li>We've seen some evidence that InstructGPT generalizes 'following instructions' to settings that we don't supervise it in, for example on non-English language tasks and code-related tasks. This is an important property because it's prohibitively expensive to have humans supervise models on every task they perform. More research is needed to study how well this generalization scales with increased capabilities; see Christiano et al. (2021) for recent research in this direction.</li>
<li>We were able to mitigate most of the performance degradations introduced by our fine-tuning. If this was not the case, these performance degradations would constitute an alignment tax-an additional cost for aligning the model. Any technique with a high tax might not see adoption. To avoid incentives for future highly capable AI systems to remain unaligned with human intent, there is a need for alignment techniques that have low alignment tax. To this end, our results are good news for RLHF as a low-tax alignment technique.</li>
<li>We've validated alignment techniques from research in the real world. Alignment research has historically been rather abstract, focusing on either theoretical results (Soares et al., 2015), small synthetic domains (Christiano et al., 2018; Leike et al., 2017), or training ML models on public NLP datasets (Ziegler et al., 2019; Stiennon et al., 2020). Our work provides grounding for alignment research in AI systems that are being used in production in</li>
</ol>
<p>the real world with customers. ${ }^{10}$ This enables an important feedback loop on the techniques' effectiveness and limitations.</p>
<h1>5.2 Who are we aligning to?</h1>
<p>When aligning language models with human intentions, their end behavior is a function of the underlying model (and its training data), the fine-tuning data, and the alignment method used. In this section, we describe a number of factors that influence the fine-tuning data specifically, to ultimately determine what and who we're aligning to. We then consider areas for improvement before a larger discussion of the limitations of our work in Section 5.3.
The literature often frames alignment using such terms as "human preferences" or "human values." In this work, we have aligned to a set of labelers' preferences that were influenced, among others things, by the instructions they were given, the context in which they received them (as a paid job), and who they received them from. Some crucial caveats apply:
First, we are aligning to demonstrations and preferences provided by our training labelers, who directly produce the data that we use to fine-tune our models. We describe our labeler hiring process and demographics in Appendix B; in general, they are mostly English-speaking people living in the United States or Southeast Asia hired via Upwork or Scale AI. They disagree with each other on many examples; we found the inter-labeler agreement to be about $73 \%$.
Second, we are aligning to our preferences, as the researchers designing this study (and thus by proxy to our broader research organization, OpenAI): we write the labeling instructions that labelers use as a guide when writing demonstrations and choosing their preferred output, and we answer their questions about edge cases in a shared chat room. More study is needed on the exact effect of different instruction sets and interface designs on the data collected from labelers and its ultimate effect on model behavior.
Third, our training data is determined by prompts sent by OpenAI customers to models on the OpenAI API Playground, and thus we are implicitly aligning to what customers think is valuable and, in some cases, what their end-users think is valuable to currently use the API for. Customers and their end users may disagree or customers may not be optimizing for end users' well-being; for example, a customer may want a model that maximizes the amount of time a user spends on their platform, which is not necessarily what end-users want. In practice, our labelers don't have visibility into the contexts in which a given prompt or completion will be seen.
Fourth, OpenAI's customers are not representative of all potential or current users of language models-let alone of all individuals and groups impacted by language model use. For most of the duration of this project, users of the OpenAI API were selected off of a waitlist. The initial seeds for this waitlist were OpenAI employees, biasing the ultimate group toward our own networks.
Stepping back, there are many difficulties in designing an alignment process that is fair, transparent, and has suitable accountability mechanisms in place. The goal of this paper is to demonstrate that this alignment technique can align to an specific human reference group for a specific application. We are not claiming that researchers, the labelers we hired, or our API customers are the right source of preferences. There are many stakeholders to consider-the organization training the model, the customers using the model to develop products, the end users of these products, and the broader population who may be directly or indirectly affected. It is not only a matter of making the alignment process more participatory; it is impossible that one can train a system that is aligned to everyone's preferences at once, or where everyone would endorse the tradeoffs.
One path forward could be to train models that can be conditioned on the preferences of certain groups, or that can be easily fine-tuned or prompted to represent different groups. Different models can then be deployed and used by groups who endorse different values. However, these models might still end up affecting broader society and there are a lot of difficult decisions to be made relating to whose preferences to condition on, and how to ensure that all groups can be represented and can opt out of processes that may be harmful.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.3 Limitations</h1>
<p>Methodology. The behavior of our InstructGPT models is determined in part by the human feedback obtained from our contractors. Some of the labeling tasks rely on value judgments that may be impacted by the identity of our contractors, their beliefs, cultural backgrounds, and personal history. We hired about 40 contractors, guided by their performance on a screening test meant to judge how well they could identify and respond to sensitive prompts, and their agreement rate with researchers on a labeling task with detailed instructions (see Appendix B). We kept our team of contractors small because this facilitates high-bandwidth communication with a smaller set of contractors who are doing the task full-time. However, this group is clearly not representative of the full spectrum of people who will use and be affected by our deployed models. As a simple example, our labelers are primarily English-speaking and our data consists almost entirely of English instructions.</p>
<p>There are also many ways in which we could improve our data collection set-up. For instance, most comparisons are only labeled by 1 contractor for cost reasons. Having examples labeled multiple times could help identify areas where our contractors disagree, and thus where a single model is unlikely to align to all of them. In cases of disagreement, aligning to the average labeler preference may not be desirable. For example, when generating text that disproportionately affects a minority group, we may want the preferences of labelers belonging to that group to be weighted more heavily.</p>
<p>Models. Our models are neither fully aligned nor fully safe; they still generate toxic or biased outputs, make up facts, and generate sexual and violent content without explicit prompting. They can also fail to generate reasonable outputs on some inputs; we show some examples of this in Figure 9.</p>
<p>Perhaps the greatest limitation of our models is that, in most cases, they follow the user's instruction, even if that could lead to harm in the real world. For example, when given a prompt instructing the models to be maximally biased, InstructGPT generates more toxic outputs than equivalently-sized GPT-3 models. We discuss potential mitigations in the following sections.</p>
<h3>5.4 Open questions</h3>
<p>This work is a first step towards using alignment techniques to fine-tune language models to follow a wide range of instructions. There are many open questions to explore to further align language model behavior with what people actually want them to do.</p>
<p>Many methods could be tried to further decrease the models' propensity to generate toxic, biased, or otherwise harmful outputs. For example, one could use an adversarial set-up where labelers find the worst-case behaviors of the model, which are then labeled and added to the dataset (Dinan et al., 2019b). One could also combine our method with ways of filtering the pretraining data (Ngo et al., 2021), either for training the initial pretrained models, or for the data we use for our pretraining mix approach. Similarly, one could combine our approach with methods that improve models' truthfulness, such as WebGPT (Nakano et al., 2021).</p>
<p>In this work, if the user requests a potentially harmful or dishonest response, we allow our model to generate these outputs. Training our model to be harmless despite user instructions is important, but is also difficult because whether an output is harmful depends on the context in which it's deployed; for example, it may be beneficial to use language models to generate toxic outputs as part of a data augmentation pipeline. Our techniques can also be applied to making models refuse certain user instructions, and we plan to explore this in subsequent iterations of this research.</p>
<p>Getting models to do what we want is directly related to the steerability and controllability literature (Dathathri et al., 2019; Krause et al., 2020). A promising future path is combining RLHF with other methods of steerability, for example using control codes (Keskar et al., 2019), or modifying the sampling procedure at inference time using a smaller model (Dathathri et al., 2019).</p>
<p>While we mainly focus on RLHF, there are many other algorithms that could be used to train policies on our demonstration and comparison data to get even better results. For example, one could explore expert iteration (Anthony et al., 2017; Silver et al., 2017), or simpler behavior cloning methods that use a subset of the comparison data. One could also try constrained optimization approaches (Achiam et al., 2017) that maximize the score from a reward model conditioned on generating a small number of harmful behaviors.</p>
<p>Comparisons are also not necessarily the most efficient way of providing an alignment signal. For example, we could have labelers edit model responses to make them better, or generate critiques of model responses in natural language. There is also a vast space of options for designing interfaces for labelers to provide feedback to language models; this is an interesting human-computer interaction problem.</p>
<p>Our proposal for mitigating the alignment tax, by incorporating pretraining data into RLHF finetuning, does not completely mitigate performance regressions, and may make certain undesirable behaviors more likely for some tasks (if these behaviors are present in the pretraining data). This is an interesting area for further research. Another modification that would likely improve our method is to filter the pretraining mix data for toxic content (Ngo et al., 2021), or augment this data with synthetic instructions.</p>
<p>As discussed in detail in Gabriel (2020), there are subtle differences between aligning to instructions, intentions, revealed preferences, ideal preferences, interests, and values. Gabriel (2020) advocate for a principle-based approach to alignment: in other words, for identifying "fair principles for alignment that receive reflective endorsement despite widespread variation in people's moral beliefs." In our paper we align to the inferred user intention for simplicity, but more research is required in this area. Indeed, one of the biggest open questions is how to design an alignment process that is transparent, that meaningfully represents the people impacted by the technology, and that synthesizes peoples' values in a way that achieves broad consensus amongst many groups. We discuss some related considerations in Section 5.2.</p>
<h1>5.5 Broader impacts</h1>
<p>This work is motivated by our aim to increase the positive impact of large language models by training them to do what a given set of humans want them to do. By default, language models optimize the next word prediction objective, which is only a proxy for what we want these models to do. Our results indicate that our techniques hold promise for making language models more helpful, truthful, and harmless. In the longer term, alignment failures could lead to more severe consequences, particularly if these models are deployed in safety-critical situations. We expect that as model scaling continues, greater care has to be taken to ensure that they are aligned with human intentions (Bostrom, 2014).</p>
<p>However, making language models better at following user intentions also makes them easier to misuse. It may be easier to use these models to generate convincing misinformation, or hateful or abusive content.</p>
<p>Alignment techniques are not a panacea for resolving safety issues associated with large language models; rather, they should be used as one tool in a broader safety ecosystem. Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement. If these models are open-sourced, it becomes challenging to limit harmful applications in these and other domains without proper regulation. On the other hand, if large language model access is restricted to a few organizations with the resources required to train them, this excludes most people from access to cutting-edge ML technology. Another option is for an organization to own the end-to-end infrastructure of model deployment, and make it accessible via an API. This allows for the implementation of safety protocols like use case restriction (only allowing the model to be used for certain applications), monitoring for misuse and revoking access to those who misuse the system, and rate limiting to prevent the generation of large-scale misinformation. However, this can come at the cost of reduced transparency and increased centralization of power because it requires the API provider to make decisions on where to draw the line on each of these questions.</p>
<p>Finally, as discussed in Section 5.2, the question of who these models are aligned to is extremely important, and will significantly affect whether the net impact of these models is positive or negative.</p>
<h1>Acknowledgements</h1>
<p>First, we would like to thank Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam, Steven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadfield, Irene Soliaman, Christy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta, Jonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, and others at OpenAI for discussions throughout the course of the project that helped shape our research direction. We thank Brian Green, Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul Röttger for discussions and feedback on our approach. Finally, we thank Sam Bowman, Matthew Rahtz, Ben Mann, Liam Fedus, Helen Ngo, Josh Achiam, Leo Gao, Jared Kaplan, Cathy Yeh, Miles Brundage, Gillian Hadfield, Cooper Raterink, Gretchen Krueger, Tyna Eloundou, Rafal Jakubanis, and Steven Adler for providing feedback on this paper. We'd also like to thank Owain Evans and Stephanie Lin for pointing out the fact that the automatic TruthfulQA metrics were overstating the gains of our PPO models.
Thanks to those who contributed in various ways to the infrastructure used to train and deploy our models, including: Daniel Ziegler, William Saunders, Brooke Chan, Dave Cummings, Chris Hesse, Shantanu Jain, Michael Petrov, Greg Brockman, Felipe Such, Alethea Power, and the entire OpenAI supercomputing team. We'd also like to thank Suchir Balaji for help with recalibration, to Alper Ercetin and Justin Wang for designing the main diagram in this paper, and to the OpenAI Comms team for helping with the release, including: Steve Dowling, Hannah Wong, Natalie Summers, and Elie Georges.</p>
<p>Finally, we want to thank our labelers, without whom this work would not have been possible: Meave Fryer, Sara Tirmizi, James Carroll, Jian Ouyang, Michelle Brothers, Conor Agnew, Joe Kwon, John Morton, Emma Duncan, Delia Randolph, Kaylee Weeks, Alexej Savreux, Siam Ahsan, Rashed Sorwar, Atresha Singh, Muhaiminul Rukshat, Caroline Oliveira, Juan Pablo Castaño Rendón, Atqiya Abida Anjum, Tinashe Mapolisa, Celeste Fejzo, Caio Oleskovicz, Salahuddin Ahmed, Elena Green, Ben Harmelin, Vladan Djordjevic, Victoria Ebbets, Melissa Mejia, Emill Jayson Caypuno, Rachelle Froyalde, Russell M. Bernandez, Jennifer Brillo, Jacob Bryan, Carla Rodriguez, Evgeniya Rabinovich, Morris Stuttard, Rachelle Froyalde, Roxanne Addison, Sarah Nogly, Chait Singh.</p>
<h2>References</h2>
<p>Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et al. (2020). Imitating interactive intelligence. arXiv preprint arXiv:2012.05672.
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization. In International Conference on Machine Learning, pages 22-31. PMLR.
Anthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. arXiv preprint arXiv:1705.08439.
Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H. S., Mehta, S. V., Zhuang, H., Tran, V. Q., Bahri, D., Ni, J., et al. (2021). Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952.
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. (2021). A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y. (2016). An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086.</p>
<p>Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and Grefenstette, E. (2018). Learning to understand goal specifications by modelling reward. arXiv preprint arXiv:1806.01946.
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623.
Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. arXiv preprint arXiv:2005.14050.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Note that while fine-tuning models using human data is common practice when deploying ML systems, the purpose of these efforts is to obtain a model that performs well on a company's specific use case, rather than advancing the alignment of general-purpose ML models.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2368 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2368</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2368</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-5d87d55f8bf58f574324f42f4a3a8826f59383e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5d87d55f8bf58f574324f42f4a3a8826f59383e8" target="_blank">Auto-Keras: An Efficient Neural Architecture Search System</a></p>
                <p><strong>Paper Venue:</strong> Knowledge Discovery and Data Mining</p>
                <p><strong>Paper TL;DR:</strong> A novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search is proposed and an open-source AutoML system based on the developed framework is built, namely Auto-Keras.</p>
                <p><strong>Paper Abstract:</strong> Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2368",
    "paper_id": "paper-5d87d55f8bf58f574324f42f4a3a8826f59383e8",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.006595,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Auto-Keras: An Efficient Neural Architecture Search System</h1>
<p>Haifeng Jin, Qingquan Song, Xia Hu<br>Department of Computer Science and Engineering, Texas A\&amp;M University<br>{jin, song_3134,xiahu}@tamu.edu</p>
<h4>Abstract</h4>
<p>Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet [51], PNAS [29], usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an opensource AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Computing methodologies $\rightarrow$ Neural networks; Supervised learning; Discrete space search.</li>
</ul>
<h2>KEYWORDS</h2>
<p>Automated Machine Learning, AutoML, Neural Architecture Search, Bayesian Optimization, Network Morphism</p>
<h2>ACM Reference Format:</h2>
<p>Haifeng Jin, Qingquan Song, Xia Hu. 2019. Auto-Keras: An Efficient Neural Architecture Search System. In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '19), August 4-8, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3292500. 3330648</p>
<h2>1 INTRODUCTION</h2>
<p>Automated Machine Learning (AutoML) has become a very important research topic with wide applications of machine learning techniques. The goal of AutoML is to enable people with limited machine learning background knowledge to use machine learning models easily. Work has been done on automated model selection, automated hyperparameter tunning, and etc. In the context of deep</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>learning, neural architecture search (NAS), which aims to search for the best neural network architecture for the given learning task and dataset, has become an effective computational tool in AutoML. Unfortunately, existing NAS algorithms are usually computationally expensive. The time complexity of NAS is $O(n \bar{t})$, where $n$ is the number of neural architectures evaluated during the search, and $\bar{t}$ is the average time consumption for evaluating each of the $n$ neural networks. Many NAS approaches, such as deep reinforcement learning $[2,37,47,50,51]$, gradient-based methods $[8,31,33]$ and evolutionary algorithms $[12,17,30,38,39,41]$, require a large $n$ to reach a good performance. Moreover, many of them train each of the $n$ neural networks from scratch, which is very slow.</p>
<p>Initial efforts have been devoted to making use of network morphism in neural architecture search [7, 13]. It is a technique to morph the architecture of a neural network but keep its functionality [10, 45]. Therefore, we are able to modify a trained neural network into a new architecture using the network morphism operations, e.g., inserting a layer or adding a skip-connection. Only a few more epochs are required to further train the new architecture towards better performance. Using network morphism would reduce the average training time $\bar{t}$ in neural architecture search. The most important problem to solve for network morphism-based NAS methods is the selection of operations, which is to select an operation from the network morphism operation set to morph an existing architecture to a new one. The network morphism-based NAS methods are not efficient enough. They either require a large number of training examples [7], or inefficient in exploring the large search space [13]. How to perform efficient neural architecture search with network morphism remains a challenging problem.</p>
<p>As we know, Bayesian optimization [40] has been widely adopted to efficiently explore black-box functions for global optimization, whose observations are expensive to obtain. For example, it has been used in hyperparameter tuning for machine learning models $[3,15,21,24,40,44]$, in which Bayesian optimization searches among different combinations of hyperparameters. During the search, each evaluation of a combination of hyperparameters involves an expensive process of training and testing the machine learning model, which is very similar to the NAS problem. The unique properties of Bayesian optimization motivate us to explore its capability in guiding the network morphism to reduce the number of trained neural networks $n$ to make the search more efficient.</p>
<p>It is non-trivial to design a Bayesian optimization method for network morphism-based NAS due to the following challenges. First, the underlying Gaussian process (GP) is traditionally used for learning probability distribution of functions in Euclidean space. To update the Bayesian optimization model with observations, the underlying GP is to be trained with the searched architectures and their performances. However, the neural network architectures are not in Euclidean space and hard to parameterize into a fixed-length vector. Second, an acquisition function needs to be optimized for</p>
<p>Bayesian optimization to generate the next architecture to observe. However, in the context of network morphism, it is not to maximize a function in Euclidean space, but finding a node in a tree-structured search space, where each node represents a neural architecture and each edge is a morph operation. Thus traditional gradient-based methods cannot be simply applied. Third, the changes caused by a network morphism operation is complicated. A network morphism operation on one layer may change the shapes of some intermediate output tensors, which no longer match input shape requirements of the layers taking them as input. How to maintain such consistency is a challenging problem.</p>
<p>In this paper, an efficient neural architecture search with network morphism is proposed, which utilizes Bayesian optimization to guide through the search space by selecting the most promising operations each time. To tackle the aforementioned challenges, an edit-distance neural network kernel is constructed. Being consistent with the key idea of network morphism, it measures how many operations are needed to change one neural network to another. Besides, a novel acquisition function optimizer, which is capable of balancing between the exploration and exploitation, is designed specially for the tree-structure search space to enable Bayesian optimization to select from the operations. In addition, a graph-level network morphism is defined to address the changes in the neural architectures based on layer-level network morphism. The proposed approach is compared with the state-of-the-art NAS methods [13, 22] on benchmark datasets of MNIST, CIFAR10, and FASHIONMNIST. Within a limited search time, the architectures found by our method achieves the lowest error rates on all of the datasets.</p>
<p>In addition, we have developed a widely adopted open-source AutoML system based on our proposed method, namely Auto-Keras. It is an open-source AutoML system, which can be download and installed locally. The system is carefully designed with a concise interface for people not specialized in computer programming and data science to use. To speed up the search, the workload on CPU and GPU can run in parallel. To address the issue of different GPU memory, which limits the size of the neural architectures, a memory adaption strategy is designed for deployment.</p>
<p>The main contributions of the paper are as follows:</p>
<ul>
<li>Propose an algorithm for efficient neural architecture search based on network morphism guided by Bayesian optimization.</li>
<li>Conduct intensive experiments on benchmark datasets to demonstrate the superior performance of the proposed method over the baseline methods.</li>
<li>Develop an open-source system, namely Auto-Keras, which is one of the most widely used AutoML systems.</li>
</ul>
<h2>2 PROBLEM STATEMENT</h2>
<p>The general neural architecture search problem we studied in this paper is defined as: Given a neural architecture search space $\mathcal{F}$, the input data $D$ divided into $D_{\text {train }}$ and $D_{\text {val }}$, and the cost function $\operatorname{Cost}(\cdot)$, we aim at finding an optimal neural network $f^{<em>} \in \mathcal{F}$, which could achieve the lowest cost on dataset $D$. The definition is equivalent to finding $f^{</em>}$ satisfying:</p>
<p>$$
f^{<em>}=\underset{f \in \mathcal{F}}{\operatorname{argmin}} \operatorname{Cost}\left(f\left(\boldsymbol{\theta}^{</em>}\right), D_{\text {val }}\right)
$$</p>
<p>$$
\boldsymbol{\theta}^{*}=\underset{\boldsymbol{\theta}}{\operatorname{argmin}} \mathcal{L}\left(f(\boldsymbol{\theta}), D_{\text {train }}\right)
$$</p>
<p>where $\operatorname{Cost}(\cdot, \cdot)$ is the evaluation metric function, e.g., accuracy, mean squared error, $\boldsymbol{\theta}^{*}$ is the learned parameter of $f$.</p>
<p>The search space $\mathcal{F}$ covers all the neural architectures, which can be morphed from the initial architectures. The details of the morph operations are introduced in 3.3. Notably, the operations can change the number of filters in a convolutional layer, which makes $\mathcal{F}$ larger than methods with fixed layer width [31].</p>
<h2>3 NETWORK MORPHISM GUIDED BY BAYESIAN OPTIMIZATION</h2>
<p>The key idea of the proposed method is to explore the search space via morphing the neural architectures guided by Bayesian optimization (BO) algorithm. Traditional Bayesian optimization consists of a loop of three steps: update, generation, and observation. In the context of NAS, our proposed Bayesian optimization algorithm iteratively conducts: (1) Update: train the underlying Gaussian process model with the existing architectures and their performances; (2) Generation: generate the next architecture to observe by optimizing a delicately defined acquisition function; (3) Observation: obtain the actual performance by training the generated neural architecture. There are three main challenges in designing a method for morphing the neural architectures with Bayesian optimization. We introduce three key components separately in the subsequent sections coping with the three challenges.</p>
<h3>3.1 Edit-Distance Neural Network Kernel for Gaussian Process</h3>
<p>The first challenge we need to address is that the NAS space is not a Euclidean space, which does not satisfy the assumption of traditional Gaussian process (GP). Directly vectorizing the neural architecture is impractical due to the uncertain number of layers and parameters it may contain. Since the Gaussian process is a kernel method, instead of vectorizing a neural architecture, we propose to tackle the challenge by designing a neural network kernel function. The intuition behind the kernel function is the editdistance for morphing one neural architecture to another. More edits needed from one architecture to another means the further distance between them, thus less similar they are. The proof of the validity of the kernel function is presented in Appendix E.</p>
<p>Kernel Definition: Suppose $f_{a}$ and $f_{b}$ are two neural networks. Inspired by Deep Graph Kernels [48], we propose an edit-distance kernel for neural networks. Edit-distance here means how many operations are needed to morph one neural network to another. The concrete kernel function is defined as:</p>
<p>$$
\kappa\left(f_{a}, f_{b}\right)=e^{-\rho^{2}\left(d\left(f_{a}, f_{b}\right)\right)}
$$</p>
<p>where function $d(\cdot, \cdot)$ denotes the edit-distance of two neural networks, whose range is $[0,+\infty), \rho$ is a mapping function, which maps the distance in the original metric space to the corresponding distance in the new space. The new space is constructed by embedding the original metric space into a new one using Bourgain Theorem [4], which ensures the validity of the kernel.</p>
<p>Calculating the edit-distance of two neural networks can be mapped to calculating the edit-distance of two graphs, which is</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Neural Network Kernel. Given two neural networks $f_{a}, f_{b}$, and matchings between the similar layers, the figure shows how the layers of $f_{a}$ can be changed to the same as $f_{b}$. Similarly, the skip-connections in $f_{a}$ also need to be changed to the same as $f_{b}$ according to a given matching.
an NP-hard problem [49]. Based on the search space $\mathcal{F}$ defined in Section 2, we tackle the problem by proposing an approximated solution as follows:</p>
<p>$$
d\left(f_{a}, f_{b}\right)=D_{l}\left(L_{a}, L_{b}\right)+\lambda D_{s}\left(S_{a}, S_{b}\right)
$$</p>
<p>where $D_{l}$ denotes the edit-distance for morphing the layers, i.e., the minimum edits needed to morph $f_{a}$ to $f_{b}$ if the skip-connections are ignored, $L_{a}=\left{l_{a}^{(1)}, l_{a}^{(2)}, \ldots\right}$ and $L_{b}=\left{l_{b}^{(1)}, l_{b}^{(2)}, \ldots\right}$ are the layer sets of neural networks $f_{a}$ and $f_{b}, D_{s}$ is the approximated edit-distance for morphing skip-connections between two neural networks, $S_{a}=\left{s_{a}^{(1)}, s_{a}^{(2)}, \ldots\right}$ and $S_{b}=\left{s_{b}^{(1)}, s_{b}^{(2)}, \ldots\right}$ are the skipconnection sets of neural network $f_{a}$ and $f_{b}$, and $\lambda$ is the balancing factor between the distance of the layers and the skip-connections.</p>
<p>Calculating $D_{l}$ : We assume $\left|L_{a}\right|&lt;\left|L_{b}\right|$, the edit-distance for morphing the layers of two neural architectures $f_{a}$ and $f_{b}$ is calculated by minimizing the follow equation:</p>
<p>$$
D_{l}\left(L_{a}, L_{b}\right)=\min \sum_{i=1}^{\left|L_{a}\right|} d_{l}\left(l_{a}^{(i)}, \varphi_{l}\left(l_{a}^{(i)}\right)\right)+\left|\left|L_{b}\right|-\left|L_{a}\right|\right|
$$</p>
<p>where $\varphi_{l}: L_{a} \rightarrow L_{b}$ is an injective matching function of layers satisfying: $\forall i&lt;j, \varphi_{l}\left(l_{a}^{(i)}\right)&lt;\varphi_{l}\left(l_{a}^{(j)}\right)$ if layers in $L_{a}$ and $L_{b}$ are all sorted in topological order. $d_{l}(\cdot, \cdot)$ denotes the edit-distance of widening a layer into another defined in Equation (6),</p>
<p>$$
d_{l}\left(l_{a}, l_{b}\right)=\frac{\left|w\left(l_{a}\right)-w\left(l_{b}\right)\right|}{\max \left[w\left(l_{a}\right), w\left(l_{b}\right)\right]}
$$</p>
<p>where $w(l)$ is the width of layer $l$.
The intuition of Equation (5) is consistent with the idea of network morphism shown in Figure 1. Suppose a matching is provided between the nodes in two neural networks. The sizes of the tensors are indicators of the width of the previous layers (e.g., the output vector length of a fully-connected layer or the number of filters of a convolutional layer). The matchings between the nodes are marked by light blue. So a matching between the nodes can be seen as matching between the layers. To morph $f_{a}$ to $f_{b}$ with the given matching, we need to first widen the three nodes in $f_{a}$ to the same width as their matched nodes in $f_{b}$, and then insert a new node of width 20 after the first node in $f_{a}$. Based on this morphing scheme, the edit-distance of the layers is defined as $D_{l}$ in Equation (5).</p>
<p>Since there are many ways to morph $f_{a}$ to $f_{b}$, to find the best matching between the nodes that minimizes $D_{l}$, we propose a dynamic programming approach by defining a matrix $\boldsymbol{A}<em a="a">{\left|L</em>$, which is recursively calculated as follows:}\right| \times\left|L_{b}\right|</p>
<p>$$
\boldsymbol{A}<em i-1_="i-1," j="j">{i, j}=\max \left[\boldsymbol{A}</em>}+1, \boldsymbol{A<em i-1_="i-1," j-1="j-1">{i, j-1}+1, \boldsymbol{A}</em>\right)\right]
$$}+d_{l}\left(l_{a}^{(i)}, l_{b}^{(j)</p>
<p>where $\boldsymbol{A}<em l="l">{i, j}$ is the minimum value of $D</em>\right}$.}\left(L_{a}^{(i)}, L_{b}^{(j)}\right)$, where $L_{a}^{(i)}=$ $\left{l_{a}^{(1)}, l_{a}^{(2)}, \ldots, l_{a}^{(i)}\right}$ and $L_{b}^{(j)}=\left{l_{b}^{(1)}, l_{b}^{(2)}, \ldots, l_{b}^{(j)</p>
<p>Calculating $D_{s}$ : The intuition of $D_{s}$ is the sum of the the editdistances of the matched skip-connections in two neural networks into pairs. As shown in Figure 1, the skip-connections with the same color are matched pairs. Similar to $D_{l}(\cdot, \cdot), D_{s}(\cdot, \cdot)$ is defined as follows:</p>
<p>$$
D_{s}\left(S_{a}, S_{b}\right)=\min \sum_{i=1}^{\left|S_{a}\right|} d_{s}\left(s_{a}^{(i)}, \varphi_{s}\left(s_{a}^{(i)}\right)\right)+\left|\left|S_{b}\right|-\left|S_{a}\right|\right|
$$</p>
<p>where we assume $\left|S_{a}\right|&lt;\left|S_{b}\right| \cdot\left(\left|S_{b}\right|-\left|S_{a}\right|\right)$ measures the total editdistance for non-matched skip-connections since each of the nonmatched skip-connections in $S_{b}$ calls for an edit of inserting a new skip connection into $f_{a}$. The mapping function $\varphi_{s}: S_{a} \rightarrow S_{b}$ is an injective function. $d_{s}(\cdot, \cdot)$ is the edit-distance for two matched skip-connections defined as:</p>
<p>$$
d_{s}\left(s_{a}, s_{b}\right)=\frac{\left|u\left(s_{a}\right)-u\left(s_{b}\right)\right|+\left|\delta\left(s_{a}\right)-\delta\left(s_{b}\right)\right|}{\max \left[u\left(s_{a}\right), u\left(s_{b}\right)\right]+\max \left[\delta\left(s_{a}\right), \delta\left(s_{b}\right)\right]}
$$</p>
<p>where $u(s)$ is the topological rank of the layer the skip-connection $s$ started from, $\delta(s)$ is the number of layers between the start and end point of the skip-connection $s$.</p>
<p>This minimization problem in Equation (8) can be mapped to a bipartite graph matching problem, where $f_{a}$ and $f_{b}$ are the two disjoint sets of the graph, each skip-connection is a node in its corresponding set. The edit-distance between two skip-connections is the weight of the edge between them. The weighted bipartite graph matching problem is solved by the Hungarian algorithm (Kuhn-Munkres algorithm) [26].</p>
<h3>3.2 Optimization for Tree Structured Space</h3>
<p>The second challenge of using Bayesian optimization to guide network morphism is the optimization of the acquisition function. The</p>
<p>traditional acquisition functions are defined in Euclidean space. The optimization methods are not applicable to the tree-structured search via network morphism. To optimize our acquisition function, we need a method to efficiently optimize the acquisition function in the tree-structured space. To deal with this problem, we propose a novel method to optimize the acquisition function on tree-structured space.</p>
<p>Upper-confidence bound (UCB) [1] is selected as our acquisition function, which is defined as:</p>
<p>$$
\alpha(f)=\mu\left(y_{f}\right)-\beta \sigma\left(y_{f}\right)
$$</p>
<p>where $y_{f}=\operatorname{Cost}(f, D), \beta$ is the balancing factor, $\mu\left(y_{f}\right)$ and $\sigma\left(y_{f}\right)$ are the posterior mean and standard deviation of variable $y_{f}$ predicted by the Gaussian process. It has two important properties, which fit our problem. First, it has an explicit balance factor $\beta$ for exploration and exploitation. Second, $\alpha(f)$ is directly comparable with the cost function value $c^{(i)}$ in search history $\mathcal{H}=\left{\left(f^{(i)}, \theta^{(i)}, c^{(i)}\right)\right}$. The UCB estimates the lowest possible cost given the neural network $f . \hat{f}=\operatorname{argmin}_{f} \alpha(f)$ is the generated neural architecture for next observation.</p>
<p>The tree-structured space is defined as follows. During the optimization of $\alpha(f), \hat{f}$ should be obtained from $f^{(i)}$ and $O$, where $f^{(i)}$ is an observed architecture in the search history $\mathcal{H}, O$ is a sequence of operations to morph the architecture into a new one. Morph $f$ to $\hat{f}$ with $O$ is denoted as $\hat{f} \leftarrow \mathcal{M}(f, O)$, where $\mathcal{M}(\cdot, \cdot)$ is the function to morph $f$ with the operations in $O$. Therefore, the search can be viewed as a tree-structured search, where each node is a neural architecture, whose children are morphed from it by network morphism operations.</p>
<p>The most common defect of network morphism is it only grows the size of the architecture instead of shrinking them. Using network morphism for NAS may end up with a very large architecture without enough exploration on the smaller architectures. However, in our tree-structure search, we not only expand the leaves but also the inner nodes, which means the smaller architectures found in the early stage can be selected multiple times to morph to more comparatively small architectures.</p>
<p>Inspired by various heuristic search algorithms for exploring the tree-structured search space and optimization methods balancing between exploration and exploitation, a new method based on A<em> search [19] and simulated annealing [23] is proposed. A</em> algorithm is widely used for tree-structure search. It maintains a priority queue of nodes and keeps expanding the best node in the queue. Since A* always exploits the best node, simulated annealing is introduced to balance the exploration and exploitation by not selecting the estimated best architecture with a probability.</p>
<p>As shown in Algorithm 1, the algorithm takes minimum temperature $T_{l o w}$, temperature decreasing rate $r$ for simulated annealing, and search history $\mathcal{H}$ described in Section 2 as the input. It outputs a neural architecture $f \in \mathcal{H}$ and a sequence of operations $O$ to morph $f$ into the new architecture. From line 2 to 6 , the searched architectures are pushed into the priority queue, which sorts the elements according to the cost function value or the acquisition function value. Since UCB is chosen as the acquisiton function, $\alpha(f)$ is directly comparable with the history observation values $c^{(i)}$. From line 7 to 18, it is the loop optimizing the acquisition function.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Optimize</span><span class="w"> </span><span class="n">Acquisition</span><span class="w"> </span><span class="n">Function</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">H</span><span class="p">},</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">T_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">low</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">T</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">PriorityQueue</span><span class="p">()</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">c_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="nb">min</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">lowest</span><span class="w"> </span>\<span class="p">(</span><span class="n">c</span>\<span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">H</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span>\<span class="n">theta_</span><span class="p">{</span><span class="n">f</span><span class="p">},</span><span class="w"> </span><span class="n">c</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">H</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Push</span><span class="p">}(</span><span class="n">f</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span>\<span class="p">(</span><span class="n">Q</span><span class="w"> </span>\<span class="n">neq</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span>\<span class="p">(</span><span class="n">T</span><span class="o">&gt;</span><span class="n">T_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">low</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">T</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">T</span><span class="w"> </span>\<span class="n">times</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="n">P</span><span class="w"> </span><span class="n">o</span><span class="w"> </span><span class="n">p</span><span class="p">()</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">o</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">Omega</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">M</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">f</span><span class="p">,</span>\<span class="p">{</span><span class="n">o</span>\<span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">e</span><span class="o">^</span><span class="p">{</span>\<span class="n">frac</span><span class="p">{</span><span class="n">r_</span><span class="p">{</span>\<span class="nb">min</span><span class="w"> </span><span class="p">}</span><span class="o">-</span>\<span class="n">alpha</span>\<span class="n">left</span><span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="n">right</span><span class="p">)}{</span><span class="n">T</span><span class="p">}}</span><span class="o">&gt;</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Rand</span><span class="p">}()</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span>\<span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Push</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">            </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">c_</span><span class="p">{</span>\<span class="nb">min</span><span class="w"> </span><span class="p">}</span><span class="o">&gt;</span>\<span class="n">alpha</span>\<span class="n">left</span><span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span>\<span class="p">(</span><span class="n">c_</span><span class="p">{</span>\<span class="nb">min</span><span class="w"> </span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">alpha</span>\<span class="n">left</span><span class="p">(</span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="n">right</span><span class="p">),</span><span class="w"> </span><span class="n">f_</span><span class="p">{</span>\<span class="nb">min</span><span class="w"> </span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">f</span><span class="o">^</span><span class="p">{</span>\<span class="n">prime</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">            </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="n">Return</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">nearest</span><span class="w"> </span><span class="n">ancestor</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">f_</span><span class="p">{</span>\<span class="nb">min</span><span class="w"> </span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">H</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">operation</span><span class="w"> </span><span class="n">se</span><span class="o">-</span>
<span class="w">    </span><span class="n">quence</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">reach</span><span class="w"> </span>\<span class="p">(</span><span class="n">f_</span><span class="p">{</span>\<span class="nb">min</span><span class="w"> </span><span class="p">}</span>\<span class="p">)</span>
</code></pre></div>

<p>Following the setting in A* search, in each iteration, the architecture with the lowest acquisition function value is popped out to be expanded on line 8 to 10 , where $\Omega(f)$ is all the possible operations to morph the architecture $f, \mathcal{M}(f, o)$ is the function to morph the architecture $f$ with the operation sequence $o$. However, not all the children are pushed into the priority queue for exploration purpose. The decision of whether it is pushed into the queue is made by simulated annealing on line 11 , where $e^{\frac{r_{\min }-\alpha\left(f^{\prime}\right)}{T}}$ is a typical acceptance function in simulated annealing. $c_{\text {min }}$ and $f_{\text {min }}$ are updated from line 14 to 16 , which record the minimum acquisition function value and the corresponding architecture.</p>
<h3>3.3 Graph-Level Network Morphism</h3>
<p>The third challenge is to maintain the intermediate output tensor shape consistency when morphing the architectures. Previous work showed how to preserve the functionality of the layers the operators applied on, namely layer-level morphism. However, from a graphlevel view, any change of a single layer could have a butterfly effect on the entire network. Otherwise, it would break the input and output tensor shape consistency. To tackle the challenge, a graphlevel morphism is proposed to find and morph the layers influenced by a layer-level operation in the entire network.</p>
<p>Follow the four network morphism operations on a neural network $f \in \mathcal{F}$ defined in [13], which can all be reflected in the change of the computational graph $G$. The first operation is inserting a layer to $f$ to make it deeper denoted as deep $(G, u)$, where $u$ is the node marking the place to insert the layer. The second one is widening a node in $f$ denoted as $\operatorname{wide}(G, u)$, where $u$ is the node representing the intermediate output tensor to be widened. Widen here could be either making the output vector of the previous fullyconnected layer of $u$ longer, or adding more filters to the previous convolutional layer of $u$, depending on the type of the previous</p>
<p>layer. The third is adding an additive connection from node $u$ to node $v$ denoted as $\operatorname{add}(G, u, v)$. The fourth is adding an concatenative connection from node $u$ to node $v$ denoted as concat $(G, u, v)$. For $\operatorname{deep}(G, u)$, no other operation is needed except for initializing the weights of the newly added layer. However, for all other three operations, more changes are required to $G$.</p>
<p>First, we define an effective area of $\operatorname{wide}\left(G, u_{0}\right)$ as $\gamma$ to better describe where to change in the network. The effective area is a set of nodes in the computational graph, which can be recursively defined by the following rules: 1. $u_{0} \in \gamma$. 2. $v \in \gamma$, if $\exists e_{u \rightarrow v} \notin L_{s}$, $u \in \gamma .3 . v \in \gamma$, if $\exists e_{v \rightarrow u} \notin L_{s}, u \in \gamma . L_{s}$ is the set of fully-connected layers and convolutional layers. Operation wide $\left(G, u_{0}\right)$ needs to change two set of layers, the previous layer set $L_{p}=\left{e_{u \rightarrow v} \in\right.$ $\left.L_{s} \mid v \in \gamma\right}$, which needs to output a wider tensor, and next layer set $L_{n}=\left{e_{u \rightarrow v} \in L_{s} \mid u \in \gamma\right}$, which needs to input a wider tensor. Second, for operator $\operatorname{add}\left(G, u_{0}, v_{0}\right)$, additional pooling layers may be needed on the skip-connection. $u_{0}$ and $v_{0}$ have the same number of channels, but their shape may differ because of the pooling layers between them. So we need a set of pooling layers whose effect is the same as the combination of all the pooling layers between $u_{0}$ and $v_{0}$, which is defined as $L_{o}=\left{e \in L_{\text {pool }} \mid e \in p_{u_{0} \rightarrow v_{0}}\right}$. where $p_{u_{0} \rightarrow v_{0}}$ could be any path between $u_{0}$ and $v_{0}, L_{\text {pool }}$ is the pooling layer set. Another layer $L_{c}$ is used after to pooling layers to process $u_{0}$ to the same width as $v_{0}$. Third, in concat $\left(G, u_{0}, v_{0}\right)$, the concatenated tensor is wider than the original tensor $v_{0}$. The concatenated tensor is input to a new layer $L_{c}$ to reduce the width back to the same width as $v_{0}$. Additional pooling layers are also needed for the concatenative connection.</p>
<h3>3.4 Time Complexity Analysis</h3>
<p>As described at the start of Section 3, Bayesian optimization can be roughly divided into three steps: update, generation, and observation. The bottleneck of the algorithm efficiency is observation, which involves the training of the generated neural architecture. Let $n$ be the number of architectures in the search history. The time complexity of the update is $O\left(n^{2} \log <em 2="2">{2} n\right)$. In each generation, the kernel is computed between the new architectures during optimizing acquisition function and the ones in the search history, the number of values in which is $O(n m)$, where $m$ is the number of architectures computed during the optimization of the acquisition function. The time complexity for computing $d(\cdot, \cdot)$ once is $O\left(l^{2}+s^{3}\right)$, where $l$ and $s$ are the number of layers and skip-connections. So the overall time complexity is $O\left(n m\left(l^{2}+s^{3}\right)+n^{2} \log </em> n\right)$. The magnitude of these factors is within the scope of tens. So the time consumption of update and generation is trivial comparing to the observation.</p>
<h2>4 AUTO-KERAS</h2>
<p>Based on the proposed neural architecture search method, we developed an open-source AutoML system, namely Auto-Keras. It is named after Keras [11], which is known for its simplicity in creating neural networks. Similar to SMAC [21], TPOT [35], AutoWEKA [44], and Auto-Sklearn [15], the goal is to enable domain experts who are not familiar with machine learning technologies to use machine learning techniques easily. However, Auto-Keras is focusing on the deep learning tasks, which is different from the systems focusing on the shallow models mentioned above.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Auto-Keras System Overview. (1) The user calls the API. (2) The Searcher generates neural architectures on CPU. (3) Graph builds real neural networks with parameters on RAM from the neural architectures. (4) The neural network is copied to GPU for training. (5) Trained neural networks are saved on storage devices. The Searcher is updated based on the training results. Step (2) to (5) will repeat until it reaches the time limit.</p>
<p>Although, there are several AutoML services available on large cloud computing platforms, three things are prohibiting the users from using them. First, cloud services are not free to use, which may not be affordable for everyone who wants to use AutoML techniques. Second, the cloud-based AutoML usually requires complicated configurations of Docker containers and Kubernetes, which is not easy for people without a rich computer science background. Third, the AutoML service providers are honest-but-curious [9], which cannot guarantee the security and privacy of the data. An open-source software, which is easily downloadable and runs locally, would solve these problems and make the AutoML accessible to everyone. To bridge the gap, we developed Auto-Keras.</p>
<p>It is challenging, to design an easy-to-use and locally deployable system. First, we need a concise and configurable application programming interface (API). For the users who don't have rich experience in programming, they could easily learn how to use the API. For the advanced users, they can still configure the details of the system to meet their requirements. Second, local computation resources may be limited. We need to make full use of the local computation resources to speed up the search. Third, the available GPU memory may be of different sizes in different environments. We need to adapt the neural architecture sizes to the GPU memory during the search.</p>
<h3>4.1 System Overview</h3>
<p>The system architecture of Auto-Keras is shown in Figure 2. We design this architecture to fully make use of the computational resource of both CPU and GPU, and utilize the memory efficiently by only placing the currently useful information on the RAM, and save the rest on the storage devices, e.g., hard drives. The top part</p>
<p>is the API, which is directly called by the users. It is responsible for calling corresponding middle-level modules to complete certain functionalities. The Searcher is the module of the neural architecture search algorithm containing Bayesian Optimizer and Gaussian Process. These search algorithms run on CPU. The Model Trainer is a module responsible for the computation on GPUs. It trains given neural networks with the training data in a separate process for parallelism. The Graph is the module processing the computational graphs of neural networks, which is controlled by the Searcher for the network morphism operations. The current neural architecture in the Graph is placed on RAM for faster access. The Model Storage is a pool of trained models. Since the size of the neural networks are large and cannot be stored all in memory, the model storage saves all the trained models on the storage devices.</p>
<p>A typical workflow for the Auto-Keras system is as follows. The user initiated a search for the best neural architecture for the dataset. The API received the call, preprocess the dataset, and pass it to the Searcher to start the search. The Bayesian Optimizer in the Searcher would generate a new architecture using CPU. It calls the Graph module to build the generated neural architecture into a real neural network in the RAM. The new neural architecture is copied the GPU for the Model Trainer to train with the dataset. The trained model is saved in the Model Storage. The performance of the model is feedback to the Searcher to update the Gaussian Process.</p>
<h3>4.2 Application Programming Interface</h3>
<p>The design of the API follows the classic design of the Scikit-Learn API [6, 36], which is concise and configurable. The training of a neural network requires as few as three lines of code calling the constructor, the fit and predict function respectively. To accommodate the needs of different users, we designed two levels of APIs. The first level is named as task-level. The users only need to know their task, e.g., Image Classification, Text Regression, to use the API. The second level is named search-level, which is for advanced users. The user can search for a specific type of neural network architectures, e.g., multi-layer perceptron, convolutional neural network. To use this API, they need to preprocess the dataset by themselves and know which type of neural network, e.g., CNN or MLP, is the best for their task.</p>
<p>Several accommodations have been implemented to enhance the user experience with the Auto-Keras package. First, the user can restore and continue a previous search which might be accidentally killed. From the users' perspective, the main difference of using Auto-Keras comparing with the AutoML systems aiming at shallow models is the much longer time consumption, since a number of deep neural networks are trained during the neural architecture search. It is possible for some accident to happen to kill the process before the search finishes. Therefore, the search outputs all the searched neural network architectures with their trained parameters into a specific directory on the disk. As long as the path to the directory is provided, the previous search can be restored. Second, the user can export the search results, which are neural architectures, as saved Keras models for other usages. Third, for advanced users, they can specify all kinds of hyperparameters of the search process and neural network optimization process by the default parameters in the interface.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: CPU and GPU Parallelism. The Searcher obtains the next neural architecture to be trained and starts the training on GPU in a separate process. Then, instead of waiting for the training to finish, it directly starts to generate the next neural architecture on CPU.</p>
<h3>4.3 CPU and GPU Parallelism</h3>
<p>To make full use of the limited local computation resources, the program can run in parallel on the GPU and the CPU at the same time. If we do the observation (training of the current neural network), update, and generation of Bayesian optimization in sequential order. The GPUs will be idle during the update and generation. The CPUs will be idle during the observation. To improve efficiency, the observation is run in parallel with the generation in separated processes. A training queue is maintained as a buffer for the Model Trainer. Figure 3 shows the Sequence diagram of the parallelism between the CPU and the GPU. First, the Searcher requests the queue to pop out a new graph and pass it to GPU to start training. Second, while the GPU is busy, the searcher requests the CPU to generate a new graph. At this time period, the GPU and the CPU work in parallel. Third, the CPU returns the generated graph to the searcher, who pushes the graph into the queue. Finally, the Model Trainer finished training the graph on the GPU and returns it to the Searcher to update the Gaussian process. In this way, the idle time of GPU and CPU are dramatically reduced to improve the efficiency of the search process.</p>
<h3>4.4 GPU Memory Adaption</h3>
<p>The size of the neural networks needs to be limited according to the GPU memory. Otherwise, the system would crash because of running out of GPU memory. Many approaches have been taken to search for memory-efficient neural architectures [42]. In AutoKeras, we implement a memory estimation function on our own data structure for the neural architectures. An integer value is used to mark the upper bound of the neural architecture size. Any new computational graph whose estimated size exceeds the upper bound is discarded. However, the system may still crash because the management of the GPU memory is very complicated, which cannot be precisely estimated. So whenever it runs out of GPU memory, the upper bound is lowered down to further limit the size of the generated neural networks.</p>
<h2>5 EXPERIMENTS</h2>
<p>In the experiments, we aim at answering the following questions. 1) How effective is the search algorithm with limited running time? 2) How much efficiency is gained from Bayesian optimization and network morphism? 3) Does the proposed kernel function correctly measure the similarity among neural networks in terms of their actual performance?</p>
<p>Datasets Three benchmark datasets, MNIST [27], CIFAR10 [25], and FASHION [46] are used in the experiments to evaluate our method. They prefer very different neural architectures to achieve good performance.</p>
<p>Baselines Four categories of baseline methods are used for comparison, which are elaborated as follows:</p>
<ul>
<li>Straightforward Methods: random search (RAND) and grid search (GRID). They search the number of convolutional layers and the width of those layers.</li>
<li>Conventional Methods: SPMT [40] and SMAC [21]. Both SPMT and SMAC are designed for general hyperparameters tuning tasks of machine learning models instead of focusing on the deep neural networks. They tune the 16 hyperparameters of a threelayer convolutional neural network, including the width, dropout rate, and regularization rate of each layer.</li>
<li>State-of-the-art Methods: SEAS [13], NASBOT [22]. We carefully implemented the SEAS as described in their paper. For NASBOT, since the experimental settings are very similar, we directly trained their searched neural architecture in the paper. They did not search architectures for MNIST and FASHION dataset, so the results are omitted in our experiments.</li>
<li>Variants of the proposed method: BFS and BO. Our proposed method is denoted as AK. BFS replaces the Bayesian optimization in AK with the breadth-first search. BO is another variant, which does not employ network morphism to speed up the training. For AK, $\beta$ is set to 2.5 , while $\lambda$ is set to 1 according to the parameter sensitivity analysis.</li>
</ul>
<p>In addition, the performance of the deployed system of Auto-Keras (AK-DP) is also evaluated in the experiments. The difference from the AK above is that AK-DP uses various advanced techniques to improve the performance including learning rate scheduling, multiple manually defined initial architectures.</p>
<p>Experimental Setting The general experimental setting for evaluation is described as follows: First, the original training data of each dataset is further divided into training and validation sets by 80-20. Second, the testing data of each dataset is used as the testing set. Third, the initial architecture for SEAS, BO, BFS, and AK is a three-layer convolutional neural network with 64 filters in each layer. Fourth, each method is run for 12 hours on a single GPU (NVIDIA GeForce GTX 1080 Ti ) on the training and validation set with batch size of 64 . Fifth, the output architecture is trained with both the training and validation set. Sixth, the testing set is used to evaluate the trained architecture. Error rate is selected as the evaluation metric since all the datasets are for classification. For a fair comparison, the same data processing and training procedures are used for all the methods. The neural networks are trained for 200 epochs in all the experiments. Notably, AK-DP uses a real deployed system setting, whose result is not directly comparable with the</p>
<p>Table 1: Classification Error Rate</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">MNIST</th>
<th style="text-align: center;">CIFAR10</th>
<th style="text-align: center;">FASHION</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RANDOM</td>
<td style="text-align: center;">$1.79 \%$</td>
<td style="text-align: center;">$16.86 \%$</td>
<td style="text-align: center;">$11.36 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GRID</td>
<td style="text-align: center;">$1.68 \%$</td>
<td style="text-align: center;">$17.17 \%$</td>
<td style="text-align: center;">$10.28 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SPMT</td>
<td style="text-align: center;">$1.36 \%$</td>
<td style="text-align: center;">$14.68 \%$</td>
<td style="text-align: center;">$9.62 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SMAC</td>
<td style="text-align: center;">$1.43 \%$</td>
<td style="text-align: center;">$15.04 \%$</td>
<td style="text-align: center;">$10.87 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SEAS</td>
<td style="text-align: center;">$1.07 \%$</td>
<td style="text-align: center;">$12.43 \%$</td>
<td style="text-align: center;">$8.05 \%$</td>
</tr>
<tr>
<td style="text-align: left;">NASBOT</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">$12.30 \%$</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: left;">BFS</td>
<td style="text-align: center;">$1.56 \%$</td>
<td style="text-align: center;">$13.84 \%$</td>
<td style="text-align: center;">$9.13 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BO</td>
<td style="text-align: center;">$1.83 \%$</td>
<td style="text-align: center;">$12.90 \%$</td>
<td style="text-align: center;">$7.99 \%$</td>
</tr>
<tr>
<td style="text-align: left;">AK</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 \%}$</td>
<td style="text-align: center;">$\mathbf{1 1 . 4 4 \%}$</td>
<td style="text-align: center;">$\mathbf{7 . 4 2 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">AK-DP</td>
<td style="text-align: center;">$0.60 \%$</td>
<td style="text-align: center;">$3.60 \%$</td>
<td style="text-align: center;">$6.72 \%$</td>
</tr>
</tbody>
</table>
<p>rest of the methods. Except for AK-DP, all other methods are fairly compared using the same initial architecture to start the search.</p>
<h3>5.1 Evaluation of Effectiveness</h3>
<p>We first evaluate the effectiveness of the proposed method. The results are shown in Table 1. The following conclusions can be drawn based on the results.
(1) AK-DP is evaluated to show the final performance of our system, which shows the deployed system (AK-DP) achieved state-of-the-art performance on all three datasets.
(2) The proposed method AK achieves the lowest error rate on all the three datasets, which demonstrates that AK is able to find simple but effective architectures on small datasets (MNIST) and can explore more complicated structures on larger datasets (CIFAR10).
(3) The straightforward approaches and traditional approaches perform well on the MNIST dataset, but poorly on the CIFAR10 dataset. This may come from the fact that: naive approaches like random search and grid search only try a limited number of architectures blindly while the two conventional approaches are unable to change the depth and skip-connections of the architectures.
(4) Though the two state-of-the-art approaches achieve acceptable performance, SEAS could not beat our proposed model due to its subpar search strategy. The hill-climbing strategy it adopts only takes one step at each time in morphing the current best architecture, and the search tree structure is constrained to be unidirectionally extending. Comparatively speaking, NASBOT possesses stronger search expandability and also uses Bayesian optimization as our proposed method. However, the low efficiency in training the neural architectures constrains its power in achieving comparable performance within a short time period. By contrast, the network morphism scheme along with the novel searching strategy ensures our model to achieve desirable performance with limited hardware resources and time budges.
(5) For the two variants of AK, BFS preferentially considers searching a vast number of neighbors surrounding the initial architecture, which constrains its power in reaching the better architectures away from the initialization. By comparison, BO can jump far from the initial architecture. But without network morphism, it needs to train each neural architecture with a much longer time, which limits the number of architectures it can search within a given time.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation of Efficiency. The two figures plot the same result with different X-axis. BFS uses network morphism. BO uses Bayesian optimization. AK uses both.</p>
<h3>5.2 Evaluation of Efficiency</h3>
<p>In this experiment, we try to evaluate the efficiency gain of the proposed method in two aspects. First, we evaluate whether Bayesian optimization can really find better solutions with a limited number of observations. Second, we evaluated whether network morphism can enhance training efficiency.</p>
<p>We compare the proposed method AK with its two variants, BFS and BO, to show the efficiency gains from Bayesian optimization and network morphism, respectively. BFS does not adopt Bayesian optimization but only network morphism, and use breadth-first search to select the network morphism operations. BO does not employ network morphism but only Bayesian optimization. Each of the three methods is run on CIFAR10 for twelve hours. The two figures in Figure 4 shows the same results but with different X -axes. The Y-axis is the lowest error rate achieved. The X -axes are the number of neural networks searched and the searching time.</p>
<p>Two conclusions can be drawn by comparing BFS and AK. First, Bayesian optimization can efficiently find better architectures with a limited number of observations. When searched the same number of neural architectures, AK could achieve a much lower error rate than BFS. It demonstrates that Bayesian optimization could effectively guide the search in the right direction, which is much more efficient in finding good architectures than the naive BFS approach. Second, the overhead created by Bayesian optimization during the search is low. In the left part of Figure 4, it shows BFS and AK searched similar numbers of neural networks within twelve hours. BFS is a naive search strategy, which does not consume much time during the search besides training the neural networks. AK searched slightly less neural architectures than BFS because of higher time complexity.</p>
<p>Two conclusions can be drawn by comparing BO and AK. First, network morphism does not negatively impact search performance. In the left part of Figure 4, when BO and AK search a similar number of neural architectures, they achieve similar lowest error rates. Second, network morphism increases training efficiency, thus improve the performance. As shown in the left part of Figure 4, AK could search much more architectures than BO within the same amount of time due to the adoption of network morphism. Since network morphism does not degrade the search performance, searching more architectures results in finding better architectures. This could also be confirmed in the right part of Figure 4. At the end of the searching time, AK achieves lower error rate than BO.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Kernel and Performance Matrix Visualization. (a) shows the proposed kernel matrix. (b) is a matrix of similarity in the performance of the neural architectures.</p>
<p>To show the quality of the edit-distance neural network kernel, we investigate the difference between the two matrices $\boldsymbol{K}$ and $\boldsymbol{P}$. $\boldsymbol{K} \in \mathbb{R}^{n \times n}$ is the kernel matrix, where $\boldsymbol{K}<em i_="i," j="j">{i, j}=\kappa\left(f^{(i)}, f^{(j)}\right) . \boldsymbol{P} \in$ $\mathbb{R}^{n \times n}$ describes the similarity of the actual performance between neural networks, where $\boldsymbol{P}</em>$.
$\boldsymbol{K}$ and $\boldsymbol{P}$ are visualized in Figure 5a and 5b. Lighter color means larger values. There are two patterns can be observed in the figures. First, the white diagonal of Figure 5a and 5b. According to the definiteness property of the kernel, $\kappa\left(f_{x}, f_{x}\right)=1, \forall f_{x} \in \mathcal{F}$, thus the diagonal of $\boldsymbol{K}$ is always 1 . It is the same for $\boldsymbol{P}$ since no difference exists in the performance of the same neural network. Second, there is a small light square area on the upper left of Figure 5a. These are the initial neural architectures to train the Bayesian optimizer, which are neighbors to each other in terms of network morphism operations. A similar pattern is reflected in Figure 5b, which indicates that when the kernel measures two architectures as similar, they tend to have similar performance.}=-\left|c^{(i)}-c^{(j)}\right|$, where $c^{(i)}$ is the cost function value in the search history $\mathcal{H}$ described in Section 3. We use CIFAR10 as an example here, and adopt error rate as the cost metric. Since the values in $\boldsymbol{K}$ and $\boldsymbol{P}$ are in different scales, both matrices are normalized to the range $[-1,1]$. The difference between $\boldsymbol{K}$ and $\boldsymbol{P}$ are measured quantitatively with mean square error, which is $1.12 \times 10^{-3</p>
<h2>6 CONCLUSION AND FUTURE WORK</h2>
<p>In this paper, a novel method for efficient neural architecture search with network morphism is proposed. It enables Bayesian optimization to guide the search by designing a neural network kernel, and an algorithm for optimizing acquisition function in tree-structured space. The proposed method is wrapped into an open-source AutoML system, namely Auto-Keras, which can be easily downloaded and used with an extremely simple interface. The method has shown good performance in the experiments and outperformed several traditional hyperparameter-tuning methods and state-of-the-art neural architecture search methods. The following open questions may be studied in future work. (1) Tune the neural architecture and the hyperparameters of the training process jointly. (2) Design task-oriented NAS to solve specific machine learning problems, e.g., image segmentation [28], object detection [16, 32], network analysis $[20,43]$.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>The authors thank the anonymous reviewers for their helpful comments, and all the contributors from the open-source community. This work is, in part, supported by DARPA (#FA8750-17- 2-0116) and NSF (#IIS-1718840 and #IIS-1750074). The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.</p>
<h2>REFERENCES</h2>
<p>[1] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. 2002. Finite-time analysis of the multiarmed bandit problem. Machine learning (2002).
[2] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167 (2016).
[3] James Bergstra, Dan Tamins, and David D Cox. 2013. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Python in Science Conference.
[4] Jean Bourgain. 1985. On Lipschitz embedding of finite metric spaces in Hilbert space. Israel Journal of Mathematics (1985).
[5] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. 2017. SMASH: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344 (2017).
[6] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, et al. 2013. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning.
[7] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Efficient architecture search by network transformation. In AAAI Conference on Artificial Intelligence.
[8] Han Cai, Ligeng Zhu, and Song Han. 2019. ProxylessNAS: Direct neural architecture search on target task and hardware. In International Conference on Learning Representations.
[9] Qi Chai and Guang Gong. 2012. Verifiable symmetric searchable encryption for semi-honest-but-curious cloud servers. In International Conference on Communications.
[10] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. 2015. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641 (2015).
[11] Franois Chollet et al. 2015. Keras. https://keras.io.
[12] Travis Desert. 2017. Large scale evolution of convolutional neural networks using volunteer computing. In Genetic and Evolutionary Computation Conference Companion.
[13] Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. 2017. Simple And Efficient Architecture Search for Convolutional Neural Networks. arXiv preprint arXiv:1711.04528 (2017).
[14] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2018. Neural Architecture Search: A Survey. arXiv preprint arXiv:1808.05377 (2018).
[15] Matthias Feuers, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. 2015. Efficient and robust automated machine learning. In Advances in Neural Information Processing Systems.
[16] Golinaz Ghiasi, Trung-Yi Lin, Ruoming Pang, and Quoc V Le. 2019. NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection. arXiv preprint arXiv:1904.07392 (2019).
[17] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. 2019. Single Path One-Shot Neural Architecture Search with Uniform Sampling. arXiv preprint arXiv:1904.00420 (2019).
[18] Bernard Haasdonk and Claus Bahrmann. 2004. Learning with distance substitution kernels. In Joint Pattern Recognition Symposium.
[19] Peter E Hart, Nils J Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics (1968).
[20] Xiao Huang, Qiangquan Song, Fan Yang, and Xia Hu. 2019. Large-scale heterogeneous feature embedding. In AAAI Conference on Artificial Intelligence.
[21] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. Sequential ModelBased Optimization for General Algorithm Configuration. In International Conference on Learning and Intelligent Optimization.
[22] Kirthevasan Kandasamy, Willie Neuwanger, Jeff Schneider, Barnabas Poczosi, and Eric Xing. 2018. Neural Architecture Search with Bayesian Optimization and Optimal Transport. Advances in Neural Information Processing Systems (2018).
[23] Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. 1983. Optimization by simulated annealing. science (1983).
[24] Lars Kotthoff, Chris Thornton, Holger H Hoos, Frank Hutter, and Kevin LeytonBrown. 2016. Auto-WEKA 2.0: Automatic model selection and hyperparameter
optimization in WEKA. Journal of Machine Learning Research (2016).
[25] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features from tiny images. Technical Report. Citeseer.
[26] Harold W Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics (1955).
[27] Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradientbased learning applied to document recognition. Proc. IEEE (1998).
[28] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan Yuille, and Li Fei-Fei. 2019. Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation. arXiv preprint arXiv:1901.02985 (2019).
[29] Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. 2017. Progressive neural architecture search. In European Conference on Computer Vision.
[30] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. 2017. Hierarchical representations for efficient architecture search. arXiv preprint arXiv:1711.00436 (2017).
[31] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018).
[32] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. 2016. Ssd: Single shot multibox detector. In European Conference on Computer Vision.
[33] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. Neural architecture optimization. In Advances in Neural Information Processing Systems.
[34] Hiroshi Maehara. 2013. Euclidean embeddings of finite metric spaces. Discrete Mathematics (2013).
[35] Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore. 2016. Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. In Genetic and Evolutionary Computation Conference 2016.
[36] Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research (2011).
[37] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. Efficient Neural Architecture Search via Parameter Sharing. arXiv preprint arXiv:1802.03268 (2018).
[38] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2018. Regularized Evolution for Image Classifier Architecture Search. arXiv preprint arXiv:1802.01548 (2018).
[39] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc Le, and Alex Kurakin. 2017. Large-scale evolution of image classifiers. In International Conference on Machine Learning. arXiv preprint arXiv:1703.01041.
[40] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems.
[41] Masanori Suganuma, Shinichi Shirakawa, and Tomoharu Nagao. 2017. A genetic programming approach to designing convolutional neural network architectures. In Genetic and Evolutionary Computation Conference.
[42] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. 2018. Mnasnet: Platform-aware neural architecture search for mobile. arXiv preprint arXiv:1807.11626 (2018).
[43] Qiaoyu Tan, Ninghao Liu, and Xia Hu. 2019. Deep Representation Learning for Social Network Analysis. arXiv preprint arXiv:1904.08547 (2019).
[44] Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2013. Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms. In International Conference on Knowledge Discovery and Data Mining.
[45] Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. 2016. Network morphism. In International Conference on Machine Learning.
[46] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:cs.LG/cs.LG:1708.07747.
[47] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. 2019. SNAS: stochastic neural architecture search. In International Conference on Learning Representations.
[48] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In International Conference on Knowledge Discovery and Data Mining.
[49] Zhiping Zeng, Anthony KH Tang, Jianyong Wang, Jianhua Feng, and Lizhu Zhou. 2009. Comparing stars: On approximating graph edit distance. In International Conference on Very Large Data Bases.
[50] Zhao Zhong, Junjie Yan, and Cheng-Lin Liu. 2017. Practical Network Blocks Design with Q-Learning. arXiv preprint arXiv:1708.05552 (2017).
[51] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. In International Conference on Learning Representations.</p>
<h2>APPENDIX: REPRODUCIBILITY</h2>
<p>In this section, we provide the details of our implementation and proofs for reproducibility.</p>
<ul>
<li>The default architectures used to initialized are introduced.</li>
<li>The details of the implementation of the four network morphism operations are provided.</li>
<li>The details of preprocessing the datasets are shown.</li>
<li>The details of the training process are described.</li>
<li>The proof of the validity of the kernel function is provided.</li>
<li>The process of using $\rho(\cdot)$ to distort the approximated editdistance of the neural architectures $d(\cdot, \cdot)$ is introduced.
Notably, the code and detailed documentation are available at AutoKeras official website (https://autokeras.com).</li>
</ul>
<h2>A DEFAULT ARCHITECTURES</h2>
<p>As we introduced in the experiment section, for all other methods except AK-DP, are using the same three-layer convolutional neural network as the default architecture. The AK-DP is initialized with ResNet, DenseNet and the three-layer CNN. In the current implementation, ResNet18 and DenseNet121 specifically are chosen as the among all the ResNet and DenseNet architectures.</p>
<p>The three-layer CNN is constructed as follows. Each convolutional layer is actually a convolutional block of a ReLU layer, a batch-normalization layer, the convolutional layer, and a pooling layer. All the convolutional layers are with kernel size equal to three, stride equal to one, and number of filters equal to 64 .</p>
<p>All the default architectures share the same fully-connected layers design. After all the convolutional layers, the output tensor passes through a global average pooling layer followed by a dropout layer, a fully-connected layer of 64 neurons, a ReLU layer, another fully-connected layer, and a softmax layer.</p>
<h2>B NETWORK MORPHISM IMPLEMENTATION</h2>
<p>The implementation of the network morphism is introduced from two aspects. First, we describe how the new weights are initialized. Second, we introduce a pool of possible operations which the Bayesian optimizer can select from, e.g. the possible start and end points of a skip connection.</p>
<p>The four network morphism operations all involve adding new weights during inserting new layers and expanding existing layers. We initialize the newly added weights with zeros. However, it would create a symmetry prohibiting the newly added weights to learn different values during backpropagation. We follow the Net2Net [10] to add noise to break the symmetry. The amount of noise added is the largest noise possible not changing the output.</p>
<p>There are a large amount of possible network morphism operations we can choose. Although there are only four types of operations we can choose, a parameter of the operation can be set to a large number of different values. For example, when we use the deep $(G, u)$ operation, we need to choose the location $u$ to insert the layer. In the tree-structured search, we actually cannot exhaust all the operations to get all the children. We will keep sampling from the possible operations until we reach eight children for a node. For the sampling, we randomly sample an operation from deep, wide and skip (add and concat), with equally likely probability. The
parameters of the corresponding operation are sampled accordingly. If it is the deep operation, we need to decide the location to insert the layer. In our implementation, any location except right after a skip-connection. Moreover, we support inserting not only convolutional layers, but activation layers, batch-normalization layers, dropout layer, and fully-connected layers as well. They are randomly sampled with equally likely probability. If it is the wide operation, we need to choose the layer to be widened. It can be any convolutional layer or fully-connected layer, which are randomly sampled with equally likely probability. If it is the skip operations, we need to decide if it is add or concat. The start point and end point of a skip-connection can be the output of any layer except the already-exist skip-connection layers. So all the possible skipconnections are generated in the form of tuples of the start point, end point and type (add or concat), among which we randomly sample a skip-connection with equally likely probability.</p>
<h2>C PREPROCESSING THE DATASETS</h2>
<p>The benchmark datasets, e.g., MNIST, CIFAR10, FASHION, are preprocessed before the neural architecture search. It involves normalization and data augmentation. We normalize the data to the standard normal distribution. For each channel, a mean and a standard deviation are calculated since the values in different channels may have different distributions. The mean and standard deviation are calculated using the training and validation set together. The testing set is normalized using the same values. The data augmentation includes random crop, random horizontal flip, and cutout, which can improve the robustness of the trained model.</p>
<h2>D PERFORMANCE ESTIMATION</h2>
<p>During the observation phase, we need to estimate the performance of a neural architecture to update the Gaussian process model in Bayesian optimization. Since the quality of the observed performances of the neural architectures is essential to the neural architecture search algorithm, we propose to train the neural architectures instead of using the performance estimation strategies used in literatures [5, 14, 37]. The quality of the observations is essential to the neural architecture search algorithm. So the neural architectures are trained during the search in our proposed method.</p>
<p>There two important requirements for the training process. First, it needs to be adaptive to different architectures. Different neural networks require different numbers of epochs in training to converge. Second, it should not be affected by the noise in the performance curve. The final metric value, e.g., mean squared error or accuracy, on the validation set is not the best performance estimation since there is random noise in it.</p>
<p>To be adaptive to architectures of different sizes, we use the same strategy as the early stop criterion in the multi-layer perceptron algorithm in Scikit-Learn [36]. It sets a maximum threshold $\tau$. If the loss of the validation set does not decrease in $\tau$ epochs, the training stops. Comparing with the methods using a fixed number of training epochs, it is more adaptive to different neural architectures.</p>
<p>To avoid being affected by the noise in the performance, the mean of metric values of the last $\tau$ epochs on the validation set is used as the estimated performance for the given neural architecture. It is more accurate than the final metric value on the validation set.</p>
<h2>E VALIDITY OF THE KERNEL</h2>
<p>Theorem 1. $d\left(f_{a}, f_{b}\right)$ is a metric space distance.</p>
<h2>Proof of Theorem 1:</h2>
<p>Theorem 1 is proved by proving the non-negativity, definiteness, symmetry, and triangle inequality of $d$.</p>
<h2>Non-negativity:</h2>
<p>$\forall f_{x} f_{y} \in \mathcal{F}, d\left(f_{x}, f_{y}\right) \geq 0$.
From the definition of $w(l)$ in Equation (6), $\forall l, w(l)&gt;0 .$. $\forall l_{x} l_{y}, d_{l}\left(l_{x}, l_{y}\right) \geq 0 . \therefore \forall L_{x} L_{y}, D_{l}\left(L_{x}, L_{y}\right) \geq 0$. Similarly, $\forall s_{x} s_{y}$, $d_{s}\left(s_{x}, s_{y}\right) \geq 0$, and $\forall S_{x} S_{y}, D_{s}\left(S_{x}, S_{y}\right) \geq 0$. In conclusion, $\forall f_{x} f_{y} \in$ $\mathcal{F}, d\left(f_{x}, f_{y}\right) \geq 0$.</p>
<h2>Definiteness:</h2>
<p>$f_{a}=f_{b} \Longleftrightarrow d\left(f_{a}, f_{b}\right)=0$.
$f_{a}=f_{b} \Longrightarrow d\left(f_{a}, f_{b}\right)=0$ is trivial. To prove $d\left(f_{a}, f_{b}\right)=0 \Longrightarrow$ $f_{a}=f_{b}$, let $d\left(f_{a}, f_{b}\right)=0 . \therefore \forall L_{x} L_{y}, D_{l}\left(L_{x}, L_{y}\right) \geq 0$ and $\forall S_{x} S_{y}$, $D_{s}\left(S_{x}, S_{y}\right) \geq 0$. Let $L_{a}$ and $L_{b}$ be the layer sets of $f_{a}$ and $f_{b}$. Let $S_{a}$ and $S_{b}$ be the skip-connection sets of $f_{a}$ and $f_{b}$.
$\therefore D_{l}\left(L_{a}, L_{b}\right)=0$ and $D_{s}\left(S_{a}, S_{b}\right)=0 . \because \forall l_{x} l_{y}, d_{l}\left(l_{x}, l_{y}\right) \geq 0$ and $\forall s_{x} s_{y}, d_{s}\left(s_{x}, s_{y}\right) \geq 0 . \therefore\left|L_{a}\right|=\left|L_{b}\right|,\left|S_{a}\right|=\left|S_{b}\right|, \forall l_{a} \in L_{a}$, $l_{b}=\varphi_{l} l_{a}^{\prime} \in L_{b}, d_{l}\left(l_{a}, l_{b}\right)=0, \forall s_{a} \in S_{a}, s_{b}=\varphi_{s}\left(s_{a}\right) \in S_{b}$, $d_{s}\left(s_{a}, s_{b}\right)=0$. According to Equation (6), each of the layers in $f_{a}$ has the same width as the matched layer in $f_{b}$. According to the restrictions of $\varphi_{l}(\cdot)$, the matched layers are in the same order, and all the layers are matched, i.e. the layers of the two networks are exactly the same. Similarly, the skip-connections in the two neural networks are exactly the same. $\therefore f_{a}=f_{b}$. So $d\left(f_{a}, f_{b}\right)=0 \Longrightarrow f_{a}=f_{b}$, let $d\left(f_{a}, f_{b}\right)=0$. Finally, $f_{a}=f_{b} \Longleftrightarrow d\left(f_{a}, f_{b}\right)$.</p>
<h2>Symmetry:</h2>
<p>$\forall f_{x} f_{y} \in \mathcal{F}, d\left(f_{x}, f_{y}\right)=d\left(f_{y}, f_{x}\right)$.
Let $f_{a}$ and $f_{b}$ be two neural networks in $\mathcal{F}$. Let $L_{a}$ and $L_{b}$ be the layer sets of $f_{a}$ and $f_{b}$. If $\left|L_{a}\right| \neq\left|L_{b}\right|, D_{l}\left(L_{a}, L_{b}\right)=D_{l}\left(L_{b}, L_{a}\right)$ since it will always swap $L_{a}$ and $L_{b}$ if $L_{a}$ has more layers. If $\left|L_{a}\right|=\left|L_{b}\right|$, $D_{l}\left(L_{a}, L_{b}\right)=D_{l}\left(L_{b}, L_{a}\right)$ since $\varphi_{l}(\cdot)$ is undirected, and $d_{l}(\cdot, \cdot)$ is symmetric. Similarly, $D_{s}(\cdot, \cdot)$ is symmetric. In conclusion, $\forall f_{x} f_{y} \in$ $\mathcal{F}, d\left(f_{x}, f_{y}\right)=d\left(f_{y}, f_{x}\right)$.</p>
<h2>Triangle Inequality:</h2>
<p>$\forall f_{x} f_{y} f_{z} \in \mathcal{F}, d\left(f_{x}, f_{y}\right) \leq d\left(f_{x}, f_{z}\right)+d\left(f_{z}, f_{y}\right)$.
Let $l_{x}, l_{y}, l_{z}$ be neural network layers of any width. If $w\left(l_{x}\right)&lt;$ $w\left(l_{y}\right)&lt;w\left(l_{z}\right), d_{l}\left(l_{x}, l_{y}\right)=\frac{w\left(l_{y}\right)-w\left(l_{z}\right)}{w\left(l_{y}\right)}=2-\frac{w\left(l_{x}\right)+w\left(l_{y}\right)}{w\left(l_{y}\right)} \leq 2-$ $\frac{w\left(l_{x}\right)+w\left(l_{y}\right)}{w\left(l_{x}\right)}=d_{l}\left(l_{x}, l_{z}\right)+d_{l}\left(l_{z}, l_{y}\right)$. If $w\left(l_{x}\right) \leq w\left(l_{z}\right) \leq w\left(l_{y}\right)$, $d_{l}\left(l_{x}, l_{y}\right)=\frac{w\left(l_{y}\right)-w\left(l_{x}\right)}{w\left(l_{y}\right)}=\frac{w\left(l_{y}\right)-w\left(l_{z}\right)}{w\left(l_{y}\right)}+\frac{w\left(l_{x}\right)-w\left(l_{x}\right)}{w\left(l_{y}\right)} \leq \frac{w\left(l_{y}\right)-w\left(l_{z}\right)}{w\left(l_{y}\right)}+$ $\frac{w\left(l_{z}\right)-w\left(l_{x}\right)}{w\left(l_{z}\right)}=d_{l}\left(l_{x}, l_{z}\right)+d_{l}\left(l_{z}, l_{y}\right)$. If $w\left(l_{z}\right) \leq w\left(l_{x}\right) \leq w\left(l_{y}\right)$, $d_{l}\left(l_{x}, l_{y}\right)=\frac{w\left(l_{y}\right)-w\left(l_{x}\right)}{w\left(l_{y}\right)}=2-\frac{w\left(l_{y}\right)}{w\left(l_{y}\right)}-\frac{w\left(l_{x}\right)}{w\left(l_{y}\right)} \leq 2-\frac{w\left(l_{z}\right)}{w\left(l_{x}\right)}-\frac{w\left(l_{x}\right)}{w\left(l_{y}\right)} \leq$ $2-\frac{w\left(l_{z}\right)}{w\left(l_{x}\right)}-\frac{w\left(l_{z}\right)}{w\left(l_{y}\right)}=d_{l}\left(l_{x}, l_{z}\right)+d_{l}\left(l_{z}, l_{y}\right)$. By the symmetry property of $d_{l}(\cdot, \cdot)$, the rest of the orders of $w\left(l_{x}\right), w\left(l_{y}\right)$ and $w\left(l_{z}\right)$ also satisfy the triangle inequality. $\therefore \forall l_{x} l_{y} l_{z}, d_{l}\left(l_{x}, l_{y}\right) \leq d_{l}\left(l_{x}, l_{z}\right)+d_{l}\left(l_{z}, l_{y}\right)$.
$\forall L_{a} L_{b} L_{c}$, given $\varphi_{l: a \rightarrow c}$ and $\varphi_{l: c \rightarrow b}$ used to compute $D_{l}\left(L_{a}\right.$, $\left.L_{c}\right)$ and $D_{l}\left(L_{c}, L_{b}\right)$, we are able to construct $\varphi_{l: a \rightarrow b}$ to compute $D_{l}\left(L_{a}, L_{b}\right)$ satisfies $D_{l}\left(L_{a}, L_{b}\right) \leq D_{l}\left(L_{a}, L_{c}\right)+D_{l}\left(L_{c}, L_{b}\right)$.</p>
<p>Let $L_{a 1}={l \mid \varphi_{l: a \rightarrow c}(l) \neq \varnothing \wedge \varphi_{l: c \rightarrow b}\left(\varphi_{l: c \rightarrow a}(l)\right) \neq \varnothing}$. $L_{b 1}={l \mid l=\varphi_{l: c \rightarrow b}\left(\varphi_{l: a \rightarrow c}\left(l^{\prime}\right)\right), l^{\prime} \in L_{a 1}}, L_{c 1}={l \mid l=$ $\left.\varphi_{l: a \rightarrow c}\left(l^{\prime}\right) \neq \varnothing, l^{\prime} \in L_{a 1}\right}, L_{a 2}=L_{a}-L_{a 1}, L_{b 2}=L_{b}-L_{b 1}$, $L_{c 2}=L_{c}-L_{c 1}$.</p>
<p>From the definition of $D_{l}(\cdot, \cdot)$, with the current matching functions $\varphi_{l: a \rightarrow c}$ and $\varphi_{l: c \rightarrow b}, D_{l}\left(L_{a 2}, L_{c 2}\right)=D_{l}\left(L_{a 1}, L_{c 1}\right)+D_{l}\left(L_{a 2}, L_{c 2}\right)$ and $D_{l}\left(L_{c}, L_{b}\right)=D_{l}\left(L_{c 1}, L_{b 1}\right)+D_{l}\left(L_{c 2}, L_{b 2}\right)$. First, $\forall l_{a} \in L_{a 1}$ is matched to $l_{b}=\varphi_{l: c \rightarrow b}\left(\varphi_{l: a \rightarrow c}\left(l_{a}\right)\right) \in L_{b}$. Since the triangle inequality property of $d_{l}(\cdot, \cdot), D_{l}\left(L_{a 1}, L_{b 1}\right) \leq D_{l}\left(L_{a 1}, L_{c 1}\right)+D_{l}\left(L_{c 1}\right.$, $\left.L_{b 1}\right)$. Second, the rest of the $l_{a} \in L_{a}$ and $l_{b} \in L_{b}$ are free to match with each other.</p>
<p>Let $L_{a 21}=\left{l \mid \varphi_{l: a \rightarrow c}(l) \neq \varnothing \wedge \varphi_{l: c \rightarrow b}\left(\varphi_{l: c \rightarrow a}(l)\right)=\varnothing\right}$, $L_{b 21}=\left{l \mid l=\varphi_{l: c \rightarrow b}\left(l^{\prime}\right) \neq \varnothing, l^{\prime} \in L_{c 2}\right}, L_{c 21}={l \mid l=$ $\left.\varphi_{l: a \rightarrow c}\left(l^{\prime}\right) \neq \varnothing, l^{\prime} \in L_{a 2}\right}, L_{a 22}=L_{a 2}-L_{a 21}, L_{b 22}=L_{b 2}-L_{b 21}$, $L_{c 22}=L_{c 2}-L_{c 21}$.</p>
<p>From the definition of $D_{l}(\cdot, \cdot)$, with the current matching functions $\varphi_{l: a \rightarrow c}$ and $\varphi_{l: c \rightarrow b}, D_{l}\left(L_{a 2}, L_{c 2}\right)=D_{l}\left(L_{a 21}, L_{c 21}\right)+D_{l}\left(L_{a 22}\right.$, $\left.L_{c 22}\right)$ and $D_{l}\left(L_{c 2}, L_{b 2}\right)=D_{l}\left(L_{c 22}, L_{b 21}\right)+D_{l}\left(L_{c 21}, L_{b 22}\right)$.
$\therefore D_{l}\left(L_{a 22}, L_{c 22}\right)+D_{l}\left(L_{c 21}, L_{b 22}\right) \geq\left|L_{a 2}\right|$
and $D_{l}\left(L_{a 21}, L_{c 21}\right)+D_{l}\left(L_{c 22}, L_{b 21}\right) \geq\left|L_{b 2}\right|$
$\therefore D_{l}\left(L_{a 2}, L_{b 2}\right) \leq\left|L_{a 2}\right|+\left|L_{b 2}\right| \leq D_{l}\left(L_{a 2}, L_{c 2}\right)+D_{l}\left(L_{c 2}, L_{b 2}\right)$.
So $D_{l}\left(L_{a}, L_{b}\right) \leq D_{l}\left(L_{a}, L_{c}\right)+D_{l}\left(L_{c}, L_{b}\right)$.
Similarly, $D_{s}\left(S_{a}, S_{b}\right) \leq D_{s}\left(S_{a}, S_{c}\right)+D_{s}\left(S_{c}, S_{b}\right)$.
Finally, $\forall f_{x} f_{y} f_{z} \in \mathcal{F}, d\left(f_{x}, f_{y}\right) \leq d\left(f_{x}, f_{z}\right)+d\left(f_{z}, f_{y}\right)$.
In conclusion, $d\left(f_{a}, f_{b}\right)$ is a metric space distance.
Theorem 2. $\kappa\left(f_{a}, f_{b}\right)$ is a valid kernel.
Proof of Theorem 2: The kernel matrix of generalized RBF kernel in the form of $e^{-T D^{2}(x, y)}$ is positive definite if and only if there is an isometric embedding in Euclidean space for the metric space with metric $D$ [18]. Any finite metric space distance can be isometrically embedded into Euclidean space by changing the scale of the distance measurement [34]. By using Bourgain theorem [4], metric space $d$ is embedded to Euclidean space with distortion. $\rho\left(d\left(f_{a}, f_{b}\right)\right)$ is the embedded distance for $d\left(f_{a}, f_{b}\right)$. Therefore, $e^{-\rho^{2}\left(d\left(f_{a}, f_{b}\right)\right)}$ is always positive definite. So $\kappa\left(f_{a}, f_{b}\right)$ is a valid kernel.</p>
<h2>F DISTANCE DISTORTION</h2>
<p>In this section, we introduce how Bourgain theorem is used to distort the learned calculated edit-distance into an isometrically embeddable distance for Euclidean space in the Bayesian optimization process.</p>
<p>From Bourgain theorem, a Bourgain embedding algorithm is designed. The input for the algorithm is a metric distance matrix. Here we use the edit-distance matrix of neural architectures. The outputs of the algorithm are some vectors in Euclidean space corresponding to the instances. In our case, the instances are neural architectures. From these vectors, we can calculate a new distance matrix using Euclidean distance. The objective of calculating these vectors is to minimize the difference between the new distance matrix and the input distance matrix, i.e., minimize the distortions on the distances.</p>
<p>We apply this Bourgain algorithm during the update process of the Bayesian optimization. The edit-distance matrix of previous training examples, i.e., the neural architectures, is stored in memory. Whenever new examples are used to train the Bayesian optimization, the edit-distance is expanded to include the new distances. The distorted distance matrix is computed using Bourgain algorithm from the expanded edit-distance matrix. It is isometrically embeddable to the Euclidean space. The kernel matrix computed using the distorted distance matrix is a valid kernel.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
KDD '19, August 4-8, 2019, Anchorage, AK, USA
(C) 2019 Association for Computing Machinery.</p>
<p>ACM ISBN 978-1-4503-6201-6/19/08... $\$ 15.00$
https://doi.org/10.1145/3292500.3330648&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
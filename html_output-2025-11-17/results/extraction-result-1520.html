<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1520 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1520</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1520</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-16503693</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/P16-1043.pdf" target="_blank">Easy Questions First? A Case Study on Curriculum Learning for Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called self-paced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algo-rithm ﬁrst and harder samples can be introduced successively. We introduce a number of heuristics that improve upon self-paced learning. Then, we argue that incorporating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1520.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1520.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Paced Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An optimization-based curriculum learning method that jointly selects training samples and model parameters by weighting examples via a self-paced regularizer (age/pace parameter λ) so easy examples (low loss) are introduced first and harder ones later.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-paced learning for latent variable models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Alignment-based QA models; QANTA (recursive NN); Memory Networks (recurrent NN with memory)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Non-convex QA models used in experiments: (1) alignment-based latent-structural SVMs that convert QA pairs to hypothesis sentences and learn alignment latent structures; (2) QANTA, a dependency-tree recursive neural network for quiz-bowl style factoid QA; (3) Memory Networks, recurrent architectures with an external memory trained with margin ranking loss.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Science QA (grade 3-5 using textbooks and Simple English Wikipedia); MCTest reading comprehension; Quiz-bowl dataset; Weston synthetic 20 memory tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual QA benchmarks where the agent must score candidate answers/hypotheses against text: multiple-choice reading passages (MCTest), elementary science questions with textbook/Wikipedia retrieval, quiz-bowl questions with Wikipedia pages, and synthetic statement+question tasks probing reasoning; interactions are retrieval+scoring or memory reads/writes rather than multi-turn interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>science question answering / textual reasoning (elementary science reasoning and factoid QA)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Answering multiple-choice science questions (grade 3-5) using textbook snippets; reading-comprehension QA on passages; quiz-bowl factoid answering; synthetic memory reasoning tasks (single-word answers to story-style statements).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks are not presented as multi-step physical procedures; complexity is graded (grade 3 → grade 5) and some synthetic tasks require multi-step reasoning over statements; alignment models include latent compositional alignment structures but explicit procedural composition is not the focus.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>self-paced curriculum (SPL) with multiple SP-regularizers (hard, soft linear, soft logarithmic, mixture)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>SPL jointly optimizes model parameters and per-example weights v∈[0,1] using a self-paced regularizer g(v,λ). Examples with smaller loss get larger weights; as age parameter λ increases the model incorporates harder examples. Four SP-regularizers considered: hard (binary selection), soft linear, soft logarithmic, and a mixture; closed-form update for v is used and λ is gradually increased across epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task difficulty estimated by local loss / expected objective (i.e., examples with lower loss considered easier)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Science QA: grade 3 → grade 5 (three difficulty levels); Memory Networks: 20 types of synthetic tasks probing varying reasoning complexity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: SPL variants improved final test accuracy for alignment-based models and QANTA; soft SP-regularizers outperformed hard weighting and mixture performed best among SPL variants. Exact numeric accuracies are not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Models trained without curriculum (random ordering / NC) achieved lower test accuracy and slower early gains on easier questions; specific numeric baselines are not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared SPL regularizers: soft regularizers (soft linear, soft logarithmic) > hard; mixture SP-regularizer performed best. SPL performance comparable to many heuristics; extending SPL with Explore & Exploit (diversity) improved results for deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SPL is effective for non-convex QA models, especially alignment-based and recursive NN (QANTA); choice of SP-regularizer matters (soft/mixed > hard). However, naive SPL/curriculum may harm memory networks unless diversity is incorporated, because memory restructuring is required when only easy examples are seen early.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1520.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1520.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy and Inference-based Curriculum Heuristics (GO, CiO, M2, ECiO, CiO-ECiO, CA, FfDB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of lightweight selection heuristics that order training examples for curriculum learning by estimating expected effect on the model or by inference signals: Greedy Optimal, Change-in-Objective, Mini-max, Expected Change-in-Objective, surprise (CiO-ECiO), Correctly Answered, and Farthest-from-Decision-Boundary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Alignment-based QA models; QANTA; Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same set of QA agents as above: latent-structural alignment SVMs, QANTA recursive NN, and Memory Networks recurrent NN with memory.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Science QA (grade 3-5); MCTest; Quiz-bowl; Weston synthetic memory tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual QA datasets requiring retrieval/alignment or memory-based reasoning; selection heuristics operate by scoring candidate training questions and selecting small batches each epoch to add to training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>science and factual QA / textual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Selecting which elementary science (grade 3-5) questions to present during training; selecting quiz-bowl questions; selecting synthetic memory tasks to introduce progressively.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Not explicitly compositional; selection is at example granularity though some tasks require multi-step reasoning; heuristics may indirectly encourage exposure to varied substructures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>heuristic-based incremental curriculum (various heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Heuristics pick new training examples Q\Q0 given current model M and already-presented set Q0. GO estimates expected retraining effect; CiO picks the question with smallest increase in objective; M2 minimizes worst-case error over answers; ECiO uses expected effect via normalized model scores; CiO-ECiO selects by surprise (difference); CA selects questions already answered correctly with minimum cost; FfDB selects examples far from decision boundary (for latent structural SVMs). Heuristics can be applied in batches for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>estimated model impact / expected change-in-objective / difficulty via model loss or confidence / distance from decision boundary / surprise (difference between actual and expected)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>As above: grade-3 → grade-5 science QA; variety of reasoning difficulty across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: Heuristics produce significant improvements for alignment-based models and QANTA, performing as well as or better than SPL in many cases. Heuristics ranked (increasing performance): CA, M2, ECiO, GO, CiO, FfDB, CiO-ECiO. Ensemble of heuristics further improves performance. No numeric metrics provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Outperforms random/no-curriculum baseline (NC) in accuracy and speeds learning on easier questions; exact numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Individual heuristics compared against SPL and each other; Ensemble > individual heuristics; CiO and CiO-ECiO among top heuristics; ECiO similar to SPL(hard). Efficiency trade-offs noted: GO and CiO require retraining (expensive); M2, ECiO, CA, FfDB require inference only (cheaper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple heuristics can match or exceed SPL on QA tasks and are computationally cheaper; choice of heuristic affects efficiency and final performance; ensemble of heuristics gives consistent gains; heuristics without diversity can still fail for memory networks due to memory restructuring issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1520.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1520.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E&E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explore and Exploit (E&E) — Curriculum with Diversity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum extension that mixes easy examples with strategically chosen diverse (dissimilar) examples to prevent representational overfitting to easy samples; diversity measured as angular distance between induced feature hyperplanes and combined with curriculum objective via a convex combination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Alignment-based QA models; QANTA; Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same QA agents; E&E is applied as a selection modifier to ensure example diversity while preserving easy-first ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Science QA (grade 3-5 using textbooks/Wikipedia); MCTest; Weston memory tasks; Quiz-bowl</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Textual QA benchmarks where selected training examples are added progressively; diversity computed in model feature space (angle between feature vectors induced by examples).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>science question answering and textual reasoning (aimed at improving representation learning and handling harder examples)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Mixing in grade 4/5 questions early together with grade 3 examples to avoid representation collapse; selecting diverse examples by cosine-angle measure between feature vectors of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Not explicitly a compositional curriculum; diversity encourages exposure to varied substructures and prevents narrow representational specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>difficulty-based curriculum augmented with diversity (Explore & Exploit)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>At each selection step the algorithm optimizes a convex combination of the curriculum objective (favor easy examples) and the sum of angular distances between candidate example and already-selected examples Q0. The convex weight is tuned on development data; small non-zero weight encourages injecting harder/diverse examples early.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>primarily task difficulty (easy → hard) combined with explicit diversity (feature-space angular distance) to ensure varied training signal</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Science QA grade 3 → grade 5; memory tasks with varying reasoning complexity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: E&E provided further improvements across experiments, and was crucial for making curriculum learning effective for deep models (especially memory networks) — it sharply reduced parameter-change oscillations and prevented catastrophic memory restructuring; Ensemble+E&E showed faster gains on easier grades and improved final accuracy across grades. Exact numbers not provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Without E&E, deep models (especially memory networks) exhibited poor or negative gains from naive curricula (abrupt memory restructuring), and final accuracy was lower than when diversity was used.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>E&E improves SPL and heuristic approaches when added; as the diversity-interpolant increases (0→1) more diverse questions selected and relative change in parameters decreases; small diversity weight (e.g., 0.1) sharply reduces parameter changes for neural models and yields better stability and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating a small proportion of diverse/harder examples (E&E) alongside easy examples is critical for deep models: it stabilizes memory/network parameters, avoids costly restructuring when harder examples arrive, and leads to consistent accuracy improvements; E&E is especially important for memory networks where naive curricula hurt performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1520.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1520.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble+IW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble Heuristics with Importance Weighting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combines multiple heuristics by scoring candidate questions relative to average heuristic scores (ensemble) and applies importance-weighting to correct for selection bias introduced by non-uniform sampling during curriculum selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Alignment-based QA models; QANTA; Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same QA agents; ensemble scoring ranks candidate questions by aggregated heuristic signal; importance-weighting rescales losses/gradients or regularization terms to obtain unbiased generalization estimates under biased selection distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Science QA (grade 3-5); MCTest; Weston memory tasks; Quiz-bowl</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text QA datasets where training is progressively built by selecting batches of examples according to ensemble scores and applying sample weights during optimization to compensate for non-i.i.d. sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>science/question answering / textual reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Selecting questions from mixed-grade science QA using ensemble ranking; weighting gradients or C_i terms to correct selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Not explicitly compositional; ensemble and IW operate at example selection and loss-weighting level.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>ensemble-of-heuristics curriculum with importance-weighting correction</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Ensemble computes ratio of each heuristic's score for a candidate to the average score across remaining examples and picks the highest ratio; importance-weighting constructs a biased sampling distribution D and assigns weights w(q,a) = p_D(q,a)/p_D(q,a) (note: paper describes reweighting to compute unbiased generalization estimates) and adjusts loss/regularization accordingly (multiply gradients or C_i).</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>aggregated heuristic signals (expected model impact, loss/confidence, distance from boundary) with bias correction via importance weights</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Grade 3→5 science QA; variety of QA difficulty levels</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Qualitative: Ensemble significantly outperforms individual heuristics; importance-weighting and E&E provide further improvements. Ensemble+E&E used in reported figures shows aggressive curriculum progression and improved grade-wise accuracy. Exact numeric metrics absent from excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Qualitative: Single heuristics or no curriculum (NC) yield slower learning and lower final accuracy; ensemble and IW yield consistent improvements over NC.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Ensemble > individual heuristics; adding IW and E&E to ensemble further improves performance and stability, particularly for deep models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregating multiple selection heuristics into an ensemble yields stable, higher gains than any single heuristic; importance-weighting is a practical correction for sample-selection bias introduced by active selection; combining ensemble, IW, and E&E gives the best empirical results across most QA settings in the study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-paced learning for latent variable models <em>(Rating: 2)</em></li>
                <li>Self-paced learning with diversity <em>(Rating: 2)</em></li>
                <li>Easy samples first: Self-paced reranking for zero-example multimedia search <em>(Rating: 1)</em></li>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Memory networks <em>(Rating: 2)</em></li>
                <li>Towards ai-complete question answering: A set of prerequisite toy tasks <em>(Rating: 1)</em></li>
                <li>A neural network for factoid question answering over paragraphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1520",
    "paper_id": "paper-16503693",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "SPL",
            "name_full": "Self-Paced Learning",
            "brief_description": "An optimization-based curriculum learning method that jointly selects training samples and model parameters by weighting examples via a self-paced regularizer (age/pace parameter λ) so easy examples (low loss) are introduced first and harder ones later.",
            "citation_title": "Self-paced learning for latent variable models",
            "mention_or_use": "use",
            "agent_name": "Alignment-based QA models; QANTA (recursive NN); Memory Networks (recurrent NN with memory)",
            "agent_description": "Non-convex QA models used in experiments: (1) alignment-based latent-structural SVMs that convert QA pairs to hypothesis sentences and learn alignment latent structures; (2) QANTA, a dependency-tree recursive neural network for quiz-bowl style factoid QA; (3) Memory Networks, recurrent architectures with an external memory trained with margin ranking loss.",
            "agent_size": null,
            "environment_name": "Science QA (grade 3-5 using textbooks and Simple English Wikipedia); MCTest reading comprehension; Quiz-bowl dataset; Weston synthetic 20 memory tasks",
            "environment_description": "Textual QA benchmarks where the agent must score candidate answers/hypotheses against text: multiple-choice reading passages (MCTest), elementary science questions with textbook/Wikipedia retrieval, quiz-bowl questions with Wikipedia pages, and synthetic statement+question tasks probing reasoning; interactions are retrieval+scoring or memory reads/writes rather than multi-turn interactive environments.",
            "procedure_type": "science question answering / textual reasoning (elementary science reasoning and factoid QA)",
            "procedure_examples": "Answering multiple-choice science questions (grade 3-5) using textbook snippets; reading-comprehension QA on passages; quiz-bowl factoid answering; synthetic memory reasoning tasks (single-word answers to story-style statements).",
            "compositional_structure": "Tasks are not presented as multi-step physical procedures; complexity is graded (grade 3 → grade 5) and some synthetic tasks require multi-step reasoning over statements; alignment models include latent compositional alignment structures but explicit procedural composition is not the focus.",
            "uses_curriculum": true,
            "curriculum_name": "self-paced curriculum (SPL) with multiple SP-regularizers (hard, soft linear, soft logarithmic, mixture)",
            "curriculum_description": "SPL jointly optimizes model parameters and per-example weights v∈[0,1] using a self-paced regularizer g(v,λ). Examples with smaller loss get larger weights; as age parameter λ increases the model incorporates harder examples. Four SP-regularizers considered: hard (binary selection), soft linear, soft logarithmic, and a mixture; closed-form update for v is used and λ is gradually increased across epochs.",
            "curriculum_ordering_principle": "task difficulty estimated by local loss / expected objective (i.e., examples with lower loss considered easier)",
            "task_complexity_range": "Science QA: grade 3 → grade 5 (three difficulty levels); Memory Networks: 20 types of synthetic tasks probing varying reasoning complexity",
            "performance_with_curriculum": "Qualitative: SPL variants improved final test accuracy for alignment-based models and QANTA; soft SP-regularizers outperformed hard weighting and mixture performed best among SPL variants. Exact numeric accuracies are not provided in the excerpt.",
            "performance_without_curriculum": "Qualitative: Models trained without curriculum (random ordering / NC) achieved lower test accuracy and slower early gains on easier questions; specific numeric baselines are not provided in the excerpt.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared SPL regularizers: soft regularizers (soft linear, soft logarithmic) &gt; hard; mixture SP-regularizer performed best. SPL performance comparable to many heuristics; extending SPL with Explore & Exploit (diversity) improved results for deep models.",
            "transfer_generalization": null,
            "key_findings": "SPL is effective for non-convex QA models, especially alignment-based and recursive NN (QANTA); choice of SP-regularizer matters (soft/mixed &gt; hard). However, naive SPL/curriculum may harm memory networks unless diversity is incorporated, because memory restructuring is required when only easy examples are seen early.",
            "uuid": "e1520.0"
        },
        {
            "name_short": "Heuristics",
            "name_full": "Greedy and Inference-based Curriculum Heuristics (GO, CiO, M2, ECiO, CiO-ECiO, CA, FfDB)",
            "brief_description": "A set of lightweight selection heuristics that order training examples for curriculum learning by estimating expected effect on the model or by inference signals: Greedy Optimal, Change-in-Objective, Mini-max, Expected Change-in-Objective, surprise (CiO-ECiO), Correctly Answered, and Farthest-from-Decision-Boundary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Alignment-based QA models; QANTA; Memory Networks",
            "agent_description": "Same set of QA agents as above: latent-structural alignment SVMs, QANTA recursive NN, and Memory Networks recurrent NN with memory.",
            "agent_size": null,
            "environment_name": "Science QA (grade 3-5); MCTest; Quiz-bowl; Weston synthetic memory tasks",
            "environment_description": "Textual QA datasets requiring retrieval/alignment or memory-based reasoning; selection heuristics operate by scoring candidate training questions and selecting small batches each epoch to add to training.",
            "procedure_type": "science and factual QA / textual reasoning",
            "procedure_examples": "Selecting which elementary science (grade 3-5) questions to present during training; selecting quiz-bowl questions; selecting synthetic memory tasks to introduce progressively.",
            "compositional_structure": "Not explicitly compositional; selection is at example granularity though some tasks require multi-step reasoning; heuristics may indirectly encourage exposure to varied substructures.",
            "uses_curriculum": true,
            "curriculum_name": "heuristic-based incremental curriculum (various heuristics)",
            "curriculum_description": "Heuristics pick new training examples Q\\Q0 given current model M and already-presented set Q0. GO estimates expected retraining effect; CiO picks the question with smallest increase in objective; M2 minimizes worst-case error over answers; ECiO uses expected effect via normalized model scores; CiO-ECiO selects by surprise (difference); CA selects questions already answered correctly with minimum cost; FfDB selects examples far from decision boundary (for latent structural SVMs). Heuristics can be applied in batches for efficiency.",
            "curriculum_ordering_principle": "estimated model impact / expected change-in-objective / difficulty via model loss or confidence / distance from decision boundary / surprise (difference between actual and expected)",
            "task_complexity_range": "As above: grade-3 → grade-5 science QA; variety of reasoning difficulty across tasks",
            "performance_with_curriculum": "Qualitative: Heuristics produce significant improvements for alignment-based models and QANTA, performing as well as or better than SPL in many cases. Heuristics ranked (increasing performance): CA, M2, ECiO, GO, CiO, FfDB, CiO-ECiO. Ensemble of heuristics further improves performance. No numeric metrics provided in excerpt.",
            "performance_without_curriculum": "Qualitative: Outperforms random/no-curriculum baseline (NC) in accuracy and speeds learning on easier questions; exact numbers not provided.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Individual heuristics compared against SPL and each other; Ensemble &gt; individual heuristics; CiO and CiO-ECiO among top heuristics; ECiO similar to SPL(hard). Efficiency trade-offs noted: GO and CiO require retraining (expensive); M2, ECiO, CA, FfDB require inference only (cheaper).",
            "transfer_generalization": null,
            "key_findings": "Simple heuristics can match or exceed SPL on QA tasks and are computationally cheaper; choice of heuristic affects efficiency and final performance; ensemble of heuristics gives consistent gains; heuristics without diversity can still fail for memory networks due to memory restructuring issues.",
            "uuid": "e1520.1"
        },
        {
            "name_short": "E&E",
            "name_full": "Explore and Exploit (E&E) — Curriculum with Diversity",
            "brief_description": "A curriculum extension that mixes easy examples with strategically chosen diverse (dissimilar) examples to prevent representational overfitting to easy samples; diversity measured as angular distance between induced feature hyperplanes and combined with curriculum objective via a convex combination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Alignment-based QA models; QANTA; Memory Networks",
            "agent_description": "Same QA agents; E&E is applied as a selection modifier to ensure example diversity while preserving easy-first ordering.",
            "agent_size": null,
            "environment_name": "Science QA (grade 3-5 using textbooks/Wikipedia); MCTest; Weston memory tasks; Quiz-bowl",
            "environment_description": "Textual QA benchmarks where selected training examples are added progressively; diversity computed in model feature space (angle between feature vectors induced by examples).",
            "procedure_type": "science question answering and textual reasoning (aimed at improving representation learning and handling harder examples)",
            "procedure_examples": "Mixing in grade 4/5 questions early together with grade 3 examples to avoid representation collapse; selecting diverse examples by cosine-angle measure between feature vectors of examples.",
            "compositional_structure": "Not explicitly a compositional curriculum; diversity encourages exposure to varied substructures and prevents narrow representational specialization.",
            "uses_curriculum": true,
            "curriculum_name": "difficulty-based curriculum augmented with diversity (Explore & Exploit)",
            "curriculum_description": "At each selection step the algorithm optimizes a convex combination of the curriculum objective (favor easy examples) and the sum of angular distances between candidate example and already-selected examples Q0. The convex weight is tuned on development data; small non-zero weight encourages injecting harder/diverse examples early.",
            "curriculum_ordering_principle": "primarily task difficulty (easy → hard) combined with explicit diversity (feature-space angular distance) to ensure varied training signal",
            "task_complexity_range": "Science QA grade 3 → grade 5; memory tasks with varying reasoning complexity",
            "performance_with_curriculum": "Qualitative: E&E provided further improvements across experiments, and was crucial for making curriculum learning effective for deep models (especially memory networks) — it sharply reduced parameter-change oscillations and prevented catastrophic memory restructuring; Ensemble+E&E showed faster gains on easier grades and improved final accuracy across grades. Exact numbers not provided in the excerpt.",
            "performance_without_curriculum": "Qualitative: Without E&E, deep models (especially memory networks) exhibited poor or negative gains from naive curricula (abrupt memory restructuring), and final accuracy was lower than when diversity was used.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "E&E improves SPL and heuristic approaches when added; as the diversity-interpolant increases (0→1) more diverse questions selected and relative change in parameters decreases; small diversity weight (e.g., 0.1) sharply reduces parameter changes for neural models and yields better stability and accuracy.",
            "transfer_generalization": null,
            "key_findings": "Incorporating a small proportion of diverse/harder examples (E&E) alongside easy examples is critical for deep models: it stabilizes memory/network parameters, avoids costly restructuring when harder examples arrive, and leads to consistent accuracy improvements; E&E is especially important for memory networks where naive curricula hurt performance.",
            "uuid": "e1520.2"
        },
        {
            "name_short": "Ensemble+IW",
            "name_full": "Ensemble Heuristics with Importance Weighting",
            "brief_description": "Combines multiple heuristics by scoring candidate questions relative to average heuristic scores (ensemble) and applies importance-weighting to correct for selection bias introduced by non-uniform sampling during curriculum selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Alignment-based QA models; QANTA; Memory Networks",
            "agent_description": "Same QA agents; ensemble scoring ranks candidate questions by aggregated heuristic signal; importance-weighting rescales losses/gradients or regularization terms to obtain unbiased generalization estimates under biased selection distribution.",
            "agent_size": null,
            "environment_name": "Science QA (grade 3-5); MCTest; Weston memory tasks; Quiz-bowl",
            "environment_description": "Text QA datasets where training is progressively built by selecting batches of examples according to ensemble scores and applying sample weights during optimization to compensate for non-i.i.d. sampling.",
            "procedure_type": "science/question answering / textual reasoning",
            "procedure_examples": "Selecting questions from mixed-grade science QA using ensemble ranking; weighting gradients or C_i terms to correct selection bias.",
            "compositional_structure": "Not explicitly compositional; ensemble and IW operate at example selection and loss-weighting level.",
            "uses_curriculum": true,
            "curriculum_name": "ensemble-of-heuristics curriculum with importance-weighting correction",
            "curriculum_description": "Ensemble computes ratio of each heuristic's score for a candidate to the average score across remaining examples and picks the highest ratio; importance-weighting constructs a biased sampling distribution D and assigns weights w(q,a) = p_D(q,a)/p_D(q,a) (note: paper describes reweighting to compute unbiased generalization estimates) and adjusts loss/regularization accordingly (multiply gradients or C_i).",
            "curriculum_ordering_principle": "aggregated heuristic signals (expected model impact, loss/confidence, distance from boundary) with bias correction via importance weights",
            "task_complexity_range": "Grade 3→5 science QA; variety of QA difficulty levels",
            "performance_with_curriculum": "Qualitative: Ensemble significantly outperforms individual heuristics; importance-weighting and E&E provide further improvements. Ensemble+E&E used in reported figures shows aggressive curriculum progression and improved grade-wise accuracy. Exact numeric metrics absent from excerpt.",
            "performance_without_curriculum": "Qualitative: Single heuristics or no curriculum (NC) yield slower learning and lower final accuracy; ensemble and IW yield consistent improvements over NC.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Ensemble &gt; individual heuristics; adding IW and E&E to ensemble further improves performance and stability, particularly for deep models.",
            "transfer_generalization": null,
            "key_findings": "Aggregating multiple selection heuristics into an ensemble yields stable, higher gains than any single heuristic; importance-weighting is a practical correction for sample-selection bias introduced by active selection; combining ensemble, IW, and E&E gives the best empirical results across most QA settings in the study.",
            "uuid": "e1520.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-paced learning for latent variable models",
            "rating": 2,
            "sanitized_title": "selfpaced_learning_for_latent_variable_models"
        },
        {
            "paper_title": "Self-paced learning with diversity",
            "rating": 2,
            "sanitized_title": "selfpaced_learning_with_diversity"
        },
        {
            "paper_title": "Easy samples first: Self-paced reranking for zero-example multimedia search",
            "rating": 1,
            "sanitized_title": "easy_samples_first_selfpaced_reranking_for_zeroexample_multimedia_search"
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 2,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "Memory networks",
            "rating": 2,
            "sanitized_title": "memory_networks"
        },
        {
            "paper_title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "rating": 1,
            "sanitized_title": "towards_aicomplete_question_answering_a_set_of_prerequisite_toy_tasks"
        },
        {
            "paper_title": "A neural network for factoid question answering over paragraphs",
            "rating": 1,
            "sanitized_title": "a_neural_network_for_factoid_question_answering_over_paragraphs"
        }
    ],
    "cost": 0.01336625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Easy Questions First? A Case Study on Curriculum Learning for Question Answering
Association for Computational LinguisticsCopyright Association for Computational LinguisticsAugust 7-12, 2016. 2016</p>
<p>Mrinmaya Sachan mrinmays@cs.cmu.edu 
School of Computer Science
Carnegie Mellon University</p>
<p>Eric P Xing epxing@cs.cmu.edu 
School of Computer Science
Carnegie Mellon University</p>
<p>Easy Questions First? A Case Study on Curriculum Learning for Question Answering</p>
<p>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsAugust 7-12, 2016. 2016
Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems. Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning. Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively. We introduce a number of heuristics that improve upon selfpaced learning. Then, we argue that incorporating easy, yet, a diverse set of samples can further improve learning. We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them.</p>
<p>Introduction</p>
<p>A key challenge in building an intelligent agent is in modeling the incrementality and the cumulative nature of human learning (Skinner, 1958;Peterson, 2004;Krueger and Dayan, 2009). Children typically learn grade by grade, progressing from simple concepts to more complex ones. Given a complex set of concepts, it is often the case that some concepts are easier than others. Some concepts can even be prerequisite to learning other concepts. Hence, evolving a useful curriculum where easy concepts are presented first and more complex concepts are gradually introduced can be beneficial for learning.</p>
<p>We explore methods for learning a curriculum in the context of non-convex models for question answering. Curriculum learning (CL) (Bengio et al., 2009) and self-paced learning (SPL) (Kumar et al., 2010) have been recently introduced in machine learning literature. However, their usefulness in the context of NLP tasks such as QA has not been studied so far. The main challenge in learning a curriculum is that it requires the identification of easy and hard concepts in the given training dataset. However, in real-world applications, such a ranking of training samples is difficult to obtain. Furthermore, a human judgement of 'easiness' of a task might not correlate with what is easy for the algorithm in the feature and hypothesis space employed for the given application. SPL combines the selection of the curriculum and the learning task in a single objective. The easiness of a question in self-paced learning is defined by its local loss. We propose and study other heuristics that define a measure of easiness and learn the curriculum by selecting samples using this measure. These heuristics are similar to those used in active learning, but with one key difference. In curriculum learning, all the training examples and labels are already known, which is not the case in active learning. Our experiments show that these heuristics work well in practice. While the strategy of learning from easy questions first and then gradually handling harder questions is supported by many cognitive scientists, others (Cantor, 1946) argue that it is also important to expose the learning to diverse (even if sometimes harder) examples. We argue that the right curriculum should not only be arranged in the increasing order of difficulty but also introduce the learner to sufficient number of diverse examples that are sufficiently dissimilar from what has already been introduced to the learning process. We showed that the above heuristics when coupled with diversity lead to significant improvements.</p>
<p>We provide empirical evaluation on four QA models: (a) an alignment-based approach (Sachan et al., 2015) for machine comprehension -a reading comprehension task (Richardson et al., 2013) with a set of questions and associated texts, (b) an alignment-based approach (Sachan et al., 2016) for a multiple-choice elementary science test (Clark and Etzioni, 2016), (c) QANTA (Iyyer et al., 2014) -a recursive neural network for answering quiz bowl questions, and (d) memory networks (Weston et al., 2014) -a recurrent neural network with a long-term memory component for answering 20 pre-defined tasks for machine comprehension. We show value in our approaches for curriculum learning on all these settings. Our paper has the following contributions:</p>
<ol>
<li>In our knowledge, this is the first application of curriculum learning to the task of QA and one of the first in NLP. We hope to make the NLP and ML communities aware of the benefits of CL for non-convex optimization. 2. We perform an in-depth analysis of SPL, and propose heuristics which offer significant improvements over SPL; the state-of-the-art in curriculum learning. 3. We stress on diversity of questions in the curriculum during learning and propose a method that learns a curriculum while capturing diversity to gain more improvements.</li>
</ol>
<p>Problem Setting for QA</p>
<p>For each question q i ∈ Q, let A i = {a i1 , . . . , a im } be the set of candidate answers to the question. Let a * i be the correct answer. The candidate answers may be pre-defined, as in multiple-choice QA, or may be undefined but easy to extract with a high degree of confidence (e.g., by using a pre-existing system). We want to learn a function f : (q, K) → a that, given a question q i and background knowledge K (texts/resources required to answer the question), outputs an answerâ i ∈ A i . We consider a scoring function S w (q, a; K) (with model parameters w) and a prediction rule f w (q i ) = a i = arg max a ij ∈A i S w (q i , a ij ; K). Let ∆(â i , a * i ) be the cost of giving a wrong answer. We consider the empirical risk minimization (ERM) framework given a loss function L and a regularizer Ω:
min w q i ∈Q L w (a * i , f w (q i ); K) + Ω(w)(1)</p>
<p>QA Models</p>
<p>The field of QA is quite rich. Solutions proposed have ranged from various IR based approaches that treat this as a problem of retrieval from existing knowledge bases or perform inference using a large corpus of unstructured texts by learning a similarity between the question and a set of candidate answers (Yih et al., 2013). A comprehensive review of QA is out of scope of this paper. So we point the interested readers to Jurafsky and Martin (2000), chapter 28 for a more comprehensive review. In this paper, we will explore curriculum learning in the context of non-convex models for QA. The models will be (1) latent structural SVM (Yu and Joachims, 2009) based solutions for standardized question-answering tests and (2) deep learning models (Iyyer et al., 2014;Weston et al., 2014) for QA. Recently, researchers have proposed standardized tests as 'drivers for progress in AI' (Clark and Etzioni, 2016). Some example standardized tests are reading comprehensions (Richardson et al., 2013), algebra word problems (Kushman et al., 2014), geometry problems (Seo et al., 2014), entrance exams (Fujita et al., 2014;Arai and Matsuzaki, 2014), etc. These tests are usually in the form of question-answers and focus on elementary learning. The idea of learning the curriculum could be especially useful in the context of standardized tests. Standardized tests (Clark and Etzioni, 2016) are implicitly incremental in nature, covering various levels of difficulty. Thus they are rich sources of data for building systems that learn incrementally. These datasets can also help us understand the shaping hypothesis as we can use them to verify if easier questions are indeed getting picked by our incremental learning algorithm before harder questions.</p>
<p>On the other hand, deep learning models (Le-Cun et al., 2015) have recently shown good performance in many standard NLP and vision tasks, including QA. These models usually learn representations of data and the QA model jointly. The models use a cascade of many layers of nonlinear processing units, leading to a highly non-convex model and a large parameter space. This renders these models susceptible to local-minima. Hence, the idea of learning the curricula is also very useful in the context of deep-learning models, as the technique of processing questions in the increasing order of difficulty often leads to better minima Text: … Natural greenhouse gases include carbon dioxide, methane, water vapor, and ozone ... CFCs and ! some other man-made compounds are also greenhouse gases … Hypothesis: The important greenhouse gases are Carbon dioxide , Methane, Ozone and CFC Q: What are the important greenhouse gases? ! A: Carbon dioxide, Methane, Ozone and CFC Figure 1: Alignment structure for an example question from the science QA dataset. The question and answer candidate are combined to generate a hypothesis sentence. Then alignments (shown by red lines) are found between the hypothesis and the appropriate snippet in the texts.</p>
<p>(as shown in our results).</p>
<p>Alignment Based Models</p>
<p>Alignment based models for QA (Yih et al., 2013;Sachan et al., 2015;Sachan et al., 2016) cast QA as a textual entailment problem by converting each question-answer candidate pair (q i , a ij ) into a hypothesis statement h ij . For example, the question "What are the important greenhouse gases?" and answer candidate "Carbon dioxide, Methane, Ozone and CFC" in Figure 1 can be combined to achieve a hypothesis "The important greenhouse gases are Carbon dioxide , Methane, Ozone and CFC.". A set of question matching/rewriting rules are used to achieve this transformation. These rules match the question into one of a large set of pre-defined templates and apply a unique transformation to the question and answer candidate to achieve the hypothesis statement. For each question q i , the QA task thereby reduces to picking the hypothesisĥ i that has the highest likelihood among the set of hypotheses h i = {h i1 , . . . , h im } generated for that question of being entailed by a body of relevant texts. The body of relevant texts can vary for each instance of the QA task. For example, it could be just the passage in a reading comprehension task, or a set of science textbooks in the science QA task. Let h * i ∈ h i be the correct hypothesis. The model considers the quality of word alignment from a hypothesis h ij (formed by combining question-answer candidates (q i , a ij )) to snippets in the textbooks as a proxy for the evidence. The alignment depends on: (a) snippet from the relevant texts chosen to be aligned to the hypothesis and (b) word alignment from the hypothesis to the snippet. The snippet from the texts to be aligned to the hypothesis is determined by picking a subset of sentences in the texts. Then each hypothesis word is aligned to a unique word in the snippet. See Figure 1 for an illustration. The choice of snippets composed with the word alignment is latent. Let z ij represent the latent structure for the question-answer candidate pair (q i , a i,j ). A natural solution is to treat QA as a problem of ranking the hypothesis set h i such that the correct hypothesis is at the top of this ranking. Hence, a scoring function S w (h, z) is learnt such that the score given to the correct hypothesis h * i and the corresponding latent structure z * i is higher than the score given to any other hypothesis and its corresponding latent structure. In fact, in a max-margin fashion, the model learns the scoring function such
that S w (h * i , z * i ) &gt; S w (h ij , z ij ) + ∆(h * i , h ij ) − ξ i for all h j ∈ h \ h * for some slack ξ i .
This can be formulated as the following optimization problem:
min ||w|| 1 2 ||w|| 2 2 + C i ξi s.t. Sw(h * i , z * i ) ≥ max z ij Sw(hij, zij) + ∆(h * i , hij) − ξi It is intuitive to use 0-1 cost, i.e. ∆(h * i , h ij ) = 1(h * i = h ij )
If the scoring function is convex then this objective is in concave-convex form and can be minimized by the concave-convex programming procedure (CCCP) (Yuille and Rangarajan, 2003). The scoring function is assumed to be lin- Sachan et al. (2015) and Sachan et al. (2016) for details).
ear: S w (h, z) = w T ψ(h, z). Here, ψ(h, z) is a task-dependent feature map (see</p>
<p>Deep Learning Models</p>
<p>We briefly review two neural network models for QA - Iyyer et al. (2014) and Weston et al. (2014). QANTA: QANTA (Iyyer et al., 2014) answers quiz bowl questions using a dependency tree structured recursive neural network. It combines predictions across sentences to produce a question answering neural network with trans-sentential averaging. The model is optimized using AdaGrad (Duchi et al., 2011). In quiz bowl, questions typically consist of four to six sentences and are associated with factoid answers. Every sentence in the question is guaranteed to contain clues that uniquely identify its answer, even without the context of previous sentences 1 . Recently, QANTA had beaten the well-known Jeopardy! star Ken Jennings at an exhibition quiz bowl contest. Memory Networks: Memory networks (Weston et al., 2014) are essentially recurrent neural networks with a long-term memory component. The memory can be read and written to, and can be used for prediction. The memory can be seen as acting like a dynamic knowledge base. The model is trained using a margin ranking loss and stochastic gradient descent. It was evaluated on a set of synthetic QA tasks. For each task, a set of statements were generated by a simulation of 4 characters, 3 objects and 5 rooms using an automated grammar with characters moving around, picking up and dropping objects are given, followed by a question whose answer is typically a single word 2 .</p>
<p>Curriculum Learning</p>
<p>Studies in cognitive science (Skinner, 1958;Peterson, 2004;Krueger and Dayan, 2009) have shown that humans learn much better when the training examples are not randomly presented but organized in increasing order of difficulty. The idea of shaping, which consists of training a machine learning algorithm with a curriculum was first introduced by (Elman, 1993) in the context of grammatical structure learning using a recurrent connectionist network. This idea also lent support for the much debated Newport's "less is more" hypothesis (Goldowsky and Newport, 1993;Newport, 1990) that child language acquisition is aided, rather than hindered, by limited cognitive resources. Curriculum learning (Bengio et al., 2009) is a recent idea in machine learning, where a curriculum is designed by ranking samples based on manually curated difficulty measures. These measurements are usually not known in real-world scenarios, and are hard to elicit from humans.</p>
<p>Self-paced Learning</p>
<p>Self-paced learning (SPL) (Kumar et al., 2010;Jiang et al., 2014a; reformulates curriculum learning as an optimization problem by jointly modeling the curriculum and the task at hand. Let v ∈ [0, 1] |Q| be the weight vector that models the weight of the sample questions in the curriculum. The SPL model includes a weighted loss term on all samples and an additional self-paced regularizer imposed on sample weights v. SPL formulation for the ERM framework described in eq 1 can be rewritten as:
min w,v∈[0,1] |Q| q i ∈Q v i L w (a * i , f w (q i ); K) + g(v i , λ)</p>
<p>+Ω(w)</p>
<p>2 Refer to Table 1 in (Weston et al., 2015) for examples</p>
<p>The problem usually has closed-form solution with respect to v (described later; lets call the solution v * (λ; L) for now). g(v, λ) is usually called the self-paced regularizer with the "age" or "pace" parameter λ. g is convex with respect to v ∈ [0, 1] |Q| . Furthermore, v(λ; L) is monotonically decreasing with respect to L, and
lim L→0 v * (λ; L) = 1 and lim L→∞ v * (λ; L) = 0.
This means that the model inclines to select easy samples (with smaller losses) in favor of complex samples (with larger losses). Finally, v * (λ; L) is monotonically increasing with respect to λ, and lim λ→0 v * (λ; L) = 0 and lim λ→∞ v * (λ; L) ≤ 1. This means that when the model "ages" (i.e. the age parameter λ gets larger), it tends to incorporate more, probably complex samples to train a 'mature' model.</p>
<p>Four popular self-paced regularizers in the literature (Kumar et al., 2010;Jiang et al., 2014a; are hard, soft logarithmic, soft linear and mixture. These SP-regularizers, summarized with corresponding closed form solutions for v are shown in Table 1. Hard weighting is usually less appropriate as it cannot discriminate the importance of samples. However, soft weighting assigns real-valued weights and reflects the latent importance of samples in training. The soft linear regularizer linearly weighs samples with respect to their losses and the soft logarithmic penalizes the weight logarithmically. Mixture weighting combines both hard and soft weighting schemes. We can solve the model in the SPL regime by iteratively updating v (closed form solution for v is shown in Table 1) and w (by CCCP, AdaGrad or SGD), and gradually increasing the age parameter λ to let harder and harder problems in.</p>
<p>Since its inception, variations of SPL such as self-paced re-ranking (Jiang et al., 2014a), selfpaced learning with diversity (Jiang et al., 2014b), self-paced multiple-instance learning (Zhang et al., 2015) and self-paced curriculum learning  have been proposed. The techniques have been shown to be useful in some computer vision tasks (Lee and Grauman, 2011;Kumar et al., 2011;Tang et al., 2012;Supancic and Ramanan, 2013;Jiang et al., 2014a). SPL is different from active learning (Settles, 1995) in the sense that the training examples (and labels) are already provided and the solution only orders the examples to achieve a better solution. On the other hand, active learning tries to interactively query Regularizer g(v; λ) v * (λ; L) the user (or another information source) to achieve a better model with few queries. Curriculum learning is also related to teaching dimension (Khan et al., 2011) which studies the strategies that humans follow as they teach a target concept to a robot by assuming a teaching goal of minimizing the learner's expected generalization error at each iteration. One can also think of curriculum learning as an approach for achieving a better local optimum in non-convex problems.
Hard −λv 1, if L ≤ λ 0, o/w Soft Linear λ( 1 2 v 2 − v) − L λ + 1, if L ≤ λ 0, otherwise Soft Logarithmic q i ∈Q (1 − λ)vi − (1−λ) v i log(1−λ) log(L+1−λ) log(1−λ) , if L ≤ λ 0, o/w Mixed γ 2 v+ γ λ        1, if L ≤ λγ λ+γ 2 0, if L ≥ λ 2 γ 1 √ L − 1 λ , o/w</p>
<p>Improved Curriculum Learning Heuristics</p>
<p>SPL selects questions based on the local loss term of the question. This is not the only way to define 'easiness' of the question. Hence, we suggest some other heuristics for selecting the order of questions to be presented to our learning algorithm. The heuristics select the next question q i ∈ Q \ Q 0 given the current model (M) and the set of questions already presented for learning (Q 0 ). We assume access to a minimization oracle (CCCP/AdaGrad/SGD) for the QA models. We explore the following heuristics: 1) Greedy Optimal (GO): The simplest and greedy optimal heuristic (Schohn and Cohn, 2000) would be to pick a question q i ∈ Q \ Q 0 which has the minimum expected effect on the model. The expected effect on adding q i can be written as:
a ij ∈A i p(a * i = a ij ) q j ∈Q 0 ∪q i E L w (a * j , f w (q j ); K) .
p(a * i = a ij ) can be estimated by normalizing S w (q, a; K).
q j ∈Q 0 ∪q i E L w (a * j , f w (q j ); K) can
be estimated by retraining the model on Q 0 ∪ q i .</p>
<p>2) Change in Objective (CiO): Choose the question q i ∈ Q \ Q 0 that causes the smallest increase in the objective. If there are multiple questions with the smallest increase in objective, pick one of them randomly.</p>
<p>3) Mini-max (M 2 ): Chooses question q i ∈ Q\Q 0 that minimizes the regularized expected risk when including the question with the answer candidate a ij that yields the maximum error. q i = arg min
q i ∈Q\Q 0 max a ij ∈A i L w (a ij , f w (q i ); K)</p>
<p>4) Expected Change in Objective (ECiO):</p>
<p>In this greedy heuristic, we pick a question q i ∈ Q \ Q 0 which has the minimum expected effect on the model. The expected effect can be written as
a ij ∈A i p(a * i = a ij ) × E [L w (a * i , f w (q i ); K)].
Here, p(a * i = a ij ) can be achieved by normalizing S w (q, a; K) and E [L w (a * i , f w (q i ); K)] can be estimated by running inference for q i .</p>
<p>4) Change in Objective-Expected Change in</p>
<p>Objective (CiO -ECiO): We pick a question q i ∈ Q \ Q 0 which has the minimum value of the difference between the change in objective and the expected change in objective. Intuitively, the difference represents how much the model is surprised to see this new question. 5) Correctly Answered (CA): Pick a question q i ∈ Q \ Q 0 which is answered by the model M with the minimum cost ∆(â i , a * i ). If there are multiple questions with minimum cost, pick one of them randomly.</p>
<p>6) Farthest from Decision Boundary (FfDB):</p>
<p>This heuristic applies for latent structural SVMs only. Here, we choose the question q i ∈ Q \ Q 0 whose predicted answerâ i is farthest from the decision boundary: max z * w T φ(q i , a * , z * , K) = max z w T φ(q,â,ẑ, K) + ∆(â, a * ).</p>
<p>Timing Considerations:</p>
<p>A key consideration in applying the above heuristics is efficiency as the QA models considered (latent structural SVM and deep learning) are compu-tationally expensive. Among our selection strategies, GO and CiO require updating the model, M 2 , ECiO, CA and FfDB require performing inference on the candiate questions, while CiO -ECiO requires both retraining as well as inference. Consequently, M 2 , ECiO, CA and FfDB are most efficient. We can also gain considerable speed-up by picking questions in batches. This results in significant speed-up with small loss in accuracy. We will discuss the batch question selection setup in more detail in our experiments.</p>
<p>Smarter Selection Strategies:</p>
<p>We further describe some improvements to the above selection strategies: 1) Ensemble Strategy: In this strategy, we combine all of the above heuristics into an ensemble. The ensemble computes the ratio of the score of the suggested question pick and the average score over remaining Q \ Q 0 questions for all the heuristics and picks the question with the highest ratio. As we will see in our results, this ensemble works well in practice.</p>
<p>2) Importance-Weighting (IW): Importance weighting is a common technique in active learning literature (Tong and Koller, 2002;Beygelzimer et al., 2009;Beygelzimer et al., 2010), which mitigates the problem that if we query questions actively instead of selecting them uniformly at random, the training (and test) question sets are no longer independent and identically distributed (i.i.d.). In other words, the training set will have a sample selection bias that can impair prediction performance. To mitigate this, we propose to sample questions from a biased sample distribution D. To achieve D, we introduce the weighted loss L w (a, f w (q); K) = w(q, a) × L w (a, f w (q); K) where w(q, a) is the weighting function w(q, a) = p D (q,a) p D (q,a) which represents how likely it is to observe (q, a) under D compared to D. In this setting, we can show that the generalization error under D is the same as that under D:
E (q,a)∼ D L w (a, f w (q); K) = (q,a) p D (q, a) p D (q, a) p D (q, a) L w (a, f w (q); K)d(q, a) = (q,a) p D (q, a)L w (a, f w (q); K)d(q, a) = E (q,a)∼D [L w (a, f w (q); K)]
Thus, given appropriate weights w(q, a), we modify our loss-function in order to compute an unbiased estimator of the generalization error. Each question-answer is assigned with a non-negative weight. For latent structural SVMs, one can minimize the weighted loss by simply multiplying the corresponding regularization parameter C i with a corresponding term. In neural networks, this is simply achieved by multiplying the gradients with the corresponding weights. The weights can be set by an appropriate heuristic, e.g. proportional to distance from the decision boundary.</p>
<p>Incorporating Diversity with Explore and Exploit (E&amp;E):</p>
<p>The strategy of learning from easy questions first and then gradually handling harder questions is intuitive as it helps the learning process. Yet, it has one key deficiency. Under curriculum learning, by focusing on easy questions first, our learning algorithm is usually not exposed to a diverse set of questions. This is particularly a problem for deeplearning approaches that learn representations during the process of learning. Hence, when a harder question arrives, it is usually hard for the learner to adjust to this new question as the current representation may not be appropriate for the new level of difficulty. This motivates our E&amp;E strategy. The explore and exploit strategy ensures that while we still select easy questions first, we also want to make our selection as diverse as possible.</p>
<p>We define a measure for diversity as the angle between the hyperplanes that the question samples induce in feature space:
∠(φ(q i , a * i , z * i , K), φ(q i , a * i , z * i , K)) = Cosine −1 |φ(q i ,a * i ,z * i ,K)φ(q i ,a * i ,z * i ,K)| ||φ(q i ,a * i ,z * i ,K)||||φ(q i ,a * i ,z * i ,K)|| .
The E&amp;E solution picks the question which optimizes a convex combination of the curriculum learning objective and the sum of angles between the candidate question pick and questions in Q 0 . The convex combination is tuned on the development set. 6 Experiments 6.1 Datasets As described, we study curriculum learning on four different tasks. The first task is question answering for reading comprehensions. We use MCTest-500 dataset (Richardson et al., 2013), a freely available set of 500 stories (300 train, 50 dev and 150 test) and associated questions to evaluate our model. Each story in MCTest has four  Table 2: Accuracy on the test set obtained on the four experiments, comparing results when no curriculum (NC) was learnt, when we use self-paced learning (SPL) with four variations of SP-regularizers, the six heuristics and four improvements proposed by us. Each cell reports the mean±se (standard error) accuracy over 10 repetitions of each experimental configuration.</p>
<p>multiple-choice questions, each with four answer choices. Each question has exactly one correct answer. The second task is science question answering. We use a mix of 855 third, fourth and fifth grade science questions derived from a variety of regional and state science exams 3 for training and evaluating our model. We used publicly available science textbooks available through ck12.org and Simple English Wikipedia 4 as texts required to answer the questions. The model retrieves a section from the textbook or a Wikipedia page (using a lucene index on the sections and Wikipedia pages) by querying for the hypothesis h ij and then aligning the hypothesis to snippets in the document. For QANTA (Iyyer et al., 2014), we use questions from quiz bowl tournaments for training as in Iyyer et al. (2014). The dataset contains 20,407 questions with 2347 answers. For each answer in the dataset, its corresponding Wikipedia page is also provided. Finally, for memory networks (Weston et al., 2014), we use the synthetic QA tasks defined in Weston et al. (2015) (version 1.1 of the dataset). There are 20 different types of tasks that probe different forms of reasoning and deduction. Each task consists of a set of statements, followed by a question whose answer is typically a single word or a set of words. We report mean accuracy 3 http://aristo-public-data.s3.amazonaws.com/AI2-Elementary-NDMC-Feb2016.zip 4 https://dumps.wikimedia.org/simplewiki/20151102/ across these 20 tasks.</p>
<p>Results</p>
<p>We implemented and compared the six selection heuristics ( §5) with the suggested improvements ( §5.2) and self-paced learning ( §4) with the explore and exploit extension for both alignment based models ( §3.1) and two deep learning models ( §3.2). We use accuracy (proportion of test questions correctly answered) as our evaluation metric. In all our experiments, we begin with zero training data (random initialization). For alignment based models, we select 1 percent of training set questions after every epoch (an epoch is defined as a single pass through the current training set by the optimization oracle) and add them to the training set based on the selection strategy. For deep learning models, we discovered that the learning was a lot slower so we added 0.1 percent of new training set questions after every epoch. Hyper parameters of the alignment based models and the deep learning models were fixed to the corresponding values proposed in their corresponding papers (pre-tuned for the optimization oracle on a held-out development set). All the results reported in this paper are averaged over 10 runs of each experiment. Table 5.3 reports test accuracies obtained on all the QA tasks, comparing the aforementioned proposals against corresponding models when curriculum learning is not used. We can observe from these results that variants of SPL (and E&amp;E) as well as the heuristics (and improvements) lead to improvements in the final test accuracy for both alignment-based models and QANTA.</p>
<p>The surprising ineffectiveness of the heuristics and SPL for memory networks essentially boils down to the abrupt restructure of memory the model has to do for curriculum learning. We provide support for this argument in Figure 2 which plots the net relative change in all the parameters W until convergence  Table 5.3 , we can observe that the choice of the SP-regularizer is important. The soft regularizers perform better than the hard regularizer. The mixed regularizer (with mixture weighting) performs even better. We can also observe that all the heuristics work as well as SPL, despite being a lot simpler. The heuristics arranged in increasing order of performance are: CA, M 2 , ECiO, GO, CiO, FfDB and CiO-ECiO,. The differences between the heuristics are larger for alignment-based models and smaller for deep learning models. The ECiO heuristic has very similar performance to SPL with hard SP-regularizer. This is understandable as SPL also selects 'easy' questions based on their expected objective value. The Ensemble is a significant improvement over the individual heuristics. Importance weighting (IW) and the explore and exploit strategies (E&amp;E) provide further improvements. E&amp;E is crucial to making curriculum learning work for deep learning approaches as described before. Motivated by the success of E&amp;E, we also extended it to SPL 5 by tuning a convex combination as before. E&amp;E provides improvements across all the experiments for all the SPL experiments. While, the strategy is more important for memory networks, it leads to improvements on all the tasks.</p>
<p>In order to understand the curriculum learning process and to test the hypothesis that the procedure indeed selects easy questions first, successively moving on to harder questions, we plot the number of questions of grade 3, 4 and 5 picked by SPL, Ensemble and Ensemble+E&amp;E against the epoch number in Figure 3. We can observe that all the three methods pick more questions from grade 3 initially, successively moving on to more and more grade 4 questions and finally more grade 5 questions. Both Ensemble and Ensemble+E&amp;E are more aggressive at learning this curriculum than SPL. Ensemble becomes too aggressive so E&amp;E, initially increases the number of grade 4 and grade 5 questions received by the learner, thereby incorporating diversity in learning. In order to further the claim that curriculum learning follows the principal of learning simpler concepts first and then learning successively harder and harder concepts, we plot the test accuracy on grade 3, 4 and 5 questions with curriculum learning (CL) -i.e. Ensemble+E&amp;E and without curriculum learning (NC) against the epoch number in Figure 4. Here, we can see that the test accuracy increases for questions in all three grade levels. With curriculum learning, the accuracy on grade 3 questions rises sharply in the beginning. This rise is sharper than the case when curriculum learning is not used. Grade 3 test accuracy for curriculum learning then saturates (saturates earlier compared to the case when curriculum learning is not used). The improvements due to curriculum learning for grade 4 questions mainly occur in epochs 30-140. The final epochs of curriculum learning see greater gain in test accuracy for grade 5 questions over the case when curriculum learning is not used. All these experiments together support the intuition of curriculum learning. The models indeed pick and learn from easier questions first and successively learn from harder and harder questions. We also tried variants of our models where we used curriculum learning on grade 3 questions, followed by grade 4 and grade 5 questions. However, this did not lead to significant improvements. Perhaps, this is because questions that are easy for humans may not always correspond to what is easy for our algorithms. Characterizing what is easy for algorithms and how it relates to what is easy for humans is an interesting question for future research.</p>
<p>Conclusion</p>
<p>Curriculum learning is inspired by the way humans acquire knowledge and skills: by mastering simple concepts first, and progressing through information with increasing difficulty to grasp more complex topics. We studied self-paced learning, an approach for curriculum learning that expresses the difficulty of a data sample in terms of the value of the objective function and builds the curriculum via a joint optimization framework. We proposed a number of heuristics, an ensemble, and several improvements for selecting the curriculum that improves upon self-paced learning. We stressed on another important aspect of human learningdiversity, that requires that the right curriculum should not only arrange the data samples in increasing order of difficulty but should also introduce the learner to a small number of samples that are sufficiently dissimilar to the samples that have already been introduced to the learning process. We showed that our heuristics when coupled with diversity lead to significant improvements in a number of question answering tasks. The approach is quite general and we hope that this paper will encourage more NLP researchers to explore curriculum learning in their own works.</p>
<p>Figure 2 :
2Relative change in parameters*10 −x where x = 2 for machine comprehension and science QA, 4 for QANTA and memory networks when CL is used.</p>
<p>Figure 3 :Figure 4 :
34Number of grade 3, 4 and 5 questions picked vs Epoch for various CL approaches for Science QA. Test split accuracy on grade 3, 4 and 5 questions picked vs Epoch for Science QA when CL is used/not used.</p>
<p>Table 1 :
1Various SP-regularizers for SPL.</p>
<p>tion every time in order to free space and accommodate the harder example. This process of memory pattern restructuring is difficult to achieve, so it could be the reason for the relatively poor per-formance of naive curriculum learning and SPL strategies. However, as we can see from the previous results, the explore and exploit strategy of mixing in some harder examples avoids the problem of having to abruptly restructure memory patterns. The extra samples of all difficulties prevent the network from utilizing all the memory on the easy examples, thus eliminating the need to restructure memory patterns.From1 
No. of parameters </p>
<p>∞ </p>
<p>e:epoch=1 </p>
<p>||W e+1 −We|| 1 
||We|| 1 </p>
<p>for each </p>
<p>of the four tasks on the model Ensemble+E&amp;E 
against the linear interpolant used to tune the ex-
plore and exploit combination. As the interpolant 
grows from 0 to 1, more and more diverse ques-
tions get selected. We can observe that the change 
in parameters decreases as more diverse questions 
are selected for all the four tasks. Furthermore, 
once we bring in diversity (change the interpolant 
from 0 to 0.1), the relative change in parameters 
drops sharply for both neural network approaches. 
The drop is sharpest for memory networks. Easier 
examples usually require less memory than hard 
examples. Memory networks have no incentive to 
utilize only a fraction of its state for easy exam-
ples. They simply use the entire memory capacity. 
This implies that harder examples appearing later 
require a restructuring of all memory patterns. The 
network needs to change its memory representa-</p>
<p>Refer toFigure 1in(Iyyer et al., 2014) for an example
This is different fromJiang et al. (2014c) which encourages diversity in samples across groups. On the other hand, we encourage diversity in feature space.
AcknowledgmentsWe thank the anonymous reviewers, along with Emmanouil A. Platanios and Snigdha Chaturvedi for their valuable comments and suggestions that helped improve the quality of this paper. This work was supported by the following research grants: NSF IIS1218282, NSF IIS1447676 and AFOSR FA95501010247.
The impact of ai on educationcan a robot get into the university of tokyo?. ] Matsuzaki2014, H Noriko, Takuya Arai, Matsuzaki, Proc. ICCE. ICCEand Matsuzaki2014] Noriko H Arai and Takuya Matsuzaki. 2014. The impact of ai on education- can a robot get into the university of tokyo? In Proc. ICCE, pages 1034-1042.</p>
<p>Curriculum learning. [ Bengio, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACM[Bengio et al.2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curricu- lum learning. In Proceedings of the 26th annual in- ternational conference on machine learning, pages 41-48. ACM.</p>
<p>Importance weighted active learning. [ Beygelzimer, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningACM[Beygelzimer et al.2009] Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. 2009. Importance weighted active learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 49-56. ACM.</p>
<p>Agnostic active learning without constraints. [ Beygelzimer, Advances in Neural Information Processing Systems. [Beygelzimer et al.2010] Alina Beygelzimer, John Langford, Zhang Tong, and Daniel J Hsu. 2010. Agnostic active learning without constraints. In Ad- vances in Neural Information Processing Systems, pages 199-207.</p>
<p>Dynamics of learning. Foster and Stewart publishing corporation. Nathaniel Freeman, Cantor , Buffalo, NYNathaniel Freeman Cantor. 1946. Dy- namics of learning. Foster and Stewart publishing corporation, Buffalo, NY.</p>
<p>My computer is an honor student -but how intelligent is it? standardized tests as a measure of ai. [ Clark, Peter Clark, Oren Etzioni, Proceedings of AI Magazine. AI Magazine[Clark and Etzioni2016] Peter Clark and Oren Etzioni. 2016. My computer is an honor student -but how intelligent is it? standardized tests as a measure of ai. In Proceedings of AI Magazine.</p>
<p>Adaptive subgradient methods for online learning and stochastic optimization. The Journal of. [ Duchi, Machine Learning Research. 12[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for on- line learning and stochastic optimization. The Jour- nal of Machine Learning Research, 12:2121-2159.</p>
<p>Learning and development in neural networks: The importance of starting small. Jeffrey L Elman, Cognition. 481Jeffrey L Elman. 1993. Learning and de- velopment in neural networks: The importance of starting small. Cognition, 48(1):71-99.</p>
<p>Overview of todai robot project and evaluation framework of its nlp-based problem solving. [ Fujita, World History. 3636[Fujita et al.2014] Akira Fujita, Akihiro Kameda, Ai Kawazoe, and Yusuke Miyao. 2014. Overview of todai robot project and evaluation framework of its nlp-based problem solving. World History, 36:36.</p>
<p>Modeling the effects of processing limitations on the acquisition of morphology: The less is more hypothesis. [ Goldowsky, ] B N Newport1993, E L Goldowsky, Newport, Proceedings of the 11th West Coast Conference on Formal Linguistics. the 11th West Coast Conference on Formal Linguistics[Goldowsky and Newport1993] B.N. Goldowsky and E.L. Newport. 1993. Modeling the effects of pro- cessing limitations on the acquisition of morphol- ogy: The less is more hypothesis. In Proceedings of the 11th West Coast Conference on Formal Lin- guistics.</p>
<p>A neural network for factoid question answering over paragraphs. Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, Hal Daumé, Iii , Proceedings of Empirical Methods in Natural Language Processing. Empirical Methods in Natural Language ProcessingIyyer et al.2014[Iyyer et al.2014] Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daumé III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of Empirical Methods in Natural Language Process- ing.</p>
<p>Easy samples first: Self-paced reranking for zero-example multimedia search. [ Jiang, Proceedings of the ACM International Conference on Multimedia. the ACM International Conference on MultimediaACM[Jiang et al.2014a] Lu Jiang, Deyu Meng, Teruko Mi- tamura, and Alexander G Hauptmann. 2014a. Easy samples first: Self-paced reranking for zero-example multimedia search. In Proceedings of the ACM In- ternational Conference on Multimedia, pages 547- 556. ACM.</p>
<p>Self-paced learning with diversity. [ Jiang, Advances in Neural Information Processing Systems. [Jiang et al.2014b] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. 2014b. Self-paced learning with diver- sity. In Advances in Neural Information Processing Systems, pages 2078-2086.</p>
<p>Self-paced learning with diversity. [ Jiang, Advances in Neural Information Processing Systems. [Jiang et al.2014c] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. 2014c. Self-paced learning with diver- sity. In Advances in Neural Information Processing Systems, pages 2078-2086.</p>
<p>Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. [ Jiang, Twenty-Ninth AAAI Conference on Artificial Intelligence. Prentice HallSelf-paced curriculum learning[Jiang et al.2015] Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. 2015. Self-paced curriculum learning. In Twenty- Ninth AAAI Conference on Artificial Intelligence. [Jurafsky and Martin2000] Daniel Jurafsky and James H Martin. 2000. Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. Prentice Hall.</p>
<p>How do humans teach: On curriculum learning and teaching dimension. [ Khan, Advances in Neural Information Processing Systems. [Khan et al.2011] Faisal Khan, Bilge Mutlu, and Xiao- jin Zhu. 2011. How do humans teach: On cur- riculum learning and teaching dimension. In Ad- vances in Neural Information Processing Systems, pages 1449-1457.</p>
<p>Flexible shaping: How learning in small steps helps. [ Krueger, A Kai, Peter Krueger, Dayan, Cognition. 1103[Krueger and Dayan2009] Kai A Krueger and Peter Dayan. 2009. Flexible shaping: How learning in small steps helps. Cognition, 110(3):380-394.</p>
<p>Self-paced learning for latent variable models. [ Kumar, Advances in Neural Information Processing Systems. [Kumar et al.2010] M Pawan Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-paced learning for latent variable models. In Advances in Neural Infor- mation Processing Systems, pages 1189-1197.</p>
<p>Learning specific-class segmentation from diverse data. [ Kumar, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE[Kumar et al.2011] M Pawan Kumar, Haithem Turki, Dan Preston, and Daphne Koller. 2011. Learning specific-class segmentation from diverse data. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 1800-1807. IEEE.</p>
<p>Learning to automatically solve algebra word problems. [ Kushman, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics[Kushman et al.2014] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the Annual Meeting of the Associa- tion for Computational Linguistics.</p>
<p>Deep learning. [ Lecun, Nature. 5217553[LeCun et al.2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature, 521(7553):436-444.</p>
<p>Learning the easy things first: Self-paced visual category discovery. [ Lee, Grauman2011] Yong Jae Lee, Kristen Grauman, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE[Lee and Grauman2011] Yong Jae Lee and Kristen Grauman. 2011. Learning the easy things first: Self-paced visual category discovery. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1721-1728. IEEE.</p>
<p>Maturational constraints on language learning. L Elissa, Newport, Cognitive science. 141Elissa L Newport. 1990. Maturational constraints on language learning. Cognitive science, 14(1):11-28.</p>
<p>A day of great illumination: Bf skinner's discovery of shaping. B Gail, Peterson, Journal of the Experimental Analysis of Behavior. 823Gail B Peterson. 2004. A day of great illumination: Bf skinner's discovery of shap- ing. Journal of the Experimental Analysis of Behav- ior, 82(3):317-328.</p>
<p>Mctest: A challenge dataset for the open-domain machine comprehension of text. Richardson, Proceedings of Empirical Methods in Natural Language Processing. Empirical Methods in Natural Language ProcessingEMNLP[Richardson et al.2013] Matthew Richardson, Christo- pher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of Em- pirical Methods in Natural Language Processing (EMNLP).</p>
<p>Learning answer-entailing structures for machine comprehension. Sachan, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics[Sachan et al.2015] Mrinmaya Sachan, Avinava Dubey, Eric P Xing, and Matthew Richardson. 2015. Learning answer-entailing structures for machine comprehension. In Proceedings of the Annual Meet- ing of the Association for Computational Linguis- tics.</p>
<p>Sachan, abs/1602.04375Mrinmaya Sachan, Avinava Dubey, and Eric P. Xing. 2016. Science question answering using instructional materials. CoRR. [Sachan et al.2016] Mrinmaya Sachan, Avinava Dubey, and Eric P. Xing. 2016. Science question answering using instructional materials. CoRR, abs/1602.04375.</p>
<p>Less is more: Active learning with support vector machines. Greg Cohn2000, David Schohn, Cohn, Proceedings of the 17th Annual International Conference on Machine Learning. the 17th Annual International Conference on Machine LearningACM[Schohn and Cohn2000] Greg Schohn and David Cohn. 2000. Less is more: Active learning with support vector machines. In Proceedings of the 17th An- nual International Conference on Machine Learn- ing. ACM.</p>
<p>Diagram understanding in geometry questions. [ Seo, Proceedings of AAAI. AAAI[Seo et al.2014] Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and Oren Etzioni. 2014. Diagram un- derstanding in geometry questions. In Proceedings of AAAI.</p>
<p>Active learning literature survey. Burr Settles, 5211MadisonUniversity of WisconsinBurr Settles. 1995. Active learning lit- erature survey. University of Wisconsin, Madison, 52(55-66):11.</p>
<p>Reinforcement today. F Burrhus, Skinner, American Psychologist. 13394Burrhus F Skinner. 1958. Reinforce- ment today. American Psychologist, 13(3):94.</p>
<p>Self-paced learning for long-term tracking. James Supancic, Deva Ramanan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSupancic and Ramanan2013[Supancic and Ramanan2013] James Supancic and Deva Ramanan. 2013. Self-paced learning for long-term tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2379-2386.</p>
<p>Shifting weights: Adapting object detectors from image to video. [ Tang, Advances in Neural Information Processing Systems. [Tang et al.2012] Kevin Tang, Vignesh Ramanathan, Li Fei-Fei, and Daphne Koller. 2012. Shifting weights: Adapting object detectors from image to video. In Advances in Neural Information Process- ing Systems, pages 638-646.</p>
<p>Support vector machine active learning with applications to text classification. Koller2002] Simon Tong, Daphne Tong, Koller, The Journal of Machine Learning Research. 2[Tong and Koller2002] Simon Tong and Daphne Koller. 2002. Support vector machine active learning with applications to text classification. The Journal of Machine Learning Research, 2:45-66.</p>
<p>Jason Weston, Sumit Chopra, and Antoine Bordes. [ Weston, abs/1410.3916[Weston et al.2014] Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. CoRR, abs/1410.3916.</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. [ Weston, arXiv:1502.05698Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiakthe 51st Annual Meeting of the Association for Computational LinguisticsarXiv preprintQuestion answering using enhanced lexical semantic models[Weston et al.2015] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequi- site toy tasks. arXiv preprint arXiv:1502.05698. [Yih et al.2013] Wentau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question answering using enhanced lexical se- mantic models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</p>
<p>Learning structural svms with latent variables. Joachims2009] Chun-Nam Yu, T Joachims, Proceedings of International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)and Joachims2009] Chun-Nam Yu and T. Joachims. 2009. Learning structural svms with latent variables. In Proceedings of Interna- tional Conference on Machine Learning (ICML).</p>
<p>The concave-convex procedure. [ Yuille And Rangarajan2003, ] A L Yuille, Anand Rangarajan, Neural Comput. [Yuille and Rangarajan2003] A. L. Yuille and Anand Rangarajan. 2003. The concave-convex procedure. Neural Comput.</p>
<p>A self-paced multiple-instance learning framework for co-saliency detection. Zhang, [Zhang et al.2015] Dingwen Zhang, Deyu Meng, Chao Li, Lu Jiang, Qian Zhao, and Junwei Han. 2015. A self-paced multiple-instance learning framework for co-saliency detection. June.</p>
<p>Self-paced learning for matrix factorization. [ Zhao, Proceedings of AAAI. AAAI[Zhao et al.2015] Qian Zhao, Deyu Meng, Lu Jiang, Qi Xie, Zongben Xu, and Alexander G Hauptmann. 2015. Self-paced learning for matrix factorization. In Proceedings of AAAI.</p>            </div>
        </div>

    </div>
</body>
</html>
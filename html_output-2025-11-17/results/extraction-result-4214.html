<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4214 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4214</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4214</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-276618526</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.18791v3.pdf" target="_blank">Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs</a></p>
                <p><strong>Paper Abstract:</strong> The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding&multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4214.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4214.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMEVALDB pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMEVALDB: Semi-automated literature analysis and dataset of extracted LLM evaluation results</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semiautomated pipeline introduced in this paper that uses LLMs to scan arXiv LaTeX sources, identify relevant papers/tables, extract numerical experimental results and prompting attributes, augment records with paper context, generate dataset descriptions, and assemble a continuously updatable structured database (LLMEVALDB) for automated literature analysis of LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLMEVALDB (semi-automated literature-extraction pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-stage pipeline that (1) downloads arXiv LaTeX sources (Jan 2023–Dec 2024), (2) pre-filters tables with keyword heuristics and an Llama-3.1-70B-Instruct filter to find leaderboard-like tables mentioning target proprietary models, (3) applies schema-driven extraction to table LaTeX to produce structured records (dataset name, subset, model, prompting method, number of demonstrations, metric, performance value), (4) augments extracted records by prompting an LLM on selected contextual sections of the paper (captions, surrounding text, BibTeX) to fill or refine attributes, (5) generates dataset/task descriptions either from the LLM's internal knowledge or by extracting from the original dataset paper when the LLM abstains, (6) canonicalizes dataset names via rule-based heuristics, normalizes metrics to standard ranges, de-duplicates and post-processes records, and (7) outputs LLMEVALDB and runs automated analyses (e.g., computing performance deltas across prompting methods). The process relies on schema-driven prompts, context augmentation prompts, and rule-based preprocessing/postprocessing to reduce scope and API cost.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (primary extraction/augmentation); Llama-3.1-70B-Instruct (table filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer Science / Natural Language Processing (empirical ML literature)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Scanned >300k arXiv source papers (Jan 2023–Dec 2024); extracted records from 1,737 table source papers and 2,694 tables yielding 18,127 experimental records across 1,737 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships / statistical patterns and correlations between prompting configurations (e.g., Chain-of-Thought, in-context learning) and performance on benchmark tasks; meta-analytic effect sizes (performance deltas) rather than formal physical/mathematical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples discovered/extracted by the pipeline include: (1) Chain-of-Thought (CoT) shows a large mean improvement on Math tasks (Table 7 Total Results: Mean Δ = 14.6, p = 0.0000) and substantial improvement on Symbolic/Algorithmic tasks (Mean Δ = 8.85, p = 0.0002); (2) In-context learning (ICL) yields modest benefits for math but larger median improvements for Coding and Multimodal tasks (Figure 7); (3) CoT with demonstrations (few-shot CoT) typically outperforms CoT without demonstrations, though CoT's relative improvement over direct prompting remains stable across demonstration counts (Table 8/9; median few-shot CoT − zero-shot CoT ≈ 3.0).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Parsing LaTeX table sources with regex to extract numeric cells and table structure, schema-driven LLM prompts to map cells to target attributes, selective context augmentation using LLM prompts over heuristically filtered paper sections (captions, surrounding paragraphs, BibTeX) to fill/verify attributes, and dataset description generation via LLM or extracting from cited dataset papers when LLM abstains.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human expert annotation of sampled records and descriptions (fields marked correct/incorrect), replication validation by reproducing findings from a prior manual meta-analysis (Sprague et al., 2024), statistical testing (bootstrap tests) on aggregated deltas, and reporting inter-annotator agreement (Cohen's Kappa).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Extraction attribute accuracies reported in Table 3: Dataset Name 95%, Model Name 100%, Prompting Method 86.3%, Number of Few-Shot Examples 95%, Metric 100%, Metric Value 98.8%; Description quality mean = 4.55/5 (Likert). Cohen's Kappa: 0.68 (extraction), 0.57 (description). Cost/time metrics: pipeline processed the corpus in a single day for < $500 vs ~350 human hours for manual extraction of 2,694 tables.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>High extraction accuracy on evaluated fields: metric values correct 98.8%; overall field accuracies around mid-90s% for most structured attributes; description quality averaged 4.55/5 on the sampled set.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limitations reported include incomplete or non-descriptive extracted attributes (e.g., poor prompt descriptions like 'Batch CoT'), difficulty and false negatives in automatic dataset canonicalization, inaccuracies when generating dataset 'collection process', challenges linking descriptions to concrete dataset instances, LLM uncertainty requiring a two-stage description generation (abstain then extract), dependence on LaTeX availability, and possible missing/unreported experimental details in source papers. Pipeline also excludes fine-tuned model results and relies on heuristics that can miss matches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against manual expert extraction (annotators ~7m50s per table; ~350 hours for 2,694 tables), the pipeline reduced manual effort by >93% and produced comparable aggregated scientific findings (replicated Sprague et al., 2024). Compared to prior automated extraction studies the paper claims improved tuple enrichment (prompting attributes and dataset descriptions) but does not present a head-to-head numeric comparison against specific baselines beyond manual extraction and reproduction of prior meta-analysis findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4214.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4214.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar (Asai et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenScholar: Synthesizing scientific literature with retrieval-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented language-model approach cited in this paper that synthesizes content from scientific literature by retrieving documents and using an LM to produce synthesized outputs; noted as limited by the number of documents retrievable at once.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openscholar: Synthesizing scientific literature with retrieval-augmented lms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenScholar (retrieval-augmented literature synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A retrieval-augmented LM pipeline: retrieve a set of relevant documents from a literature corpus and use a language model to synthesize summaries/analyses from the retrieved set. The paper notes that this approach can only process a limited number of documents during retrieval, constraining large-scale literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific literature synthesis / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper; described as limited by retrieval capacity</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Synthesis of findings/patterns from retrieved literature (qualitative/summary-level rather than formal quantitative law extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in this paper; cited as an approach for synthesizing literature rather than extracting numerical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Retrieval-augmented LM: retrieve relevant documents and synthesize via LM prompts (limited number of documents per retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Primary limitation cited: restricted number of documents that can be retrieved/processed together, limiting scalability for large-scale literature analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper; presented as related prior work with different retrieval/processing capacity trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4214.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4214.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCT numeric-extraction (Yun et al., 2024 ref.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatically extracting numerical results from randomized controlled trials with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that uses large language models to automatically extract numerical results from randomized controlled trial (RCT) publications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatically extracting numerical results from randomized controlled trials with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based numeric extractor for RCTs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described (in citation) as using LLMs to read clinical trial manuscripts and extract numerical outcome results, presumably via table/text parsing and LLM prompts tailored to identify numeric experiment outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Clinical trial literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of numerical experimental results (numeric outcome values) from scientific articles; supports quantitative meta-analysis but not necessarily discovery of formal laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not detailed in this paper beyond the title/description; the referenced work targets numeric extraction from RCTs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Likely LLM prompting over article text/Tables to extract numeric results (citation only; specific method not described here).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed here; cited as related work indicating LLMs' ability to extract numerical results from domain literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4214.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4214.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM leaderboards automation (Şahinuç et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that leverages LLMs to automate construction of leaderboards (benchmark result tables) from scientific publications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based leaderboard construction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an approach to automate the construction of leaderboards by extracting task/dataset/metric/score tuples from papers using LLMs or related automated extraction tools; focuses on automating performance tracking across literature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP literature / benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper; cited as prior relevant work.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of numeric benchmark results and assembling leaderboards (empirical comparative relationships across models/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in this paper; referenced as relating to automated leaderboard construction.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLM-guided extraction of leaderboard tuples (task, dataset, metric, numeric score) from tables and text per the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed in this paper; referenced as related prior art that focuses on extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified here; referenced as complementary prior work to the present pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4214.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4214.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Axcell (Kardas et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Axcell: Automatic extraction of results from machine learning papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier automated system for extracting result tuples (task, dataset, metric) from ML papers; cited as prior work focused primarily on improving extraction accuracy for leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Axcell: Automatic extraction of results from machine learning papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Axcell (leaderboard/result extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated extraction pipeline for mining result tuples (task/dataset/metric/score) from ML paper tables and text, emphasized in prior literature as a baseline for automated leaderboard construction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning / NLP literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of numeric benchmark results (empirical leaderboards) rather than abstract quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided here; cited as background.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Automated table/text parsing and extraction (prior work; specifics not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not detailed here; included as historical relevant work on extraction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4214.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4214.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schema-driven table extraction (Bai et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Schemadriven information extraction from heterogeneous tables</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A schema-driven extraction approach cited and used as the conceptual basis for structuring table-to-tuple extraction in the LLMEVALDB pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Schemadriven information extraction from heterogeneous tables</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Schema-driven extraction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach where an explicit schema of target attributes is defined and used to drive extraction from heterogeneous table structures; the LLMEVALDB pipeline implements schema-driven prompts to map table cells into standardized attributes (dataset, model, metric, value, prompting method, number of demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Information extraction from ML literature / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Method applied to the extracted subset (see LLMEVALDB counts); original Bai et al. paper not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not a 'law' extractor per se; enables extraction of structured numerical experimental results for downstream empirical relationship analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>N/A (enables extraction of the numeric records that support meta-analytic findings in LLMEVALDB).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Schema specification + mapping prompts applied to table rows/cells to populate standardized attribute fields.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Human annotation and accuracy checks as reported for LLMEVALDB; schema-driven mapping validated by sampled human evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported separately here beyond the LLMEVALDB extraction accuracies (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Contributed to high field accuracies reported in LLMEVALDB; exact attribution not separately quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Schema design can miss unanticipated attributes or nonstandard table encodings; requires augmentation with paper context for completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Presented as an improvement over naive table parsing because it constrains extraction to a predefined target schema; the pipeline extends it with LLM-based augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning <em>(Rating: 2)</em></li>
                <li>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. <em>(Rating: 2)</em></li>
                <li>Automatically extracting numerical results from randomized controlled trials with large language models <em>(Rating: 2)</em></li>
                <li>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards <em>(Rating: 2)</em></li>
                <li>Schemadriven information extraction from heterogeneous tables <em>(Rating: 2)</em></li>
                <li>Axcell: Automatic extraction of results from machine learning papers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4214",
    "paper_id": "paper-276618526",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "LLMEVALDB pipeline",
            "name_full": "LLMEVALDB: Semi-automated literature analysis and dataset of extracted LLM evaluation results",
            "brief_description": "A semiautomated pipeline introduced in this paper that uses LLMs to scan arXiv LaTeX sources, identify relevant papers/tables, extract numerical experimental results and prompting attributes, augment records with paper context, generate dataset descriptions, and assemble a continuously updatable structured database (LLMEVALDB) for automated literature analysis of LLM behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLMEVALDB (semi-automated literature-extraction pipeline)",
            "system_description": "A multi-stage pipeline that (1) downloads arXiv LaTeX sources (Jan 2023–Dec 2024), (2) pre-filters tables with keyword heuristics and an Llama-3.1-70B-Instruct filter to find leaderboard-like tables mentioning target proprietary models, (3) applies schema-driven extraction to table LaTeX to produce structured records (dataset name, subset, model, prompting method, number of demonstrations, metric, performance value), (4) augments extracted records by prompting an LLM on selected contextual sections of the paper (captions, surrounding text, BibTeX) to fill or refine attributes, (5) generates dataset/task descriptions either from the LLM's internal knowledge or by extracting from the original dataset paper when the LLM abstains, (6) canonicalizes dataset names via rule-based heuristics, normalizes metrics to standard ranges, de-duplicates and post-processes records, and (7) outputs LLMEVALDB and runs automated analyses (e.g., computing performance deltas across prompting methods). The process relies on schema-driven prompts, context augmentation prompts, and rule-based preprocessing/postprocessing to reduce scope and API cost.",
            "model_name": "GPT-4o (primary extraction/augmentation); Llama-3.1-70B-Instruct (table filtering)",
            "model_size": null,
            "scientific_domain": "Computer Science / Natural Language Processing (empirical ML literature)",
            "number_of_papers": "Scanned &gt;300k arXiv source papers (Jan 2023–Dec 2024); extracted records from 1,737 table source papers and 2,694 tables yielding 18,127 experimental records across 1,737 papers.",
            "law_type": "Empirical relationships / statistical patterns and correlations between prompting configurations (e.g., Chain-of-Thought, in-context learning) and performance on benchmark tasks; meta-analytic effect sizes (performance deltas) rather than formal physical/mathematical laws.",
            "law_examples": "Examples discovered/extracted by the pipeline include: (1) Chain-of-Thought (CoT) shows a large mean improvement on Math tasks (Table 7 Total Results: Mean Δ = 14.6, p = 0.0000) and substantial improvement on Symbolic/Algorithmic tasks (Mean Δ = 8.85, p = 0.0002); (2) In-context learning (ICL) yields modest benefits for math but larger median improvements for Coding and Multimodal tasks (Figure 7); (3) CoT with demonstrations (few-shot CoT) typically outperforms CoT without demonstrations, though CoT's relative improvement over direct prompting remains stable across demonstration counts (Table 8/9; median few-shot CoT − zero-shot CoT ≈ 3.0).",
            "extraction_method": "Parsing LaTeX table sources with regex to extract numeric cells and table structure, schema-driven LLM prompts to map cells to target attributes, selective context augmentation using LLM prompts over heuristically filtered paper sections (captions, surrounding paragraphs, BibTeX) to fill/verify attributes, and dataset description generation via LLM or extracting from cited dataset papers when LLM abstains.",
            "validation_approach": "Human expert annotation of sampled records and descriptions (fields marked correct/incorrect), replication validation by reproducing findings from a prior manual meta-analysis (Sprague et al., 2024), statistical testing (bootstrap tests) on aggregated deltas, and reporting inter-annotator agreement (Cohen's Kappa).",
            "performance_metrics": "Extraction attribute accuracies reported in Table 3: Dataset Name 95%, Model Name 100%, Prompting Method 86.3%, Number of Few-Shot Examples 95%, Metric 100%, Metric Value 98.8%; Description quality mean = 4.55/5 (Likert). Cohen's Kappa: 0.68 (extraction), 0.57 (description). Cost/time metrics: pipeline processed the corpus in a single day for &lt; $500 vs ~350 human hours for manual extraction of 2,694 tables.",
            "success_rate": "High extraction accuracy on evaluated fields: metric values correct 98.8%; overall field accuracies around mid-90s% for most structured attributes; description quality averaged 4.55/5 on the sampled set.",
            "challenges_limitations": "Limitations reported include incomplete or non-descriptive extracted attributes (e.g., poor prompt descriptions like 'Batch CoT'), difficulty and false negatives in automatic dataset canonicalization, inaccuracies when generating dataset 'collection process', challenges linking descriptions to concrete dataset instances, LLM uncertainty requiring a two-stage description generation (abstain then extract), dependence on LaTeX availability, and possible missing/unreported experimental details in source papers. Pipeline also excludes fine-tuned model results and relies on heuristics that can miss matches.",
            "comparison_baseline": "Compared against manual expert extraction (annotators ~7m50s per table; ~350 hours for 2,694 tables), the pipeline reduced manual effort by &gt;93% and produced comparable aggregated scientific findings (replicated Sprague et al., 2024). Compared to prior automated extraction studies the paper claims improved tuple enrichment (prompting attributes and dataset descriptions) but does not present a head-to-head numeric comparison against specific baselines beyond manual extraction and reproduction of prior meta-analysis findings.",
            "uuid": "e4214.0",
            "source_info": {
                "paper_title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OpenScholar (Asai et al., 2024)",
            "name_full": "OpenScholar: Synthesizing scientific literature with retrieval-augmented LMs",
            "brief_description": "A retrieval-augmented language-model approach cited in this paper that synthesizes content from scientific literature by retrieving documents and using an LM to produce synthesized outputs; noted as limited by the number of documents retrievable at once.",
            "citation_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms.",
            "mention_or_use": "mention",
            "system_name": "OpenScholar (retrieval-augmented literature synthesis)",
            "system_description": "A retrieval-augmented LM pipeline: retrieve a set of relevant documents from a literature corpus and use a language model to synthesize summaries/analyses from the retrieved set. The paper notes that this approach can only process a limited number of documents during retrieval, constraining large-scale literature synthesis.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Scientific literature synthesis / NLP",
            "number_of_papers": "Not specified in this paper; described as limited by retrieval capacity",
            "law_type": "Synthesis of findings/patterns from retrieved literature (qualitative/summary-level rather than formal quantitative law extraction).",
            "law_examples": "Not provided in this paper; cited as an approach for synthesizing literature rather than extracting numerical laws.",
            "extraction_method": "Retrieval-augmented LM: retrieve relevant documents and synthesize via LM prompts (limited number of documents per retrieval).",
            "validation_approach": "Not detailed in this paper (only cited).",
            "performance_metrics": "Not specified here.",
            "success_rate": null,
            "challenges_limitations": "Primary limitation cited: restricted number of documents that can be retrieved/processed together, limiting scalability for large-scale literature analyses.",
            "comparison_baseline": "Not specified in this paper; presented as related prior work with different retrieval/processing capacity trade-offs.",
            "uuid": "e4214.1",
            "source_info": {
                "paper_title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RCT numeric-extraction (Yun et al., 2024 ref.)",
            "name_full": "Automatically extracting numerical results from randomized controlled trials with large language models",
            "brief_description": "A referenced study that uses large language models to automatically extract numerical results from randomized controlled trial (RCT) publications.",
            "citation_title": "Automatically extracting numerical results from randomized controlled trials with large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-based numeric extractor for RCTs",
            "system_description": "Described (in citation) as using LLMs to read clinical trial manuscripts and extract numerical outcome results, presumably via table/text parsing and LLM prompts tailored to identify numeric experiment outcomes.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Biomedical / Clinical trial literature",
            "number_of_papers": "Not specified in this paper (reference only).",
            "law_type": "Extraction of numerical experimental results (numeric outcome values) from scientific articles; supports quantitative meta-analysis but not necessarily discovery of formal laws.",
            "law_examples": "Not detailed in this paper beyond the title/description; the referenced work targets numeric extraction from RCTs.",
            "extraction_method": "Likely LLM prompting over article text/Tables to extract numeric results (citation only; specific method not described here).",
            "validation_approach": "Not described in this paper.",
            "performance_metrics": "Not reported here.",
            "success_rate": null,
            "challenges_limitations": "Not detailed here; cited as related work indicating LLMs' ability to extract numerical results from domain literature.",
            "comparison_baseline": "Not specified in this paper.",
            "uuid": "e4214.2",
            "source_info": {
                "paper_title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "LLM leaderboards automation (Şahinuç et al., 2024)",
            "name_full": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "brief_description": "A cited work that leverages LLMs to automate construction of leaderboards (benchmark result tables) from scientific publications.",
            "citation_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards.",
            "mention_or_use": "mention",
            "system_name": "LLM-based leaderboard construction",
            "system_description": "Cited as an approach to automate the construction of leaderboards by extracting task/dataset/metric/score tuples from papers using LLMs or related automated extraction tools; focuses on automating performance tracking across literature.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning / NLP literature / benchmarking",
            "number_of_papers": "Not specified in this paper; cited as prior relevant work.",
            "law_type": "Extraction of numeric benchmark results and assembling leaderboards (empirical comparative relationships across models/datasets).",
            "law_examples": "Not provided in this paper; referenced as relating to automated leaderboard construction.",
            "extraction_method": "LLM-guided extraction of leaderboard tuples (task, dataset, metric, numeric score) from tables and text per the cited work.",
            "validation_approach": "Not detailed here.",
            "performance_metrics": "Not reported in this paper.",
            "success_rate": null,
            "challenges_limitations": "Not detailed in this paper; referenced as related prior art that focuses on extraction accuracy.",
            "comparison_baseline": "Not specified here; referenced as complementary prior work to the present pipeline.",
            "uuid": "e4214.3",
            "source_info": {
                "paper_title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Axcell (Kardas et al., 2020)",
            "name_full": "Axcell: Automatic extraction of results from machine learning papers",
            "brief_description": "An earlier automated system for extracting result tuples (task, dataset, metric) from ML papers; cited as prior work focused primarily on improving extraction accuracy for leaderboards.",
            "citation_title": "Axcell: Automatic extraction of results from machine learning papers.",
            "mention_or_use": "mention",
            "system_name": "Axcell (leaderboard/result extractor)",
            "system_description": "Automated extraction pipeline for mining result tuples (task/dataset/metric/score) from ML paper tables and text, emphasized in prior literature as a baseline for automated leaderboard construction.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning / NLP literature",
            "number_of_papers": "Not specified in this paper (cited as prior work).",
            "law_type": "Extraction of numeric benchmark results (empirical leaderboards) rather than abstract quantitative laws.",
            "law_examples": "Not provided here; cited as background.",
            "extraction_method": "Automated table/text parsing and extraction (prior work; specifics not reproduced here).",
            "validation_approach": "Not detailed in this paper.",
            "performance_metrics": "Not reported here.",
            "success_rate": null,
            "challenges_limitations": "Not detailed here; included as historical relevant work on extraction accuracy.",
            "comparison_baseline": "Not specified in this paper.",
            "uuid": "e4214.4",
            "source_info": {
                "paper_title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Schema-driven table extraction (Bai et al., 2023)",
            "name_full": "Schemadriven information extraction from heterogeneous tables",
            "brief_description": "A schema-driven extraction approach cited and used as the conceptual basis for structuring table-to-tuple extraction in the LLMEVALDB pipeline.",
            "citation_title": "Schemadriven information extraction from heterogeneous tables",
            "mention_or_use": "use",
            "system_name": "Schema-driven extraction",
            "system_description": "An approach where an explicit schema of target attributes is defined and used to drive extraction from heterogeneous table structures; the LLMEVALDB pipeline implements schema-driven prompts to map table cells into standardized attributes (dataset, model, metric, value, prompting method, number of demonstrations).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Information extraction from ML literature / NLP",
            "number_of_papers": "Method applied to the extracted subset (see LLMEVALDB counts); original Bai et al. paper not quantified here.",
            "law_type": "Not a 'law' extractor per se; enables extraction of structured numerical experimental results for downstream empirical relationship analysis.",
            "law_examples": "N/A (enables extraction of the numeric records that support meta-analytic findings in LLMEVALDB).",
            "extraction_method": "Schema specification + mapping prompts applied to table rows/cells to populate standardized attribute fields.",
            "validation_approach": "Human annotation and accuracy checks as reported for LLMEVALDB; schema-driven mapping validated by sampled human evaluation in this paper.",
            "performance_metrics": "Not reported separately here beyond the LLMEVALDB extraction accuracies (see Table 3).",
            "success_rate": "Contributed to high field accuracies reported in LLMEVALDB; exact attribution not separately quantified.",
            "challenges_limitations": "Schema design can miss unanticipated attributes or nonstandard table encodings; requires augmentation with paper context for completeness.",
            "comparison_baseline": "Presented as an improvement over naive table parsing because it constrains extraction to a predefined target schema; the pipeline extends it with LLM-based augmentation.",
            "uuid": "e4214.5",
            "source_info": {
                "paper_title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning",
            "rating": 2,
            "sanitized_title": "to_cot_or_not_to_cot_chainofthought_helps_mainly_on_math_and_symbolic_reasoning"
        },
        {
            "paper_title": "Openscholar: Synthesizing scientific literature with retrieval-augmented lms.",
            "rating": 2,
            "sanitized_title": "openscholar_synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "Automatically extracting numerical results from randomized controlled trials with large language models",
            "rating": 2,
            "sanitized_title": "automatically_extracting_numerical_results_from_randomized_controlled_trials_with_large_language_models"
        },
        {
            "paper_title": "Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards",
            "rating": 2,
            "sanitized_title": "efficient_performance_tracking_leveraging_large_language_models_for_automated_construction_of_scientific_leaderboards"
        },
        {
            "paper_title": "Schemadriven information extraction from heterogeneous tables",
            "rating": 2,
            "sanitized_title": "schemadriven_information_extraction_from_heterogeneous_tables"
        },
        {
            "paper_title": "Axcell: Automatic extraction of results from machine learning papers",
            "rating": 1,
            "sanitized_title": "axcell_automatic_extraction_of_results_from_machine_learning_papers"
        }
    ],
    "cost": 0.017027,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs
26 May 2025</p>
<p>Jungsoo Park 
The Hebrew University of Jerusalem</p>
<p>Junmo Kang junmo.kang@gatech.edu 
The Hebrew University of Jerusalem</p>
<p>Gabriel Stanovsky gabriel.stanovsky@mail.huji.ac.il 
Instruction Following Knowledge Multilingual Multimodal Math Reasoning Safety</p>
<p>Alan Ritter alan.ritter@cc.gatech.edu 
The Hebrew University of Jerusalem</p>
<p>Yang Liu 
Dan Iter 
Yichong Xu 
Shuohang Wang </p>
<p>Georgia Institute of Technology</p>
<p>Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs
26 May 20250E713A0F742E0644A00007C34116518FarXiv:2502.18791v3[cs.CL]
The surge of LLM studies makes synthesizing their findings challenging.Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use.Our study presents a semiautomated approach for literature analysis that accelerates data extraction using LLMs.It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEVALDB.We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches.We validate LLMEVALDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding &amp; multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT.Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available.Through LLMEVALDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.</p>
<p>Introduction</p>
<p>The rapid advancement of Large Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Achiam et al., 2023;Touvron et al., 2023;Team et al., 2023;Anthropic, 2024) has led to a proliferation of empirical studies.Recent "surveys of surveys" (Lv et al., 2024;Ruan et al., 2024) highlight the overwhelming growth of LLM publications, surpassing what individual researchers can manually review.The growing number of evaluations-each using different models, datasets, and prompting configurations-makes it challenging to analyze and synthesize findings across studies, underscoring the urgent need for automated analysis.</p>
<p>A manual literature analysis that examines data from multiple independent studies to identify overall trends, offers a solution (Reiter, 2018;Hupkes et al., 2023).However, conducting such an analysis is time-consuming (Yun et al., 2023(Yun et al., , 2024)).For instance, as shown in Fig. 1, understanding the effects of combining Chain-of-Thought (CoT) with in-context examples requires identifying relevant papers, extracting specific experimental results, and aggregating the data.Furthermore, analyses based on existing studies are likely to become quickly outdated in a fast-moving field, motivating the need for automated analyses that can be continuously updated as new results are shared on preprint servers such as arXiv.org.</p>
<p>In this study, we conduct the most extensive quantitative literature analysis of frontier LLMs to date.Our dataset, LLMEVALDB, comprises 18,127 experimental records extracted from 1,737 papers, along with 359 referenced papers describing datasets used in these experiments.LLMEVALDB can also be dynamically updated with minimal human effort to incorporate results from new models or studies as they are published.This comprehensive dataset enables us to present new insights into LLM performance, where previous studies on extracting leaderboards from machine learning papers have primarily focused on improving data extraction accuracy (Kardas et al., 2020;Yun et al., 2024).</p>
<p>To achieve this, we experiment with an automated approach to data extraction that efficiently processes research literature.This approach uses an LLM to scan the arXiv database, identify relevant papers with experimental results on target models, and extract experimental results along with pertinent attributes.This reduces the manual effort needed for surveying and extraction by more than 93% compared to expert manual data extraction.The approach employs schema-driven extraction (Bai et al., 2023) and context augmentation from paper contents to capture essential attributes for a comprehensive understanding of evaluation results.Additionally, the process creates dataset descriptions that summarize key characteristics of the dataset associated with the evaluation record, enhancing LLMEVALDB's potential utility in literature analysis.</p>
<p>Our analysis shows that LLMEVALDB can efficiently replicate a previous manual analysis (Sprague et al., 2024), which found that CoT reasoning primarily improves performance on mathematical and symbolic reasoning benchmarks.We also identify three novel insights from the dataset on prompting configurations.First, in-context learning (ICL) enhances coding &amp; multimodal tasks but offers limited benefits for mathematical tasks compared to a zero-shot setup.Second, our qualitative analysis reveals specific characteristics of datasets that tend to reduce the effectiveness of CoT prompting and ICL.Third, we find that CoT prompting with demonstrations-commonly implemented through ICL-typically yields better performance than CoT without demonstrations.However, the relative improvement of CoT over standard direct prompting remains stable regardless of the number of demonstrations used (i.e., it holds true in both zero-shot and few-shot settings).We release our dataset and source code to support the ongoing automated integration of new findings in the literature.1</p>
<p>Data Extraction</p>
<p>Automated data extraction is essential for scaling and improving literature analysis efficiency.This section outlines our pipeline for extracting experimental results and model attributes from arXiv sources, which are used to construct LLMEVALDB.We define target models and attributes ( §2.1) and introduce our LLM-powered extraction process, which includes three stages: preprocessing &amp; filtering ( §2.2), extraction &amp; augmentation ( §2.3), and description generation ( §2.4).Unless stated otherwise, we utilize GPT-4o (Hurst et al., 2024) throughout the pipeline.</p>
<p>Defining Target Models and Attributes</p>
<p>Target Models We analyze four leading proprietary models in the NLP/AI field: GPT-4 (Achiam et al., 2023), GPT4-o (Hurst et al., 2024), Claude3 Opus (Anthropic, 2024), and Gemini 1.0 Pro (Team et al., 2023).Our analysis focuses on proprietary LLMs accessible via APIs that are not possible to fine-tune. 2We chose to exclude fine-tuned models from our study because comparing diverse finetuning methods would make controlled cross-study comparisons much more challenging.We also exclude recently released advanced reasoning models (GPT4-o1 (OpenAI, 2024) and Deepseek-R1 (Guo et al., 2025)) due to very limited published results to date.In the future, our pipeline can readily extract experimental data for these models along with those of other target models, as more studies are published.For further discussion, see §7.</p>
<p>Target Attributes To enable thorough analysis across different studies, we extract various performance-related fields from proprietary models.The targeted attributes include: Dataset Name, Subset, Model Name, Prompting Method, Number of Demonstrations, Metric Name, and Performance.Note that the subset refers to any division of the dataset, such as a subtask, domain,  (Cheng et al., 2023).Target attributes are identified and extracted from the table, augmented with paper content.Figure 3: Dataset description generation pipeline overview using the example of "SVAMP" dataset (Patel et al., 2021).Since LLM lacked knowledge of the given dataset, the model references the original dataset's arXiv source retrieved through extracted BibTex.If confident, however, the LLM's generated descriptions will be used.</p>
<p>split, or language pair.For instance, as shown in Fig. 2, the arXiv paper with ID 2301.08721(Cheng et al., 2023) presents experimental results for GPT-4 (Achiam et al., 2023) on the SVAMP dataset (Patel et al., 2021).The evaluation used Batch Prompting with CoT (Wei et al., 2022;Cheng et al., 2023) and 12 in-context samples (Brown et al., 2020), achieving an accuracy score of 95.0.</p>
<p>Preprocessing &amp; Filtering</p>
<p>To extract the experimental results of target models and attributes, we used arXiv sources from January 2023 to December 2024, focusing on machine learning papers (cs.CV, cs.AI, cs.CL, cs.LG).arXiv LaTeX sources preserve structure better than parsed PDFs, enabling easier dataset extraction-especially from tables-using simple regex, while PDFs require complex tools.Hence, following prior work (Kardas et al., 2020;Bai et al., 2023;Wang et al., 2024), we adopt arXiv LaTeX sources for data extraction, as over 90% of arXiv papers are submitted in LaTeX format (Frankston et al., 2024).We downloaded LaTeX source files and applied regex-based methods to extract structured data like tables.Before extraction, we filtered tables to reduce time and API costs by selecting those with target model results.We also concentrated on leaderboard tables presenting benchmark results follow-ing Kardas et al. (2020) and used Llama3.1-70B-Instruct(Dubey et al., 2024) for the filtering.</p>
<p>Extraction &amp; Augmentation</p>
<p>Based on the selected tables, we extract records from tables and paper contents.Using schemadriven extraction, we obtain records matching target attributes from tables, focusing only on relevant model data to save costs while maintaining accuracy.This selective approach significantly reduces API costs compared to extracting full results from each table (Bai et al., 2023).After table extraction, we augment records with context from the entire paper, adding experimental specifics.During this stage, we also gather BibTeX references for datasets to link and retrieve the original papers describing these datasets from arXiv.</p>
<p>Dataset Description Generation</p>
<p>In addition to extracting structured representations of experiments, we also generate summary descriptions of the relevant tasks and datasets (Fig. 3) that can aid in-depth literature analysis, such as classifying records by research subarea.We use a two-stage approach to create these descriptions.Initially, the LLM generates descriptions using its internal knowledge based on the dataset name and subset, which is cost-effective.When the LLM is uncertain and refuses to answer, we prompt it to extract descriptions using the full content of the  (Achiam et al., 2023) 12,475 GPT-4o (Hurst et al., 2024) 4,589 Claude3-Opus (Anthropic, 2024) 661 Gemini1.0-Pro(Team et al.,  source papers as reference.The source papers can be either the paper containing the table or the original dataset paper cited within it, which is retrieved using the BibTex references obtained from §2.3.</p>
<p>Comprehensive Analysis of the Dataset</p>
<p>Table 1 presents sampled instances of LLMEVALDB, while the dataset's statistics are summarized in Table 2.A total of 18,127 experimental records of four target models were extracted from scanning over 300k arXiv source papers.GPT-4 (Achiam et al., 2023) and GPT-4o (Achiam et al., 2023) dominate the results, while Claude 3 Opus (Anthropic, 2024) and Gemini 1.0 Pro (Team et al., 2023) (Bai et al., 2023) and rated descriptions on a 5-point scale from (1) unrelated to (5) fully relevant and accurate (Amidei et al., 2019;Liu et al., 2023a).Missing values were marked correct if the information was genuinely unavailable in the source papers.More details are in appendix B.</p>
<p>Table 3 shows strong extraction accuracy and description quality of LLMEVALDB.GPT-4o (Hurst During the study, we recorded the time experts spent annotating target attribute information (excluding descriptions) from additional samples based on a list of papers and tables indexing experimental results of target models.On average, it took 7 minutes and 50 seconds per table, totaling approximately about 350 hours for LLMEVALDB of 2,694 tables.This does not include the initial effort of surveying and identifying papers and tables.In contrast, our pipeline identifies and extracts data from arXiv sources from 2023 to 2024 in a single day for under $500 using a batching API.</p>
<p>Categorization by Required Skills</p>
<p>We categorize experimental records from LLMEVALDB by required skills to enable comprehensive literature analyses and offer researchers a searchable resource.Expanding on Tulu3's core skills (Lambert et al., 2024), we defined 10 categories: Knowledge, Reasoning, Math, Coding, Multimodality, Instruction Following, Safety, Multilinguality, Tool Use (Agent Framework), and Other.Records were classified using an LLM API (Hurst et al., 2024) based on dataset names, subsets, and descriptions.Since datasets can fit multiple categories, we applied multi-label classification.This categorized information is used throughout the analyses in §4.</p>
<p>Figure 4 shows the log-scale frequency of skill categories evaluated every three months based on arXiv publication dates.Over time, experimental results for all skills have increased due to rising interest in LLMs.Reasoning tasks are the most popular and continue to grow, while Multimodality has shown recent growth.Knowledge, Multilinguality, and Math have steadily increased in evaluation rates.Frequency is based on the unique count of dataset-subsets aggregated by paper.The most frequently used datasets for each category are listed in appendix E.</p>
<p>Prompting Behavior in Frontier LLMs:</p>
<p>A Literature Review LLMEVALDB enables partially automated literature analyses of LLM-prompting behaviors through its structured attributes.We validate our semiautomated approach to literature analysis by replicating Sprague et al. (2024)'s manual analysis, confirming CoT's advantages over direct prompting in mathematical and symbolic reasoning tasks ( §4.1).</p>
<p>We then show that LLMEVALDB enables a finegrained analysis, showing that in-context examples boost performance in coding &amp; multimodal tasks but not in mathematical ones ( §4.2).We qualitatively analyze dataset characteristics, such as required expert knowledge, that may negatively affect the performance of CoT and ICL ( §4.3).Moreover, we analyze CoT and ICL interactions, finding that while demonstrations enhance CoT performance, they do not affect CoT's relative improvement over standard direct prompting ( §4.4).Finally, we analyze a subset of peer-reviewed papers published in major journals or conferences, revealing consistent trends that further support the robustness and generalizability of our findings ( §4.5).</p>
<p>Figure 5: CoT shows significant performance improvement over direct prompting in mathematical tasks, whereas its impact on reasoning tasks is less distinct due to their complexity and diversity.Grey dots indicate individual deltas (improvements), blue dots represent the mean delta per paper, and a purple star marks the mean delta for each category.</p>
<p>Which Tasks Benefit from CoT?</p>
<p>Motivation and Setup CoT (Wei et al., 2022), a prompting technique for eliciting reasoning, has attracted considerable attention, with extensive literature on the subject (Wang et al., 2023;Yao et al., 2024) Recently, Sprague et al. (2024) conducted a manual analysis, concluding that CoT improves over standard direct prompting primarily in mathematical and symbolic reasoning tasks.We extract instances from LLMEVALDB that meet the criteria for replicating their investigation.This identification is achieved through filtering using the dataset's attributes.We focus on instances from the same source paper and table, identifying experiments that feature both CoT and standard direct prompts under the same conditions (model, dataset, subset, metric, and few-shot setup).Human input is used only to identify whether a prompt is a CoT prompt or a standard direct prompt based on the prompting method attribute; this is the only manual effort required for the analysis aside from implementing the analysis itself.We exclude CoT variations like "xx of Thoughts" (Yao et al., 2024) and CoT-SC (Wang et al., 2022).We then calculate CoT's performance improvement over standard direct prompting, namely delta, where a positive delta indicates CoT outperforming direct prompting, and a negative delta indicates the reverse.</p>
<p>Analysis Our analysis, shown in Fig. 5, reveals that CoT significantly enhances performance in mathematical tasks, with both median and mean improvements surpassing the overall median.However, we do not see clear improvements for reasoning tasks, likely due to their diversity, such as commonsense and logical reasoning.</p>
<p>To investigate further, we apply more detailed classification categories (but less general) from Sprague et al. (2024) and redo our study using LLM classification.The truncated results, presented in Fig. 6, clearly show that symbolic reasoning and mathematical tasks exhibit notable improvement over other reasoning and non-reasoning tasks.This confirms the reliability and efficiency of LLMEVALDB for literature analyses, as it provides consistent results with those obtained through manual curation in previous studies.Statistical tests, as shown in Total Results from Table .7, confirm these patterns.</p>
<p>Which Tasks Benefit from ICL?</p>
<p>Motivation and Setup Previous research has systematically explored ICL behavior (Min et al., 2022;Agarwal et al., 2024;Wei et al., 2023;Bertsch et al., 2024).In line with our study in §4.1, we analyze the improvement of using in-context examples compared to not using any.To achieve this, we extract instances from LLMEVALDB that come from the same papers and tables comparing fewshot and zero-shot setups under identical conditions (i.e., model, dataset, subset, metric, and prompting method).No manual labor was needed beyond implementation, as the number of in-context examples is an integer value that can be easily computed for filtering.We measure performance deltas between few-shot and zero-shot setups, where positive values indicate few-shot advantages and negative values indicate the reverse.</p>
<p>Analysis Figure 7 presents the results.In contrast to CoT's strong performance in mathematical reasoning, ICL shows only modest benefits for math tasks.However, ICL demonstrates more substantial improvements in coding and multimodal applications despite considerable performance variability across different cases.This variance may be due to the broad scope of multimodal tasks, including speech and image processing.ICL shows more uniform improvements over zero-shot performance across different categories compared to the study in Fig. 5  examples and zero-shot learning.We also plot the performance improvement distribution of more versus fewer in-context examples in appendix C and find a similar overall median.This suggests that the presence of demonstrations is more important than their quantity.</p>
<p>Which Dataset Characteristics Negatively Affect CoT and ICL?</p>
<p>Motivation and Setup To comprehend the patterns of performance degradation, we analyze cases where performance declined: specifically, where CoT resulted in worse outcomes than direct prompting and where few-shot learning performed worse than zero-shot learning (i.e.deltas below zero).Table 4 summarizes the key dataset characteristics associated with performance declines under both approaches based on analyzing the descriptions.</p>
<p>Analysis Out of the cases of performance decline, tasks requiring expert-level knowledge showed the highest ratio for both cases, approximately 31%, indicating that knowledge-intensive tasks do not benefit much from different prompting configurations alone.Sprague et al. (2024) notes a similar point that apparent performance gains in knowledge tasks mainly stem from "reasoning" or "math" components in datasets like MMLU (Hendrycks et al., 2020).This also aligns partly with Motivation and Setup LLMEVALDB includes information from papers published on arXiv.To examine whether the trends observed in our general analysis align with those in more selective venues, we conducted a focused analysis on a subset of peer-reviewed papers accepted at journals or conferences, using metadata from DBLP. 3 We report statistical significant test results from 4.1 using a subset of peer-reviewed papers.Following Sprague et al. (2024), we perform a one-sided bootstrap test to assess whether each category shows performance gains (mean improvement &gt; 0).</p>
<p>Analysis Table 7 represents the results.The statistically significant test results from our Total Results and Filtered Results indicate that math and symbolic reasoning tasks derive substantial benefit from using Chain-of-Thought (CoT) prompting, which aligns with both manual analysis (Sprague et al., 2024) and our examination in §4.1.We can see that the core finding remains consistent even if we used a subset of peer-reviewed papers.This invariance in analysis patterns extends to other examinations like those in §4.4, with additional details provided in Appendix C.These consistent results across both the complete dataset and the filtered selection of high-quality published papers serve to corroborate our findings and implicitly confirm our hypothesis.</p>
<p>Related Work</p>
<p>Information Extraction Previous works have focused on extracting basic result tuples (e.g., task, dataset, metric) from scientific literature (Singh et al., 2019;Hou et al., 2019;Kardas et al., 2020;Yang et al., 2022;Bai et al., 2023;Singh et al., 2024;Şahinuç et al., 2024;Kabongo et al., 2024).</p>
<p>Our extraction pipeline improves upon this approach in two significant ways: it extracts enriched tuples that include prompting-related attributes and generates detailed dataset descriptions by leveraging LLM and automatically linked source papers.Hence, unlike previous works that primarily compiled leaderboard tables, our enhanced extraction pipeline enables deeper review analysis, contributing to the broader goal of AI-driven scientific discovery (Xu et al., 2021;Majumder et al., 2024;M. Bran et al., 2024).</p>
<p>LLM &amp; Prompting Our study focuses on extracting experimental results of frontier proprietary LLMs (Achiam et al., 2023;Anthropic, 2024;Team et al., 2023), with a specific emphasis on target attributes that incorporate information about prompting methods (Brown et al., 2020;Wei et al., 2022).</p>
<p>In the context of prompting, prior studies have analyzed the mechanisms behind prompting methods, focusing either on the use of in-context examples (Min et al., 2022;Lampinen et al., 2022;Weber et al., 2023;Zhang et al., 2022a) or techniques that elicit reasoning, such as CoT prompting (Wei et al., 2022;Shaikh et al., 2023;Wang et al., 2023;Turpin et al., 2024).Conversely, we examine the model's behavior by conducting a literature analysis, which compiles data from scientific sources to reveal insights.</p>
<p>Literature Analysis Literature analysis systematically aggregates and examines data from multiple independent studies on a given topic to derive more precise and reliable conclusions.It has been widely applied in the biomedical domain for identifying target materials or clinical records (Bao et al., 2019;Yun et al., 2024).In the NLP domain, review analysis has been used for metric standardization (Reiter, 2018), literature review (Santu et al., 2024;Du et al., 2024), and assessing evaluation criteria for specific domains (Ostheimer et al., 2023).</p>
<p>In contrast, our work employs a review analysis approach to evaluate the behavior of LLMs.In the context of LLMs, Asai et al. (2024) utilizes retrieval-augmented language models to synthesize scientific literature.However, this approach can only process a limited number of documents during retrieval to synthesize.The work by Sprague et al. (2024) is perhaps the most closely related to ours.They conducted a review analysis through a literature survey to examine the effectiveness of CoT prompting for LLMs.However, their study is focused on CoT prompting, conducted on a limited scale, and relies on manual extraction methods.</p>
<p>Conclusion</p>
<p>Our study streamlines literature analysis by using an LLM for dataset extraction, demonstrating that the automatically extracted dataset, LLMEVALDB, can yield novel findings and replicate manual analyses.We confirm the dataset's quality by replicating a key finding from Sprague et al. (2024).Our analysis provides insights into prompting configurations, such as the benefits of ICL, the combined effects of CoT and ICL, and the dataset characteristics on performance declines with CoT or ICL.Overall, our resources support ongoing literature analyses, enhancing the understanding of LLM behaviors.</p>
<p>Target Model Scope Our study focuses on four leading proprietary LLMs selected for their widespread adoption and extensive documentation in existing literature.We excluded newer models like GPT4-o1 and Deepseek-R1 due to limited published results, though our analysis pipeline can easily accommodate them as more experimental data becomes available.While most analyzed models are not fine-tunable, this limitation presents an opportunity for future research combining fine-tuning and prompting methods analysis.</p>
<p>Further Validation Our literature analysis aims to identify trends by aggregating information across multiple studies, which generates potential hypotheses but requires systematic validation.</p>
<p>While we report findings and patterns observed in the aggregated scientific literature, we did not independently validate each claim or finding.This limitation suggests the need for future work to rigorously test the hypotheses emerging from our analysis.</p>
<p>Attributes' Descriptiveness We frequently observed that the extracted attributes were not descriptive enough, which can hinder the dataset's utility for further analysis.Techniques like Batch COT, for example, would benefit from more detailed descriptions.Additionally, the dataset descriptions could be enhanced to better differentiate between various dataset characteristics.Our attempts to further generate the "collection process" of the dataset resulted in numerous inaccuracies.Moreover, efforts to automatically link descriptions to actual dataset instances also encountered technical challenges, necessitating extensive manual intervention.Future work should aim to develop more effective methods for comprehensive dataset characterization.</p>
<p>Dataset Canonicalization Cross-study analysis requires standardizing dataset names and formats.While we implemented strict rules for dataset canonicalization, our approach likely missed potential matches.Alternative matching techniques we explored using LLM produced too many false negatives, whereas linking to the PaperswithCode dataset ontology4 was limited by its incomplete coverage of datasets.</p>
<p>A Details and Trials During Extraction</p>
<p>We provide the details and trials during the data extraction process.</p>
<p>Hyperparameters and Prompt Tuning We did not extensively tune any LLM hyperparameters, instead, we used OpenAI's default generation setup with greedy decoding.For prompting, we initially built upon previous work (Bai et al., 2023) as a foundation.Still, we adapted the prompts to better suit our project's objectives, such as (1) determining whether the table includes experimental results for our target model or (2) enhancing records using contextual information from the paper.</p>
<p>Preprocessing To extract the experimental results of target models and attributes, we utilize arXiv sources 5 published between January 2023 and December 2024, as this timeframe aligns with the release of proprietary models, ensuring relevance to the latest advancements in the field.We specifically targeted papers in the machine learning (ML) domain (cs.CV, cs.AI, cs.CL, cs.</p>
<p>LG) and downloaded their LaTeX source files for detailed analysis.To extract structured data, such as tables, we utilized a regex-based rule extraction method to retrieve the LaTeX source of tables along with their indices (Bai et al., 2023).</p>
<p>Filtering Before the extraction stage, we filter tables to reduce extraction time and API costs.To cut computational expenses, we pre-filter tables for those containing results of target models.Using simple heuristics, we filter tables based on keywords related to the models (e.g., "gemini," "gpt"), which significantly reduces irrelevant data and minimizes LLM API usage.Moreover, we focus on leaderboard tables, which present the main results on specific benchmarks (Kardas et al., 2020), to assess LLM performance.We employ an LLAMA3.1-70B-Instruct(Dubey et al., 2024) to filter out non-leaderboard tables.After extracting information from the table, we further augment the extracted records by incorporating additional context from the entire paper.This is important because valuable details, such as experimental specifics, are often found in various sections of the paper.We heuristically filter out irrelevant sections from the full latex source and provide the LLM with the extracted information to enhance the attribute details using the context.During this process, we also gathered BibTeX references for the datasets associated with each record, allowing us to link to and retrieve the original papers describing these datasets.</p>
<p>Extraction and Augmentation</p>
<p>Dataset Description Generation Initially, we asked the LLM to generate descriptions using only the dataset name and subset information from its internal knowledge.This approach was cost-effective as it didn't require processing additional context.However, the LLM may be uncertain-particularly with lesser-known datasets or those beyond its knowledge cutoff.To address this, we instructed the model to refuse to answer when unsure about dataset information (Bai et al., 2022).In such cases, a second stage is triggered, where the model is prompted to extract the necessary information directly from the source papers.When LLM knowledge was insufficient, we directly extracted dataset descriptions from source papers.These source papers could be either the paper containing the table being processed or the original dataset paper cited by it.We used citation tags from §2.3 and rulebased heuristics to link to external arXiv papers associated with the original dataset.We then used the full content of these source papers to prompt the LLM to extract the dataset descriptions again, using the paper's contents as a reference.</p>
<p>We combined the dataset name and subset name to facilitate information generation or extraction.The schema for generation includes Dataset Summary, Task Explanation, and Subset Description.Initially, we attempted to generate a Collection Process section, detailing how the dataset was sourced and curated, but this led to excessive inaccuracies, so we decided to omit this part.</p>
<p>Models</p>
<p>We utilize GPT-4o (Hurst et al., 2024) as the LLM for the pipeline and employ Llama-3.1 70B (Dubey et al., 2024) for filtering tables.</p>
<p>Valid Metrics To ensure consistent analysis, we established a standardized set of metrics and excluded records using metrics outside this set.We also normalized all metric values to maintain uniformity across the analysis.The approved metrics are: Accuracy, Exact Match, F1, BLEU, Rouge, MRR (Mean Reciprocal Rank), Precision, Recall, Pearson Correlation Coefficient, MAE (Mean Absolute Error), and MSE (Mean Square Error).For all metrics except Pearson correlation coefficient, MAE, and MSE, we standardized the values to range from a minimum of 0 to a maximum of 100 (e.g., 0.63% was scaled to 63).</p>
<p>Missing Values If certain target attributes were unavailable from the paper or the LLM couldn't locate specific information, we instructed it to mark these missing values with "xx" as a placeholder.</p>
<p>Canonicalization Using rule-based heuristics, we canonicalized the Dataset name by grouping identical names or methods under different labels.We took care to avoid merging similar but distinct names representing different versions, variations, or separate entities by applying strict character and abbreviation matching rules.We initially explored linking datasets to the PaperswithCode ontology6 .However, the outdated and incomplete nature of its dataset list made this approach infeasible.We then experimented with clustering algorithms and large language models (LLMs) for grouping, but these methods resulted in an excessive number of false positives.Ultimately, we adopted a rule-based grouping algorithm as our approach.</p>
<p>Postprocessing To ensure dataset quality, we removed duplicate records with identical values for Dataset Name, Subset, Number of FewShots, Prompting Method, Metric Name but differing Performance.Additionally, we filtered out records with invalid dataset descriptions.</p>
<p>B Details in Human Evaluation</p>
<p>We detail the human evaluation process.</p>
<p>Extraction Evaluation</p>
<p>The objective is to assess whether the fields within records have been accurately extracted.This evaluation involves checking each field in a record against the original source paper to confirm its correctness.Fields are marked with 'o' for success or 'x' for failure, depending on the accuracy of the extraction.Annotators were instructed to use the original_extracted_dictionary for verification, as metric names have been standardized and canonicalized.A table index was provided to help locate records and tables more quickly for annotation.Additionally, it is permissible for prompting methods to include few-shot examples if relevant information is not found in the paper.</p>
<p>Description Evaluation This step evaluates whether the description aligns appropriately with the dataset-subset pair.The evaluation protocol (5-Point Likert Scale) involves assessing the quality of the dataset description based on the following rubric:</p>
<p>• Score 1: The description is completely unrelated to the dataset-subset pair.</p>
<p>• Score 2: The description has minimal relevance but lacks alignment or context.</p>
<p>• Score 3: The description is moderately relevant, capturing the essence of the datasetsubset pair but includes noticeable inaccuracies.</p>
<p>• Score 4: The description is highly relevant with only minor inaccuracies.</p>
<p>• Score 5: The description is fully relevant and entirely accurate, with no errors.</p>
<p>When scoring, annotators used references from the web or literature searches to ensure the evaluation was well-informed.Our scoring rubric is designed to approximate an interval scale, allowing us to compute average scores.This approach aligns with standard practices in the machine learning field for evaluating response quality, whether through model-based or human-based assessments (Liu et al., 2023b;Kim et al., 2023).categories provide a fine-grained definition of traditional NLP tasks, they lack generalizability in encompassing the broader capabilities of modern LLMs, such as multimodality, safety, and tool use.The complete version of Fig. 6 is presented below in Fig. 8.</p>
<p>C Details in</p>
<p>Extracting records for this type of analysis is challenging due to the strict criteria for sample selection.While Sprague et al. ( 2024)'s dataset contains a larger overall sample size, as it is not limited to a single model and LLMEVALDB is not specifically oriented to replicating their study.For the overlapping model (GPT-4), their dataset includes 168 instances, whereas ours contains 553.This demonstrates the scalability of our pipeline.</p>
<p>C.2 Which Categories Benefit from ICL?</p>
<p>We visualize the distribution of in-context examples, comparing cases with more versus fewer demonstrations, in Fig. 9.The results show that the overall median and distribution are similar to those in Fig. 7, suggesting that the presence of demonstrations is more crucial than their quantity.We reanalyzed the joint behavior findings from §4.4 using only peer-reviewed papers published in journals or conferences.Table 8 and Table 9 present these results.The filtered data confirms our original finding regarding the interaction between CoT and ICL: CoT with demonstrations (ICL) consistently outperforms CoT without demonstrations.Additionally, CoT's improvement over standard prompting remains consistent regardless of demonstration count (zero-shot or few-shot).</p>
<p>D Dataset Examples</p>
<p>We provide a curated selection of examples from LLMEVALDB in Table 10 and Table 11.</p>
<p>E Frequent Datasets by Skill Category</p>
<p>We present a list of the top 10 most frequently used datasets in multiple categories due to multi-label categorization and the use of different subsets focusing on distinct skill areas.</p>
<p>F Prompts Used during Data Extraction</p>
<p>We provide the prompts used during the data extraction in Table 13 and Table 14.This dataset is a multiple-choice question answering dataset focused on commonsense knowledge.It is designed to evaluate a model's ability to understand and apply commonsense reasoning to answer questions correctly.The dataset is widely used in the natural language processing domain to benchmark the performance of models on tasks requiring commonsense understanding.</p>
<p>Task Explanation: The task involves providing a question along with multiple answer choices, where the input is the question and the list of possible answers.The output is the correct answer choice.The task is evaluated based on the accuracy of the model's predictions compared to the correct answers provided in the dataset.</p>
<p>GPT-4 12 Batch CoT Acc 86</p>
<p>2301.08745 8 Flores-101 De → En Dataset Summary: This dataset is a multilingual translation benchmark designed to evaluate machine translation systems across a wide range of languages.It covers 101 languages, providing a comprehensive resource for assessing translation quality and performance.The dataset is intended for tasks involving language translation, with a focus on both high-resource and low-resource languages.Key characteristics include its diverse language pairings and standardized evaluation metrics, which aim to facilitate consistent and fair comparisons of translation models.</p>
<p>Task Explanation: The primary task associated with this dataset is machine translation.The input consists of text in one language, and the output is the translated text in another language.For this specific subset, the task involves translating text from German (De) to English (En).Evaluation of the task is typically conducted using metrics such as BLEU, which measures the accuracy and fluency of the translated output compared to reference translations.Subset Description: The De→En subset specifically focuses on translations from German to English.This subset is used to evaluate the performance of translation models in converting German text into English, providing insights into the model's ability to handle this particular language pair.</p>
<p>GPT-4 0 Direct BLEU 46</p>
<p>2303.07992 3 KQApro xx Dataset Summary: This dataset is a large-scale knowledge-based question answering dataset designed to evaluate the ability of models to understand and reason over complex questions.It is primarily used in the domain of natural language processing and artificial intelligence, focusing on tasks that require comprehension of structured knowledge bases.The dataset features a diverse set of questions that test various reasoning skills, such as logical reasoning, comparison, and temporal reasoning.The evaluation goals are to assess the accuracy and reasoning capabilities of models in answering these questions correctly.</p>
<p>Task Explanation The primary task associated with this dataset is knowledge-based question answering.The input consists of a natural language question, and the output is the correct answer derived from a structured knowledge base.Models are expected to interpret the question, retrieve relevant information from the knowledge base, and perform the necessary reasoning to arrive at the correct answer.The task evaluation method typically involves measuring the accuracy of the model's answers compared to a set of ground truth answers.</p>
<p>GPT-4 0 xx Accuracy 57.2 2304.08244 3 API-Bank Call Dataset Summary: This dataset is a comprehensive benchmark designed to evaluate and enhance the capabilities of Large Language Models (LLMs) in utilizing external API tools.It focuses on toolaugmented LLMs, assessing their abilities in planning, retrieving, and calling APIs.The dataset includes a wide range of domains and APIs, aiming to provide a realistic and diverse evaluation environment.The primary goal is to understand the effectiveness of current LLMs in tool usage, improve their capabilities, and identify challenges in leveraging tools.</p>
<p>Task Explanation: The task involves evaluating LLMs on their ability to interact with APIs based on user queries.The input consists of user queries and API documentation, while the output is the correct API call and response.The task is evaluated based on the correctness of API calls and the quality of responses, using metrics like accuracy and ROUGE-L.Subset Description: The ""Call"" subset specifically focuses on evaluating the ability of LLMs to make API calls based on given queries when the APIs are known.This subset tests the basic capability of LLMs to interact with APIs directly without the need for retrieval or planning.</p>
<p>GPT-4 0 Zero-shot Rouge 36.91 2310.05915 1 HotpotQA xx Dataset Summary: This dataset is a large-scale, high-quality question answering dataset designed to facilitate research in the domain of natural language processing, specifically in multi-hop question answering tasks.It contains questions that require reasoning over multiple documents to arrive at the correct answer.The dataset is intended to evaluate the ability of models to perform complex reasoning and synthesis of information across different contexts.</p>
<p>Task Explanation: The primary task involves providing a natural language question as input and expecting a concise answer as output.</p>
<p>The questions are designed to require multi-hop reasoning, meaning that the answer cannot be found in a single document but requires synthesizing information from multiple sources.The task is evaluated based on the accuracy of the answers provided by the model, often using metrics such as exact match and F1 score to assess performance.</p>
<p>GPT-4 xx IO Exact Match 37.2 Task Explanation: The primary task involves taking multiple images as input and producing an output that demonstrates understanding of the relationships or narratives between them.This could involve answering questions, generating descriptive text, or identifying similarities and differences.The task evaluation method typically involves comparing the model's output to human-generated annotations or answers, using metrics such as accuracy, BLEU score, or human judgment for qualitative assessments.</p>
<p>Subset Description: The Overall subset encompasses the entire dataset, providing a comprehensive collection of all image pairs or groups and their associated annotations.This subset is intended for general evaluation and benchmarking of models on the full range of tasks supported by the dataset.</p>
<p>GPT-4o xx xx Accuracy 55.7</p>
<p>2405.02861 3 LexBench IED Dataset Summary: This dataset is part of a comprehensive evaluation suite designed to test language models on various semantic phrase processing tasks.It focuses on idiomatic expression detection, which is one of the ten tasks included in the suite.The dataset aims to evaluate the ability of language models to understand and process idiomatic expressions, which are non-compositional phrases whose meanings cannot be deduced from their individual components.The evaluation goals include assessing model performance in classification, extraction, and interpretation tasks related to idiomatic expressions.</p>
<p>Task Explanation: The task involves detecting idiomatic expressions within a given context.The input consists of a sentence containing an idiomatic expression, and the output is a choice from multiple options that best describes the idiomatic expression's meaning.The task is evaluated using exact match accuracy, where the model's prediction is compared against the correct option.Subset Description: The Idiomatic Expression Detection (IED) subset specifically focuses on identifying idiomatic expressions within sentences and selecting the correct interpretation from multiple options.This subset is designed to challenge language models in understanding the non-compositional nature of idiomatic expressions.Opus (Anthropic, 2024), and Gemini1.0Pro (Team et al., 2023)).Your task is to extract all numeric cells representing the experimental results of a specified target model from a table LaTeX source, following the provided template.When extracting results for the target model, exclude results from its variant models.</p>
<p>For example: -For GPT-4, exclude GPT-4o, GPT-4-v, or Deplot + GPT-4.</p>
<p>-For Claude3 Opus, exclude Claude3 Sonnet, Claude3 Haiku, Claude2, Claude 3.5.</p>
<p>-For Gemini 1.0 Pro, exclude Gemini 1.5, Gemini 1.5 Pro, Gemini Ultra, and Gemini Flash.</p>
<p>-For GPT-4o, exclude GPT-4, GPT-4-o1, GPT4-Turbo, and GPT4-V.</p>
<p>However, include results from different versions of the target model across time periods (e.g., GPT-4, GPT-4-0828, GPT-4-0623, GPT-4-0314).If the table contains results where the target model is used for generation or evaluation, exclude those results.The goal is to extract results about the target model itself, not those where it is used as a tool.If no numeric cells related to the target model are found, output "<FAILED>".During output, return only the extracted results in the following template.Do not provide explanations.For any unanswerable attributes, leave their value as "xx".Template: {"value": "xx", "dataset": "xx", "dataset_citation_tag": "xx", "subset": "xx", "model_name": "xx", "metric": "xx", "prompting_method": "xx", "number_of_shots": "xx"} Field Descriptions: -value: Extracted numeric cell value.</p>
<p>-dataset: Name of the dataset or benchmark (must be a proper noun, e.g., "synthetic dataset" is not acceptable).</p>
<p>-dataset_citation_tag: Citation tag for the dataset.</p>
<p>-subset: Dataset subset used (e.g., subtask, domain, split).</p>
<p>-model_name: Name of the model used in the experiment.</p>
<p>-metric: Evaluation metric used.</p>
<p>-prompting_method: Prompting method used (do not include shot count here).</p>
<p>-number_of_shots: Integer value representing the number of shots used.Input: Target Model:</p>
<p>Component Prompt</p>
<p>Context Augmentation</p>
<p>Augment the extracted records from the table's LaTeX source by incorporating additional context from the text source to enrich and complete the records.Extracted Record Template: {"value": "xx", "dataset": "xx", "dataset_citation_tag": "xx", "subset": "xx", "model_name": "xx", "metric": "xx", "prompting_method": "xx", "number_of_shots": "xx"} To accurately augment and enrich the extracted records, follow these steps systematically: 1. value: Referencing the table source, if a numeric value is only partially extracted from a table cell, ensure that the entire content of the cell is used to update the value.2. dataset: Referencing the table source, table caption, and source text, locate the full name of the dataset and update the name accordingly.3. dataset_citation_tag: Referencing the table source, table caption, and source text, identify the dataset citation tag to the extracted record.Avoid using LaTeX syntax (e.g., cite and curly brackets); return only the tag name contained within.4. subset: Referencing the table source, table caption, and source text, identify specific subsets of the dataset, such as subtasks, domains, splits, or language pairs, and provide detailed descriptions of each subset.Prioritize the use of column information from the table source to identify the subset.If the subset is not explicitly mentioned in the table source, refer to the table caption or source text to identify the subset.5. model_name: Referencing the table source, if a model name is partially extracted, revisit the corresponding table cell to ensure the entire content is included.6. metric: Referencing the table source, table caption, and source text, extract the metrics used in the experiment along with detailed descriptions and any additional information about the evaluation protocol.7. prompting_method: Referencing the table source, table caption, and source text, search for and identify the prompting technique (e.g., direct, CoT, etc.) applied in the experiment and provide a detailed explanation of it.Do not include any information related to the number of shots (e.g., few-shot, zero-shot, three-shot) in this field.8. number_of_shots: Referencing the table source, table caption, and source text, specify the number of shots used in the experiment.This must be an integer value.</p>
<p>During output, output only the template following extracted results.Do not output any explanations or use LaTeX grammar.For any unanswerable attributes in the templates, leave their value as "xx".Input Extracted Records:</p>
<p>Figure 1 :
1
Figure 1: The diagram of our semi-automated literature analysis process and a key finding derived from data automatically extracted from the arXiv database.</p>
<p>Figure 2 :
2
Figure2: Data extraction pipeline overview with an extracted example from arXiv paper(Cheng et al., 2023).Target attributes are identified and extracted from the table, augmented with paper content.</p>
<p>Figure 4 :
4
Figure 4: Log-scale frequency trends show a rapid increase of evaluation studies over successive quarters (Q).Reasoning remains the most popular evaluation category, while Multimodality exhibits recent rapid growth.</p>
<p>Figure 6 :
6
Figure 6: In the reasoning category, CoT shows notable improvement over direct prompting in Symbolic Reasoning tasks compared to other reasoning tasks.The figure displays an abbreviated version of our study, using categories defined by Sprague et al. (2024).</p>
<p>Figure 7 :
7
Figure7: Few-shot learning significantly improves performance over zero-shot learning in coding and multimodal tasks, whereas its impact on math tasks is less pronounced compared to CoT, as shown in Fig.5.Grey dots represent individual deltas (improvements), sky pink dots show the mean delta per paper, and a pink star marks the mean delta for each category.</p>
<p>Figure 8 :
8
Figure 8: Full version of box plots showing performance improvements with CoT compared to standard prompting, categorized according to Sprague et al. (2024) for a fine-grained investigation in various reasoning tasks.</p>
<p>Figure 9 :
9
Figure 9: Box plot distributions showcasing performance enhancements using more in-context examples compared to less in-context examples setup, categorized by skill sets in §3.2.Grey dots represent individual deltas, while sky pink dots show the mean delta aggregated for each paper.A pink star indicates the mean delta for each category.</p>
<p>Table 1 :
1
Sampled dataset instances.'xx' indicates missing values from the paper.Dataset descriptions, including summaries, tasks, and subsets, are abbreviated due to space limits.Full versions are in appendix D.
ArXiv ID TableDatasetSubsetDataset DescriptionModel Shots PromptMetric Value2301.08721 2 CommonsenseQA xx Dataset Summary This datasetGPT-412 Batch CoTAcc86is a multiple-choice question an-swering dataset focused on com-monsense knowledge...2408.02718 3MMIUOverall Dataset Summary This datasetGPT-4oxxxxAccuracy 55.7is designed for the domainof multimodal understanding,specifically focusing on tasksthat require the integration ofinformation from multiple im-ages...2405.02861 3LexBenchIED Dataset Summary This datasetClaude30zero-shot Accuracy 66.3is part of a comprehensive eval-uation suite designed to test lan-guage models on various seman-tic phrase processing tasks...2409.19667 5ProGraphBGT Dataset Summary This datasetGemini1.0 xxxxAccuracy 27.7is a benchmark designed to eval-uate the capability of large lan-guage models (LLMs) in graphanalysis tasks...MetricValueGeneral StatisticsTotal Records18,127Number of Unique Datasets2,984Number of Table Source Papers1,737Number of Unique Tables2,694Records per ModelGPT-4</p>
<p>Table 3 :
3
Results of Human Evaluation.
AttributeAccuracy (Score)ExtractionDataset Name95 %Model Name100 %Prompting Method86.3 %Number of Few-Shot Examples95 %Metric100 %Metric Value98.8 %DescriptionDataset Description4.55et al., 2024), which we used during the data ex-traction, demonstrated effective long-context in-formation extraction, showcasing its potential forscientific literature synthesis. Moreover, high val-idation scores suggest missing values stem fromunreported setups. Cohen's Kappa scores of 0.68(extraction) and 0.57 (description) indicate substan-tial inter-annotator agreement (McHugh, 2012).</p>
<p>(std: 8.3).Note that we did not account for the number of in-context examples used; we only distinguished between the use of in-context
Key CharacteristicCoT Ratio ICL RatioExpert Knowledge31.6%31.0%Faithfulness20.9%4.6%Complex Reasoning17.9%13.8%Information Synthesis9.7%10.3%Cognitive Tasks8.7%-Affective Analysis7.1%13.8%Structured Prediction-12.6%Other2.5%13.8%Table 4: A qualitative analysis of dataset characteristicsthat resulted in decreased performance with CoT or ICL,compared to direct prompting or zero-shot set-up. Keycharacteristics refer to the representative traits identifiedthrough qualitative analysis. The CoT and ICL ratiosindicate the proportion of these traits among all negativecases for each method.</p>
<p>Table 5 :
5
Effect of in-context demonstrations withinChain-of-Thought prompting.
Liu et al.
(Wei et al., 2022;Zhang et al., 2022b)This replicates the uplift reported by(Wei et al., 2022;Zhang et al., 2022b).2.Demonstrations do not "rescue" CoT.Table6shows that, whether we include demon-</p>
<p>Table 7 :
7
Sprague et al. (2024)lts across different categories for replicating the study fromSprague et al. (2024).Total Results refers to the results without filtering any papers.Filtered Results refer to the results from the filtered papers that are published in peer-reviewed journals &amp; conferences.Mean ∆ shows the average improvement when using CoT over standard prompting, and significance is determined at p = 0.00227 after applying a Bonferroni correction.
CategoryTotal Results Mean ∆ p-value Significant Mean ∆ p-value Significant Filtered ResultsMath14.610.0000Yes13.530.0000YesSymbolic and algorithmic8.850.0002Yes9.130.0000YesSpatial and temporal reasoning3.030.0166No2.070.0056NoLogical reasoning2.390.0084No1.180.3776NoCommonsense reasoning5.410.0450No5.610.0748NoMulti-hop QA2.050.0000Yes1.210.0064NoContext-aware QA2.450.0014Yes0.280.7060NoEncyclopedic knowledge2.180.0076No3.310.0138NoGeneration4.240.0280No1.920.3920NoText classification1.010.4464No0.270.8644NoEntailment0.810.2070No0.930.1666No
3 https://dblp.org/</p>
<p>Table 8 :
8
Table 12, aggregated according to our defined skill categories, in §3.2.Frequency is measured by the number of unique papers that evaluate a dataset, without counting multiple experiments within the same paper.Some datasets appear in Effect of in-context demonstrations within Chain-of-Thought prompting.Original uses all papers; Filtered keeps only top-venue papers.
OriginalFilteredPerformance comparisonMedian Q1 Q3 Median Q1 Q3Few-shot CoT − Zero-shot CoT3.00.4 9.25.02.3 10.4Demonstration levelOriginal ∆Filtered ∆Median Q1Q3 Median Q1 Q3Zero-shot+1.3−0.4 +4.7+1.30.0 +3.6Few-shot+0.9−1.2 +3.7+1.30.0 +3.0</p>
<p>Table 9 :
9
CoT versus standard prompting when the demonstration setting is fixed.Positive ∆ means CoT outperforms Direct prompting.Original ∆ uses all papers; Filtered ∆ keeps only top-venue papers.</p>
<p>Table 10 :
10
Curated list of examples from LLMEVALDB (Target Model: GPT-4(Achiam et al., 2023)).This dataset is designed for the domain of highperformance computing (HPC) code translation, specifically focusing on translating between OpenMP Fortran and C++ code.It aims to train and evaluate large language models (LLMs) to enhance their translation capabilities between these two languages.The dataset is characterized by its diverse and representative collection of open-source OpenMP benchmarks, refined through code similarity tests.The evaluation goals include improving translation accuracy, as measured by CodeBLEU scores, and achieving human-level translation proficiency.Task Explanation: The primary task is to translate code from OpenMP Fortran to C++.The input is a Fortran code snippet, and the output is its equivalent C++ translation.The task is evaluated using CodeBLEU, a metric that assesses the quality of code translation by considering both syntactic and semantic elements.Human evaluation is also employed to ensure the translations are correct, readable, and maintain the original code's functionality.Subset Description: The Fortran to C++ translation subset focuses specifically on translating code from Fortran to C++.It serves the purpose of training models to understand and convert Fortran code into its C++ equivalent, addressing the challenges of translating between these two prominent HPC languages.This dataset is designed for the domain of multimodal understanding, specifically focusing on tasks that require the integration of information from multiple images.It aims to facilitate research in areas such as image comparison, visual storytelling, and cross-image reasoning.The key characteristics of the dataset include a diverse set of image pairs or groups, each accompanied by annotations or questions that require understanding the relationships or narratives across the images.The evaluation goals are to assess the ability of models to comprehend and reason about multiple images simultaneously.
ArXiv ID Table DatasetSubsetDataset DescriptionModel Shots Prompt Metric Value2407.19619 1HPCFortran2C++ Dataset Summary: GPT-4o0 Zero-Shot BLEU 37.12408.02718 3MMIUOverallDataset Summary:</p>
<p>This dataset is a benchmark designed to evaluate the capability of large language models (LLMs) in graph analysis tasks.It focuses on enabling LLMs to process and analyze graphs using external APIs, similar to how human experts would approach such tasks.The dataset is crafted to assess the models' ability to generate code solutions for graph-related problems, rather than relying solely on reasoning over raw inputs.The benchmark includes 512 problems across three categories: basic graph theory, graph statistical learning, and graph embedding.The evaluation aims to measure the models' performance in leveraging programming libraries to solve graph tasks.
Claude30zero-shot Accuracy 66.32409.19667 5ProGraphBGTDataset Summary: Task Explanation: The primary task involves generating Python codeGemini1.0 xxxxAccuracy 27.7to solve graph-related problems using specified APIs. The input consistsof a problem statement describing a graph task, and the output is thecorresponding Python code that utilizes relevant APIs to solve the task.The evaluation method involves assessing the pass rate (the ratio ofexecutable code) and accuracy (the ratio of correct answers from theexecutable code) of the generated solutions.Subset Description: The Basic Graph Theory (BGT) subset focuseson fundamental concepts of graph theory, including types, properties,classical algorithms, and basic operations. Tasks in this subset mayinvolve checking graph acyclicity, computing node centrality, or findingmaximum cardinality matching.
Table 11: Curated list of examples from LLMEVALDB (Target Models: GPT-4o (Hurst et al., 2024), Claude3</p>
<p>Table 12 :
12
Top 10 Most Frequent Datasets per Skill Category.Frequency is measured by the number of unique papers evaluating a dataset, without counting multiple experiments within the same paper.Determine if the given Table LaTeX represents a leaderboard table.Leaderboard tables showcase the main results of the paper on a specific benchmark, often comparing these results with those from other studies.Tables are NOT considered leaderboard tables if they focus on ablation studies, hyperparameter tuning, dataset statistics, or other supplementary experiments.Respond with 'true' if the table is a leaderboard table, and 'false' otherwise, with no additional explanation.
Skill CategoryDatasetReasoningMMLU, CommonsenseQA, StrategyQA, HotPotQA, ARC Challenge, BIG-Bench Hard, Spider, DROP,Winogrande, ScienceQAKnowledgeMMLU, MedQA, MedMCQA, PubmedQA, NQ, TruthfulQA, United States USMLE, TriviaQA,ScienceQA, Financial Phrase BankMultilingualityC-Eval, WMT22, FLORES-200, XQuAD, Flores-101, MKQA, Flores, MMMLU, SIB-200, TOEFL11MultimodalityScienceQA, EgoSchema, MMMU, DAIC-WOZ, MathVerse, MathVista, MMBench, ChartQA, Text-VQA, DocVQAMathGSM8K, MATH, SVAMP, AQuA, AddSub, Multi-Arith, ASDIV, Multilingual Grade School Math,Single Eq, MathVistaCodingSpider, BIRD, Human Eval, MBPP, APPS, CodeReviewer, Exception Handling Code Generation,BigCloneBench, WikiSQL, SecurityEvalInstructionMind2Web, SCAN, PromptCBLUE, InstructExcel, Len, RuLES, SFRES, SemEval, InstructDial++,Alpaca EvalSafetyAdvBench, ETHICS, Bias Benchmark for Question Answering (BBQ), HHH Alignment, Simple-SafetyTests, i2b2/UTHealth de-identification challenge dataset, Fifty Shades of Bias, LLM-generatedemails, Tensor Trust, MULTITuDE benchmarkTool UseAPI-Bank, NERetrieve, PubMed Retrieval and Synthesis (PubMedRS-200), student help requests,PowerPoint Task Completion (PPTC), ToolTalk, TaskBench, AgentBench-WB, Compliance Checking,CardBench</p>
<p>Table LaTeX Source: Output Extracted Results:</p>
<p>Table 13 :
13
Prompts for identifying leaderboard tables and schema-driven data extraction, where the model needs to identify if the table contains the experimental results of target models.</p>
<p>Table LaTeX
LaTeXSource:Text Source:OutputAugmented Extracted Records:</p>
<p>Table 14 :
14
Prompt for context augmentation.</p>
<p>The code is available at https://github.com/ JJumSSu/meta-analysis-frontier-LLMs, and the dataset can be accessed at https://huggingface.co/datasets/ jungsoopark/LLMEvalDB.
 and Gemini 1.0 Pro became available for finetuning during our study period, but we used rule-based heuristics to filter out limited fine-tuned results.
https://paperswithcode.com/datasets
https://paperswithcode.com/datasets
AcknowledgmentsWe would like to thank Microsoft's Azure Accelerate Foundation Models Research Program and NVIDIA's Academic Grant Program for providing computational resources to support this work.This research is supported in part by the NSF under grant numbers IIS-2052498 and SMA-2418946.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.We also appreciate Ethan Mendes, Duong Minh Le, and Seongeun Park for their valuable discussions and feedback on the paper.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Rishabh Agarwal, Avi Singh, M Lei, Bernd Zhang, Luis Bohnet, Stephanie Rosias, Biao Chan, Ankesh Zhang, Zaheer Anand, Azade Abbas, Nova, arXiv:2404.11018Many-shot in-context learning. 2024arXiv preprint</p>
<p>The use of rating and likert scales in natural language generation human evaluation tasks: A review and some recommendations. Jacopo Amidei, Paul Piwek, Alistair Willis, Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language Generation2019</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Semantic Scholar. Anthropic, CorpusID: 2682324992024</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , arXiv:2411.141992024arXiv preprint</p>
<p>Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Mark Dredze, Alan Ritter, arXiv:2305.14336Schemadriven information extraction from heterogeneous tables. 2023arXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Using machine learning and natural language processing to review and classify the medical literature on cancer susceptibility genes. Yujia Bao, Zhengyi Deng, Yan Wang, Heeyoon Kim, Diego Victor, Francisco Armengol, Nofal Acevedo, Cathy Ouardaoui, Giovanni Wang, Regina Parmigiani, Barzilay, JCO Clinical Cancer Informatics. 12019</p>
<p>Incontext learning with long-context models: An indepth exploration. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Graham Matthew R Gormley, Neubig, arXiv:2405.002002024arXiv preprint</p>
<p>Introduces GPT-3 and demonstrates few-shot learning capabilities in language models. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprintLanguage models are few-shot learners</p>
<p>Batch prompting: Efficient inference with large language model apis. Zhoujun Cheng, Jungo Kasai, Tao Yu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track2023</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Introduces PaLM and demonstrates benefits of extreme scale in language models. 2022arXiv preprintPalm: Scaling language modeling with pathways</p>
<p>Bias-augmented consistency training reduces biased reasoning in chain-of-thought. James Chua, Edward Rees, Hunar Batra, Julian Samuel R Bowman, Ethan Michael, Miles Perez, Turpin, arXiv:2403.055182024arXiv preprint</p>
<p>Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, arXiv:2406.16253Llms assist nlp researchers: Critique paper (meta-) reviewing. 2024arXiv preprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Charles Frankston, Jonathan Godfrey, Shamsi Brinn, Alison Hofer, Mark Nazzaro, arXiv:2402.08954Html papers on arxiv-why it is important, and how we made it happen. 2024arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. 2020</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>A taxonomy and review of generalization research in nlp. Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Nature Machine Intelligence. 5102023</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Orkg-leaderboards: a systematic workflow for mining leaderboards as a knowledge graph. Salomon Kabongo, D' Jennifer, Sören Souza, Auer, International Journal on Digital Libraries. 2512024</p>
<p>Axcell: Automatic extraction of results from machine learning papers. Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, Robert Stojnic, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, arXiv:2411.15124Pushing frontiers in open language model post-training. 3arXiv preprintet al. 2024. T\" ulu</p>
<p>Can language models learn from explanations in context?. Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James Mcclelland, Jane Wang, Felix Hill, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Ryan Liu, Jiayi Geng, J Addison, Ilia Wu, Tania Sucholutsky, Thomas L Lombrozo, Griffiths, arXiv:2410.21333Mind your step (by step): Chain-of-thought can reduce performance on tasks where thinking makes humans worse. 2024arXiv preprint</p>
<p>Openai o1-preview. 2024OpenAI</p>
<p>. Model Webiste, </p>
<p>A call for standardization and validation of text style transfer evaluation. Phil Ostheimer, Mayank Nagda, Marius Kloft, Sophie Fellenz, arXiv:2306.005392023arXiv preprint</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>A structured review of the validity of bleu. Ehud Reiter, Computational Linguistics. 4432018</p>
<p>. Junhao Ruan, Long Meng, Weiqiao Shan, Tong Xiao, Jingbo Zhu, 2024</p>
<p>Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards. Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Shubhra Kanti, Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Matthew Freestone, arXiv:2402.15589Prompting llms to compose meta-review drafts from peer-review narratives of scholarly manuscripts. 2024arXiv preprint</p>
<p>On second thought, let's not think step by step! bias and toxicity in zeroshot reasoning. Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Automated early leaderboard generation from comparative tables. Mayank Singh, Rajdeep Sarkar, Atharva Vyas, Pawan Goyal, Animesh Mukherjee, Soumen Chakrabarti, Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019. Cologne, GermanySpringer2019. April 14-18, 2019Proceedings, Part I 41</p>
<p>Legobench: Leaderboard generation benchmark for scientific models. Shruti Singh, Shoaib Alam, Mayank Singh, arXiv:2401.062332024arXiv preprint</p>
<p>To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett, arXiv:2409.121832024arXiv preprint</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-ofthought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, The 61st Annual Meeting Of The. Association For Computational Linguistics2023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Advances in Neural Information Processing Systems. 202437</p>
<p>Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. Lucas Weber, Elia Bruni, Dieuwke Hupkes, Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL). the 27th Conference on Computational Natural Language Learning (CoNLL)2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint</p>
<p>Artificial intelligence: A powerful paradigm for scientific research. Yongjun Xu, Xin Liu, Xin Cao, Changping Huang, Enke Liu, Sen Qian, Xingchen Liu, Yanjun Wu, Fengliang Dong, Cheng-Wei Qiu, The Innovation. 242021</p>
<p>Telin: Table entity linker for extracting leaderboards from machine learning publications. Sean Yang, Chris Tensmeyer, Curtis Wigington, Proceedings of the first Workshop on Information Extraction from Scientific Publications. the first Workshop on Information Extraction from Scientific Publications2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Appraising the potential uses and harms of llms for medical systematic reviews. Hye Sun Yun, Iain J Marshall, Thomas A Trikalinos, Byron C Wallace, arXiv:2305.118282023arXiv preprint</p>
<p>Automatically extracting numerical results from randomized controlled trials with large language models. Hye Sun, Yun , David Pogrebitskiy, Iain J Marshall, Byron C Wallace, arXiv:2405.016862024arXiv preprint</p>
<p>Robustness of demonstration-based learning under limited data scenario. Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, Diyi Yang, arXiv:2210.106932022aarXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
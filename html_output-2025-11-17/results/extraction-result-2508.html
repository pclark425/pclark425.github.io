<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2508 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2508</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2508</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-273507627</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.17218v1.pdf" target="_blank">Creativity in AI: Progresses and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition. Machine creativity on the other hand has been a long-standing challenge. With the rise of advanced generative AI, there has been renewed interest and debate regarding AI’s creative capabilities. Therefore, it is imperative to revisit the state of creativity in AI and identify key progresses and remaining challenges. In this work, we survey leading works studying the creative capabilities of AI systems, focusing on creative problem-solving, linguistic, artistic, and scientific creativity. Our review suggests that while the latest AI models are largely capable of producing linguistically and artistically creative outputs such as poems, images, and musical pieces, they struggle with tasks that require creative problem-solving, abstract thinking and compositionality and their generations suffer from a lack of diversity, originality, long-range incoherence and hallucinations. We also discuss key questions concerning copyright and authorship issues with generative models. Furthermore, we highlight the need for a comprehensive evaluation of creativity that is process-driven and considers several dimensions of creativity. Finally, we propose future research directions to improve the creativity of AI outputs, drawing inspiration from cognitive science and psychology.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2508.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2508.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGATHA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-graph + transformer approach that mines structured relations in scientific text/graphs and proposes candidate hypotheses by combining graph-mining with transformer-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AGATHA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines automated graph-mining over literature/knowledge graphs to surface candidate associations, and a transformer-based generator to convert graph-derived signals into natural-language hypothesis proposals; described in the survey as an example of transformer+graph hypothesis generation frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-graph-based + transformer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Graph mining to identify candidate relations/associations followed by transformer-based natural language generation to verbalize hypotheses (graph → candidate edges → transformer proposal).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes general limitations of automated LLM/graph discovery pipelines: outputs require expert validation and are susceptible to hallucination, vague implementation details, and dataset/bias issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2508.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian machine scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Bayesian machine scientist to aid in the solution of challenging scientific problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian-statistics-driven system that searches symbolic model space and uses Bayesian model scoring to propose candidate scientific models/relationships from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian machine scientist to aid in the solution of challenging scientific problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Machine Scientist (Guimerà et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses Bayesian inference/statistics to search for compact symbolic explanations of empirical data (candidate equations or concept sets), scoring candidate models by Bayesian model evidence / likelihood and priors to select plausible explanatory hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Bayesian/statistical (symbolic discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Enumerative or guided search in symbolic model space with Bayesian scoring (prior + likelihood) to propose models that explain observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Implicitly via Bayesian model evidence (models with higher posterior probability considered more plausible).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Balances fit and complexity via Bayesian priors and evidence (penalizes overcomplex models), i.e., trades novelty against posterior probability.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational validation against observed data (model fit / posterior); survey cites this class of methods as employing Bayesian statistics to produce and score candidate discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Bayesian posterior probabilities / model evidence (survey highlights Bayesian methods for discovery and surprise quantification).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey warns that Bayesian/symbolic discovery systems still require human interpretation and are subject to data limitations and model search biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2508.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON: Scientific Inspiration Machines Optimized for Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system explicitly optimized to generate scientifically novel research ideas by applying an objective targeting novelty during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciMON: Scientific Inspiration Machines Optimized for Novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-centered pipeline that generates research ideas/hypotheses and uses a novelty-oriented optimization objective (explicitly optimizes for novelty) to surface less conventional ideas; described in the survey as a recent approach to idea/hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (novelty-optimized)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Prompting large language models to propose ideas, combined with an objective or scoring function that ranks or optimizes for novelty across proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Optimization for a novelty objective (survey cites SciMON's focus on novelty optimization), though exact metric not described in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Emphasizes novelty in generation; survey notes potential trade-offs where optimizing novelty can yield proposals with vague implementation details or unrealistic assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey reports that large-scale idea-generation studies (including novelty-optimized systems) often yield many non-unique ideas and common failure modes: vague implementation details, misuse of datasets, inappropriate baselines and unrealistic assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2508.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative LLM-based system that reads scientific literature and iteratively proposes research ideas and hypotheses by cycling between retrieval and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-driven iterative pipeline that retrieves relevant literature passages, prompts an LLM to propose ideas/hypotheses, and uses iterative refinement loops over the literature to generate and expand research ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based + retrieval-augmented (iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Retrieval-augmented prompting with iterative refinement: retrieve literature, prompt LLM to propose ideas, and iterate to refine proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey highlights that LLM-generated idea pools often contain redundancy and low implementation detail; many generated ideas lack novelty or feasibility without human curation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2508.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot LLM hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models are Zero Shot Hypothesis Proposers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that large language models can propose scientific hypotheses with zero-shot prompting, i.e., without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models are Zero Shot Hypothesis Proposers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Zero-shot LLM hypothesis proposers (Qi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses off-the-shelf large language models and zero-shot prompts to generate candidate hypotheses directly from prompts, treating the LLM as a general hypothesis proposer.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Zero-shot natural language prompting of LLMs to generate hypothesis statements without additional supervision or domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey contextualizes these results with caveats: zero-shot proposals can be numerous but often include vague/unrealistic ideas and hallucinated claims requiring domain verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2508.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that uses intelligent agents reasoning over graphs to automate parts of scientific discovery and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciAgents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent architecture where agents operate on knowledge graphs and perform reasoning, graph transformations, or search to generate candidate scientific hypotheses and conceptual links; presented in the survey as a modern approach to automated discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent + knowledge-graph-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Multi-agent graph reasoning: agents explore and manipulate knowledge graph structures to surface novel associations and candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey-level caution: multi-agent and graph-based discovery systems still require downstream experimental validation and are subject to the same hallucination and reasoning limitations as LLM-driven systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2508.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (end-to-end)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A label applied to end-to-end automation frameworks that attempt to run the full scientific loop (idea generation → experiment design/execution → analysis → paper writing) largely driven by LLMs and automation modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (end-to-end automation frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end pipelines integrating literature review, hypothesis/idea generation (often LLM-driven), automated experiment planning/execution and manuscript drafting; survey references multiple recent works aiming to automate the whole scientific process.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based + automation / workflow orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLM-driven idea/hypothesis generation possibly combined with retrieval and experiment-planning modules in an integrated workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Often includes automated experiment execution or human-verifiable experiments in the loop according to the cited works, but the survey cautions that many such systems suffer from hallucinations and poor experimental planning.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey explicitly warns to 'take these results with a grain of salt' because end-to-end frameworks are typically LLM-powered and inherit hallucination, lack of robust reasoning, poor planning, and low novelty/diversity issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2508.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system that predicts 3D protein structures from sequences at high accuracy, enabling large-scale structure prediction and downstream biological discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A deep neural network model (attention-based/structure-prediction architecture) trained to predict 3D protein structures from amino-acid sequences; cited in the survey as a major example of AI enabling scientific discovery (millions of predicted structures).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep neural network (structure prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / structural biology</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Validated by comparison to experimentally-determined structures (survey highlights AlphaFold's accurate predictions and impact on biology).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Comparison against experimentally-determined protein structures and large-scale biological validation; survey notes AlphaFold's capability to predict structures at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Survey cites AlphaFold as enabling discovery of millions of protein structures (impactful domain-level discoveries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2508.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2508.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-set contamination detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proving Test Set Contamination in Black Box Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to detect memorization / training-set leakage in black-box LLMs; useful to detect whether claims originate from memorized training data rather than genuine reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Proving Test Set Contamination in Black Box Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Test-set contamination / memorization detection methods (Oren et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Statistical and forensic techniques to detect whether a language model's outputs stem from memorized training examples (test-set contamination) rather than model generalization; survey cites these methods as part of efforts to detect unintended memorization and data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>statistical / forensic detection</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detects memorization/test-set contamination to identify outputs that may be copied from training data (survey references Oren et al. and related works).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>These detection methods address memorization/data leakage specifically and do not in themselves verify factual correctness or scientific plausibility of novel model-generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Creativity in AI: Progresses and Challenges', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero Shot Hypothesis Proposers <em>(Rating: 2)</em></li>
                <li>A Bayesian machine scientist to aid in the solution of challenging scientific problems. <em>(Rating: 2)</em></li>
                <li>SciMON: Scientific Inspiration Machines Optimized for Novelty. <em>(Rating: 2)</em></li>
                <li>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. <em>(Rating: 2)</em></li>
                <li>SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. <em>(Rating: 2)</em></li>
                <li>Proving Test Set Contamination in Black Box Language Models. <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2508",
    "paper_id": "paper-273507627",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "AGATHA",
            "name_full": "AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach",
            "brief_description": "A knowledge-graph + transformer approach that mines structured relations in scientific text/graphs and proposes candidate hypotheses by combining graph-mining with transformer-based generation.",
            "citation_title": "AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach",
            "mention_or_use": "mention",
            "system_name": "AGATHA",
            "system_description": "Combines automated graph-mining over literature/knowledge graphs to surface candidate associations, and a transformer-based generator to convert graph-derived signals into natural-language hypothesis proposals; described in the survey as an example of transformer+graph hypothesis generation frameworks.",
            "system_type": "knowledge-graph-based + transformer",
            "scientific_domain": null,
            "hypothesis_generation_method": "Graph mining to identify candidate relations/associations followed by transformer-based natural language generation to verbalize hypotheses (graph → candidate edges → transformer proposal).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Survey notes general limitations of automated LLM/graph discovery pipelines: outputs require expert validation and are susceptible to hallucination, vague implementation details, and dataset/bias issues.",
            "uuid": "e2508.0",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Bayesian machine scientist",
            "name_full": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
            "brief_description": "A Bayesian-statistics-driven system that searches symbolic model space and uses Bayesian model scoring to propose candidate scientific models/relationships from data.",
            "citation_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems.",
            "mention_or_use": "mention",
            "system_name": "Bayesian Machine Scientist (Guimerà et al.)",
            "system_description": "Uses Bayesian inference/statistics to search for compact symbolic explanations of empirical data (candidate equations or concept sets), scoring candidate models by Bayesian model evidence / likelihood and priors to select plausible explanatory hypotheses.",
            "system_type": "Bayesian/statistical (symbolic discovery)",
            "scientific_domain": null,
            "hypothesis_generation_method": "Enumerative or guided search in symbolic model space with Bayesian scoring (prior + likelihood) to propose models that explain observed data.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Implicitly via Bayesian model evidence (models with higher posterior probability considered more plausible).",
            "novelty_plausibility_balance": "Balances fit and complexity via Bayesian priors and evidence (penalizes overcomplex models), i.e., trades novelty against posterior probability.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Computational validation against observed data (model fit / posterior); survey cites this class of methods as employing Bayesian statistics to produce and score candidate discoveries.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Bayesian posterior probabilities / model evidence (survey highlights Bayesian methods for discovery and surprise quantification).",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Survey warns that Bayesian/symbolic discovery systems still require human interpretation and are subject to data limitations and model search biases.",
            "uuid": "e2508.1",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
            "brief_description": "An LLM-based system explicitly optimized to generate scientifically novel research ideas by applying an objective targeting novelty during generation.",
            "citation_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty.",
            "mention_or_use": "mention",
            "system_name": "SciMON",
            "system_description": "An LLM-centered pipeline that generates research ideas/hypotheses and uses a novelty-oriented optimization objective (explicitly optimizes for novelty) to surface less conventional ideas; described in the survey as a recent approach to idea/hypothesis generation.",
            "system_type": "LLM-based (novelty-optimized)",
            "scientific_domain": null,
            "hypothesis_generation_method": "Prompting large language models to propose ideas, combined with an objective or scoring function that ranks or optimizes for novelty across proposals.",
            "novelty_assessment_method": "Optimization for a novelty objective (survey cites SciMON's focus on novelty optimization), though exact metric not described in the survey.",
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": "Emphasizes novelty in generation; survey notes potential trade-offs where optimizing novelty can yield proposals with vague implementation details or unrealistic assumptions.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Survey reports that large-scale idea-generation studies (including novelty-optimized systems) often yield many non-unique ideas and common failure modes: vague implementation details, misuse of datasets, inappropriate baselines and unrealistic assumptions.",
            "uuid": "e2508.2",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
            "brief_description": "An iterative LLM-based system that reads scientific literature and iteratively proposes research ideas and hypotheses by cycling between retrieval and generation.",
            "citation_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models.",
            "mention_or_use": "mention",
            "system_name": "ResearchAgent",
            "system_description": "An LLM-driven iterative pipeline that retrieves relevant literature passages, prompts an LLM to propose ideas/hypotheses, and uses iterative refinement loops over the literature to generate and expand research ideas.",
            "system_type": "LLM-based + retrieval-augmented (iterative)",
            "scientific_domain": null,
            "hypothesis_generation_method": "Retrieval-augmented prompting with iterative refinement: retrieve literature, prompt LLM to propose ideas, and iterate to refine proposals.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Survey highlights that LLM-generated idea pools often contain redundancy and low implementation detail; many generated ideas lack novelty or feasibility without human curation.",
            "uuid": "e2508.3",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Zero-shot LLM hypothesis proposers",
            "name_full": "Large Language Models are Zero Shot Hypothesis Proposers",
            "brief_description": "Work demonstrating that large language models can propose scientific hypotheses with zero-shot prompting, i.e., without task-specific fine-tuning.",
            "citation_title": "Large Language Models are Zero Shot Hypothesis Proposers",
            "mention_or_use": "mention",
            "system_name": "Zero-shot LLM hypothesis proposers (Qi et al.)",
            "system_description": "Uses off-the-shelf large language models and zero-shot prompts to generate candidate hypotheses directly from prompts, treating the LLM as a general hypothesis proposer.",
            "system_type": "LLM-based (zero-shot)",
            "scientific_domain": null,
            "hypothesis_generation_method": "Zero-shot natural language prompting of LLMs to generate hypothesis statements without additional supervision or domain-specific fine-tuning.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Survey contextualizes these results with caveats: zero-shot proposals can be numerous but often include vague/unrealistic ideas and hallucinated claims requiring domain verification.",
            "uuid": "e2508.4",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SciAgents",
            "name_full": "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning",
            "brief_description": "A multi-agent framework that uses intelligent agents reasoning over graphs to automate parts of scientific discovery and hypothesis generation.",
            "citation_title": "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning.",
            "mention_or_use": "mention",
            "system_name": "SciAgents",
            "system_description": "Multi-agent architecture where agents operate on knowledge graphs and perform reasoning, graph transformations, or search to generate candidate scientific hypotheses and conceptual links; presented in the survey as a modern approach to automated discovery.",
            "system_type": "multi-agent + knowledge-graph-based",
            "scientific_domain": null,
            "hypothesis_generation_method": "Multi-agent graph reasoning: agents explore and manipulate knowledge graph structures to surface novel associations and candidate hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Survey-level caution: multi-agent and graph-based discovery systems still require downstream experimental validation and are subject to the same hallucination and reasoning limitations as LLM-driven systems.",
            "uuid": "e2508.5",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI Scientist (end-to-end)",
            "name_full": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "brief_description": "A label applied to end-to-end automation frameworks that attempt to run the full scientific loop (idea generation → experiment design/execution → analysis → paper writing) largely driven by LLMs and automation modules.",
            "citation_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (end-to-end automation frameworks)",
            "system_description": "End-to-end pipelines integrating literature review, hypothesis/idea generation (often LLM-driven), automated experiment planning/execution and manuscript drafting; survey references multiple recent works aiming to automate the whole scientific process.",
            "system_type": "LLM-based + automation / workflow orchestration",
            "scientific_domain": null,
            "hypothesis_generation_method": "LLM-driven idea/hypothesis generation possibly combined with retrieval and experiment-planning modules in an integrated workflow.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Often includes automated experiment execution or human-verifiable experiments in the loop according to the cited works, but the survey cautions that many such systems suffer from hallucinations and poor experimental planning.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Survey explicitly warns to 'take these results with a grain of salt' because end-to-end frameworks are typically LLM-powered and inherit hallucination, lack of robust reasoning, poor planning, and low novelty/diversity issues.",
            "uuid": "e2508.6",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AlphaFold",
            "name_full": "Highly accurate protein structure prediction with AlphaFold",
            "brief_description": "A deep learning system that predicts 3D protein structures from sequences at high accuracy, enabling large-scale structure prediction and downstream biological discovery.",
            "citation_title": "Highly accurate protein structure prediction with AlphaFold.",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_description": "A deep neural network model (attention-based/structure-prediction architecture) trained to predict 3D protein structures from amino-acid sequences; cited in the survey as a major example of AI enabling scientific discovery (millions of predicted structures).",
            "system_type": "deep neural network (structure prediction)",
            "scientific_domain": "biology / structural biology",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Validated by comparison to experimentally-determined structures (survey highlights AlphaFold's accurate predictions and impact on biology).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Comparison against experimentally-determined protein structures and large-scale biological validation; survey notes AlphaFold's capability to predict structures at scale.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Survey cites AlphaFold as enabling discovery of millions of protein structures (impactful domain-level discoveries).",
            "limitations": null,
            "uuid": "e2508.7",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Test-set contamination detection",
            "name_full": "Proving Test Set Contamination in Black Box Language Models",
            "brief_description": "Methods to detect memorization / training-set leakage in black-box LLMs; useful to detect whether claims originate from memorized training data rather than genuine reasoning.",
            "citation_title": "Proving Test Set Contamination in Black Box Language Models.",
            "mention_or_use": "mention",
            "system_name": "Test-set contamination / memorization detection methods (Oren et al.)",
            "system_description": "Statistical and forensic techniques to detect whether a language model's outputs stem from memorized training examples (test-set contamination) rather than model generalization; survey cites these methods as part of efforts to detect unintended memorization and data leakage.",
            "system_type": "statistical / forensic detection",
            "scientific_domain": null,
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": null,
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Detects memorization/test-set contamination to identify outputs that may be copied from training data (survey references Oren et al. and related works).",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "These detection methods address memorization/data leakage specifically and do not in themselves verify factual correctness or scientific plausibility of novel model-generated hypotheses.",
            "uuid": "e2508.8",
            "source_info": {
                "paper_title": "Creativity in AI: Progresses and Challenges",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach",
            "rating": 2,
            "sanitized_title": "agatha_automatic_graph_mining_and_transformer_based_hypothesis_generation_approach"
        },
        {
            "paper_title": "Large Language Models are Zero Shot Hypothesis Proposers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems.",
            "rating": 2,
            "sanitized_title": "a_bayesian_machine_scientist_to_aid_in_the_solution_of_challenging_scientific_problems"
        },
        {
            "paper_title": "SciMON: Scientific Inspiration Machines Optimized for Novelty.",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models.",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning.",
            "rating": 2,
            "sanitized_title": "sciagents_automating_scientific_discovery_through_multiagent_intelligent_graph_reasoning"
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Proving Test Set Contamination in Black Box Language Models.",
            "rating": 2,
            "sanitized_title": "proving_test_set_contamination_in_black_box_language_models"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold.",
            "rating": 2,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        }
    ],
    "cost": 0.029663250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>29 Jun 2025</p>
<p>EPFL, SwitzerlandDebjit Paul debjit.paul@epfl.ch 
EPFL, SwitzerlandAntoine Bosselut antoine.bosselut@epfl.ch </p>
<p>METE ISMAYILZADA*
EPFL
Università della Svizzera italiana (USI)
Switzerland</p>
<p>LONNEKE VAN DER PLAS*
Università della Svizzera italiana (USI)
Switzerland</p>
<p>Idiap Research Institute Authors' Contact Information: Mete Ismayilzada*</p>
<p>EPFL
Università della Svizzera italiana (USI)
Debjit PaulSwitzerland
29 Jun 2025E98221E02442D0CCA269E644EC6A607CarXiv:2410.17218v5[cs.AI]
Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition.Machine creativity on the other hand has been a long-standing challenge.With the rise of advanced generative artificial intelligence (AI), there has been renewed interest and debate regarding AI's creative capabilities.Therefore, it is imperative to revisit the state of creativity in AI and identify key progresses and remaining challenges.In this work, we survey leading works studying the creative capabilities of AI systems, focusing on creative problem-solving, linguistic, artistic, and scientific creativity.Our review suggests that while the latest AI models are largely capable of producing linguistically and artistically creative outputs such as poems, images, and musical pieces, they struggle with tasks that require creative problem-solving, abstract thinking and compositionality and their generations suffer from a lack of diversity, originality, long-range incoherence and hallucinations.We also discuss key questions concerning copyright and authorship issues with generative models.Furthermore, we highlight the need for a comprehensive evaluation of creativity that is process-driven and considers several dimensions of creativity.Finally, we propose future research directions to improve the creativity of AI outputs, drawing inspiration from cognitive science and psychology.</p>
<p>Introduction</p>
<p>Computers can't create anything.For creation requires, minimally, originating something.But computers originate nothing; they merely do that which we order them, via programs, to do.</p>
<p>Ada Lovelace</p>
<p>Creativity, the ability to produce novel, useful, and surprising ideas [Boden 2004], is one of the major hallmarks of human intelligence [Guilford 1967].Since the invention of the first known general-purpose mechanical computer (known as Analytical Engine) designed by Babbage [Babbage 1837], the question of whether machines can truly think or create anything new has intrigued the scientific community [Newell et al. 1959;Turing 1950;Wang et al. 2024b].Ada Lovelace, recognized as the first programmer by many, famously stated that the Analytical Engine has no pretensions to originate anything [Lovelace 1843] and Alan Turing, who laid the foundations of computer science, asserted that machines can never take us by surprise [Turing 1950].Nevertheless, alongside the development of personal computers and advancements in Artificial Intelligence (AI), several symbolic-based and stochastic approaches were developed to endow machines with story generation [Lebowitz 1983;Meehan 1977;Turner 1994;y Pérez and Sharples 2001], poetry writing [Masterman 1971;Racter 1984] and music composition skills [Brooks et al. 1957;Hiller and Isaacson 1958].However, these early approaches could not generalize beyond a set of limited domains [Colton et al. 2012;Ji et al. 2020;Yao et al. 2019].</p>
<p>DOMAINS TYPES</p>
<p>DIMENSIONS PROCESSES</p>
<p>Fig. 1.A summary of domains, dimensions, types and processes of creativity covered in this survey.</p>
<p>Fast-forward to now, the advent of the Transformer architecture [Vaswani et al. 2017] and the development of large language models (LLMs) [Zhao et al. 2023c] in the past decade ushered a new age of intelligent systems with remarkable generative, reasoning, coding, mathematical and multimodal capabilities [Bubeck et al. 2023;Gemini 2024;Wei et al. 2022].Transformer-based models can now produce long stories in various domains [Yang et al. 2022;Yao et al. 2019], write poems about diverse topics [Chakrabarty et al. 2022a;Ormazabal et al. 2022a], compose high-fidelity songs [Dhariwal et al. 2020], generate impressive high-quality images and videos [Betker et al. 2020;Brooks et al. 2024] and discover new scientific knowledge [Jumper et al. 2021].</p>
<p>While these remarkable achievements can be seen as signs of the presence of creative capacity in transformer-based language models, it should be noted that these models rely on an astronomically large number of parameters and are trained on massive amounts of public and private data [Brown et al. 2020a].Hence, it is not entirely clear whether the seemingly extraordinary outputs of these models are the result of a truly creative inner process and robust generalization or the result of powerful interpolation and strong memorization skills [Bender et al. 2021;Bender and Koller 2020;Carlini et al. 2022;Hupkes et al. 2022;Marcus 2020;McCoy et al. 2023].Recent works have shown that language models fail at real-world commonsense reasoning and compositionality tasks [Dziri et al. 2023;Ismayilzada et al. 2023], occasionally copy large amounts of text from their training data [Lu et al. 2024b;McCoy et al. 2021], significantly lag behind humans in creative writing [Chakrabarty et al. 2023a;Ismayilzada et al. 2024b], produce less diverse content [Anderson et al. 2024a;Padmakumar and He 2023], struggle with creative problem-solving [Huang et al. 2024b;Tian et al. 2023] and abstract reasoning [Gendron et al. 2023;Mitchell et al. 2023] and suffer from factual inconsistency and hallucination issues [Banerjee et al. 2024;Elazar et al. 2021].While previous works have reviewed developments on some aspects of AI creativity [Amin and Burghardt 2020;Elzohbi and Zhao 2023;Franceschelli and Musolesi 2021b;Lai and Nissim 2024;Oliveira 2017;Rowe and Partridge 1993], a more holistic and broader review of the field is necessary to understand its rapid advancements.</p>
<p>In this work, we provide the general AI audience with a timely summary of the state of the creative capabilities of the latest AI systems.While creativity is a broad concept that can be explored in a wide range of areas, in this survey, we focus on four main areas where machine creativity has been extensively investigated: Linguistic Creativity ( §3.1), Creative Problem-Solving ( §3.2), Artistic Creativity ( §3.3) and Scientific Creativity ( §3.4).These areas capture four crucial pillars of creative thinking in humans: linguistic creativity enables us to manipulate language in novel ways for effective communication of ideas; creative problem-solving helps us find efficient solutions by thinking out-of-the-box; artistic creativity allows for the expression of emotions, ideas, and aesthetics through various media; and scientific creativity drives innovation by enabling the formulation of new hypotheses, theories, and discoveries.Together, these areas represent distinct yet interconnected facets of creativity, providing a comprehensive framework for studying how machines can emulate or assist human-like creative processes across different domains.For each area, we survey representative tasks, resources, methods, and major findings and present a taxonomy of these works in Figure 2. Our review indicates that although the latest AI models are generally proficient in generating linguistically and artistically creative outputs, such as poems, images, and music, they face challenges with tasks demanding creative problem-solving, abstract reasoning, and compositionality.Their outputs often lack diversity and originality, exhibit long-term incoherence, and are prone to hallucinations.We also briefly discuss the emerging challenges brought by generative models concerning copyright and authorship of artworks ( §4).Finally, in the last section ( §5), we argue for a comprehensive evaluation of creativity in AI that considers several dimensions of creativity and the creative process at its core.Furthermore, we discuss future research directions to enhance the creativity of AI systems, potentially drawing ideas from cognitive science and psychology.</p>
<p>Our goal in this survey is to provide a high-level yet comprehensive overview of the state of creativity in AI.We expect our survey to provide researchers working on machine creativity with comprehensive background knowledge and encourage them to explore new avenues for developing intelligent systems that can do creative generation.</p>
<p>Creativity</p>
<p>Defining Creativity</p>
<p>While creativity as a concept seems intuitively easy to understand on the surface, there is still no consensus on what constitutes true creativity.This is primarily due to the subjective nature of creativity, as what is deemed novel and of quality can vary significantly across cultures, disciplines, and periods.Aleinikov et al. [2000] lists more than 100 proposed definitions, and the number keeps growing.Despite the lack of global consensus, there is one definition of creativity that has seen wide adoption by many philosophers and psychologists and has been dubbed as the "standard definition" [Barron 1955;Runco and Jaeger 2012;Stein 1953].According to this definition, creativity requires novelty (a.k.a originality, uniqueness, etc.) and value (a.k.a utility, effectiveness, usefulness, appropriateness, relevance, meaningfulness, etc.).</p>
<p>The novelty criterion is typically self-explanatory to the point that people equate it to creativity in everyday life.However, many theorists have argued that novelty is insufficient for creativity, and value dimension is needed to filter out original nonsense, such as something generated by a truly random process.While value is generally understood as something inherently "good" for the respective audience, there appears to be such a thing as malevolent or "dark" creativity.For instance, one can be creative in producing torture instruments or in committing terrorist atrocities [Gaut 2010].Therefore, the interpretation of the value of a product as being "effective" towards its intended end, regardless of whether that end is morally good or bad, has been suggested as a better alternative [Livingston 2018].However, we should note that evaluating utility or value still requires an outside judgment which is subjective, can be faulty or biased and can change over time and across cultures.This is especially apparent in arts as there are many great artists (Bach, Van Gogh etc.) whose "value" have only been recognized longtime after their death.Moreover, sometimes novelty is itself the value created by the artist because no one has done it before, particularly, in visual arts.Hence, some researchers have recently argued to drop the value criterion altogether from the definition of creativity [Brandt 2021;Weisberg 2015a].</p>
<p>Despite its wide adoption, the sufficiency of the novelty and value conditions for creativity has also been challenged [Arnheim 2001;Gaut 2010;Weisberg 2015b].It has been argued that agency, a capacity to have beliefs, desires or intentional states, is a required attribute of creativity.For example, Gaut [2010] mentions the tectonic movement of the earth's crust that can produce valuable (financially and aesthetically) and sometimes original (new variation) diamonds, but we would hardly call tectonic movements creative.However, mere agency without intentionality is also insufficient.Gaut [2010] illustrates this with an example where a person walking in a studio accidentally knocks over a set of paints, which spill onto a canvas and happen to create a beautiful and original painting.Weisberg [2015a] has gone even further to suggest that creativity is simply intentional novelty.</p>
<p>While the creative process should be intentionally initiated, others have argued that the creative process should involve an element of spontaneity [Kronfeldner 2009].This allows the creative product to induce a surprise in the audience since the output of the process is not foreseen from the beginning.Being ignorant of the end at the outset of the creative process opens the room for creativity as opposed to a mechanical routine or algorithm that, by definition, is exact and excludes any type of spontaneous modifications.To illustrate this contrast Perkins [2001] makes a distinction between reasonable problems (i.e. that can be reasoned out step-by-step such as anagrams) and unreasonable problems (i.e. that are hard to describe with a step-by-step thinking).</p>
<p>The element of surprise has been further developed by [Boden 2004] into a widely recognized third dimension of the "standard definition" of creativity.This new definition can be seen as an elaborated version of the three criteria (i.e.new, useful and non-obvious) used by the United States Patent Office to determine whether an invention can come under patent protection 1 [Simonton  2012].In this survey, we will also take this extended definition as our working definition throughout the paper.</p>
<p>Types of Creativity</p>
<p>While creativity manifests itself in various forms across domains, even within a particular domain, different types of creativity can be distinguished based on its timing or target audience and the difficulty level of the inherent creative process involved.A person might come up with a creative idea that is new to him/her but already invented by someone else in history.This is generally known as intrapersonal or personal (a.k.a psychological) creativity (often denoted as P-creativity), i.e. the product is novel within the frame of a person's life [Boden 2004;Stein 1953;Weisberg 1986].Researchers distinguish it from the interpersonal or historical creativity (often denoted as H-creativity), i.e. the product is novel with respect to the entire history of people such as Einstein's general relativity theory.</p>
<p>Four-C model of creativity, on the other hand, differentiates between four types of creativity corresponding to four levels of difficulty involved in producing creative artifacts [Kaufman and Beghetto 2009].The first major type of creativity is known as little-c creativity which is what we find in everyday life as solutions to minor problems.Examples might include combining unusual ingredients to make a new type of meal or using a hand-held vacuum cleaner on the ceiling to remove flies.Almost everyone possesses this type of creativity in one way or another.The second main type of creativity in this model is the Big-C creativity that includes major works of scientific, technological, social, or artistic importance.Examples could be Darwin's theory of evolution, the invention of the printing press, or Leonardo Da Vinci's painting of the Mona Lisa.In addition to these two major categories, Kaufman and Beghetto [2009] also defines two minor categories of creativity.First is the mini-c creativity for very small-scale cases of creativity such as young children's drawing or their creative experiments with Lego pieces.Second is the Pro-C creativity which is proposed for work produced by professional but non-prominent practitioners such as professional musicians or artists who generate novel work, but do not make historical contributions.</p>
<p>Recently, there has been a suggestion to differentiate three types of creativity corresponding to three major levels of innovation that can be achieved [Hassabis 2018].First can be achieved through interpolation where a prototypical creative artifact is produced by averaging all the artifacts of the same class seen before.While it is an original product that did not exist before, it still relies heavily on the other existing products.An example would be to come up with a novel winning strategy in chess that is a combination of existing different strategies.Consequently, a second type of innovation can be achieved through extrapolation where a creative artifact extends the boundaries of what has been seen before, but is still of the same class.A completely new chess move that is not related to any existing moves can be seen as an example of extrapolative creativity.Finally, the highest level of creativity can be termed as invention where a creative artifact introduces a novel class of its own.Inventing chess itself or any major scientific invention is a perfect example of this type of creativity.This type of creativity typically requires transformation of the existing conceptual space and is also known as transformational creativity [Boden 2004].</p>
<p>Evaluation</p>
<p>Evaluating creativity remains a challenging task in artificial intelligence due to its inherently subjective nature [Lamb et al. 2018].Interestingly, some research work even argued against the quantitative evaluation of creativity, suggesting it is either too domain-specific to be measured effectively [Baer 2012], or that creativity is an inherently human trait that cannot be accurately modeled computationally [Boden 1991;Minsky 1982].However, an overwhelming majority of the scientific community favors the possibility of computational modeling and evaluation of creativity [Veale and Cardoso 2019].Hence, numerous evaluation methods have been proposed in the past [Lamb et al. 2018].However, most of the proposed metrics are either formal frameworks that are hard to implement in practice or manual psychometric creativity tests that require costly human involvement [Kim 2006] or automated metrics that are too domain-specific [França et al. 2016].We refer the reader to Franceschelli and Musolesi [2021b] and Lamb et al. [2018] for more details on formal evaluation frameworks, and here we briefly summarize some of the relevant manual and automated metrics for creativity.</p>
<p>Manual Evaluation.</p>
<p>Since creative products vary greatly in their forms and are hard to characterize with objective measures, the simplest and most common way to evaluate them is to ask other humans to manually rate them based on some criteria associated with creativity, which differs from task to task [Lamb et al. 2015].For example, in story generation, humans are typically asked to rate a generated story on aspects such as interestingness, coherence, relevance, humanlikeness and etc. [Goldfarb-Tarrant et al. 2020;Rashkin et al. 2020;Yang and Jin 2024;Yang et al. 2022].In other tasks where the goal is to produce multiple responses such as common psychometric creativity tests Alternative Uses Task (AUT) [Guilford 1967] and Torrance Tests of Creative Thinking (TTCT) [Torrance 1974], evaluation is centered around four dimensions of creativity: fluency (the total number of meaningful, and relevant ideas generated in response to the stimulus), flexibility (the number of different categories of relevant responses), originality (the uniqueness or rarity of responses) and elaboration (the amount of detail in the responses).</p>
<p>While it is common and straightforward to conduct human evaluation with ordinary humans, some have argued that people who are not experts on a kind of creative artifact might not be good judges of those artifacts [Gervás 2019;Lamb et al. 2015Lamb et al. , 2018;;Mirowski et al. 2022;Veale 2015].This typically results in poor interrater reliability and even when they agree, their judgments do not correlate well with expert judgment [Lamb et al. 2018].Therefore, it is generally recommended to employ Consensual Assessment Technique [Amabile 1983], an evaluation method that relies on the collective judgment of experts in a given field.</p>
<p>Automated Evaluation.</p>
<p>While creativity is generally evaluated by humans, several attempts have also been made to devise automated measures of it [Cook and Colton 2015;França et al. 2016;Jordanous et al. 2015;Maher and Fisher 2012;Organisciak et al. 2023].These measures often target a specific dimension of creativity.Below, we review some automated measures for three dimensions of creativity: novelty, value, and surprise.</p>
<p>Novelty.It is typically defined as the measure of how different an artifact is from other known artifacts in its class [Maher 2010].Then a distance metric is established to quantify this difference based on the attributes of the artifact and the task space.For example, in the text generation task, a notion of semantic distance is commonly employed as a distance measure [Beaty and Johnson 2020;Dunbar and Forster 2009;Harbison and Haarmann 2014;Johnson et al. 2022;Prabhakaran et al. 2013].More specifically, the text is embedded into a vector in semantic space and some distance or dissimilarity metric (e.g.typically 1-cosine_similarity) is used to compute how much semantically different is one text from another.However, the granularity of the text can differ from task to task.For example, in the story generation task, Karampiperis et al. [2014] defines the novelty of a story as the average semantic distance between the dominant terms included in the textual representation of the story, compared to the average semantic distance of the dominant terms in all stories where distance is measured based on the embeddings of terms.</p>
<p>Novelty can also be characterized by the degree an artifact differs from the previously produced works that one has already seen [Elgammal and Saleh 2015;Gunkle and Berlyne 1975].This definition has inspired the development of Creative Adversarial Networks (CANs) [Elgammal et al. 2017] similar to the popular Generative Adversarial Networks (GANs) [Goodfellow et al. 2014a].In CANs, the generator tries to fool the discriminator into thinking its generation is "art" and at the same time, the style of its generation is nothing known to the discriminator.Consequently, the score assigned by the discriminator (more specifically, 1-score) can be used to measure the novelty of the generated artifact as suggested by Franceschelli and Musolesi [2022].</p>
<p>Value.This dimension is generally the hardest to evaluate as it depends on the subjective utility or performance of the artifact which is typically judged by domain experts of that artifact and can radically change across domains [Maher 2010].In visual arts, this might correspond to "beauty", whereas in science to "logical correctness".Therefore, a metric appropriate for its domain should be employed.For example, in open-ended story generation, a minimally useful story can be defined as a relevant, coherent, and meaningful story.In this sense, automated metrics measuring the overall quality of a story can be leveraged [Chen et al. 2022b;Guan and Huang 2020;Xie et al. 2023a,b], however, it is often challenging to measure coherence [Laban et al. 2021;Zhao et al. 2023b].Another more general evaluation of utility has been suggested by Franceschelli and Musolesi [2022] based on the discriminator score in GANs.Since in GANs, the discriminator learns the distribution of the real (and valuable) data, its score can directly be used as a proxy metric to measure value.</p>
<p>Surprise.Also known as unexpectedness, surprise measures the artifact's degree of deviation from what is expected [Maher 2010].Therefore, automatic metrics for surprise tend to be informationtheoretic [Bunescu and Uduehi 2022; Kuznetsova et al. 2013] and estimate the violation of expectation based on uncertainty reduction [Frank 2010;Hale 2006].However, semantic distance-based measures of surprise have also been suggested.For example, in the story generation task, Karampiperis et al. [2014] conceptualizes surprise as the average semantic distances between the consecutive fragments of a given story.Recent work has also suggested an automated measure based on the Bayesian theory of surprise [Baldi and Itti 2010;Franceschelli and Musolesi 2022].</p>
<p>Domains of Creativity</p>
<p>Creativity is a multifaceted concept that spans across various domains, each harnessing its unique form of imaginative thought and innovation.In this section, we will review the state of creativity in AI across four major domains where machine creativity is most extensively explored: linguistics, art, science, and problem-solving.</p>
<p>Linguistic Creativity</p>
<p>The creative aspect of language in linguistics has been discussed since the early days [Chomsky 1965].Chomsky, in this paper, attributes creativity mainly to the essential property of language to provide means to express many thoughts indefinitely.However, several linguists since Chomsky have argued against using this characterization since it does not align with the everyday definition of creativity [Bergs 2019;Sampson 2017;Zawada 2006].Chomsky's theory of grammar might generate an infinite number of sentences; it, however, relies on a fixed set of rules, while creativity requires deviation from rules.In this sense, Sampson [2017] suggests distinguishing between F-creativity (fixed) and E-creativity (extending), where F-creativity refers to the Chomskian interpretation of linguistic creativity (a.k.a productivity in morphology) and E-creativity corresponds to the real linguistic innovation such as metaphors, jokes, neologisms, etc.Some recent works have explored the F-creativity of large language models and found that this task is challenging in general and even harder in more morphological complex languages [Anh et al. 2024;Ismayilzada et al. 2024a;Weissweiler et al. 2023].Most past works however have focused on studying the E-creativity of AI systems which we review in the following sections.</p>
<p>Humor.</p>
<p>Humor is one of the most common ways in which humans creatively use language to express their ideas and feelings.Early works to model humor focused on hand-crafted linguistic templates and wordplay [Raskin and Attardo 1994;Stock and Strapparava 2005;Taylor and Mazlack 2004].Subsequent works have leveraged language's lexical and syntactic properties as humorspecific features for humor detection [Liu et al. 2018b;Yang et al. 2015;Zhang and Liu 2014].The growing interest in computational humor in recent years has resulted in several shared tasks organized by the NLP community [Castro et al. 2018;Hossain et al. 2020a;Meaney et al. 2021;Miller et al. 2017;Potash et al. 2017;Van Hee et al. 2018].Latest works have developed methods based on neural networks and language models to generate and detect humorous content [Amin and Burghardt 2020;Annamoradnejad and Zoghi 2020;Arora et al. 2022;Bertero and Fung 2016;Chen and Soo 2018;Hossain et al. 2019;Peyrard et al. 2021;Ravi et al. 2024;Ziser et al. 2020], jokes [Horvitz et al. 2024;Ren and Yang 2017;Tang et al. 2022;Weller and Seppi 2019;Xie et al. 2021], puns [He et al. 2019;Mittal et al. 2022;Yu et al. 2018], and sarcasm [Chakrabarty et al. 2020a[Chakrabarty et al. , 2022c]].Several datasets have also been proposed to benchmark the humor capacity of LLMs in several languages including English [Horvitz et al. 2024;Hossain et al. 2019Hossain et al. , 2020b;;Jain et al. 2024;Meaney et al. 2021;Miller et al. 2017;Tang et al. 2022], Chinese [Zhang et al. 2019b], Italian [Buscaldi and Rosso 2007], Spanish [Castro et al. 2017], Dutch [Winters and Delobelle 2020] and</p>
<p>Datasets</p>
<p>Humorous headlines [Horvitz et al. 2024;Hossain et al. 2019Hossain et al. , 2020b]], Puns [Miller et al. 2017;Yang et al. 2015], Jokes [Jain et al. 2024;Meaney et al. 2021;Tang et al. 2022;Weller and Seppi 2019;Zhang et al. 2019b;Zhong et al. 2023], [Hasan et al. 2019;Mihalcea and Strapparava 2005]</p>
<p>Methods</p>
<p>Humorous content [Arora et al. 2022;Hessel et al. 2023b;Kayatani et al. 2021;Peyrard et al. 2021;Ravi et al. 2024;Xie et al. 2023c], [Amin and Burghardt 2020;Annamoradnejad and Zoghi 2020;Bertero and Fung 2016;Chen and Soo 2018;Liu et al. 2018b;Ziser et al. 2020], [Radev et al. 2016;Raskin and Attardo 1994;Stock and Strapparava 2005;Taylor and Mazlack 2004;Yang et al. 2015;Zhang andLiu 2014] [Hasan et al. 2019], Pun [He et al. 2019;Mittal et al. 2022;Yu et al. 2018], Sarcasm [Chakrabarty et al. 2020a[Chakrabarty et al. , 2022c]], Joke [Jentzsch and Kersting 2023;Meaney et al. 2021;Ren and Yang 2017;Shahaf et al. 2015;Weller and Seppi 2019;Xie et al. 2021], Figurative Language ( §3.1.2)</p>
<p>Datasets</p>
<p>Metaphor [Chakrabarty et al. 2022cMohammad et al. 2016;Mohler et al. 2016;Stowe et al. 2021Stowe et al. , 2020]],</p>
<p>Simile [Chakrabarty et al. 2021a[Chakrabarty et al. , 2022c]], Idiom [Chakrabarty et al. 2021a, 2022c] [Ames 1987;Brooks et al. 1957 Russian [Blinov et al. 2019].Computational humor has also been explored in multimodal settings involving images, audio, and video in addition to text [Bertero and Fung 2016;Hasan et al. 2019;Hessel et al. 2023a;Radev et al. 2016;Shahaf et al. 2015;Xie et al. 2023c].While the latest methods particularly LLMs show an impressive ability to generate and detect humorous content, recent work has also shown that these models still fail to reliably understand humor [Borji 2023a;Góes et al. 2023a;Hessel et al. 2023a;Kocoń et al. 2023] and generated jokes typically lack diversity [Jentzsch and Kersting 2023] which has been attributed to training on less diverse humor datasets [Baranov et al. 2023].Creative training frameworks have also been developed to improve the humor generation capabilities of LLMs [Zhong et al. 2023].We refer the reader to Amin and Burghardt [2020] for an in-depth survey on computational humor.</p>
<p>3.1.2Figurative Language.Figurative language is a term in language studies encompassing various figures of speech like hyperbole, similes and metaphor [Paul 1970;Roberts and Kreuz 1994;Veale et al. 2016].These elements can be used to achieve a range of communicative goals.Figurative language generation involves transforming a text into a specific figure of speech while maintaining the original meaning [Lai and Nissim 2024].Generating figurative language requires an understanding of abstract concepts, commonsense reasoning, and an ability to make analogies and deviate from literal meaning.Recent works have shown that language models with injected commonsense knowledge can generate textual and visual metaphors [Chakrabarty et al. 2023f, 2021b, similes [Chakrabarty et al. 2020b;He et al. 2023], idioms [Chakrabarty et al. 2021a] and hyperboles [Tian et al. 2021].Chakrabarty et al. [2023a] reveals that metaphors generated by large language models are often incoherent or cliched.Chakrabarty et al. [2023a] highlights the following example of such a metaphor generated by an LLM: "-However, she managed to laugh louder and louder until her laughter transformed into an embrace of the sun's atmosphere." We refer the reader to Lai and Nissim [2024] and Abulaish et al. [2020] for an in-depth survey on the automatic generation and detection of figurative language.</p>
<p>3.1.3Lexical Innovation.Understanding and generating novel words or word compounds is a challenging linguistic task that often requires creativity, commonsense knowledge, and an ability to generalize over seen concepts [Costello and Keane 2000;Wisniewski 1997].Similar noun compounds might have different meanings based on our common understanding.For example, knowing that "chocolate croissant" means a "croissant filled with chocolate" does not necessarily imply that "chocolate bunny" would mean "a bunny filled with chocolate", but rather a piece of "chocolate in the shape of a bunny".Several works have evaluated and analyzed language models on the task of interpreting and predicting the emergence of these noun compounds and found that models generally show a moderate performance [Coil and  Similarly, a recent work explores the linguistic creativity of both large language models and humans by reconstructing their text output from the existing text snippets on the web and finds that the seemingly remarkable creativity of model outputs may be in large part attributable to the remarkable creativity of human-written texts on the web [Lu et al. 2024b].</p>
<p>Creative Problem-Solving</p>
<p>Creative problem-solving is the mental process of searching and coming up with creative solutions to a given problem [Duncker and Lees 1948].It is a challenging task for machines as it not only requires creativity but also commonsense reasoning, and compositional generalization [Davidson et al. 2022].</p>
<p>In addition, creatively solving a problem is usually characterized by two kinds of thinking, namely, convergent and divergent thinking, and involves deep abstraction and analogy-making abilities.</p>
<p>3.2.1 Convergent Thinking.Convergent thinking models creativity in terms of an ability to produce a single optimal solution for a given problem [Guilford 1967].This type of creativity requires one to be able to associate seemingly remote ideas and converge to a unified solution.To evaluate this type of thinking in humans, psychologists have come up with several creativity tests such as Remote Associates Test (RAT) [Mednick 1962] and insight problems [Webb et al. 2017].</p>
<p>For example, the goal in RAT is to connect several unrelated words with one concept, e.g.words "broken", "clear" and "eye" can be connected with the word "glass".Language models have recently been evaluated on problems that require convergent thinking.Lin et al. [2021] tests language models on solving riddles that require creativity and commonsense and finds a significant gap between model and human performance.Naeini et al. [2023] uses the popular British quiz show Only Connect's Connecting Wall that mimics RAT formulation with built-in, deliberate red herrings (i.e.misleading stimuli or distractors) and evaluates large language models such as GPT-4 on these problems.They report poor model performance and show that models are highly susceptible to distractors in the input and manifest a form of fixation effect (a.k.a  functional fixedness or Einstellung effect) [Barber 1960;Smith and Blankenship 1991;Wiley 1998].This type of cognitive bias forces the model to fixate on its past knowledge and prevents it from thinking "out-of-the-box".The same effect is also found when models are evaluated on everyday problems involving unconventional use of objects [Tian et al. 2023].Very recently, large language models such as GPT-4o have been evaluated on the popular New York Times game Connections and have been found to struggle with associating encyclopedic and linguistic knowledge at an abstract level [Samadarshi et al. 2024].Another study investigating both convergent and divergent creativity of language models has revealed that language models also fall short of demonstrating human-like convergent creativity in code generation [Lu et al. 2024c].</p>
<p>Divergent Thinking</p>
<p>. Divergent thinking requires one to conceptualize multiple, often seemingly disconnected ideas [Guilford 1967].It essentially plays the opposite role to convergent thinking and therefore, the goal is to start with a unified idea and diverge from this idea into the space of all ideas to find the ones that are relevant to the task at hand.Psychologists have also devised creativity tests to evaluate humans' divergent thinking abilities, such as Alternate Uses Test (AUT) [Guilford 1967] and Torrance Tests of Creative Thinking (TTCT).AUT tests creativity based on whether the participant can come up with unusual (creative) uses for an everyday object and the results are typically evaluated either manually or using semantic distance.For example, a "brick" can be used as a "paperweight" or "to break a window" and "coffee cup" can be used as "small bowl", or "a hat for an elf" etc. TTCT consists of several verbal and non-verbal tasks such as imagining impossibilities or the consequences of actions.Works evaluating GPT-3 [Brown et al. 2020b] and GPT-4 [OpenAI 2023] on these tests report near-human performance results [Góes et al. 2023b;Guzik et al. 2023;Haase and Hanel 2023;Hubert et al. 2024a;Koivisto and Grassini 2023;Stevenson et al. 2022;Zhao et al. 2024].Other tests that highly correlate with human creativity measured by AUT have also been proposed such as the task of naming unrelated words (a.k.a Divergent Associations Task) [Olson et al. 2021].Some recent works have used this test to evaluate the creativity of large language models and found that models outperform humans [Bellemare-Pepin et al. 2024;Chen and Ding 2023;Cropley 2023].</p>
<p>While language models perform strongly on AUT-like divergent thinking tasks, they, however, struggle when these tasks require some form of lateral thinking or "thinking out-of-the-box" [Huang et al. 2024b].For example, recent works have found that defying default commonsense associations and modeling unexpected or unlikely situations are challenging for large language models [Jiang et al. 2023;Tian et al. 2023;Zhao et al. 2023a].Figure 4a illustrates a creative problemsolving example from Tian et al. [2023] that involves unconventional use of everyday objects.</p>
<p>Abstraction and Analogy-Making.</p>
<p>Conceptual abstraction and analogy-making lie at the core of human cognition and intelligence [Chollet 2019;Hofstadter 2001;Mitchell 2021].These are abilities that enable humans to generalize to new domains, invent novel concepts, and make useful and often surprising connections between concepts.In other words, abstraction and analogy-making serve as foundational building blocks for creative thinking.</p>
<p>Abstraction.As the cornerstone of human intelligence, abstraction, and abstract reasoning are typically evaluated using visual IQ tests in humans.Popular examples of these tests are the RAVEN progressive matrices [Raven 1938], Bongard problems [Bongard 1970] and the recently introduced Kandinsky Patterns [Holzinger et al. 2019], the Abstraction and Reasoning Corpus (ARC) [Chollet 2019] and its variations [Moskvichev et al. 2023].These tests require the participants to identify and complete an abstract visual pattern based on given examples.Although several attempts have been made to solve these tasks using both symbolic-based and neural network-driven approaches [Hu et al. 2023;Lorello et al. 2024;Mirchandani et al. 2023;Santoro et al. 2018;Xu et al. 2022], modern AI systems still struggle with solving RAVEN-like [Ahrabian et al. 2024;Gendron et al. 2023;Odouard and Mitchell 2022;Zhang et al. 2019a] and ARC-like tasks [Mitchell et al. 2023;Moskvichev et al. 2023;Odouard and Mitchell 2022;Xu et al. 2023;Zhang et al. 2021].Analysis of abstraction via a serial reproduction task [Langlois et al. 2021] where participants are asked to produce a textual stimulus for the next participant upon observing a visual stimulus and vice versa, has suggested that GPT-4 unlike humans relies heavily on linguistic representations even in vision-only paradigm [Kumar et al. 2024].Figure 5 illustrates an example from the ARC task [Chollet 2019].The problems in this corpus are quite hard to solve to the extent that this task has been recognized as the de facto benchmark for measuring progress towards Artificial General Intelligence (AGI) and a public competition with a grand prize of $1, 000, 000 has recently been launched2 .At the time of writing this paper, the highest score is 49.5% far from the passing threshold of 85% (human-level).</p>
<p>Analogy-Making.In its basic form, analogy-making is the ability to identify a relation between two concepts and apply it to a new concept.For example, Paris is to France as Tokyo is to Japan (i.e.capital:country relation).Early approaches to computational analogy-making were symbolic-based and required extensive hand-coded input i.e. structured representations of both the entities and their relations [Falkenhainer et al. 1989;Gentner 1983;Turney 2008].Later, word embedding models based on neural networks were shown to exhibit analogy-making abilities at the word level and most works focused on a limited set of analogy types based on a handful of relations that are often of a morphological nature.[Gladkova et al. 2016;Marquer et al. 2022;Mikolov et al. 2013].These do not encompass the typical analogical reasoning humans perform in everyday life about complex  [Lewis and Mitchell 2024;Musker et al. 2024].Figure 4b illustrates an example from the analogical reasoning over narratives benchmark [Sourati et al. 2023].</p>
<p>Artistic Creativity</p>
<p>Artistic creativity is the ability to produce original, imaginative, and expressive works in various art forms, such as creative writing, poetry, visual arts, music, dance, theater, and more.In this section, we will focus on the advancements made in AI to produce creative stories, poetry, visual, and musical content automatically and also point out the remaining challenges.</p>
<p>Story Generation.</p>
<p>Storytelling is at the heart of human communication, a powerful tool for connecting and conveying ideas effectively [Suzuki et al. 2018].It requires creativity, particularly when crafting an engaging and compelling narrative.Early approaches to this task focused on algorithmic planning based on character traits and social and physical constraints [Lebowitz 1984;Meehan 1977].With the advent of powerful neural networks, the focus shifted to machine learningbased data-driven approaches [Akoury et al. 2020;Du and Chilton 2023;Fan et al. 2018;Hong et al. 2023;Louis and Sutton 2018].While these networks are trained on large datasets of stories and prompted to directly generate a new story, often producing locally coherent narratives, they suffer from long-term coherence, irrelevance to premise, and repetitive text problems [Yao et al. 2019].Latest approaches have addressed these problems by using content planning and recursive prompting techniques where a high-level plan of the story is first generated, followed by iterative prompting that aims to generate the story in multiple steps based on the plan [Goldfarb-Tarrant et al. 2020;Yang et al. 2022;Yao et al. 2019].Since language models are designed for open-ended text generation, controlling the attributes of its generations (e.g.topic, characters) is another major challenge [Dathathri et al. 2019].While several methods have been developed towards controllable text generation [Chung et al. 2022;Dathathri et al. 2019;Pascual et al. 2021;Paul and Frank 2021;Rashkin et al. 2020;Tambwekar et al. 2018], language models still struggle with following constraints [Sun et al. 2023].In addition, long-term factual inconsistency and hallucinations still remain as major issues in language model generated texts [Banerjee et al. 2024;Elazar et al. 2021;Tam et al. 2022;Zhang et al. 2023].</p>
<p>Language models have also been evaluated on their ability to produce and judge creative content as a professional writer [Chakrabarty et al. 2023b;Gómez-Rodríguez and Williams 2023;Marco et al. 2024].Chakrabarty et al. [2023b] generates short stories from LLMs based on the plots of popular fictional stories published in the New York Times and conducts a fine-grained expert assessment of both model-generated and original stories.Their study shows that LLMs significantly lag behind seasoned writers in producing inherently creative content.Studies also demonstrate that LLMs are unreliable evaluators of creativity [Chakrabarty et al. 2023b;Chhun et al. 2024].Additionally, [Tian et al. 2024] finds that LLM-generated stories are positively homogenous and typically lack suspense and tension.LLMs have also been shown to produce more complex, but less creative stories than average humans [Ismayilzada et al. 2024b].</p>
<p>To complement the shortcomings of LLMs in creative content generation, recently several works have developed frameworks to use these models as creative assistants for humans and these collaborative systems have shown strong performance across domains and editing tasks [Chakrabarty et al. 2023c;Mirowski et al. 2022;Schick et al. 2022;Swanson et al. 2021;Yuan et al. 2022].However, recent works have also demonstrated that the output of human and language model collaboration lacks lexical and idea diversity [Anderson et al. 2024b;Padmakumar and He 2023].Particularly, adapting language models with human feedback [Ouyang et al. 2022] has been found to be a main contributing factor in diversity reduction [Bai et al. 2022;Mohammadi 2024;Padmakumar and He 2023].</p>
<p>Poetry.</p>
<p>Poetry is a form of literary expression that uses rhythmic and often condensed creative language to evoke emotions, convey ideas, or tell stories.Early approaches to poetry generation have been based on hand-crafted templates, heuristics, and linguistic features of the target language which were limited in their expressivity [Colton et al. 2012;Manurung 2004;Manurung et al. 2000Manurung et al. , 2012;;Masterman 1971;Milic 1970;Oliveira 2012;Racter 1984].However, recent statistical approaches using (recurrent) neural networks [Ghazvininejad et al. 2016;Lau et al. 2018;Zhang and Lapata 2014] and language models [Agarwal and Kann 2020;Belouadi and Eger 2022;Chakrabarty et al. 2022b;Ormazabal et al. 2022b;Popescu-Belis et al. 2023;Tian and Peng 2022;Van de Cruys 2020] have been shown to generate high-quality poems.While these generations almost always follow natural poetic style with appropriate rhyme and meter, they typically fail to express a poetically deep meaning [Chakrabarty et al. 2023d;Elam 2023].Figure 3 illustrates the qualitative difference between human and machine-generated poems.We refer the reader to [Elzohbi and Zhao 2023;Oliveira 2017] for an in-depth survey on automatic poetry generation.</p>
<p>Visual Creativity.</p>
<p>Humans have been producing visual content to convey emotions, concepts, and narratives since ancient times, from cave paintings and hieroglyphics to classical and Renaissance art masterpieces.For centuries, visual creativity was primarily the domain of professional artists, however, the invention of photography in the 19th century and the traditional image editing software, such as Adobe Photoshop in the past few decades enabled ordinary individuals to produce visually creative outputs without the need for formal artistic training.The advancements of AI have further transformed the landscape of visual creativity, pushing the boundaries of what can be created and who can create it.Early works employed Generative Adversarial Networks (GAN) [Goodfellow et al. 2014b] and Convolutional Neural Networks (CNN) [LeCun et al. 1989] to model images [Li and Wand 2016;Radford et al. 2015;van den Oord et al. 2016b,a] and generate images by applying specific transformations such as style transfer [Abdal et al. 2019;Dumoulin et al. 2016;Gatys et al. 2015;Johnson et al. 2016;Karras et al. 2018Karras et al. , 2019]], super-resolution [Dong et al. 2014;Ledig et al. 2016], colorization [Zhang et al. 2016a] and inpainting [Pathak et al. 2016] or learning a generic mapping between two images [Huang et al. 2018a;Isola et al. 2016;Richardson et al. 2020] or conditioning on text [Mansimov et al. 2015;Mirza and Osindero 2014;Reed et al. 2016a,b;Yan et al. 2015;Zhang et al. 2016b].In recent years, the development of Transformer architecture [Vaswani et al. 2017] and Diffusion models [Ho et al. 2020] has further pushed AI-driven art to new heights.Trained on large amounts of multimodal data, these models are capable of generating from arbitrary instructions not only high-quality images [Brooks et al. 2022;Chakrabarty et al. 2023e;Gafni et al. 2022;Geng et al. 2023;Hertz et al. 2022;Huang et al. 2022;Nichol et al. 2021;Patashnik et al. 2021;Ramesh et al. 2022Ramesh et al. , 2021;;Rombach et al. 2021;Ruiz et al. 2022;Saharia et al. 2022;Shen et al. 2024] but also short photo-realistic videos [Arnab et al. 2021;Brooks et al. 2024;Gupta et al. 2023;Ho et al. 2022a,b;Kondratyuk et al. 2023;Luo et al. 2023;Singer et al. 2022;Tulyakov et al. 2017;Vondrick et al. 2016;Xing et al. 2023;Yan et al. 2021].</p>
<p>Despite their impressive quality, AI systems still exhibit trivial errors in their generations.Recent work has shown that these models struggle to effectively compose objects with different attributes and relationships [Conwell and Ullman 2022;Huang et al. 2023;Leivada et al. 2022;Marcus et al. 2022;Murphy et al. 2024;Thrush et al. 2022;Zarei et al. 2024], fails to reliably capture common syntactic processes such as negation, word order, comparatives etc. [Leivada et al. 2022;Marcus et al. 2022;Murphy et al. 2024], struggles with representing numbers and texts in images [Borji 2023b;Marcus et al. 2022], often fall short when it comes to accurately depicting the intricate details of human extremities such as hands and fingers [Borji 2023b] and lacks robust commonsense reasoning ability [Borji 2023b;Marcus et al. 2022;Rassin et al. 2022;Thrush et al. 2022].Similarly, video generation models often suffer from a lack of reliable spatial reasoning, appearance inconsistency, temporal inalignment, body deformation and occlusion issues [Brooks et al. 2024;Lei et al. 2024].</p>
<p>Musical Creativity.</p>
<p>Music is another major artistic medium that allows individuals to express emotions, ideas, and cultural narratives through sound, often transcending language barriers to connect people across diverse backgrounds and experiences.Automatic music generation using computers has also a long history dating back to the 1950s [Ji et al. 2023].Early attempts at music generation employed rule-based methods [Hiller and Isaacson 1958], stochastic models (typically Hidden Markov Models) [Ames 1987;Brooks et al. 1957;Farbood and Schöner 2001], evolutionary algorithms [Biles et al. 1994;Cope 1996;Lavrenko and Pickens 2003] and recurrent neural networks [Eck and Schmidhuber 2002;Todd 1989].However, these methods suffered from long-range incoherence and produced only short pieces often with low music quality [Ji et al. 2020].</p>
<p>With the advent of powerful deep generative models, it became possible to capture the long-term structure of polyphonic music.Recent years have seen models that can compose multi-instrument polyphonic pieces using variational auto-encoders [Kingma and Welling 2013;Roberts et al. 2018], generative adversarial networks [Dong et al. 2017;Goodfellow et al. 2014a;Yang et al. 2017;Yu et al. 2016] and transformers [Agostinelli et al. 2023;Copet et al. 2023;Deng et al. 2024;Dhariwal et al. 2020;Donahue et al. 2019;Huang et al. 2018b;Huang and Yang 2020;Payne 2019;Qu et al. 2024;Yuan et al. 2024].While music generated by these systems often seems quite impressive, an automatic objective evaluation of music composition remains a challenge because of its subjective and complex nature [Yang and Lerch 2018].It is not yet entirely clear whether the AI-generated pieces are truly novel as past work has found that deep learning-based music generation models gradually copy increasingly distinctive chunks from pieces in the training set [Yin et al. 2021].Recent studies also show that humans exhibit a preference for human compositions over AI compositions and they report something "off" about the latter such as a lack of sense of coherence or consistency, odd note choices, unnecessary complexity, repetition, uninterestingness, and failure to come to a resolution [Sarmento et al. 2024].</p>
<p>Scientific Creativity</p>
<p>Scientific creativity refers to the ability to generate novel ideas, approaches, or solutions within the realm of science, often leading to new discoveries, theories, or technologies.Automating the process of scientific discovery [Kramer et al. 2023;Savage 2012;Waltz and Buchanan 2009] has long been a focus of AI research dating back to the 1970s when early attempts mainly targeted automated equation discovery and symbolic regression and were based on methods such as heuristic search and genetic programming [Dzeroski and Todorovski 1993;Koza 1994;Langley 1977;Rzevski et al. 1987;Schmidt and Lipson 2009;Todorovski 1997].Recent methods, however, often employ Bayesian statistics [Guimerà et al. 2020] and neural networks [Chen et al. 2022a;Cranmer et al. 2020;Garcon et al. 2021;Petersen and Landajuela 2019;Udrescu et al. 2020].</p>
<p>Another line of work has focused on automating the discovery of other scientific knowledge such as generating new mathematical conjectures or theories [Buchberger et al. 2006;Chen et al. 2016;Fajtlowicz 1988;Raayoni et al. 2021;Wu and Tegmark 2019], automatically proving theorems [Hubert et al. 2024b;Trinh et al. 2024], discovering new concepts [Hakuk and Reich 2020;Iten et al. 2020;Lenat and Brown 1984] and predicting new molecular structures [Abramson et al. 2024;Jumper et al. 2021;Lindsay 1980;Zambaldi et al. 2024] among others.A notable example in this area is the recent AlphaFold model [Jumper et al. 2021] that can predict millions of intricate 3D protein structures which has the potential to significantly accelerate research in biology.</p>
<p>While the above works have mainly targeted one aspect of the scientific process, namely the automatic discovery of particular scientific knowledge, there have also been attempts to partially or fully automate the entire process itself.The scientific process typically starts with a scientific question or an idea that is then used to formulate a hypothesis, followed up with designing and running experiments and analyzing results to test the validity of the hypothesis and ends with communicating the findings to the scientific community [Kramer et al. 2023].Recently, the field has seen a surge in the development of frameworks using neural networks and especially, large language models to automate several steps of the scientific process such as literature review [Skarlinski et al. 2024], idea generation [Baek et al. 2024;Castelo et al. 2024;Girotra et al. 2023;Si et al. 2024;Wang et al. 2023Wang et al. , 2024a]], hypothesis generation [Ghafarollahi and Buehler 2024;Majumder et al. 2024;Qi et al. 2023;Sybrandt et al. 2020;Wang et al. 2024a;Yang et al. 2023] and paper writing [Altmäe et al. 2023;Wang et al. 2019].Yet other works have gone further to introduce a so-called "AI Scientist" that automates almost the entire scientific process from the idea generation to experiment execution to even paper writing [Boiko et al. 2023;Ifargan et al. 2024;King et al. 2009;Li et al. 2024b;Liu et al. 2024;Lu et al. 2024a].</p>
<p>While the latest advancements in the automation of scientific creativity are remarkable, these results should be taken with a grain of salt.Most of the recent end-to-end automation frameworks are powered by LLMs, hence, they face the same challenges and issues we discussed in the previous sections such as hallucinations, lack of content diversity, novelty, and robust reasoning capabilities.For example, one of the aforementioned large-scale idea generation studies [Si et al. 2024] finds that out of 4, 000 LLM-generated ideas only 200 are unique.Their qualitative analysis also reveals some common failure modes such as vague implementation details, misuse of datasets, inappropriate baselines, unrealistic assumptions, and overall poorly-motivated ideas.Similarly, another study benchmarking the machine learning experimentation capabilities of LLMs reports hallucinations and poor planning as some of the major issues with these models [Huang et al. 2024c].</p>
<p>Another important aspect of scientific discovery is the explainability [Li et al. 2021] which helps humans prevent or better prepare for a possible future technological singularity [Good 1965;Ulam 1958].However, current LLMs are largely black-box AI systems, and allowing them to make discoveries that are incomprehensible to humans may lead to a scenario where human knowledge is left far behind the machine's knowledge resulting in machines that humans can't control [Good 1965].</p>
<p>Creativity and Copyright</p>
<p>Our brief survey into the methods used to produce creative outputs showed that the predominant approach is currently the generative deep learning techniques, especially LLMs.These models typically have billions of adjustable parameters [Brown et al. 2020b] and are trained on massive amounts of public and private data [Raffel et al. 2023].Consequently, these models have been found to exhibit strong memorization skills [Carlini et al. 2022[Carlini et al. , 2018] ] such that they can sometimes copy large passages [Chang et al. 2023;McCoy et al. 2021] or replicate images from their training data [Somepalli et al. 2022].While this could be of little concern when the duplicated content is public and generic, however, the training datasets of popular LLMs are often undisclosed and can include private and copyrighted data leading to concerns about copyright infringement and privacy violation [Franceschelli and Musolesi 2021a].Although several approaches have been developed to detect [Carlini et al. 2021;Duarte et al. 2024;Li et al. 2024a;Oren et al. 2023;Shi et al. 2024] and prevent [Hans et al. 2024;Ippolito et al. 2022;Kandpal et al. 2022;Zhao et al. 2022] unintended memorization in LLMs, major questions concerning the use of copyrighted material for training and authorship of the machine-generated content remain unresolved [Abbott and Rothman 2023;Franceschelli and Musolesi 2021a].Recent lawsuit between The New York Times and OpenAI [Grynbaum and Mac 2023] and the class action3 against Stable Diffusion, Midjourney, and DeviantArt [Brittain 2023] have further highlighted the urgency of the matter and the need for clear legal frameworks that address the complex issues surrounding intellectual property rights, ethical use, and the boundaries of fair use in AI development.</p>
<p>More specifically, two key questions concerning copyright and authorship are of interest and here we briefly discuss them with respect to machine-generated artworks.We refer the reader to Franceschelli and Musolesi [2021a] and Abbott and Rothman [2023] for a detailed discussion of these questions.</p>
<p>Is it copyright infringement to use protected works for the training of generative models?</p>
<p>To answer this question, we will review the implications of the existing relevant laws from the US and EU.Under the US Law Code, reproduction of a copyrighted work can be allowed if the use can be considered a fair use of the work [Netanel 2011].Analyzing the criteria used to determine fair use, Franceschelli and Musolesi [2021a] concludes that it is not straightforward to assess this for generative deep models and if these models do not add any form of novelty to their output.Their outputs may not qualify for fair use, which can potentially derail the progress in AI [Sobel 2017].</p>
<p>Under the EU law, on the other hand, the use of lawfully accessible protected work for training is permitted as long as 1) the rightsholder of the used data has not reserved the right to withhold its data from being reproduced and 2) the accessed data is retained only for the time required for the purposes of scientific research [Franceschelli and Musolesi 2021a].However, Franceschelli and Musolesi [2021a] also notes while the second criterion is reasonably easy to satisfy, the first criterion is hard to verify in practice because nowadays, models are being trained on large amounts of data published on the internet for which there is no centralized repository allowing to filter reservation-free works.Finally, whether providers of such a repository or the developers of the models should be forced to perform this check is unclear.</p>
<p>Who is the author (if someone) or who will own the copyright on the generated work?To answer this question, first, we have to make a distinction between the AI-assisted and AI-generated content.If the generative model is merely used as a tool to assist a human to produce a creative artwork, then the human will be considered the author and own the copyright.However, it becomes tricky to determine the authorship and the copyright status of the work that is generated mostly by AI with little human involvement (e.g.human as prompter).First, let's consider the authorship issue.Some have argued that for an author to exist there has to be a message that the author wants to convey through their work, but since no one can reliably predict the output of a generative model, no author exists [Ginsburg 2018].However, if we suppose that there is an author, then there are mainly three contenders in question: 1) the person who developed the AI model (developers) 2) the person who used the AI model to produce creative work (users), and finally 3) the AI model itself.Since the existing laws in most countries only attribute copyright to a human, but not to a machine, the main tension is around deciding whether to attribute the authorship (also the copyright) to users or developers [Abbott and Rothman 2023;Deltorn and Macrez 2018;dos Santos and Machado 2020;Guadamuz 2017].Some have argued that the criterion to determine authorship should center around the incentives to create and promote the work, not the ideation and creation of the work itself [Miller 1993] and since the users of the generative models are best positioned to do so, they should be assigned the authorship [Denicola 2016;Franceschelli and Musolesi 2021a;Samuelson 1986].Another argument supporting this assignment is by ruling out the developer as the author since they just create the potentiality for the creation of the output, but not its actuality [Franceschelli and Musolesi 2021a;Samuelson 1986].Using the analogy proposed by Ralston [2005], it would be similar to claim a knife manufacturer is more responsible for murder than the person who wielded the knife or assigning copyright to the teacher of the painter rather than the painter himself/herself [Franceschelli and Musolesi 2021a].Finally, arguments in favor of AI authorship have also been made recently suggesting that this will promote transparency, efficient allocations of rights, and even counterintuitively protect human authors [Abbott and Rothman 2023].</p>
<p>Future Directions</p>
<p>In the previous sections, our brief exploration into the creativity of modern AI systems revealed that these systems exhibit some capacity for producing linguistically and artistically creative outputs and thinking creatively.However, true human-like creative abilities seem to be still out of reach, as indicated by challenges with tasks demanding creative problem-solving [Jiang et al. 2023;Tian et al. 2023], abstract reasoning [Gendron et al. 2023;Mitchell et al. 2023], and compositionality [Huang et al. 2023;Murphy et al. 2024].Some studies also highlighted major issues in machine outputs, such as lack of originality [Chakrabarty et al. 2023b;Lu et al. 2024b], diversity [Anderson et al. 2024b;Padmakumar and He 2023] and incoherence [Sarmento et al. 2024;Tam et al. 2022].From the Four-C model perspective, these models seem to manifest only mini-c or little-c type of creativity while Pro-C and Big-C creativity remain elusive.Similarly, current AI models exhibit strong interpolation and moderate extrapolation capabilities.However, they are still far from truly inventing a completely new type of creative artefact.In this section, we discuss potential research directions that can help us better measure and improve the creative abilities of AI systems.</p>
<p>Evaluating Creativity</p>
<p>5.1.1Creative Process.Cognitive scientists and psychologists have proposed theoretical frameworks to evaluate creativity such as characterizing it based on input, process and output [Jordanous 2012; Pease et al. 2002;Ritchie 2007] or four Ps: person, product, process and press [Jordanous 2016;Rhodes 1961].A common thread across all these theories is their emphasis on evaluating the process aspect of creativity.However, most works in AI, including the ones we reviewed before, focus on evaluating and analyzing creativity from the output or product perspective.Creative process, on the other hand, is an equally (or perhaps more) important aspect of creativity that can tell us how creativity "arises" in the first place and what the key ingredients involved [Colton 2008].For example, in computational creativity, one popular theory by Boden [2004] defines the creative process in terms of manipulations over a conceptual space.This theory divides creativity into three types: combinatorial that makes unfamiliar connections between familiar concepts (e.g.creating hybrid fictional creatures such as pegasus, sphinx, or mermaid), exploratory that involves an open-ended search in a conceptual space (e.g. a novel chess move) and transformational that requires a fundamental transformation of the existing conceptual space (e.g.non-Euclidean geometry4 ).Another popular theory by Wallas [1926] explains the creative process in four stages akin to how scientists develop their ideas: preparation stage where the problem at hand is investigated in all directions, information is gathered and analyzed, incubation stage where you step back from the problem and let your unconscious work through it in the background, inspiration stage where a creative insight is typically realized (an "Aha!" or "Eureka!" moment) and finally, verification stage where you test, evaluate and build further on your creative idea to make it perhaps useful.</p>
<p>While AI systems produce seemingly creative outputs, the nature of the creative process they employ (if any) remains unknown.Only very recently attempts have been made to study the creative process of machines [Nath et al. 2024] which analyzes the creative process of language models and humans to solve AUT task using response pathways (persistent vs. flexible) [Baas et al. 2013;Nijstad et al. 2010] and finds that while humans are able to follow a mixture of pathways, models are biased towards either one of them pointing to a limited capacity.Hence, analyzing the creative process of machines is an emerging and exciting area for which much work remains to be explored.We believe a strong collaboration between the computational creativity [Colton and Wiggins 2012;Veale and Cardoso 2019] and NLP communities drawing ideas from past research on studying human creative process and techniques from research on (mechanistic) interpretability5 [Bereska and Gavves 2024;Saphra and Wiegreffe 2024] could lead to a better understanding of the creative capacity of AI systems.</p>
<p>Dimensions of Creativity.</p>
<p>As we discussed earlier, there are many dimensions of creativity, but most works generally focus on evaluation of the novelty and usefulness dimensions.However, surprise, agency and spontaneity dimensions are also equally important.Humans typically communicate an emotion or a deeper meaning through creative products and their creative process is characterized by spontaneous "Aha" or "Eureka" moments coupled with deliberate decisions made at each step of the way6 .However, current AI systems lack agency and are typically trained to generate the most likely output leaving no room for any intentional or spontaneous action [Franceschelli and Musolesi 2023;Peeperkorn et al. 2023].Therefore, a holistic evaluation of machine creativity should involve consideration of all these different dimensions that characterize human creativity.</p>
<p>Improving Creativity</p>
<p>Recent years have seen a surge in human-AI creative collaboration [Vinchon et al. 2023] popularized by the introduction of chat-based products such as ChatGPT7 and Gemini8 .However, the poor creative capacity of current AI systems necessitates the innovation of new techniques to improve the creativity of their outputs.In this section, we discuss several possible directions to take.</p>
<p>Creative Architectures.</p>
<p>As we argued before, current AI architectures optimized for the most likely outcome might have fundamental limitations to exhibit true human-like creativity.In fact, by definition, current AI models are optimized to model the training distribution while creating something new requires the model to diverge from its learned distribution.Therefore, innovating at the architecture level to endow machines with mechanisms to actively diverge from the training data and a capacity for agency and spontaneity might be a necessary step towards robust creativity.An emerging new research area called active divergence attempts to optimize models for creativity using methods such as novelty search, divergent fine-tuning, and objective functions targeting different dimensions of creativity [Broad et al. 2021;Bunescu and Uduehi 2019;Elgammal et al. 2017;Guimaraes et al. 2017].</p>
<p>Creative Prompt</p>
<p>Engineering.Natural language-based interaction with the current AI systems has created an intuitive playground to elicit more capabilities from these systems [Qiao et al. 2022].These so-called prompt engineering techniques have also been shown to enhance the creativity of large language models [Mehrotra et al. 2024;Nair et al. 2024;Summers-Stay et al. 2023;Tian et al. 2023].We can draw ideas from psychology that has shown techniques such as brainstorming [Osborn 1957], competence injection [Liu and Xu 2020] and threatening situations [Riley and Gabora 2019] stimulate creativity of humans.Hence, designing prompts inspired by these methods is a promising direction to get the most out of future AI systems.</p>
<p>Creative</p>
<p>Decoding.An important component in natural language generation is the decoding strategy which is a significant contributor to the quality of the generation [Meister et al. 2022].Past work has shown that simple greedy decoding results in repetitive and uninteresting generations [Li et al. 2023] and numerous powerful decoding algorithms have been developed to address these problems [Fan et al. 2018;Holtzman et al. 2019;Meister et al. 2023].These decoding strategies mainly target generating human-like text and do not directly target creativity.A popular approach is to increase the randomness of the output by increasing the temperature parameter, however, recent work shows that this parameter is weakly correlated with the novelty of the output [Peeperkorn et al. 2024].A potential direction could be to devise new creative decoding algorithms that go beyond the temperature parameter by injecting semantic planning or intentionality [Franceschelli and Musolesi 2024] and employing information-theoretic measures of novelty, utility, and surprise [Bunescu and Uduehi 2022;Heinen and Johnson 2017;Kuznetsova et al. 2013].</p>
<p>Conclusion</p>
<p>In conclusion, while the rapid advancements in AI, particularly through state-of-the-art models such as large language models, diffusion models, etc., have demonstrated impressive capabilities in generating creative outputs, the question of genuine machine creativity remains unresolved.This survey has explored key areas of linguistic creativity, creative problem-solving, and artistic and scientific creativity, providing a comprehensive overview of the state of AI creativity.We also discussed pressing copyright and authorship issues with generative artworks, highlighted major challenges facing current AI systems and proposed potential research directions on how to evaluate and improve the creativity of these systems.We believe our suggestions can help future research to determine if machines can achieve a human-like creative process, ultimately enriching our understanding of artificial intelligence and its capabilities.</p>
<p>Fig. 3 .
3
Fig.3.Illustration of a qualitative difference between poetry written by humans and machines.Left: Poem about love published on New Yorker.Right: Poem about love generated by GPT-4o.While New Yorker poem draws deep metaphoric parallels between linguistic features of French and love, the GPT-4o generated poem merely describes love using cliché phrases.A similar comparison was made between Grok and the same New Yorker poem in[Chakrabarty et al. 2023d].</p>
<p>(a) Example from the MacGyver dataset for creative problem-solving[Tian et al. 2023].Problems in this dataset require innovative usage of objects and involve both convergent and divergent thinking.(b)Example from Analogical Reasoning over Narratives benchmark[Sourati et al. 2023].The task is to distinguish between analogous narrative  and distractor  for the query narrative .</p>
<p>Fig. 4 .
4
Fig. 4. Examples from Creative Problem-Solving datasets.</p>
<p>Fig. 5 .
5
Fig. 5. Example from the Abstraction and Reasoning Corpus (ARC) [Chollet 2019] designed to test the abstractive thinking capabilities of both humans and machines.</p>
<p>A</p>
<p>Fig. 6.Illustration of progress and challenges in image generation.Left: A creative image made by Imagen [Ho et al. 2022a] based on text instruction.Right: Various model generations showing failures in compositionality [Huang et al. 2023], commonsense [Rassin et al. 2022], object relationships [Marcus et al. 2022], and negation [Murphy et al. 2024].</p>
<p>;Eck and Schmidhuber 2002;Farbood and Schöner 2001;Hiller and Isaacson 1958;Lavrenko and Pickens 2003;Todd 1989]
Equation discoverySymbolic RegressionScientific Creativity ( §3.4)Knowledge discoveryconjectures [Buchberger et al. 2006; Chen et al. 2016; Fajtlowicz 1988; Raayoni et al. 2021; Wu and Tegmark 2019], theorem-proving [Hubert et al. 2024b; Trinh et al. 2024], concepts [Hakuk and Reich 2020; Iten et al. 2020; Lenat and Brown 1984], molecular structures [Abramson et al. 2024; Jumper et al. 2021; Lindsay 1980; Zambaldi et al. 2024]Scientific process automationliterature review [Skarlinski et al. 2024], idea generation [Baek et al. 2024; Castelo et al. 2024; Girotra et al. 2023; Si et al. 2024; Wang et al. 2023, 2024a], hypothesis generation [Ghafarollahi and Buehler 2024; Majumder et al. 2024; Qi et al. 2023; Sybrandt et al. 2020; Wang et al. 2024a; Yang et al. 2023], paper writing [Altmäe et al. 2023; Wang et al. 2019], AI Scientist [Boiko et al. 2023; Ifargan et al. 2024; King et al. 2009; Li et al. 2024b; Liu et al. 2024; Lu et al. 2024a]
[Langley 1977]Todorovski 1993;Koza 1994;Rzevski et al. 1987;Todorovski 1997]nmer et al. 2020;Petersen and Landajuela 2019;Schmidt and Lipson 2009],[Dzeroski and Todorovski 1993;Koza 1994;Rzevski et al. 1987;Todorovski 1997], BACON[Langley 1977]Fig.2.Taxonomy of creativity in AI covering areas of linguistic creativity, creative problem-solving, artistic and scientific creativity.Note that this taxonomy is not exhaustive, but rather a representative view of the key works.</p>
<p>[McCoy et al. 20217;LencionWeissweiler et al. 2023a et al. 2013].Other works have successfully trained neural networks to generate neologisms (i.e.newly coined words or phrases)[Das and Ghosh 2017;Lencione et al. 2022].On the other hand, previous works have also shown that large language models can fail at linguistic generalization tasks such as morphologically deriving new words from nonce roots[Ismayilzada  et al. 2024a;Weissweiler et al. 2023] and can occasionally duplicate large amounts of text from its training data[McCoy et al. 2021].</p>
<p>http://www.uspto.gov/inventors/patents.jsp
https://arcprize.org/
A class action is a type of civil lawsuit brought on behalf of many similarly situated people who have been harmed in the same way by the same entity.
https://en.wikipedia.org/wiki/Non-Euclidean_geometry
https://www.neelnanda.io/mechanistic-interpretability
https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art
https://chat.openai.com
https://gemini.google.com
AcknowledgmentsWe gratefully acknowledge the support of Swiss National Science Foundation (grant 205121_207437: C -LING).We also thank Angelika Romanou for her help with creating figures for this paper., Università della Svizzera italiana (USI), Switzerland.
Disrupting Creativity: Copyright Law in the Age of Generative Artificial Intelligence. Ryan Abbott, Elizabeth Rothman, 2023. 2023Elsevier</p>
<p>Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?. Rameen Abdal, Yipeng Qin, Peter Wonka, IEEE/CVF International Conference on Computer Vision (ICCV). 2019. 2019. 2019102350964</p>
<p>Accurate structure prediction of biomolecular interactions with AlphaFold 3. Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, Sebastian W Bodenstein, David A Evans, Chia-Chun Hung, O' Michael, David Neill, Kathryn Reiman, Zachary Tunyasuvunakool, Akvilė Wu, Eirini Žemgulytė, Charles Arvaniti, Ottavia Beattie, Alex Bertolli, Alexey Bridgland, Miles Cherepanov, Congreve, I Alexander, Andrew Cowen-Rivers, Michael Cowie, Fabian B Figurnov, Hannah Fuchs, Rishub Gladman, Jain, A Yousuf, Caroline M R Khan, Kuba Low, Anna Perlin, Pascal Potapenko, Sukhdeep Savy, Adrian Singh, Stecula, 10.1038/s41586-024-07487-wMax Jaderberg, Demis Hassabis, and John M. Jumper. Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, 2024. 01 Jun 2024630</p>
<p>A Survey of Figurative Language and Its Computational Detection in Online Social Networks. Muhammad Abulaish, Ashraf Kamal, Mohammed J Zaki, 10.1145/3375547ACM Trans. Web. 143522020. Feb. 2020</p>
<p>Rajat Agarwal, Katharina Kann, arXiv:2010.02239[cs.CLAcrostic Poem Generation. 2020</p>
<p>Andrea Agostinelli, I Timo, Zalán Denk, Jesse Borsos, Mauro Engel, Antoine Verzetti, Qingqing Caillon, Aren Huang, Adam Jansen, Marco Roberts, Matthew Tagliasacchi, Neil Sharifi, Christian Zeghidour, Frank Havnø, ArXiv abs/2301.11325MusicLM: Generating Music From Text. 2023. 2023256274504</p>
<p>The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models. Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, Jay Pujara, ArXiv abs/2401.121172024. 2024267068363</p>
<p>STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, Mohit Iyyer, 10.18653/v1/2020.emnlp-main.525Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>Creating Creativity: 101 Definitions (what Webster Never Told You). A G Aleinikov, S Kackmeister, R Koenig, 2000Dow Creativity Center PressAlden B</p>
<p>Artificial intelligence in scientific writing: a friend or a foe?. Signe Altmäe, Alberto Sola-Leyva, Andres Salumets, 10.1016/j.rbmo.2023.04.009BioMedicine Online. 472023. 04 2023</p>
<p>The social psychology of creativity: A componential conceptualization. T M Amabile, Journal of Personality and Social Psychology. 451457195431983. 1983</p>
<p>Automated composition in retrospect: 1956-1986. Charles Ames, 1987. 1987Leonardo</p>
<p>A Survey on Approaches to Computational Humor Generation. Miriam Amin, Manuel Burghardt, Proceedings of the 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage. Stefania Degaetano, Anna Kazantseva, Nils Reiter, Stan Szpakowicz, the 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage2020Social Sciences, Humanities and Literature</p>
<p>Homogenization Effects of Large Language Models on Human Creative Ideation. Jash Hemant Barrett R Anderson, Max Shah, Kreminski, 10.1145/3635636.3656204Proceedings of the 16th Conference on Creativity and Cognition. the 16th Conference on Creativity and CognitionChicago, IL, USA; New York, NY, USAAssociation for Computing Machinery2024a</p>
<p>Homogenization Effects of Large Language Models on Human Creative Ideation. Jash Hemant Barrett R Anderson, Max Shah, Kreminski, Proceedings of the 16th Conference on Creativity &amp; Cognition. the 16th Conference on Creativity &amp; Cognition2024b. 2024267406608</p>
<p>Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test. Dang Anh, Limor Raviv, Lukas Galke, 10.18653/v1/2024.cmcl-1.15Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics. Tatsuki Kuribayashi, Giulia Rambelli, Ece Takmaz, Philipp Wicke, Yohei Oseki, the Workshop on Cognitive Modeling and Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>ColBERT: Using BERT sentence embedding in parallel neural networks for computational humor. Issa Annamoradnejad, Gohar Zoghi, Expert Syst. Appl. 2492541257742020. 2020</p>
<p>ViViT: A Video Vision Transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid, IEEE/CVF International Conference on Computer Vision (ICCV). 2021. 2021. 2021232417054</p>
<p>What it means to be creative. Rudolf Arnheim, British Journal of Aesthetics. 411914843642001. 2001</p>
<p>Transfer Learning for Humor Detection by Twin Masked Yellow Muppets. Aseem Arora, Gaël Dias, Adam Jatowt, Asif Ekbal, Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing. Heng Ji, Sujian Li, Yang Liu, Chua-Hui Chang, the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics20222Short Papers)</p>
<p>Matthijs Baas, Marieke Roskes, Daniel Sligte, Bernard Nijstad, Carsten De Dreu, 10.1111/spc3.12062Personality and Creativity: The Dual Pathway to Creativity Model and a Research Agenda. 2013. 20137</p>
<p>. Charles Babbage, 63432444</p>
<p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, ArXiv abs/2404.077382024. 2024</p>
<p>Domain Specificity and the Limits of Creativity Theory. John Baer, Journal of Creative Behavior. 46459830302012. 2012</p>
<p>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, ArXiv abs/2204.05862Jared Kaplan2022. 2022248118878Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann</p>
<p>Of bits and wows: A Bayesian theory of surprise with applications to attention. Pierre Baldi, Laurent Itti, Neural networks : the official journal of the International Neural Network Society. 2320705852010. 2010</p>
<p>LLMs Will Always Hallucinate, and We Need to Live With This. Sourav Banerjee, Ayushi Agarwal, Saloni Singla, arXiv:2409.057462024stat.ML</p>
<p>You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models. Alexander Baranov, Vladimir Kniazhevsky, Pavel Braslavski, 10.18653/v1/2023.emnlp-main.845Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Rigidity of Behavior. A Variational Approach to the Effect of Einstellung. Theodore Xenophon Barber, Abraham S Luchins, Edith Hirsch Luchins, The Quarterly Review of Biology. 35850225311960. 1960</p>
<p>The disposition toward originality. Frank Barron, Journal of abnormal psychology. 511955. 1955</p>
<p>Automating creativity assessment with SemDis: An open platform for computing semantic distance. Roger E Beaty, Dan Richard, Johnson , Behavior Research Methods. 532214032862020. 2020</p>
<p>Antoine Bellemare-Pepin, François Lespinasse, Philipp Thölke, Yann Harel, Kory Mathewson, Jay A Olson, Yoshua Bengio, Karim Jerbi, arXiv:2405.13012Divergent Creativity in Humans and Large Language Models. 2024. 2024arXiv preprint</p>
<p>ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models. Jonas Belouadi, Steffen Eger, ArXiv abs/2212.104742022. 2022254877406</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021. 2021262580630</p>
<p>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. Emily M Bender, Alexander Koller, Annual Meeting of the Association for Computational Linguistics. 2020211029226</p>
<p>Mechanistic Interpretability for AI Safety -A Review. Leonard Bereska, Efstratios Gavves, ArXiv abs/2404.140822024. 2024</p>
<p>What, If Anything, Is Linguistic Creativity?. Alexander Bergs, 10.2478/gth-2019-0017Gestalt Theory. 412019. 2019</p>
<p>Deep Learning of Audio and Language Features for Humor Prediction. Dario Bertero, Pascale Fung, ; , Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). Asuncion Moreno, Jan Odijk, Stelios Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Portorož, Slovenia2016European Language Resources Association (ELRA)</p>
<p>Improving Image Generation with Better Captions. James Betker, Gabriel Goh, Li Jing, Jianfeng Timbrooks, Linjie Wang, Long Li, Juntang Ouyang, Joyce Zhuang, Yufei Lee, Wesam Guo, Prafulla Manassra, Casey Dhariwal, Yunxin Chu, Aditya Jiao, Ramesh, 2020264403242</p>
<p>GenJam: A genetic algorithm for generating jazz solos. John Biles, In ICMC. 941994</p>
<p>Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, Gabriel Stanovsky, arXiv:2212.04542[cs.CVVASR: Visual Analogies of Situation Recognition. 2022</p>
<p>Large dataset and language model fun-tuning for humor recognition. Vladislav Blinov, Valeria Bolotova-Baranova, Pavel Braslavski, Proceedings of the 57th annual meeting of the association for computational linguistics. the 57th annual meeting of the association for computational linguistics2019</p>
<p>M A Boden, The Creative Mind: Myths and Mechanisms. 2004</p>
<p>Margaret A Boden, The creative mind : myths &amp; mechanisms. 1991</p>
<p>A Daniil, Robert Boiko, Gabe Macknight, Gomes, arXiv:2304.05332Emergent autonomous scientific research capabilities of large language models. 2023physics.chem-ph</p>
<p>Pattern recognition. M M Bongard, 1970267827262</p>
<p>Ali Borji, arXiv:2302.03494[cs.CLA Categorical Archive of ChatGPT Failures. 2023a</p>
<p>Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. Ali Borji, ArXiv abs/2304.064702023b. 2023257826680</p>
<p>Defining Creativity: A View from the Arts. Anthony Brandt, 10.1080/10400419.2020.1855905Creativity Research Journal. 3322021. 2021</p>
<p>B Brittain, Artists take new shot at Stability, Midjourney in updated copyright lawsuit. 2023</p>
<p>Active Divergence with Generative Deep Learning -A Survey and Taxonomy. Terence Broad, Sebastian Berns, Simon Colton, Mick Grierson, ArXiv abs/2107.055992021. 2021235794880</p>
<p>An experiment in musical composition. A L Frederick P Brooks, Peter G Hopkins, William V Neumann, Wright, IRE Transactions on Electronic Computers. 31957. 1957</p>
<p>Tim Brooks, Aleksander Holynski, Alexei A Efros, InstructPix2Pix: Learning to Follow Image Editing Instructions. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2022253581213</p>
<p>Video generation models as world simulators. Tim Brooks, Bill Peebles, Connor Holmes, Will Depue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, Aditya Ramesh, 2024. 2024</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, ArXiv abs/2005.14165Ilya Sutskever, and Dario Amodei. 2020a. Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020218971783</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2005.14165[cs.CL]Ilya Sutskever, and Dario Amodei. 2020b. Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv:2303.12712[cs.CL]Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>Theorema: Towards computer-aided mathematical theory exploration. Bruno Buchberger, Adrian Crǎciun, Tudor Jebelean, Laura Kovács, Temur Kutsia, Koji Nakagawa, Florina Piroi, Nikolaj Popov, Judit Robu, Markus Rosenkranz, Wolfgang Windsteiger, 10.1016/j.jal.2005.10.006Journal of Applied Logic. 442006. 2006</p>
<p>Learning to Surprise: A Composer-Audience Architecture. C Razvan, Oseremen O Bunescu, Uduehi, International Conference on Innovative Computing and Cloud Computing. 2019218606180</p>
<p>Distribution-Based Measures of Surprise for Creative Language: Experiments with Humor and Metaphor. C Razvan, Oseremen O Bunescu, ; Uduehi, ) Flp, Debanjan Ghosh, Beata Beigman Klebanov, Smaranda Muresan, 10.18653/v1/2022.flp-1.10Proceedings of the 3rd Workshop on Figurative Language Processing. Anna Feldman, Soujanya Poria, Tuhin Chakrabarty, the 3rd Workshop on Figurative Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Some experiments in humour recognition using the italian wikiquote collection. Davide Buscaldi, Paolo Rosso, International workshop on fuzzy logic and applications. Springer2007</p>
<p>SemEval-2010 Task 9: The Interpretation of Noun Compounds Using Paraphrasing Verbs and Prepositions. Cristina Butnariu, Nam Su, Preslav Kim, Nakov, Ó Diarmuid, Stan Séaghdha, Tony Szpakowicz, Veale, Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions. Eneko Agirre, Lluís Màrquez, Richard Wicentowski, the Workshop on Semantic Evaluations: Recent Achievements and Future DirectionsBoulder, ColoradoAssociation for Computational Linguistics2009</p>
<p>Quantifying Memorization Across Neural Language Models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, Chiyuan Zhang, ArXiv abs/2202.076462022. 2022246863735</p>
<p>The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Xiaodong Song, USENIX Security Symposium. 2018170076423</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, arXiv:2012.07805[cs.CRUlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large Language Models. </p>
<p>How AI Outperforms Humans at Creative Idea Generation. Available at SSRN. Noah Castelo, Zsolt Katona, Peiyao Li, Miklos Sarvary, 2024. 20244751779</p>
<p>Overview of the HAHA Task: Humor Analysis Based on Human Annotation at IberEval. Santiago Castro, Luis Chiruzzo, Aiala Rosá, IberEval@SEPLN. 2018. 201851940157</p>
<p>A crowd-annotated spanish corpus for humor analysis. Santiago Castro, Luis Chiruzzo, Aiala Rosá, Diego Garat, Guillermo Moncecchi, arXiv:1710.004772017. 2017arXiv preprint</p>
<p>It's not Rocket Science: Interpreting Figurative Language in Narratives. Tuhin Chakrabarty, Yejin Choi, Vered Shwartz, Transactions of the Association for Computational Linguistics. 102373718452021a. 2021</p>
<p>Rˆ3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge. Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng, 10.18653/v1/2020.acl-main.711Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020a</p>
<p>Art or Artifice? Large Language Models and the False Promise of Creativity. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-Sheng Wu, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2023a. 2023</p>
<p>Art or Artifice? Large Language Models and the False Promise of Creativity. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-Sheng Wu, arXiv:2309.14556[cs.CL]2023b</p>
<p>Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation. Tuhin Chakrabarty, Smaranda Muresan, Nanyun Peng, 10.18653/v1/2020.emnlp-main.524Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020b</p>
<p>Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, Smaranda Muresan, ArXiv abs/2309.125702023c. 2023262217523</p>
<p>Help me write a Poem -Instruction Tuning as a Vehicle for Collaborative Poetry Writing. Tuhin Chakrabarty, Vishakh Padmakumar, Hengxing He, ArXiv abs/2210.136692022a. 2022253107865</p>
<p>Help me write a Poem -Instruction Tuning as a Vehicle for Collaborative Poetry Writing. Tuhin Chakrabarty, Vishakh Padmakumar, He He, 10.18653/v1/2022.emnlp-main.460Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>Creative Natural Language Generation. Tuhin Chakrabarty, Vishakh Padmakumar, He He, Nanyun Peng, 10.18653/v1/2023.emnlp-tutorial.6Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Tutorial Abstracts, Qi Zhang, Hassan Sajjad, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023d</p>
<p>FLUTE: Figurative Language Understanding through Textual Explanations. Tuhin Chakrabarty, Arkadiy Saakyan, Debanjan Ghosh, Smaranda Muresan, Conference on Empirical Methods in Natural Language Processing. 2022c252780978</p>
<p>I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan, Annual Meeting of the Association for Computational Linguistics. 2023e258865878</p>
<p>I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan, 10.18653/v1/2023.findings-acl.465Findings of the Association for Computational Linguistics: ACL 2023. Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers; Toronto, CanadaAssociation for Computational Linguistics2023f</p>
<p>MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding. Tuhin Chakrabarty, Xurui Zhang, Smaranda Muresan, Nanyun Peng, 10.18653/v1/2021.naacl-main.336Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021b</p>
<p>Kent K Chang, Mackenzie Cramer, Sandeep Soni, David Bamman, arXiv:2305.00118[cs.CLSpeak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. 2023</p>
<p>Automated discovery of fundamental variables hidden in experimental data. Boyuan Chen, Kuang Huang, Sunand Raghupathi, Ishaan Preetam Chandratreya, Qi Du, Hod Lipson, Nature Computational Science. 22510871192022a. 2022</p>
<p>Probing the Creativity of Large Language Models: Can models produce divergent semantic association?. Honghua Chen, Nai Ding, Conference on Empirical Methods in Natural Language Processing. 2023264172665</p>
<p>StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning. Hong Chen, Minh Duc, Hiroya Vo, Yusuke Takamura, Hideki Miyao, Nakayama, ArXiv abs/2210.084592022b. 2022252918409</p>
<p>Humor Recognition Using Deep Learning. Peng-Yu Chen, V Soo, North American Chapter. the Association for Computational Linguistics201844158782</p>
<p>Automated discovery and proof of congruence theorems for partial sums of combinatorial sequences. Qing-Hu William Yc Chen, Doron Hou, Zeilberger, Journal of Difference Equations and Applications. 222016. 2016</p>
<p>Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation. Cyril Chhun, Fabian M Suchanek, Chloé Clavel, Transactions of the Association for Computational Linguistics. 122699821252024. 2024</p>
<p>On the Measure of Intelligence. François Chollet, ArXiv abs/1911.015472019. 2019207870692</p>
<p>Noam Chomsky, Aspects of the Theory of Syntax. The MIT Press196550 ed.</p>
<p>TaleBrush: Sketching Stories with Generative Pretrained Language Models. John Joon, Young Chung, Wooseok Kim, Min Kang, Hwaran Yoo, Eytan Lee, Minsuk Adar, Chang, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022. 2022247625751</p>
<p>From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. Albert Coil, Vered Shwartz, 10.18653/v1/2023.findings-acl.169Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Creativity Versus the Perception of Creativity in Computational Systems. Simon Colton, AAAI Spring Symposium: Creative Intelligent Systems. 20082876679</p>
<p>Full-FACE Poetry Generation. Simon Colton, Jacob Goodwin, Tony Veale, International Conference on Innovative Computing and Cloud Computing. 20122218980</p>
<p>Computational Creativity: The Final Frontier. Simon Colton, Geraint A Wiggins, European Conference on Artificial Intelligence. 20125880786</p>
<p>Testing Relational Understanding in Text-Guided Image Generation. Colin Conwell, Tomer David Ullman, ArXiv abs/2208.000052022. 2022251224307</p>
<p>Generating Code For Expressing Simple Preferences: Moving On From Hardcoding And Randomness. Michael Cook, Simon Colton, ICCC. 2015</p>
<p>Experiments in musical intelligence. David Cope, 1996WI12</p>
<p>. Jade Copet, Simple and Controllable Music GenerationFelix Kreuk, Simple and Controllable Music GenerationItai Gat, Simple and Controllable Music GenerationTal Remez, Simple and Controllable Music GenerationDavid Kant, Simple and Controllable Music GenerationGabriel Synnaeve, Simple and Controllable Music GenerationYossi Adi, Simple and Controllable Music GenerationAlexandre D' Efossez, Simple and Controllable Music GenerationArXiv abs/2306.052842023. 2023259108357</p>
<p>Efficient creativity: Constraint-guided conceptual combination. J Fintan, Mark T Costello, Keane, Cognitive Science. 242000. 2000</p>
<p>Discovering Symbolic Models from Deep Learning with Inductive Biases. M Cranmer, Alvaro Sanchez-Gonzalez, Peter W Battaglia, Rui Xu, Kyle Cranmer, David N Spergel, Shirley Ho, ArXiv abs/2006.112872020. 2020219966125</p>
<p>Is artificial intelligence more creative than humans?: ChatGPT and the divergent association task. David Cropley, Learning Letters. 22023. 2023</p>
<p>Scientific and Creative Analogies in Pretrained Language Models. Tamara Czinczoll, Helen Yannakoudakis, Pushkar Mishra, Ekaterina Shutova, 10.18653/v1/2022.findings-emnlp.153Findings of the Association for Computational Linguistics: EMNLP 2022. Zornitsa Goldberg, Yue Kozareva, Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Neuramanteau: A Neural Network Ensemble Model for Lexical Blends. Kollol Das, Shaona Ghosh, Proceedings of the Eighth International Joint Conference on Natural Language Processing. Long Papers. Greg Kondrak, Taro Watanabe, the Eighth International Joint Conference on Natural Language ProcessingTaipei, Taiwan20171Asian Federation of Natural Language Processing</p>
<p>Plug and Play Language Models: A Simple Approach to Controlled Text Generation. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu, ArXiv abs/1912.021642019. 2019208617790</p>
<p>Guy Davidson, Todd M Gureckis, Brenden M Lake, 10.31234/osf.io/byzs5Creativity, Compositionality, and Common Sense in Human Goal Generation. 2022</p>
<p>Authorship in the Age of Machine learning and Artificial Intelligence. Legal Perspectives in Information Systems eJournal. Jean-Marc Deltorn, Franck Macrez, 2018. 201869806575</p>
<p>Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, arXiv:2404.18081ComposerX: Multi-Agent Symbolic Music Composition with LLMs. 2024. 2024arXiv preprint</p>
<p>Ex Machina: Copyright Protection for Computer-Generated Works. C Robert, Denicola, Innovation Law &amp; Policy eJournal. 884869492016. 2016</p>
<p>Prajit Dhar, Lonneke Van Der Plas ; Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever, arXiv:1906.03634[cs.CL]ArXiv abs/2005.00341Learning to Predict Novel Noun-Noun Compounds. 2019. 2020. 2020218470180Jukebox: A Generative Model for Music</p>
<p>LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training. Chris Donahue, Huanru Henry Mao, Yiting Li, G Cottrell, Julian Mcauley, ArXiv abs/1907.048682019. 2019195886341</p>
<p>Learning a Deep Convolutional Network for Image Super-Resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, European Conference on Computer Vision. 201418874645</p>
<p>MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment. Hao-Wen, Wen-Yi Dong, Li-Chia Hsiao, Yi-Hsuan Yang, Yang, AAAI Conference on Artificial Intelligence. 201719098155</p>
<p>Intellectual Property on Works of Art Made by Artificial Intelligence. Cláudio Lisboa, Dos Santos, Ângela Rocha Machado, International Journal of Advanced Engineering Research and Science. 2345411782020. 2020</p>
<p>StoryWars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation. Yulun Du, Lydia Chilton, 10.18653/v1/2023.acl-long.171Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Naoaki Boyd-Graber, Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsAnna Rogers, Jordan; Toronto, CanadaAssociation for Computational Linguistics20231</p>
<p>V André, Xuandong Duarte, Arlindo L Zhao, Lei Oliveira, Li, arXiv:2402.09910[cs.CLDE-COP: Detecting Copyrighted Content in Language Models Training Data. 2024</p>
<p>A Learned Representation For Artistic Style. Jonathon Vincent Dumoulin, Manjunath Shlens, Kudlur, ArXiv abs/1610.076292016. 20165687613</p>
<p>Kevin Dunbar, Eve Forster, Creativity Evaluation through Latent Semantic Analysis. 200918209155</p>
<p>On Problem-solving. K Duncker, L S Lees, American Psychological Ass. 1948</p>
<p>Discovering Dynamics. Saso Dzeroski, Ljupco Todorovski, International Conference on Machine Learning. 199317202248</p>
<p>Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, ArXiv abs/2305.18654Faith and Fate: Limits of Transformers on Compositionality. 2023. 2023258967391</p>
<p>Finding temporal structure in music: Blues improvisation with LSTM recurrent networks. Douglas Eck, Juergen Schmidhuber, Proceedings of the 12th IEEE workshop on neural networks for signal processing. the 12th IEEE workshop on neural networks for signal processingIEEE2002</p>
<p>Poetry Will Not Optimize; or, What Is Literature to AI?. Michele Elam, American literature. 952023. 2023</p>
<p>Measuring and Improving Consistency in Pretrained Language Models. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard H Hovy, Hinrich Schütze, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms. A Elgammal, Bingchen Liu, Mohamed Elhoseiny, Marian Mazzone, International Conference on Innovative Computing and Cloud Computing. 201724986117</p>
<p>Quantifying Creativity in Art Networks. A Elgammal, Babak Saleh, ArXiv abs/1506.007112015. 201514607648</p>
<p>Creative Data Generation: A Review Focusing on Text and Poetry. Mohamad Elzohbi, Richard Zhao, ArXiv abs/2305.084932023. 2023258686736</p>
<p>On Conjectures of Graffiti. 10.1016/S0167-5060(08)70776-3S0167-5060(08)70776-3Annals of Discrete Mathematics. J Akiyama, Y Egawa, H Enomoto, Elsevier198838Graph Theory and Applications</p>
<p>Brian Falkenhainer, Kenneth D Forbus, Dedre Gentner, The Structure-Mapping Engine: Algorithm and Examples. 1989. 198941</p>
<p>Hierarchical Neural Story Generation. Angela Fan, Mike Lewis, Yann Dauphin, Annual Meeting of the Association for Computational Linguistics. 201844134226</p>
<p>Analysis and synthesis of Palestrina-style counterpoint using Markov chains. Mary Farbood, Bernd Schöner, Data &amp; Policy. ICMC. Giorgio Franceschelli and Mirco Musolesi42347777512001. 2021a. 2021Copyright in generative deep learning</p>
<p>Creativity and Machine Learning: A Survey. Giorgio Franceschelli, Mirco Musolesi, ArXiv abs/2104.027262021b. 2021233168627</p>
<p>DeepCreativity: Measuring Creativity with Deep Learning Techniques. Giorgio Franceschelli, Mirco Musolesi, Intelligenza Artificiale. 162460157192022. 2022</p>
<p>Giorgio Franceschelli, Mirco Musolesi, arXiv:2304.00008[cs.AIOn the Creativity of Large Language Models. 2023</p>
<p>Creative Beam Search: LLM-as-a-Judge For Improving Response Generation. Giorgio Franceschelli, Mirco Musolesi, ArXiv abs/2405.000992024. 2024</p>
<p>Uncertainty Reduction as a Measure of Cognitive Processing Effort. S Frank, CMCL@ACL. 20103097133</p>
<p>Regent-Dependent Creativity: A Domain Independent Metric for the Assessment of Creative Artifacts. Celso França, Luís Fabrício, Wanderley Góes, Alvaro Amorim, Rodrigo C O Rocha, Alysson Ribeiro, Da Silva, International Conference on Innovative Computing and Cloud Computing. 20167817742</p>
<p>Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman, ArXiv abs/2203.131312022. 2022247628171</p>
<p>Deep neural networks to recover unknown physical parameters from oscillating time series. Antoine Garcon, Julian Vexler, Dmitry Budker, Stefan Kramer, PLoS ONE. 172315729592021. 2021</p>
<p>Leon A Gatys, Alexander S Ecker, Matthias Bethge, ArXiv abs/1508.06576A Neural Algorithm of Artistic Style. 2015. 201513914930</p>
<p>The Philosophy of Creativity. Berys Gaut, 10.1111/j.1747-9991.2010.00351.xarXiv:2312.11805[cs.CL]Gemini: A Family of Highly Capable Multimodal Models. 2010. 20105</p>
<p>Large Language Models Are Not Strong Abstract Reasoners. Gaël Gendron, Qiming Bao, M Witbrock, Gillian Dobbie, 2023258988045</p>
<p>InstructDiffusion: A Generalist Modeling Interface for Vision Tasks. Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dongdong Chen, Baining Guo, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023. 2024. 2023261582721</p>
<p>Structure-Mapping: A Theoretical Framework for Analogy. Dedre Gentner, Cogn. Sci. 753714921983. 1983</p>
<p>Exploring Quantitative Evaluations of the Creativity of Automatic Poets. Pablo Gervás, 10.1007/978-3-319-43610-4_132019</p>
<p>SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, 2024272524655</p>
<p>Generating Topical Poetry. Marjan Ghazvininejad, Xing Shi, Yejin Choi, Kevin Knight, 10.18653/v1/D16-1126Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Jian Su, Kevin Duh, Xavier Carreras, the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>People Not Machines: Authorship and What It Means in the Berne Convention. IIC -International Review of Intellectual Property and Competition Law. Jane C Ginsburg, 2018. 201849158580780</p>
<p>Ideas are Dimes a Dozen: Large Language Models for Idea Generation in Innovation. Karan Girotra, Lennart Meincke, Christian Terwiesch, Karl T Ulrich, SSRN Electronic Journal. 2604678862023. 2023</p>
<p>Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't. Anna Gladkova, Aleksandr Drozd, Satoshi Matsuoka, 10.18653/v1/N16-2002Proceedings of the NAACL Student Research Workshop. Jacob Andreas, Eunsol Choi, Angeliki Lazaridou, the NAACL Student Research WorkshopSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>. Fabrício Góes, Piotr Sawicki, Marek Grze´s, Daniel Brown, Marco Volpe, 2023aIs GPT-4 Good Enough to Evaluate Jokes</p>
<p>Fabrício Góes, Marco Volpe, Piotr Sawicki, Marek Grze´s, Jacob Watson, Pushing GPT's Creativity to Its Limits: Alternative Uses and Torrance Tests. 2023b</p>
<p>Content Planning for Neural Story Generation with Aristotelian Rescoring. Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, Nanyun Peng, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing. Carlos Gómez, -Rodríguez , Paul Williams, 10.18653/v1/2023.findings-emnlp.966Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Speculations Concerning the First Ultraintelligent Machine. I J Good, Adv. Comput. 6178868721965. 1965</p>
<p>Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, arXiv:1406.2661stat.ML]Generative Adversarial Networks. 2014a</p>
<p>Generative adversarial networks. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio, Commun. ACM. 6310336822014b. 2014</p>
<p>M M Grynbaum, R Mac, The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work. 2023</p>
<p>Do Androids Dream of Electric Copyright? Comparative Analysis of Originality in Artificial Intelligence Generated Works. Econometrics: Computer Programs &amp; Software eJournal. Andres Guadamuz, 2017. 201721670175</p>
<p>UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation. Jian Guan, Minlie Huang, ArXiv abs/2009.076022020. 2020221739231</p>
<p>Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models. J P Guilford, ; -Zjaaaamaaj Gabriel Lima Guimaraes, Benjamín Sánchez-Lengeling, Pedro , Luis Cunha Farias, Alán Aspuru-Guzik, ArXiv abs/1705.108431967. 2017. 2017McGraw-Hill35911567The Nature of Human Intelligence</p>
<p>A Bayesian machine scientist to aid in the solution of challenging scientific problems. Roger Guimerà, Ignasi Reichardt, Antoni Aguilar-Mogas, Francesco A Massucci, Manuel Miranda, Jordi Pallarès, Marta Sales-Pardo, 10.1126/sciadv.aav6971Science Advances. 669712020. 2020</p>
<p>George Gunkle, Daniel E Berlyne, Aesthetics and Psychobiology. 1975144314694</p>
<p>Photorealistic Video Generation with Diffusion Models. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, José Lezama, ArXiv abs/2312.066622023. 2023266163109</p>
<p>The Originality of Machines: AI Takes the Torrance Test. Erik E Guzik, Christian Byrge, Christian Gilde, Journal of Creativity. 2610871852023. 2023</p>
<p>Artificial muses: Generative artificial intelligence chatbots have risen to human-level creativity. Jennifer Haase, Paul Hp Hanel, Journal of Creativity. 331000662023. 2023</p>
<p>Automated discovery of scientific concepts: Replicating three recent discoveries in mechanics. Yaron Hakuk, Yoram Reich, 10.1016/j.aei.2020.101080Advanced Engineering Informatics. 441010802020. 2020</p>
<p>Uncertainty About the Rest of the Sentence. John Hale, Cognitive science. 30159226332006. 2006</p>
<p>Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs. Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein, ArXiv abs/2406.102092024. 2024270521774</p>
<p>Automated scoring of originality using semantic representations. J , Isaiah Harbison, Henk J Haarmann, Cognitive Science. 362014. 2014</p>
<p>UR-FUNNY: A Multimodal Language Dataset for Understanding Humor. Md Kamrul Hasan, Wasifur Rahman, Amirali Bagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, Mohammed (ehsan) Hoque, 10.18653/v1/D19-1211Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Pun Generation with Surprise. Demis Hassabis, 10.18653/v1/N19-1172Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics2018. 20191Creativity and AI</p>
<p>HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation. Qianyu He, Yikai Zhang, Jiaqing Liang, Yuncheng Huang, Yanghua Xiao, Yunwen Chen, 10.18653/v1/2023.acl-long.702Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Semantic Distance: An Automated Measure of Creativity That Is Novel and Appropriate. David Heinen, Dan Johnson, 10.1037/aca0000125Psychology of Aesthetics, Creativity, and the Arts. 122017. 04 2017</p>
<p>SemEval-2013 Task 4: Free Paraphrases of Noun Compounds. Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Ó Diarmuid, Stan Séaghdha, Tony Szpakowicz, Veale, Proceedings of the Seventh International Workshop on Semantic Evaluation. Suresh Manandhar, Deniz Yuret, the Seventh International Workshop on Semantic EvaluationAtlanta, Georgia, USAAssociation for Computational Linguistics2013. SemEval 20132Second Joint Conference on Lexical and Computational Semantics (*SEM)</p>
<p>Prompt-to-Prompt Image Editing with Cross Attention Control. Amir Hertz, Ron Mokady, Jay M Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or, ArXiv abs/2208.016262022. 2022251252882</p>
<p>Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest. Jack Hessel, Ana Marasovic, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi, 10.18653/v1/2023.acl-long.41Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Jack Hessel, Ana Marasović, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi, arXiv:2209.06293Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest. 2023b</p>
<p>Musical composition with a high-speed digital computer. Lejaren Hiller, Leonard Isaacson, 1958. 1958</p>
<p>Imagen Video: High Definition Video Generation with Diffusion Models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Tim Fleet, Salimans, ArXiv abs/2210.023032022a. 2022252715883</p>
<p>Jonathan Ho, Ajay Jain, Pieter Abbeel, arXiv:2006.11239Denoising Diffusion Probabilistic Models. 2020</p>
<p>Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, ArXiv abs/2204.03458Video Diffusion Models. 2022b. 2022248006185</p>
<p>Analogy as the Core of Cognition. Douglas Hofstadter, 2001. 2001MIT Press</p>
<p>The Curious Case of Neural Text Degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ArXiv abs/1904.097512019. 2019</p>
<p>KANDINSKY Patterns as IQ-Test for Machine Learning. Andreas Holzinger, D Michael, Heimo Kickmeier-Rust, Müller, International Cross-Domain Conference on Machine Learning and Knowledge Extraction. 2019</p>
<p>Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences. Xudong Hong, Asad Sayeed, Khushboo Mehra, Vera Demberg, Bernt Schiele, 10.1162/tacl_a_00553Transactions of the Association for Computational Linguistics. 112023. 2023</p>
<p>Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models. Zachary Horvitz, Jingru Chen, Rahul Aditya, Harshvardhan Srivastava, Robert West, Zhou Yu, Kathleen Mckeown, ArXiv abs/2403.007942024. 2024268230695</p>
<p>President Vows to Cut <Taxes> Hair": Dataset and Analysis of Creative Text Editing for Humorous Headlines. Nabil Hossain, John Krumm, Michael Gamon, 10.18653/v1/N19-1012Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>SemEval-2020 Task 7: Assessing Humor in Edited News Headlines. Nabil Hossain, John Krumm, Michael Gamon, Henry Kautz, 10.18653/v1/2020.semeval-1.98Proceedings of the Fourteenth Workshop on Semantic Evaluation. Aurelie Herbelot, Xiaodan Zhu, Alexis Palmer, Nathan Schneider, Jonathan May, Ekaterina Shutova, the Fourteenth Workshop on Semantic EvaluationBarcelona2020aInternational Committee for Computational Linguistics</p>
<p>Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines. Nabil Hossain, John Krumm, Tanvir Sajed, Henry Kautz, 10.18653/v1/2020.acl-demos.28Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Asli Celikyilmaz and. Tsung-Hsien Wen, the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Asli Celikyilmaz andOnline2020b</p>
<p>In-Context Analogical Reasoning with Pre-Trained Language Models. Xiaoyang Hu, Shane Storks, Richard L Lewis, Joyce Yue Chai, Annual Meeting of the Association for Computational Linguistics. 2023258959097</p>
<p>Music Transformer: Generating Music with Long-Term Structure. Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam M Shazeer, Ian Simon, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, Douglas Eck, International Conference on Learning Representations. 2018b54477714</p>
<p>T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, Xihui Liu, ArXiv abs/2307.063502023. 2023259847295</p>
<p>CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Changsheng Xu, ArXiv abs/2401.140662024a. 2024</p>
<p>Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion. Nisha Huang, Fan Tang, Weiming Dong, Changsheng Xu, Proceedings of the 30th ACM International Conference on Multimedia. the 30th ACM International Conference on Multimedia2022. 2022252544936</p>
<p>Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.03302[cs.LGMLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. 2024c</p>
<p>LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Haitao Zheng, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024Torino, Italia2024bELRA and ICCL</p>
<p>Multimodal Unsupervised Image-to-Image Translation. Xun Huang, Ming-Yu Liu, Serge J Belongie, Jan Kautz, European Conference on Computer Vision. 2018a4883312</p>
<p>Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions. Yu-Siang Huang, Yi-Hsuan Yang, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on Multimedia2020. 2020220919638</p>
<p>The current state of artificial intelligence generative language models is more creative than humans on divergent thinking tasks. Kim N Kent F Hubert, Darya L Awa, Zabelina, Scientific Reports. 142676161812024a. 2024</p>
<p>. Thomas Hubert, Rishi Mehta, Laurent Sartran, 2024b</p>
<p>Ryan Cotterell, and Zhijing Jin. 2022. A taxonomy and review of generalization research in NLP. Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella J Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Nature Machine Intelligence. 52527351242022</p>
<p>Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, Roy Kishony, arXiv:2404.17605Autonomous LLM-driven research from data to human-verifiable research papers. 2024</p>
<p>Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, Nicholas Carlini, ArXiv abs/2210.175462022. 2022253237404</p>
<p>Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, arXiv:2410.12656[cs.CLLonneke van der Plas, and Duygu Ataman. 2024a. Evaluating Morphological Compositional Generalization in Large Language Models. </p>
<p>CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. Mete Ismayilzada, Debjit Paul, Syrielle Montariol, Mor Geva, Antoine Bosselut, ArXiv abs/2310.152392023. 2023264438931</p>
<p>Evaluating Creative Short Story Generation in Humans and Large Language Models. Mete Ismayilzada, Claire Stevenson, Lonneke Van Der Plas, arXiv:2411.02316[cs.CL2024b</p>
<p>Image-to-Image Translation with Conditional Adversarial Networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2017. 20166200260</p>
<p>Discovering Physical Concepts with Neural Networks. Raban Iten, Tony Metger, Henrik Wilming, Lídia Del Rio, Renato Renner, 10.1103/PhysRevLett.124.010508Phys. Rev. Lett. 124105082020. Jan 2020</p>
<p>FAME: Flexible, Scalable Analogy Mappings Engine. Shahar Jacob, Chen Shani, Dafna Shahaf, Conference on Empirical Methods in Natural Language Processing. 2023259267034</p>
<p>The Gutenberg English Poetry Corpus: Exemplary Quantitative Narrative Analyses. M Arthur, Jacobs, 10.3389/fdigh.2018.00005Frontiers in Digital Humanities. 52018. 2018</p>
<p>Is AI fun? HumorDB: a curated dataset and benchmark to investigate graphical humor. Veedant Jain, Felipe Dos, Santos Alves Feitosa, Gabriel Kreiman, 2024270619380</p>
<p>ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. Sophie Jentzsch, Kristian Kersting, 10.18653/v1/2023.wassa-1.29Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis. Jeremy Barnes, Orphée De Clercq, Roman Klinger, the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media AnalysisToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Shulei Ji, Jing Luo, Xinyu Yang, ArXiv abs/2011.06801A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions. 2020. 2020226956064</p>
<p>A Survey on Deep Learning for Symbolic Music Generation: Representations, Algorithms, Evaluations, and Challenges. Shulei Ji, Xinyu Yang, Jing Luo, 10.1145/3597493ACM Comput. Surv. 567392023. aug 2023</p>
<p>BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Conference on Empirical Methods in Natural Language Processing. 2023263830212</p>
<p>StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. Cheng Jiayang, Lin Qiu, Tsz Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang, 10.18653/v1/2023.emnlp-main.706Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Divergent semantic integration (DSI): Extracting creativity from narratives with distributional semantic modeling. Dan Richard Johnson, J Kaufman, Brendan S Baker, John D Patterson, Baptiste Barbot, Adam E Green, Janet G Van Hell, Evan S Kennedy, Grace F Sullivan, Christa L Taylor, Thomas Ward, Roger E Beaty, Behavior Research Methods. 552529693362022. 2022</p>
<p>Perceptual Losses for Real-Time Style Transfer and Super-Resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, ArXiv abs/1603.081552016. 2016980236</p>
<p>A Standardised Procedure for Evaluating Creative Systems: Computational Creativity Evaluation Based on What it is to be Creative. Anna Jordanous, Cognitive Computation. 43111892012. 2012</p>
<p>The longer term value of creativity judgements in computational creativity. Anna Jordanous, 2016637189</p>
<p>Measuring cultural value using social network analysis: a case study on valuing electronic musicians. Anna Jordanous, Daniel Allington, Byron Dueck, 2015. 2015</p>
<p>Pushmeet Kohli, and Demis Hassabis. 2021. Highly accurate protein structure prediction with AlphaFold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, A A Simon, Andrew J Kohl, Andrew Ballard, Bernardino Cowie, Stanislav Romera-Paredes, Rishub Nikolov, Jonas Jain, Trevor Adler, Stig Back, David Petersen, Ellen Reiman, Michal Clancy, Martin Zielinski, Michalina Steinegger, Tamas Pacholska, Sebastian Berghammer, David Bodenstein, Oriol Silver, Andrew W Vinyals, Koray Senior, Kavukcuoglu, 10.1038/s41586-021-03819-2Nature. 59601 Aug 2021</p>
<p>Deduplicating Training Data Mitigates Privacy Risks in Language Models. Nikhil Kandpal, Eric Wallace, Colin Raffel, ArXiv abs/2202.065392022. 2022246823128</p>
<p>Towards Machines for Measuring Creativity: The Use of Computational Tools in Storytelling Activities. Pythagoras Karampiperis, Antonis Koukourikos, Evangelia Koliopoulou, IEEE 14th International Conference on Advanced Learning Technologies. 2014. 2014. 2014</p>
<p>A Style-Based Generator Architecture for Generative Adversarial Networks. Tero Karras, Samuli Laine, Timo Aila, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2018. 2019. 201854482423</p>
<p>Analyzing and Improving the Image Quality of StyleGAN. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019. 2020. 2019209202273</p>
<p>Beyond Big and Little: The Four C Model of Creativity. James C Kaufman, Ronald A Beghetto, 10.1037/a0013688Review of General Psychology. 132009. 2009</p>
<p>The Laughing Machine: Predicting Humor in Video. Yuta Kayatani, Zekun Yang, Mayu Otani, Noa Garcia, Chenhui Chu, Yuta Nakashima, Haruo Takemura, 10.1109/WACV48630.2021.002122021. 2072-2081</p>
<p>Can We Trust Creativity Tests? A Review of the Torrance Tests of Creative Thinking (TTCT). Kyung H Kim, Creativity Research Journal. 18176368882006. 2006</p>
<p>The Automation of Science. Ross D King, Jem Rowland, Stephen G Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena Markham, Pinar Pir, Larisa N Soldatova, Andrew Sparkes, Kenneth E Whelan, Amanda Clare, 10.1126/science.1165620Science. 3242009. 2009</p>
<p>Auto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, CoRR abs/1312.61142013. 2013216078090</p>
<p>ChatGPT: Jack of all trades, master of none. Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Information Fusion. 991018612023. 2023</p>
<p>Best humans still outperform artificial intelligence in a creative divergent thinking task. Mika Koivisto, Simone Grassini, Scientific Reports. 132023. 2023</p>
<p>VideoPoet: A Large Language Model for Zero-Shot Video Generation. D Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David C Minnen, David A Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming Yang, Xuan Yang, Bryan Seybold, Lu Jiang, ArXiv abs/2312.141252023. 2023</p>
<p>Genetic programming as a means for programming computers by natural selection. John R Koza, Statistics and Computing. 487501491994. 1994</p>
<p>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems. Stefan Kramer, Mattia Cerrato, Saso Dzeroski, Ross D King, ArXiv abs/2305.022512023. 2023258461620</p>
<p>Creativity Naturalized. Maria E Kronfeldner, 10.1111/j.1467-9213.2009.637.xThe Philosophical Quarterly. 592009. 07 2009</p>
<p>Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction. Sreejan Kumar, Raja Marjieh, Byron Zhang, Declan Campbell, Michael Y Hu, Umang Bhatt, Brenden Lake, Thomas L Griffiths, ArXiv abs/2402.036182024. 2024267499989</p>
<p>Understanding and Quantifying Creativity in Lexical Composition. Polina Kuznetsova, Jianfu Chen, Yejin Choi, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, Steven Bethard, the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational Linguistics2013</p>
<p>Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test. Philippe Laban, Luke Dai, Lucas Bandarkar, ArXiv abs/2107.034482021. 2021235656966</p>
<p>A Survey on Automatic Generation of Figurative Language: From Rule-based Systems to Large Language Models. Huiyuan Lai, Malvina Nissim, Comput. Surveys. 562024. 2024</p>
<p>Human Competence in Creativity Evaluation. Carolyn Lamb, Daniel G Brown, Charles L A Clarke, International Conference on Innovative Computing and Cloud Computing. 201514806090</p>
<p>Evaluating Computational Creativity: An Interdisciplinary Tutorial. Carolyn Lamb, Daniel G Brown, Charles L A Clarke, 10.1145/3167476ACM Comput. Surv. 51342018. Feb. 2018</p>
<p>BACON: A Production System That Discovers Empirical Laws. Pat Langley, International Joint Conference on Artificial Intelligence. 19772320342</p>
<p>Serial reproduction reveals the geometry of visuospatial representations. Thomas A Langlois, Nori Jacoby, Jordan W Suchow, Thomas L Griffiths, Proceedings of the National Academy of Sciences of the United States of America. 1182323767632021. 2021</p>
<p>Deep-speare: A joint neural model of poetic language, meter and rhyme. Han Jey, Trevor Lau, Timothy Cohn, Julian Baldwin, Adam Brooke, Hammond, Annual Meeting of the Association for Computational Linguistics. 201849671404</p>
<p>Polyphonic music modeling with random fields. Victor Lavrenko, Jeremy Pickens, Proceedings of the eleventh ACM international conference on Multimedia. the eleventh ACM international conference on Multimedia2003</p>
<p>Creating a Story-Telling Universe. Michael Lebowitz, International Joint Conference on Artificial Intelligence. 198337655080</p>
<p>Creating characters in a story-telling universe. Michael Lebowitz, 10.1016/0304-422X(84)90001-9Poetics. 131984. 1984</p>
<p>Backpropagation Applied to Handwritten Zip Code Recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D Jackel, 10.1162/neco.1989.1.4.541Neural Computation. 11989. 1989</p>
<p>Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, IEEE Conference on Computer Vision and Pattern Recognition. 2016. 2017. 2016211227</p>
<p>Wen-Ling Lei, Jinting Wang, Fengji Ma, Guanjie Huang, Li Liu, ArXiv abs/2407.08428A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights. 2024. 2024271097523</p>
<p>DALL-E 2 Fails to Reliably Capture Common Syntactic Processes. Evelina Leivada, Elliot Murphy, Gary Marcus, ArXiv abs/2210.128892022. 2022253098841</p>
<p>Why AM and EURISKO appear to work. B Douglas, John Lenat, Seely Brown, Artificial intelligence. 231984. 1984</p>
<p>Nameling: Creative Neologism Generation with Transfer Learning. Rodrigo F Gabriel R Lencione, Paula Y Nogueira, Pasqualini, International Conference on Computational Creativity. 2022. 06 2022</p>
<p>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models. Martha Lewis, Melanie Mitchell, ArXiv abs/2402.089552024. 2024267657861</p>
<p>Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis. Chuan Li, Michael Wand, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2016. 20166635779</p>
<p>Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang, arXiv:2401.00676[cs.CRDigger: Detecting Copyright Content Mis-usage in Large Language Model Training. 2024a</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, Qingyun Wang, Xinya Du, MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents. 2024b271957477</p>
<p>Lisa Xiang, Ari Li, Daniel Holtzman, Percy Fried, Jason Liang, Tatsunori Eisner, Luke Hashimoto, Mike Zettlemoyer, Lewis, arXiv:2210.15097[cs.CL]Contrastive Decoding: Open-ended Text Generation as Optimization. 2023</p>
<p>Zelong Li, Jianchao Ji, Yongfeng Zhang, ArXiv abs/2111.12210From Kepler to Newton: Explainable AI for Science Discovery. 2021. 2021244728403</p>
<p>RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge. Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, Findings. 2021235731975</p>
<p>Applications of artificial intelligence for organic chemistry: the DENDRAL project. Lindsay Robert, No Title)1980. 1980</p>
<p>Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training. Bei Liu, Jianlong Fu, Makoto P Kato, Masatoshi Yoshikawa, 10.1145/3240508.3240587Proceedings of the 26th ACM international conference on Multimedia (MM '18). the 26th ACM international conference on Multimedia (MM '18)ACM2018a</p>
<p>Exploiting Syntactic Structures for Humor Recognition. Lizhen Liu, Donghai Zhang, Wei Song, Proceedings of the 27th International Conference on Computational Linguistics. Emily M Bender, Leon Derczynski, Pierre Isabelle, the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational Linguistics2018b</p>
<p>Feeling of Competence Affects Children's Curiosity and Creativity. Rongzhi Liu, Fei Xu, Cognitive Science. 2200398942020. 2020</p>
<p>How AI Processing Delays Foster Creativity: Exploring Research Question Co-Creation with an LLM-based Agent. Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, Yun Huang, 10.1145/3613904.3642698Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USAAssociation for Computing Machinery202417CHI '24)</p>
<p>Explicating "Creativity. Paisley Livingston, Routledge Handbook on Creativity and Philosophy. Berys Gaut, Matthew Kieran, Routledge2018</p>
<p>Salvatore Luca, Marco Lorello, Stefano Lippi, Melacci, ArXiv abs/2402.17431The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns. 2024. 2024268033655</p>
<p>Deep Dungeons and Dragons: Learning Character-Action Interactions from Role-Playing Game Transcripts. Annie Louis, Charles Sutton, 10.18653/v1/N18-2111Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. Marilyn Walker, Ji Heng, Amanda Stent, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>Lady Lovelace, Sketch of the Analytical Engine invented by Charles Babbage, Esq. Ada's Legacy: Cultures of Computing from the Victorian to the Digital Age. 1843. 1843257837181</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292[cs.AIThe AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024a</p>
<p>Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi, arXiv:2410.04265[cs.CLAI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text. 2024b</p>
<p>Benchmarking Language Model Creativity: A Case Study on Code Generation. Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Daniel Khashabi, ArXiv abs/2407.090072024c. 2024271162279</p>
<p>VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation. Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liangsheng Wang, Yujun Shen, Deli Zhao, Jinren Zhou, Tien-Ping Tan, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023. 2023. 2023257532642</p>
<p>Evaluating creativity in humans, computers, and collectively intelligent systems. Mary Lou, Maher , Network Conference on Creativity and Innovation in Design. 20104840039</p>
<p>Using AI to evaluate creative designs. Mary , Lou Maher, Douglas H Fisher, DS 73-1 Proceedings of the 2nd International Conference on Design Creativity. 20121</p>
<p>Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.01725[cs.CLDiscoveryBench: Towards Data-Driven Discovery with Large Language Models. 2024</p>
<p>Generating Images from Captions with Attention. Elman Mansimov, Emilio Parisotto, Jimmy Ba, Ruslan Salakhutdinov, CoRR abs/1511.027932015. 20159996719</p>
<p>An evolutionary algorithm approach to poetry generation. Hisar Maruli, Manurung , 2004170162691</p>
<p>A Flexible Integrated Architecture For Generating Poetic Texts. Ruli Manurung, Graeme Ritchie, Henry Thompson, 2000. 2000</p>
<p>Using genetic algorithms to create meaningful poetic text. Ruli Manurung, Graeme D Ritchie, Henry S Thompson, Journal of Experimental &amp; Theoretical Artificial Intelligence. 2445287812012. 2012</p>
<p>Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?. Guillermo Marco, Julio Gonzalo, Ram ' Del Castillo, Mar'ia Teresa, Mateo Girona, ArXiv abs/2407.011192024. 2024270870769</p>
<p>A very preliminary analysis of DALL-E 2. Gary Marcus, Ernest Davis, Scott Aaronson, ArXiv abs/2204.138072022. 2022248476147</p>
<p>The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence. F Gary, Marcus, ArXiv abs/2002.061772020. 2020211126492</p>
<p>Esteban Marquer, Pierre-Alexandre Murena, Miguel Couceiro, Transferring Learned Models of Morphological Analogy. 2022258688324</p>
<p>How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN. Margaret Masterman, ; R Thomas Mccoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz, Cybernetics, art and ideas. London Studio Vista. 1971. 2021. 202111244345615Computerized haiku</p>
<p>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve. R Thomas Mccoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L Griffiths, ArXiv abs/2309.136382023. 2023262464572</p>
<p>SemEval 2021 Task 7: HaHackathon, Detecting and Rating Humor and Offense. J A Meaney, Steven Wilson, Luis Chiruzzo, Adam Lopez, Walid Magdy, 10.18653/v1/2021.semeval-1.9Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021). Natalie Schluter, Guy Emerson, Aurelie Herbelot, Xiaodan Zhu, the 15th International Workshop on Semantic Evaluation (SemEval-2021)Alexis Palmer, Nathan SchneiderOnline2021</p>
<p>The associative basis of the creative process. A Sarnoff, Mednick, Psychological review. 6967027591962. 1962</p>
<p>TALE-SPIN, An Interactive Program that Writes Stories. R James, Meehan, International Joint Conference on Artificial Intelligence. 19772372981</p>
<p>Enhancing Creativity in Large Language Models through Associative Thinking Strategies. Pronita Mehrotra, Aishni Parab, Sumit Gulwani, ArXiv abs/2405.067152024. 2024269756853</p>
<p>Locally Typical Sampling. Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell, 10.1162/tacl_a_00536Transactions of the Association for Computational Linguistics. 112023. 2023</p>
<p>On the probability-quality paradox in language generation. Clara Meister, Gian Wiher, Tiago Pimentel, Ryan Cotterell, 10.18653/v1/2022.acl-short.5Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Short Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20222</p>
<p>Making Computers Laugh: Investigations in Automatic Humor Recognition. Rada Mihalcea, Carlo Strapparava, Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Raymond Mooney, Chris Brew, Lee-Feng Chien, Katrin Kirchhoff, Human Language Technology Conference and Conference on Empirical Methods in Natural Language ProcessingVancouver, British Columbia, CanadaAssociation for Computational Linguistics2005</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in Neural Information Processing Systems. C J Burges, L Bottou, M Welling, Z Ghahramani, K Q Weinberger, Curran Associates, Inc201326</p>
<p>Louis T Milic, COPYRIGHT PROTECTION FOR COMPUTER PROGRAMS, DATABASES, AND COMPUTER-GENERATED WORKS: IS ANYTHING NEW SINCE CONTU. 1970. 1993. 1993106141462591The Possible Usefulness of Poetry Generation</p>
<p>SemEval-2017 Task 7: Detection and Interpretation of English Puns. Tristan Miller, Christian Hempelmann, Iryna Gurevych, 10.18653/v1/S17-2005Proceedings of the 11th International Workshop on Semantic Evaluation. Steven Bethard, Marine Carpuat, Marianna Apidianaki, M Saif, Daniel Mohammad, David Cer, Jurgens, the 11th International Workshop on Semantic EvaluationVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Why People Think Computers Can't. Marvin Minsky, AI Mag. 3425655541982. 1982</p>
<p>Large Language Models as General Pattern Machines. F Suvir Mirchandani, Peter R Xia, Brian Florence, Danny Ichter, Montse Driess, Kanishka Gonzalez Arenas, Dorsa Rao, Andy Sadigh, Zeng, ArXiv abs/2307.047212023. 2023259501163</p>
<p>Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals. Piotr Wojciech Mirowski, Kory Wallace Mathewson, Jaylen Pittman, Richard Evans, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2022. 2022252596159</p>
<p>Conditional Generative Adversarial Nets. Mehdi Mirza, Simon Osindero, ArXiv abs/1411.17842014. 201412803511</p>
<p>Abstraction and analogy-making in artificial intelligence. Melanie Mitchell, 10.1111/nyas.14619Annals of the New York Academy of Sciences. 150512021. June 2021</p>
<p>Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. Melanie Mitchell, Alessandro B Palmarini, Arseny Moskvichev, ArXiv abs/2311.092472023. 2023265220802</p>
<p>AmbiPun: Generating Humorous Puns with Ambiguous Context. Anirudh Mittal, Yufei Tian, Nanyun Peng, 10.18653/v1/2022.naacl-main.77Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMarine Carpuat, Marie-Catherine de Marneffe; Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Metaphor as a Medium for Emotion: An Empirical Study. M Saif, Ekaterina Mohammad, Peter D Shutova, Turney, International Workshop on Semantic Evaluation. 2016989439</p>
<p>Behnam Mohammadi, arXiv:2406.05587[cs.CLCreativity Has Left the Chat: The Price of Debiasing Language Models. 2024</p>
<p>Introducing the LCC Metaphor Datasets. Michael Mohler, Mary Brunson, Bryan Rink, Marc Tomlinson, ; , Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). Asuncion Moreno, Jan Odijk, Stelios Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Portorož, Slovenia2016European Language Resources Association (ELRA)</p>
<p>Arseny Moskvichev, Victor Vikram Odouard, Melanie Mitchell, ArXiv abs/2305.07141The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. 2023. 2023258676355</p>
<p>Elliot Murphy, Jill De Villiers, Sofia Morales, ArXiv abs/2403.12294A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2. 2024. 2024268531835</p>
<p>Sam Musker, Alex Duchnowski, Raphael Milliere, Ellie Pavlick, Semantic Structure-Mapping in LLM and Human Analogical Reasoning. 2024270619799</p>
<p>Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. Saeid Naeini, Raeid Saqur, Mozhgan Saeidi, John Giorgi, Babak Taati, arXiv:2306.111672023</p>
<p>Understanding Narratives through Dimensions of Analogy. Thiloshon Nagarajah, Filip Ilievski, Jay Pujara, arXiv:2206.07167[cs.AI2022</p>
<p>Creative Problem Solving in Large Language and Vision Models -What Would it Take?. Lakshmi Nair, Evana Gizzi, Jivko Sinapov, ArXiv abs/2405.014532024. 2024269502505</p>
<p>S Surabhi, Peter Nath, Claire Dayan, Stevenson, arXiv:2405.00899[cs.HCCharacterising the Creative Process in Humans and Large Language Models. 2024</p>
<p>Neil Weinstock, Netanel , Making Sense of Fair Use. 2011152743703</p>
<p>Allen Newell, J C Shaw, Herbert A Simon, The Processes of Creative Thinking. 1959121728459</p>
<p>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, International Conference on Machine Learning. 2021245335086</p>
<p>The dual pathway to creativity model: Creative ideation as a function of flexibility and persistence. Bernard A Nijstad, C D De Dreu, Eric F Rietzschel, Matthijs Baas, European Review of Social Psychology. 211452909672010. 2010</p>
<p>Victor Vikram, Odouard , Melanie Mitchell, ArXiv abs/2206.14187Evaluating Understanding on Conceptual Abstraction Benchmarks. 2022. 2022250089291</p>
<p>PoeTryMe : a versatile platform for poetry generation. Gonçalo Hugo, Oliveira, 201251964516</p>
<p>A Survey on Intelligent Poetry Generation: Languages, Features, Techniques, Reutilisation and Evaluation. Gonçalo Hugo, Oliveira, International Conference on Natural Language Generation. 201732461868</p>
<p>Naming unrelated words predicts creativity. Jay A Olson, Johnny Nahas, Denis Chmoulevitch, Simon J Cropper, Margaret E Webb, Proceedings of the National Academy of Sciences of the United States of America. 1182354724422021. 2021</p>
<p>Do Large Language Models Solve ARC Visual Analogies Like People Do?. Gustaw Opielka, Hannes Rosenbusch, Veerle Vijverberg, Claire E Stevenson, ArXiv abs/2403.097342024. 2024268510601</p>
<p>Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B Hashimoto, arXiv:2310.17623[cs.CLProving Test Set Contamination in Black Box Language Models. 2023</p>
<p>Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models. Peter Organisciak, Selcuk Acar, Denis Dumas, Kelly Berthiaume, Thinking Skills and Creativity. 491013562023. 2023</p>
<p>PoeLM: A Meter-and Rhyme-Controllable Language Model for Unsupervised Poetry Generation. Aitor Ormazabal, Mikel Artetxe, Manex Agirrezabal, Conference on Empirical Methods in Natural Language Processing. 2022a249018166Aitor Soroa Etxabe, and Eneko Agirre</p>
<p>PoeLM: A Meter-and Rhyme-Controllable Language Model for Unsupervised Poetry Generation. Aitor Ormazabal, Mikel Artetxe, Manex Agirrezabal, Aitor Soroa, Eneko Agirre, 10.18653/v1/2022.findings-emnlp.268Findings of the Association for Computational Linguistics: EMNLP 2022. Zornitsa Goldberg, Yue Kozareva, Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>Applied imagination : principles and procedures of creative problem-solving. Alexander Faickney, Osborn , 1957142858886</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J Lowe, ArXiv abs/2203.021552022. 2022</p>
<p>Does Writing with Language Models Reduce Content Diversity?. Vishakh Padmakumar, He He, ArXiv abs/2309.051962023. 2023</p>
<p>A Plug-and-Play Method for Controlled Text Generation. Damian Pascual, Béni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer, ArXiv abs/2109.097072021. 2021237571784</p>
<p>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski, IEEE/CVF International Conference on Computer Vision (ICCV). 2021. 2021. 2021232428282</p>
<p>Context Encoders: Feature Learning by Inpainting. Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, Alexei A Efros, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2016. 20162202933</p>
<p>. Anthony M Paul, Figurative Language. Philosophy and Rhetoric. 31970. 1970</p>
<p>COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion. Debjit Paul, Anette Frank, 10.18653/v1/2021.acl-long.395Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Fei Zong, Wenjie Xia, Roberto Li, Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>. Christine Payne, MuseNet. 2019. 2019. 2019</p>
<p>Alison Pease, Daniel Winterstein, Simon Colton, Evaluating Machine Creativity. 2002. 10 2002</p>
<p>On characterizations of large language models and creativity evaluation. Max Peeperkorn, Dan Brown, Anna Jordanous, 2023. 2023</p>
<p>Is Temperature the Creativity Parameter of Large Language Models?. Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous, arXiv:2405.00492[cs.CL2024</p>
<p>The Eureka Effect: The Art and Logic of Breakthrough Thinking. D N Perkins, 2001</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. K Brenden, Mikel Petersen, Landajuela, arXiv: Learning2019. 2019212956516</p>
<p>Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance. Molly R Petersen, Lonneke Van Der Plas, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. Maxime Peyrard, Beatriz Borges, Kristina Gligoric, Robert West, International Joint Conference on Artificial Intelligence. 2021234778118</p>
<p>GPoeT: a Language Model Trained for Rhyme Generation on Synthetic Data. Andrei Popescu-Belis, Àlex R Atrio, Bastien Bernath, Etienne Boisson, Teo Ferrari, Xavier Theimer-Lienhard, Giorgos Vernikos, Proceedings of the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage. the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage2023. 2023258486928</p>
<p>SemEval-2017 Task 6: #HashtagWars: Learning a Sense of Humor. Peter Potash, Alexey Romanov, Anna Rumshisky, 10.18653/v1/S17-2004Proceedings of the 11th International Workshop on Semantic Evaluation. Steven Bethard, Marine Carpuat, Marianna Apidianaki, M Saif, Daniel Mohammad, David Cer, Jurgens, the 11th International Workshop on Semantic EvaluationVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Thin slices of creativity: Using single-word utterances to assess creative cognition. Ranjani Prabhakaran, Adam E Green, Jeremy R Gray, Behavior Research Methods. 4621598172013. 2013</p>
<p>Large Language Models are Zero Shot Hypothesis Proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.05965[cs.CL2023</p>
<p>Reasoning with Language Model Prompting: A Survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, ArXiv abs/2212.095972022. 2022254854219</p>
<p>Mupt: A generative symbolic music pretrained transformer. Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, arXiv:2404.063932024. 2024arXiv preprint</p>
<p>Generating conjectures on fundamental constants with the Ramanujan Machine. Gal Raayoni, Shahar Gottlieb, Yahel Manor, George Pisha, Yoav Harris, Uri Mendlovic, Doron Haviv, Yaron Hadad, Ido Kaminer, 10.1038/s41586-021-03229-4Nature. 5902021. Feb. 2021</p>
<p>The Policeman's Beard Is Half Constructed. Racter, 1984146585648</p>
<p>Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest. Dragomir Radev, Amanda Stent, Joel Tetreault, Aasish Pappu, Aikaterini Iliakopoulou, Agustin Chanfreau, Paloma De Juan, Jordi Vallmitjana, Alejandro Jaimes, Rahul Jha, Robert Mankoff, ; Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). Stelios Odijk, Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Asuncion Moreno; Portorož, Slovenia2016. JanEuropean Language Resources Association (ELRA)</p>
<p>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. Alec Radford, Luke Metz, Soumith Chintala, CoRR abs/1511.064342015. 201511758569</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683[cs.LG2023</p>
<p>Copyright in computer-composed music: Hal meets Handel. W T Ralston, Journal of the Copyright Society of the U.S.A. 522005. 03 2005</p>
<p>Hierarchical Text-Conditional Image Generation with CLIP Latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, ArXiv abs/2204.061252022. 2022248097655</p>
<p>Zero-Shot Text-to-Image Generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, ArXiv abs/2102.120922021. 2021232035663</p>
<p>PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking. Asli Hannah Rashkin, Yejin Celikyilmaz, Jianfeng Choi, Gao, ArXiv abs/2004.149672020. 2020216868683</p>
<p>Non-literalness and non-bona-fîde in language: An approach to formal and computational treatments of humor. Jonathan D Raskin, Salvatore Attardo, Pragmatics &amp; Cognition. 2622113471994. 1994</p>
<p>DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models. Shauli Royi Rassin, Yoav Ravfogel, Goldberg, ArXiv abs/2210.106062022. 2022252992545</p>
<p>John Carlyle, Raven , Raven Progressive Matrices. 193861756432</p>
<p>Small But Funny: A Feedback-Driven Approach to Humor Distillation. Sahithya Ravi, Patrick Huber, Akshat Shrivastava, Vered Shwartz, Arash Einolghozati, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Learning What and Where to Draw. Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, Honglak Lee, ArXiv abs/1610.024542016a. 20161515901</p>
<p>Generative Adversarial Text to Image Synthesis. Scott E Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee, International Conference on Machine Learning. 2016b1563370</p>
<p>. He Ren, Quan Yang, Neural Joke Generation. 2017</p>
<p>An Analysis of Creativity. Mel Rhodes, The Phi Delta Kappan. 421961. 1961</p>
<p>Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020. 2021. 2020220936362</p>
<p>Sean N Riley, Liane Gabora, arXiv:1308.4245Evidence that Threatening Situations Enhance Creativity. 2019q-bio.NC</p>
<p>Some Empirical Criteria for Attributing Creativity to a Computer Program. Minds and Machines. Graeme D Ritchie, 2007. 2007177947377</p>
<p>A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music. Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck, ArXiv abs/1803.054282018. 20183891811</p>
<p>Why do people use figurative language?. M Richard, Roger J Roberts, Kreuz, Psychological science. 51994. 1994</p>
<p>High-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach, A Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021. 2022. 2021245335280</p>
<p>Creativity: a survey of AI approaches. Jon Rowe, Derek Partridge, Artificial Intelligence Review. 7134331121993. 1993</p>
<p>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2023. 2022251800180</p>
<p>The Standard Definition of Creativity. Mark A Runco, Garrett J Jaeger, 10.1080/10400419.2012.650092Creativity Research Journal. 242012. 2012</p>
<p>Scientific discovery: compulalional explorations of the creative process. George Rzevski, Pat Langley, Herbert A Simon, Gary L Bradshaw, Jan M Zytkow, 1987</p>
<p>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Seyed Kamyar, Seyed Ghasemipour, Burcu Karagol Ayan, Sara Seyedeh, Raphael Gontijo Mahdavi, Tim Lopes, Jonathan Salimans, David J Ho, Mohammad Fleet, Norouzi, ArXiv abs/2205.114872022. 2022248986576</p>
<p>Prisha Samadarshi, Mariam Mustafa, Anushka Kulkarni, Raven Rothkopf, Tuhin Chakrabarty, Smaranda Muresan, arXiv:2406.11012[cs.CLConnecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game. 2024</p>
<p>Allocating Ownership Rights in Computer-Generated Works. Geoffrey Sampson, Pittsburgh Law Review. 471429112552017. 1986. 1986Equinox PublishingUniversity ofThe Linguistics Delusion</p>
<p>Measuring abstract reasoning in neural networks. Adam Santoro, Felix Hill, G T David, Ari S Barrett, Timothy P Morcos, Lillicrap, ArXiv abs/1807.042252018. 201849665167</p>
<p>Mechanistic?. Naomi Saphra, Sarah Wiegreffe, arXiv:2410.09087[cs.AI2024</p>
<p>Between the AI and Me: Analysing Listeners' Perspectives on AIand Human-Composed Progressive Metal Music. Pedro Sarmento, John H Loth, Mathieu Barthet, ArXiv abs/2407.216152024. 2024271571547</p>
<p>Automating scientific discovery. Neil Savage, Commun. ACM. 552012. 2012</p>
<p>PEER: A Collaborative Language Model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, Sebastian Riedel, ArXiv abs/2208.116632022. 2022251765117</p>
<p>Distilling free-form natural laws from experimental data. Michael Schmidt, Hod Lipson, science. 3242009. 2009</p>
<p>Dafna Shahaf, Eric Horvitz, Robert Mankoff, Inside Jokes: Identifying Humorous Cartoon Captions. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2015. 201514570747</p>
<p>Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations. Tiancheng Shen, Jun Hao Liew, Long Mai, Lu Qi, Jiashi Feng, Jiaya Jia, ArXiv abs/2406.001212024. 2024270218211</p>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, arXiv:2310.16789[cs.CLDetecting Pretraining Data from Large Language Models. 2024</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109[cs.CLCan LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. 2024</p>
<p>Taking the U.S. Patent Office Criteria Seriously: A Quantitative Three-Criterion Creativity Definition and Its Implications. Dean Keith, Simonton , 10.1080/10400419.2012.676974Creativity Research Journal. 242012. 2012</p>
<p>Make-A-Video: Text-to-Video Generation without Text-Video Data. Uriel Singer, Adam Polyak, Thomas Hayes, Xiaoyue Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman, ArXiv abs/2209.147922022. 2022252595919</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. D Michael, Sam Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Michael J Hinks, Manvitha Hammerling, Ponnapati, G Samuel, Andrew D Rodriques, White, 2024. 2024preprint</p>
<p>Incubation and the persistence of fixation in problem solving. Steven M Smith, Steven E Blankenship, The American journal of psychology. 104103596321991. 1991</p>
<p>Artificial Intelligence's Fair Use Crisis. Benjamin Sobel, Columbia Journal of Law and the Arts. 412017. 2017</p>
<p>Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models. Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2023. 2022254366634</p>
<p>Zhivar Sourati, Filip Ilievski, Pia Sommerauer, ARN: Analogical Reasoning on Narratives. 2023263605669</p>
<p>Creativity and Culture. Creativity in Art, Religion, and Culture. Morris Isaac, Stein , 1953. 1953144504562</p>
<p>Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, Han Van Der Maas, arXiv:2206.08932[cs.AI]Putting GPT-3's Creativity to the (Alternative Uses) Test. 2022</p>
<p>HAHAcronym: A Computational Humor System. Oliviero Stock, Carlo Strapparava, 10.3115/1225753.1225782Proceedings of the ACL Interactive Poster and Demonstration Sessions. Masaaki Nagata, Ted Pedersen, the ACL Interactive Poster and Demonstration SessionsAnn Arbor, MichiganAssociation for Computational Linguistics2005</p>
<p>Metaphor Generation with Conceptual Mappings. Kevin Stowe, Tuhin Chakrabarty, Nanyun Peng, Smaranda Muresan, Iryna Gurevych, ArXiv abs/2106.012282021. 2021</p>
<p>Metaphoric Paraphrase Generation. Kevin Stowe, Leonardo Ribeiro, Iryna Gurevych, ArXiv abs/2002.128542020. 2020211572789</p>
<p>Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes. Oren Sultan, Dafna Shahaf, 10.18653/v1/2022.emnlp-main.232Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Douglas Summers-Stay, Stephanie M Lukin, Clare R Voss, Brainstorm, then Select: a Generative Language Model Improves Its Creativity Score. 2023</p>
<p>Evaluating Large Language Models on Controlled Generation Tasks. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, Xuezhe Ma, 10.18653/v1/2023.emnlp-main.190Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Dialogues: The Science and Power of Storytelling. Wendy A Suzuki, I Mónica, Uri Feliú-Mójer, Rachel Hasson, Jean Mary Yehuda, Zarate, 10.1523/JNEUROSCI.1942-18.2018arXivJournal of Neuroscience. 382018. 2018. 2018</p>
<p>Benjamin Swanson, Kory Wallace Mathewson, Ben Pietrzak, Sherol Chen, Monica Dinalescu, Story Centaur: Large Language Model Few Shot Learning as a Creative Writing Tool. the Association for Computational Linguistics2021233365238Conference of the European Chapter</p>
<p>AGATHA: Automatic Graph Mining And Transformer based Hypothesis Generation Approach. Justin Sybrandt, Ilya Tyagin, Michael Shtutman, Ilya Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management (Virtual Event, Ireland) (CIKM '20). the 29th ACM International Conference on Information &amp; Knowledge Management (Virtual Event, Ireland) (CIKM '20)New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Evaluating the Factual Consistency of Large Language Models Through News Summarization. Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Annual Meeting of the Association for Computational Linguistics. 2022253523092Mohit Bansal, and Colin Raffel</p>
<p>Controllable Neural Story Plot Generation via Reward Shaping. Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J Martin, Animesh Mehta, Brent Harrison, Mark O Riedl, International Joint Conference on Artificial Intelligence. 2018</p>
<p>Leonard Tang, Alexander Cai, Steve Li, Jason Wang, arXiv:2211.14369[cs.CLThe Naughtyformer: A Transformer Understands Offensive Humor. 2022</p>
<p>Computationally recognizing wordplay in jokes. M Julia, Lawrence J Taylor, Mazlack, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society200426</p>
<p>Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2022. 2022248006414</p>
<p>Are Large Language Models Capable of Generating Human-Level Narratives?. Yufei Tian, Tenghao Huang, Miri Liu, Derek Jiang, Alexander Spangher, Muhao Chen, Jonathan May, Nanyun Peng, ArXiv abs/2407.132482024. 2024271270813</p>
<p>HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. Yufei Tian, Arvind Krishna Sridhar, Nanyun Peng, ArXiv abs/2109.050972021. 2021237491428</p>
<p>Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features. Yufei Tian, Nanyun Peng, 10.18653/v1/2022.naacl-main.262Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMarine Carpuat, Marie-Catherine de Marneffe; Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>MacGyver: Are Large Language Models Creative Problem Solvers?. Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Joseph, Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L Griffiths, Faeze Brahman, ArXiv abs/2311.096822023. 2023</p>
<p>A connectionist approach to algorithmic composition. Todd Peter, Computer Music Journal. 131989. 1989</p>
<p>Ljup Co, Todorovski , Declarative Bias in Equation Discovery. 19977320694</p>
<p>Torrance Tests of Creative Thinking: Verbal Tests, Forms A and B, Figural Tests, Forms A and B. E P Torrance, 1974</p>
<p>Solving olympiad geometry without human demonstrations. H Trieu, Yuhuai Trinh, Quoc V Wu, He Le, Thang He, Luong, 10.1038/s41586-023-06747-5Nature. 6252024. 01 Jan 2024</p>
<p>MoCoGAN: Decomposing Motion and Content for Video Generation. S Tulyakov, Ming-Yu Liu, Xiaodong Yang, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Jan Kautz. 2017. 2018. 20174475365</p>
<p>A M Turing, 10.1093/mind/LIX.236.433COMPUTING MACHINERY AND INTELLIGENCE. Mind LIX. 1950. 10 1950</p>
<p>R Scott, Turner, The Creative Process: A Computer Model of Storytelling and Creativity. 1994260575817</p>
<p>The Latent Relation Mapping Engine: Algorithm and Experiments. D Peter, Turney, J. Artif. Intell. Res. 3371126022008. 2008</p>
<p>AI Feynman 2.0: Paretooptimal symbolic regression exploiting graph modularity. S M Udrescu, Andrew , Yong-Yi Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, Max Tegmark, ArXiv abs/2006.107822020. 2020219955930</p>
<p>John von Neumann 1903-1957. Stanisław Ulam, Bull. Amer. Math. Soc. 641223742681958. 1958</p>
<p>BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?. Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, Jose Camacho-Collados, 10.18653/v1/2021.acl-long.280Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics20211</p>
<p>Automatic Poetry Generation from Prosaic Text. Tim Van De Cruys, 10.18653/v1/2020.acl-main.223Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Conditional Image Generation with PixelCNN Decoders. Aäron Van Den Oord, Nal Kalchbrenner, Lasse Espeholt, Koray Kavukcuoglu, ArXiv abs/1606.053282016b. 201614989939Oriol Vinyals, and Alex Graves</p>
<p>Pixel Recurrent Neural Networks. Aäron Van Den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, International Conference on Machine Learning. 2016a8142135</p>
<p>SemEval-2018 Task 3: Irony Detection in English Tweets. Cynthia Van Hee, Els Lefever, Véronique Hoste, 10.18653/v1/S18-1005Proceedings of the 12th International Workshop on Semantic Evaluation. Marianna Apidianaki, M Saif, Jonathan Mohammad, Ekaterina May, Steven Shutova, Marine Bethard, Carpuat, the 12th International Workshop on Semantic EvaluationNew Orleans, LouisianaAssociation for Computational Linguistics2018</p>
<p>Attention is All you Need. Ashish Vaswani, Noam M Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Neural Information Processing Systems. 201713756489</p>
<p>Game of Tropes: Exploring the Placebo Effect in Computational Creativity. Tony Veale, International Conference on Innovative Computing and Cloud Computing. 20153966712</p>
<p>Computational Creativity: The Philosophy and Engineering of Autonomously Creative Systems. T Veale, F A Cardoso, 2019Springer</p>
<p>Metaphor: A Computational Perspective. T Veale, E Shutova, B Klebanov, 2016Morgan &amp; Claypool Publishers</p>
<p>Spicy Adjectives and Nominal Donkeys: Capturing Semantic Deviance Using Compositionality in Distributional Spaces. Eva M Vecchi, Marco Marelli, Roberto Zamparelli, Marco Baroni, 10.1111/cogs.12330Cognitive Science. 412017. 2017</p>
<p>Artificial Intelligence &amp; Creativity: A Manifesto for Collaboration. Florent Vinchon, Todd Lubart, Sabrina Bartolotta, Valentin Gironnay, Marion Botella, Samira Bourgeois-Bougrine, Jean-Marie Burkhardt, Nathalie Bonnardel, Giovanni Emanuele Corazza, Vlad Petre Glăveanu, Michael Hanchett Hanson, Zorana Ivcevic, Maciej Karwowski, J Kaufman, Takeshi Okada, Roni Reiter-Palmon, Andrea Gaggioli, The Journal of Creative Behavior. 2594804962023. 2023</p>
<p>Generating Videos with Scene Dynamics. Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, Neural Information Processing Systems. 20169933254</p>
<p>The Art of Thought. G Wallas, 1926Solis Press</p>
<p>Automating Science. David Waltz, Bruce G Buchanan, 10.1126/science.1172781Science. 3242009. 2009</p>
<p>Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi, arXiv:2401.01623[cs.AICan AI Be as Creative as Humans. 2024b</p>
<p>SciMON: Scientific Inspiration Machines Optimized for Novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Annual Meeting of the Association for Computational Linguistics. 2023258841365</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.14259[cs.CLSciMON: Scientific Inspiration Machines Optimized for Novelty. 2024a</p>
<p>Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan, ArXiv abs/1905.07870PaperRobot: Incremental Draft Generation of Scientific Ideas. 2019. 2019</p>
<p>Once more with feeling: Normative data for the aha experience in insight and noninsight problems. Margaret Webb, Daniel Little, Simon Cropper, 10.3758/s13428-017-0972-9Behavior Research Methods. 50102017. 2017</p>
<p>Emergent analogical reasoning in large language models. Taylor W Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 72548545752022. 2022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, arXiv:2206.07682[cs.CL]Emergent Abilities of Large Language Models. 2022</p>
<p>On the Usefulness of "Value" in the Definition of Creativity. R W Weisberg, W H Freeman, 10.1080/10400419.2015.1030320Creativity Research Journal. 271986. 2015a. 2015Creativity: Genius and Other Myths</p>
<p>On the Usefulness of "Value" in the Definition of Creativity. Robert W Weisberg, Creativity Research Journal. 271468311402015b. 2015</p>
<p>Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model. Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schuetze, Kemal Oflazer, David Mortensen, 10.18653/v1/2023.emnlp-main.401Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Humor Detection: A Transformer Gets the Last Laugh. Orion Weller, Kevin Seppi, 10.18653/v1/D19-1372Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Vincent Jiang, Xiaojun Ng, Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019Kentaro Inui</p>
<p>ANALOGICAL -A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models. Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bimal Gajera, Mukul Shreeyash, Chandan Gowaikar, Aman Gupta, Aishwarya N Chadha, Reganti, P Amit, Amitava Sheth, Das, ArXiv abs/2305.050502023. 2023258947824</p>
<p>Expertise as mental set: The effects of domain knowledge in creative problem solving. Jennifer Wiley, Memory &amp; Cognition. 26457755091998. 1998</p>
<p>Dutch humor detection by generating negative examples. Thomas Winters, Pieter Delobelle, arXiv:2010.136522020. 2020arXiv preprint</p>
<p>When concepts combine. J Edward, Wisniewski, Psychonomic bulletin &amp; review. 41997. 1997</p>
<p>Toward an artificial intelligence physicist for unsupervised learning. Tailin Wu, Max Tegmark, Physical Review E. 100333112019. 2019</p>
<p>FunQA: Towards Surprising Video Comprehension. Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu, ArXiv abs/2306.148992023c. 2023259262297</p>
<p>Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition. Yubo Xie, Junze Li, Pearl Pu, 10.18653/v1/2021.acl-short.6Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Short Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20212</p>
<p>A Study of Large Language Models in Storytelling. Zhuohan Xie, Trevor Cohn, Jey Han Lau, 10.18653/v1/2023.inlg-main.23Proceedings of the 16th International Natural Language Generation Conference. C , Maria Keet, Hung-Yi Lee, Sina Zarrieß, the 16th International Natural Language Generation ConferencePrague, CzechiaAssociation for Computational Linguistics2023aThe Next Chapter</p>
<p>DeltaScore: Fine-Grained Story Evaluation with Perturbations. Zhuohan Xie, Miao Li, Trevor Cohn, Jey Lau, 10.18653/v1/2023.findings-emnlp.353Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023b</p>
<p>A Survey on Video Diffusion Models. Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Hang-Rui Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang, ArXiv abs/2310.106472023. 2023</p>
<p>Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus. Yudong Xu, Elias Boutros Khalil, Scott Sanner, AAAI Conference on Artificial Intelligence. 2022252968055</p>
<p>LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations. Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias Boutros Khalil, ArXiv abs/2305.183542023. 2023258968016</p>
<p>MEXICA: A computer model of a cognitive account of creative writing. Rafael Pérez, Y Pérez, Mike Sharples, Journal of Experimental &amp; Theoretical Artificial Intelligence. 13186763342001. 2001</p>
<p>VideoGPT: Video Generation using VQ-VAE and Transformers. Wilson Yan, Yunzhi Zhang, P Abbeel, A Srinivas, ArXiv abs/2104.101572021. 2021233307257</p>
<p>Attribute2Image: Conditional Image Generation from Visual Attributes. Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee, European Conference on Computer Vision. 20157577075</p>
<p>What makes a good story and how can we measure it? a comprehensive survey of story evaluation. Dingyi Yang, Qin Jin, arXiv:2408.146222024. 2024arXiv preprint</p>
<p>Humor Recognition and Humor Anchor Extraction. Diyi Yang, Alon Lavie, Chris Dyer, Eduard H Hovy, Conference on Empirical Methods in Natural Language Processing. 2015</p>
<p>Re3: Generating Longer Stories With Recursive Reprompting and Revision. Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein, 10.18653/v1/2022.emnlp-main.296Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation. Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang, ArXiv abs/1703.108472017. 2017</p>
<p>On the evaluation of generative models in music. Li-Chia Yang, Alexander Lerch, Neural Computing and Applications. 32532582712018. 2018</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, E Cambria, Annual Meeting of the Association for Computational Linguistics. 2023261557055</p>
<p>Plan-And-Write: Towards Better Automatic Storytelling. Lili Yao, Nanyun Peng, Weischedel Ralph, Kevin Knight, Dongyan Zhao, Rui Yan, The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19). 2019</p>
<p>Large Language Models as Analogical Reasoners. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed Huai Hsin, Chi , Denny Zhou, ArXiv abs/2310.017142023. 2023263608847</p>
<p>A Good Algorithm Does Not Steal -It Imitates": The Originality Report as a Means of Measuring When a Music Generation Algorithm Copies Too Much. Zong Yin, Federico Reuben, Susan Stepney, Tom Collins, EvoMUSART. 2021233236232</p>
<p>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, AAAI Conference on Artificial Intelligence. 20163439214</p>
<p>A Neural Approach to Pun Generation. Zhiwei Yu, Jiwei Tan, Xiaojun Wan, Annual Meeting of the Association for Computational Linguistics. 201851874992</p>
<p>Wordcraft: Story Writing With Large Language Models. Ann Yuan, Andy Coenen, Emily Reif, Daphne Ippolito, Proceedings of the 27th International Conference on Intelligent User Interfaces. the 27th International Conference on Intelligent User Interfaces2022. 2022247585187</p>
<p>Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, arXiv:2402.16153Chatmusician: Understanding and generating music intrinsically with llm. 2024. 2024arXiv preprint</p>
<p>Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. Siyu Yuan, Jiangjie Chen, Xuyang Ge, Yanghua Xiao, Deqing Yang, Conference on Empirical Methods in Natural Language Processing. 2023258833317</p>
<p>Vinicius Zambaldi, David La, Alexander E Chu, Harshnira Patani, Amy E Danson, Tristan O C Kwan, Thomas Frerix, Rosalia G Schneider, David Saxton, Ashok Thillaisundaram, Zachary Wu, Isabel Moraes, Oskar Lange, Eliseo Papa, Gabriella Stanton, Victor Martin, Sukhdeep Singh, Lai H Wong, Russ Bates, Simon A Kohl, Josh Abramson, Andrew W Senior, Yilmaz Alguel, Mary Y Wu, Irene M Aspalter, Katie Bentley, L V David, Peter Bauer, Demis Cherepanov, Pushmeet Hassabis, Rob Kohli, Jue Fergus, Wang, arXiv:2409.08022De novo design of high-affinity protein binders with AlphaProteo. 2024q-bio.BM</p>
<p>Understanding and Mitigating Compositional Issues in Text-to-Image Generative Models. Arman Zarei, Keivan Rezaei, Samyadeep Basu, Mehrdad Saberi, Mazda Moayeri, Priyatham Kattakinda, Soheil Feizi, ArXiv abs/2406.078442024. 2024270391565</p>
<p>Linguistic creativity from a cognitive perspective. Britta Zawada, 10.2989/16073610609486419Southern African Linguistics and Applied Language Studies. 242006. 2006</p>
<p>RAVEN: A Dataset for Relational and Analogical Visual REasoNing. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019a. 2019. 201971148268</p>
<p>ACRE: Abstract Causal REasoning Beyond Covariation. Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021. 2021. 2021232380074</p>
<p>Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes. Dongyu Zhang, Heting Zhang, Xikai Liu, Hongfei Lin, Feng Xia, 10.18653/v1/D19-1673Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019b</p>
<p>StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas, IEEE International Conference on Computer Vision (ICCV). 2016b. 2017. 20161277217</p>
<p>How Language Model Hallucinations Can Snowball. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, Noah A Smith, ArXiv abs/2305.135342023. 2023258841857</p>
<p>Colorful Image Colorization. Richard Zhang, Phillip Isola, Alexei A Efros, European Conference on Computer Vision. 2016a50698</p>
<p>Recognizing Humor on Twitter. Renxian Zhang, Naishi Liu, 10.1145/2661829.2661997Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. the 23rd ACM International Conference on Conference on Information and Knowledge ManagementShanghai, China; New York, NY, USAAssociation for Computing Machinery2014CIKM '14)</p>
<p>Chinese Poetry Generation with Recurrent Neural Networks. Xingxing Zhang, Mirella Lapata, Conference on Empirical Methods in Natural Language Processing. 201412964363</p>
<p>Wenting Zhao, Justin T Chiu, Jena D Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang , Lorraine Li, Alane Suhr, ArXiv abs/2311.08469UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations. 2023a. 2023</p>
<p>DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. Wei Zhao, Michael Strube, Steffen Eger, 10.18653/v1/2023.eacl-main.278Proceedings of the 17th Conference of the European Chapter. Andreas Vlachos, Isabelle Augenstein, the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational Linguistics2023b</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Jiang, arXiv:2303.18223[cs.CL]A Survey of Large Language Models. Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, Ruiyang Ren, Yifan Li, Xinyu Tang2023c</p>
<p>Xuandong Zhao, Lei Li, Yu-Xiang Wang, arXiv:2205.01863[cs.CLProvably Confidential Language Modelling. 2022</p>
<p>Assessing and Understanding Creativity in Large Language Models. Yunpu Zhao, Rui Zhang, Wenyi Li, Di Huang, Jiaming Guo, Shaohui Peng, Yifan Hao, Yuanbo Wen, Xingui Hu, Zidong Du, Qi Guo, Ling Li, Yunji Chen, ArXiv abs/2401.124912024. 2024</p>
<p>Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation. Shan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, Pan Zhou, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023. 2024. 2023265659076</p>
<p>Sentence Analogies: Linguistic Regularities in Sentence Embeddings. Xunjie Zhu, Gerard De, Melo , 10.18653/v1/2020.coling-main.300Proceedings of the 28th International Conference on Computational Linguistics. Donia Scott, Nuria Bel, Chengqing Zong, the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Humor Detection in Product Question Answering Systems. Yftah Ziser, Elad Kravi, David Carmel, 10.1145/3397271.3401077Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, ChinaNew York, NY, USAAssociation for Computing Machinery2020SIGIR '20)</p>            </div>
        </div>

    </div>
</body>
</html>
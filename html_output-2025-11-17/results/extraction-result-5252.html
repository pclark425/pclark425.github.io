<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5252 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5252</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5252</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-269761536</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.08011v3.pdf" target="_blank">A Survey of Large Language Models for Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at \url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5252.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5252.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graphtext</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text conversion method that translates graph structures into natural language by deriving a graph-syntax tree and feeding the resulting textual serialization to an LLM for reasoning without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-syntax-tree linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Derive a graph-syntax tree from the graph and convert it into a natural-language sequence (a textual, tree-based serialization) that captures node/edge relations and is consumed by an LLM; intended as a training-free textual encoding for LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / text-attributed graphs (evaluated on graph reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Explicitly tree-structured linearization; designed for interpretability and to be training-free; aims to preserve relational structure via syntax-tree ordering but may be verbose; suitable when nodes carry textual attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph reasoning / interactive graph reasoning (training-free evaluation with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as a training-free alternative to GNN prefixes and learned tokenizers; claimed to enable interactive graph reasoning with LLMs, but no numeric comparisons reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential verbosity and token-budget issues for large graphs; ordering choices may cause loss of some structural nuances; relies on LLM's ability to interpret the text encoding (risk of hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5252.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkLikeGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk like a Graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analyses and proposes text-based encoding strategies for graphs to be processed by LLMs and introduces the GraphQA benchmark to measure encoding strategy effects on performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>text-based graph encoder / serialization strategies</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A family of strategies that linearize graphs into textual formats (various encoder functions / templates) designed to be consumed by LLMs; the paper systematically studies different encodings and their effect on downstream QA and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / graph QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Focus on encoder function design and their effect on faithfulness and model comprehension; examines trade-offs between compactness (token budget) and fidelity to structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>GraphQA (graph question answering) benchmark; graph reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported numerically in survey (study focuses on methodological comparison); metric types implied: QA accuracy / reasoning correctness</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compares multiple text-encoding strategies (different encoder functions/templates) and studies their relative effectiveness; highlights that encoding choice materially affects LLM performance, but concrete numeric winners are not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choosing an encoding that balances token limits and structural faithfulness is challenging; encoder design may not generalize across graph types or scales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5252.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tuning-required approach that transforms graph entities into textual sequences via random walks, then fine-tunes language models on these sequences to produce graph-aware embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Walklm: A uniform language model fine-tuning framework for attributed graph embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>random-walk-based textual sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate sequences from random walks over the graph (walks produce ordered node sequences), convert node attributes and walk context into textual prompts/sequences, and fine-tune an LM to produce attributed graph embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>attributed graphs (general / citation / social-like graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Captures local structural connectivity via walk orderings; relatively compact compared to full-graph serializations; preserves neighborhood patterns but may lose global structure unless long or many walks used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Attributed graph embedding tasks (e.g., node classification) after LM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as a language-model fine-tuning alternative to GNN encoders or other serializations; claims improved attributed embeddings via LM fine-tuning on walk-derived sequences (survey does not tabulate numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Random-walk sampling introduces stochasticity and potential coverage gaps; choosing walk length/count trades off token budget vs. structural coverage; global structure may be under-represented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5252.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaGA: Large Language and Graph Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tuning-required LLM-only method that restructures graph data using node-level templates into organized token sequences, mapping them into the LM token embedding space for downstream graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLaGA: Large Language and Graph Assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>node-level template serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use predefined node-level templates to serialize graph nodes (and local structure) into ordered textual tokens; the serialized sequences are then embedded via the LM token embeddings and used in fine-tuning or prompt-based inference.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs (node classification, reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Template-based, systematic serialization intended to preserve per-node local context; interpretable and consistent across nodes; can be mapped directly to LM input embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Various graph tasks (generalization, interpretability, node-level tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Contrasted with GNN-as-prefix and random-walk serializations; claimed to improve versatility, generalizability and interpretability but survey does not list numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on template design; token budget and ordering still constrain representable structure; may require tuning for different graph types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5252.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GNN-as-prefix method that first aligns a graph encoder with natural language (text-graph grounding) and then projects node representations to language token space for LLM instruction tuning, enabling zero-shot graph task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>projected node-token prefix (graph-encoder → language token projector)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Train/align a graph encoder to natural-language semantics (text-graph grounding) and use a learned projector to map per-node GNN embeddings into fixed-length semantic tokens that act as a prefix/input to an LLM for instruction tuning and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs / text-attributed graphs / heterogeneous graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Structure-aware tokens produced by learned projection; designed to be compact (fixed-length tokens per node) and directly consumable by LLM prefix; aims for high semantic alignment and strong zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node-level downstream tasks including node classification and link prediction; zero-shot and multi-task evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Positioned against purely textual serializations and graph-level pooling; claims strong zero-shot transfer and multi-task capability due to learned alignment, but survey provides no numeric figures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires training a graph encoder and projector (not purely tuning-free); effectiveness on non-text-attributed graphs and optimal coordination between GNN and LLM training remain open questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5252.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTranslator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-level tokenization approach that uses a translator with shared self-attention and cross-attention to map graph-encoded node representations into fixed-length semantic tokens for LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>translator-based semantic token mapping</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A translator module with shared self-attention aligns target node and instruction and uses cross-attention to map graph-encoder (node) representations into fixed-length semantic tokens that can be used as prompts or prefix tokens for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs; open-ended graph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Produces fixed-length, instruction-aligned tokens preserving local node semantics; designed for open-ended tasks and instruction-following by LLMs; reduces raw-graph verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Open-ended graph tasks and instruction-based graph prediction (e.g., varied graph reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared conceptually with direct textual serializations and other projector/prefix approaches; survey reports improved instruction alignment and LLM utilization but no numeric benchmarks are given.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Depends on training the translator and alignment mechanism; may be sensitive to instruction design and to distribution shifts across graph domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5252.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical benchmark and study that converts graphs into textual descriptions to evaluate off-the-shelf LLMs' ability to understand and reason about graph-structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>textual graph descriptions / prompt-based serializations</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph instances into textual descriptions (natural-language prompts) or question-focused textual encodings and feed them to LLMs for empirical evaluation; focuses on zero-shot/few-shot LLM capabilities without additional finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs (graph-structured data benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Tuning-free textual prompts emphasize simplicity and immediate LLM compatibility; good for probing LLM innate graph reasoning but limited by token constraints and LLM training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Benchmarking LLM understanding: graph reasoning, QA over graphs, empirical evaluation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as empirical evaluation results in the original paper (accuracy/QA scores) — survey does not reproduce numerical metrics here</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Serves as a baseline to compare tuning-free textual serializations vs tuned or GNN-augmented methods; demonstrates limitations of off-the-shelf LLMs on complex graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLMs often struggle with complex structural reasoning from plain text encodings; token limits and hallucination; performance depends heavily on prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5252.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of works that study LLMs applied directly to graph learning tasks (e.g., node classification) often via textual encodings or LLM-generated supervision; explores two pipelines: LLMs-as-Enhancers and LLMs-as-Predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the potential of large language models (llms) in learning on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>LLM-centric textual pipelines (LLMs-as-Enhancers / LLMs-as-Predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>LLMs generate text-based signals (explanations, embeddings, labels) or consume textual serializations of graph substructures; pipelines either use LLMs to enrich node textual profiles (Enhancers) or to directly predict node labels from text (Predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs, general graphs (node classification, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LLM common knowledge and semantic summarization; flexible and suitable for low-data regimes; may be decoupled from GNNs (two-stage).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification, link prediction, graph reasoning; zero-shot and few-shot transfer tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared qualitatively to GNN-based pipelines and GNN-as-prefix methods; LLM-based pipelines can provide rich semantic signals but often require post-filtering or calibration to match GNN performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Often two-stage (decoupled) and sensitive to prompt engineering; can be computationally expensive for large graphs; alignment between LLM outputs and GNN training objectives is an open issue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5252.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical study and multimodal encoding approach (GraphTMI) that compares text, image, and motif encodings for graph understanding with LLMs and introduces benchmarks; finds image modality often better at balancing token limits and preserving essential structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>multimodal encodings (text / image / motif serializations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Explore multiple modalities to encode graphs: textual serialization, motif-based compact descriptions, and image renderings of graphs (or subgraphs) fed into multimodal LLMs; each modality addresses token/expressivity trade-offs differently.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs; evaluated on graph structure analysis tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Image modality shown to be more token-efficient and better at preserving certain structural cues; motif encodings capture recurring structural patterns compactly; text is most interpretable but can hit token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Graph structure analysis and graph understanding benchmarks (GraphTMI benchmark introduced)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states image modality outperforms text and prior GNNs on the proposed benchmark (numeric values not reproduced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Direct comparison across text/motif/image encodings; image modality reported best balance between token constraints and information preservation, outperforming text serializations and some GNN baselines on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Image-based encodings require multimodal LLMs (vision-capable); potential OCR/interpretation challenges for complex graphs; modality choice may depend on graph size and downstream task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5252.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5252.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuseGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tuning-required instruction-tuning framework that converts graphs into compact textual descriptions and uses graph-aware instruction tuning to teach LLMs generic graph mining capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>compact graph description + instruction templates</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create compact textual descriptions of graphs (concise summaries of structure and attributes) combined with diverse instruction generation to instruction-tune LLMs for multiple graph mining tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>attributed graphs / general graph mining tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Compactness-focused serialization to respect token budgets; aims to reduce verbosity while retaining task-relevant structure; instruction diversity helps generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Generic graph mining tasks (reasoning, classification, generation) after instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared conceptually to naive text serializations and to GNN-based approaches; claims general-purpose gains through instruction tuning but survey does not list numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing compact yet faithful descriptions is nontrivial; may lose fine-grained structural details necessary for some tasks; instruction tuning can be costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Graphs', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graphtext: Graph reasoning in text space. <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models. <em>(Rating: 2)</em></li>
                <li>Walklm: A uniform language model fine-tuning framework for attributed graph embedding. <em>(Rating: 2)</em></li>
                <li>LLaGA: Large Language and Graph Assistant. <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models. <em>(Rating: 2)</em></li>
                <li>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models. <em>(Rating: 2)</em></li>
                <li>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining. <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5252",
    "paper_id": "paper-269761536",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GraphText",
            "name_full": "Graphtext",
            "brief_description": "A graph-to-text conversion method that translates graph structures into natural language by deriving a graph-syntax tree and feeding the resulting textual serialization to an LLM for reasoning without finetuning.",
            "citation_title": "Graphtext: Graph reasoning in text space.",
            "mention_or_use": "mention",
            "representation_name": "graph-syntax-tree linearization",
            "representation_description": "Derive a graph-syntax tree from the graph and convert it into a natural-language sequence (a textual, tree-based serialization) that captures node/edge relations and is consumed by an LLM; intended as a training-free textual encoding for LLM inference.",
            "graph_type": "general graphs / text-attributed graphs (evaluated on graph reasoning tasks)",
            "representation_properties": "Explicitly tree-structured linearization; designed for interpretability and to be training-free; aims to preserve relational structure via syntax-tree ordering but may be verbose; suitable when nodes carry textual attributes.",
            "evaluation_task": "Graph reasoning / interactive graph reasoning (training-free evaluation with LLMs)",
            "performance_metrics": null,
            "comparison_to_other_representations": "Presented as a training-free alternative to GNN prefixes and learned tokenizers; claimed to enable interactive graph reasoning with LLMs, but no numeric comparisons reported in this survey.",
            "limitations_or_challenges": "Potential verbosity and token-budget issues for large graphs; ordering choices may cause loss of some structural nuances; relies on LLM's ability to interpret the text encoding (risk of hallucination).",
            "uuid": "e5252.0",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "TalkLikeGraph",
            "name_full": "Talk like a Graph: Encoding graphs for large language models",
            "brief_description": "Analyses and proposes text-based encoding strategies for graphs to be processed by LLMs and introduces the GraphQA benchmark to measure encoding strategy effects on performance.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models.",
            "mention_or_use": "mention",
            "representation_name": "text-based graph encoder / serialization strategies",
            "representation_description": "A family of strategies that linearize graphs into textual formats (various encoder functions / templates) designed to be consumed by LLMs; the paper systematically studies different encodings and their effect on downstream QA and reasoning.",
            "graph_type": "general graphs / graph QA benchmarks",
            "representation_properties": "Focus on encoder function design and their effect on faithfulness and model comprehension; examines trade-offs between compactness (token budget) and fidelity to structure.",
            "evaluation_task": "GraphQA (graph question answering) benchmark; graph reasoning tasks",
            "performance_metrics": "Not reported numerically in survey (study focuses on methodological comparison); metric types implied: QA accuracy / reasoning correctness",
            "comparison_to_other_representations": "Compares multiple text-encoding strategies (different encoder functions/templates) and studies their relative effectiveness; highlights that encoding choice materially affects LLM performance, but concrete numeric winners are not provided in the survey.",
            "limitations_or_challenges": "Choosing an encoding that balances token limits and structural faithfulness is challenging; encoder design may not generalize across graph types or scales.",
            "uuid": "e5252.1",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "WalkLM",
            "name_full": "WalkLM",
            "brief_description": "A tuning-required approach that transforms graph entities into textual sequences via random walks, then fine-tunes language models on these sequences to produce graph-aware embeddings.",
            "citation_title": "Walklm: A uniform language model fine-tuning framework for attributed graph embedding.",
            "mention_or_use": "mention",
            "representation_name": "random-walk-based textual sequences",
            "representation_description": "Generate sequences from random walks over the graph (walks produce ordered node sequences), convert node attributes and walk context into textual prompts/sequences, and fine-tune an LM to produce attributed graph embeddings.",
            "graph_type": "attributed graphs (general / citation / social-like graphs)",
            "representation_properties": "Captures local structural connectivity via walk orderings; relatively compact compared to full-graph serializations; preserves neighborhood patterns but may lose global structure unless long or many walks used.",
            "evaluation_task": "Attributed graph embedding tasks (e.g., node classification) after LM fine-tuning",
            "performance_metrics": null,
            "comparison_to_other_representations": "Presented as a language-model fine-tuning alternative to GNN encoders or other serializations; claims improved attributed embeddings via LM fine-tuning on walk-derived sequences (survey does not tabulate numbers).",
            "limitations_or_challenges": "Random-walk sampling introduces stochasticity and potential coverage gaps; choosing walk length/count trades off token budget vs. structural coverage; global structure may be under-represented.",
            "uuid": "e5252.2",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaGA",
            "name_full": "LLaGA: Large Language and Graph Assistant",
            "brief_description": "A tuning-required LLM-only method that restructures graph data using node-level templates into organized token sequences, mapping them into the LM token embedding space for downstream graph tasks.",
            "citation_title": "LLaGA: Large Language and Graph Assistant.",
            "mention_or_use": "mention",
            "representation_name": "node-level template serialization",
            "representation_description": "Use predefined node-level templates to serialize graph nodes (and local structure) into ordered textual tokens; the serialized sequences are then embedded via the LM token embeddings and used in fine-tuning or prompt-based inference.",
            "graph_type": "general graphs (node classification, reasoning tasks)",
            "representation_properties": "Template-based, systematic serialization intended to preserve per-node local context; interpretable and consistent across nodes; can be mapped directly to LM input embeddings.",
            "evaluation_task": "Various graph tasks (generalization, interpretability, node-level tasks)",
            "performance_metrics": null,
            "comparison_to_other_representations": "Contrasted with GNN-as-prefix and random-walk serializations; claimed to improve versatility, generalizability and interpretability but survey does not list numeric comparisons.",
            "limitations_or_challenges": "Quality depends on template design; token budget and ordering still constrain representable structure; may require tuning for different graph types.",
            "uuid": "e5252.3",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT: Graph instruction tuning for large language models",
            "brief_description": "A GNN-as-prefix method that first aligns a graph encoder with natural language (text-graph grounding) and then projects node representations to language token space for LLM instruction tuning, enabling zero-shot graph task completion.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models.",
            "mention_or_use": "mention",
            "representation_name": "projected node-token prefix (graph-encoder → language token projector)",
            "representation_description": "Train/align a graph encoder to natural-language semantics (text-graph grounding) and use a learned projector to map per-node GNN embeddings into fixed-length semantic tokens that act as a prefix/input to an LLM for instruction tuning and inference.",
            "graph_type": "general graphs / text-attributed graphs / heterogeneous graphs",
            "representation_properties": "Structure-aware tokens produced by learned projection; designed to be compact (fixed-length tokens per node) and directly consumable by LLM prefix; aims for high semantic alignment and strong zero-shot transfer.",
            "evaluation_task": "Node-level downstream tasks including node classification and link prediction; zero-shot and multi-task evaluations",
            "performance_metrics": null,
            "comparison_to_other_representations": "Positioned against purely textual serializations and graph-level pooling; claims strong zero-shot transfer and multi-task capability due to learned alignment, but survey provides no numeric figures.",
            "limitations_or_challenges": "Requires training a graph encoder and projector (not purely tuning-free); effectiveness on non-text-attributed graphs and optimal coordination between GNN and LLM training remain open questions.",
            "uuid": "e5252.4",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphTranslator",
            "name_full": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks",
            "brief_description": "A node-level tokenization approach that uses a translator with shared self-attention and cross-attention to map graph-encoded node representations into fixed-length semantic tokens for LLM consumption.",
            "citation_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks.",
            "mention_or_use": "mention",
            "representation_name": "translator-based semantic token mapping",
            "representation_description": "A translator module with shared self-attention aligns target node and instruction and uses cross-attention to map graph-encoder (node) representations into fixed-length semantic tokens that can be used as prompts or prefix tokens for LLMs.",
            "graph_type": "general graphs; open-ended graph tasks",
            "representation_properties": "Produces fixed-length, instruction-aligned tokens preserving local node semantics; designed for open-ended tasks and instruction-following by LLMs; reduces raw-graph verbosity.",
            "evaluation_task": "Open-ended graph tasks and instruction-based graph prediction (e.g., varied graph reasoning tasks)",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared conceptually with direct textual serializations and other projector/prefix approaches; survey reports improved instruction alignment and LLM utilization but no numeric benchmarks are given.",
            "limitations_or_challenges": "Depends on training the translator and alignment mechanism; may be sensitive to instruction design and to distribution shifts across graph domains.",
            "uuid": "e5252.5",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT4Graph",
            "name_full": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "brief_description": "An empirical benchmark and study that converts graphs into textual descriptions to evaluate off-the-shelf LLMs' ability to understand and reason about graph-structured data.",
            "citation_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "mention_or_use": "mention",
            "representation_name": "textual graph descriptions / prompt-based serializations",
            "representation_description": "Convert graph instances into textual descriptions (natural-language prompts) or question-focused textual encodings and feed them to LLMs for empirical evaluation; focuses on zero-shot/few-shot LLM capabilities without additional finetuning.",
            "graph_type": "general graphs (graph-structured data benchmarks)",
            "representation_properties": "Tuning-free textual prompts emphasize simplicity and immediate LLM compatibility; good for probing LLM innate graph reasoning but limited by token constraints and LLM training distribution.",
            "evaluation_task": "Benchmarking LLM understanding: graph reasoning, QA over graphs, empirical evaluation tasks",
            "performance_metrics": "Reported as empirical evaluation results in the original paper (accuracy/QA scores) — survey does not reproduce numerical metrics here",
            "comparison_to_other_representations": "Serves as a baseline to compare tuning-free textual serializations vs tuned or GNN-augmented methods; demonstrates limitations of off-the-shelf LLMs on complex graph tasks.",
            "limitations_or_challenges": "LLMs often struggle with complex structural reasoning from plain text encodings; token limits and hallucination; performance depends heavily on prompt design.",
            "uuid": "e5252.6",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Graph-LLM",
            "name_full": "Graph-LLM",
            "brief_description": "A class of works that study LLMs applied directly to graph learning tasks (e.g., node classification) often via textual encodings or LLM-generated supervision; explores two pipelines: LLMs-as-Enhancers and LLMs-as-Predictors.",
            "citation_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "mention_or_use": "mention",
            "representation_name": "LLM-centric textual pipelines (LLMs-as-Enhancers / LLMs-as-Predictors)",
            "representation_description": "LLMs generate text-based signals (explanations, embeddings, labels) or consume textual serializations of graph substructures; pipelines either use LLMs to enrich node textual profiles (Enhancers) or to directly predict node labels from text (Predictors).",
            "graph_type": "text-attributed graphs, general graphs (node classification, reasoning)",
            "representation_properties": "Leverages LLM common knowledge and semantic summarization; flexible and suitable for low-data regimes; may be decoupled from GNNs (two-stage).",
            "evaluation_task": "Node classification, link prediction, graph reasoning; zero-shot and few-shot transfer tasks",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared qualitatively to GNN-based pipelines and GNN-as-prefix methods; LLM-based pipelines can provide rich semantic signals but often require post-filtering or calibration to match GNN performance.",
            "limitations_or_challenges": "Often two-stage (decoupled) and sensitive to prompt engineering; can be computationally expensive for large graphs; alignment between LLM outputs and GNN training objectives is an open issue.",
            "uuid": "e5252.7",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GraphTMI",
            "name_full": "Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models",
            "brief_description": "An empirical study and multimodal encoding approach (GraphTMI) that compares text, image, and motif encodings for graph understanding with LLMs and introduces benchmarks; finds image modality often better at balancing token limits and preserving essential structure.",
            "citation_title": "Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models.",
            "mention_or_use": "mention",
            "representation_name": "multimodal encodings (text / image / motif serializations)",
            "representation_description": "Explore multiple modalities to encode graphs: textual serialization, motif-based compact descriptions, and image renderings of graphs (or subgraphs) fed into multimodal LLMs; each modality addresses token/expressivity trade-offs differently.",
            "graph_type": "general graphs; evaluated on graph structure analysis tasks",
            "representation_properties": "Image modality shown to be more token-efficient and better at preserving certain structural cues; motif encodings capture recurring structural patterns compactly; text is most interpretable but can hit token limits.",
            "evaluation_task": "Graph structure analysis and graph understanding benchmarks (GraphTMI benchmark introduced)",
            "performance_metrics": "Survey states image modality outperforms text and prior GNNs on the proposed benchmark (numeric values not reproduced in survey)",
            "comparison_to_other_representations": "Direct comparison across text/motif/image encodings; image modality reported best balance between token constraints and information preservation, outperforming text serializations and some GNN baselines on the benchmark.",
            "limitations_or_challenges": "Image-based encodings require multimodal LLMs (vision-capable); potential OCR/interpretation challenges for complex graphs; modality choice may depend on graph size and downstream task.",
            "uuid": "e5252.8",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MuseGraph",
            "name_full": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
            "brief_description": "A tuning-required instruction-tuning framework that converts graphs into compact textual descriptions and uses graph-aware instruction tuning to teach LLMs generic graph mining capabilities.",
            "citation_title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
            "mention_or_use": "mention",
            "representation_name": "compact graph description + instruction templates",
            "representation_description": "Create compact textual descriptions of graphs (concise summaries of structure and attributes) combined with diverse instruction generation to instruction-tune LLMs for multiple graph mining tasks.",
            "graph_type": "attributed graphs / general graph mining tasks",
            "representation_properties": "Compactness-focused serialization to respect token budgets; aims to reduce verbosity while retaining task-relevant structure; instruction diversity helps generalization.",
            "evaluation_task": "Generic graph mining tasks (reasoning, classification, generation) after instruction tuning",
            "performance_metrics": null,
            "comparison_to_other_representations": "Compared conceptually to naive text serializations and to GNN-based approaches; claims general-purpose gains through instruction tuning but survey does not list numeric comparisons.",
            "limitations_or_challenges": "Designing compact yet faithful descriptions is nontrivial; may lose fine-grained structural details necessary for some tasks; instruction tuning can be costly.",
            "uuid": "e5252.9",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Graphs",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graphtext: Graph reasoning in text space.",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models.",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Walklm: A uniform language model fine-tuning framework for attributed graph embedding.",
            "rating": 2,
            "sanitized_title": "walklm_a_uniform_language_model_finetuning_framework_for_attributed_graph_embedding"
        },
        {
            "paper_title": "LLaGA: Large Language and Graph Assistant.",
            "rating": 2,
            "sanitized_title": "llaga_large_language_and_graph_assistant"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models.",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks.",
            "rating": 2,
            "sanitized_title": "graphtranslator_aligning_graph_model_to_large_language_model_for_openended_tasks"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models.",
            "rating": 2,
            "sanitized_title": "which_modality_should_i_usetext_motif_or_image_understanding_graphs_with_large_language_models"
        },
        {
            "paper_title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining.",
            "rating": 2,
            "sanitized_title": "musegraph_graphoriented_instruction_tuning_of_large_language_models_for_generic_graph_mining"
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space.",
            "rating": 1,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        }
    ],
    "cost": 0.01750225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Large Language Models for Graphs
11 Sep 2024</p>
<p>Xubin Ren xubinrencs@gmail.com 
Jiabin Tang jiabintang77@gmail.com 
Dawei Yin yindawei@acm.org 
Nitesh Chawla nchawla@nd.edu 
Chao Huang chaohuang75@gmail.com </p>
<p>University of Hong Kong Hong Kong
China</p>
<p>University of Hong Kong Hong Kong
China</p>
<p>Baidu Inc
BeijingChina</p>
<p>University of Notre Dame Indiana
USA</p>
<p>University of Hong Kong Hong Kong
China</p>
<p>11 pagesBarcelonaSpain</p>
<p>A Survey of Large Language Models for Graphs
11 Sep 20248DA5988C13EF3CB6AF206EC40AF035F310.1145/3637528.3671460arXiv:2405.08011v3[cs.LG]Large Language Models, Graph Learning
Graphs are an essential data structure utilized to represent relationships in real-world scenarios.Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification.Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist.Recently, Large Language Models (LLMs) have gained attention in natural language processing.They excel in language comprehension and summarization.Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks.In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design.We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category.We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas.This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field.We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers.</p>
<p>INTRODUCTION</p>
<p>Graphs, comprising nodes and edges that signify relationships, are essential for illustrating real-world connections across various domains.These include social networks [39,52], molecular graphs [4], recommender systems [23,59], and academic networks [27].This structured data form is integral in mapping complex interconnections relevant to a wide range of applications.</p>
<p>In recent years, Graph Neural Networks (GNNs) [79] have emerged as a powerful tool for a variety of tasks, including node classification [82] and link prediction [89].By passing and aggregating information across nodes and iteratively refining node features through supervised learning, GNNs have achieved remarkable results in capturing structural nuances and enhancing model accuracy.To accomplish this, GNNs leverage graph labels to guide the learning process.Several notable models have been proposed in the literature, each with its own strengths and contributions.For instance, Graph Convolutional Networks (GCNs) [34] have been shown to be effective in propagating embeddings across nodes, while Graph Attention Networks (GATs) [67] leverage attention mechanisms to perform precise aggregation of node features.Additionally, Graph Transformers [35,86] employ self-attention and positional encoding to capture global signals among the graph, further improving the expressiveness of GNNs.To address scalability challenges in large graphs, methods such as Nodeformer [77] and DIFFormer [76] have been proposed.These approaches employ efficient attention mechanisms and differentiable pooling techniques to reduce computational complexity while maintaining high levels of accuracy.Despite these advancements, current GNN methodologies still face several challenges.For example, data sparsity remains a significant issue, particularly in scenarios where the graph structure is incomplete or noisy [85].Moreover, the generalization ability of GNNs to new graphs or unseen nodes remains an open research question, with recent works highlighting the need for more robust and adaptive models [17,80,93].</p>
<p>Large Language Models (LLMs) [96], which show great generalization abilities for unseen tasks [12,56,72], have emerged as powerful tools in various research fields, including natural language processing [1], computer vision [43,44], and information retrieval [26,41,99].The advent of LLMs has sparked significant interest within the graph learning community [29,33,37], prompting investigations into the potential of LLMs to enhance performance on graph-related tasks.Researchers have explored various approaches to leverage the strengths of LLMs for graph learning, resulting in a new wave of methods that combine the power of LLMs with graph neural networks.One promising direction is to develop prompts that enable LLMs to understand graph structures and respond to queries effectively.For instance, approaches such as InstructGLM [84] and NLGraph [68] have designed specialized prompts that allow LLMs to reason over graph data and generate accurate responses.Alternatively, other methods have integrated GNNs to feed tokens into the LLMs, allowing them to understand graph structures more directly.For example, GraphGPT [63] and GraphLLM [5] use GNNs to encode graph data into tokens, which are then fed into the LLMs for further processing.This synergy between LLMs and GNNs has not only improved task performance but also demonstrated impressive zero-shot generalization capabilities, where the models can accurately answer queries about unseen graphs or nodes.</p>
<p>In this survey, we offer a systematic review of the advancements in Large Language Models (LLMs) for graph applications, and we explore potential avenues for future research.Unlike prior surveys that categorize studies based on the role of LLMs [33,37] or focus primarily on integrating LLMs with knowledge graphs [53], our work highlights the model framework design, particularly the inference and training processes, to distinguish between existing taxonomies.This perspective allows readers to gain a deeper understanding of how LLMs effectively address graph-related challenges.We identify and discuss four distinct architectural approaches: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, each illustrated with representative examples.In summary, the contributions of our work can be summarized as:</p>
<p>• Comprehensive Review of LLMs for Graph Learning.We offer a comprehensive review of the current state-of-the-art Large Language Models (LLMs) for graph learning, elucidating their strengths and pinpointing their limitations.</p>
<p>PRELIMINARIES AND TAXONOMY</p>
<p>In this section, we first provide essential background knowledge on large language models and graph learning.Then, we present our taxonomy of large language models for graphs.</p>
<p>Definitions</p>
<p>Graph-Structured Data.In computer science, a graph G = (V, E) is a non-linear data structure that consists of a set of nodes V, and a set of edges E connecting these nodes.Each edge  ∈ E is associated with a pair of nodes (, ), where  and  are the endpoints of the edge.The edge may be directed, meaning it has a orientation from  to , or undirected, meaning it has no orientation.Furthermore, A Text-Attributed Graph (TAG) is a graph that assigns a sequential text feature (i.e., sentence) to each node, denoted as t  , which is widely used in the era of large language models.The text-attributed graph can be formally represented as G  = (V, E, T ), where T is the set of text features.</p>
<p>Graph Neural Networks (GNNs) are deep learning architectures for graph-structured data that aggregate information from neighboring nodes to update node embeddings.Formally, the update of a node embedding h  ∈ R  in each GNN layer can be represented as:
h (𝑙+1) 𝑣 = 𝜓 (𝜙 ({h (𝑙 ) 𝑣 ′ : 𝑣 ′ ∈ N (𝑣)}), h (𝑙 ) 𝑣 ),(1)
where  ′ ∈ N () denotes a neighbor node of , and  (•) and  (•) are aggregation and update functions, respectively.By stacking  GNN layers, the final node embeddings can be used for downstream graph-related tasks such as node classification and link prediction.</p>
<p>Large Language Models (LLMs).Language Models (LMs) is a statistical model that estimate the probability distribution of words for a given sentence.Recent research has shown that LMs with billions of parameters exhibit superior performance in solving a wide range of natural language tasks (e.g., translation, summarization and instruction following), making them Large Language Models (LLMs).In general, most recent LLMs are built with transformer blocks that use a query-key-value (QKV)-based attention mechanism to aggregate information in the sequence of tokens.Based on the direction of attention, LLMs can be categorized into two types (given a sequence of tokens x = [ 0 ,  1 , ...,   ]):</p>
<p>• Masked Language Modeling (MLM).Masked Language Modeling is a popular pre-training objective for LLMs that involves masking out certain tokens in a sequence and training the model to predict the masked tokens based on the surrounding context.Specifically, the model takes into account both the left and right context of the masked token to make accurate predictions:
𝑝 (𝑥 𝑖 |𝑥 0 , 𝑥 1 , ..., 𝑥 𝑛 ).(2)
Representative models include BERT [12] and RoBERTa [47].• Causal Language Modeling (CLM).Causal Language Modeling is another popular training objective for LLMs that involves predicting the next token in a sequence based on the previous tokens.Specifically, the model only considers the left context of the current token to make accurate predictions:
𝑝 (𝑥 𝑖 |𝑥 0 , 𝑥 1 , ..., 𝑥 𝑖 −1 )(3)
Notable examples include the GPT (e.g., ChatGPT) and Llama [66].</p>
<p>Taxonomy</p>
<p>In this survey, we present our taxonomy focusing on the model inference pipeline that processes both graph data and text with LLMs.Specifically, we summarize four main types of model architecture design for large language models for graphs, as follows:</p>
<p>Large Language Models for Graphs</p>
<p>GNNs as Prefix</p>
<p>Node-level Tokenization GprahGPT [63], HiGPT [64], GraphTranslator [88], UniGraph [25], GIMLET [92], XRec [51] Graph-level Tokenization GraphLLM [5], GIT-Mol [45], MolCA [48], InstructMol [4], G-Retriever [24], GNP [65] LLMs as Prefix Embs.from LLMs for GNNs G-Prompt [30], SimTeG [14], GALM [81], OFA [42], TAPE [22], LLMRec [73] Labels from LLMs for GNNs OpenGraph [80], LLM-GNN [9], GraphEdit [21], RLMRec [58] LLMs-Graphs Integration Alignment between GNNs and LLMs MoMu [60], ConGraT [3], G2P2 [74], GRENADE [36], MoleculeSTM [46], THLM [100], GLEM [94] Fusion Training of GNNs and LLMs GreaseLM [90], DGTL [54], ENGINE [98], GraphAdapter [31] LLMs Agent for Graphs Pangu [19], Graph Agent [71], FUXI [18], Readi [10], RoG [49] LLMs-Only Tuning-free NLGraph [68], GPT4Graph [20], Beyond Text [28], Graph-LLM [8], GraphText [95], Talk like a Graph [15], LLM4DyG [91], GraphTMI [11], Ai et al. [2] Tuning-required InstructGLM [84], WalkLM [62], LLaGA [7], InstructGraph [69], ZeroG [38], GraphWiz [6], GraphInstruct [50], MuseGraph [61] Figure 1: The proposed taxonomy of Large Language Models (LLMs) for graphs, featuring representative works.</p>
<p>• GNNs as Prefix.GNNs serve as the first component to process graph data and provide structure-aware tokens (e.g., node-level, edge-level, or graph-level tokens) for LLMs for inference.• LLMs as Prefix.LLMs first process graph data with textual information and then provide node embeddings or generated labels for improved training of graph neural networks.• LLMs-Graphs Integration.In this line, LLMs achieve a higher level of integration with graph data, such as fusion training or alignment with GNNs, and also build LLM-based agents to interact with graph information.• LLMs-Only.This line designs practical prompting methods to ground graph-structured data into sequences of words for LLMs to infer, while some also incorporate multi-modal tokens.</p>
<p>LARGE LANGUAGE MODELS FOR GRAPHS 3.1 GNNs as Prefix</p>
<p>In this section, we discuss the application of graph neural networks (GNNs) as structural encoders to enhance the understanding of graph structures by LLMs, thereby benefiting various downstream tasks, i.e., GNNs as Prefix.In these methods, GNNs generally play the role of a tokenizer, encoding graph data into a graph token sequence rich in structural information, which is then input into LLMs to align with natural language.These methods can generally be divided into two categories: i) Node-level Tokenization: each node of the graph structure is input into the LLM, aiming to make the LLM understand fine-grained node-level structural information and distinguish relationships.ii) Graph-level Tokenization: the graph is compressed into a fixed-length token sequence using a specific pooling method, aiming to capture high-level global semantic information of the graph structure.</p>
<p>3.1.1Node-level Tokenization.For some downstream tasks in graph learning, such as node classification and link prediction, the model needs to model the fine-grained structural information at node level, and distinguish the semantic differences between different nodes.Traditional GNNs usually encode a unique representation for each node based on the information of neighboring nodes, and directly perform downstream node classification or link prediction.In this line, the node-level tokenization method is utilized, which can retain the unique structural representation of each node as much as possible, thereby benefiting downstream tasks.Within this line, GraphGPT [63] proposes to initially align the graph encoder with natural language semantics through textgraph grounding, and then combine the trained graph encoder with the LLM using a projector.Through the two-stage instruction tuning paradigm, the model can directly complete various graph learning downstream tasks with natural language, thus perform strong zero-shot transferability and multi-task compatibility.The proposed Chain-of-Thought distillation method empowers GraphGPT to migrate to complex tasks with small parameter sizes.Then, HiGPT [64] proposes to combine the language-enhanced in-context heterogeneous graph tokenizer with LLMs, solving the challenge of relation type heterogeneity shift between different heterogeneous graphs.Meanwhile, the two-stage heterogeneous graph instruction-tuning injects both homogeneity and heterogeneity awareness into the LLM.And the Mixture-of-Thought (MoT) method combined with various prompt engineering further solves the common data scarcity problem in heterogeneous graph learning.GIMLET [92], as a unified graph-text model, leverages natural language instructions to address the label insufficiency challenge in molecule-related tasks, effectively alleviating the reliance on expensive lab experiments for data annotation.It employs a generalized position embedding and attention mechanism to encode both graph structures and textual instructions as a unified token combination that is fed into a transformer decoder.GraphTranslator [88] proposes the use of a translator with shared self-attention to align both the target node and instruction, and employs cross attention to map the node representation encoded by the graph model to fixed-length semantic tokens.The proposed daul-phase training paradigm empowers the LLM to make predictions based on language instructions, providing a unified solution for both pre-defined and open-ended graph-based tasks.Instead of using pre-computed node features of varying dimensions, UniGraph [25] leverages Text-Attributed Graphs for unifying node representations, featuring a cascaded architecture of language models and graph</p>
<p>GNNs Graphs</p>
<p>What is this node?neural networks as backbone networks.In recent research on recommendation systems, XRec [51] has been proposed as a method that utilizes the encoded user/item embeddings from graph neural networks as collaborative signals.These signals are then integrated into each layer of large language models, enabling the generation of explanations for recommendations, even in zero-shot scenarios.</p>
<p>3.1.2</p>
<p>Graph-level Tokenization.On the other hand, to adapt to other graph-level tasks, models need to be able to extract global information from node representations, to obtain high-level graph semantic tokens.In the method of GNN as Prefix, Graph-level tokenization abstracts node representations into unified graph representations through various "pooling" operations, further enhancing various downstream tasks.</p>
<p>Within this domain, GraphLLM [5] utilizes a graph transformer that incorporates the learnable query and positional encoding to encode the graph structure and obtain graph representations through pooling.These representations are directly used as graph-enhanced prefix for prefix tuning in the LLM, demonstrating remarkable effectiveness in fundamental graph reasoning tasks.MolCA [48] with Cross-Modal Projector and Uni-Modal Adapter is a method that enables a language model to understand both text-and graph-based molecular contents through the proposed dual-stage pre-training and fine-tuning stage.It employs a cross-modal projector implemented as a Q-Former to connect a graph encoder's representation space and a language model's text space, and a uni-modal adapter for efficient adaptation to downstream tasks.InstructMol [4] introduces a projector that aligns the molecular graph encoded by the graph encoder with the molecule's Sequential information and natural language instructions, with the first stage of Alignment Pretraining and the second stage of Task-specific Instruction Tuning enabling the model to achieve excellent performance in various drug discovery-related molecular tasks.GIT-Mol [45] further unifies the graph, text, and image modalities through interaction cross-attention between different modality encoders, and aligns these three modalities, enabling the model to simultaneously perform four downstream tasks: captioning, generation, recognition, and prediction.GNP [65] employs cross-modality pooling to integrate the node representations encoded by the graph encoder with the natural language tokens, resulting in a unified graph representation.This representation is aligned with the instruction through the LLM to demonstrate superiority in commonsense and biomedical reasoning tasks.Recently, G-Retriever [24] utilizes retrieval-augmented techniques to obtain subgraph structures.It completes various downstream tasks in GraphQA (Graph Question Answering) through the collaboration of graph encoder and LLMs.</p>
<p>3.1.3Discussion.The GNN as Prefix approach aligns the modeling capability of GNNs with the semantic modeling capability of LLMs, demonstrating unprecedented generalization, i.e., zeroshot capability, in various graph learning downstream tasks and real-world applications.However, despite the effectiveness of the aforementioned approach, the challenge lies in whether the GNN as Prefix method remains effective for non-text-attributed graphs.Additionally, the optimal coordination between the architecture and training of GNNs and LLMs remains an unresolved question.</p>
<p>LLMs as Prefix</p>
<p>The methods presented in this section leverage the information produced by large language models to improve the training of graph neural networks.This information includes textual content, labels, or embeddings derived from the large language models.These techniques can be categorized into two distinct groups: i) Embeddings from LLMs for GNNs, which involves using embeddings generated by large language models for graph neural networks, and ii) Labels from LLMs for GNNs, which involves integrating labels generated by large language models for graph neural networks.</p>
<p>3.2.1</p>
<p>Embeddings from LLMs for GNNs.The inference process of graph neural networks involves passing node embeddings through the edges and then aggregating them to obtain the nextlayer node embeddings.In this process, the initial node embeddings are diverse across different domains.For instance, ID-based embeddings in recommendation systems or bag-of-words embeddings in citation networks can be unclear and non-diverse.Sometimes, the poor quality of initial node embeddings can result in suboptimal performance of GNNs.Furthermore, the lack of a universal design for node embedders makes it challenging to address the generalization capability of GNNs in unseen tasks with different node sets.Fortunately, the works in this line leverage the powerful language summarization and modeling capabilities of LLMs to generate meaningful and effective embeddings for GNNs' training.</p>
<p>In this domain, G-Prompt [30] adds a GNN layer at the end of a pre-trained language models (PLMs) to achieve graph-aware fillmasking self-supervised learning.By doing so, G-Prompt can generate task-specific, explainable node embeddings for downstream tasks using prompt tuning.SimTeG [14] first leverages parameterefficient fine-tuning on the text embeddings obtained by LLMs for downstream tasks (e.g., node classification).Then, the node embeddings are fed into GNNs for inference.Similarly, GALM [81] utilizes BERT as a pre-language model to encode text embeddings for each node.Then, the model is pre-trained through unsupervised learning tasks, such as link prediction, to minimize empirical loss and find optimal model parameters, which enables GALM to be applied for various downstream tasks.Recently, OFA [42] leverages LLMs to unify graph data from different domains into a common embedding space for cross-domain learning.It also uses LLMs to encode taskrelevant text descriptions for constructing prompt graphs, allowing the model to perform specific tasks based on context.TAPE [22] uses customized prompts to query LLMs, generating both prediction and text explanation for each node.Then, DeBERTa is fine-tuned to convert the text explanations into node embeddings for GNNs.Finally, GNNs can use a combination of the original text features, explanation features, and prediction features to predict node labels.In the field of recommendation, LLMRec [73] achieves graph augmentation on user-item interaction data using GPT-3.5, which not only filters out noise interactions and adds meaningful training data, but also enriches the initial node embeddings for users and items with generated rich textual profiles, ultimately improving the performance of recommenders.</p>
<p>3.2.2Labels from LLMs for GNNs.Another approach leverages the generated labels from large language models as supervision to improve the training of graph neural networks.Notably, the supervised labels in this context are not limited to categorized labels in classification tasks, but can take various forms such as embeddings, graphs, and more.The generated information from the LLMs is not used as input to the GNNs, but rather forms the supervision signals for better optimization, which enables GNNs to achieve higher performance on various graph-related tasks.Follow this line, OpenGraph [80] employs LLMs to generate nodes and edges, mitigating the issue of sparse training data.The generation process for nodes and edges is refined using the Gibbs sampling algorithm and a tree-of-prompt strategy, which is then utilized to train the graph foundation model.LLM-GNN [9] leverages LLMs as annotators to generate node category predictions with confidence scores, which serve as labels.Post-filtering is then employed to filter out low-quality annotations while maintaining label diversity.Finally, the generated labels are used to train GNNs.GraphEdit [21] leverages the LLMs to build an edge predictor, which is used to evaluate and refine candidate edges against the original graph's edges.In recommender systems, RLMRec [58] leverages LLMs to generate text descriptions of user/item preferences.These descriptions are then encoded as semantic embeddings to guide the representation learning of ID-based recommenders using contrastive and generative learning techniques [57].</p>
<p>3.2.3</p>
<p>Discussion.Despite the progress made by the aforementioned methods in enhancing graph learning performance, a limitation persists in their decoupled nature, where LLMs are not co-trained with GNNs, resulting in a two-stage learning process.This decoupling is often due to computational resource limitations arising from the large size of the graph or the extensive parameters of LLMs.Consequently, the performance of the GNNs is heavily dependent on the pre-generated embeddings/labels of LLMs or even the design of task-specific prompts.</p>
<p>LLMs-Graphs Integration</p>
<p>The methods introduced in this section aim to further integrate large language models with graph data, encompassing various methodologies that enhance not only the ability of LLMs to tackle graph tasks but also the parameter learning of GNNs.These works can be categorized into three types: i) Fusion Training of GNNs and LLMs, which aims to achieve fusion-co-training of the parameters of both models; ii) Alignment between GNNs and LLMs, which focuses on achieving representation or task alignment between the two models; and iii) LLMs Agent for Graphs, which builds an autonomous agent based on LLMs to plan and solve graph tasks.</p>
<p>Alignment between GNNs and LLMs.</p>
<p>In general, GNNs and LLMs are designed to handle different modalities of data, with GNNs focusing on structural data and LLMs focusing on textual data.This results in different feature spaces for the two models.To address this issue and make both modalities of data more beneficial for the learning of both GNNs and LLMs, several methods use techniques such as contrastive learning or Expectation-Maximization (EM) iterative training to align the feature spaces of the two models.This enables better modeling of both graph and text information, resulting in improved performance on various tasks.</p>
<p>Within this topic, MoMu [60] is a multimodal molecular foundation model that includes two separate encoders, one for handling molecular graphs (GIN) and another for handling text data (BERT).It uses contrastive learning to pre-train the model on a dataset of molecular graph-text pairs.This approach enables MoMu to directly imagine new molecules from textual descriptions.Also in the bioinfo domain, MoleculeSTM [46] combines the chemical structure information of molecules (i.e., molecular graph) with their textual descriptions (i.e., SMILES strings), and uses a contrastive learning to jointly learn the molecular structure and textual descriptions.It show great performance on multiple benchmark tests, including structure-text retrieval, text-based editing tasks, and molecular property prediction.Similarly, in ConGraT  through a variational expectation-maximization (EM) framework.By iteratively using LMs and GNNs to provide labels for each other in node classification, GLEM aligns their capabilities in graph tasks.</p>
<p>Fusion Training of GNNs and LLMs.</p>
<p>Although alignment between the representations of GNNs and LLMs achieves co-optimization and embedding-level alignment of the two models, they remain separate during inference.To achieve a higher level of integration between LLMs and GNNs, several works have focused on designing a deeper fusion of the architecture of the modules, such as transformer layers in LLMs and graph neural layers in GNNs.Co-training GNNs and LLMs can result in a win-win bi-directional benefit for both modules in graph tasks.</p>
<p>Along this line, GreaseLM [90] integrates transformer layers and GNN layers by designing a specific forward propagation layer that enables bidirectional information passing between LM and GNN through special interaction markers and interaction nodes.This approach allows language context representations to be grounded in structured world knowledge, while subtle linguistic differences (such as negation or modifiers) can affect the representation of the knowledge graph, which enables GreaseLM to achieve high performance on Question-Answering tasks.DGTL [54] proposes disentangled graph learning to leverage GNNs to encode disentangled representations, which are then injected into each transformer layer of the LLMs.This approach enables the LLMs to be aware of the graph structure and leverage the gradient from the LLMs to finetune the GNNs.By doing so, DGTL achieves high performance on both citation network and e-commerce graph tasks.ENGINE [98] adds a lightweight and tunable G-Ladder module to each layer of the LLM, which uses a message-passing mechanism to integrate structural information.This enables the output of each LLM layer (i.e., token-level representations) to be passed to the corresponding G-Ladder, where the node representations are enhanced and then used for downstream tasks such as node classification.More directly, GraphAdapter [31] uses a fusion module (typically a multi-layer perceptrons) to combine the structural representations obtained from GNNs with the contextual hidden states of LLMs (e.g., the encoded node text).This enables the structural information from the GNN adapter to complement the textual information from the LLMs, resulting in a fused representation that can be used for supervision training and prompting for downstream tasks.</p>
<p>LLMs Agent for Graphs. With the powerful capabilities of</p>
<p>LLMs in understanding instructions and self-planning to solve tasks, an emerging research direction is to build autonomous agents based on LLMs to tackle human-given or research-related tasks.Typically, an agent consists of a memory module, a perception module, and an action module to enable a loop of observation, memory recall, and action for solving given tasks.In the graph domain, LLMs-based agents can interact directly with graph data to perform tasks such as node classification and link prediction.</p>
<p>In this field, Pangu [19] pioneered the use of LMs to navigate KGs.In this approach, the agent is designed as a symbolic graph search algorithm, providing a set of potential search paths for the language models to evaluate in response to a given query.The remaining path is then utilized to retrieve the answer.Graph Agent (GA) [71] converts graph data into textual descriptions and generates embedding vectors, which are stored in long-term memory.During inference, GA retrieves similar samples from long-term memory and integrates them into a structured prompt, which is used by LLMs to explain the potential reasons for node classification or edge connection.FUXI [18] framework integrates customized tools and the ReAct [83] algorithm to enable LLMs to act as agents that can proactively interact with KGs.By leveraging tool-based navigation and exploration of data, these agents perform chained reasoning to progressively build answers and ultimately solve complex queries efficiently and accurately.Readi [10] is another approach that first uses in-context learning and chain-of-thought prompts to generate reasoning paths with multiple constraints, which are then instantiated based on the graph data.The instantiated reasoning paths are merged and used as input to LLMs to generate an answer.This method has achieved significant performance improvements on KGQA (knowledge graph question answering) and TableQA (table question answering) tasks.Recently, RoG [49] is proposed to answer graph-retaled question in three steps: planning, retrieval, and reasoning.In the planning step, it generates a set of associated paths based on the structured information of the knowledge graph according to the problem.In the retrieval step, it uses the associated paths generated in the planning stage to retrieve the corresponding reasoning paths from the KG.Finally, it uses the retrieved reasoning paths to generate the answer and explanation for the problem using LLMs.</p>
<p>3.3.4</p>
<p>Discussion.The integration of LLMs and graphs has shown promising progress in minimizing the modality gap between structured data and textual data for solving graph-related tasks.By combining the strengths of LLMs in language understanding and the ability of graphs to capture complex relationships between entities, we can enable more accurate and flexible reasoning over graph data.However, despite the promising progress, there is still room for improvement in this area.One of the main challenges in integrating LLMs and graphs is scalability.In alignment and fusion training, current methods often use small language models or fix the parameters of LLMs, which limits their ability to scale to larger graph datasets.Therefore, it is crucial to explore methods for scaling model training with larger models on web-scale graph data, which can enable more accurate and efficient reasoning over large-scale graphs.Another challenge in this area is the limited interaction between graph agents and graph data.Current methods for graph agents often plan and execute only once, which may not be optimal for complex tasks requiring multiple runs.Therefore, it is necessary to investigate methods for agents to interact graph data multiple times, refining their plans and improving their performance based on feedback from the graph.This can enable more sophisticated reasoning over graph data and improve the accuracy of downstream tasks.Overall, the integration of LLMs and graphs is a promising research direction with significant potential for advancing the state-of-the-art in graph learning.By addressing the aforementioned challenges and developing more advanced methods for integrating LLMs and graphs, we can enable more accurate and flexible reasoning over graph data and unlock new applications in areas such as knowledge graph reasoning, molecular modeling, and social network analysis.</p>
<p>LLMs-Only</p>
<p>In this section, we will elaborate in detail on the direct application of LLMs for various graph-oriented tasks, namely the LLMs-Only category.These methods aim to allow LLMs to directly accept graph structure information, understand it, and perform inference for various downstream tasks in combination with this information.These methods can mainly be divided into two broad categories: i) Tuning-free methods aim to design prompts that LLMs can understand to express graphs, directly prompting pre-trained LLMs to perform graph-oriented tasks; ii) Tuning-required approaches focus on converting graphs into sequences in a specific way and aligning graph token sequences and natural language token sequences using fine-tuning methods.</p>
<p>3.4.1 Tuning-free.Given the unique structured characteristics of graph data, two critical challenges arise: effectively constructing a graph in natural language format and determining whether Large Language Models (LLMs) can accurately comprehend graph structures as represented linguistically.To address these issues, tuning-free approaches are being developed to model and infer graphs solely within the text space, thereby exploring the potential of pre-trained LLMs for enhanced structural understanding.</p>
<p>NLGraph [68], GPT4Graph [20] and Beyond Text [28] collectively examine the capabilities of LLMs in understanding and reasoning with graph data.NLGraph proposes a benchmark for graph-based problem solving and introduces instruction-based approaches, while GPT4Graph and Beyond Text investigate the proficiency of LLMs in comprehending graph structures and emphasizes the need for advancements in their graph processing capabilities.And Graph-LLM [8] explores the potential of LLMs in graph machine learning, focusing on the node classification task.Two pipelines, LLMs-as-Enhancers and LLMs-as-Predictors, are investigated to leverage LLMs' extensive common knowledge and semantic comprehension abilities.Through comprehensive studies, it provides original observations and insights that open new possibilities for utilizing LLMs in learning on graphs.Meanwhile, GraphText [95] translates graphs into natural language by deriving a graph-syntax tree and processing it with an LLM.It offers training-free graph reasoning and enables interactive graph reasoning, showcasing the unexplored potential of LLMs.Talk like a Graph [15] conducts an in-depth examination of text-based graph encoder functions for LLMs, evaluating their efficacy in transforming graph data into textual format to enhance LLMs' capabilities in executing graph reasoning tasks, and proposes the GraphQA benchmark to systematically measure the influence of encoding strategies on model performance.And LLM4DyG [91] benchmarks the spatial-temporal comprehension of LLMs on dynamic graphs, introducing tasks that evaluate both temporal and spatial understanding, and suggests the Disentangled Spatial-Temporal Thoughts (DST2) prompting technique for improved performance.To facilitate the integration of multimodality, GraphTMI [11] presents an innovative approach to integrating graph data with LLMs, introducing diverse modalities such as text, image, and motif encoding to enhance LLMs' efficiency in processing complex graph structures, and proposes the GraphTMI benchmark for evaluating LLMs in graph structure analysis, revealing that the image modality outperforms text and prior GNNs in balancing token limits and preserving essential information.Ai et al. [2] introduces a multimodal framework for graph understanding and reasoning, utilizing image encoding and GPT-4V's advanced capabilities to interpret and process diverse graph data, while identifying challenges in Chinese OCR and complex graph types, suggesting directions for future enhancements in AI's multimodal interaction and graph data processing.</p>
<p>3.4.2</p>
<p>Tuning-required.Due to the limitations of expressing graph structural information using pure text, the recent mainstream approach is to align graphs as node token sequences with natural language token sequences when inputting them to LLMs.In contrast to the aforementioned GNN as Prefix approach, the Tuning-required LLM-only approach discards the graph encoder and adopts a specific arrangement of graph token sequences, along with carefully designed embeddings of graph tokens in prompts, achieving promising performances in various downstream graph-related tasks.</p>
<p>InstructGLM [84] introduces an innovative framework for graph representation learning that combines natural language instructions with graph embeddings to fine-tune LLMs.This approach allows LLMs to effectively process graph structures without relying on specialized GNN architectures.WalkLM [62] integrates language models with random walks to create unsupervised attributed graph embeddings, focusing on the technical innovation of transforming graph entities into textual sequences and utilizing graph-aware fine-tuning.This technique captures both attribute semantics and graph structures.Recently, LLaGA [7] has utilized node-level templates to restructure graph data into organized sequences, which are then mapped into the token embedding space.This allows Large Language Models to process graph-structured data with enhanced versatility, generalizability, and interpretability.InstructGraph [69] proposes a methodological approach to improve LLMs for graph reasoning and generation through structured format verbalization, graph instruction tuning, and preference alignment.This aims to bridge the semantic gap between graph data and textual language models, and to mitigate the issue of hallucination in LLM outputs.</p>
<p>ZeroG [38] then leverages a language model to encode node attributes and class semantics, employing prompt-based subgraph and lightweight fine-tuning strategies address crossdataset zero-shot transferability challenges in graph learning.Furthermore, GraphWiz [6] utilizes GraphInstruct, an instructiontuning dataset, to augment language models for addressing various graph problems, employing Direct Preference Optimization (DPO) [55] to enhance the clarity and accuracy of reasoning processes.GraphInstruct [50] presents a comprehensive benchmark of 21 graph reasoning tasks, incorporating diverse graph generation methods and detailed reasoning steps to enhance LLMs with improved graph understanding and reasoning capabilities.And, MuseGraph [61] fuses the capabilities of LLMs with graph mining tasks through a compact graph description mechanism, diverse instruction generation, and graph-aware instruction tuning, enabling a generic approach for analyzing and processing attributed graphs.</p>
<p>3.4.3</p>
<p>Discussion.The LLMs-Only approach is an emerging research direction that explores the potential of pre-training Large Language Models specifically for interpreting graph data and merging graphs with natural language instructions.The main idea behind this approach is to leverage the powerful language understanding capabilities of LLMs to reason over graph data and generate accurate responses to queries.However, effectively transforming large-scale graphs into text prompts and reordering graph token sequences to preserve structural integrity without a graph encoder present significant ongoing challenges.These challenges arise due to the complex nature of graph data, which often contains intricate relationships between nodes and edges, as well as the limited ability of LLMs to capture such relationships without explicit guidance.As such, further research is needed to develop more advanced methods for integrating LLMs with graph data and overcoming the aforementioned challenges.</p>
<p>FUTURE DIRECTIONS</p>
<p>In this section, we explore several open problems and potential future directions in the field of large language models for graphs.</p>
<p>LLMs for Multi-modal Graphs</p>
<p>Recent studies have demonstrated the remarkable ability of large language models to process and understand multi-modal data [78], such as images [44] and videos [87].This capability has opened up new avenues for integrating LLMs with multi-modal graph data, where nodes may contain features from multiple modalities [40].By developing multi-modal LLMs that can process such graph data, we can enable more accurate and comprehensive reasoning over graph structures, taking into account not only textual information but also visual, auditory, and other types of data.</p>
<p>Efficiency and Less Computational Cost</p>
<p>In the current landscape, the substantial computational expenses associated with both the training and inference phases of LLMs pose a significant limitation [13,16], impeding their capacity to process large-scale graphs that encompass millions of nodes.This challenge is further compounded when attempting to integrate LLMs with GNNs, as the fusion of these two powerful models becomes increasingly arduous due to the aforementioned computational constraints [94].Consequently, the necessity to discover and implement efficient strategies for training LLMs and GNNs with reduced computational costs becomes paramount.This is not only to alleviate the current limitations but also to pave the way for the enhanced application of LLMs in graph-related tasks, thereby broadening their utility and impact in the field of data science.</p>
<p>Tackling Different Graph Tasks</p>
<p>The prevailing methodologies LLMs have primarily centered their attention on conventional graph-related tasks, such as link prediction and node classification.However, considering the remarkable capabilities of LLMs, it is both logical and promising to delve into their potential in tackling more complex and generative tasks, including but not limited to graph generation [97], graph understanding, and graph-based question answering [32].By expanding the horizons of LLM-based approaches to encompass these intricate tasks, we can unlock a myriad of new opportunities for their application across diverse domains.For instance, in the realm of drug discovery, LLMs could facilitate the generation of novel molecular structures; in social network analysis, they could provide deeper insights into intricate relationship patterns; and in knowledge graph construction, they could contribute to the creation of more comprehensive and contextually accurate knowledge bases.</p>
<p>User-Centric Agents on Graphs</p>
<p>The majority of contemporary LLM-based agents, specifically designed to address graph-related tasks, are predominantly tailored for single graph tasks.These agents typically adhere to a one-time-run procedure, aiming to resolve the provided question in a single attempt.Consequently, these agents are neither equipped to function as multi-run interactive agents, capable of adjusting their generated plans based on feedback or additional information, nor are they designed to be user-friendly agents that can effectively manage a wide array of user-given questions.An LLM-based agent [70] that embodies the ideal qualities should not only be user-friendly but also possess the capability to dynamically search for answers within graph data in response to a diverse range of open-ended questions posed by users.This would necessitate the development of an agent that is both adaptable and robust, able to engage in iterative interactions with users and adept at navigating the complexities of graph data to provide accurate and relevant answers.</p>
<p>CONCLUSION</p>
<p>In this comprehensive survey, we delve into the current state of large language models specifically tailored for graph data, proposing an innovative taxonomy grounded in the distinctive designs of their inference frameworks.We meticulously categorize these models into four unique framework designs, each characterized by its own set of advantages and limitations.Additionally, we provide a detailed discussion on these characteristics, enriching our analysis with insights into potential challenges and opportunities within this field.Our survey not only serves as a critical resource for researchers keen on exploring and leveraging large language models for graph-related tasks but also aims to inspire and guide future research endeavors in this evolving domain.Through this work, we hope to foster a deeper understanding and stimulate further innovation in the integration of LLMs with graphs.</p>
<p>Figure 2 :
2
Figure 2: GNNs as Prefix.</p>
<p>Figure 3 :
3
Figure 3: LLMs as Prefix.</p>
<p>[3], a contrastive graph-text pretraining technique is proposed to align the node embeddings encoded by LMs and GNNs simultaneously.The experiments are conducted on social networks, citation networks, and link networks, and show great performance on node and text classification as well as link prediction tasks.Furthermore, G2P2[74,75] enhances graph-grounded contrastive pre-training by proposing three different types of alignment: text-node, textsummary, and node-summary alignment.This enables G2P2 to leverage the rich semantic relationships in the graph structure to improve text classification performance in low-resource environments.GRENADE[36] is a graph-centric language model that proposes graph-centric contrastive learning and knowledge alignment to achieve both node-level and neighborhood-level alignment based on the node embeddings encoded from GNNs and LMs.This enables the model to capture text semantics and graph structure information through self-supervised learning, even in the absence of human-annotated labels.In addition to contrastive learning, THLM[100] leverages BERT and HGNNs to encode node embeddings and uses a positive-negative classification task with negative sampling to improve the alignment of embeddings from two different modalities.Recently, GLEM[94] adopts an efficient and effective solution that integrates graph structure and language learning</p>
<p>Figure 4 :
4
Figure 4: LLMs-Graphs Integration.</p>
<p>Figure 5:</p>
<p>GNNs asGraphGPT[63]Node-level Tokenization General Graph SIGIR 2024 HiGPT[64]Tokenization Heterogeneous Graph KDD 2024 GraphTranslator[88]Node-level Tokenization General Graph WWW 2024 UniGraph[25]Node-level Tokenization General Graph arXiv 2024 GIMLET[92]Node-level Tokenization Bioinformatics NeurIPS 2024 XRec[51]Node-level Tokenization Recommendation arXiv 2024 GraphLLM[5]Graph-level Tokenization Graph Reasoning arXiv 2023 GIT-Mol[45]Graph-level Tokenization Bioinformatics Comput Biol Med 2024 MolCA[48]Graph-level Tokenization Bioinformatics EMNLP 2023 InstructMol[4]Graph-level Tokenization Bioinformatics arXiv 2023 G-Retriever[24]Graph-level Tokenization Graph-based QA arXiv 2024 GNP[65]Graph-level Tokenization Graph-based QA AAAI 2024LLMs as Prefix G-Prompt[30]Embs.from LLMs for GNNs General Graph arXiv 2023 SimTeG[14]Embs.from LLMs for GNNs General Graph arXiv 2023 GALM[81]Embs.from LLMs for GNNs General Graph KDD 2023 OFA[42]Embs.from LLMs for GNNs General Graph ICLR 2024 TAPE[22]Embs.from LLMs for GNNs General Graph ICLR 2024 LLMRec[73]Embs.from LLMs for GNNs Recommendation WSDM 2024 OpenGraph[80]Labels from LLMs for GNNs General Graph arXiv 2024 LLM-GNN[9]Labels from LLMs for GNNs General Graph ICLR 2024 GraphEdit[21]Labels from LLMs for GNNs General Graph arXiv 2023 RLMRec[58]Labels from LLMs for GNNs Recommendation WWW 2024LLMs-Graphs InteractionMoMu[63]Alignment between GNNs and LLMs Bioinformatics arXiv 2022 ConGraT[64]Alignment between GNNs and LLMs General Graph arXiv 2023 G2P2[88]Alignment between GNNs and LLMs General Graph SIGIR 2023 GRENADE[25]Alignment between GNNs and LLMs General Graph EMNLP 2023 MoleculeSTM[92]Alignment between GNNs and LLMs Bioinformatics Nature MI 2023 THLM[51]Alignment between GNNs and LLMs Heterogeneous Graph EMNLP 2023 GLEM[5]Alignment between GNNs and LLMs General Graph ICLR 2023 GreaseLM[90]Fusion Training of GNNs and LLMs Graph-based QA ICLR 2022 DGTL[54]Fusion Training of GNNs and LLMs General Graph arXiv 2023 ENGINE[98]Fusion Training of GNNs and LLMs General Graph arXiv 2024 GraphAdapter[31]Fusion Training of GNNs and LLMs General Graph WWW 2024 Pangu[19]LLMs Agent for Graphs Graph-based QA ACL 2023 Graph Agent[71]LLMs Agent for Graphs General Graph arXiv 2023 FUXI[18]LLMs Agent for Graphs Graph-based QA arXiv 2024 Readi[10]LLMs Agent for Graphs Graph-based QA arXiv 2024 RoG[49]LLMs Agent for Graphs Graph-based QA ICLR 2024LLMs-OnlyNLGraph[68]Tuning-free Graph Reasoning NeurIPS 2024 GPT4Graph[20]Tuning-free Graph Reasoning &amp; QA arXiv 2023 Beyond Text[28]Tuning-free General Graph arXiv 2023 Graph-LLM[8]Tuning-free General Graph KDD Exp.News.2023 GraphText[95]Tuning-free General Graph arXiv 2023 Talk like a Graph[15]Tuning-free Graph Reasoning arXiv 2023 LLM4DyG[91]Tuning-free Dynamic Graph arXiv 2023 GraphTMI[11]Tuning-free General Graph arXiv 2023 Ai et al.[2]Tuning-free Multi-modal Graph arXiv 2023 InstructGLM[84]Tuning-required General Graph EACL 2024 WalkLM[62]Tuning-required General Graph NeurIPS 2024 LLaGA[7]Tuning-required General Graph ICML 2024 InstructGraph[69]Tuning-required General Graph &amp; QA &amp; Reasoning arXiv 2024 ZeroG[38]Tuning-required General Graph arXiv 2024 GraphWiz[6]Tuning-required Graph Reasoning arXiv 2024 GraphInstruct[50]Tuning-required Graph Reasoning &amp; Generation arXiv 2024 MuseGraph[61]Tuning-required General Graph arXiv 2024
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Shyamal Altman, Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023. 2023</p>
<p>When Graph Data Meets Multimodal: A New Paradigm for Graph Understanding and Reasoning. Qihang Ai, Jianwu Zhou, Haiyun Jiang, Lemao Liu, Shuming Shi, arXiv:2312.103722023. 2023arXiv preprint</p>
<p>Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings. William Brannon, arXiv:2305.143212023. 2023arXiv preprint</p>
<p>Instructmol: Multimodal integration for building a versatile and reliable molecular assistant in drug discovery. He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li, arXiv:2311.162082023. 2023arXiv preprint</p>
<p>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang, arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023. 2023arXiv preprint</p>
<p>GraphWiz: An Instruction-Following Language Model for Graph Problems. Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li, arXiv:2402.160292024. 2024arXiv preprint</p>
<p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang, arXiv:2402.08170LLaGA: Large Language and Graph Assistant. 2024. 2024arXiv preprint</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, ACM SIGKDD Explorations Newsletter. 252024. 2024</p>
<p>Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang, arXiv:2310.04668Label-free node classification on graphs with large language models (llms). 2023. 2023arXiv preprint</p>
<p>Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, arXiv:2403.08593Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments. 2024. 2024arXiv preprint</p>
<p>Which Modality should I use-Text, Motif, or Image?: Understanding Graphs with Large Language Models. Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang, arXiv:2311.098622023. 2023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Parameterefficient fine-tuning of large-scale pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Nature Machine Intelligence. 52023. 2023</p>
<p>Qian Keyu Duan, Tat-Seng Liu, Shuicheng Chua, Wei Tsang Yan, Qizhe Ooi, Junxian Xie, He, arXiv:2308.02565Simteg: A frustratingly simple approach improves textual graph learning. 2023. 2023arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023. 2023arXiv preprint</p>
<p>Parameter-efficient mixture-of-experts architecture for pre-trained language models. Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen, arXiv:2203.011042022. 2022arXiv preprint</p>
<p>Generalization and representational limits of graph neural networks. Vikas Garg, Stefanie Jegelka, Tommi Jaakkola, ICML. PMLR. 2020</p>
<p>Yu Gu, arXiv:2402.14672Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments. 2024. 2024arXiv preprint</p>
<p>Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. Yu Gu, Xiang Deng, Yu Su, arXiv:2212.097362022. 2022arXiv preprint</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. Jiayan Guo, Lun Du, Hengyu Liu, arXiv:2305.150662023. 2023arXiv preprint</p>
<p>Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang, arXiv:2402.15183GraphEdit: Large Language Models for Graph Structure Learning. 2024. 2024arXiv preprint</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann Lecun, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Lightgcn: Simplifying and powering graph convolution network for recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. the 43rd International ACM SIGIR conference on research and development in Information Retrieval2020</p>
<p>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering. Xiaoxin He, Yijun Tian, Yifei Sun, V Nitesh, Thomas Chawla, Yann Laurent, Xavier Le-Cun, Bryan Bresson, Hooi, arXiv:2402.076302024. 2024arXiv preprint</p>
<p>UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language. Yufei He, Bryan Hooi, arXiv:2402.136302024. 2024arXiv preprint</p>
<p>Large language models are zero-shot rankers for recommender systems. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian Mcauley, Wayne Xin Zhao, European Conference on Information Retrieval. Springer2024</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, Advances in neural information processing systems. 332020. 2020</p>
<p>Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data. Yuntong Hu, Zheng Zhang, Liang Zhao, arXiv:2310.049442023. 2023arXiv preprint</p>
<p>Large Language Models for Graphs: Progresses and Directions. Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Prompt-based node feature extractor for few-shot learning on text-attributed graphs. Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang, Yang Yang, Qi Zhu, arXiv:2309.028482023. 2023arXiv preprint</p>
<p>Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu, arXiv:2402.12984Can GNN be Good Adapter for LLMs?. 2024. 2024arXiv preprint</p>
<p>Knowledge graph embedding based question answering. Xiao Huang, Jingyuan Zhang, Dingcheng Li, Ping Li, Proceedings of the twelfth ACM international conference on web search and data mining. the twelfth ACM international conference on web search and data mining2019</p>
<p>Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.02783Large language models on graphs: A comprehensive survey. 2023. 2023arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>Graph transformer for recommendation. Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, Chao Huang, SIGIR. 2023</p>
<p>GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs. Yichuan Li, Kaize Ding, Kyumin Lee, arXiv:2310.151092023. 2023arXiv preprint</p>
<p>Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu, Yu , arXiv:2311.12399A survey of graph meets large language model: Progress and future directions. 2023. 2023arXiv preprint</p>
<p>Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li, arXiv:2402.11235ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs. 2024. 2024arXiv preprint</p>
<p>Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang, arXiv:2403.00813Urbangpt: Spatio-temporal large language models. 2024. 2024arXiv preprint</p>
<p>A Survey of Multi-modal Knowledge Graphs: Technologies and Trends. Wanying Liang, Pasquale De Meo, Yong Tang, Jia Zhu, Comput. Surveys. 2024. 2024</p>
<p>Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, Tat-Seng Chua, arXiv:2401.17197Data-efficient Fine-tuning for LLM-based Recommendation. 2024. 2024arXiv preprint</p>
<p>One for all: Towards training one graph model for all classification tasks. Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang, arXiv:2310.001492023arXiv preprint</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 362024. 2024</p>
<p>Git-mol: A multimodal large language model for molecular science with graph, image, and text. Pengfei Liu, Yiming Ren, Computers in Biology and Medicine. 171108073Jun Tao, and Zhixiang Ren. 2024. 2024</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Nature Machine Intelligence. 2023. 2023</p>
<p>Yinhan Liu, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, arXiv:2310.127982023. 2023arXiv preprint</p>
<p>Linhao Luo, Yuan-Fang Li, arXiv:2310.01061Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. 2023arXiv preprint</p>
<p>GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability. Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin, arXiv:2403.044832024. 2024arXiv preprint</p>
<p>XRec: Large Language Models for Explainable Recommendation. Qiyao Ma, Xubin Ren, Chao Huang, arXiv:2406.023772024. 2024arXiv preprint</p>
<p>Information network or social The structure of the follow graph. Seth A Myers, Aneesh Sharma, Pankaj Gupta, Jimmy Lin, Proceedings of the 23rd international conference on world wide web. the 23rd international conference on world wide web2014</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, IEEE Transactions on Knowledge and Data Engineering. 2024. 2024</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Yijian Qin, Xin Wang, Ziwei Zhang, Wenwu Zhu, arXiv:2310.181522023. 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of machine learning research. 212020. 2020</p>
<p>Xubin Ren, Wei Wei, Lianghao Xia, Chao Huang, arXiv:2404.03354A Comprehensive Survey on Self-Supervised Learning for Recommendation. 2024. 2024arXiv preprint</p>
<p>Representation learning with large language models for recommendation. Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Disentangled contrastive collaborative filtering. Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Bing Su, Dazhao Du, Zhao Yang, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022. 2022arXiv preprint</p>
<p>MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining. Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl Yang, arXiv:2403.047802024. 2024arXiv preprint</p>
<p>Walklm: A uniform language model fine-tuning framework for attributed graph embedding. Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Graphgpt: Graph instruction tuning for large language models. Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang, arXiv:2310.130232023. 2023arXiv preprint</p>
<p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang, arXiv:2402.16024HiGPT: Heterogeneous Graph Language Model. 2024. 2024arXiv preprint</p>
<p>Graph neural prompting with large language models. Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, Panpan Xu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Hugo Touvron, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017. 2017arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia Tsvetkov, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment. Jianing Wang, Junda Wu, Yupeng Hou, arXiv:2402.087852024. 2024arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Frontiers of Computer Science. 181863452024. 2024</p>
<p>Graph Agent: Explicit Reasoning Agent for Graphs. Qinyong Wang, Zhenxiang Gao, Rong Xu, arXiv:2310.164212023. 2023arXiv preprint</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Zhihao Wen, Yuan Fang, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Prompt tuning on graph-augmented lowresource text classification. Zhihao Wen, Yuan Fang, arXiv:2307.102302023. 2023arXiv preprint</p>
<p>Difformer: Scalable (graph) transformers induced by energy constrained diffusion. Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan, arXiv:2301.094742023. 2023arXiv preprint</p>
<p>Nodeformer: A scalable graph structure learning transformer for node classification. Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, Junchi Yan, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Next-gpt: Any-to-any multimodal llm. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua, arXiv:2309.055192023. 2023arXiv preprint</p>
<p>A comprehensive survey on graph neural networks. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip Yu, IEEE transactions on neural networks and learning systems. 322020. 2020</p>
<p>Lianghao Xia, Ben Kao, Chao Huang, arXiv:2403.01121OpenGraph: Towards Open Graph Foundation Models. 2024. 2024arXiv preprint</p>
<p>Graph-aware language model pre-training on a large graph corpus can help multiple graph applications. Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Xiang Vassilis N Ioannidis, Qing Song, Sheng Ping, Carl Wang, Yi Yang, Xu, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018. 2018arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Natural language is all a graph needs. Ruosong Ye, Caiqi Zhang, arXiv:2308.071342023. 2023arXiv preprint</p>
<p>Graph contrastive learning automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang, International Conference on Machine Learning. PMLR2021</p>
<p>Graph transformer networks. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J Kim, Advances in neural information processing systems. 322019. 2019</p>
<p>Video-llama: An instructiontuned audio-visual language model for video understanding. Hang Zhang, Xin Li, Lidong Bing, arXiv:2306.028582023. 2023arXiv preprint</p>
<p>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks. Mengmei Zhang, arXiv:2402.071972024. 2024arXiv preprint</p>
<p>Link prediction based on graph neural networks. Muhan Zhang, Yixin Chen, Advances in neural information processing systems. 2018. 201831</p>
<p>Greaselm: Graph reasoning enhanced language models for question answering. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, arXiv:2201.088602022. 2022arXiv preprint</p>
<p>LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, Wenwu Zhu, arXiv:2310.171102023. 2023arXiv preprint</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, Qi Liu, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Jianan Zhao, Hesham Mostafa, Michael Galkin, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2405.20445GraphAny: A Foundation Model for Node Classification on Any Graph. 2024. 2024arXiv preprint</p>
<p>Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang, arXiv:2210.14709Learning on large-scale text-attributed graphs via variational inference. 2022. 2022arXiv preprint</p>
<p>Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023. 2023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>A survey on deep graph generation: Methods and applications. Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, Shu Wu, Learning on Graphs Conference. PMLR2022</p>
<p>Efficient Tuning and Inference for Large Language Models on Textual Graphs. Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang, arXiv:2401.155692024. 2024arXiv preprint</p>
<p>Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li, arXiv:2311.01343Collaborative large language model for recommender systems. 2023. 2023arXiv preprint</p>
<p>Pretraining language models with text-attributed heterogeneous graphs. Tao Zou, Le Yu, Yifei Huang, Leilei Sun, Bowen Du, arXiv:2310.125802023. 2023arXiv preprint</p>
<p>APPENDIX In Table 1, we provide an overview of notable graph learning techniques that utilize large language models. </p>            </div>
        </div>

    </div>
</body>
</html>
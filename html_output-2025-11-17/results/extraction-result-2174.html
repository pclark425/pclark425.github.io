<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2174 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2174</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2174</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-281681196</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.26603v1.pdf" target="_blank">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></p>
                <p><strong>Paper Abstract:</strong> While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of"hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2174.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2174.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepScientist: A Progressive Autonomous Scientific Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven, multi-agent autonomous research system that frames scientific discovery as a goal-directed Bayesian Optimization loop and conducts hierarchical, fidelity-tiered ideation, surrogate scoring, implementation, and deep analysis to produce SOTA-surpassing scientific methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid multi-agent LLM-based automated discovery system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific research (demonstrated on LLM agent failure attribution, LLM inference acceleration, and AI text detection)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates research hypotheses, experimental plans, concrete algorithmic methods, and full research papers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Hierarchical 3-stage pipeline: (1) low-cost surrogate scoring by an LLM-based reviewer to estimate utility/quality/exploration (valuation vector ⟨v_u,v_q,v_e⟩); (2) acquisition-function-driven selection (UCB) and sandboxed implementation by a code-execution agent producing experimental logs and quantitative metrics; (3) for successful implementations, automated deeper analyses (ablations, extra-dataset evaluations) and paper synthesis via MCP tools; additionally, human supervisors manually inspect all experimental results and run a secondary verification execution to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Operationalized by empirical gains over human SOTA performance metrics (task-specific metrics: accuracy, tokens/sec, AUROC), human expert review scores, and promotion in the Findings Memory from Idea→Implement→Progress; novelty is also signaled by the surrogate's 'exploration' score and by being promoted to the high-fidelity Analyze & Report stage.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Generated ~5,000 unique ideas; selected ~1,100 for experimental validation; produced 21 'Progress Findings' (SOTA-surpassing discoveries). Reported task gains: +183.7% (Accuracy) on Agent Failure Attribution, +1.9% (Tokens/sec) on LLM Inference Acceleration, +7.9% (AUROC) on AI Text Detection. Overall post-selection success rate ≈ 1-3% (21/1100 ≈ 1.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is costly but effective for confirmed findings: experiments produce concrete benchmark metrics (e.g., Who&When, MBPP, RAID). However, the paper reports high rates of validation failure: ~60% of failed implementations were due to implementation errors; many validated claims required human inspection; no calibrated ROC-like measures for the surrogate are given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported as a single number for the overall pipeline; implementation-stage issues observed: ~50% of initial implementation attempts failed to complete due to agent timeouts (leading to spurious/invalid outputs that required secondary verification). After secondary verification and human inspection, many false positives were removed; exact end-to-end FPR not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not directly quantified in the paper; selection and surrogate filtering remove many ideas before implementation (only ~1,100/5,000 advanced), so potential false negatives (valid ideas filtered out) are possible but not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Validation becomes more difficult and costly for highly novel proposals: many novel ideas require expensive, high-fidelity experiments and are more likely to fail due to implementation errors or insufficient evaluation design. The paper emphasizes that frontier (novel) tasks with high evaluation cost face impractically low success rates for brute-force AI exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Clear asymmetry: DeepScientist excels at ideation (human reviewers praised novelty) but struggles with reliable implementation and convincing validation; ideation-to-confirmation success is rare, making validation the bottleneck (paper explicitly states generation >> validation reliability).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Mixed: on some novel discoveries the system produced SOTA improvements (three tasks shown), demonstrating capability on out-of-distribution / frontier problems; generally, performance depends strongly on feedback loop speed—rapid-feedback tasks are feasible, high-cost (pretraining, wet-lab) are currently impractical for large-scale autonomous exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Surrogate outputs are numeric scores (0–100) but the paper does not report formal calibration analyses; empirical evidence (high rate of downstream implementation failures and incorrect completions) suggests imperfect calibration, especially for novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Generation: ~ $5 per idea in API calls. Implement & Verify: ~ $20 in Claude-4-opus API + ~1 GPU hour (median ~70 minutes equated to ~1e16 FLOPs per experiment in examples). Analyze & Report per successful finding: ~ $150. Total reported cost for the study: ~ $100,000 and ~20,000 GPU hours. Validation is therefore substantially more expensive than low-cost ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Findings Memory (cumulative knowledge from human and system findings), surrogate LLM reviewer for low-cost filtering, acquisition function (UCB) balancing exploitation/exploration, sandboxed implementation with secondary verification, MCP analysis tools, human-in-the-loop inspection, and scaling via parallel GPU workers with shared Findings Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepScientist demonstrates that LLM-driven systems can autonomously generate hundreds-to-thousands of plausible scientific ideas and produce SOTA-surpassing methods, but the dominant bottleneck is verification: validation is costly, error-prone (many implementation failures), and far less efficient than generation, resulting in a low end-to-end success rate (~1-3%).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2174.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surrogate LLM Reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surrogate LLM Reviewer (Findings Valuator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-cost, LLM-based surrogate that approximates the true (expensive) scientific value function f(·) by scoring candidate ideas with a structured valuation vector ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) on a 0–100 scale, used to prioritize ideas for expensive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Surrogate LLM Reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as surrogate evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>meta-scientific evaluation within automated discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate methods; it scores/evaluates generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>approximates objective f by contextualizing the Findings Memory and assigning numeric scores; these are used in an acquisition function (UCB) to choose experiments for real validation; surrogate itself is not formally validated with calibration metrics in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>contains an 'exploration' score v_e to quantify novelty/uncertainty; high v_e promotes out-of-distribution ideas for testing via UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (evaluation-only component).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>No formal calibration metrics reported; surrogate-driven selection improves success vs random baseline (selection strategy yields ~1-3% success vs effectively 0% for random sampling), implying useful but imperfect predictive power.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically; indirectly, surrogate filtering still allowed many downstream implementation failures, indicating notable false positives in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified; selection prunes many ideas—some true positives may be filtered out but rate unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Surrogate explicitly attempts to quantify novelty (v_e), but paper reports limited surrogate reliability on truly novel, high-cost frontier ideas; novelty increases uncertainty of surrogate estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Surrogate helps reduce search space but cannot fully bridge the gap—many surrogate-high ideas still fail in implementation, indicating an asymmetry between surrogate prediction and high-fidelity validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Surrogate can highlight exploratory candidates but performance on out-of-distribution (highly novel) ideas is not quantified and appears limited.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; qualitative evidence suggests imperfect calibration (many surrogate-promoted ideas still fail).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Low-cost relative to full experiments (operates via LLM API calls during Strategize & Hypothesize stage); exact per-call cost not separately itemized beyond general $5 per idea average for strategize stage.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Used alongside Findings Memory and UCB acquisition to prioritize scarce high-fidelity validation resources; proposed future improvements include better low-cost surrogates and acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A surrogate LLM reviewer can meaningfully prioritize candidates and raise success relative to random sampling, but it remains imperfect—especially for novel, high-cost frontier ideas—and is a major contributor to the generation→validation bottleneck.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2174.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Findings Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Findings Memory (Cumulative Knowledge Repository)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuously expanding, structured database that stores both human frontier knowledge (papers, code) and the system's own Idea/Implement/Progress records, used to contextualize ideation and inform surrogate evaluation and acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Findings Memory</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge base / persistent memory component</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>meta-scientific management within automated discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not generate outputs but seeds future hypothesis generation by retrieval and contextualization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>serves as context for the surrogate LLM reviewer and ideation agents; its use is evaluated indirectly by improved selection and scaling behavior when shared across parallel workers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>records stage labels (Idea, Implement, Progress) and metrics, enabling the system to detect novelty as lack of prior coverage and to prefer unexplored directions via the surrogate 'exploration' score.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (passive memory), but system-level outcomes (more discoveries with shared memory across parallel workers) indicate high utility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Contributes to improved selection and scaling (near-linear increase in progress findings with more GPUs when shared Findings Memory is used); no isolated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not applicable directly; poor or noisy memory entries can mislead surrogate and agents but rates not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>A shared memory helps accelerate discovery by reusing failures and successes, reducing redundant exploration; this is especially helpful in novel search areas where previous failures are informative.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Findings Memory narrows the gap by preventing repeated testing of failed paths, but cannot eliminate implementation/validation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Enables better collective search across out-of-distribution regions by sharing intermediate findings across parallel workers.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Low relative to experiments (storage and retrieval overhead); retrieval count K=15 used in system.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Acts as the principal memory mechanism to learn from failures and successes; shared across parallel agents to produce synergistic effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A centralized Findings Memory that accumulates both human and machine findings materially improves search efficiency and enables near-linear scaling of discoveries with parallel compute, helping mitigate the validation bottleneck by preventing redundant experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2174.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude Code Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude Code Agent (Claude-4-Opus based code execution agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-generation and execution agent (powered by Claude-4-Opus and Claude Code framework) responsible for implementing candidate ideas in sandboxed repositories and executing experiments to produce empirical results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude Code agent (Claude-4-Opus)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model-based code synthesis and execution agent</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>software implementation and experimental execution within automated discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates implementation code, experiment scripts, and runs experiments in an isolated sandbox</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Executes experiments in sandboxed folders; after completion DeepScientist re-executes main scripts as secondary verification to reduce erroneous success reports; human supervisors manually inspect experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>N/A (implementation agent), but its reliability affects whether novel ideas can be validated successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used to implement ~1,100 ideas; observed high implementation failure rate (implementation-level errors were responsible for ~60% of failed trials). Approximately 50% of initial implementations failed to complete due to internal timeouts in Claude Code (leading to false-positive or incomplete reports).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Implementation-stage instability is a dominant source of validation failure; secondary verification (re-execution) reduced false positives but no formal success/failure ROC reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Operationally high at the raw implementation-step level: ~50% of initial runs failed to complete (timeouts) and required additional verification; precise end-to-end FPR after verification not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified; some successful ideas might be marked failed if implementation agent crashed, but rate unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>More novel implementations often require code paths not present in baseline repositories, increasing fragility and implementation error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Implementation agent is a primary contributor to asymmetry: many promising hypotheses fail not because the idea is wrong, but because of implementation errors, exacerbating the generation→validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Implementation success degrades for ideas that require extensive unfamiliar code changes; paper reports many premature terminations for non-standard experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable beyond engineering logs; agent reported completion states were not fully reliable without secondary verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Approximately 1 GPU-hour per implementation attempt on H800; Claude API cost ~ $20 per attempt. Median execution ~70 minutes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Secondary verification (re-execution), sandbox isolation, human supervision, improved code-generation models recommended as future work to reduce implementation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Implementation-level failures (agent timeouts, bugs) accounted for the majority (~60%) of exploratory failures, making robust implementation agents and secondary verification essential to converting generated ideas into validated scientific findings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2174.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer: An LLM-based automated peer-review agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated reviewer agent designed to emulate human peer review with external search capability; used here both as a surrogate reviewer (DeepReviewer-14B) and as an automated benchmark to evaluate the quality of AI-generated research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepreview: Improving llm-based paper review with human-like deep thinking process.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepReviewer (DeepReviewer-14B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model-based automated reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>peer review and assessment of research outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates review scores and textual reviews assessing soundness, presentation, contribution</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluates papers via automated scoring and external literature search; used to benchmark DeepScientist outputs against 28 other AI Scientist papers and to produce acceptance-rate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Produces ordinal ratings and review scores as proxies for novelty and quality; used to compute an automated acceptance-rate metric.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported that DeepScientist's papers achieved a 60% acceptance rate under DeepReviewer evaluation (higher than other compared AI Scientist outputs); also used to produce structured valuation vectors in other contexts (related systems).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>DeepReviewer produced numeric ratings and was used as an automated comparator; the paper also convened human expert reviews to corroborate DeepReviewer assessments. No formal calibration metrics for DeepReviewer reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not quantified herein; potential for automated reviewer to over- or under-estimate novelty is discussed conceptually but not measured.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Automated review recognizes ideation novelty but human reviewers flagged empirical/validation weaknesses that automated review may miss, indicating limits for automated review on rigorous experimental critique.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Shows that automated review can accept novel textual contributions but may not fully detect weak experimental rigor—further supporting the gap between ideation and validated evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; used as a benchmark for machine-generated papers across varying novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Operates via LLM inference and external search; per-paper cost not itemized separately in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Used in tandem with human program committee to provide both automated and human assessments; suggested as part of future pipelines to scale review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated reviewers like DeepReviewer can identify and rate novel ideas and achieved a 60% acceptance-rate signal for DeepScientist outputs, but human reviewers detected recurring empirical weaknesses that automated review alone can miss.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2174.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A2P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A2P (Abduction-Action-Prediction) method</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system-generated method for agent failure attribution that uses causal/counterfactual reasoning: infer the hidden cause (abduction), propose a corrective action, and predict whether that action would have led to success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>A2P (system-generated algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>algorithmic method for causal attribution</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM multi-agent failure attribution</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>a novel algorithm that produces counterfactual-based attribution predictions and fixes</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Implemented by DeepScientist and evaluated on the Who&When benchmark in both handcrafted and algorithm-generated settings, producing quantitative benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by benchmark performance improvement over prior SOTA and by human review of conceptual novelty (counterfactual/causal framing).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Achieved benchmark scores of 29.31 (handcraft setting) and 47.46 (algorithm-generated setting), establishing a new SOTA for that task as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validated empirically on Who&When; training-free A2P maintained SOTA as of September 2025 and outperformed even 7B models trained on synthetic data (per the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>The causal, zero-shot framework provided robust gains without additional training data, suggesting that methodological novelty can translate to reliable validation when the evaluation feedback is well-defined and inexpensive.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>A2P is an example where ideation and validation aligned successfully—i.e., a generated novel method produced reproducible, validated improvement—contrasting with many other generated ideas that failed at implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Performed well on this frontier, training-free and generalizing beyond prior engineered heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Evaluated within existing benchmark runs; relative cost not separately quantified but described as training-free (low data/training cost).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Succeeded in part because the task evaluation was relatively cheap / well-specified and the method provided a principled causal framing, reducing reliance on expensive brute-force validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A2P demonstrates that when tasks provide clear, low-cost evaluation signals, a generated, novel algorithm can be validated and achieve large SOTA gains; this contrasts with high-cost tasks where validation bottlenecks dominate.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2174.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACRA (Autogenerated method for LLM Inference Acceleration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DeepScientist-discovered method that improves LLM inference throughput by identifying stable suffix patterns and dynamically conditioning decoding guesses, effectively grafting a long-term memory onto decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACRA (system-generated algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>algorithmic method for LLM inference/decoding</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>LLM inference acceleration</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates a decoding strategy that accelerates token generation while preserving accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Implemented on the MPBB/MBPP benchmark and measured in tokens/second throughput and correctness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by tokens/second performance improvement over human SOTA (TokenRecycling baseline), plus human review of conceptual novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Improved throughput from human SOTA 190.25 tokens/sec to 193.90 tokens/sec as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Empirically validated to provide a modest but measurable throughput gain while retaining lossless verification.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Gains were modest and achieved in a domain (inference speed) where rapid feedback and concrete numeric evaluation made validation feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>ACRA shows that in engineering-dominated, fast-feedback tasks the system can both generate and validate improvements; contrasted with high-cost or poorly-specified tasks where validation lags.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Demonstrated incremental improvement on a specialized inference task; generalization to other model families not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Experimentation used standard inference benchmarks; specific cost per experiment not isolated beyond general implementation costs (~1 GPU-hour per implement).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Benefited from a rapid-feedback experimental setting and focused exploitation of stable suffix patterns, reducing reliance on high-cost validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ACRA shows that system-generated engineering innovations can be validated in domains with fast, low-cost evaluation loops, producing modest SOTA gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2174.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T-Detect/TDT/PA-Detect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T-Detect, TDT (Time/Domain Transform), and PA-Detect (Phase Alignment Detect)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence of DeepScientist-generated AI-text detection methods progressing from robust statistical modeling (T-Detect) to signal-processing approaches (TDT, PA-Detect) that treat text as a non-stationary signal and analyze localized anomalies, culminating in SOTA detection performance and faster inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>T-Detect / TDT / PA-Detect (system-generated detection methods)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>algorithmic detection methods (signal/statistics-based)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>AI-generated text detection</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generate detection models/algorithms that output binary classification (human vs LLM-generated) and associated anomaly localizations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Implemented and benchmarked on the RAID dataset; successive methods were evaluated for AUROC and latency (inference speed) and compared to existing human SOTA detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Measured by AUROC improvement over prior SOTA and by computational latency; novelty also judged by program committee reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Discovered three progressively superior methods within two weeks; final method achieved +7.9% AUROC over prior SOTA and also doubled inference speed as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validated empirically on RAID with AUROC and inference-latency tradeoffs; human reviewers praised ideation but noted experimental and contextual weaknesses in some generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported; ROC-based metrics (AUROC improvement) reported instead.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Rapid iterative progression (T-Detect → TDT → PA-Detect) indicates that when a novel idea yields measurable metric gains, the system can quickly refine and validate further novelty; success benefited from concrete benchmark metrics and relatively fast experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>These methods exemplify cases where generation and validation aligned because the task allowed localized, numeric evaluation—contrasting with higher-cost tasks where alignment was poor.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Demonstrated strong improvement on the RAID benchmark; generalization to other corpora not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Evaluations performed in the standard detection benchmarking pipeline; per-experiment cost not separated beyond general system cost accounting.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Progressive refinement via Findings Memory and rapid low-to-high fidelity loop enabled the chain of improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The chain of T-Detect→TDT→PA-Detect shows DeepScientist can quickly iterate on a promising novel direction when the task offers cheap, well-defined evaluation metrics, producing both higher accuracy and faster inference.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2174.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist-V2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist-V2: Workshop-level automated scientific discovery via agentic tree search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior automated discovery system cited as related work; reported to produce published scientific artifacts but criticized here for undirected exploration lacking human-defined goal focus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist-V2</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based automated discovery system (agentic tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>reports to generate research artifacts and papers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported in literature (Yamada et al., 2025); in this paper it is discussed qualitatively as having outputs sometimes judged naive under human evaluation due to lack of explicit goal orientation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not quantified in this paper beyond citation and qualitative critique.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Critiqued for producing undirected discoveries that may lack human-perceived scientific value; suggests that goal-orientation affects validation and perceived novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Mentioned as an example where generation may occur without rigorous goal-driven validation; no numeric data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior work showing capacity to generate research artifacts but highlighted as lacking a goal-driven approach that DeepScientist adopts to ensure scientifically meaningful validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2174.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2174.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaEvolve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaEvolve: A coding agent for scientific and algorithmic discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system (DeepMind-style) that uses massive trial-and-error with known engineering methods to improve codebases and algorithms; cited as an engineering-optimization counterpart to scientific discovery systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Alphaevolve: A coding agent for scientific and algorithmic discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaEvolve</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>automated coding/algorithmic discovery agent</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>algorithm discovery / engineering optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates algorithmic optimizations and code-level improvements</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Uses trial-and-error experiments within engineering contexts (as described in citation); in this paper it is mentioned as focusing on engineering-driven optimization within known paradigms rather than establishing new scientific paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Paper contrasts this engineering optimization approach with DeepScientist's aim to change foundational methods—suggesting different validation dynamics when working inside vs. outside established paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to contrast engineering-heavy automated optimization systems with DeepScientist's goal-driven scientific-discovery focus; helps contextualize where validation may be cheaper (engineering) vs. expensive (frontier science).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. <em>(Rating: 2)</em></li>
                <li>Alphaevolve: A coding agent for scientific and algorithmic discovery. <em>(Rating: 2)</em></li>
                <li>The need for verification in ai-driven scientific discovery. <em>(Rating: 2)</em></li>
                <li>Scaling laws in scientific discovery with ai and robot scientists. <em>(Rating: 1)</em></li>
                <li>Deepreview: Improving llm-based paper review with human-like deep thinking process. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2174",
    "paper_id": "paper-281681196",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "DeepScientist",
            "name_full": "DeepScientist: A Progressive Autonomous Scientific Discovery System",
            "brief_description": "An LLM-driven, multi-agent autonomous research system that frames scientific discovery as a goal-directed Bayesian Optimization loop and conducts hierarchical, fidelity-tiered ideation, surrogate scoring, implementation, and deep analysis to produce SOTA-surpassing scientific methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DeepScientist",
            "system_type": "hybrid multi-agent LLM-based automated discovery system",
            "domain": "general scientific research (demonstrated on LLM agent failure attribution, LLM inference acceleration, and AI text detection)",
            "generation_capability": "generates research hypotheses, experimental plans, concrete algorithmic methods, and full research papers",
            "validation_method": "Hierarchical 3-stage pipeline: (1) low-cost surrogate scoring by an LLM-based reviewer to estimate utility/quality/exploration (valuation vector ⟨v_u,v_q,v_e⟩); (2) acquisition-function-driven selection (UCB) and sandboxed implementation by a code-execution agent producing experimental logs and quantitative metrics; (3) for successful implementations, automated deeper analyses (ablations, extra-dataset evaluations) and paper synthesis via MCP tools; additionally, human supervisors manually inspect all experimental results and run a secondary verification execution to reduce false positives.",
            "novelty_measure": "Operationalized by empirical gains over human SOTA performance metrics (task-specific metrics: accuracy, tokens/sec, AUROC), human expert review scores, and promotion in the Findings Memory from Idea→Implement→Progress; novelty is also signaled by the surrogate's 'exploration' score and by being promoted to the high-fidelity Analyze & Report stage.",
            "generation_performance": "Generated ~5,000 unique ideas; selected ~1,100 for experimental validation; produced 21 'Progress Findings' (SOTA-surpassing discoveries). Reported task gains: +183.7% (Accuracy) on Agent Failure Attribution, +1.9% (Tokens/sec) on LLM Inference Acceleration, +7.9% (AUROC) on AI Text Detection. Overall post-selection success rate ≈ 1-3% (21/1100 ≈ 1.9%).",
            "validation_performance": "Validation is costly but effective for confirmed findings: experiments produce concrete benchmark metrics (e.g., Who&When, MBPP, RAID). However, the paper reports high rates of validation failure: ~60% of failed implementations were due to implementation errors; many validated claims required human inspection; no calibrated ROC-like measures for the surrogate are given.",
            "false_positive_rate": "Not reported as a single number for the overall pipeline; implementation-stage issues observed: ~50% of initial implementation attempts failed to complete due to agent timeouts (leading to spurious/invalid outputs that required secondary verification). After secondary verification and human inspection, many false positives were removed; exact end-to-end FPR not provided.",
            "false_negative_rate": "Not directly quantified in the paper; selection and surrogate filtering remove many ideas before implementation (only ~1,100/5,000 advanced), so potential false negatives (valid ideas filtered out) are possible but not measured.",
            "novelty_effect_on_validation": "Validation becomes more difficult and costly for highly novel proposals: many novel ideas require expensive, high-fidelity experiments and are more likely to fail due to implementation errors or insufficient evaluation design. The paper emphasizes that frontier (novel) tasks with high evaluation cost face impractically low success rates for brute-force AI exploration.",
            "generation_validation_asymmetry": "Clear asymmetry: DeepScientist excels at ideation (human reviewers praised novelty) but struggles with reliable implementation and convincing validation; ideation-to-confirmation success is rare, making validation the bottleneck (paper explicitly states generation &gt;&gt; validation reliability).",
            "out_of_distribution_performance": "Mixed: on some novel discoveries the system produced SOTA improvements (three tasks shown), demonstrating capability on out-of-distribution / frontier problems; generally, performance depends strongly on feedback loop speed—rapid-feedback tasks are feasible, high-cost (pretraining, wet-lab) are currently impractical for large-scale autonomous exploration.",
            "calibration_quality": "Surrogate outputs are numeric scores (0–100) but the paper does not report formal calibration analyses; empirical evidence (high rate of downstream implementation failures and incorrect completions) suggests imperfect calibration, especially for novel ideas.",
            "validation_computational_cost": "Generation: ~ $5 per idea in API calls. Implement & Verify: ~ $20 in Claude-4-opus API + ~1 GPU hour (median ~70 minutes equated to ~1e16 FLOPs per experiment in examples). Analyze & Report per successful finding: ~ $150. Total reported cost for the study: ~ $100,000 and ~20,000 GPU hours. Validation is therefore substantially more expensive than low-cost ideation.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Findings Memory (cumulative knowledge from human and system findings), surrogate LLM reviewer for low-cost filtering, acquisition function (UCB) balancing exploitation/exploration, sandboxed implementation with secondary verification, MCP analysis tools, human-in-the-loop inspection, and scaling via parallel GPU workers with shared Findings Memory.",
            "evidence_type": "supports",
            "key_findings": "DeepScientist demonstrates that LLM-driven systems can autonomously generate hundreds-to-thousands of plausible scientific ideas and produce SOTA-surpassing methods, but the dominant bottleneck is verification: validation is costly, error-prone (many implementation failures), and far less efficient than generation, resulting in a low end-to-end success rate (~1-3%).",
            "uuid": "e2174.0"
        },
        {
            "name_short": "Surrogate LLM Reviewer",
            "name_full": "Surrogate LLM Reviewer (Findings Valuator)",
            "brief_description": "A low-cost, LLM-based surrogate that approximates the true (expensive) scientific value function f(·) by scoring candidate ideas with a structured valuation vector ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) on a 0–100 scale, used to prioritize ideas for expensive evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Surrogate LLM Reviewer",
            "system_type": "large language model used as surrogate evaluator",
            "domain": "meta-scientific evaluation within automated discovery",
            "generation_capability": "does not generate methods; it scores/evaluates generated hypotheses",
            "validation_method": "approximates objective f by contextualizing the Findings Memory and assigning numeric scores; these are used in an acquisition function (UCB) to choose experiments for real validation; surrogate itself is not formally validated with calibration metrics in paper.",
            "novelty_measure": "contains an 'exploration' score v_e to quantify novelty/uncertainty; high v_e promotes out-of-distribution ideas for testing via UCB.",
            "generation_performance": "N/A (evaluation-only component).",
            "validation_performance": "No formal calibration metrics reported; surrogate-driven selection improves success vs random baseline (selection strategy yields ~1-3% success vs effectively 0% for random sampling), implying useful but imperfect predictive power.",
            "false_positive_rate": "Not reported numerically; indirectly, surrogate filtering still allowed many downstream implementation failures, indicating notable false positives in practice.",
            "false_negative_rate": "Not quantified; selection prunes many ideas—some true positives may be filtered out but rate unknown.",
            "novelty_effect_on_validation": "Surrogate explicitly attempts to quantify novelty (v_e), but paper reports limited surrogate reliability on truly novel, high-cost frontier ideas; novelty increases uncertainty of surrogate estimates.",
            "generation_validation_asymmetry": "Surrogate helps reduce search space but cannot fully bridge the gap—many surrogate-high ideas still fail in implementation, indicating an asymmetry between surrogate prediction and high-fidelity validation.",
            "out_of_distribution_performance": "Surrogate can highlight exploratory candidates but performance on out-of-distribution (highly novel) ideas is not quantified and appears limited.",
            "calibration_quality": "Not reported; qualitative evidence suggests imperfect calibration (many surrogate-promoted ideas still fail).",
            "validation_computational_cost": "Low-cost relative to full experiments (operates via LLM API calls during Strategize & Hypothesize stage); exact per-call cost not separately itemized beyond general $5 per idea average for strategize stage.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Used alongside Findings Memory and UCB acquisition to prioritize scarce high-fidelity validation resources; proposed future improvements include better low-cost surrogates and acquisition functions.",
            "evidence_type": "supports",
            "key_findings": "A surrogate LLM reviewer can meaningfully prioritize candidates and raise success relative to random sampling, but it remains imperfect—especially for novel, high-cost frontier ideas—and is a major contributor to the generation→validation bottleneck.",
            "uuid": "e2174.1"
        },
        {
            "name_short": "Findings Memory",
            "name_full": "Findings Memory (Cumulative Knowledge Repository)",
            "brief_description": "A continuously expanding, structured database that stores both human frontier knowledge (papers, code) and the system's own Idea/Implement/Progress records, used to contextualize ideation and inform surrogate evaluation and acquisition.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Findings Memory",
            "system_type": "knowledge base / persistent memory component",
            "domain": "meta-scientific management within automated discovery",
            "generation_capability": "does not generate outputs but seeds future hypothesis generation by retrieval and contextualization",
            "validation_method": "serves as context for the surrogate LLM reviewer and ideation agents; its use is evaluated indirectly by improved selection and scaling behavior when shared across parallel workers.",
            "novelty_measure": "records stage labels (Idea, Implement, Progress) and metrics, enabling the system to detect novelty as lack of prior coverage and to prefer unexplored directions via the surrogate 'exploration' score.",
            "generation_performance": "N/A (passive memory), but system-level outcomes (more discoveries with shared memory across parallel workers) indicate high utility.",
            "validation_performance": "Contributes to improved selection and scaling (near-linear increase in progress findings with more GPUs when shared Findings Memory is used); no isolated metrics.",
            "false_positive_rate": "Not applicable directly; poor or noisy memory entries can mislead surrogate and agents but rates not quantified.",
            "false_negative_rate": "Not quantified.",
            "novelty_effect_on_validation": "A shared memory helps accelerate discovery by reusing failures and successes, reducing redundant exploration; this is especially helpful in novel search areas where previous failures are informative.",
            "generation_validation_asymmetry": "Findings Memory narrows the gap by preventing repeated testing of failed paths, but cannot eliminate implementation/validation errors.",
            "out_of_distribution_performance": "Enables better collective search across out-of-distribution regions by sharing intermediate findings across parallel workers.",
            "calibration_quality": "N/A",
            "validation_computational_cost": "Low relative to experiments (storage and retrieval overhead); retrieval count K=15 used in system.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Acts as the principal memory mechanism to learn from failures and successes; shared across parallel agents to produce synergistic effects.",
            "evidence_type": "supports",
            "key_findings": "A centralized Findings Memory that accumulates both human and machine findings materially improves search efficiency and enables near-linear scaling of discoveries with parallel compute, helping mitigate the validation bottleneck by preventing redundant experiments.",
            "uuid": "e2174.2"
        },
        {
            "name_short": "Claude Code Agent",
            "name_full": "Claude Code Agent (Claude-4-Opus based code execution agent)",
            "brief_description": "A code-generation and execution agent (powered by Claude-4-Opus and Claude Code framework) responsible for implementing candidate ideas in sandboxed repositories and executing experiments to produce empirical results.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude Code agent (Claude-4-Opus)",
            "system_type": "large language model-based code synthesis and execution agent",
            "domain": "software implementation and experimental execution within automated discovery",
            "generation_capability": "generates implementation code, experiment scripts, and runs experiments in an isolated sandbox",
            "validation_method": "Executes experiments in sandboxed folders; after completion DeepScientist re-executes main scripts as secondary verification to reduce erroneous success reports; human supervisors manually inspect experimental results.",
            "novelty_measure": "N/A (implementation agent), but its reliability affects whether novel ideas can be validated successfully.",
            "generation_performance": "Used to implement ~1,100 ideas; observed high implementation failure rate (implementation-level errors were responsible for ~60% of failed trials). Approximately 50% of initial implementations failed to complete due to internal timeouts in Claude Code (leading to false-positive or incomplete reports).",
            "validation_performance": "Implementation-stage instability is a dominant source of validation failure; secondary verification (re-execution) reduced false positives but no formal success/failure ROC reported.",
            "false_positive_rate": "Operationally high at the raw implementation-step level: ~50% of initial runs failed to complete (timeouts) and required additional verification; precise end-to-end FPR after verification not reported.",
            "false_negative_rate": "Not quantified; some successful ideas might be marked failed if implementation agent crashed, but rate unknown.",
            "novelty_effect_on_validation": "More novel implementations often require code paths not present in baseline repositories, increasing fragility and implementation error rates.",
            "generation_validation_asymmetry": "Implementation agent is a primary contributor to asymmetry: many promising hypotheses fail not because the idea is wrong, but because of implementation errors, exacerbating the generation→validation gap.",
            "out_of_distribution_performance": "Implementation success degrades for ideas that require extensive unfamiliar code changes; paper reports many premature terminations for non-standard experiments.",
            "calibration_quality": "Not applicable beyond engineering logs; agent reported completion states were not fully reliable without secondary verification.",
            "validation_computational_cost": "Approximately 1 GPU-hour per implementation attempt on H800; Claude API cost ~ $20 per attempt. Median execution ~70 minutes reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Secondary verification (re-execution), sandbox isolation, human supervision, improved code-generation models recommended as future work to reduce implementation errors.",
            "evidence_type": "supports",
            "key_findings": "Implementation-level failures (agent timeouts, bugs) accounted for the majority (~60%) of exploratory failures, making robust implementation agents and secondary verification essential to converting generated ideas into validated scientific findings.",
            "uuid": "e2174.3"
        },
        {
            "name_short": "DeepReviewer",
            "name_full": "DeepReviewer: An LLM-based automated peer-review agent",
            "brief_description": "An automated reviewer agent designed to emulate human peer review with external search capability; used here both as a surrogate reviewer (DeepReviewer-14B) and as an automated benchmark to evaluate the quality of AI-generated research papers.",
            "citation_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process.",
            "mention_or_use": "use",
            "system_name": "DeepReviewer (DeepReviewer-14B variant)",
            "system_type": "large language model-based automated reviewer",
            "domain": "peer review and assessment of research outputs",
            "generation_capability": "generates review scores and textual reviews assessing soundness, presentation, contribution",
            "validation_method": "Evaluates papers via automated scoring and external literature search; used to benchmark DeepScientist outputs against 28 other AI Scientist papers and to produce acceptance-rate comparisons.",
            "novelty_measure": "Produces ordinal ratings and review scores as proxies for novelty and quality; used to compute an automated acceptance-rate metric.",
            "generation_performance": "Reported that DeepScientist's papers achieved a 60% acceptance rate under DeepReviewer evaluation (higher than other compared AI Scientist outputs); also used to produce structured valuation vectors in other contexts (related systems).",
            "validation_performance": "DeepReviewer produced numeric ratings and was used as an automated comparator; the paper also convened human expert reviews to corroborate DeepReviewer assessments. No formal calibration metrics for DeepReviewer reported in this paper.",
            "false_positive_rate": "Not quantified herein; potential for automated reviewer to over- or under-estimate novelty is discussed conceptually but not measured.",
            "false_negative_rate": "Not quantified.",
            "novelty_effect_on_validation": "Automated review recognizes ideation novelty but human reviewers flagged empirical/validation weaknesses that automated review may miss, indicating limits for automated review on rigorous experimental critique.",
            "generation_validation_asymmetry": "Shows that automated review can accept novel textual contributions but may not fully detect weak experimental rigor—further supporting the gap between ideation and validated evidence.",
            "out_of_distribution_performance": "Not quantified; used as a benchmark for machine-generated papers across varying novelty.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Operates via LLM inference and external search; per-paper cost not itemized separately in the study.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Used in tandem with human program committee to provide both automated and human assessments; suggested as part of future pipelines to scale review.",
            "evidence_type": "mixed",
            "key_findings": "Automated reviewers like DeepReviewer can identify and rate novel ideas and achieved a 60% acceptance-rate signal for DeepScientist outputs, but human reviewers detected recurring empirical weaknesses that automated review alone can miss.",
            "uuid": "e2174.4"
        },
        {
            "name_short": "A2P",
            "name_full": "A2P (Abduction-Action-Prediction) method",
            "brief_description": "A system-generated method for agent failure attribution that uses causal/counterfactual reasoning: infer the hidden cause (abduction), propose a corrective action, and predict whether that action would have led to success.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "A2P (system-generated algorithm)",
            "system_type": "algorithmic method for causal attribution",
            "domain": "LLM multi-agent failure attribution",
            "generation_capability": "a novel algorithm that produces counterfactual-based attribution predictions and fixes",
            "validation_method": "Implemented by DeepScientist and evaluated on the Who&When benchmark in both handcrafted and algorithm-generated settings, producing quantitative benchmark scores.",
            "novelty_measure": "Measured by benchmark performance improvement over prior SOTA and by human review of conceptual novelty (counterfactual/causal framing).",
            "generation_performance": "Achieved benchmark scores of 29.31 (handcraft setting) and 47.46 (algorithm-generated setting), establishing a new SOTA for that task as reported in the paper.",
            "validation_performance": "Validated empirically on Who&When; training-free A2P maintained SOTA as of September 2025 and outperformed even 7B models trained on synthetic data (per the paper).",
            "false_positive_rate": "Not reported numerically.",
            "false_negative_rate": "Not reported numerically.",
            "novelty_effect_on_validation": "The causal, zero-shot framework provided robust gains without additional training data, suggesting that methodological novelty can translate to reliable validation when the evaluation feedback is well-defined and inexpensive.",
            "generation_validation_asymmetry": "A2P is an example where ideation and validation aligned successfully—i.e., a generated novel method produced reproducible, validated improvement—contrasting with many other generated ideas that failed at implementation.",
            "out_of_distribution_performance": "Performed well on this frontier, training-free and generalizing beyond prior engineered heuristics.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Evaluated within existing benchmark runs; relative cost not separately quantified but described as training-free (low data/training cost).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Succeeded in part because the task evaluation was relatively cheap / well-specified and the method provided a principled causal framing, reducing reliance on expensive brute-force validation.",
            "evidence_type": "supports",
            "key_findings": "A2P demonstrates that when tasks provide clear, low-cost evaluation signals, a generated, novel algorithm can be validated and achieve large SOTA gains; this contrasts with high-cost tasks where validation bottlenecks dominate.",
            "uuid": "e2174.5"
        },
        {
            "name_short": "ACRA",
            "name_full": "ACRA (Autogenerated method for LLM Inference Acceleration)",
            "brief_description": "A DeepScientist-discovered method that improves LLM inference throughput by identifying stable suffix patterns and dynamically conditioning decoding guesses, effectively grafting a long-term memory onto decoders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ACRA (system-generated algorithm)",
            "system_type": "algorithmic method for LLM inference/decoding",
            "domain": "LLM inference acceleration",
            "generation_capability": "generates a decoding strategy that accelerates token generation while preserving accuracy",
            "validation_method": "Implemented on the MPBB/MBPP benchmark and measured in tokens/second throughput and correctness metrics.",
            "novelty_measure": "Measured by tokens/second performance improvement over human SOTA (TokenRecycling baseline), plus human review of conceptual novelty.",
            "generation_performance": "Improved throughput from human SOTA 190.25 tokens/sec to 193.90 tokens/sec as reported.",
            "validation_performance": "Empirically validated to provide a modest but measurable throughput gain while retaining lossless verification.",
            "false_positive_rate": "Not reported numerically.",
            "false_negative_rate": "Not reported numerically.",
            "novelty_effect_on_validation": "Gains were modest and achieved in a domain (inference speed) where rapid feedback and concrete numeric evaluation made validation feasible.",
            "generation_validation_asymmetry": "ACRA shows that in engineering-dominated, fast-feedback tasks the system can both generate and validate improvements; contrasted with high-cost or poorly-specified tasks where validation lags.",
            "out_of_distribution_performance": "Demonstrated incremental improvement on a specialized inference task; generalization to other model families not reported.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Experimentation used standard inference benchmarks; specific cost per experiment not isolated beyond general implementation costs (~1 GPU-hour per implement).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Benefited from a rapid-feedback experimental setting and focused exploitation of stable suffix patterns, reducing reliance on high-cost validation.",
            "evidence_type": "supports",
            "key_findings": "ACRA shows that system-generated engineering innovations can be validated in domains with fast, low-cost evaluation loops, producing modest SOTA gains.",
            "uuid": "e2174.6"
        },
        {
            "name_short": "T-Detect/TDT/PA-Detect",
            "name_full": "T-Detect, TDT (Time/Domain Transform), and PA-Detect (Phase Alignment Detect)",
            "brief_description": "A sequence of DeepScientist-generated AI-text detection methods progressing from robust statistical modeling (T-Detect) to signal-processing approaches (TDT, PA-Detect) that treat text as a non-stationary signal and analyze localized anomalies, culminating in SOTA detection performance and faster inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "T-Detect / TDT / PA-Detect (system-generated detection methods)",
            "system_type": "algorithmic detection methods (signal/statistics-based)",
            "domain": "AI-generated text detection",
            "generation_capability": "generate detection models/algorithms that output binary classification (human vs LLM-generated) and associated anomaly localizations",
            "validation_method": "Implemented and benchmarked on the RAID dataset; successive methods were evaluated for AUROC and latency (inference speed) and compared to existing human SOTA detectors.",
            "novelty_measure": "Measured by AUROC improvement over prior SOTA and by computational latency; novelty also judged by program committee reviewers.",
            "generation_performance": "Discovered three progressively superior methods within two weeks; final method achieved +7.9% AUROC over prior SOTA and also doubled inference speed as reported.",
            "validation_performance": "Validated empirically on RAID with AUROC and inference-latency tradeoffs; human reviewers praised ideation but noted experimental and contextual weaknesses in some generated papers.",
            "false_positive_rate": "Not numerically reported; ROC-based metrics (AUROC improvement) reported instead.",
            "false_negative_rate": "Not reported numerically.",
            "novelty_effect_on_validation": "Rapid iterative progression (T-Detect → TDT → PA-Detect) indicates that when a novel idea yields measurable metric gains, the system can quickly refine and validate further novelty; success benefited from concrete benchmark metrics and relatively fast experiments.",
            "generation_validation_asymmetry": "These methods exemplify cases where generation and validation aligned because the task allowed localized, numeric evaluation—contrasting with higher-cost tasks where alignment was poor.",
            "out_of_distribution_performance": "Demonstrated strong improvement on the RAID benchmark; generalization to other corpora not reported.",
            "calibration_quality": "Not reported.",
            "validation_computational_cost": "Evaluations performed in the standard detection benchmarking pipeline; per-experiment cost not separated beyond general system cost accounting.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Progressive refinement via Findings Memory and rapid low-to-high fidelity loop enabled the chain of improvements.",
            "evidence_type": "supports",
            "key_findings": "The chain of T-Detect→TDT→PA-Detect shows DeepScientist can quickly iterate on a promising novel direction when the task offers cheap, well-defined evaluation metrics, producing both higher accuracy and faster inference.",
            "uuid": "e2174.7"
        },
        {
            "name_short": "AI Scientist-V2",
            "name_full": "AI Scientist-V2: Workshop-level automated scientific discovery via agentic tree search",
            "brief_description": "A prior automated discovery system cited as related work; reported to produce published scientific artifacts but criticized here for undirected exploration lacking human-defined goal focus.",
            "citation_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.",
            "mention_or_use": "mention",
            "system_name": "AI Scientist-V2",
            "system_type": "LLM-based automated discovery system (agentic tree search)",
            "domain": "automated scientific discovery",
            "generation_capability": "reports to generate research artifacts and papers",
            "validation_method": "Reported in literature (Yamada et al., 2025); in this paper it is discussed qualitatively as having outputs sometimes judged naive under human evaluation due to lack of explicit goal orientation.",
            "novelty_measure": "Not quantified in this paper beyond citation and qualitative critique.",
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Critiqued for producing undirected discoveries that may lack human-perceived scientific value; suggests that goal-orientation affects validation and perceived novelty.",
            "generation_validation_asymmetry": "Mentioned as an example where generation may occur without rigorous goal-driven validation; no numeric data provided.",
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": null,
            "evidence_type": "neutral",
            "key_findings": "Cited as prior work showing capacity to generate research artifacts but highlighted as lacking a goal-driven approach that DeepScientist adopts to ensure scientifically meaningful validation.",
            "uuid": "e2174.8"
        },
        {
            "name_short": "AlphaEvolve",
            "name_full": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
            "brief_description": "A referenced system (DeepMind-style) that uses massive trial-and-error with known engineering methods to improve codebases and algorithms; cited as an engineering-optimization counterpart to scientific discovery systems.",
            "citation_title": "Alphaevolve: A coding agent for scientific and algorithmic discovery.",
            "mention_or_use": "mention",
            "system_name": "AlphaEvolve",
            "system_type": "automated coding/algorithmic discovery agent",
            "domain": "algorithm discovery / engineering optimization",
            "generation_capability": "generates algorithmic optimizations and code-level improvements",
            "validation_method": "Uses trial-and-error experiments within engineering contexts (as described in citation); in this paper it is mentioned as focusing on engineering-driven optimization within known paradigms rather than establishing new scientific paradigms.",
            "novelty_measure": null,
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Paper contrasts this engineering optimization approach with DeepScientist's aim to change foundational methods—suggesting different validation dynamics when working inside vs. outside established paradigms.",
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": null,
            "evidence_type": "neutral",
            "key_findings": "Cited to contrast engineering-heavy automated optimization systems with DeepScientist's goal-driven scientific-discovery focus; helps contextualize where validation may be cheaper (engineering) vs. expensive (frontier science).",
            "uuid": "e2174.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search.",
            "rating": 2
        },
        {
            "paper_title": "Alphaevolve: A coding agent for scientific and algorithmic discovery.",
            "rating": 2
        },
        {
            "paper_title": "The need for verification in ai-driven scientific discovery.",
            "rating": 2
        },
        {
            "paper_title": "Scaling laws in scientific discovery with ai and robot scientists.",
            "rating": 1
        },
        {
            "paper_title": "Deepreview: Improving llm-based paper review with human-like deep thinking process.",
            "rating": 2
        }
    ],
    "cost": 0.02406975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1</p>
<p>Yixuan Weng wengsyx@gmail.com 
Minjun Zhu zhu.minjun@westlake.edu.cn 
Qiujie Xie 
Qiyao Sun 
Zhen Lin 
Sifan Liu 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Engineering School
Westlake University</p>
<p>5 10 15 061 0.65 0.70 0.75 0.80 0.85</p>
<p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1CA0C9325E7EF8170420249F9A6A1FE33arXiv:2509.26603v1[cs.CL]
While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges.We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines.It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze".Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation.Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7%, 1.9%, and 7.9%.This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery.</p>
<p>Figure 1: Comparison of research progress timelines for AI text detection on the RAID (Dugan et al., 2024).The right panel shows that DeepScientist achieves progress in two weeks that is comparable to three years of human research (Su et al.;Bao et al., a;b;Hu et al., 2023) (left panel).All zero-shot methods, including the system-generated T-Detect, TDT, and PA-Detect, uniformly adopt Falcon-7B (Almazrouei et al., 2023) as the base model.Additionally, all methods produced by DeepScientist demonstrate higher throughput than the previous SOTA method, Binoculars (Hans et al., 2024).</p>
<p>INTRODUCTION</p>
<p>Scientific discovery is inherently a process of continuous exploration and trial-and-error, where vast amounts of time and effort are invested to push the boundaries of human knowledge forward by a small step.This principle of persistent, incremental advancement is visible across the history of technology.For example, the decades-long optimization of semiconductor manufacturing has seen the feature size of transistors systematically reduced from micrometers to single-digit nanometers (Moore, 1965).Similarly, the efficiency of photovoltaic cells has been continuously advanced over half a century, with myriad material and architectural innovations pushing conversion rates from nascent single-digit percentages ever closer to their theoretical limits (Green, 1993).These historical trajectories underscore a process where human scientists engage in decades of goal-directed, iterative work to advance the SoTA artifacts continuously.</p>
<p>Recently, the emergence of Large Language Models (LLMs) has propelled automated scientific discovery, where LLM-based AI Scientist systems take the lead in exploration (Xie et al., 2025b).With their powerful capacity for long-form generation and comprehension, LLMs enable end-toend, full-cycle automation in scientific discovery.This has inspired influential work such as AI SCIENTIST-V2 (Yamada et al., 2025), whose scientific artifacts have been published in top-tier conference workshops.However, in the absence of clearly defined scientific goals, current AI Scientist systems often fall into the trap of blindly recombining existing knowledge and methods.As a result, their research outputs frequently appear naive under human evaluation and lack genuine scientific value (Zhu et al., 2025c).AI Scientists are yet to solve human challenges.</p>
<p>To solve real-world challenges, We formally model the full cycle of scientific discovery as a goaldriven Bayesian Optimization problem, where the singular objective is to find a novel method that maximally improves a target performance metric.Building on this formulation, we introduce DeepScientist, a LLM-based agent system designed to explore progressively across the unknown space of possible candidate research methods to identify the optimal plan that maximizes a highly expensive-to-evaluate function of true scientific value.Specifically, DeepScientist employs an iterative workflow, together with a continuously expanding memory of prior research knowledge to efficiently manage uncertainty during exploration.It intelligently balances exploitation (deepening investigations into promising high-value directions) with exploration (venturing into uncharted areas to acquire new knowledge).Through large-scale parallel exploration, DeepScientist can generate innovative hypotheses and ultimately yield both valuable new methods and validation-proven scientific findings through continuous exploration.</p>
<p>We select three frontier scientific tasks (Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection ), take their state-of-the-art methods (ICML 2025Spotlight, ACL 2025Outstanding, ICLR 2024) as starting points, and ask DeepScientist to conduct continuous research.As shown in Figures 1 and 3, within a month-long cycle of exploration, validation, and iteration on 16 H800 GPUs, DeepScientist exceeds their respective human SOTA methods by 183.7% (Accuracy), 1.9% (Tokens/second), and 7.9% (AUROC) through autonomously redesigning core methodologies, rather than simply combining existing techniques (Section 4.1).</p>
<p>To understand how such progress emerged, we analyze DeepScientist's discovery logs, and formed a small program committee to review the generated papers (Section 4.2).These logs show that the system generated over 5,000 unique ideas, of which only 1,100 are selected for experimental validation, and just 21 ultimately lead to scientific innovations (Section 4.3).Moreover, through the scaling experiment on computational resource, we discover a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.</p>
<p>To our knowledge, we provide the first empirical demonstration of an automated full-cycle scientific discovery system capable of producing novel, SOTA-surpassing methods and continuously advancing scientific frontiers at a pace that substantially exceeds human researchers.Our findings reveal a stark reality: while the AI's exploratory speed is immense, its inherent success rate for innovation remains exceptionally low, making effective validation and filtering the new bottleneck at the frontier of automated science.Therefore, the central question of the field is no longer 'Can AI innovate?',but rather 'How can we efficiently guide its powerful, yet highly dissipative, exploratory process to maximize scientific return?'We hope this work can inspire the research community to develop AI Scientist systems with greater exploration efficiency to accelerate scientific discovery at a larger scale, paving the way for ground-breaking discoveries.</p>
<p>Replication and Optimization.A significant body of research focuses on engineering tasks that operate within established scientific frameworks.This includes replication-oriented works like Pa-perBench (Starace et al., 2025) and Paper2Agent (Miao et al., 2025), which aim to reproduce existing papers.Other works, such as Agent Laboratory (Schmidgall et al., 2025b) and MLE-Bench (Chan et al., 2024), tackle early-stage machine learning engineering problems.Similarly, systems like Al-phaTensor (Fawzi et al., 2022) and AlphaEvolve (Novikov et al., 2025) use massive trial-and-error with known engineering methods to improve the performance of codebases.The common goal of these efforts is engineering-driven optimization within an established scientific paradigm, enhancing existing systems without questioning their foundational assumptions.DeepScientist, in contrast, pursues scientific discovery by targeting the core limitations of the SOTA itself.Its objective is not to refine the current state-of-the-art, but to establish a new one by introducing fundamentally different methodologies.</p>
<p>Semi-Automated Scientific Assistance.The path toward automating scientific discovery begin not with replacing the scientist, but with assisting them, leading to the development of a paradigm of specialized AI tools for individual research tasks.Systems like CycleResearcher (Weng et al., 2025) handle writing, DeepReview (Zhu et al., 2025a) manages reviewing, and co-scientists (Gottweis et al., 2025;Penadés et al., 2025;Swanson et al., 2025;Baek et al., 2025) aid in hypothesis generation.These powerful tools address only isolated fragments of the scientific process, leaving the crucial loop of learning from failure and exploration to humans.In contrast, DeepScientist is an autonomous agent of inquiry, managing the entire end-to-end research cycle and closing the loop by learning from its own experiments and self-directing its research path.</p>
<p>Automated Scientific Discovery.Building on the capabilities of specialized assistants, a line of research pursue full, end-to-end research automation (Yang et al., 2023;Xie et al., 2025a).Pioneering efforts, such as the AI Scientist systems (Lu et al., 2024;Yamada et al., 2025) and subsequent work (Intology, 2025;Jiabin et al., 2025), successfully demonstrate that an AI system could manage the full research cycle and produce novel findings.However, their primary limitation often lies in their exploratory strategy, which lacks a specific scientific goal rooted in a field's grand challenges, resulting undirected discoveries that may be perceived as lacking genuine scientific value.Instead, DeepScientist is thus the first automated scientific discovery system that leverages a closedloop, iterative process to discover methods surpassing the human state-of-the-art.The exploration of DeepScientist is goal-oriented and insight-driven, beginning by identifying a recognized limitation in the human SOTA and then using failure attribution to ensure discoveries are both novel and scientifically meaningful.</p>
<p>DEEPSCIENTIST: A PROGRESSIVE SYSTEM FOR DISCOVERING</p>
<p>SOTA-SURPASSING FINDINGS</p>
<p>MODELING SCIENTIFIC DISCOVERY AS AN OPTIMIZATION PROBLEM</p>
<p>The fundamental goal of automated scientific discovery is to autonomously identify novel methods that yield significant advancements in a given scientific domain.This process can be formally conceptualized as a search for an optimal solution within a vast and unstructured space of possibilities.Let the space of all possible candidate research methods be denoted by I.Each individual method I ∈ I, such as a novel algorithm or a new model architecture, possesses an intrinsic scientific value.This value is determined by a latent, black-box true value function, f : I → R, which maps a method to its ultimate empirical impact.The objective of scientific discovery is therefore to find the optimal method I * that maximizes this function:
I * = arg max I∈I f (I)(1)
Unlike previously studied tasks such as early-stage machine learning (Schmidgall et al., 2025a), algorithmic design (Novikov et al., 2025;Lange et al., 2025), or scientific software development (Aygün et al., 2025), A defining characteristic of frontier scientific discovery is that each exploratory step demands immense computational and intellectual resources, making the evaluation of the true scientific value function, f (•), prohibitively costly.Any single evaluation, f (I),</p>
<p>Strategize &amp; Hypothesize Implement &amp; Verify Analyze &amp; Report</p>
<p>Paper Repositories  corresponds to a complete and resource-intensive research cycle of implementation, experimentation, and analysis, often consuming vast computational resources (e.g., on the order of 10 16 FLOPs for a frontier LLM problem, as illustrated in Figure 4.c).This extreme sample inefficiency renders brute-force or random exploration of the space I intractable.Therefore, we model the problem within the framework of Bayesian Optimization (Frazier, 2018;Garnett, 2023), which provides a principled methodology for global optimization of expensive black-box functions.By constructing a surrogate model to intelligently guide the search, Bayesian Optimization effectively reduces the number of costly real-world evaluations through a careful balance of exploration and exploitation.However, for scientific discovery, I is a conceptual space that is not explicitly defined.Candidate methods I must be formulated as creative, plausible, and coherent scientific hypotheses.The generation of high-quality candidate hypotheses is a critical bottleneck that traditional Bayesian Optimization algorithms are not designed to address.This challenge necessitates a new mechanism that integrates creative ideation with sample-efficient optimization.We detail our solution to this problem in the following subsections.</p>
<p>Human Findings DeepScientist Findings Memory</p>
<p>THE DEEPSCIENTIST FRAMEWORK</p>
<p>The architecture of DeepScientist actualizes the Bayesian Optimization loop through a multi-agent system equipped with an open-knowledge system and a continuously accumulating Findings Memory.This memory is composed of both frontier human knowledge (e.g., papers and codes) and the system's own historical findings, and it intelligently guides subsequent explorations.The entire discovery process is structured as a hierarchical and iterative three-stage exploration cycle.In this hierarchical scheme, only research ideas that exhibit promise are advanced to more expensive evaluations, while others are retained in the Findings Memory to inform subsequent explorations.This design ensures the computational resources are dynamically and precisely allocated to the most promising scientific trajectories, thereby maximizing discovery efficiency under constrained budgets.Specifically, each stage within the three-stage exploration cycle is associated with a distinct fidelity-cost tradeoff (Figure 2):</p>
<p>Strategize &amp; Hypothesize.Each research cycle begins by analyzing the Findings Memory (M t ), a list-style database containing thousands of structured records.Each record represents a unique scientific finding, which is categorized according to its stage of development.To overcome the LLM's context length constraints, we use a separate retrieval model (Wolters et al., 2024) when needed to select the Top-K Findings as input.The vast majority of records begin as Idea Findings-unverified hypotheses.During this first stage, the system identifies limitations in existing knowledge and gen- erates a new collection of hypotheses (P new ), and then they evaluated by a low-cost Surrogate Model (g t ).The surrogate model (an LLM Reviewer) is first contextualized with the entire Findings Memory.It then approximates the true value function f and, for each candidate finding I ∈ P new , produces a structured valuation vector V = ⟨v u , v q , v e ⟩, quantifying its estimated utility, quality, and exploration value as integer scores on a scale of 0 to 100.Each new hypothesis and its valuation vector is then used to initialize a new record in the Findings Memory as an "Idea Finding".</p>
<p>Implement &amp; Verify.This stage serves as the primary filter in the Findings Memory.To decide which of the numerous "Idea Findings" warrants the significant resource investment to be advanced in a real-world experiment, the system employs an Acquisition Function (α).Specifically, it uses the classic Upper Confidence Bound (UCB) algorithm to select the most promising record.The UCB formula maps the valuation vector V to balance the trade-off between exploiting promising avenues (represented by v u and v q ) and exploring uncertain ones (represented by v e ):
I t+1 = arg max I∈Pnew w u v u + w q v q Exploitation Score +κ • v e Exploration Score ,(2)
where w u and w q are hyperparameters and κ controls the intensity of exploration.The highestscoring finding I t+1 is selected for validation, and its record is promoted to the status of an Implement Finding.A coding agent then performs a repository-level implementation to executed the experiment.This agent operates within a sandboxed environment with full permissions, allowing it to read the complete code repository and access the internet for literature and code searches.Its objective is to implement the new hypothesis on top of the existing SOTA method's repositories.</p>
<p>The agent typically begins by planning the task, then reads the code to understand its structure, and finally implements the changes to produce the experimental logs and results.The experiment logs and results, f (I t+1 ), is used to update the corresponding record, enriching it with empirical evidence and thus closing the learning loop.</p>
<p>Analyze &amp; Report.The final and most selective stage of the Findings Memory is triggered only by a successful validation.When an "Implement Finding" succeeds in surpassing the baseline, its record is promoted to a Progress Finding.This transformation is implemented by a series of specialized agents capable of utilizing a suite of MCP (Hou et al., 2025) tools.These agents first autonomously design and execute a series of deeper analytical experiments (e.g., ablations, evaluations on new datasets), leveraging MCP tools to manage the experimental lifecycle, data collection, and result parsing.Subsequently, a synthesis agent employs the same toolset to collate all experimental results, analytical insights, and generated artifacts into a coherent, reproducible research paper.This deeply validated record becomes a new record in the system's knowledge base, thus influencing the decision-making process in all subsequent cycles.</p>
<p>EXPERIMENTS</p>
<p>As detailed in Table 1, we select three distinct SOTA methods (published in 2024 and 2025) as starting points, chosen for their frontier status, community interest, and human supervisability.Each SOTA method is manually reproduced, and we preserve execution logs and test scripts to allow DeepScientist to focus on research advancement.DeepScientist is provided with two servers, each with 8 Nvidia H800 GPUs.To maximize utilization, we launch a separate system instance for each GPU, employing the Gemini-2.5-Promodel for core logic and the Claude-4-Opus model for its robust code-generation capabilities.Three human experts supervise the process to verify outputs and filter out hallucinations.For more implementation details, please see Appendix C.</p>
<p>DEEPSCIENTIST ACHIEVEMENTS ON THREE RESEARCH DOMAINS</p>
<p>Agents Failure Attribution.The task addresses the question: in an LLM-based multi-agent system, which agent caused the task to fail and when?Starting from the baseline "All at once" method (Zhang et al., 2025c), DeepScientist identified that the current approach lacks the counterfactual reasoning capabilities essential for attribution.Through a process of trial, error, and synthesizing new findings-discovering the effectiveness of hypothetical prediction and simulated attempts-it ultimately proposed the A2P method.Named for its Abduction-Action-Prediction process, its core innovation elevates failure attribution from pattern recognition to causal reasoning, filling the critical gap in counterfactual capabilities by predicting if a proposed fix would have led to success.As shown in Figure 3.(a-b), A2P achieved scores of 29.31 and 47.46 in the "handcraft" and "algorithm-generated" settings of the Who&amp;When benchmark, respectively, setting a new state-of-the-art (SOTA).In this task, DeepScientist validated that a structured, zero-shot causal reasoning framework can be superior to less principled methods.As of September 2025, the training-free A2P method maintains its SOTA position, outperforming even 7B models trained on synthetic data.(Zhang et al., 2025a).</p>
<p>LLM Inference Acceleration is a highly optimized field aiming to maximize throughput and reduce latency during LLM inference (Xia et al., 2024).In this process, the system actively made many different attempts, such as using a Kalman Filter (Zarchan, 2005) to dynamically adjust an adjacency matrix to address the original method's lack of a memory function.Although most of these attempts failed, the system-generated ACRA method ultimately advanced the MPBB (Austin et al., 2021) from a human SOTA of 190.25 to 193.90 tokens/second by identifying stable suffix patterns, as shown in Figure 3. Scientifically, this innovation is significant because it uses this extra contextual information to dynamically adjust the decoding guess, effectively grafting a long-term memory onto the process and breaking the context-collapsing of standard decoders.This discovery highlights the system's primary goal: the creation of new, human-unknown knowledge rather than mere engineering optimization.For instance, one could likely achieve greater performance gains by combining ACRA with an established technique like layer skipping (Wang et al., 2022) or PageAttention (Kwon et al., 2023), but this would represent an engineering effort, not a scientific one.The exploration assessment within our process avoids such combinations of existing knowledge.</p>
<p>Table 2: Evaluation of AI-generated papers produced by various AI Scientist systems.Scores represent the average ratings given by DeepReviewer-14B (Zhu et al., 2025a) across the number ("Num") of available papers.Note: Publicly available papers may be curated and therefore may not fully represent the typical output of each system.AI Text Detection is a binary classification task where, given a text that may contain content from an LLM (and possibly additional noise), the goal is to determine if it was produced by a human or an LLM (Li et al., 2022;Ghosal et al., 2023).To validate its capacity for sustained advancement, DeepScientist made numerous attempts that included addressing the Boundary-Aware Extension problem and exploring approaches like Volatility-Aware and Wavelet Subspace Energy methods.</p>
<p>The final results show a dramatic acceleration in scientific discovery: in a rapid evolution over just two weeks, the system produced three distinct, progressively superior methods (T-Detect, TDT, and PA-Detect).This began with T-Detect fixing core statistics with a robust t-distribution, then evolved conceptually with TDT and PA-Detect, which treat text as a signal and use wavelet and phase congruency analysis to pinpoint anomalies.Scientifically, this shift reveals the "non-stationarity" of AI-generated text, alleviating the information bottleneck in prior paradigms that average away localized evidence.As shown in Figure 1 and 3(d), this entire discovery trajectory demonstrates DeepScientist's ability for advancing frontier-pushing scientific findings progressively, establishing a new SOTA with a 7.9% higher AUROC while also doubling the inference speed.</p>
<p>ASSESSING THE QUALITY OF AI-GENERATED RESEARCH PAPER</p>
<p>Experimental Setup.To assess the quality of the final output, we evaluate the five research papers autonomously generated by DeepScientist's end-to-end process.Our evaluation protocol is twofold.First, to benchmark against existing work, we employ DeepReviewer (Zhu et al., 2025a), an AI agent that simulates the human peer-review process with an external search capability, comparing Deep-Scientist's output against 28 publicly available papers from other AI Scientist systems.Second, for a more rigorous assessment, we convene a dedicated program committee consisting of three active LLM researchers: two volunteers who have served as ICLR reviewers and one senior volunteer who has been invited to be an ICLR Area Chair.Human Expert Evaluation.The evaluation from our human program committee, shown in Table 3, reveal a remarkable and unanimous consensus: DeepScientist consistently excels at ideation, the most challenging and often rate-limiting step in human-led research.Full details on the review protocol are provided in Appendix A, and the core ideas within each paper are praised for their genuine novelty, ingenuity, and scientific contributions.The quality of these innovations is further demonstrated by the review scores: the system's average rating (5.00) closely mirrors the average of all ICLR 2025 submissions (5.08), with two of its papers significantly exceeding this (5.67).</p>
<p>ANALYSIS OF THE ITERATIVE TRAJECTORY OF AUTONOMOUS EXPLORATION</p>
<p>Experimental Setup.The findings in this section are derived from a series of post-hoc analyses conducted on the complete operational data generated by DeepScientist across the three frontier tasks.This data includes the full set of execution logs and the Findings Memory, providing the basis for all subsequent statistical analysis.To visualize the conceptual search space (Figure 5), we embed the complete description of each generated finding using the Qwen3-Embedding-8B model.</p>
<p>To assess scalability (Figure 6), we conduct a dedicated one-week experiment where N identified limitations of a single SOTA method are assigned to N parallel GPU instances.These instances explore solutions independently but share their findings to a central database, which are synchronized globally every five cycles to accommodate the asynchronous nature of the discovery process.Finally, to better understand the low success rate, our program committee experts perform a detailed causal attribution analysis on a sample of 300 failed implementations.</p>
<p>Our analysis of DeepScientist's experimental logs reveals the sheer scale of the trial-and-error process inherent in autonomous scientific discovery.Even in our relatively fast-executing domains, achieving progress required hundreds of trials per task.As show in Figure 4, the execution time distributions show that while individual experiments may be quick, the sheer volume of trialand-error necessary to uncover a successful idea is substantial.This suggests a clear application boundary for current autonomous science: for tasks with rapid feedback loops, such as knowledge editing or aspects of chip design, delegating massive-scale experimentation to AI is a powerful strategy.However, for high-cost endeavors like pre-training foundation models or pharmaceutical synthesis, the low success rate makes such an approach currently impractical, mandating continued reliance on human-led ideation.The autonomous research process is characterized by a vast exploratory funnel where promising ideas are exceptionally rare.Across the three tasks, DeepScientist generate over 5,000 unique ideas, yet only about 1,100 are deemed worthy of experimental validation by the system's selection mechanism, and a mere 21 ultimately result in scientific progress.An ablation study underscores the criticality of this selection process: without it, randomly sampling 100 ideas for each task and testing them yields a success rate of effectively zero.With our selection strategy, the success rate rises to approximately 1-3%, demonstrating that while still low, intelligent filtering is essential.The low success rate is not merely a matter of failed hypotheses; analysis by human experts on a sample of failed trials reveals that approximately 60% were terminated prematurely due to implementation errors, while the vast majority of the remaining 40% simply offered no performance improvement or caused a regression.This highlights that the probability of an LLMgenerated idea being both correct in its premise and flawless in its implementation is exceedingly  low.The success of this work, therefore, is not a product of brute-force computation but of search efficiency.A naive approach of fully testing all 5000 promising candidates would have required over 100,000 GPU hours, whereas our targeted exploration achieved its breakthroughs using only 20,000.</p>
<p>DeepScientist's discovery process follows a purposeful and progressive trajectory.The semantic distribution of ideas generated for the AI text detection task, as shown in Figure 5, reveals the characteristics of this sophisticated strategy.While the system generates thousands of diverse ideas across a vast conceptual landscape, its path to success is not random but is a series of focused, logical advancements.This indicates a capacity to progressively deepen its understanding: after achieving an initial breakthrough with T-Detect, the system effectively establishes a SoTA, identifies its subsequent limitations, and reorients its search towards a new goal.This dynamic exploration is exemplified by the conceptual shift towards TDT and PA-Detect, which build upon the previous success by leveraging new positional and temporal information.This ability to build upon its own discoveries, turning each successful finding into a new starting point for identifying and solving the next set of limitations, demonstrates a powerful capacity for scientific exploration.</p>
<p>Scaling Laws in DeepScientist's Scientific Discovery.To investigate the relationship between computational scale and the rate of scientific progress, we evaluated the number of "Progress Findings" generated by DeepScientist within a fixed one-week period as a function of available parallel resources in Figure 6.In this setup, the system first identified a set of limitations in the baseline method, and each parallel exploration path was tasked with resolving a distinct limitation, with all paths periodically synchronizing their results into a shared Findings Memory.Our results indicate a promising scaling trend.While minimal resources yielded no breakthroughs, the rate of discovery began to increase effectively as we scaled to 4 GPUs and beyond, growing from one SOTAsurpassing finding with 4 GPUs to eleven with 16 GPUs.This appears to establish a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.We hypothesize this efficiency stems from more than just parallel trial-and-error; it is a direct result of the shared knowledge architecture.As each parallel path explores, it enriches the shared Findings Memory.This creates a synergistic effect where the collective intelligence of the system grows (Schmidgall &amp; Moor, 2025;Zhang et al., 2025b), allowing each independent path to benefit from the successes and, just as importantly, the failures of others.This suggests that effectively scaling autonomous science is not just a matter of increasing brute-force computation, but of fostering a richer, interconnected knowledge base that accelerates discovery across all concurrent efforts.</p>
<p>DISCUSSION</p>
<p>The results from DeepScientist suggest a new paradigm in scientific exploration.The system's 1-5% progress rate mirrors the reality of frontier research, where breakthroughs are inherently rare.Its core strength is not infallibility, but the ability to conduct this trial-and-error process at a scale and speed previously unimaginable, compressing years of human exploration into weeks.The primary path forward, therefore, is to focus on systematically improving this discovery efficiency, enhancing both the quality of generated hypotheses and the robustness of their implementation.</p>
<p>This challenge highlights a powerful opportunity for human-AI synergy.We envision a future where DeepScientist serves as a massive-scale exploration engine, with its trajectory guided by human intellect.The role of human researchers can shift from laborious experimentation to the high-level cognitive tasks of formulating valuable scientific questions and providing strategic direction, thereby leveraging the AI for rapid, exhaustive exploration.To make the AI a more capable partner, future work should focus on key enhancements: developing simulated discovery environments to accelerate learning via reinforcement, creating frameworks for integrating feedback from the scientific community, and ultimately, bridging the gap to the physical sciences through robotics.</p>
<p>CONCLUSION</p>
<p>This work presents the first large-scale empirical evidence that an autonomous AI can achieve progressively, SOTA-surpassing progress on modern scientific frontiers.We introduced DeepScientist, a goal-oriented system achieving end-to-end autonomy from ideation to real progress, which learns by synthesizing human knowledge with its own findings from iteration of trials.Results across multiple domains serves to accelerate the progress of real-world scientific discovery, providing a crucial foundation.Our findings can signal a foundational shift in AI research, heralding an era where the pace of discovery is no longer solely dictated by the cadence of human thought.</p>
<p>ETHICS STATEMENT</p>
<p>The development of DeepScientist, an autonomous system capable of advancing scientific frontiers, carries profound ethical responsibilities.Our primary goal is to accelerate discovery for the benefit of humanity, but we recognize the potential for misuse.The most significant risks include the application of this technology to advance dangerous research and the potential degradation of the academic ecosystem.We have implemented specific, robust measures to address these concerns proactively.</p>
<p>A primary concern is the dual-use risk, where the system could be co-opted to accelerate research in harmful domains, such as developing novel toxins or malicious software.To assess and mitigate this, we conducted red-teaming exercises specifically targeting the generation of computer viruses.We tasked the system, powered by leading foundation models (including GPT-5, Gemini-2.5-Pro,and Claude-4.1-Opus in our testbed), with this malicious objective.In all instances, the underlying models exhibited robust safety alignment, refusing to proceed with the research.They correctly identified the task as illegal and harmful, and autonomously terminated the research cycle, demonstrating that foundation model safety protocols provide a critical defense layer.</p>
<p>We are also deeply conscious of the potential negative impact on the academic ecosystem.It is crucial to state that all results from DeepScientist presented in this paper, including code and experimental findings, have undergone rigorous human verification.Recognizing that others might neglect this critical oversight, we are adopting a selective open-sourcing policy to mitigate the risk of proliferating unreliable publications.We will open-source the core components that drive continuous discovery, as we believe their potential to accelerate progress for the community outweighs the risks.However, we will deliberately refrain from open-sourcing the "Analyze &amp; Report" module.This decision is made to prevent the automated generation of seemingly credible but scientifically unverified papers, thereby safeguarding the integrity of the academic record.</p>
<p>Ultimately, we envision DeepScientist as a powerful tool to augment, not replace, human intellect and judgment.To enforce this vision, our open-source components will be released under a license based on MIT, but with explicit addendums that codify our ethical framework.This license will strictly prohibit any use of the software for harmful research.Furthermore, it will legally require that a human user must supervise the entire operational process of DeepScientist and assumes full and final responsibility for all its outputs.By embedding these requirements directly into our terms of use, we aim to foster a research environment where AI-driven discovery proceeds with the necessary human accountability and ethical oversight.</p>
<p>A HUMAN EXPERT REVIEW</p>
<p>A.1 REVIEW PROCESS AND CRITERIA</p>
<p>To ensure a rigorous and impartial evaluation of the generated papers, we convened a small, dedicated program committee.The committee was composed of two active researchers who served as volunteer reviewers for ICLR 2025, and one senior researcher who had previously been invited to serve as an ICLR Area Chair.All committee members possess significant expertise in the field of Large Language Models.The entire review process, with the exception of a rebuttal phase, was designed to meticulously emulate the official standards of ICLR 2025.Each of the five papers generated by our system was assigned to the three reviewers for a thorough and independent assessment.The average review time for each paper was 55 minutes, during which reviewers were required to provide not only scores but also detailed written feedback, including a summary of the paper's strengths and weaknesses.</p>
<p>The evaluation was conducted on a custom-deployed review website where reviewers could not see each other's scores or feedback, ensuring that all initial assessments were made independently.The review form was structured to gather concise yet comprehensive feedback.First, reviewers were asked to state their Confidence in their review on a scale of 1 to 5. The core of the evaluation consisted of three sub-scores, each rated on a 1 to 4 scale: Soundness, assessing the technical correctness and experimental rigor; Presentation, evaluating the clarity and quality of the writing; and Contribution, measuring the significance and novelty of the work.Finally, reviewers provided a holistic Rating on a scale of 1 to 10, where a score of 5 represented a 'borderline reject' and a score of 6 represented a 'borderline accept'.</p>
<p>After the three reviewers submitted their independent evaluations for a paper, the volunteer acting as Area Chair would then read all submitted reviews.Drawing upon their experience from the ICLR review process, the Area Chair synthesized the feedback, weighed the arguments presented by the reviewers, and made a final executive decision on whether the paper should be accepted or rejected in the context of our study.This final decision was recorded as the definitive outcome for each paper's evaluation.</p>
<p>A.2 SUMMARY OF REVIEWER FEEDBACK</p>
<p>Across the five generated papers, a clear consensus emerged from the human reviewers: Deep-Scientist consistently excels at the ideation stage of research.The committee unanimously lauded the methods for their genuine novelty and tangible contributions, noting that each paper proposed a unique approach that meaningfully advanced the state-of-the-art in its respective subfield.This feedback validates the system's core strength as a powerful engine for identifying relevant research gaps and generating innovative, impactful solutions, confirming that it can successfully ideate beyond mere incremental improvements.</p>
<p>However, this strength in ideation was systematically undermined by a recurring pattern of weaknesses in scientific execution and rigor.The most critical and frequent concern was a lack of empirical soundness; reviewers consistently noted that DeepScientist failed to design comprehensive validation plans, citing insufficient evaluation on standard benchmarks and a lack of in-depth analytical experiments (e.g., ablations, motivation studies) to justify its claims.This was compounded by a failure to properly contextualize its contributions, with papers often omitting comparisons to essential baselines or failing to discuss closely related work, thereby weakening the perceived significance of the results.</p>
<p>This feedback pinpoints the primary bottleneck in current autonomous systems: a profound gap between the ability to generate novel concepts and the capacity for rigorous scientific execution and articulation.The observed weaknesses in experimental design directly reflect the low-success-rate problem discussed previously; the system struggles not just to implement ideas correctly, but to validate them convincingly.To bridge this gap, future work must endow these systems with a deeper, procedural understanding of the scientific method itself.This requires moving beyond simple implementation and reporting capabilities towards two key areas: First, developing agents explicitly trained in experimental design, capable of planning comprehensive evaluations that anticipate and address potential scientific critiques.Second, enhancing the system's ability for analytical reasoning, enabling it to not just describe results but to interpret their significance, formulate compelling arguments, and engage in the kind of deep, reflective discussion that characterizes high-impact research.</p>
<p>B ADDRESSING THE BOTTLENECKS IN AUTONOMOUS SCIENTIFIC DISCOVERY</p>
<p>The ever-increasing value of LLM is reshaping the paradigm of scientific exploration through their ability to generate hypotheses at a massive scale (Li &amp; Weng, 2022;Weng et al., 2023;Weng et al.;Wei et al., 2024;Weng et al., 2024;Berkovich et al., 2025;Zhu et al., 2025b).Consequently, this capability has pushed "verification" to the center stage, making it a critical bottleneck in the discovery process.Our research empirically reveals the severity of this challenge: on frontier scientific tasks, the success rate of ideas generated by AI systems that ultimately lead to substantial progress is typically below 3%, meaning the vast majority of computational resources are consumed exploring low-value hypotheses.This inefficient "needle in a haystack" model is the core obstacle preventing AI Scientists from evolving from "novel tools" to "efficient discoverers."(Cornelio et al., 2025) Therefore, to further accelerate the process of scientific discovery, future research must focus on constructing a systematic solution to overcome this bottleneck.As shown in Figure 7, future AI Scientist systems need to evolve synergistically in three key directions: optimizing the quality of initial hypotheses (Optimize Hypothesis Quality), enhancing filtering capabilities during the process (Enhance Filtering), and improving the quality of implementation and verification at the final stage (Improve Implementation Quality).One of the core future research directions is to develop AI systems capable of generating higherquality, more reliable hypotheses (as shown in Figure 7e), equipped with more precise filtering mechanisms to predict their success rate (as shown in Figure 7d).Methods that rely purely on a data-driven approach, while capable of discovering patterns, often produce outputs that lack a theoretical foundation and are prone to generating "hallucinations" that contradict known scientific theories.Future systems must move beyond this by more deeply integrating background knowledge and theory.For instance, the direction represented by "derivable models" (such as AI-Descartes (Cornelio et al., 2023) and AI-Hilbert (Cory-Wright et al., 2024)), which incorporate scientific axioms as constraints during the hypothesis generation phase, offers a promising path to improving hypothesis quality.Furthermore, systems must have the ability to learn from their own exploratory history.By establishing mechanisms similar to a "Findings Memory," a system can systematically record and analyze every success and failure, thereby avoiding redundant exploration of ineffective paths in subsequent iterations and gradually developing a more insightful scientific intuition.Building on this foundation, developing more advanced, low-cost surrogate models and acquisition functions to more accurately predict the scientific value of an idea will be key to enhancing filtering efficiency and conserving verification resources.</p>
<p>Hypotheses Implement &amp; Verify</p>
<p>Concurrently, an often-overlooked yet crucial future research direction is to significantly improve the quality and reliability of AI systems in the engineering implementation and verification stages (as shown in Figure 7c).Even the most brilliant scientific concept can never have its value confirmed if it cannot be accurately and flawlessly translated into an executable experiment.Our analysis indicates that up to 60% of exploratory failures stem from implementation-level errors, which represents a massive waste of resources and directly impedes scientific progress.History has repeatedly warned us that a lack of rigorous verification can lead to catastrophic consequences, whether in NASA missions or medical practice.Therefore, building a scalable and reliable automated verification platform is an essential path forward.This requires not only more powerful code-generation and self-debugging agents to reduce implementation errors but also standardized sandbox environments and automated testing procedures to ensure the stability and reproducibility of experimental results.</p>
<p>Ensuring the absolute reliability of the verification process is the final and most critical line of defense in transforming AI-generated "plausible ideas" into "solid scientific evidence."</p>
<p>Looking ahead, to truly accelerate scientific discovery, it is necessary to integrate the aforementioned strategies into an organic whole, advancing AI Scientists from "random explorers" to "goaloriented strategists."This is not about replacing humans with AI, but about pioneering a more efficient paradigm of human-AI collaboration.In this model, human scientists are responsible for defining grander, more valuable scientific goals and providing high-level strategic guidance, while the AI system serves as a powerful "exploration engine," executing efficient trial-and-error and verification cycles at an unprecedented scale and speed under human direction.To realize this vision, the community must also address a series of challenges, such as building benchmarks that can truly evaluate innovation and designing mechanisms that encourage diverse exploration to avoid the homogenization of research paradigms, thereby preserving the potential for serendipitous discoveries like Alexander Fleming's discovery of penicillin (Fleming, 1941).</p>
<p>C IMPLEMENTATION DETAILS</p>
<p>Our implementation relies on a distributed architecture to manage the distinct tasks of scientific reasoning and code execution.The core logic of DeepScientist is powered by the Gemini-2.5-promodel, while all code implementation tasks are delegated to Claude-4-opus, executed within the Claude Code framework (v1.0.53).To ensure stability and security, the DeepScientist system and the Claude Code agent are isolated in separate Docker containers, communicating via a port-based API.During the 'Implement &amp; Verify' stage, a human-verified baseline code repository is first duplicated into a new, sandboxed folder.The Claude Code agent's operations are strictly confined to this new directory to prevent unintended modifications.A critical step in our pipeline is a secondary verification process: after Claude Code reports completion, DeepScientist independently re-executes the main script via the command line.This measure was implemented to counteract a high rate of false positives-we observed that approximately 50% of initial implementation attempts failed to complete fully due to internal timeouts within the Claude Code agent.Throughout this project, all experimental results were manually inspected by human supervisors to guarantee their authenticity.</p>
<p>For the 'Analyze &amp; Report' stage, a similar process is followed: the validated code is replicated for each analytical experiment, with Claude Code executing them sequentially.Upon completion, DeepScientist aggregates all results, generates a paper outline, and then employs automated tools to write and compile the final PDF manuscript.For all experiments, we used a fixed set of hyperparameters: the retrieval count was set to K = 15, and the UCB parameters were set to utility weight w u = 1, quality weight w q = 1, and exploration coefficient κ = 1.</p>
<p>Figure 2 :
2
Figure 2: The autonomous, closed-loop discovery process of DeepScientist.The system iterates through a three-stage cycle, learning from both human knowledge and its own experiments.</p>
<p>Figure 3 :
3
Figure 3: Performance evaluation of DeepScientist across three research domains: (a-b) Agent Failure Attribution on Who&amp;When benchmark in handcraft and algorithm-generated settings; (c) LLM Inference Acceleration on MBPP dataset; (d) AI Text Detection with performance-latency tradeoff analysis.DeepScientist (shown in pink) consistently outperform human-designed SoTA approaches (shown in blue) across all tasks.</p>
<p>Figure 4 :
4
Figure 4: DeepScientist's experimental statistics.(a) The research pipeline from generated ideas to validated progress.(b) Success rates comparing our selection strategy against a baseline.(c) Distribution of wall-clock execution times for all implemented trials.</p>
<p>Figure 5 :
5
Figure5: Visualization of the conceptual search space for the AI text detection task.The plot shows a t-SNE visualization of the semantic embeddings for all 2,472 generated ideas.Markers identify the initial SOTA method (Initial Idea) and the three final SOTA-surpassing methods (Progress Ideas).</p>
<p>Figure 6 :
6
Figure6: Scaling analysis of autonomous scientific discovery.The plot illustrates the relationship between parallel computational resources (number of GPUs) and the number of SOTA-surpassing "Progress Findings" found by DeepScientist across all tasks within a one-week period.</p>
<p>Figure 7 :
7
Figure 7: Three strategies for improving the efficiency of autonomous scientific discovery.(a) and (b) illustrate the low success rate currently faced by both AI and human research.Future directions will need to accelerate the discovery process through the synergy of three approaches: (c) improving implementation success rates, (d) adding an efficient filtering stage before implementation, and (e) optimizing the quality of initial hypotheses from the source.</p>
<p>t e r a t i o n
IdeaImplementProgressFindingsFindingsFindings|---TODO.md|---Result.md|---USAGDP.csv |---plot_gdp.py.........Motivation I Human Findings Findings Search Papers DeepScientist+1-run.sh -src -logs -latex.tex -main.py -plan.md FILES Code Repositories if use_TDT: model = TDT model = Model from models import TDT main.py-Replace -Delete -ADDOpen Source PDF CompileDemo Create Review SelfExecutableExperimentEvidenceHypothesisCodeLogModel SurrogateFeasibility Effectiveness ExplorationSuccess FailedSuccess FailedOutlineHyper-parametersAblationExp EnvironmentCode Experiments Figure</p>
<p>Table 1 :
1
Overview of the three different human SOTA methods we selected.
TaskMethodVenueBenchmark Github StarAgents Failure Attribution All at OnceICML 2025 SpotlightWho&amp;When 302LLM Inference Accel.TokenRecycling ACL 2025 Outstanding MBPP323AI Text DetectionFastDetectGPTICLR 2024RAID414</p>
<p>Table 3 :
3
Evaluation of DeepScientist's papers produced by human experts.Values are presented as mean (variance) from three reviewers.Inter-rater reliability for Rating: Krippendorff's α = 0.739.
PaperConfidenceSoundnessPresentationContributionRatingHUMAN Avg. (ICLR 2025)-2.592.362.625.081. T-DE T E C T4.33 (0.33)2.00 (1.00)2.67 (0.33)2.67 (0.33)5.00 (0.00)2. TDT4.67 (0.33)3.00 (0.00)3.00 (0.00)3.00 (0.00)5.67 (0.33)3. PA-DE T E C T4.00 (0.00)1.67 (0.33)2.00 (1.00)2.00 (1.00)4.33 (1.33)4. A2P4.00 (0.00)3.00 (0.00)3.00 (0.00)2.67 (0.33)5.67 (0.33)5. ACRA3.33 (0.33)1.67 (0.33)2.00 (1.00)1.67 (0.33)4.33 (1.33)DeepScientist Avg.4.072.272.532.405.00</p>
<p>Average Execution Time of Implemented Ideas
0 500 1000 1500 2000 2500 Number of IdeasAI Text Detection 7 600 2,472 (a) Research Idea Pipeline Statistics Agents Failure Attribution LLM Inference Acceleration 12 2 196 312 1,077 1,330 Progress Ideas Implemented Ideas Total Ideas0 95 100 90 10 2 4 6 8 Success Rate (%)0 50 100 150 200 250 300 Execution Time (minutes) (b) Progress Success Rate (Progress/Implemented) AI Text Detection Agents Failure Attribution LLM Inference Acceleration with Selected w/o SelectedAI Text Detection n = 600 (c) Median Agents Failure Attribution LLM Inference Acceleration n = 196 n = 312 Mean Q1 Q3
The generated papers are available in Appendix C. Automated Review Against Other AI Scientist Systems.As shown in Table2, the results from the LLM-based automatic evaluation indicate that the system's outputs are recognized for their scientific novelty and value.When benchmarked against 28 publicly available papers from other AI Scientist systems using DeepReviewer, DeepScientist is the only AI Scientist system to produce papers that achieves a 60% acceptance rate.</p>
<p>ACKNOWLEDGEMENTSWe are grateful to Professor Linyi Yang for his insightful discussions on this paper.This work is inspired by pioneering efforts in automated scientific discovery, including AI Scientist(Lu et al., 2024;Yamada et al., 2025)and AlphaEvolve (Novikov et al., 2025).https://ai-researcher.netCode: https://github.com/ResearAI/DeepScientistThe financial and computational costs of this autonomous discovery process are substantial.Each idea generated during the 'Strategize &amp; Hypothesize' stage incurred an approximate cost of $5 in API calls.For each attempt in the 'Implement &amp; Verify' stage, the cost averaged $20 for Claude-4-opus API usage, in addition to the computational cost of approximately 1 GPU hour, as detailed in Figure ??.c.A successful finding that progressed to the 'Analyze &amp; Report' stage required a further expenditure of around $150, which includes $100 for running analytical experiments and $50 for the final report generation.The total cost to achieve the scientific advancements presented in this paper amounted to approximately $100,000.While significant, we believe these costs can be substantially reduced.We recommend that future iterations explore more economical alternatives, such as deploying high-throughput models like Qwen-3-Next-80B for the core DeepScientist system and leveraging subscription-based API access (e.g., Claude Max or OpenAI Pro) to mitigate per-call expenses.In this paper, each implementation was provided with a single H800 server for exploration.Since the H800 GPU has an FP16 computing power of approximately 2 TFLOPS, an average execution of 70 minutes corresponds to about 1 × 10 16 floating-point operations.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.
The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>An ai system to help scientists write expert-level empirical software. Anastasiya Eser Aygün, Gheorghe Belyaeva, Marc Comanici, Hao Coram, Jake Cui, Renee Garrison, Anton Johnston, Cory Y Kast, Peter Mclean, Zahra Norgaard, David Shamsi, James Smalling, Subhashini Thompson, Brian P Venugopalan, Chujun Williams, Sarah He, Martyna Martinson, Lai Plomecka, Yuchen Wei, Qian-Ze Zhou, Matthew Zhu, Erica Abraham, Anna Brand, Jeffrey A Bulanova, Chris Cardille, Scott Co, Grace Ellsworth, Malcolm Joseph, Ryan Kane, Johan Krueger, Dan Kartiwa, Jan-Matthis Liebling, Paul Lueckmann, Raccuglia, Xuefei, Katherine Wang, James Chou, Yossi Manyika, John C Matias, Lizzie Platt, Shibl Dorfman, Michael P Mourad, Brenner, 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas Chapterthe Association for Computational Linguistics20251</p>
<p>Glimpse: Enabling white-box methods to use proprietary models for zero-shot llm-generated text detection. Guangsheng Bao, Yanbin Zhao, Juncai He, Yue Zhang, The Thirteenth International Conference on Learning Representations. </p>
<p>Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, The Twelfth International Conference on Learning Representations, b. </p>
<p>Automatagpt: Forecasting and ruleset inference for two-dimensional cellular automata. Jaime A Berkovich, Noah S David, Markus J Buehler, 2025</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R Josephson, Joao Goncalves, Kenneth L Clarkson, Nimrod Megiddo, Bachir El Khadir, Lior Horesh, 10.1038/s41467-023-37236-yNature Communications. 2041-17231411777April 2023</p>
<p>The need for verification in ai-driven scientific discovery. Cristina Cornelio, Takuya Ito, Ryan Cory-Wright, Sanjeeb Dash, Lior Horesh, 2025</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. Ryan Cory-Wright, Cristina Cornelio, Sanjeeb Dash, Bachir El Khadir, Lior Horesh, 10.1038/s41467-024-50074-wNature Communications. 155922July 2024</p>
<p>RAID: A shared benchmark for robust evaluation of machinegenerated text detectors. Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Discovering faster matrix multiplication algorithms with reinforcement learning. Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, J R Francisco, Julian Ruiz, Grzegorz Schrittwieser, Swirszcz, Nature. 61079302022</p>
<p>. Alexander Fleming, Penicillin. British medical journal. 242103861941</p>
<p>Frazier Peter, arXiv:1807.02811A tutorial on bayesian optimization. 2018arXiv preprint</p>
<p>Bayesian optimization. Roman Garnett, 2023Cambridge University Press</p>
<p>A survey on the possibilities &amp; impossibilities of AI-generated text detection. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, Amrit Bedi, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Silicon solar cells: evolution, high-efficiency design and efficiency enhancements. Martin A Green, Semiconductor science and technology. 199381</p>
<p>Spotting llms with binoculars: Zeroshot detection of machine-generated text. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, International Conference on Machine Learning. PMLR2024</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Radar: Robust ai-text detection via adversarial learning. Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho, Advances in neural information processing systems. 2023. 202536</p>
<p>Ai-researcher: Autonomous scientific innovation. Tang Jiabin, Xia Lianghao, Li Zhonghang, Huang Chao, 2025</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th symposium on operating systems principles. the 29th symposium on operating systems principles2023</p>
<p>Shinkaevolve: Towards open-ended and sample-efficient program evolution. Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin, arXiv:2509.193492025arXiv preprint</p>
<p>Prompt-based system for personality and interpersonal reactivity prediction. Bin Li, Yixuan Weng, Software Impacts. 121002962022</p>
<p>Artificial text detection with multiple training strategies. Bin Li, Yixuan Weng, Qiya Song, Hanjun Deng, arXiv:2212.051942022arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Jiacheng Miao, Joe R Davis, Jonathan K Pritchard, James Zou, Paper2agent: Reimagining research papers as interactive and reliable ai agents. 2025</p>
<p>Moore's law. Gordon Moore, Electronics Magazine. 3881141965</p>
<p>Alphaevolve: A coding agent for scientific and algorithmic discovery. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Abbas Francisco Jr Ruiz, Mehrabian, Google DeepMind, 05 2025. Technical report</p>
<p>Ai mirrors experimental science to uncover a novel mechanism of gene transfer crucial to bacterial evolution. Juraj José R Penadés, Lingchen Gottweis, He, B Jonasz, Alexander Patkowski, Wei-Hung Shurick, Tao Weng, Anil Tu, Artiom Palepu, Annalisa Myaskovsky, Pawlosky, bioRxiv. 2025</p>
<p>Samuel Schmidgall, Michael Moor, arXiv:2503.18102Agentrxiv: Towards collaborative autonomous research. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227v1Agent laboratory: Using llm agents as research assistants. 2025aarXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025barXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov, The 2023 Conference on Empirical Methods in Natural Language Processing. </p>
<p>The virtual lab of ai agents designs new sars-cov-2 nanobodies. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, Nature. 2025</p>
<p>Skipbert: Efficient inference with shallow layer skipping. Jue Wang, Ke Chen, Gang Chen, Lidan Shou, Julian Mcauley, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Does knowledge localization hold true? surprising differences between entity and relation perspectives in language models. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Huanhuan Ma, Yuanzhe Zhang, Jun Zhao, Kang Liu, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Mastering symbolic operations: Augmenting language models with compiled neural networks. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, The Twelfth International Conference on Learning Representations. </p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao, arXiv:2402.10151Controllm: Crafting diverse personalities for language models. 2024arXiv preprint</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Memory is all you need: An overview of compute-in-memory architectures for accelerating large language model inference. Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, Toyotaro Suzumura, 2024</p>
<p>Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui, 10.18653/v1/2024.findings-acl.456Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>An empirical analysis of uncertainty in large language model evaluations. Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025a</p>
<p>How far are ai scientists from changing the world?. Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, arXiv:2507.232762025barXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Ai becomes a masterbrain scientist. Zijie Yang, Yukai Wang, Lijing Zhang, bioRxiv. 2023</p>
<p>Paul Zarchan, Progress in astronautics and aeronautics: fundamentals of Kalman filtering: a practical approach. Aiaa2005208</p>
<p>Agentracer: Who is inducing failure in the llm agentic systems?. Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan, 2025a</p>
<p>Scaling laws in scientific discovery with ai and robot scientists. Pengsong Zhang, Heng Zhang, Huazhe Xu, Renjun Xu, Zhenting Wang, Cong Wang, Animesh Garg, Zhibin Li, Arash Ajoudani, Xinyu Liu, arXiv:2503.224442025barXiv preprint</p>
<p>Which agent causes task failures and when? on automated failure attribution of LLM multi-agent systems. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu, Forty-second International Conference on Machine Learning. 2025c</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025aarXiv preprint</p>
<p>Personality alignment of large language models. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, The Thirteenth International Conference on Learning Representations. 2025b</p>
<p>Ai scientists fail without strong implementation capability. Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, Yue Zhang, arXiv:2506.013722025carXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4389 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4389</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4389</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-280536738</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.04306v2.pdf" target="_blank">Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation</a></p>
                <p><strong>Paper Abstract:</strong> Compounding error is critical in long-form literature review generation, where minor inaccuracies cascade and amplify across subsequent steps, severely compromising the faithfulness of the final output. To address this challenge, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which proactively mitigates errors by orchestrating LLM-based agents into three specialized taskforces: (1) an exploration taskforce that interleaves retrieval and outlining using a tree-based strategy to establish a grounded structure; (2) an exploitation taskforce that iteratively cycles between fact location and draft refinement to ensure evidential support; and (3) a feedback taskforce that leverages historical experience for self-correction before errors propagate. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks (AutoSurvey and SurveyEval), significantly outperforming strong baselines in both citation quality (e.g., +15.7% recall) and content quality. We further contribute TopSurvey, a new large-scale benchmark of 195 peer-reviewed survey topics, on which MATC maintains robust performance, demonstrating its generalizability.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4389.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4389.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Taskforce Collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent orchestration framework that arranges LLM-based agents into three specialized taskforces (exploration, exploitation, feedback) to reduce compounding errors when generating long-form literature reviews from many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Agent Taskforce Collaboration (MATC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Manager and executor agent architecture: a manager agent orchestrates temporary taskforces of specialized executor agents. Exploration taskforce builds a hierarchical tree by interleaving retrieval and outline generation (searching + outlining agents). Exploitation taskforce iteratively cycles between a locating agent (extracts snippet-level evidence from retrieved full texts using LLM-based previewing) and a drafting agent (synthesizes drafts from snippets and prior drafts) until convergence (ROUGE-1 similarity). Feedback taskforce applies an experience-aware review loop where manager uses a historical feedback database to provide demonstrations and dynamic checklists guiding worker agent revisions. Final integration merges section drafts into the long-form manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-guided full-text previewing and snippet extraction (locating agent); retrieval via search-agent queries (searching agent); tree-structured, per-section retrieval tied to outline nodes</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative draft refinement using evidence-guided synthesis (drafting agent) with section-wise aggregation and manager-level integration; tree-based hierarchical composition</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies per topic; evaluated across benchmarks from 20 topics (AutoSurvey test set) up to 195 survey topics (TopSurvey); evaluated on SurveyEval which contains 384 surveys citing >26,000 references in aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science (LLM research subfields) and general scientific literature for survey generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form literature reviews / survey manuscripts (structured sections and subsections)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Citation recall & precision, content metrics (coverage, structure, relevance rated by LLMs and humans), ROUGE-1 for draft iteration similarity, Spearman rank correlation between LLM and human rankings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>State-of-the-art on benchmarks reported: e.g., compared to baselines MATC achieved substantial gains (paper reports up to +15.7% recall vs strong baselines); example benchmark numbers: recall 86.63% and precision 81.98% (one reported comparative table), earlier-benchmark numbers up to recall 98.17% and precision 89.28% under specific settings/benchmarks and ablations</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Naive RAG-based LLMs, AutoSurvey (Wang et al., 2024b), SurveyX (Liang et al., 2025), human experts for meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Substantially higher citation recall and precision than baselines in reported comparisons (e.g., MATC recall 86.63% vs SurveyX 75.51% and AutoSurvey 70.03%; precision 81.98% vs SurveyX 77.90% and AutoSurvey 71.66% in one table), and claimed +15.7% recall gain over strong baselines in the abstract</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Organizing LLM agents into focused taskforces with explicit grounding (tree-based exploration), iterative evidence-guided drafting (exploitation), and experience-driven feedback materially reduces error accumulation across long multi-step generation pipelines, improving citation grounding, structure, and content quality; iterative (2â€“3) refinement rounds yield most gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Still sensitive to upstream retrieval misses (search engines may omit relevant literature); additional iterations eventually degrade results by overwriting correct content; compounding errors remain a core challenge; computational cost and latency (reported average ~8.45 minutes to generate an 8k-token review) and dependence on quality of retrieved references.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance declines on a larger, more diverse benchmark (TopSurvey, 195 topics) with drops in recall and coverage; most marginal gains occur by the second and third iterations and additional iterations show diminishing returns or small declines by iteration five.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4389.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end framework that designs LLM-based autonomous agents to automate literature retrieval, outline generation, parallel subsection drafting, integration, and evaluation to produce survey papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline with components for retrieval, outline generation, parallel subsection drafting, integration, and automated evaluation. Uses LLMs as controllers to perform the stages of literature review generation in an autonomous fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented methods (retrieval of references then LLM processing); multi-document summarization style extraction described in original work</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Parallel subsection drafting and integration of drafted subsections into a coherent survey</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on the AutoSurvey benchmark (20 topics used in this paper's comparisons) and compared on datasets with many references (SurveyEval contains thousands of cited refs across surveys)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science surveys / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey articles / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Citation recall & precision, content quality metrics (coverage, structure, relevance), LLM-based and human evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported baseline performance in paper: e.g., in one comparison AutoSurvey recall ~70.03% and precision ~71.66% (table entries reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Naive RAG, SurveyX, and the proposed MATC system in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AutoSurvey underperforms MATC in the reported comparisons (example: AutoSurvey recall ~70.03% vs MATC 86.63% in one table), but outperforms Naive RAG in some metrics</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autonomous LLM pipelines can produce plausible survey drafts but are vulnerable to compounding errors across multistep workflows, especially affecting citation grounding and structure without explicit self-correction strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Struggles with citation precision and logical organization for longer surveys; susceptible to error accumulation across pipeline stages; details of self-correction not integrated in original design per this paper's critique.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance decreases as survey length increases; error accumulation becomes more pronounced for longer outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4389.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyX: Academic survey automation via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end solution for automated survey generation that covers online literature search, organization, and survey writing using LLM-based agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Surveyx: Academic survey automation via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SurveyX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end autonomous pipeline that performs online literature search, organization of retrieved materials, and automated survey writing using LLMs; official offline implementation used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Online literature search and organization prior to LLM-driven drafting (retrieval + organization)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Automated writing of surveys from organized references; integration of subsection drafts</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on the same benchmark topics as AutoSurvey (20 topics) and on larger sets in comparative experiments</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science surveys / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey articles / literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Citation recall & precision, coverage, structure, relevance, LLM and human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported baseline numbers in this paper: e.g., SurveyX recall ~75.51% and precision ~77.90% in a comparative table</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Naive RAG, AutoSurvey, and MATC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Per reported tables, SurveyX outperforms AutoSurvey and Naive RAG on some metrics but is outperformed by MATC (example: SurveyX recall ~75.51% vs MATC 86.63%)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows strong end-to-end automation but still suffers from citation grounding and organizational issues in long-form generation contexts; lacks the explicit multi-taskforce self-correction design proposed by MATC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Citation precision and support for generated statements remain problematic in long outputs; still prone to compounding errors across pipeline steps.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance degrades with longer survey lengths and larger topic sets, similar to other pipelines lacking targeted self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4389.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Naive RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive Retrieval-Augmented Generation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that uses retrieval-augmented generation (RAG) with LLMs to produce long-form literature summaries by conditioning generation on retrieved documents without multi-agent orchestration or iterative self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Naive RAG-based LLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Standard RAG approach: formulate queries, retrieve documents, and prompt an LLM to generate summaries conditioned on retrieved passages; no explicit taskforce orchestration or iterative evidence-guided refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval and passage-level retrieval-augmentation (implicit in RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-pass LLM generation conditioned on retrieved passages (no iterative locate/draft cycles)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Varies; used as baseline across the same benchmarks (per-survey number of retrieved refs unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature; used as baseline in computer science survey experiments</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form surveys / related work summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Citation recall & precision, coverage, structure, relevance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported lower performance on many metrics; example numbers in paper: Naive RAG recall ~64.57% and precision ~61.89% in one comparative table</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against AutoSurvey, SurveyX, and MATC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Underperforms both AutoSurvey and SurveyX and is substantially outperformed by MATC on citation grounding and structure</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-pass RAG without structured iterative correction is especially vulnerable to compounding errors as output length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High susceptibility to error accumulation for long-form outputs; lower citation precision and recall in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance drops more sharply than multi-agent methods as survey length (workflow horizon) increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4389.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STORM (prewriting for Wikipedia-like articles)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that models the prewriting stage by discovering diverse perspectives, simulating expert conversations, and curating information to create an outline used to generate a full article; leverages LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assisting in writing wikipedia-like articles from scratch with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prewriting pipeline: discover perspectives, simulate expert dialogues, curate information and create structured outline, then generate full article in later stages; uses LLMs to simulate experts and to curate content.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-driven perspective discovery and curated retrieval of informative content</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Outline-driven article generation via LLMs after simulated multi-perspective prewriting</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (STORM described as a related work example)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Wikipedia-style article generation (general text writing)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Encyclopedic articles / long-form articles</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prewriting that simulates multiple viewpoints and curated outlines can improve article drafting; cited here as related work for outline and perspective discovery mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes that such self-correction approaches are not directly sufficient for multi-step literature review workflows without targeted mitigation of compounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4389.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRIMERA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained encoder-decoder model with objectives tailored to link and aggregate information across multiple documents for multi-document summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PRIMERA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pretrained encoder-decoder architecture with pyramid-based masked sentence pretraining objectives designed to capture cross-document relations for abstractive multi-document summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Cross-document encoding and representation learning for aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Abstractive multi-document summarization via pretrained encoder-decoder decoding</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for multi-document summarization tasks; not directly evaluated on the survey benchmarks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-document summarization of scientific and other documents</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Abstractive summaries / related work sections</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining objectives that promote cross-document linking help multi-document summarization, referenced as a prior approach for aggregating info across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not a multi-agent LLM controller approach; pretrained model limitations for very long-form, heavily-referenced survey generation noted implicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4389.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based iterative self-feedback method where the model critiques and refines its outputs across rounds to improve quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM is prompted to produce an output, critique it, and refine the output iteratively using self-generated feedback; used to improve instruction following and generation fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Self-generated critique and selective extraction of weaknesses to guide revision</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative self-guided refinement of a single-pass output</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Method demonstrated on various generation tasks in original work; here referenced as a class of self-correction algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General LLM generation and instruction-following tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined generated text</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative self-critique improves some generation tasks but the authors of the current paper argue these approaches alone are insufficient for complex, long-horizon literature review pipelines where cross-step error propagation is severe.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not tailored to multi-agent, multi-document literature review workflows; may not prevent error accumulation across distinct pipeline stages.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4389.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECRIM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DECRIM: Decompose, Critique, and Refine for instruction following (LLM self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-correction method where instructions are decomposed, outputs critiqued, and refined to improve adherence to multi-constraint instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DECRIM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decompose a task into parts, have the model critique outputs against constraints, and refine outputs iteratively to better follow complex instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Decomposition-guided generation and critique-based selection of failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Constraint-guided iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Instruction-following and constrained generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Corrected/generated text meeting multiple constraints</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Effective for improving instruction adherence in many tasks but according to this paper such self-correction schemes aren't directly suitable for long multi-step literature review workflows without additional orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May not prevent cross-step error propagation in workflows with interdependent agent steps.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4389.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where language agents perform verbal reinforcement learning by reflecting on failures and updating behavior, enabling improved multi-step task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agents generate actions, receive verbal feedback about failures, and update internal reasoning or prompting behavior across episodes to improve performance on multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Verbalized feedback and reflection to identify relevant information and failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Policy/behavior adaptation through reflection + revised prompts or strategies</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General multi-step agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved agent behavior and multi-step task outputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflection and verbal feedback can help language agents learn from episodic failures; cited as related work in agent self-correction literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not explicitly designed for large-scale literature synthesis across many papers; paper suggests such methods alone aren't sufficient to address compounding errors in long-horizon literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4389.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-MapReduce-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method for scaling LLM generation to very long resources using a map-reduce style processing with entropy-driven convolutional scaling at test time to produce long-form articles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llm mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM MapReduce-v2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A map-reduce style decomposition and aggregation pipeline that processes extremely long input resources by mapping subparts, using entropy-driven strategies to prioritize content, and reducing/aggregating into long-form outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Divide-and-conquer mapping over resource chunks with prioritization by informational entropy; retrieval/processing per chunk</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical reduction and aggregation of chunk-level outputs into coherent long-form articles</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Targeted at extremely long resources; referenced here as a related method for long-form article generation from many sources</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Long-form generation from extremely long resources (e.g., many documents)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form articles / surveys</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MapReduce-style decomposition with entropy-driven prioritization helps handle extremely long inputs; relevant to literature synthesis tasks where inputs are large collections of papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Aggregation/consistency and evidence-grounding across many pieces remain challenges; cited as complementary to MATC's taskforce orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for scaling to extremely long resources; intended to improve with more resources but aggregation fidelity can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4389.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixture-Minigraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of knowledge minigraph agents for literature review generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced multi-agent approach (authored by overlapping authors) that mixes knowledge minigraphs via agents to support literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mixture of knowledge minigraph agents for literature review generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mixture of knowledge minigraph agents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent architecture that constructs small knowledge graphs (minigraphs) per agent and mixes them to aggregate cross-paper knowledge for literature review generation (paper cited in references; details in that cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Knowledge-graph/minigraph construction from papers and agent-level extraction</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Mixing/aggregating minigraphs produced by agents to form consolidated knowledge for drafting surveys</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature review generation (computer science)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature reviews / knowledge-aggregated summaries</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Proposes leveraging structured minigraphs and multiple agents to capture relations across papers â€” cited as related work and a complementary structured-knowledge approach to multi-agent survey generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not described in detail in the current paper; referenced for its multi-agent and knowledge-graph ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4389.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4389.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-LM-planners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models of code as few-shot planners and reasoners for multi-document summarization with attribution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method showing that code-focused LMs can act as few-shot planners and reasoners for multi-document summarization tasks with attribution capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models of code are few-shot planners and reasoners for multi-document summarization with attribution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Language-models-of-code planners for multi-doc summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses language models trained on code to perform planning and reasoning steps in few-shot settings, enabling multi-document summarization with better attribution of evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Planner-style few-shot prompting to guide retrieval/aggregation and attribution</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Planned, reasoned summarization with attention to attribution across documents</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-document summarization and attribution (scientific documents)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries with evidence attribution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Code-oriented LMs can be effective planners and reasoners for multi-document summarization tasks; cited as related work for multi-document planning and attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not directly integrated into MATC; applicability to very long-horizon survey generation requires orchestration and evidence-grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>Surveyx: Academic survey automation via large language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization <em>(Rating: 1)</em></li>
                <li>Llm mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources <em>(Rating: 2)</em></li>
                <li>Mixture of knowledge minigraph agents for literature review generation <em>(Rating: 1)</em></li>
                <li>Assisting in writing wikipedia-like articles from scratch with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4389",
    "paper_id": "paper-280536738",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "MATC",
            "name_full": "Multi-Agent Taskforce Collaboration",
            "brief_description": "A multi-agent orchestration framework that arranges LLM-based agents into three specialized taskforces (exploration, exploitation, feedback) to reduce compounding errors when generating long-form literature reviews from many papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Multi-Agent Taskforce Collaboration (MATC)",
            "system_description": "Manager and executor agent architecture: a manager agent orchestrates temporary taskforces of specialized executor agents. Exploration taskforce builds a hierarchical tree by interleaving retrieval and outline generation (searching + outlining agents). Exploitation taskforce iteratively cycles between a locating agent (extracts snippet-level evidence from retrieved full texts using LLM-based previewing) and a drafting agent (synthesizes drafts from snippets and prior drafts) until convergence (ROUGE-1 similarity). Feedback taskforce applies an experience-aware review loop where manager uses a historical feedback database to provide demonstrations and dynamic checklists guiding worker agent revisions. Final integration merges section drafts into the long-form manuscript.",
            "llm_model_used": "GPT-4.1",
            "extraction_technique": "LLM-guided full-text previewing and snippet extraction (locating agent); retrieval via search-agent queries (searching agent); tree-structured, per-section retrieval tied to outline nodes",
            "synthesis_technique": "Iterative draft refinement using evidence-guided synthesis (drafting agent) with section-wise aggregation and manager-level integration; tree-based hierarchical composition",
            "number_of_papers": "Varies per topic; evaluated across benchmarks from 20 topics (AutoSurvey test set) up to 195 survey topics (TopSurvey); evaluated on SurveyEval which contains 384 surveys citing &gt;26,000 references in aggregate",
            "domain_or_topic": "Computer science (LLM research subfields) and general scientific literature for survey generation",
            "output_type": "Long-form literature reviews / survey manuscripts (structured sections and subsections)",
            "evaluation_metrics": "Citation recall & precision, content metrics (coverage, structure, relevance rated by LLMs and humans), ROUGE-1 for draft iteration similarity, Spearman rank correlation between LLM and human rankings",
            "performance_results": "State-of-the-art on benchmarks reported: e.g., compared to baselines MATC achieved substantial gains (paper reports up to +15.7% recall vs strong baselines); example benchmark numbers: recall 86.63% and precision 81.98% (one reported comparative table), earlier-benchmark numbers up to recall 98.17% and precision 89.28% under specific settings/benchmarks and ablations",
            "comparison_baseline": "Naive RAG-based LLMs, AutoSurvey (Wang et al., 2024b), SurveyX (Liang et al., 2025), human experts for meta-evaluation",
            "performance_vs_baseline": "Substantially higher citation recall and precision than baselines in reported comparisons (e.g., MATC recall 86.63% vs SurveyX 75.51% and AutoSurvey 70.03%; precision 81.98% vs SurveyX 77.90% and AutoSurvey 71.66% in one table), and claimed +15.7% recall gain over strong baselines in the abstract",
            "key_findings": "Organizing LLM agents into focused taskforces with explicit grounding (tree-based exploration), iterative evidence-guided drafting (exploitation), and experience-driven feedback materially reduces error accumulation across long multi-step generation pipelines, improving citation grounding, structure, and content quality; iterative (2â€“3) refinement rounds yield most gains.",
            "limitations_challenges": "Still sensitive to upstream retrieval misses (search engines may omit relevant literature); additional iterations eventually degrade results by overwriting correct content; compounding errors remain a core challenge; computational cost and latency (reported average ~8.45 minutes to generate an 8k-token review) and dependence on quality of retrieved references.",
            "scaling_behavior": "Performance declines on a larger, more diverse benchmark (TopSurvey, 195 topics) with drops in recall and coverage; most marginal gains occur by the second and third iterations and additional iterations show diminishing returns or small declines by iteration five.",
            "uuid": "e4389.0",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey: Large language models can automatically write surveys",
            "brief_description": "An end-to-end framework that designs LLM-based autonomous agents to automate literature retrieval, outline generation, parallel subsection drafting, integration, and evaluation to produce survey papers.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys",
            "mention_or_use": "use",
            "system_name": "AutoSurvey",
            "system_description": "Pipeline with components for retrieval, outline generation, parallel subsection drafting, integration, and automated evaluation. Uses LLMs as controllers to perform the stages of literature review generation in an autonomous fashion.",
            "llm_model_used": null,
            "extraction_technique": "Retrieval-augmented methods (retrieval of references then LLM processing); multi-document summarization style extraction described in original work",
            "synthesis_technique": "Parallel subsection drafting and integration of drafted subsections into a coherent survey",
            "number_of_papers": "Evaluated on the AutoSurvey benchmark (20 topics used in this paper's comparisons) and compared on datasets with many references (SurveyEval contains thousands of cited refs across surveys)",
            "domain_or_topic": "Computer science surveys / literature reviews",
            "output_type": "Survey articles / literature reviews",
            "evaluation_metrics": "Citation recall & precision, content quality metrics (coverage, structure, relevance), LLM-based and human evaluations",
            "performance_results": "Reported baseline performance in paper: e.g., in one comparison AutoSurvey recall ~70.03% and precision ~71.66% (table entries reported in this paper)",
            "comparison_baseline": "Compared against Naive RAG, SurveyX, and the proposed MATC system in this paper",
            "performance_vs_baseline": "AutoSurvey underperforms MATC in the reported comparisons (example: AutoSurvey recall ~70.03% vs MATC 86.63% in one table), but outperforms Naive RAG in some metrics",
            "key_findings": "Autonomous LLM pipelines can produce plausible survey drafts but are vulnerable to compounding errors across multistep workflows, especially affecting citation grounding and structure without explicit self-correction strategies.",
            "limitations_challenges": "Struggles with citation precision and logical organization for longer surveys; susceptible to error accumulation across pipeline stages; details of self-correction not integrated in original design per this paper's critique.",
            "scaling_behavior": "Performance decreases as survey length increases; error accumulation becomes more pronounced for longer outputs.",
            "uuid": "e4389.1",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "SurveyX",
            "name_full": "SurveyX: Academic survey automation via large language models",
            "brief_description": "An end-to-end solution for automated survey generation that covers online literature search, organization, and survey writing using LLM-based agents.",
            "citation_title": "Surveyx: Academic survey automation via large language models",
            "mention_or_use": "use",
            "system_name": "SurveyX",
            "system_description": "End-to-end autonomous pipeline that performs online literature search, organization of retrieved materials, and automated survey writing using LLMs; official offline implementation used as a baseline in experiments.",
            "llm_model_used": null,
            "extraction_technique": "Online literature search and organization prior to LLM-driven drafting (retrieval + organization)",
            "synthesis_technique": "Automated writing of surveys from organized references; integration of subsection drafts",
            "number_of_papers": "Evaluated on the same benchmark topics as AutoSurvey (20 topics) and on larger sets in comparative experiments",
            "domain_or_topic": "Computer science surveys / literature reviews",
            "output_type": "Survey articles / literature reviews",
            "evaluation_metrics": "Citation recall & precision, coverage, structure, relevance, LLM and human evaluation",
            "performance_results": "Reported baseline numbers in this paper: e.g., SurveyX recall ~75.51% and precision ~77.90% in a comparative table",
            "comparison_baseline": "Compared against Naive RAG, AutoSurvey, and MATC",
            "performance_vs_baseline": "Per reported tables, SurveyX outperforms AutoSurvey and Naive RAG on some metrics but is outperformed by MATC (example: SurveyX recall ~75.51% vs MATC 86.63%)",
            "key_findings": "Shows strong end-to-end automation but still suffers from citation grounding and organizational issues in long-form generation contexts; lacks the explicit multi-taskforce self-correction design proposed by MATC.",
            "limitations_challenges": "Citation precision and support for generated statements remain problematic in long outputs; still prone to compounding errors across pipeline steps.",
            "scaling_behavior": "Performance degrades with longer survey lengths and larger topic sets, similar to other pipelines lacking targeted self-correction.",
            "uuid": "e4389.2",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Naive RAG",
            "name_full": "Naive Retrieval-Augmented Generation baseline",
            "brief_description": "A baseline approach that uses retrieval-augmented generation (RAG) with LLMs to produce long-form literature summaries by conditioning generation on retrieved documents without multi-agent orchestration or iterative self-correction.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Naive RAG-based LLM pipeline",
            "system_description": "Standard RAG approach: formulate queries, retrieve documents, and prompt an LLM to generate summaries conditioned on retrieved passages; no explicit taskforce orchestration or iterative evidence-guided refinement.",
            "llm_model_used": null,
            "extraction_technique": "Embedding-based retrieval and passage-level retrieval-augmentation (implicit in RAG)",
            "synthesis_technique": "Single-pass LLM generation conditioned on retrieved passages (no iterative locate/draft cycles)",
            "number_of_papers": "Varies; used as baseline across the same benchmarks (per-survey number of retrieved refs unspecified)",
            "domain_or_topic": "General scientific literature; used as baseline in computer science survey experiments",
            "output_type": "Long-form surveys / related work summaries",
            "evaluation_metrics": "Citation recall & precision, coverage, structure, relevance",
            "performance_results": "Reported lower performance on many metrics; example numbers in paper: Naive RAG recall ~64.57% and precision ~61.89% in one comparative table",
            "comparison_baseline": "Compared against AutoSurvey, SurveyX, and MATC",
            "performance_vs_baseline": "Underperforms both AutoSurvey and SurveyX and is substantially outperformed by MATC on citation grounding and structure",
            "key_findings": "Single-pass RAG without structured iterative correction is especially vulnerable to compounding errors as output length increases.",
            "limitations_challenges": "High susceptibility to error accumulation for long-form outputs; lower citation precision and recall in reported experiments.",
            "scaling_behavior": "Performance drops more sharply than multi-agent methods as survey length (workflow horizon) increases.",
            "uuid": "e4389.3",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "STORM",
            "name_full": "STORM (prewriting for Wikipedia-like articles)",
            "brief_description": "A system that models the prewriting stage by discovering diverse perspectives, simulating expert conversations, and curating information to create an outline used to generate a full article; leverages LLMs.",
            "citation_title": "Assisting in writing wikipedia-like articles from scratch with large language models",
            "mention_or_use": "mention",
            "system_name": "STORM",
            "system_description": "Prewriting pipeline: discover perspectives, simulate expert dialogues, curate information and create structured outline, then generate full article in later stages; uses LLMs to simulate experts and to curate content.",
            "llm_model_used": null,
            "extraction_technique": "LLM-driven perspective discovery and curated retrieval of informative content",
            "synthesis_technique": "Outline-driven article generation via LLMs after simulated multi-perspective prewriting",
            "number_of_papers": "Not specified in this paper (STORM described as a related work example)",
            "domain_or_topic": "Wikipedia-style article generation (general text writing)",
            "output_type": "Encyclopedic articles / long-form articles",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Prewriting that simulates multiple viewpoints and curated outlines can improve article drafting; cited here as related work for outline and perspective discovery mechanisms.",
            "limitations_challenges": "Paper notes that such self-correction approaches are not directly sufficient for multi-step literature review workflows without targeted mitigation of compounding errors.",
            "scaling_behavior": null,
            "uuid": "e4389.4",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "PRIMERA",
            "name_full": "PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization",
            "brief_description": "A pre-trained encoder-decoder model with objectives tailored to link and aggregate information across multiple documents for multi-document summarization.",
            "citation_title": "PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization",
            "mention_or_use": "mention",
            "system_name": "PRIMERA",
            "system_description": "Pretrained encoder-decoder architecture with pyramid-based masked sentence pretraining objectives designed to capture cross-document relations for abstractive multi-document summarization.",
            "llm_model_used": null,
            "extraction_technique": "Cross-document encoding and representation learning for aggregation",
            "synthesis_technique": "Abstractive multi-document summarization via pretrained encoder-decoder decoding",
            "number_of_papers": "Designed for multi-document summarization tasks; not directly evaluated on the survey benchmarks in this paper",
            "domain_or_topic": "Multi-document summarization of scientific and other documents",
            "output_type": "Abstractive summaries / related work sections",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Pretraining objectives that promote cross-document linking help multi-document summarization, referenced as a prior approach for aggregating info across papers.",
            "limitations_challenges": "Not a multi-agent LLM controller approach; pretrained model limitations for very long-form, heavily-referenced survey generation noted implicitly.",
            "scaling_behavior": null,
            "uuid": "e4389.5",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "An LLM-based iterative self-feedback method where the model critiques and refines its outputs across rounds to improve quality.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "system_name": "Self-Refine (iterative self-feedback)",
            "system_description": "LLM is prompted to produce an output, critique it, and refine the output iteratively using self-generated feedback; used to improve instruction following and generation fidelity.",
            "llm_model_used": null,
            "extraction_technique": "Self-generated critique and selective extraction of weaknesses to guide revision",
            "synthesis_technique": "Iterative self-guided refinement of a single-pass output",
            "number_of_papers": "Method demonstrated on various generation tasks in original work; here referenced as a class of self-correction algorithms",
            "domain_or_topic": "General LLM generation and instruction-following tasks",
            "output_type": "Refined generated text",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Iterative self-critique improves some generation tasks but the authors of the current paper argue these approaches alone are insufficient for complex, long-horizon literature review pipelines where cross-step error propagation is severe.",
            "limitations_challenges": "Not tailored to multi-agent, multi-document literature review workflows; may not prevent error accumulation across distinct pipeline stages.",
            "scaling_behavior": null,
            "uuid": "e4389.6",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "DECRIM",
            "name_full": "DECRIM: Decompose, Critique, and Refine for instruction following (LLM self-correction)",
            "brief_description": "A self-correction method where instructions are decomposed, outputs critiqued, and refined to improve adherence to multi-constraint instructions.",
            "citation_title": "Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints",
            "mention_or_use": "mention",
            "system_name": "DECRIM",
            "system_description": "Decompose a task into parts, have the model critique outputs against constraints, and refine outputs iteratively to better follow complex instructions.",
            "llm_model_used": null,
            "extraction_technique": "Decomposition-guided generation and critique-based selection of failure modes",
            "synthesis_technique": "Constraint-guided iterative refinement",
            "number_of_papers": null,
            "domain_or_topic": "Instruction-following and constrained generation",
            "output_type": "Corrected/generated text meeting multiple constraints",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Effective for improving instruction adherence in many tasks but according to this paper such self-correction schemes aren't directly suitable for long multi-step literature review workflows without additional orchestration.",
            "limitations_challenges": "May not prevent cross-step error propagation in workflows with interdependent agent steps.",
            "scaling_behavior": null,
            "uuid": "e4389.7",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A framework where language agents perform verbal reinforcement learning by reflecting on failures and updating behavior, enabling improved multi-step task performance.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "system_name": "Reflexion",
            "system_description": "Agents generate actions, receive verbal feedback about failures, and update internal reasoning or prompting behavior across episodes to improve performance on multi-step tasks.",
            "llm_model_used": null,
            "extraction_technique": "Verbalized feedback and reflection to identify relevant information and failure modes",
            "synthesis_technique": "Policy/behavior adaptation through reflection + revised prompts or strategies",
            "number_of_papers": null,
            "domain_or_topic": "General multi-step agent tasks",
            "output_type": "Improved agent behavior and multi-step task outputs",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Reflection and verbal feedback can help language agents learn from episodic failures; cited as related work in agent self-correction literature.",
            "limitations_challenges": "Not explicitly designed for large-scale literature synthesis across many papers; paper suggests such methods alone aren't sufficient to address compounding errors in long-horizon literature review generation.",
            "scaling_behavior": null,
            "uuid": "e4389.8",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "LLM-MapReduce-v2",
            "name_full": "LLM mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources",
            "brief_description": "A method for scaling LLM generation to very long resources using a map-reduce style processing with entropy-driven convolutional scaling at test time to produce long-form articles.",
            "citation_title": "Llm mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources",
            "mention_or_use": "mention",
            "system_name": "LLM MapReduce-v2",
            "system_description": "A map-reduce style decomposition and aggregation pipeline that processes extremely long input resources by mapping subparts, using entropy-driven strategies to prioritize content, and reducing/aggregating into long-form outputs.",
            "llm_model_used": null,
            "extraction_technique": "Divide-and-conquer mapping over resource chunks with prioritization by informational entropy; retrieval/processing per chunk",
            "synthesis_technique": "Hierarchical reduction and aggregation of chunk-level outputs into coherent long-form articles",
            "number_of_papers": "Targeted at extremely long resources; referenced here as a related method for long-form article generation from many sources",
            "domain_or_topic": "Long-form generation from extremely long resources (e.g., many documents)",
            "output_type": "Long-form articles / surveys",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "MapReduce-style decomposition with entropy-driven prioritization helps handle extremely long inputs; relevant to literature synthesis tasks where inputs are large collections of papers.",
            "limitations_challenges": "Aggregation/consistency and evidence-grounding across many pieces remain challenges; cited as complementary to MATC's taskforce orchestration.",
            "scaling_behavior": "Designed for scaling to extremely long resources; intended to improve with more resources but aggregation fidelity can be challenging.",
            "uuid": "e4389.9",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Mixture-Minigraph",
            "name_full": "Mixture of knowledge minigraph agents for literature review generation",
            "brief_description": "A referenced multi-agent approach (authored by overlapping authors) that mixes knowledge minigraphs via agents to support literature review generation.",
            "citation_title": "Mixture of knowledge minigraph agents for literature review generation",
            "mention_or_use": "mention",
            "system_name": "Mixture of knowledge minigraph agents",
            "system_description": "Multi-agent architecture that constructs small knowledge graphs (minigraphs) per agent and mixes them to aggregate cross-paper knowledge for literature review generation (paper cited in references; details in that cited work).",
            "llm_model_used": null,
            "extraction_technique": "Knowledge-graph/minigraph construction from papers and agent-level extraction",
            "synthesis_technique": "Mixing/aggregating minigraphs produced by agents to form consolidated knowledge for drafting surveys",
            "number_of_papers": null,
            "domain_or_topic": "Literature review generation (computer science)",
            "output_type": "Literature reviews / knowledge-aggregated summaries",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Proposes leveraging structured minigraphs and multiple agents to capture relations across papers â€” cited as related work and a complementary structured-knowledge approach to multi-agent survey generation.",
            "limitations_challenges": "Not described in detail in the current paper; referenced for its multi-agent and knowledge-graph ideas.",
            "scaling_behavior": null,
            "uuid": "e4389.10",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Code-LM-planners",
            "name_full": "Language models of code as few-shot planners and reasoners for multi-document summarization with attribution",
            "brief_description": "A method showing that code-focused LMs can act as few-shot planners and reasoners for multi-document summarization tasks with attribution capabilities.",
            "citation_title": "Language models of code are few-shot planners and reasoners for multi-document summarization with attribution",
            "mention_or_use": "mention",
            "system_name": "Language-models-of-code planners for multi-doc summarization",
            "system_description": "Uses language models trained on code to perform planning and reasoning steps in few-shot settings, enabling multi-document summarization with better attribution of evidence.",
            "llm_model_used": null,
            "extraction_technique": "Planner-style few-shot prompting to guide retrieval/aggregation and attribution",
            "synthesis_technique": "Planned, reasoned summarization with attention to attribution across documents",
            "number_of_papers": null,
            "domain_or_topic": "Multi-document summarization and attribution (scientific documents)",
            "output_type": "Summaries with evidence attribution",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Code-oriented LMs can be effective planners and reasoners for multi-document summarization tasks; cited as related work for multi-document planning and attribution.",
            "limitations_challenges": "Not directly integrated into MATC; applicability to very long-horizon survey generation requires orchestration and evidence-grounding.",
            "scaling_behavior": null,
            "uuid": "e4389.11",
            "source_info": {
                "paper_title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Surveyx: Academic survey automation via large language models",
            "rating": 2,
            "sanitized_title": "surveyx_academic_survey_automation_via_large_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization",
            "rating": 1,
            "sanitized_title": "primera_pyramidbased_masked_sentence_pretraining_for_multidocument_summarization"
        },
        {
            "paper_title": "Llm mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources",
            "rating": 2,
            "sanitized_title": "llm_mapreducev2_entropydriven_convolutional_testtime_scaling_for_generating_longform_articles_from_extremely_long_resources"
        },
        {
            "paper_title": "Mixture of knowledge minigraph agents for literature review generation",
            "rating": 1,
            "sanitized_title": "mixture_of_knowledge_minigraph_agents_for_literature_review_generation"
        },
        {
            "paper_title": "Assisting in writing wikipedia-like articles from scratch with large language models",
            "rating": 1,
            "sanitized_title": "assisting_in_writing_wikipedialike_articles_from_scratch_with_large_language_models"
        }
    ],
    "cost": 0.01923175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation
8 Oct 2025</p>
<p>Zhi Zhang 
Department of Computing
The Hong Kong Polytechnic University
Hong KongChina</p>
<p>Yan Liu 
Department of Computing
The Hong Kong Polytechnic University
Hong KongChina</p>
<p>Zhejing Hu zhejing.hu@connect.polyu.hk 
Department of Computing
The Hong Kong Polytechnic University
Hong KongChina</p>
<p>Chen Gong 
FireTorch Partners
ShenzhenChina</p>
<p>Sheng-Hua Zhong csshzhong@szu.edu.cn 
College of Computer Science and Software Engineering
Shenzhen University
ShenzhenChina</p>
<p>Jiannong Cao jiannong.cao@polyu.edu.hk 
Department of Computing
The Hong Kong Polytechnic University
Hong KongChina</p>
<p>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation
8 Oct 2025C070AE6C206979C75D6E3CE197C405BBarXiv:2508.04306v2[cs.CE]
Compounding error is critical in long-form literature review generation, where minor inaccuracies cascade and amplify across subsequent steps, severely compromising the faithfulness of the final output.To address this challenge, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which proactively mitigates errors by orchestrating LLM-based agents into three specialized taskforces: (1) an exploration taskforce that interleaves retrieval and outlining using a tree-based strategy to establish a grounded structure; (2) an exploitation taskforce that iteratively cycles between fact location and draft refinement to ensure evidential support; and (3) a feedback taskforce that leverages historical experience for self-correction before errors propagate.Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks (AutoSurvey and SurveyEval), significantly outperforming strong baselines in both citation quality (e.g., +15.7% recall) and content quality.We further contribute TopSurvey, a new large-scale benchmark of 195 peer-reviewed survey topics, on which MATC maintains robust performance, demonstrating its generalizability.</p>
<p>Introduction</p>
<p>Literature reviews play an important role in scientific research by synthesizing existing knowledge to identify research gaps, establish theoretical frameworks, and guide future studies (Ermel et al., 2021;Liu et al., 2023).Over recent decades, the rapid growth in the number of scientific publications has made it increasingly challenging for researchers to efficiently retrieve, organize, and synthesize relevant information in their fields (Wang et al., 2024b).In response, automatic literature review systems that leverage advances in artificial intelligence have become increasingly valuable, as they help reduce manual workload, provide timely coverage of new * Corresponding author.</p>
<p>research, and facilitate the identification of key trends and research gaps within the extensive literature (Zhang et al., 2025;Wang et al., 2024b;Liu et al., 2023).</p>
<p>Early approaches primarily leverage multidocument summarization techniques to generate summaries from a set of reference papers (Hoang and Kan, 2010;Hu and Wan, 2014;Erera et al., 2019), enabling the automatic creation of related work sections given a set of references as input.In recent years, large language models (LLMs) demonstrate impressive planning and reasoning abilities (Wang et al., 2024a;Zhao et al., 2024;Zhong and Wang, 2024;Nandy and Bandyopadhyay, 2025;Zhang et al., 2025).This leads to a growing research area that employs LLMs as controllers to construct autonomous agents capable of effectively performing complex tasks (Wang et al., 2024a).Building on these advances, recent state-of-the-art research, e.g., AutoSurvey (Wang et al., 2024b) and SurveyX (Liang et al., 2025), achieves remarkable success by designing LLMbased autonomous agents to automate the entire pipeline of literature review and generate long-form manuscripts.</p>
<p>Although these advances enable the automatic generation of human-like literature reviews with reduced manual effort, previous studies on multistep models show that the inherent long-horizon workflow of such systems inevitably introduces the risk of compounding errors.In the workflow, small mistakes made at earlier steps can accumulate and propagate through subsequent steps, as errors that occur at a certain step will affect the next state (Wang et al., 2025a).Without effective self-correction mechanisms, these deviations will further expand the error in later steps (Han and Zhang, 2023;Venkatraman et al., 2015;Hejna et al., 2023).</p>
<p>This paper investigates the phenomenon of error accumulation in automatic literature review generation.As a demonstration, we employ the state-of-the-art large language model GPT-4.1 (OpenAI, 2025) to conduct a literature review generation workflow on the topic of EEG (electroencephalogram)-based emotion recognition, with a target length of 4,000 words.As shown in Fig. 1, when the retrieval stage fails to identify relevant literature, the resulting errors can mislead the outlining process.An irrelevant outline may further cause the drafted manuscript to deviate from the intended topic.In long-form text generation, the increased complexity and the introduction of more dependent steps further exacerbate the compounding errors.</p>
<p>To tackle this challenge, we propose the Multi-Agent Taskforce Collaboration (MATC) framework.MATC consists of two types of autonomous agents: the manager agent and the executor agents.The manager agent orchestrates the workflow, while the executor agents each specialize in a distinct step of the process, such as literature searching, outline generation, fact localization, or manuscript drafting.</p>
<p>Inspired by organizational quality management (Hackman and Wageman, 1995), we propose to orchestrate agents organized as taskforces.A taskforce refers to a temporary group of specialized agents focused on specific subtasks (Altier, 1987;Aikin, 1949;de Waard and Kramer, 2008).We formulate three types of taskforces, each designed with specific error mitigation mechanisms.</p>
<p>Given a user's instruction, the manager agent oversees the entire workflow and initiates the formation of different taskforces.To mitigate error ac-cumulation between searching and outlining agents, the manager organizes an exploration taskforce employing a tree-based strategy.The taskforce begins with a broad overview and incrementally determines the literature and outline at each level, preventing errors in ungrounded outlines or biased retrieval.To mitigate error accumulation between locating and drafting agents, the manager organizes an exploitation taskforce employing an iterative cycle: the draft guides fact location from the literature, while the located facts inform draft refinement, preventing errors in unsupported claims or mismatched evidence.To mitigate errors within each agent's workflow, the manager organizes feedback taskforces that maintain historical experience and implement dynamic checklists to guide selfcorrection before errors propagate to subsequent stages.Finally, the manager integrates the outputs of all taskforces to produce the final manuscript.</p>
<p>Related Work</p>
<p>Automating the summarization of research papers has long attracted interest.Early work mainly used multi-document summarization techniques.In 2010, Hoang et al. introduced automatic related work summarization, extracting sentences from multiple articles to form an extractive summary tailored to the target paper (Hoang and Kan, 2010).Lu et al. constructed Multi-XScience, a large-scale dataset for generating related work sections based on a paper's abstract and cited articles, serving as a benchmark for later studies (Lu et al., 2020).Chen et al. proposed a relation-aware multi-document encoder and a relation graph to capture cross-document dependencies for abstractive related work generation (Chen et al., 2021).Xiao et al. developed PRIMERA, a pre-trained encoder-decoder model with novel objectives for linking and aggregating information across documents (Xiao et al., 2022).Liu et al. proposed aligning sentences by category and using sparse transformers to organize information from many references (Liu et al., 2023).</p>
<p>Recently, advances in LLMs have greatly improved the scope and quality of automated literature reviews, enabling end-to-end systems to generate literature review papers.Wang et al. introduced AutoSurvey, a framework with retrieval, outline generation, parallel subsection drafting, integration, and rigorous evaluation (Wang et al., 2024b).Shao et al. developed STORM, which models the prewriting stage by discovering diverse perspectives, simulating expert conversations, and curating information to create an outline used to generate a full Wikipedia article (Shao et al., 2024).Liang et al. designed SurveyX, an end-to-end solution for automated survey generation, covering online literature search, organization, and survey writing (Liang et al., 2025).While these methods represent impressive progress, little research has addressed the risk of errors in generated outputs.Self-correction algorithms in LLMs are not suitable for the workflow of literature review generation (Madaan et al., 2023;Shinn et al., 2023;Ferraz et al., 2024).As systems become more complex, error accumulation becomes a significant challenge.</p>
<p>Methodology</p>
<p>As illustrated in Fig. 2, given a user instruction U , the proposed Multi-Agent Taskforce Collaboration (MATC) framework orchestrates three taskforces to produce the final literature review output Z.In the following sections, we introduce each taskforce in detail.</p>
<p>Exploration Taskforce</p>
<p>The exploration taskforce is designed to determine the outline of the literature review and to retrieve relevant references for each section of the outline.Without prior knowledge of the relevant literature, the initial draft outline tends to be suboptimal.Conversely, without a determined direction, the literature retrieval process lacks focus.To mitigate compounding errors, we propose a tree-based strat-egy.</p>
<p>In detail, given a user instruction U , the manager A M initiates the exploration taskforce.The manager constructs a tree with the user instruction as the root node, representing the overall direction for exploring the field.This tree is expanded by identifying sub-directions of interest.For the root node, A M first invokes the searching agent to retrieve literature based on the user instruction, resulting in a set of relevant papers.The searching agent A S operates as follows:
{L (0) 1 , L (0) 2 , . . . , L(0)I } = A S (U )(1)
where
L (0)
i denotes the i-th piece of literature retrieved for the root node at tree depth d = 0.The number of literature is denoted by I.</p>
<p>Next, the manager assigns the outlining agent to determine sub-directions based on the titles and abstracts of the retrieved literature.The outlining agent A O operates as follows:
{O (0) 1 , O (0) 2 , . . . , O (0) J } = A O ({L (0) i }) (2)
where O (0) j denotes the j-th sub-direction at the root level, and J is the total number of subdirections at tree depth d = 0.</p>
<p>For each outline node O (d) j at depth d, the manager decides whether further decomposition is needed according to the following criterion:
Ï•(O (d) j ) = 1, if complexity(O (d) j ) &gt; Î¸ 0, otherwise(3)
where complexity(O
(d) j ) denotes the complexity of direction O (d)
j , and Î¸ is the complexity threshold.In this work, we use a simple yet effective strategy in which each direction is treated as a section, and complexity is measured by the number of words.</p>
<p>For outline nodes requiring further decomposition, where Ï•(O (d) j ) = 1, the process is applied recursively as follows:
{L (d+1) 1 , L (d+1) 2 , . . . , L (d+1) I } = A S (O (d) j ) {O (d+1) 1 , O (d+1) 2 , . . . , O (d+1) J } = A O ({L (d+1) i }) (4)
where {L This process constructs child nodes in the tree T , where nodes {O
(d+1) 1 , O (d+1) 2 , . . . , O (d+1) J } areâˆ€v âˆˆ V : Ï•(v) = 0 or d = d max (5)
where V denotes all leaf nodes in the outline tree and d max is the maximum allowed depth.The resulting tree T provides a hierarchical outline, with associated literature at each level, for downstream generation.</p>
<p>Exploitation Taskforce</p>
<p>The exploitation taskforce is designed to extract relevant snippets from the literature and draft the review manuscript.Without clear target content, it is difficult to locate the necessary evidence in the literature.Conversely, without sufficient supporting evidence, the manuscript may lack grounding.</p>
<p>To mitigate compounding errors, we propose a treebased strategy.</p>
<p>In detail, given the outline tree T produced as mentioned above, the manager A M initiates the exploitation taskforce.For each leaf node in the tree, A M identifies its parent outline node, denoted as {O j }, and the associated literature set {L i }.The exploitation taskforce then proceeds through iterative cycles.At the initial iteration t = 0, the locating agent A L extracts relevant information based on the outline node {O j } and the literature set {L i }, and operates as follows:
{F (0) 1 , F (0) 2 , . . . , F (0) N } = A L ({L i }, {O j }) (6)
where
F (0)
n denotes the n-th note snippet extracted from the literature set {L i } relevant to the parent outline {O j }, and N is the total number of snippets.</p>
<p>The drafting agent A W composes the initial draft as follows:
M (0) k = A W ({F (0) n }, {O j })(7)
where
M (0)
k represents the initial manuscript for the outline set {O j }, incorporating the content of all its child outline nodes.</p>
<p>For subsequent iterations t &gt; 0, the locating agent A L re-examines the literature to extract new snippets that support or improve the current draft, operating as follows:
{F (t) 1 , F (t) 2 , . . . , F (t) N } = A L ({L i }, M (t) k ) (8)
where
M (t)
k denotes the draft content from the previous iteration, and {F (t) n } is the set of newly extracted note snippets.The drafting agent A W refines the section draft by incorporating the new evidence and the previous draft:
M (t+1) k = A W ({F (t) n }, {O j }, M (t) k )(9)
where M (t+1) k</p>
<p>is the updated content, and M (t) k serves as the previous context for refinement.</p>
<p>The exploitation process for each parent node O i terminates when the following condition is met:
Ïˆ(M (t+1) i , M (t) i ) &gt; Ïµ or t = t max(10)
where Ïˆ(â€¢, â€¢) denotes the similarity between consecutive iterations of the draft, measured by the ROUGE-1 score (Lin, 2004), Ïµ is a similarity threshold that indicates no further significant improvements can be made, and t max is the maximum number of allowed iterations.</p>
<p>The final content M
(t * ) i
, where t * denotes the stopping iteration for parent node i, serves as the final draft.Finally, the manager A M aggregates the finalized drafts for all sections and delivers the completed literature review to the user.</p>
<p>Feedback Taskforce</p>
<p>The feedback taskforce is designed to execute tasks assigned by the manager, taking into account the risk of deviations at each step arising from unpredictable agent-environment interactions.To mitigate compounding errors, we propose an experience-based strategy.</p>
<p>In detail, for each agent A W , the manager agent A M initiates the feedback taskforce.The agent first generates the primary results, then A M leverages historical experience to review these results, after which A W refines its outputs based on the feedback provided.At the initial step, A W produces the first output solely based on the context:
Y (0) = A W (C)(11)
where Y (0) is the initial output, and C is the context fetched by the manager agent.</p>
<p>To give useful feedback, we propose an experience-aware mechanism.We record feedback and revision history in a database and select useful feedback for demonstration to provide feedback on the worker's output at each iteration x.
r * = arg max râˆˆR min usage(r) (12)
where useful feedback is defined as the feedback that has led to the fewest further revisions, where R min = {r âˆˆ R : revision(r) = r min }.Here, revision(r) denotes the number of revisions after using feedback r.In R min , we select the most useful one, i.e., with the highest usage(r).Here, usage(r) denotes the number of times the feedback r is selected for demonstration.</p>
<p>Referring to r * , the manager agent then provides feedback on the worker's output:
R (x) = A M (Y (x) , r * ) (13)
where R (x) denotes the feedback at iteration x.</p>
<p>For subsequent iterations (x â‰¥ 1), the worker agent refines the output with the input and the feedback.
Y (x) = A W (C, R (xâˆ’1) )(14)
The feedback taskforce continues until the output meets the manager's requirements and no further feedback is provided, or until a maximum number of iterations x max is reached.In the following, we detail how workers cooperate with the manager.</p>
<p>The searching agent formulates search queries based on user instructions, submits queries to search engines, and collects relevant literature.It filters and ranks the retrieved references by citation count, removes incomplete entries, and selects the top Q k unique references for further use.The outlining agent utilizes the gathered literature and user instructions to prompt an LLM for generating suboutlines.The locating agent downloads and parses full-text papers, applies LLM-based previewing to select pertinent pages, and extracts concise text snippets for drafting.The drafting agent prompts the LLM to synthesize coherent text by incorporating selected references and prior drafts.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>We conduct experiments on two benchmark datasets, AutoSurvey (Wang et al., 2024b) and Sur-veyEval (Wang et al., 2025b), as well as a selfconstructed benchmark dataset, TopSurvey.For hyperparameters, we set d max , t max , and x max to 4. The hyperparameter Î¸ is set to 500.We use GPT-4.1 for all agents.The evaluation comprises two categories of metrics.For citation quality, we adopt citation recall and citation precision as proposed by Wang et al. (2024b): recall measures whether cited passages fully support all statements, while precision measures the proportion of relevant citations that support their corresponding statements.For content quality, following Wang et al. (2024b)  We first evaluate our framework on the Auto-Survey (Wang et al., 2024b) benchmark, following the protocols established by Liang et al. (2025); Wang et al. (2024b).We use the same 20 topics from diverse subfields of LLM research to generate survey articles for comparison.We compare our approach with three baselines: Naive RAG-based LLMs using retrieval-augmented generation, Auto-Survey (Wang et al., 2024b), and SurveyX (Liang et al., 2025).We follow the official report of Auto-Survey1 and conduct experiments with the official implementation of the offline version's SurveyX2 .</p>
<p>As shown in Table 1, most algorithms exhibit reduced performance as survey length increases, particularly in recall, precision, structure, and relevance.This decline is most pronounced in the Naive RAG baseline, indicating that longer workflows lead to greater error accumulation and propagation.In contrast, coverage remains stable or improves with longer surveys, likely due to more comprehensive topic inclusion.State-of-the-art methods such as AutoSurvey and SurveyX continue to face challenges with citation precision, frequently generating statements unsupported by references.For content quality, structure consistently receives the lowest scores, reflecting disorganized article organization.Our proposed method remains robust to these issues and consistently achieves superior results across all metrics.</p>
<p>We further evaluate our framework on the Sur-veyEval benchmark (Wang et al., 2025b), using the same protocols.SurveyEval is the first benchmark in computer science that pairs surveys with complete reference papers, comprising 384 arXiv cs.CL surveys citing over 26,000 references.Twenty topics were selected for testing based on reference completeness and reference list diversity.Experi-mental results are shown in Fig. 3 In Fig. 3, all methods achieve high coverage and relevance, indicating that LLMs like GPT-4.1 can generate comprehensive and relevant content.However, recall and precision remain low, reflecting poor reference retrieval and insufficient support for generated statements.As a result, state-of-the-art methods struggle with logical organization, leading to lower structure scores.Our method overcomes these issues, achieving the best performance across all metrics.</p>
<p>Ablation Study</p>
<p>We further conduct an ablation study to evaluate the effectiveness of each component in our proposed framework.Experiments are performed on the same benchmark dataset as before, with all experimental settings unchanged and the survey length set to 8k tokens.We compare the full model with three variants.We use a single round of retrieval and organization for the variant without the exploration taskforce to generate the overall outline.We draft the manuscript using only one round of extraction and writing for the variant without the exploitation taskforce.For the variant without the feedback taskforce, each agent completes its assigned task, without revision.</p>
<p>Table 2 presents the results.Removing the exploration taskforce causes the largest drops in recall and structure, demonstrating its importance for citation recall and logical organization.Excluding the exploitation taskforce sharply reduces precision and relevance, confirming its role in improving citation precision and content relevance.Without the feedback taskforce, all metrics decrease, especially coverage, highlighting its key role in ensuring comprehensive coverage through collaborative revision.</p>
<p>A New Large-Scale Benchmark</p>
<p>To further validate the effectiveness of our proposed framework, we construct a new large-scale benchmark dataset.This dataset consists of 195 topics from various subfields of computer science, nearly 10 times larger than previous benchmarks (Wang et al., 2024b;Liang et al., 2025).To ensure topic quality, we collect peer-reviewed survey topics from top computer science conferences, rather than from preprint sources such as arXiv.Survey papers accepted only as abstracts are excluded.To prevent data leakage from LLM pretraining data, we include only survey papers published in 2023, 2024, and 2025.We employ PhD students in computer science to verify whether a paper qualifies as a survey and to collect the final set of 195 survey papers: 9 from AAAI, 34 from ACL, 46 from EMNLP, 3 from ICLR, 2 from ICML, 77 from IJCAI, and 24 from NAACL.Of these, 68 were published in 2023, 105 in 2024, and 22 in 2025.</p>
<p>We conduct experiments using the same settings as above, generating 64k-token literature reviews for evaluation.As shown in Table 3, compared with results on the existing benchmark in Table 1, performance on the new large-scale benchmark drops significantly, particularly in recall and coverage.This may be due to the greater number and broader range of topics, which leads to some relevant literature not being retrieved, resulting in lower citation recall and coverage.Despite this, our proposed method achieves over 80% in citation scores and an average content quality score of 4.92.</p>
<p>To verify the consistency between LLM evaluation methods and human evaluation, we follow (Wang et al., 2024b) to conduct a meta-evaluation.Human experts rank the generated surveys, and we compare these rankings with those generated by LLMs using Spearman's rank correlation coefficient.The LLM evaluation achieves a correlation of 0.5117, suggesting LLMs align well with human preferences.</p>
<p>Sensitivity Analysis</p>
<p>To gain deeper insight into our framework, we conduct a sensitivity analysis to observe how d max , t max , and x max affect performance.We evaluate the experience, exploration, and exploitation taskforces across different numbers of iterations, employing a controlled variable approach: when varying one hyperparameter, all others are held constant as previously described.</p>
<p>The results are presented in Table 4.We find that the second and third iterations yield the most significant improvements.This indicates that errors can accumulate at various steps in the workflow,  and iterative refinement effectively reduces intermediate errors, thereby enhancing the quality of the final results.Both the Experience and exploitation taskforces contribute most to faithfulness; from the first to the fourth iteration, the number of references supporting claims in the final output increases by a large margin.Additionally, the Experience and exploration taskforces contribute notably to coverage and structural quality, suggesting that a single revision is often insufficient for comprehensive improvement.However, the effect of additional iterations gradually diminishes.By the fifth iteration, we observe a slight decline, due to unnecessary modifications overwriting correct results and introducing new errors.</p>
<p>Disscussions</p>
<p>We evaluated our framework in a real-world setting by deploying an online automatic literature review generation system 3 , which has produced over 20,000 reviews.We randomly selected 400 reviews, embedded them with all-MiniLM-L6-v2, 3 Anonymous for review; to be revealed upon acceptance.and applied K-means clustering to form five groups.Results show differences in citation and content quality among clusters.The best performance appears in computer science (cluster 4), while education and social sciences (cluster 3) perform worse, especially in citation quality.It may result from the search engine not including part of the relevant literature in the social sciences.We also measured system efficiency, and generating an 8,000-token review takes an average of 8.45 minutes.</p>
<p>Conclusion</p>
<p>In this paper, we identified and addressed the critical challenge of compounding errors in automated long-form literature review generation.The proposed Multi-Agent Taskforce Collaboration framework strategically orchestrates LLM-based agents through three specialized taskforces.Extensive experiments demonstrate that MATC achieves stateof-the-art performance across established benchmarks.The robustness and generalizability of our framework are further confirmed on our newly contributed TopSurvey benchmark, a large-scale collection of 195 peer-reviewed topics.The deployment of our online system, which has generated over 20,000 reviews, provides practical evidence of MATC's effectiveness and efficiency in real-world scenarios.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the error accumulation phenomenon.</p>
<p>(d+1) i } denotes the set of literature retrieved for sub-direction O (d) j at depth d + 1, and J denotes the number of sub-directions.</p>
<p>Figure 2 :
2
Figure 2: The overall framework of Multi-agent Taskforce Collaboration (MATC) for literature review generation.</p>
<p>Figure 3 :
3
Figure 3: Comparison of automatic survey generation methods on the SurveyEval benchmark.Higher scores indicate better performance.</p>
<p>Table 1 :
1
, we use coverage, structure, and relevance, each rated by LLMs on a 5-point scale.Coverage assesses the extent to which the survey addresses all relevant Comparison of automatic survey generation methods across different survey lengths (measured in tokens) on the benchmark dataset.Higher scores indicate better performance.
Citation QualityContent QualitySurvey Length MethodsRecall â†‘ Precision â†‘ Coverage â†‘ Structure â†‘ Relevance â†‘ Avg. â†‘Naive RAG (2024) 78.1471.924.403.864.864.378kAutoSurvey (2024) 82.48 SurveyX (2025) -77.42 -4.60 -4.46 -4.80 -4.62 -Proposed98.1789.284.974.955.004.97Naive RAG (2024) 71.4865.314.463.664.734.2816kAutoSurvey (2024) 81.34 SurveyX (2025) -76.94 -4.66 -4.33 -4.86 -4.62 -Proposed98.1188.524.984.954.984.97Naive RAG (2024) 79.8865.034.413.754.664.2732kAutoSurvey (2024) 83.14 SurveyX (2025) -78.04 -4.73 -4.26 -4.80 -4.60 -Proposed98.0387.924.984.934.984.96Naive RAG (2024) 68.7961.974.403.664.664.2464kAutoSurvey (2024) 82.25 SurveyX (2025) 85.1477.41 78.024.73 4.954.33 4.904.86 4.954.64 4.93Proposed97.2787.905.004.914.984.964.2 Comparision Experiments100100.0096.00100.0092.34Normalized Score70 80 9074.786050Recall Precision Coverage Structure RelevanceNaive RAG (2024)AutoSurvey (2024)SurveyX (2025)Proposed
aspects of the topic; structure evaluates logical organization and coherence; and relevance measures alignment with the specified research topic.We do not filter out fractional scores, such as 4.5.</p>
<p>Table 2 :
2
, with Coverage, Structure, and Relevance scores normalized to a 100-point scale.Pre.â†‘ Cov.â†‘ Str.â†‘ Rel.â†‘ Avg.â†‘ Ablation study of the proposed modules on the benchmark dataset.
Citation QualityContent QualityMethods Rec. â†‘ w/o exploration 94.38 88.564.834.835.004.89w/o exploitation 97.86 79.024.974.934.934.94w/o feedback97.78 80.824.794.894.974.88Proposed98.17 89.284.974.955.004.97</p>
<p>Precision â†‘ Coverage â†‘ Structure â†‘ Relevance â†‘ Avg.â†‘
Citation QualityContent QualityMethods Recall â†‘ Human 93.0787.765.004.975.004.99Naive RAG (2024) 64.5761.894.293.584.674.18AutoSurvey (2024) 70.0371.664.684.674.874.74SurveyX (2025)75.5177.904.714.844.934.83Proposed86.6381.984.854.905.004.92</p>
<p>Table 3 :
3
Comparison of automatic survey generation methods at a survey length of 64k tokens on the new large-scale benchmark dataset.Higher scores indicate better performance.Pre.â†‘ Cov.â†‘ Str.â†‘ Rel.â†‘ Avg.â†‘
Citation QualityContent QualityMethod Iteration 1 2 Rec. â†‘ Feed. 83.64 74.37 84.99 78.19 3 85.11 80.134.61 4.77 4.824.79 4.83 4.864.94 4.98 4.984.78 4.86 4.89486.63 81.984.854.905.004.92586.71 81.864.854.884.984.90182.84 81.264.714.785.004.83284.79 81.674.834.825.004.88Explor.385.69 81.974.854.845.004.90486.63 81.984.854.905.004.92586.65 81.924.854.875.004.91186.31 72.684.814.874.914.86286.00 77.344.834.904.944.89Exploi.386.75 79.104.844.894.984.90486.63 81.984.854.905.004.92586.47 80.274.844.895.004.91</p>
<p>Table 4 :
4
Sensitivity experiments across different iterations for the experience, exploration, and exploitation taskforces on the new large-scale benchmark.</p>
<p>Table 5 :
5
Real-world case study across different topics on 400 generated results.Higher scores indicate better performance.
Citation QualityContent QualityClusterRecall â†‘ Precision â†‘ Coverage â†‘ Structure â†‘ Relevance â†‘183.6182.574.674.784.93284.5475.124.754.854.85372.2976.114.594.884.87486.6381.984.854.905.00581.3769.774.664.744.73
https://github.com/AutoSurveys/AutoSurvey
https://github.com/IAAR-Shanghai/SurveyX</p>
<p>Task force. Charles Aikin, Methodology. Public Administration Review. 941949</p>
<p>Task forces: An effective management tool. J William, Altier, Management Review. 762521987</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, 2021Association for Computational Linguistics</p>
<p>Tailored task forces: Temporary organizations and modularity. Erik J De Waard, Eric-Hans Kramer, International Journal of Project Management. 2652008</p>
<p>Or Rivlin, and 1 others. 2019. A summarization system for scientific documents. Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat, Ora Peled Nakash, Odellia Boni, Haggai Roitman, Doron Cohen, Bar Weiner, Yosi Mass, Empirical Methods in Natural Language Processing. </p>
<p>Ana Paula, Cardoso Ermel, Daniel Pacheco Lacerda, Maria Isabel, W M Morandi, Leandro Gauss, Literature reviews: modern methods for investigating scientific and technological knowledge. Springer Nature2021</p>
<p>Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng, 2024Association for Computational Linguistics</p>
<p>Total quality management: Empirical, conceptual, and practical issues. Administrative science quarterly. Richard Hackman, Ruth Wageman, 1995</p>
<p>Expert data augmentation in imitation learning. Fuguang Han, Zongzhang Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Improving long-horizon imitation through instruction prediction. Joey Hejna, Pieter Abbeel, Lerrel Pinto, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Towards automated related work summarization. Cong Duy, Vu Hoang, Min-Yen Kan, Conference on Computational Linguistics. 2010</p>
<p>Automatic generation of related work sections in scientific papers: an optimization approach. Yue Hu, Xiaojun Wan, Empirical Methods in Natural Language Processing. 2014</p>
<p>Surveyx: Academic survey automation via large language models. Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Conference on Knowledge Discovery and Data Mining. 2025Hanyu Wang, and 1 others</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen, International Conference on Artificial Intelligence. 2023</p>
<p>Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles. Yao Lu, Yue Dong, Laurent Charlin, Empirical Methods in Natural Language Processing. 2020</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Advances in Neural Information Processing Systems. 202336Yiming Yang, and 1 others</p>
<p>Language models of code are few-shot planners and reasoners for multi-document summarization with attribution. Abhilash Nandy, Sambaran Bandyopadhyay, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Introducing gpt-4.1 in the api. 2025OpenAI</p>
<p>Assisting in writing wikipedia-like articles from scratch with large language models. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, Monica Lam, North American Chapter. the Association for Computational Linguistics2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Improving multi-step prediction of learned time series models. Arun Venkatraman, Martial Hebert, Bagnell, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201529</p>
<p>Are expressive models truly necessary for offline rl?. Guan Wang, Haoyi Niu, Jianxiong Li, Li Jiang, Jianming Hu, Xianyuan Zhan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2025a39</p>
<p>Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, and 1 others. 2025b. Llm mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources. arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024a. A survey on large language model based autonomous agents. 18186345</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Advances in Neural Information Processing Systems. 2024b37Qingsong Wen, Wei Ye, and 1 others</p>
<p>PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization. Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan, Annual Meeting of the Association for Computational Linguistics. 2022</p>
<p>Mixture of knowledge minigraph agents for literature review generation. Zhi Zhang, Yan Liu, Sheng-Hua Zhong, Gong Chen, Yu Yang, Jiannong Cao, AAAI Conference on Artificial Intelligence. 202539</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Can llm replace stack overflow? a study on robustness and reliability of large language model code generation. Li Zhong, Zilong Wang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202438</p>            </div>
        </div>

    </div>
</body>
</html>
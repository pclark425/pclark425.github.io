<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5231 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5231</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5231</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-55367fbade73f96181ffcf52169d0471d4c014a2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/55367fbade73f96181ffcf52169d0471d4c014a2" target="_blank">GraphText: Graph Reasoning in Text Space</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have gained the ability to assimilate human knowledge and facilitate natural language interactions with both humans and other LLMs. However, despite their impressive achievements, LLMs have not made significant advancements in the realm of graph machine learning. This limitation arises because graphs encapsulate distinct relational data, making it challenging to transform them into natural language that LLMs understand. In this paper, we bridge this gap with a novel framework, GraphText, that translates graphs into natural language. GraphText derives a graph-syntax tree for each graph that encapsulates both the node attributes and inter-node relationships. Traversal of the tree yields a graph text sequence, which is then processed by an LLM to treat graph tasks as text generation tasks. Notably, GraphText offers multiple advantages. It introduces training-free graph reasoning: even without training on graph data, GraphText with ChatGPT can achieve on par with, or even surpassing, the performance of supervised-trained graph neural networks through in-context learning (ICL). Furthermore, GraphText paves the way for interactive graph reasoning, allowing both humans and LLMs to communicate with the model seamlessly using natural language. These capabilities underscore the vast, yet-to-be-explored potential of LLMs in the domain of graph machine learning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5231.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5231.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GRAPHText (Graph Reasoning in Text Space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that converts graphs to natural-language prompts via a graph-syntax tree (an ordered DAG) whose traversal yields a text sequence fed to an LLM; supports training-free in-context learning and instruction-tuning. It composes node text attributes and inter-node relationships (matrices) and arranges them in an internal-node hierarchy (e.g., attribute type above relation type).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-syntax tree (tree-based graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Builds for each target node an ego-subgraph based on chosen relation matrices R; constructs leaf nodes from textual node-attributes F (raw text attributes, discretized continuous features, synthetic text from propagation), creates internal nodes describing relation types (e.g., center-node, 1st-hop, 2nd-hop) and attribute types, orders leaves according to relationship to center node, then topologically traverses the ordered DAG to produce a natural-language 'graph prompt' (text sequence) used as input to an LLM. Continuous features are discretized (K-means cluster indices) or mapped to token embeddings (MLP projector) for instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs (citation networks, web-page graphs) and text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Hierarchical and ordered (ordered DAG -> topologically-sortable), interpretable (prompts are human-readable natural language), flexible (can include propagated features A^k X, propagated labels A^k Y_L, PPR, shortest-path matrices, feature-similarity matrices), injects GNN inductive biases by choosing text attributes F and relations R, decouples depth and scope, supports synthetic augmentation (synthetic relations/text) to increase information; potential information loss from discretization of continuous features; compactness depends on chosen subtree size (ego-subgraph sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (multi-choice QA formulation) on Cora, Citeseer, Texas, Wisconsin, Cornell; also experiments on text-attributed graphs (Cora-TAG, Citeseer-TAG); interactive reasoning examples and ablations (few-shot ICL, instruction tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (%) for node classification. Representative results: (ICL best GraphText configurations) Cora: 76.5% (Table 3 best GraphText), Citeseer: 58.6%, Texas: 75.7% (Table 3). Comparisons in Table 1 show large variability depending on text-attribute/relation choices (e.g., naive label-only flattening: Cora 26.3% → adding discretized features and synthetic relations raises Cora to 52.1% and further tuning to best ~68.3% in some configurations). Instruction-tuned Llama-2-7B on continuous features achieved e.g. Cora (feat) 87.11% (Table 4). Interactive results: GPT-4 accuracy on a Cora example improved from 73.3% to 100% after human interaction; ChatGPT from 26.7% to 63.6% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Outperforms simple flattening/label-only text prompts (which sometimes perform near-random); compared to GNN baselines results are mixed — GraphText ICL matches or surpasses some supervised GNNs on certain datasets (e.g., Texas: GraphText 75.7% vs GCN 59.5%), but on other benchmarks GNNs remain stronger (e.g., Cora: GCN 81.4% vs GraphText ~76.5%). Ablations (sequence/set/rev-hierarchy/w.o internal nodes) demonstrate the graph-syntax tree design is superior (see Table 3: rev. hierarchy -10.7% on Cora, sequence -12.4% on Cora, set -13.9% on Cora relative to GraphText). Synthetic relations and synthetic text attributes substantially boost performance across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Design choices for text-attribute set F and relation set R form a large hyperparameter space requiring search or expertise; discretization of continuous features is lossy and can be ineffective (authors note K-means discretization may be inadequate); closed-source LLMs (ChatGPT/GPT-4) cannot ingest continuous embeddings natively leading to poor performance when relying on continuous features; smaller open-source LLMs perform worse on raw text prompts unless instruction-tuned; reliance on label-like attributes in the best settings makes method reminiscent of label-propagation and may limit generalization; sensitivity to tree hierarchy and ordering (ablation drops show this).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphText: Graph Reasoning in Text Space', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5231.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5231.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flattening / Direct serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct flattening / linearization of nodes and edges (sequential serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple graph-to-text approach that concatenates node attributes and relations into a single sequence (no hierarchical tree); used by prior works and evaluated here as baseline/ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the potential of large language models (llms) in learning on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequence flattening / linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serializes nodes and edges (e.g., raw labels or text attributes) into a flat ordered list or XML-like sequence without explicit internal-node hierarchy; can be as simple as listing center-node, neighbors, and their labels/texts in one sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs and text-attributed graphs (same datasets as GraphText experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple and easy to implement, preserves minimal structure via ordering if any, low interpretability of structure to LLMs (no explicit relation labels), no explicit hierarchical or typed internal nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (few-shot/ICL) on same datasets (Cora, Citeseer, Texas, etc.) used as a baseline/ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (%) shown to be poor when naive: example GraphText-ICL with label-only flattening produced Cora 26.3% and Citeseer 13.7% (Table 1), often close to or worse than random in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Performed substantially worse than the graph-syntax tree representation and worse than augmenting with discretized features and synthetic relations; authors cite prior flattening-based works (Wang et al., 2023a; Chen et al., 2023) and show that naive label flattening is inadequate for LLM-based graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit encoding of relation types or structured hierarchy leads LLMs to treat problem as naive label counting; ordering ambiguity and lack of typed internal nodes reduce effectiveness; poor performance unless additional inductive-bias features or synthetic relations are added.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphText: Graph Reasoning in Text Space', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5231.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5231.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation variants (sequence / set / rev. hierarchy / w/o internal nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation representations of the graph-syntax tree (sequence linearization, set prompt, reversed hierarchy, no internal nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of graph-to-text conversion tested to measure importance of tree hierarchy, internal nodes, and ordering; used in controlled ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequence / set / reversed-hierarchy / no-internal-nodes</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Sequence: remove tree hierarchy to produce a plain ordered list of text attributes; Set: remove order producing an unordered set of attributes; reversed-hierarchy: swap the hierarchy levels (relation type at top, attribute type below); w/o internal nodes: remove intermediate descriptor nodes but preserve indentation/implicit hierarchy.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Attributed graphs (Cora, Citeseer, Texas) used in ablation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Tested properties: ordering sensitivity (sequence vs set), hierarchical labeling of attribute types (with vs without internal nodes), and level arrangement (rev. hierarchy). Results measure interpretability and ability of LLMs to use structural hints.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Node classification (ICL) on Cora, Citeseer, Texas (ablation study reported in Table 3 and Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracy (%) and delta relative to GraphText: On Cora (GraphText baseline 76.5%): rev. hierarchy 68.3% (-10.7%), w/o int. nodes 67.8% (-11.4%), sequence 67.0% (-12.4%), set 65.9% (-13.9%). On Citeseer: sequence -9.6% drop, etc. (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>All ablations underperform the proposed graph-syntax tree; sequence and set prompts cause the largest drops, demonstrating that explicit hierarchical structure and ordering improve LLM graph reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ablations show LLMs rely on the explicit tree hierarchy and typed internal nodes to avoid naive label-counting; removing these design elements harms performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphText: Graph Reasoning in Text Space', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5231.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5231.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-means discretization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-means cluster discretization of continuous features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method used to convert continuous node features or embeddings into discrete textual tokens by assigning each node's continuous feature to a K-means cluster index (used as a length-1 text token).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>K-means discretization for continuous-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply K-means clustering to continuous features X (with K equal to number of classes in experiments), use cluster index as a single discrete token in the node's textual attribute set F; alternative for instruction tuning maps continuous features via an MLP projector into LLaMA-2 token embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Graphs with continuous node features (Cora, Citeseer, webKB datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple and fast; yields discrete tokens acceptable to closed-source LLMs that only accept text; potentially lossy and coarse (one-token representation), efficacy depends on clustering quality and alignment between feature clusters and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as part of GraphText input in node classification ICL experiments; also used in instruction tuning via embedding projection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When added to label-only prompts, discretized features improve performance (example: label-only Cora 26.3% → label+feat (with synthetic relations) Cora 52.1% in Table 1), but authors note discretization can limit performance and attribute mismatch can hinder reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Better than label-only flattening in many experiments, but less effective than richer synthetic text attributes or direct continuous embedding access (the latter enabled on open-source LLMs with MLP projector and LoRA fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors highlight discretization as a key limitation (lossy, may not align with label space), and recommend better continuous-to-text mappings or projection into model embedding space for improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphText: Graph Reasoning in Text Space', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the potential of large language models (llms) in learning on graphs. <em>(Rating: 2)</em></li>
                <li>Can language models solve graph problems in natural language? <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking. <em>(Rating: 2)</em></li>
                <li>Natural language is all a graph needs. <em>(Rating: 2)</em></li>
                <li>Predict then propagate: Graph neural networks meet personalized pagerank <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5231",
    "paper_id": "paper-55367fbade73f96181ffcf52169d0471d4c014a2",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "GraphText",
            "name_full": "GRAPHText (Graph Reasoning in Text Space)",
            "brief_description": "A framework that converts graphs to natural-language prompts via a graph-syntax tree (an ordered DAG) whose traversal yields a text sequence fed to an LLM; supports training-free in-context learning and instruction-tuning. It composes node text attributes and inter-node relationships (matrices) and arranges them in an internal-node hierarchy (e.g., attribute type above relation type).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "graph-syntax tree (tree-based graph-to-text)",
            "representation_description": "Builds for each target node an ego-subgraph based on chosen relation matrices R; constructs leaf nodes from textual node-attributes F (raw text attributes, discretized continuous features, synthetic text from propagation), creates internal nodes describing relation types (e.g., center-node, 1st-hop, 2nd-hop) and attribute types, orders leaves according to relationship to center node, then topologically traverses the ordered DAG to produce a natural-language 'graph prompt' (text sequence) used as input to an LLM. Continuous features are discretized (K-means cluster indices) or mapped to token embeddings (MLP projector) for instruction tuning.",
            "graph_type": "Attributed graphs (citation networks, web-page graphs) and text-attributed graphs",
            "representation_properties": "Hierarchical and ordered (ordered DAG -&gt; topologically-sortable), interpretable (prompts are human-readable natural language), flexible (can include propagated features A^k X, propagated labels A^k Y_L, PPR, shortest-path matrices, feature-similarity matrices), injects GNN inductive biases by choosing text attributes F and relations R, decouples depth and scope, supports synthetic augmentation (synthetic relations/text) to increase information; potential information loss from discretization of continuous features; compactness depends on chosen subtree size (ego-subgraph sampling).",
            "evaluation_task": "Node classification (multi-choice QA formulation) on Cora, Citeseer, Texas, Wisconsin, Cornell; also experiments on text-attributed graphs (Cora-TAG, Citeseer-TAG); interactive reasoning examples and ablations (few-shot ICL, instruction tuning).",
            "performance_metrics": "Accuracy (%) for node classification. Representative results: (ICL best GraphText configurations) Cora: 76.5% (Table 3 best GraphText), Citeseer: 58.6%, Texas: 75.7% (Table 3). Comparisons in Table 1 show large variability depending on text-attribute/relation choices (e.g., naive label-only flattening: Cora 26.3% → adding discretized features and synthetic relations raises Cora to 52.1% and further tuning to best ~68.3% in some configurations). Instruction-tuned Llama-2-7B on continuous features achieved e.g. Cora (feat) 87.11% (Table 4). Interactive results: GPT-4 accuracy on a Cora example improved from 73.3% to 100% after human interaction; ChatGPT from 26.7% to 63.6% (Table 2).",
            "comparison_to_other_representations": "Outperforms simple flattening/label-only text prompts (which sometimes perform near-random); compared to GNN baselines results are mixed — GraphText ICL matches or surpasses some supervised GNNs on certain datasets (e.g., Texas: GraphText 75.7% vs GCN 59.5%), but on other benchmarks GNNs remain stronger (e.g., Cora: GCN 81.4% vs GraphText ~76.5%). Ablations (sequence/set/rev-hierarchy/w.o internal nodes) demonstrate the graph-syntax tree design is superior (see Table 3: rev. hierarchy -10.7% on Cora, sequence -12.4% on Cora, set -13.9% on Cora relative to GraphText). Synthetic relations and synthetic text attributes substantially boost performance across datasets.",
            "limitations_or_challenges": "Design choices for text-attribute set F and relation set R form a large hyperparameter space requiring search or expertise; discretization of continuous features is lossy and can be ineffective (authors note K-means discretization may be inadequate); closed-source LLMs (ChatGPT/GPT-4) cannot ingest continuous embeddings natively leading to poor performance when relying on continuous features; smaller open-source LLMs perform worse on raw text prompts unless instruction-tuned; reliance on label-like attributes in the best settings makes method reminiscent of label-propagation and may limit generalization; sensitivity to tree hierarchy and ordering (ablation drops show this).",
            "uuid": "e5231.0",
            "source_info": {
                "paper_title": "GraphText: Graph Reasoning in Text Space",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Flattening / Direct serialization",
            "name_full": "Direct flattening / linearization of nodes and edges (sequential serialization)",
            "brief_description": "A simple graph-to-text approach that concatenates node attributes and relations into a single sequence (no hierarchical tree); used by prior works and evaluated here as baseline/ablation.",
            "citation_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "mention_or_use": "use",
            "representation_name": "sequence flattening / linearization",
            "representation_description": "Serializes nodes and edges (e.g., raw labels or text attributes) into a flat ordered list or XML-like sequence without explicit internal-node hierarchy; can be as simple as listing center-node, neighbors, and their labels/texts in one sequence.",
            "graph_type": "Attributed graphs and text-attributed graphs (same datasets as GraphText experiments)",
            "representation_properties": "Simple and easy to implement, preserves minimal structure via ordering if any, low interpretability of structure to LLMs (no explicit relation labels), no explicit hierarchical or typed internal nodes.",
            "evaluation_task": "Node classification (few-shot/ICL) on same datasets (Cora, Citeseer, Texas, etc.) used as a baseline/ablation.",
            "performance_metrics": "Accuracy (%) shown to be poor when naive: example GraphText-ICL with label-only flattening produced Cora 26.3% and Citeseer 13.7% (Table 1), often close to or worse than random in experiments.",
            "comparison_to_other_representations": "Performed substantially worse than the graph-syntax tree representation and worse than augmenting with discretized features and synthetic relations; authors cite prior flattening-based works (Wang et al., 2023a; Chen et al., 2023) and show that naive label flattening is inadequate for LLM-based graph reasoning.",
            "limitations_or_challenges": "No explicit encoding of relation types or structured hierarchy leads LLMs to treat problem as naive label counting; ordering ambiguity and lack of typed internal nodes reduce effectiveness; poor performance unless additional inductive-bias features or synthetic relations are added.",
            "uuid": "e5231.1",
            "source_info": {
                "paper_title": "GraphText: Graph Reasoning in Text Space",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Ablation variants (sequence / set / rev. hierarchy / w/o internal nodes)",
            "name_full": "Ablation representations of the graph-syntax tree (sequence linearization, set prompt, reversed hierarchy, no internal nodes)",
            "brief_description": "Variants of graph-to-text conversion tested to measure importance of tree hierarchy, internal nodes, and ordering; used in controlled ablation studies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "sequence / set / reversed-hierarchy / no-internal-nodes",
            "representation_description": "Sequence: remove tree hierarchy to produce a plain ordered list of text attributes; Set: remove order producing an unordered set of attributes; reversed-hierarchy: swap the hierarchy levels (relation type at top, attribute type below); w/o internal nodes: remove intermediate descriptor nodes but preserve indentation/implicit hierarchy.",
            "graph_type": "Attributed graphs (Cora, Citeseer, Texas) used in ablation experiments",
            "representation_properties": "Tested properties: ordering sensitivity (sequence vs set), hierarchical labeling of attribute types (with vs without internal nodes), and level arrangement (rev. hierarchy). Results measure interpretability and ability of LLMs to use structural hints.",
            "evaluation_task": "Node classification (ICL) on Cora, Citeseer, Texas (ablation study reported in Table 3 and Figure 4).",
            "performance_metrics": "Accuracy (%) and delta relative to GraphText: On Cora (GraphText baseline 76.5%): rev. hierarchy 68.3% (-10.7%), w/o int. nodes 67.8% (-11.4%), sequence 67.0% (-12.4%), set 65.9% (-13.9%). On Citeseer: sequence -9.6% drop, etc. (Table 3).",
            "comparison_to_other_representations": "All ablations underperform the proposed graph-syntax tree; sequence and set prompts cause the largest drops, demonstrating that explicit hierarchical structure and ordering improve LLM graph reasoning.",
            "limitations_or_challenges": "Ablations show LLMs rely on the explicit tree hierarchy and typed internal nodes to avoid naive label-counting; removing these design elements harms performance substantially.",
            "uuid": "e5231.2",
            "source_info": {
                "paper_title": "GraphText: Graph Reasoning in Text Space",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "K-means discretization",
            "name_full": "K-means cluster discretization of continuous features",
            "brief_description": "A method used to convert continuous node features or embeddings into discrete textual tokens by assigning each node's continuous feature to a K-means cluster index (used as a length-1 text token).",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "K-means discretization for continuous-to-text",
            "representation_description": "Apply K-means clustering to continuous features X (with K equal to number of classes in experiments), use cluster index as a single discrete token in the node's textual attribute set F; alternative for instruction tuning maps continuous features via an MLP projector into LLaMA-2 token embedding space.",
            "graph_type": "Graphs with continuous node features (Cora, Citeseer, webKB datasets)",
            "representation_properties": "Simple and fast; yields discrete tokens acceptable to closed-source LLMs that only accept text; potentially lossy and coarse (one-token representation), efficacy depends on clustering quality and alignment between feature clusters and labels.",
            "evaluation_task": "Used as part of GraphText input in node classification ICL experiments; also used in instruction tuning via embedding projection.",
            "performance_metrics": "When added to label-only prompts, discretized features improve performance (example: label-only Cora 26.3% → label+feat (with synthetic relations) Cora 52.1% in Table 1), but authors note discretization can limit performance and attribute mismatch can hinder reasoning.",
            "comparison_to_other_representations": "Better than label-only flattening in many experiments, but less effective than richer synthetic text attributes or direct continuous embedding access (the latter enabled on open-source LLMs with MLP projector and LoRA fine-tuning).",
            "limitations_or_challenges": "Authors highlight discretization as a key limitation (lossy, may not align with label space), and recommend better continuous-to-text mappings or projection into model embedding space for improved performance.",
            "uuid": "e5231.3",
            "source_info": {
                "paper_title": "GraphText: Graph Reasoning in Text Space",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the potential of large language models (llms) in learning on graphs.",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_llms_in_learning_on_graphs"
        },
        {
            "paper_title": "Can language models solve graph problems in natural language?",
            "rating": 2,
            "sanitized_title": "can_language_models_solve_graph_problems_in_natural_language"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking.",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Natural language is all a graph needs.",
            "rating": 2,
            "sanitized_title": "natural_language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Predict then propagate: Graph neural networks meet personalized pagerank",
            "rating": 1,
            "sanitized_title": "predict_then_propagate_graph_neural_networks_meet_personalized_pagerank"
        }
    ],
    "cost": 0.015578499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GRAPHText: Graph Reasoning in Text Space</h1>
<p>Jianan Zhao ${ }^{1,2}$, Le Zhuo ${ }^{3}$, Yikang Shen ${ }^{4}$, Meng Qu ${ }^{1,2}$, Kai Liu ${ }^{5}$<br>Michael Bronstein ${ }^{6}$, Zhaocheng Zhu ${ }^{1,2}$, Jian Tang ${ }^{1,7,8}$<br>${ }^{1}$ Mila - Québec AI Institute, ${ }^{2}$ Université de Montréal, ${ }^{3}$ Beihang University,<br>${ }^{4}$ MIT-IBM Watson AI Lab, ${ }^{5}$ Division of gRED Computational Science, Genentech Inc.,<br>${ }^{6}$ University of Oxford, ${ }^{7}$ HEC Montréal, ${ }^{8}$ Canadian Institute for Advanced Research (CIFAR)</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have gained the ability to assimilate human knowledge and facilitate natural language interactions with both humans and other LLMs. However, despite their impressive achievements, LLMs have not made significant advancements in the realm of graph machine learning. This limitation arises because graphs encapsulate distinct relational data, making it challenging to transform them into natural language that LLMs understand. In this paper, we bridge this gap with a novel framework, GraphTEXT, that translates graphs to natural language. GraphTEXT derives a graph-syntax tree for each graph that encapsulates both the node attributes and inter-node relationships. Traversal of the tree yields a graph text sequence, which is then processed by an LLM to treat graph tasks as text generation tasks. Notably, GRAPHTEXT offers multiple advantages. It introduces training-free graph reasoning: even without training on graph data, GRAPHTEXT with ChatGPT can achieve on par with, or even surpassing, the performance of supervised-trained graph neural networks through in-context learning (ICL). Furthermore, GRAPHTEXT paves the way for interactive graph reasoning, allowing both humans and LLMs to communicate with the model seamlessly using natural language. These capabilities underscore the vast, yet-to-be-explored potential of LLMs in the domain of graph machine learning.</p>
<h2>1 INTRODUCTION</h2>
<p>Language stands as a cornerstone of human civilization, acting as the primary medium for knowledge encoding, reasoning, and communication. Large language models (LLMs), pre-trained on extensive text corpora, have showcased remarkable reasoning skills (Brown et al., 2020; Bubeck et al., 2023). These LLMs can communicate via natural language both internally (Wei et al., 2022) and externally with humans or other LLMs (Li et al., 2023), demonstrating exceptional skills such as multi-step reasoning (Yao et al., 2023a), decision-making (Yao et al., 2023b; Liang et al., 2023), tool use (Schick et al., 2023), and multi-agent collaboration (Park et al., 2023; Hong et al., 2023).</p>
<p>Motivation. Despite the remarkable success of LLMs in handling natural languages, their application to other data modalities presents unique challenges, primarily because these data often lack straightforward transformation into sequential text. These challenges are especially severe when dealing with graph-structured data, as different graphs define structure and features in distinct ways. Therefore, existing efforts within the graph machine learning field commonly require the training of specific graph neural networks (GNNs) tailored to individual graphs (Kipf \&amp; Welling, 2017; Velickovic et al., 2018; Xu et al., 2019). Often, models trained on one graph cannot generalize to the unseen structure and feature representations of other graphs. Moreover, the gap between graphs and human languages hinders the application of natural language reasoning to facilitate graph reasoning.</p>
<p>In light of these limitations, a question arises: can we derive a language for graph in natural language? In this paper, we give an affirmative answer by proposing to use tree as an intermediary, elegantly bridging structured data and one-dimensional sequential language. Essentially, a tree exhibits a hierarchical structure, and traversing it yields a one-dimensional sequence. On top of that, as shown in Figure 1 (c), we propose a novel framework GRAPHTEXT, which takes graph data to build a graph-syntax tree. Traversing it results in a graph prompt expressed in natural language, allowing an LLM to approach graph reasoning as a text-generation task.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison between (a) the GNN framework and (b) the proposed GRAPHTEXT framework. For different graphs $G_{1}$ and $G_{2}$, different GNNs $\theta_{1} \theta_{2}$ are trained to make a graph-specific output prediction in continuous form. In contrast, GRAPHTEXT encodes the graph information to text sequences $T_{\mathrm{in}}^{(1)}$ and $T_{\mathrm{in}}^{(2)}$, and generates text reasoning and prediction $T_{\mathrm{out}}^{(1)}$ and $T_{\mathrm{out}}^{(2)}$ with a graph-shared LLM $\phi$. GRAPHTEXT leverages a pre-trained LLM to perform training-free graph reasoning and enables human and AI interaction for graph reasoning in natural language. (c) An example of the GRAPHTEXT framework that classifies node 0: Given a graph, GRAPHTEXT constructs a graph-syntax tree that contains both node attributes (e.g. feature and label) and relationships (e.g. center-node, 1st-hop, and 2nd-hop). Then, GRAPHTEXT traverses the graph-syntax tree to obtain a sequential text, i.e. graph prompt, and let LLM perform graph reasoning in text space.</p>
<p>Main contributions. First, GRAPHTEXT serves as a flexible and general framework for graph reasoning. It can incorporate common inductive bias of GNNs, such as feature propagation and feature similarity-based propagation, simply by constructing different graph-syntax trees. It also serves as a general framework for graph reasoning for both in-context learning and instruction tuning, on both general graphs and text-attributed graphs. Second, we show that GRAPHTEXT enables the possibility of training-free graph reasoning. The training-free property enables us to deploy GRAPHTEXT not only with open-source LLMs, but also with powerful closed-source LLMs. Remarkably, even without training on graph data, GRAPHTEXT with ChatGPT can deliver performance on par with, or even surpassing, supervised graph neural networks through in-context learning. This highlights the vast potential of foundation models in the realm of graph machine learning. Third, GRAPHTEXT fosters interactive graph reasoning: With its capacity to generate and explain predictions in natural language, humans can directly engage with GRAPHTEXT. As shown in Figure 2 (b), through interactions with humans and other LLMs, GRAPHTEXT refines its graph reasoning capabilities.</p>
<h1>2 Methodology</h1>
<p>In this section, we introduce GRAPHTEXT to perform graph reasoning in text space. Out of the three fundamental problems of graph ML (graph classification, node classification, and link prediction), we take node classification as an example to introduce our idea. We however note that our discussion applies to other graph tasks.</p>
<h3>2.1 The GraphText Framework</h3>
<p>Let us be given an attributed graph $G=(V, E, \boldsymbol{X})$ with nodes $V$ and edges $E$, whose structure is represented as the $|V| \times|V|$ adjacency matrix $\boldsymbol{A}$ and node attributes as the $|V| \times d$ feature matrix $\boldsymbol{X}$. Given a subset $L \subset V$ of labeled nodes with labels $Y_{L}$, the goal of node classification is to predict the labels $Y_{U}$ of the unlabeled nodes $U=V \backslash L$. Graph Neural Networks (GNNs) are the standard architecture for such problems. As shown in Figure 1 (a), a GNN directly learns a parametric map</p>
<p>$$
\hat{y}<em _mathrm_GNN="\mathrm{GNN">{i}=f</em>
$$}}\left(G ; \theta_{G}\right)_{i</p>
<p>between the input graph $G \in \mathcal{G}$ and the output labels $\hat{Y} \in \mathcal{Y}$, assigning to each node $i$ its predicted label $\hat{y}<em G="G">{i}$. The training of GNN attempts to find parameters $\theta</em>}$ such that $\hat{y<em i="i">{i} \approx y</em>$ on the training set.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Few-shot demonstrations (blue) and the target node #2188 (green) to predict on Cora. (b) An illustration of how human interaction changes the graph reasoning of an LLM, where the LLM previously has the prior that the center-node is vital. (c) Behaviors of LLMs after given demonstrations/human interaction: update graph prior to bias more on PPR (personalized pagerank); leads to confusion or refusion. Details are discussed in Section 4.2.</p>
<p>Note that standard GNNs are graph-specific functions, i.e. $f_{\mathrm{GNN}}\left(\cdot ; \theta_{G}\right): G \mapsto \hat{Y}$, which do not generalize to other graphs, since other graphs $G^{\prime} \in \mathcal{G}$ define distinct distributions of $Y^{\prime}, \boldsymbol{A}^{\prime}$, and $\boldsymbol{X}^{\prime}$, or even different types of features such as continuous, categorical, or text features.</p>
<p>To solve the generalization problem mentioned above, this paper proposes to perform graph reasoning as a text-to-text problem (Raffel et al., 2020), as shown in Figure 1 (b). Inspired by prompt tuning (Brown et al., 2020; Liu et al., 2023), we construct two graph-specific maps to form the input and output space of a text-to-text problem: a map $g: G \mapsto T_{i n}$ that maps the graph input to text space, and a map $h: T_{\text {out }} \mapsto \hat{Y}$ that maps the output of LLM to label predictions $\hat{Y}$. In this way, we can use a generative large language model $f_{\mathrm{LLM}}$ to perform graph reasoning as</p>
<p>$$
\tilde{y}<em _mathrm_LLM="\mathrm{LLM">{i}=h\left(f</em> ; \phi\right)\right)
$$}}\left(g(G)_{i</p>
<p>where $g(G)<em i="i" n="n">{i}=T</em>(\cdot ; \phi)$, but also enables interactions between human and AI agents to facilitate graph reasoning.}[i]$ denotes the text sequence representing node $i$. Different from GNNs, $f_{\mathrm{LLM}}(\cdot ; \phi): \mathcal{T} \rightarrow \mathcal{T}$ is a graph-shared function, where both input and output are in text space, i.e. $T_{i n}, T_{\text {out }} \in \mathcal{T}$, which not only activates of parametric knowledge encoded in the model $f_{\mathrm{LLM}</p>
<p>Specifically, as node classification, link prediction and graph classification are essentially classification tasks, we can naturally formulate these graph reasoning tasks as multi-choice QA problem (Robinson \&amp; Wingate, 2023) and design $h$ as the map from predicted choice $T_{\text {out }} \in \mathcal{T}$ to the corresponding prediction $\hat{Y}$. However, the design of $g$ that maps the structural graph information into the text space of natural language is still a non-trivial problem.</p>
<p>The primary challenge in converting graph data to language lies in handling its relational structure, which fundamentally deviates from the one-dimensional sequential nature of text data. Inspired by linguistic syntax trees (Chiswell \&amp; Hodges, 2007), we introduce graph-syntax trees as a bridge between relational and sequential data. The traversal of such a tree produces a sentence in natural language, which is fed to LLM for graph reasoning. Specifically, as shown in Figure 1 (c), we compose a graph-syntax tree consisting of node text attributes and inter-node relationships. Next, we describe how to compose the node text attributes and inter-node relationships in Section 2.2, and how to build a graph-syntax tree in Section 2.3.</p>
<h1>2.2 Textual and Relational Information for Syntax Trees</h1>
<p>A graph syntax tree is composed of both textual and relational information derived from the graph. For textual information, GraphTEXT constructs a text attribute set $F \in \mathcal{T}$ for an arbitrary graph $G \in \mathcal{G}$ (with or without text-attributes) composed of multiple types of attributes for each node, e.g. feature and label, in natural language. Specifically, for each node $v_{i}$ and feature type $m$, we</p>
<p>construct a text sequence $\mathrm{F}_{m}[i]$ in natural language:</p>
<p>$$
\mathrm{F}<em 1="1">{m}[i]=\left{t</em>
$$}, t_{2}, \cdots t_{l_{m}}\right}, \quad \mathrm{F}_{m}[i] \in \mathcal{T</p>
<p>where the sequence is of length $l_{m}$. Each text attribute $\mathrm{F}<em n="n">{m}$ can be derived from either sequential text features or continuous features. For text features, they can be directly added to the text attributes $F$. For example, we can directly add the text sequences of "title" and "abstract" into $F$ for citation graphs. For continuous features, e.g. the raw feature $\boldsymbol{X}$ or other graph embeddings, we propose to use discretization methods, e.g. clustering, to transform the continuous feature into a discrete space and then derive sequential data from it. For simplicity, we use the cluster index of K-means to generate a sequence of length 1 for all continuous features as K-means is effective in our experiments.
For relational information, GraphText derives a set of matrices $R$ where each $\boldsymbol{R}</em>$ may be the original graph (Kipf \&amp; Welling, 2017), high-order connectedness (Xu et al., 2018), page-rank matrices (Klicpera et al., 2019), or any matrices that encode node-pair information. These relationships play an important role in determining the nodes and structure of the graph-syntax tree, which further improves the graph text prompt.} \in R$ is a $|V| \times|V|$ matrix, depicting one type of relationship between nodes. Choices of $\boldsymbol{R}_{n</p>
<h1>2.3 Graph-Syntax Tree Composition</h1>
<p>We now describe how to build a graph prompt using a graph-syntax tree of the graph text attributes and relationships $F$ and $R$. By analogy to the syntax tree in linguistics, we define a graph-syntax tree as an ordered tree: a directed acyclic graph (DAG) with nodes $\hat{T} \in \mathcal{T}$ and edges $\hat{E}$. In a graph-syntax tree, e.g. the one in Figure 1 (c), each node stores a text sequence in natural language, where the root node is an empty node; the leaf nodes $\hat{T}<em i="i">{\mathrm{L}}$ are text sequences in the graph text attributes, i.e. $\forall \mathrm{T}</em>} \in$ $\hat{T<em i="i">{\mathrm{L}}, \mathrm{T}</em>} \in F$; the internal nodes $\hat{T<em i="i">{\mathrm{I}}$ are text sequences in natural language, i.e. $\forall \mathrm{T}</em>} \in \hat{T<em i="i">{\mathrm{I}}, \mathrm{T}</em>} \in \mathcal{T}$. A graph syntax tree is constructed in three steps: (1) construct an ego-subgraph (Hamilton et al., 2017) $G_{i}$ for target node $v_{i}$ based on relationship $R$ (2) select leaf nodes $\hat{T<em _mathrm_I="\mathrm{I">{\mathrm{L}}$ based on the relationship $R$. (3) build up internal nodes $\hat{T}</em>$. Notably, the leaf nodes are sorted according to their relationships with the center-node, preserving the relative relationship in a one-dimensional order.
We illustrate this with a node classification example shown in Figure 1 (c). Before building the graph-syntax tree, GraphText determines the text attributes composed of raw features and observed labels, i.e. $F=\left{\mathrm{F}}}$ and edges $\hat{E}$ based on the leaf nodes' types and their relationship with the graph ${ }^{1<em _mathrm_Y="\mathrm{Y">{\mathrm{X}}[i], \mathrm{F}</em>}}[i] \mid \forall v_{i} \in V\right}$, and a relationship set composed of determined by shortest path distance (SPD): center-node, 1st-hop, and 2nd-hop, i.e. $R=\left{\boldsymbol{R<em _mathrm_SPD="\mathrm{SPD">{\mathrm{SPD}=0}, \boldsymbol{R}</em>}=1}, \boldsymbol{R<em i="i">{\mathrm{SPD}=2}\right}$. Then, for target node $v</em>}$ ( 0 in the example), an egosubgraph (Hamilton et al., 2017) (with nodes [0,1,2,3,4]) is sampled based on the relative relationship between $v_{i}$ and other nodes. Finally, a graph-syntax tree is constructed with leaf nodes $\hat{T<em _mathrm_X="\mathrm{X">{\mathrm{L}}=\left{\mathrm{F}</em>}}[0], \mathrm{F<em _mathrm_X="\mathrm{X">{\mathrm{X}}[1], \mathrm{F}</em>}}[2] \mathrm{F<em _mathrm_X="\mathrm{X">{\mathrm{X}}[3], \mathrm{F}</em>}}[4], \mathrm{F<em _mathrm_Y="\mathrm{Y">{\mathrm{Y}}[1], \mathrm{F}</em>=$ {"center-node", "1sthop", "2nd-hop", "label", "feature"}, and the corresponding edges. The traversal of the resulting graph-syntax tree leads to a text sequence in natural language.
Compared with the direct flattening of (sub)graphs (Wang et al., 2023a; Chen et al., 2023), using a graph-syntax tree-based prompt has many advantages: Above all, unlike a graph, which has no topology order, a syntax tree is a DAG that can be topologically sorted, which gracefully converts a relational structure to a sequence of nodes. Moreover, GraphText easily incorporates the inductive biases of GNNs through the construction of node text attributes $F$ and relationships $R$. For example, we can easily encode the feature-propagation mechanism of GNNs by including a text attribute derived from the propagated feature $\boldsymbol{A}^{k} \boldsymbol{X}$ (Zhang et al., 2022), into the node attributes $F$. We can also incorporate the feature similarity-based aggregation (Velickovic et al., 2018) by adding $\boldsymbol{X} \boldsymbol{X}^{\top}$ to $R$. These graph-based inductive biases can significantly boost LLMs' graph reasoning performance (further discussed in Section 4.1). Last but not least, a tree naturally defines a hierarchical structure, which LLMs are proficient in reasoning on (Liang et al., 2023), by training on code data (Chen et al., 2021) and web page data (Touvron et al., 2023).}}[3]\right}$, the internal nodes $\hat{T}_{\mathrm{I}</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3 Related Work</h1>
<p>Unlock Graph Space for Language Models. Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023; Bubeck et al., 2023) possess impressive reasoning capabilities (Wei et al., 2022; Yao et al., 2023a; Fu et al., 2023). At the heart of LLMs' reasoning prowess is their ability to process and generate natural language inputs and outputs, enabling flexible interactions (Dohan et al., 2022) with both humans and AI agents. This unique capability empowers them with remarkable abilities such as complex reasoning (Fu et al., 2023) and decision-making (Yao et al., 2023b; Liang et al., 2023). Despite their success, applying LLMs to relational graph data remains challenging, primarily due to the absence of a natural language representation for graphs. GraphText bridges this gap by providing a novel framework that enables LLMs to seamlessly integrate and reason over relational graph data using the same natural language capabilities, thereby unlocking their potential for a wide range of graph-based applications.</p>
<p>Training-free Graph Reasoning Graph neural networks (GNNs) (Kipf \&amp; Welling, 2017; Xu et al., 2019) excel in handling relational graph data, thanks to the message-passing mechanism for aggregation and transformation of neighborhood representations. Their standout performance can be attributed to their intrinsic capability to assimilate graph inductive biases. This incorporation of biases is achieved by designing representations with the graph structure in perspective, such as position embeddings (Dwivedi et al., 2022; Ying et al., 2021; Kreuzer et al., 2021) and propagated features (Wu et al., 2019; Zhang et al., 2022). Furthermore, they can introduce diverse aggregation methods, like feature similarity-based message passing (Velickovic et al., 2018; Zhao et al., 2021) or high-order aggregation (Klicpera et al., 2019; Bojchevski et al., 2020; Chien et al., 2021). However, as highlighted in Section 2.1, due to the variance in both structure and feature, the majority of GNNs are graph-specific. They are tailored for a particular graph type with consistent features and structures, thus posing challenges for generalization to different graphs.</p>
<p>In a parallel vein, GraphText also taps into the potent ability to infuse graph inductive biases for graph reasoning, achieved through designing both the textual and relational aspects of the graphsyntax tree. Setting itself apart from GNNs, GraphText approaches graph reasoning in a graphshared domain, facilitating the broader applicability of a single LLM to diverse graphs and offering training-free and interactive graph reasoning.</p>
<p>Connecting Both Worlds. Recent endeavors (Chien et al., 2022; Zhao et al., 2023; He et al., 2023) have aimed to merge the language and graph domains. Most methods involve transitioning the problem into a graph-specific realm, utilizing a combination of a text-encoder (either pre-trained (Chien et al., 2022) or learned (Li et al., 2021)) and a GNN predictor. This methodology still falls into a graph-specific paradigm. Very recently, there are concurrent works (Guo et al., 2023; Ye et al., 2023; Wang et al., 2023a; Chen et al., 2023) exploring to leverage LLMs for graph-related tasks. These methods either directly flatten the nodes and edges (Guo et al., 2023; Wang et al., 2023a) or employ rule-based prompts on text-attributed graphs (Chen et al., 2023; Ye et al., 2023).</p>
<p>Nevertheless, GraphText is fundamentally different from these works. Foremost, GraphText proposes a language defined by a graph-syntax tree, offering a flexible and structured approach for seamlessly integrating graph inductive biases. Moreover, it also serves as a general framework for graph reasoning, which can be applied to scenarios encompassing in-context learning and instruction tuning. It accommodates various types of graphs, including general graphs and text-attributed graphs, and is adaptable to both closed-source Large Language Models (LLMs) (OpenAI, 2023; Bubeck et al., 2023) and open-source LLMs (Touvron et al., 2023).</p>
<h2>4 EXPERIMENTS</h2>
<p>We conduct extensive experiments to demonstrate the effectiveness of GraphText. Firstly, in Section 4.1, we delve into the remarkable capacity of GraphText for training-free graph reasoning. Subsequently, Section 4.2 highlights the interactive graph reasoning capabilities of GraphText. We further analyze various ablations of graph-syntax trees in Section 4.3. Concluding our exploration, Section 4.4 illustrates how GraphText can seamlessly function as a versatile framework, catering to both in-context learning and instruction tuning across on both general graph and textattributed graphs.</p>
<p>Table 1: Node classification results (accuracy \%).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Graph Text Attributes</th>
<th style="text-align: center;">Relations</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Citeseer</th>
<th style="text-align: center;">Texas</th>
<th style="text-align: center;">Wisconsin</th>
<th style="text-align: center;">Cornell</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GCN</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">37.8</td>
</tr>
<tr>
<td style="text-align: center;">GAT</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;">GCNII</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">40.5</td>
</tr>
<tr>
<td style="text-align: center;">GATv2</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: center;">GraphTEXT-ICL</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">21.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label+feat</td>
<td style="text-align: center;">ori.+synth.</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label+feat+</td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">24.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">label+feat+synth.</td>
<td style="text-align: center;">ori.+synth.</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">46.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">original</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ori.+synth.</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">51.4</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Few-shot in-context learning node classification accuracy. We perform 1, 3, 5, 10, 15, and 20-shot node classification on Citeseer and Texas datasets.</p>
<h1>4.1 Training-free Graph Reasoning</h1>
<p>One unique ability of GraphTEXT is the training-free graph reasoning by in-context learning (ICL). In this section, we demonstrate this capability on the node classification tasks. Specifically, we use two citation datasets (Cora (McCallum et al., 2000) and Citeseer (Giles et al., 1998)), and three webpage datasets (Texas, Wisconsin, and Cornell (Pei et al., 2020)). The detailed discussion of experimental settings and the dataset statistics can be found in Appendices A. 1 and A.2.</p>
<p>We selected standard GNNs, including GCN (Kipf \&amp; Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines. These GNN baselines are supervised and specific to individual graphs, trained solely for inference on one dataset. In contrast, GraphTEXT utilizes a single pre-trained LLM (ChatGPT) for all datasets without any graph-specific training.</p>
<p>Given an input graph, GraphTEXT constructs a graph-syntax tree by incorporating two types of information: text attributes and relations. We utilize three types of text attributes for the graph: observed labels (referred to as 'label'), features generated by K-means clustering of $\boldsymbol{X}$ (referred to as 'feat'), and synthetic text (referred to as 'synth.') derived from feature- and label- propagation. Additionally, we employ two types of relations: the original graph structure ( $\boldsymbol{A}$ with self-loops) and synthetic relations based on feature similarity, shortest-path distance, and personalized pagerank (Klicpera et al., 2019). The hyperparameter details are provided in Appendix A.3.</p>
<p>Experimental results are depicted in Table 1 and Figure 3. We observe that directly flattening raw text (labels) using relations, as proposed in (Chen et al., 2023), results in poor performance, occasionally worse than random. Incorporating discretized features into text attributes improves the performance slightly.</p>
<p>Integrating the graph inductive bias into both text attributes and relations enhances performance. The addition of synthetic relationships significantly boosts performance across all datasets, indicating that the raw graph lacks sufficient information and requires augmentation. This observation aligns with findings in graph structure learning literature (Franceschi et al., 2019; Zhao et al., 2021). Furthermore, the inclusion of synthetic text attributes is beneficial in most cases. Ultimately,</p>
<p>Table 2: Interactive graph reasoning results (accuracy \%) on Cora (node # 2188). The table showcases the performance of GPT-4 and ChatGPT before and after human interactions with 15 times of evaluation. The reasoning metrics include PPR, Center-node, and instances where the model was Confused to respond or Refused (Conf./Ref.) to make their reasoning/prediction. See Figure 2 (c) for details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Interaction</th>
<th style="text-align: right;">Accuracy</th>
<th style="text-align: center;">Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
<td style="text-align: center;">PPR</td>
<td style="text-align: center;">Center-node</td>
<td style="text-align: center;">Conf./Ref.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Before</td>
<td style="text-align: right;">73.3</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">After</td>
<td style="text-align: right;">$\mathbf{1 0 0 (+ 2 6 . 7 )}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Before</td>
<td style="text-align: right;">26.7</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">After</td>
<td style="text-align: right;">$\mathbf{6 3 . 6 (+ 3 6 . 9 )}$</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">9.1</td>
</tr>
</tbody>
</table>
<p>the combination of synthetic text attributes and synthetic relations yields the highest accuracy for GRAPHTEXT in four out of five datasets.</p>
<p>Remarkably, even though it is not trained on graph data, GRAPHTEXT surpasses several GNN baselines, particularly when the label rate is low (see Figure 3) and in heterophilic datasets. This is due to the merits that, in contrast with standard GNNs, GRAPHTEXT decouples depth and scope in graph reasoning (Zeng et al., 2021). The strong performance of GRAPHTEXT in training-free graph reasoning highlights the substantial potential of leveraging LLMs in graph machine learning.</p>
<h1>4.2 INTERPRETABLE AND INTERACTIVE GRAPH REASONING</h1>
<p>In this section, we illustrate that GRAPHTEXT facilitates effective interactive graph reasoning: through its ability to generate and clarify predictions in natural language, both humans and LLMs can directly interact with GRAPHTEXT.</p>
<p>To illustrate this concept, we will use Cora node #2188. Figure 2 (a) shows two types of text attributes we use: the center-node pseudo labels and the PPR (Personalized PageRank) pseudo label sequence, where the first PPR neighbor denotes the most important label prediction. Upon examining the demonstrations (marked in blue), it becomes apparent that the PPR pseudo-labels provide a more robust mechanism for paper topic prediction. Utilizing either a count of PPR labels followed by a majority vote, or merely referencing the foremost PPR label, consistently results in the correct categorization in the given examples. Hence, based on these graph inductive biases derived from samples, we can reasonably figure out correct topic of the target paper should be A, which not only is the first entry, but also the predominant label in the PPR pseudo-label sequence.</p>
<p>We leverage GRAPHTEXT with ChatGPT and GPT-4 to perform graph reasoning on the provided example. Their respective reasoning processes and outcomes are illustrated and summarized in Figure 2 and Table 4.1 respectively, from which we draw several key insights:</p>
<ol>
<li>LLMs inherently possess the knowledge and inductive bias toward graph reasoning. Specifically, both ChatGPT and GPT-4 acknowledge the importance of center-nodes and sometimes make predictions based on center-node labels. ChatGPT exhibits reasoning with the center-node bias $53.3 \%$ of the time, while GPT-4 does so at a rate of $26.7 \%$.</li>
<li>LLMs can adjust their prior inductive bias based on demonstrations. Through in-context learning, GRAPHTEXT can recalibrate their bias and make more accurate predictions. Our observations indicate that GPT-4 significantly outperforms ChatGPT, achieving an accuracy of $73.3 \%$, markedly superior to ChatGPT's $26.7 \%$.</li>
<li>LLMs can adapt their prior inductive bias based on human feedback. Figure 2 (b) provides an illustrative example, with a detailed reasoning of LLM can be found in Appendix C. Specifically, after human interaction, GPT-4 shows remarkable adaptability, achieving an impeccable accuracy of $100 \%$ and adhering to the PPR logic. Meanwhile, ChatGPT also enhances its performance notably (gaining $36.9 \%$ in accuracy), but occasionally maintains its antecedent biases.</li>
</ol>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablations of graph-syntax trees. (a) An example graph. (b) GraphText text prompt (The full example can be found in Figure 1). (c-f) Text prompts of different tree designs.</p>
<p>Table 3: Ablations of GraphText on Cora, Citeseer and Texas.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Citeseer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Texas</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc. \%</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Acc. \%</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">Acc. \%</td>
<td style="text-align: center;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: center;">GraphText</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">rev. hierarchy</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">-10.7 \%</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">-1.7 \%</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">-3.6 \%</td>
</tr>
<tr>
<td style="text-align: center;">w/o int. nodes</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">-11.4 \%</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">-3.9 \%</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">-0 \%</td>
</tr>
<tr>
<td style="text-align: center;">sequence</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">-12.4 \%</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">-9.6 \%</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">-7.1 \%</td>
</tr>
<tr>
<td style="text-align: center;">set</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">-13.9 \%</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">-3.8 \%</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">-10.6 \%</td>
</tr>
</tbody>
</table>
<p>In summary, through graph reasoning in natural language, GraphText can effectively leverage its pre-trained knowledge to engage in graph reasoning and, crucially, adapt its existing knowledge through demonstrations or external feedback.</p>
<h1>4.3 Ablation Studies on Graph-Syntax Trees</h1>
<p>The graph-syntax tree serves as the core design of GraphText, transforming a graph into a 1dimensional natural language sequence. Within GraphText, the text and relational data of a graph are initially formulated, followed by the construction of a graph-syntax tree. This section delves into the ablations of various methods for building graph-syntax trees.</p>
<p>As shown in Figure 4, besides the proposed GraphText method for constructing a graph-syntax tree, we present four ablation types: (1) Reverse hierarchy (denoted as rev. hierarchy in Figure 4 (c): The tree hierarchy is inverted, positioning the relationship type at the top and the text attribute type at the bottom. (2) Without internal nodes (denoted as w/o int. nodes in Figure 4 (d)): The internal nodes of the graph-syntax tree are eliminated, but the GRAPHTEXT hierarchy remains intact (note the indents are kept, maintaining the hierarchical structure of the tree). (3) Sequential prompt (denoted as sequence in Figure 4 (e)): The tree hierarchy is removed, yielding a sequence of text attributes. (4) Set prompt (denoted as set in Figure 4 (f)): Sequence order is removed, yielding a set.</p>
<p>From Table 3, several observations can be made: (1) The graph-syntax tree of GraphText consistently outperforms the others, underscoring the efficacy of our approach. (2) The hierarchical structure of the tree plays a crucial role in the design of the graph prompt. Specifically, we observe a sheer performance drop when using a sequence or a set to represent the graph information. Upon inspecting the LLM's reasoning, we found it treats graph learning purely as label counting without recognizing structure. (3) Variations in the tree hierarchy design can impact performance; for instance, rev. hierarchy underperforms compared to GraphText. (4) The comparison between w/o int. nodes and GraphText reveals the importance of making LLMs aware of text attribute types. The only exception is the Texas dataset since all types of attributes are almost identical in this dataset (detailedly discussed in Appendix B.3). This suggests that LLMs utilize text descriptions to distinguish and understand different attributes during graph reasoning.</p>
<h3>4.4 EXPERIMENTS ON TEXT-ATTRIBUTED GRAPH</h3>
<p>In this section, we demonstrate that GraphText is also applicable to text-attributed graphs. As depicted in Table 4, we conducted training-free node classification on the Cora and Citeseer datasets</p>
<p>Table 4: Node classification results (accuracy \%) on real-world text attributed graphs. Experiments are conducted using in-context learning with ChatGPT, as well as instruction tuning with Llama-2-7B. Note that "text" refers to raw text attributes, while "feat" represents the continuous features on the graph. The top results for each category are highlighted in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Framework</th>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;">Citeseer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GNNs</td>
<td style="text-align: left;">GCN</td>
<td style="text-align: center;">89.13</td>
<td style="text-align: center;">74.92</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">GAT</td>
<td style="text-align: center;">$\mathbf{8 9 . 6 8}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 3 9}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ChatGPT-text</td>
<td style="text-align: center;">$\mathbf{6 7 . 7 7}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ChatGPT-feat</td>
<td style="text-align: center;">10.68</td>
<td style="text-align: center;">16.14</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ChatGPT-feat+text</td>
<td style="text-align: center;">65.19</td>
<td style="text-align: center;">66.46</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Llama-2-7B-text</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">49.37</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Llama-2-7B-feat</td>
<td style="text-align: center;">$\mathbf{8 7 . 1 1}$</td>
<td style="text-align: center;">$\mathbf{7 4 . 7 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Llama-2-7B-feat+text</td>
<td style="text-align: center;">77.53</td>
<td style="text-align: center;">73.83</td>
</tr>
</tbody>
</table>
<p>with both raw text attributes (Chen et al., 2023) and continuous features (Reimers \&amp; Gurevych, 2019). We observed that using closed-source LLMs, such as ChatGPT, the performance lags behind the GNN baseline methods. Thus, we further explored the potential of instruction tuning on currently available open-source LLMs, such as Llama-2 (Touvron et al., 2023). For natural language prompts construction, we adopted an approach almost identical to the in-context learning setting. Furthermore, we expand the original vocabulary of Llama-2 by introducing selected options as new tokens and then fine-tune the large language model by the widely-used and efficient Low-Rank Adaptation (LoRA) (Hu et al., 2022).</p>
<p>From the results in Table 4, it is evident that even with a relatively smaller open-source model, Llama-2-7B, our best results from instruction tuning across various settings surpass those of ChatGPT and approach the GNN baselines. This validates that our method can be beneficial in an instruction-tuning scenario. It also implies that using GraphText, we can feasibly fine-tune smaller open-source LLMs with reasonable computational costs, achieving performances that can rival or even surpass those of much larger closed-source models, such as ChatGPT or GPT-4.</p>
<p>Another intriguing observation is the notably poor performance of ChatGPT in settings incorporating continuous feature - nearing a random guess. This is attributable to the inherent limitation of these closed-source LLMs: they are designed to process raw discrete text inputs and fail to directly handle the continuous inputs. In contrast, open-source LLMs possess the ability to map these continuous embeddings into their embedding space, facilitating improved performance.</p>
<p>Upon contrasting these two groups of models, we noticed a decline in the performance of opensource models when processing raw text inputs. This decline can be ascribed to the constraints imposed by the size of the LLM parameters and the volume of pre-training corpora used. It suggests that harnessing larger-scale open-source models, such as Llama-2 variants including 13B, 30B, and 70B, would significantly bolster their modeling capacity for raw text. Concurrently, by leveraging the ability to process continuous embeddings, these models would inevitably exhibit enhanced graph reasoning capabilities, paving the way for more sophisticated graph-based applications.</p>
<h1>5 CONCLUSION</h1>
<p>In this paper, we propose GraphText, a framework that enables graph reasoning in text space. It easily incorporates the inductive bias of GNNs by constructing a graph-syntax tree. The traversal of a graph-syntax tree leads to a graph prompt in natural language and is fed to LLM to perform graph reasoning as text generation. GraphText enables training-free graph reasoning where a GRAPHTEXT-LLM can deliver performance on par with, or even surpassing, supervised graph neural networks through in-context learning. What's more, GRAPHTEXT fosters explainable and interactive graph reasoning: GRAPHTEXT performs graph reasoning in natural language which enables humans and LLMs to engage with graph learning using natural language. These abilities highlight the immense and largely untapped potential of LLMs in the realm of graph machine learning.</p>
<h1>ETHICS STATEMENT</h1>
<p>Graphs are prevalent in the real world. On the bright side, GraphText alleviates the computational load and carbon footprint associated with training numerous non-transferable, graph-specific models. However, while the training-free graph reasoning capability of GraphText introduces minimal costs, there's potential for misuse in malicious recommendation systems and malware.</p>
<h2>REFERENCES</h2>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. CoRR, 2023.</p>
<p>Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek Rózemberczki, Michal Lukasik, and Stephan Günnemann. Scaling graph neural networks with approximate pagerank. In $K D D, 2020$.</p>
<p>Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In ICLR, 2022.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tülio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, 2023.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, 2021.</p>
<p>Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In ICML, 2020.</p>
<p>Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms) in learning on graphs. CoRR, 2023.</p>
<p>Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. In ICLR, 2021.</p>
<p>Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, and Inderjit S. Dhillon. Node feature extraction by self-supervised multi-scale neighborhood prediction. In $I C L R, 2022$.</p>
<p>Ian Chiswell and Wilfrid Hodges. Mathematical logic, volume 3 of Oxford texts in logic. Clarendon Press, 2007. ISBN 978-0-19-921562-1.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-Dickstein, Kevin Murphy, and Charles Sutton. Language model cascades. CoRR, 2022.</p>
<p>Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In $I C L R, 2022$.</p>
<p>Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures for graph neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Proceedings of Machine Learning Research, 2019.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In $I C L R, 2023$.</p>
<p>C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing system. In Proceedings of the third ACM conference on Digital libraries, pp. 89-98, 1998.</p>
<p>Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking. CoRR, abs/2305.15066, 2023. doi: 10.48550/arXiv.2305.15066. URL https://doi.org/ 10.48550/arXiv.2305.15066.</p>
<p>William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, 2017.</p>
<p>Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. Explanations as features: Llmbased features for text-attributed graphs. CoRR, abs/2305.19523, 2023. doi: 10.48550/arXiv. 2305.19523. URL https://doi.org/10.48550/arXiv.2305.19523.</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework, 2023.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In $I C L R, 2022$.</p>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In $I C L R, 2017$.</p>
<p>Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In $I C L R, 2019$.</p>
<p>Devin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent Létourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. In NeurIPS, 2021.</p>
<p>Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, and Qi Zhang. Adsgnn: Behavior-graph augmented relevance modeling in sponsored search. In Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (eds.), SIGIR, 2021.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: communicative agents for "mind" exploration of large scale language model society. CoRR, 2023.</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In ICRA, 2023.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 2023.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.</p>
<p>Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. Information Retrieval, 3:127-163, 2000.</p>
<p>OpenAI. Introducing chatgpt. 2023. URL https://openai.com/blog/chatgpt.
Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. CoRR, 2023.</p>
<p>Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In ICLR, 2020.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 2020.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 3505-3506. ACM, 2020. doi: 10.1145/3394486.3406703. URL https: //doi.org/10.1145/3394486.3406703.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 3982-3992, 2019.</p>
<p>Joshua Robinson and David Wingate. Leveraging large language models for multiple choice question answering. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview. net/pdf?id=yKbprarjc5B.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. 2023.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.</p>
<p>Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? CoRR, 2023a.</p>
<p>Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. CoRR, abs/2305.11175, 2023b. doi: 10.48550/arXiv.2305.11175. URL https://doi.org/10.48550/arXiv.2305.11175.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.</p>
<p>Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. Simplifying graph convolutional networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), ICML, 2019.</p>
<p>Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, 2018.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR, 2019.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. CoRR, 2023a.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In ICLR, 2023b.</p>
<p>Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. CoRR, abs/2308.07134, 2023. doi: 10.48550/arXiv.2308.07134. URL https://doi.org/10.48550/arXiv.2308.07134.</p>
<p>Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In NeurIPS, 2021.</p>
<p>Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, Viktor K. Prasanna, Long Jin, and Ren Chen. Decoupling the depth and scope of graph neural networks. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 19665-19679, 2021. URL https://proceedings.neurips.cc/ paper/2021/hash/a378383b89e6719e15cd1aa45478627c-Abstract.html.</p>
<p>Wentao Zhang, Ziqi Yin, Zeang Sheng, Yang Li, Wen Ouyang, Xiaosen Li, Yangyu Tao, Zhi Yang, and Bin Cui. Graph attention multi-layer perceptron. In $K D D, 2022$.</p>
<p>Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference. 2023.</p>
<p>Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. In AAAI, 2021.</p>
<h1>REPRODUCIbILITY STATEMENT</h1>
<p>The code to reproduce our results will be available soon. Our experimental settings and implementation details are stated in Section A.1, and the important hyper-parameters are discussed in Appendix A. 3 section.</p>
<h2>A EXPERIMENTAL SETTINGS</h2>
<h2>A. 1 IMPLEMENTATION DETAILS</h2>
<p>In our experiments node classification is approached as a multi-choice QA challenge, and we employ the prompt detailed in Appendix B. The raw text attributes can be directly leveraged to form the textual information $F$. Additionally, we utilize the dataset metadata to extract raw text labels, creating a textual feature for each node. Nodes without data are assigned the value "NA". Consequently, every general graph can be perceived as a text-attributed graph with a minimum of one type of text attribute, namely the label. For continuous attributes, we use K-means to discretize continuous features (with the $K$ being the number of classes).</p>
<p>During the in-context-learning experiments, to prevent meaningless demonstrations lacking neighboring labels, we choose a sample with the highest degree from each label set. For the instruction tuning experiments using open-source LLMs, we can leverage continuous attributes in a more flexible way. Specifically, inspired by multi-modality LLMs Wang et al. (2023b), we use a Multi-Layer Perceptron (MLP) projector to map continuous features into the input text space, the token embedding space of LLaMA-2 (Touvron et al., 2023). We utilize AdamW (Loshchilov \&amp; Hutter, 2019) in conjunction with DeepSpeed (Rasley et al., 2020) to train the huggingface LLaMA2-7b model ${ }^{2}$, with FP16 activated.</p>
<h2>A. 2 DATASETS</h2>
<p>Table 5: The statistics of the datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmarks</th>
<th style="text-align: center;">#Nodes</th>
<th style="text-align: center;">#Edges</th>
<th style="text-align: center;">#Classes</th>
<th style="text-align: center;">#Features</th>
<th style="text-align: center;">#Train</th>
<th style="text-align: center;">#Validation</th>
<th style="text-align: center;">#Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Cora</td>
<td style="text-align: center;">2708</td>
<td style="text-align: center;">5278</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1433</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Cora-TAG</td>
<td style="text-align: center;">2708</td>
<td style="text-align: center;">5278</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1433</td>
<td style="text-align: center;">1624</td>
<td style="text-align: center;">541</td>
<td style="text-align: center;">543</td>
</tr>
<tr>
<td style="text-align: center;">Citeseer</td>
<td style="text-align: center;">3327</td>
<td style="text-align: center;">4552</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3703</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">Citeseer-TAG</td>
<td style="text-align: center;">3327</td>
<td style="text-align: center;">4552</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3703</td>
<td style="text-align: center;">1911</td>
<td style="text-align: center;">637</td>
<td style="text-align: center;">638</td>
</tr>
<tr>
<td style="text-align: center;">Cornell</td>
<td style="text-align: center;">183</td>
<td style="text-align: center;">298</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1703</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">37</td>
</tr>
<tr>
<td style="text-align: center;">Texas</td>
<td style="text-align: center;">183</td>
<td style="text-align: center;">325</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1703</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">37</td>
</tr>
<tr>
<td style="text-align: center;">Wisconsin</td>
<td style="text-align: center;">251</td>
<td style="text-align: center;">515</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1703</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">51</td>
</tr>
</tbody>
</table>
<p>In this section, we provide more relevant details about the datasets we used in experiments. The dataset statistics are provided in Table 5. The datasets can be categorized into citation network datasets (i.e. Cora, Citeseer, and ogbn-arxiv) and web-page networks (i.e. Cornell, Texas, and Wisconsin), additionally, we use two text-attributed-graph (TAG) version of Cora and Citeseer, denoted as Cora-TAG and Citeseer-TAG.</p>
<p>Citation graphs: Most GNN-related studies, as referenced in works like Kipf \&amp; Welling (2017); Velickovic et al. (2018), often employ citation networks as benchmarking tools. Within these networks, nodes represent papers from the computer science domain. The features of these nodes are derived from bag-of-word vectors of the respective paper titles. Edges depict the citation links between these papers, while labels indicate the paper's specific categories. The text attributes are the title and abstract of the paper.</p>
<p>WebKB graphs Pei et al. (2020): Sourced by Carnegie Mellon University, aggregates web pages from computer science departments across several universities. We employ three specific subsets from this collection: Cornell, Texas, and Wisconsin. In these subsets, each node symbolizes a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>web page, while edges denote hyperlinks connecting them. Nodes are characterized by a bag-ofwords representation derived from their respective web pages. These pages have been meticulously categorized into five distinct classes: student, project, course, staff, and faculty.
The datasets mentioned above can be found in the following URLs: Cora ${ }^{3}$, Citeseer ${ }^{4}$, Cora-TAG ${ }^{5}$, Citeseer-TAG ${ }^{6}$, Texas ${ }^{7}$, Cornell ${ }^{8}$, Wisconsin ${ }^{9}$.</p>
<h1>A. 3 HYPERPARAMETERS</h1>
<p>In GRAPHText, the selection of text attributes $F$ and relations $R$ are the most important parameters. Here we discuss their choices and report the selected parameters in Table 6. For text attributes $F$, there are several choices: propagated features $\boldsymbol{A}^{k} \boldsymbol{X}$, propagated labels $\boldsymbol{A}^{k} \boldsymbol{Y}<em L="L">{L}$, raw features $\boldsymbol{X}$ and labels $\boldsymbol{Y}</em>$.}$; For relationship $R$, there are several choices: k-hop shortest-path distance, denoted as $\boldsymbol{S}_{k}$, propagated feature similarity, denoted as $\operatorname{sim}\left(\boldsymbol{A}^{k} \boldsymbol{X}\right)$, and pagerank matrix Klicpera et al. (2019), with restart probability $\alpha=0.25$, denoted as $\boldsymbol{\Pi</p>
<p>Table 6: GRAPHText in-context learning hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Text Attributes</th>
<th style="text-align: center;">Relations</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Cora</td>
<td style="text-align: center;">$\boldsymbol{A}^{2} \boldsymbol{Y}<em L="L">{L}, \boldsymbol{A}^{3} \boldsymbol{Y}</em>$</td>
<td style="text-align: center;">$\boldsymbol{S}_{0}, \boldsymbol{\Pi}, \operatorname{sim}\left(\boldsymbol{A}^{2} \boldsymbol{X}\right), \operatorname{sim}\left(\boldsymbol{A}^{3} \boldsymbol{X}\right)$</td>
</tr>
<tr>
<td style="text-align: left;">Citeseer</td>
<td style="text-align: center;">$\boldsymbol{X}, \boldsymbol{A}^{3} \boldsymbol{Y}_{L}$</td>
<td style="text-align: center;">$\boldsymbol{S}<em 2="2">{0}, \boldsymbol{S}</em>\right)$}, \boldsymbol{\Pi}, \operatorname{sim}\left(\boldsymbol{A}^{2} \boldsymbol{X</td>
</tr>
<tr>
<td style="text-align: left;">Taxes</td>
<td style="text-align: center;">$\boldsymbol{A}^{2} \boldsymbol{Y}_{L}$</td>
<td style="text-align: center;">$\boldsymbol{S}<em 2="2">{0}, \boldsymbol{S}</em>$</td>
</tr>
<tr>
<td style="text-align: left;">Wisconsin</td>
<td style="text-align: center;">$\boldsymbol{Y}_{L}$</td>
<td style="text-align: center;">$\boldsymbol{S}_{0}, \operatorname{sim}(\boldsymbol{X}), \operatorname{sim}(\boldsymbol{A} \boldsymbol{X})$</td>
</tr>
<tr>
<td style="text-align: left;">Cornell</td>
<td style="text-align: center;">$\boldsymbol{A}^{2} \boldsymbol{Y}_{L}$</td>
<td style="text-align: center;">$\boldsymbol{S}<em 0="0">{0}, \boldsymbol{S}</em>)$}, \operatorname{sim}(\boldsymbol{X}), \operatorname{sim}(\boldsymbol{A} \boldsymbol{X</td>
</tr>
</tbody>
</table>
<h2>B PROMPT EXAMPLES</h2>
<h2>B. 1 FEW-SHOT In-CONTEXT LEARNING</h2>
<h2>Example of Citeseer:</h2>
<p>[Human]: You are a helpful assistant that classifies the topic of an academic paper based on the labels of the cited papers. You are going to choose the correct answer from several choices of paper categories: [A: Agents, B: Artificial Intelligence, C: Database, D: Information Retrieval, E: Machine Learning, F: Human Computer Interaction]</p>
<p>Here are a few examples:
$&lt;$ information $&gt;$
$&lt;$ third-order.pseudo_labels $&gt;$
$&lt;$ center_node $&gt;$ ['A'] $&lt;/$ center_node $&gt;$
$&lt;1$ st_feature_similarity_graph $&gt;$ ['A', 'A', 'A'] $&lt;/ 1$ st_feature_similarity_graph $&gt;$
$&lt;\mathrm{ppr}&gt;[$ 'A', 'B', 'A'] $&lt;/ \mathrm{ppr}&gt;$
$&lt;/$ third-order_pseudo_labels $&gt;$
$&lt;/$ information $&gt;$
$&lt;$ question $&gt;$ What's the topic of academic paper given the information above? $&lt;/$ question $&gt;$ $&lt;$ answer $&gt;$ A $&lt;/$ answer $&gt;$</p>
<p>Remaining examples ...</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>Now<span class="w"> </span>let&#39;s<span class="w"> </span>answer<span class="w"> </span>the<span class="w"> </span>question<span class="w"> </span>below:
\(<span class="err">&lt;</span>\)<span class="w"> </span>information<span class="w"> </span>\(&gt;\)
<span class="w">    </span>\(<span class="err">&lt;</span>\)<span class="w"> </span>third-order.pseudo.labels<span class="w"> </span>\(&gt;\)
<span class="w">        </span>\(<span class="nt">&lt;1</span><span class="err">\)</span><span class="w"> </span><span class="err">st_feature_similarity_graph</span><span class="w"> </span><span class="err">\(</span><span class="nt">&gt;</span>\left\lceil^{\prime}<span class="w"> </span>\mathrm{C}^{\prime},{<span class="w"> </span>}^{\prime}<span class="w"> </span>\mathrm{B}^{\prime},{<span class="w"> </span>}^{\prime}<span class="w"> </span>\mathrm{B}^{\prime}\right\rceil<span class="err">&lt;</span>/\)<span class="w"> </span>st_feature_similarity_graph<span class="w"> </span>\(&gt;\)
<span class="w">        </span>\(<span class="err">&lt;</span>\)<span class="w"> </span>ppr<span class="w"> </span>\(&gt;\left\lceil^{\prime}<span class="w"> </span>\mathrm{C}^{\prime}\right\rceil<span class="err">&lt;</span>/\)<span class="w"> </span>ppr<span class="w"> </span>\(&gt;\)
<span class="w">    </span>\(<span class="err">&lt;</span>/\)<span class="w"> </span>third-order_pseudo_labels<span class="w"> </span>\(&gt;\)
\(<span class="err">&lt;</span>/\)<span class="w"> </span>information<span class="w"> </span>\(&gt;\)
</code></pre></div>

<p>What's the topic of the paper given the information above? Valid choices are [A: Agents, B: Artificial Intelligence, C: Database, D: Information Retrieval, E: Machine Learning, F: Human computer interaction]. Remember, your answer should be in the form of the class choice wrapped by $&lt;$ answer $&gt;&lt;/$ answer $&gt;$.
[Assistant]: <answer $>\mathrm{C}&lt;/$ answer $&gt;$</p>
<h1>B. 2 Instruction Tuning</h1>
<h2>Example of Cora:</h2>
<p>[Human]: Your goal is to perform node classification. You are given the information of each node in a xml format. Using the given information of a node, you need to classify the node to several choices: $[&lt;\mathrm{c} 0&gt;$ : Rule_Learning, $&lt;\mathrm{c} 1&gt;$ : Neural_Networks, $&lt;\mathrm{c} 2&gt;$ : Case_Based, $&lt;\mathrm{c} 3&gt;$ : Genetic_Algorithms, $&lt;\mathrm{c} 4&gt;$ : Theory, $&lt;\mathrm{c} 5&gt;$ : Reinforcement_Learning, $&lt;\mathrm{c} 6&gt;$ : Probabilistic_Methods]. Remember, your answer should be in the form of the class label.
$&lt;$ information $&gt;$
$&lt;$ feature $&gt;$
$&lt;$ center_node $&gt;&lt;\mathrm{x}&gt;&lt;\mathrm{x}$ emb $&gt;&lt;/ \mathrm{x}&gt;&lt;/$ center_node $&gt;$
$&lt;1$ st_feature_similarity_graph $&gt;&lt;\mathrm{x}&gt;&lt;\mathrm{x}$ emb $&gt;&lt;/ \mathrm{x}&gt;&lt;/ 1$ st_feature_similarity_graph $&gt;$ $&lt;/$ feature $&gt;$
$&lt;/$ information $&gt;$
[Assistant]: The answer is: $&lt;\mathrm{c} 6&gt;$
Note that the " $&lt;\mathrm{x}$ emb $&gt;$ " is the text token embedding for feature feature " x " generated by the MLP projector discussed in Appendix A.1.</p>
<h2>B. 3 EXAMPLES OF TEXAS</h2>
<p>Node # 132, 136, 143, ...
Graph information:
pseudo labels:
center-node:['D']
second-hop neighbor:['D', 'D', 'D', 'D', 'D']
Target class: D</p>
<h2>Node # 30</h2>
<p>Graph information:
pseudo labels:
center-node:['D']
second-hop neighbor:['D', 'E', 'D', 'D', 'D']
Target class: D</p>
<h2>Node # 158</h2>
<p>Graph information:
pseudo labels:</p>
<div class="codehilite"><pre><span></span><code>center-node:[&#39;A&#39;]
second-hop neighbor:[&#39;A&#39;]
Target class: A
</code></pre></div>

<p>We can observe that for the best setting in the Texas datasets, with hyperparameters discussed in Table 6, the center-node pseudo labels mostly assemble the second-hop neighbors. Consequently, removing the text information, i.e. removing the internal nodes in the graph-syntax tree in Section 4.3 does not hurt the performance.</p>
<p>This also shows the advantage of decoupling depth and scope Zeng et al. (2021) in the graph-syntax tree of GraphText, which explains the performance gain of GraphText over standard GNNs, e.g. GCN and GAT. A similar observation is also drawn in Figure 8 (i) of (Chien et al., 2021), where $\boldsymbol{A}^{2}$ serves as the most important high-order aggregation scheme for Texas dataset.</p>
<h1>C Interactive Graph Reasoning</h1>
<p>Since GRAPHTEXT facilitates graph learning within a textual domain, it allows for direct interaction between both humans and AI agents. In this section, we spotlight the interactive graph reasoning capabilities of GraphText using a practical example. First, we demonstrate how GraphText can engage in self-interaction via zero-shot chain of thought reasoning. Following that, we illustrate how human interactions can guide GRAPHTEXT to refine its graph reasoning approach.</p>
<h2>C. 1 Zero-shot Chain of Thought Reasoning</h2>
<p>Below is the example of graph reasoning on Cora node #2188 in the setting of standard zero-shot chain of thought reasoning Wei et al. (2022) ${ }^{10}$ The input prompt for "Cora node #2188" is as below:</p>
<p>Input Prompt Cora node #2188:
[Human]: You are a helpful assistant that generates a classifies the topic of an academic paper based on the labels of the cited papers. You are going to choose the correct answer from several choices of paper categories:[A: Theory, B: Reinforcement Learning, C: Genetic Algorithm, D: Neural Network, E: Probabilistic Method, F: Case Based, G: Rule Learning]
Here are a few examples:
Graph information:
pseudo labels:
center-node:['F']
ppr:['A', 'A', 'A', 'A']
Topic of paper: A
Graph information:
pseudo labels:
center-node:['B']
ppr:['B', 'B', 'B', 'B']
Topic of paper: B
Graph information:
pseudo labels:
center-node:['C']
ppr:['C', 'C', 'C', 'C']
Topic of paper: C
Graph information:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code>pseudo labels:
center-node:[&#39;D&#39;]
ppr:[&#39;D&#39;, &#39;D&#39;, &#39;E&#39;, &#39;D&#39;]
Topic of paper: D
Graph information:
pseudo labels:
center-node:[&#39;A&#39;]
ppr:[&#39;E&#39;, &#39;E&#39;, &#39;A&#39;, &#39;A&#39;]
Topic of paper: E
Graph information:
pseudo labels:
center-node:[&#39;F&#39;]
ppr:[&#39;F&#39;, &#39;F&#39;, &#39;F&#39;, &#39;F&#39;]
Topic of paper: F
Graph information:
pseudo labels:
center-node:[&#39;G&#39;]
ppr:[&#39;G&#39;, &#39;G&#39;, &#39;G&#39;, &#39;G&#39;]
Topic of paper: G
</code></pre></div>

<p>Now let's answer the question below:
Graph information:
pseudo labels:
center-node:['G']
ppr:['A', 'G', 'A', 'E']
What's the topic of the paper given the information above? Valid choices are [A: Theory, B: Reinforcement Learning, C: Genetic Algorithm, D: Neural Network, E: Probabilistic Method, F: Case Based, G: Rule Learning]</p>
<p>As discussed in Section 4.2, according to the demonstrations, the PPR pseudo-labels provide a more robust mechanism for paper topic prediction. Utilizing either a count of PPR labels followed by a majority vote, or merely referencing the foremost PPR label, consistently results in the correct categorization in the given examples. Hence, based on these graph inductive biases derived from samples, we can reasonably figure out correct topic of the target paper should be A, which not only is the first entry, but also the predominant label in the PPR pseudo-label sequence. Now, let's use ChatGPT and GPT-4 to answer the problem. We performed 15 times of evaluation and here we show one sample of correct and incorrect responses below:</p>
<h1>Correct Response without Human Interaction</h1>
<p>[ChatGPT]: Among the PPR labels, 'A' appears twice, 'G' appears once, and 'E' appears once. 'A' is the most frequent label, so we classify the topic of the paper as 'A.'
Therefore, the topic of the paper is A: Theory.
[GPT-4]: Given the examples provided:</p>
<ol>
<li>Whenever the center-node label is consistent with the majority of the PPR pseudo labels, the topic of the paper matches the center-node label.</li>
<li>
<p>However, when there's a discrepancy between the center-node label and the majority of the PPR labels, the topic of the paper is typically determined by the first (most crucial) label in the PPR list.
Given the graph information provided:</p>
</li>
<li>
<p>The center node is labeled ' $G$ ' which corresponds to 'Rule Learning'.</p>
</li>
<li>The 'ppr' nodes are labeled 'A', 'G', 'A', and 'E', which corresponds to 'Theory', 'Rule Learning', 'Theory', and 'Probabilistic Method' respectively.</li>
</ol>
<p>It seems that while the center node is ' G ', the cited papers are mostly of category ' A ' and ' E '.
From the pattern observed in the examples, the category that has the majority or at least equal emphasis with other categories in the 'ppr' nodes tends to be the category of the paper. Therefore, in this case, 'A' or 'Theory' has the highest count in the 'ppr' nodes. Thus, the topic of the paper given the information above is: A: Theory.</p>
<h1>Wrong Response without Human Interaction</h1>
<p>[ChatGPT]: Based on the labels in the "ppr" list, we can see that 'A' (Theory), 'G' (Rule Learning), and 'E' (Probabilistic Method) are mentioned. However, the 'center-node' label is 'G' (Rule Learning), which indicates a stronger association with "Rule Learning." So, the topic of the paper is most likely "Rule Learning (G)."
[GPT-4]: Given the graph information: pseudo labels: center-node:['G'] ppr:['A', 'G', 'A', 'E']
The center node label is ' $G$ ', which corresponds to "Rule Learning". The papers referenced (ppr) are mixed between "Theory", "Rule Learning", and "Probabilistic Method". However, since the center node holds the primary topic label, the academic paper's topic is: G: Rule Learning.</p>
<p>From the analysis above, it's clear that:
By recasting the graph learning challenge into problems that both humans and LLMs can interpret and reason with, GRAPHTEXT facilitates LLMs in graph reasoning. For instance, ChatGPT can deduce the right answer through label counting, as highlighted in green. Impressively, GPT-4 discerns the correct solution by employing the Personalized PageRank (PPR) prior, as illustrated in blue, and identifies the first PPR neighbor as the most important factor for prediction.</p>
<p>However, the lucid explanations rendered in natural language allowed us to pinpoint certain inadequacies in their graph reasoning approach:
Failure to Retrieve Prior Knowledge: Table 4.1 shows that LLMs, particularly ChatGPT, don't always effectively recall their previous understanding of graphs; only $26.7 \%$ of ChatGPT's reasoning relies on PPR.</p>
<p>Persistence in Prior Belief: Despite the inherent reasoning prowess of LLMs, they can sometimes be hindered by preconceived notions about graphs. A detailed examination of the results in Table 4.1, accompanied by the explanations of reasoning above, indicates that a deep-seated bias - where the central node is viewed as the most vital (denoted in red) - is a primary cause of mistakes.</p>
<p>In the following section, we engage with GRAPHTEXT to enhance its graph reasoning capabilities.</p>
<h2>C. 2 Graph Reasoning with Human Interaction</h2>
<p>As highlighted in the preceding section, LLMs occasionally struggle with recalling prior knowledge and remain anchored to their pre-existing beliefs. To address this, we engage with GRAPHTEXT by: (1) providing a clear definition of PPR, and (2) explicitly instructing them to assess the reliability of both center-node pseudo and PPR pseudo labels. This leads to the following message:</p>
<p>Human Interaction Note that PPR pseudo labels represent a sequence of labels ranked by importance, with the first one being the most crucial. Let's evaluate the reliability of center node pseudo labels against PPR pseudo labels and then make another prediction.</p>
<p>With the interaction above, in most cases LLMs refined their answer, below are some examples:</p>
<h2>Correct Reasoning after Human Interaction</h2>
<p>[ChatGPT]: In this PPR list, the labels 'A' and 'G' are present, with 'A' appearing before</p>
<p>'G,' which means 'A' is considered more crucial in the context of this paper. Now, let's make a prediction based on this evaluation: Since 'A' is the most crucial label in the PPR list, we will prioritize it for classifying the topic of the paper. Therefore, the paper's primary topic is "A: Theory" (Category A). The presence of 'G' in the PPR list indicates some relevance to "Rule Learning" (Category G), but because it comes after 'A,' it is considered less important in this context. So, the revised prediction is that the primary topic of the paper is "A: Theory."
[GPT-4]: Given the new information that PPR pseudo labels represent a sequence of labels ranked by importance, let's re-evaluate the provided data:
The center node suggests the paper is about 'G' (Rule Learning). However, considering the prominence of 'A' (Theory) in the ranked sequence, and since it's the most crucial label, the paper might be primarily focused on theoretical aspects related to Rule Learning. Considering both the center node and the importance sequence, the topic of the paper should be: A: Theory</p>
<h1>Refusion after Human Interaction</h1>
<p>[ChatGPT]: Certainly, let's evaluate the reliability of center node pseudo labels against PPR pseudo labels.
In the provided graph information: The center-node label is 'G,' which suggests that the most important label for this paper is 'G.' However, when we consider the PPR labels, we see that 'A' appears as the first and third label, which might indicate some importance as well.
Let's consider both the center-node and the PPR labels for our prediction. Given that 'G' is the center-node label and 'A' appears in the PPR labels, we can prioritize the center-node label but also consider the presence of 'A' in the PPR labels.
So, the prediction for the topic of the paper can be: Topic of paper: G (with some influence from A).</p>
<h2>Confusion after Human Interaction</h2>
<p>[ChatGPT]: Based on this information, we can classify the topic of the paper as a combination of the most frequent labels among the neighboring papers, which are 'A' and 'G'. Therefore, the topic of the paper is likely a combination of topics 'A' and 'G'.
So, the answer is a combination of two topics, and the closest match among the given options is:
Option: F: Case Based # Note that F does not present in the pseudo labels of the question</p>
<p>The provided examples, along with the consolidated findings in Table 4.1, compellingly show that both ChatGPT and GPT-4 can adjust their pre-existing biases about graphs when given human feedback. Notably, after this interaction, GPT-4 delivers a flawless accuracy rate of $100 \%$, consistently following the PPR logic. Meanwhile, ChatGPT also sees a significant performance boost, with an accuracy improvement of $36.9 \%$. However, as evidenced in the examples, ChatGPT occasionally refuses updating its predictions or becomes confused.</p>
<h2>D Limitations and Future Work</h2>
<p>While the GRAPHText framework offers notable advantages, there is ample room for enhancement and exploration of new applications.
One primary concern not fully addressed in this paper is the question of how to discretize continuous features. As evident from Table 6, most optimal settings are label-based. This makes GRAPHText resemble a label propagation model, with Citeseer being the only exception. We posit that the observed trend might be attributed to two main factors: 1) The ineffectiveness of the discretization process, and 2) The discord between feature and label spaces, making reasoning challenging for LLMs.</p>
<p>Additionally, the design space of the graph-proxy tree is extensive and often requires either expert knowledge or hyperparameter optimization. Although GRAPHText boasts flexibility and generality, crafting the text-attribute set $F$ and relation set $R$, and determining their combinations result</p>
<p>in a vast search space. However, given that GraphText operates on a training-free paradigm, hyperparameter optimization can be swift.</p>
<p>Notwithstanding its constraints, GraphText introduces avenues for novel research. Chiefly, it sets the stage for graph reasoning in natural language. Emerging advancements in the LLM realm can potentially be integrated into the graph ML domain. This includes areas such as multi-step reasoning (Yao et al., 2023a), decision-making (Yao et al., 2023b; Liang et al., 2023), tool utilization (Schick et al., 2023), and multi-agent collaboration (Park et al., 2023; Hong et al., 2023). Furthermore, the prospect of training-free graph learning streamlines the validation process for graph model designs. If one assumes the optimal relation set $R$ and feature set $H$ to be transferable, significant training time can be saved. Researchers can quickly identify suitable settings with GRAPHTEXT and then apply these configurations to the hyperparameters of other GNNs/LLMs.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ ChatGPT and GPT-4 perform reasoning with explanation by default. Therefore, there is no necessity to add the additional "Let's think step-by-step" prompt for the text input.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
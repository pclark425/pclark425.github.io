<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-444 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-444</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-444</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-202719268</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1909.09859v1.pdf" target="_blank">DECoVaC: Design of Experiments with Controlled Variability Components</a></p>
                <p><strong>Paper Abstract:</strong> Reproducible research in Machine Learning has seen a salutary abundance of progress lately: workflows, transparency, and statistical analysis of validation and test performance. We build on these efforts and take them further. We offer a principled experimental design methodology, based on linear mixed models, to study and separate the effects of multiple factors of variation in machine learning experiments. This approach allows to account for the effects of architecture, optimizer, hyper-parameters, intentional randomization, as well as unintended lack of determinism across reruns. We illustrate that methodology by analyzing Matching Networks, Prototypical Networks and TADAM on the miniImagenet dataset.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e444.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e444.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECoVaC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Design of Experiments with Controlled Variability Components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A principled experimental-design methodology using a re-seeding framework and linear mixed models to quantify and decompose sources of variability (random seeds, hyper-parameters, reruns) in machine-learning experiments and to test reproducibility of results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning (few-shot learning / empirical evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Designing, running and statistically analyzing large-scale experiments (3 models × optimizers × seeds × hyper-parameter draws × reruns) to measure variability and reproducibility of few-shot learning model test accuracy on miniImagenet.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Pseudo-random number generator (PRNG) random seeds, hyper-parameter configurations, reruns (unintended non-determinism across repeated runs), model architecture, optimizer choice, implementation differences, residual (unstructured) noise; practical limitation: lack of PRNGs designed for parallel pseudo-independent streams (leading to use of multiple independent seeds such as Mersenne Twister).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Variance components estimated via linear mixed models (random-effect variances and standard deviations), residual variance, likelihood ratio tests (LRT) for random effects, ANOVA / F-tests for fixed effects, mean differences with standard errors and 95% confidence intervals, AIC and log-likelihood for model comparison; p-values for tests.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Random-effect variance estimates (Table 3): seeds variance = 0.0000309 (std dev = 0.005559); hyper-parameters variance = 0.0017922 (std dev = 0.042334); residual variance = 0.0004338 (std dev = 0.020828). Likelihood-ratio tests (Table 2): adding (1 | seeds) LRT χ2 statistic = 24.41725, p = 7.757113e-07; adding (1 | hparams) LRT χ2 statistic = 1302.27988, p = 3.612198e-285. Fixed-effects ANOVA (Table 4): experiments F value ≈ 27.95, Pr(>F) ≈ 5.896146e-19. Pairwise mean comparisons (Table 5) show estimates near zero with large SE and non-significant p-values (e.g., m-net-adam estimate = -0.006338, SE = 0.027132, p = 0.81618), indicating no significant differences across reruns with identical seed and hyper-parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Statistical hypothesis tests using linear mixed models: likelihood-ratio tests for presence of random-effect variability (seeds, hyper-parameters), ANOVA (with Kenward & Roger / Satterthwaite corrections) to test differences across reruns' means, pairwise mean differences with standard errors and 95% confidence intervals, and assessment of variance component magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>H1 (stability across reruns with same model, optimizer, hyper-parameters and same seed) was rejected for seed effect overall (i.e., seeds introduce significant variance across experiments) and H2 (stability across different hyper-parameter configurations) was rejected (hyper-parameters introduce large, highly significant variance). H3 (stability of reruns with identical model+optimizer+hyper-parameter+seed) was not rejected: repeated runs with identical seed and hyper-parameters showed no statistically significant mean differences (pairwise p-values large), implying those reruns are stable.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Significant sources of variability are: choice of random seed (seed-stream correlations), hyper-parameter sampling, unintended non-determinism across reruns, and practical limitations in PRNG design for parallel independent streams in common ML libraries; these factors can confound claims of algorithmic superiority if not controlled or quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Controlled random re-seeding framework (systematic sampling of seeds and hyper-parameters), running many reruns per combination, fitting linear mixed models to decompose variance and test effects, using quality PRNGs (Mersenne Twister) and fixed seeds where appropriate, providing corrected p-values/DF via lmerTest and Kenward-Roger / Satterthwaite corrections, release of runnable notebooks and Docker containers to aid reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Quantitative evidence: when seed and hyper-parameters are held fixed, reruns produced stable means (no significant differences in pairwise tests; Table 5 p-values >> 0.05). However, variability due to hyper-parameters and seeds remains large (hyper-parameters variance ≈ 0.0017922 >> seed variance ≈ 0.0000309), so sampling and controlling these factors plus explicit reporting are necessary; no numerical reduction-of-variance experiment (e.g., before/after) beyond these tests is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>9000 total experiments (3 models × 2 optimizers × 10 PRNG seeds × 15 hyper-parameter configurations × 10 reruns) — equivalently 3000 experiments per model; 10 seeds; 15 hyper-parameter draws per seed; 10 reruns per (model, optimizer, seed, hyper-parameters) combination.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a controlled re-seeding experimental design and linear mixed models, the authors find that hyper-parameter configurations and random seeds introduce statistically significant variability in few-shot learning experiments (hyper-parameters the dominant component), while repeated reruns with the same seed and hyper-parameters are stable (no significant mean differences). Quantitatively, estimated variance components: hyper-parameters ≈ 0.0017922, residual ≈ 0.0004338, seeds ≈ 0.0000309; LRT p-values indicate strong significance for seed and hyper-parameter effects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DECoVaC: Design of Experiments with Controlled Variability Components', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning that matters <em>(Rating: 2)</em></li>
                <li>On the state of the art of evaluation in neural language models <em>(Rating: 2)</em></li>
                <li>Troubling trends in machine learning scholarship <em>(Rating: 2)</em></li>
                <li>Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging <em>(Rating: 2)</em></li>
                <li>A practical taxonomy of reproducibility for machine learning research <em>(Rating: 1)</em></li>
                <li>Open is not enough <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-444",
    "paper_id": "paper-202719268",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "DECoVaC",
            "name_full": "Design of Experiments with Controlled Variability Components",
            "brief_description": "A principled experimental-design methodology using a re-seeding framework and linear mixed models to quantify and decompose sources of variability (random seeds, hyper-parameters, reruns) in machine-learning experiments and to test reproducibility of results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "machine learning (few-shot learning / empirical evaluation)",
            "experimental_task": "Designing, running and statistically analyzing large-scale experiments (3 models × optimizers × seeds × hyper-parameter draws × reruns) to measure variability and reproducibility of few-shot learning model test accuracy on miniImagenet.",
            "variability_sources": "Pseudo-random number generator (PRNG) random seeds, hyper-parameter configurations, reruns (unintended non-determinism across repeated runs), model architecture, optimizer choice, implementation differences, residual (unstructured) noise; practical limitation: lack of PRNGs designed for parallel pseudo-independent streams (leading to use of multiple independent seeds such as Mersenne Twister).",
            "variability_measured": true,
            "variability_metrics": "Variance components estimated via linear mixed models (random-effect variances and standard deviations), residual variance, likelihood ratio tests (LRT) for random effects, ANOVA / F-tests for fixed effects, mean differences with standard errors and 95% confidence intervals, AIC and log-likelihood for model comparison; p-values for tests.",
            "variability_results": "Random-effect variance estimates (Table 3): seeds variance = 0.0000309 (std dev = 0.005559); hyper-parameters variance = 0.0017922 (std dev = 0.042334); residual variance = 0.0004338 (std dev = 0.020828). Likelihood-ratio tests (Table 2): adding (1 | seeds) LRT χ2 statistic = 24.41725, p = 7.757113e-07; adding (1 | hparams) LRT χ2 statistic = 1302.27988, p = 3.612198e-285. Fixed-effects ANOVA (Table 4): experiments F value ≈ 27.95, Pr(&gt;F) ≈ 5.896146e-19. Pairwise mean comparisons (Table 5) show estimates near zero with large SE and non-significant p-values (e.g., m-net-adam estimate = -0.006338, SE = 0.027132, p = 0.81618), indicating no significant differences across reruns with identical seed and hyper-parameters.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Statistical hypothesis tests using linear mixed models: likelihood-ratio tests for presence of random-effect variability (seeds, hyper-parameters), ANOVA (with Kenward & Roger / Satterthwaite corrections) to test differences across reruns' means, pairwise mean differences with standard errors and 95% confidence intervals, and assessment of variance component magnitudes.",
            "reproducibility_results": "H1 (stability across reruns with same model, optimizer, hyper-parameters and same seed) was rejected for seed effect overall (i.e., seeds introduce significant variance across experiments) and H2 (stability across different hyper-parameter configurations) was rejected (hyper-parameters introduce large, highly significant variance). H3 (stability of reruns with identical model+optimizer+hyper-parameter+seed) was not rejected: repeated runs with identical seed and hyper-parameters showed no statistically significant mean differences (pairwise p-values large), implying those reruns are stable.",
            "reproducibility_challenges": "Significant sources of variability are: choice of random seed (seed-stream correlations), hyper-parameter sampling, unintended non-determinism across reruns, and practical limitations in PRNG design for parallel independent streams in common ML libraries; these factors can confound claims of algorithmic superiority if not controlled or quantified.",
            "mitigation_methods": "Controlled random re-seeding framework (systematic sampling of seeds and hyper-parameters), running many reruns per combination, fitting linear mixed models to decompose variance and test effects, using quality PRNGs (Mersenne Twister) and fixed seeds where appropriate, providing corrected p-values/DF via lmerTest and Kenward-Roger / Satterthwaite corrections, release of runnable notebooks and Docker containers to aid reproducibility.",
            "mitigation_effectiveness": "Quantitative evidence: when seed and hyper-parameters are held fixed, reruns produced stable means (no significant differences in pairwise tests; Table 5 p-values &gt;&gt; 0.05). However, variability due to hyper-parameters and seeds remains large (hyper-parameters variance ≈ 0.0017922 &gt;&gt; seed variance ≈ 0.0000309), so sampling and controlling these factors plus explicit reporting are necessary; no numerical reduction-of-variance experiment (e.g., before/after) beyond these tests is reported.",
            "comparison_with_without_controls": true,
            "number_of_runs": "9000 total experiments (3 models × 2 optimizers × 10 PRNG seeds × 15 hyper-parameter configurations × 10 reruns) — equivalently 3000 experiments per model; 10 seeds; 15 hyper-parameter draws per seed; 10 reruns per (model, optimizer, seed, hyper-parameters) combination.",
            "key_findings": "Using a controlled re-seeding experimental design and linear mixed models, the authors find that hyper-parameter configurations and random seeds introduce statistically significant variability in few-shot learning experiments (hyper-parameters the dominant component), while repeated reruns with the same seed and hyper-parameters are stable (no significant mean differences). Quantitatively, estimated variance components: hyper-parameters ≈ 0.0017922, residual ≈ 0.0004338, seeds ≈ 0.0000309; LRT p-values indicate strong significance for seed and hyper-parameter effects.",
            "uuid": "e444.0",
            "source_info": {
                "paper_title": "DECoVaC: Design of Experiments with Controlled Variability Components",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning that matters",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_that_matters"
        },
        {
            "paper_title": "On the state of the art of evaluation in neural language models",
            "rating": 2,
            "sanitized_title": "on_the_state_of_the_art_of_evaluation_in_neural_language_models"
        },
        {
            "paper_title": "Troubling trends in machine learning scholarship",
            "rating": 2,
            "sanitized_title": "troubling_trends_in_machine_learning_scholarship"
        },
        {
            "paper_title": "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
            "rating": 2,
            "sanitized_title": "reporting_score_distributions_makes_a_difference_performance_study_of_lstmnetworks_for_sequence_tagging"
        },
        {
            "paper_title": "A practical taxonomy of reproducibility for machine learning research",
            "rating": 1,
            "sanitized_title": "a_practical_taxonomy_of_reproducibility_for_machine_learning_research"
        },
        {
            "paper_title": "Open is not enough",
            "rating": 1,
            "sanitized_title": "open_is_not_enough"
        }
    ],
    "cost": 0.007684750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DECoVaC: Design of Experiments with Controlled Variability Components</p>
<p>Thomas Boquet thomas@elementai.com 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>Laure Delisle laure.delisle@elementai.com 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>Denis Kochetkov denis.kochetkov@elementai.com 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>Nathan Schucher nathan@elementai.com 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>Parmida Atighehchian parmida@elementai.com 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>Boris Oreshkin 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>Julien Cornebise julien@elementai.com 
Element AI
Element AI
Element AI
Element AI
Element AI
Element AI
ElementAI</p>
<p>DECoVaC: Design of Experiments with Controlled Variability Components</p>
<p>Reproducible research in Machine Learning has seen a salutary abundance of progress lately: workflows, transparency, and statistical analysis of validation and test performance. We build on these efforts and take them further. We offer a principled experimental design methodology, based on linear mixed models, to study and separate the effects of multiple factors of variation in machine learning experiments. This approach allows to account for the effects of architecture, optimizer, hyper-parameters, intentional randomization, as well as unintended lack of determinism across reruns. We illustrate that methodology by analyzing Matching Networks, Prototypical Networks and TADAM on the miniImagenet dataset. * indicates equal contribution Preprint. Under review.</p>
<p>Introduction</p>
<p>Concern about reproducible science has grown in the machine learning field in the past decade, with multiple studies finding that a significant proportion of published research could not be reproduced (Henderson et al., 2018;Melis et al., 2018). To address this, the community has already come up with several recommendations for producing reproducible research along three main axes:</p>
<p>• Workflows: In their work, Chen et al. (2018); Cebrat &amp; Hartl (2018); Tatman (2018) describe workflows and processes for consuming results from other labs, as well as initial data capture processes required for reproduction. • Transparency efforts: Vowing for transparency, Chen et al. (2018); Henderson et al. (2018); Cebrat &amp; Hartl (2018) advocate for the release of the models' hyper-parameters and the method by which they were selected. • Statistical tools: Ablation studies (Lipton &amp; Steinhardt, 2018), significance testing and error analysis (Reimers &amp; Gurevych, 2017;Henderson et al., 2018;Melis et al., 2018), measure of performance and visual inspection over factors of variation (hyper-parameters, regularization, random seed, optimization regime) using bootstrap confidence bounds and power analysis (Henderson et al., 2018;Melis et al., 2018;Lipton &amp; Steinhardt, 2018).</p>
<p>We believe that these salutary initiatives, focused on studying validation and test performance through observation only, can and should be systematized and taken further.</p>
<p>In this work, we offer a principled methodology for evaluating the variability of machine learning experiments. More specifically, we propose an experimental design methodology for statistical tests of variability in model performance across runs and factors. This systematic statistical analysis addresses the failure to identify the factors of empirical variation highlighted by Lipton &amp; Steinhardt (2018, Section 3.2). Done across-and within-models, we argue that it benefits hyper-parameters selection, clarifies the stability of architectures, and strengthens state-of-the-art claims.</p>
<p>To illustrate our methodology, we then apply it to a suite of prominent algorithms in the domain of few-shot learning, a key area for applications of machine learning in data-poor environments (Fei-fei et al., 2006). Low data availability and quality arguably put results at a higher risk of uncontrollable variability, making few-shot learning algorithms a fit example for our methodology.</p>
<p>Methodology</p>
<p>Experimental design</p>
<p>Our experimental design methodology consists of three steps: establishing research hypotheses, gathering data using a random re-seeding framework, testing the research hypotheses by fitting a linear mixed model on that data.</p>
<p>Hypotheses The first step of the procedure we follow is to propose a set of research hypotheses regarding the stability of the algorithm across different experimental conditions. Each null hypothesis to be tested should assume stability, which can then be rejected if variations are statistically significant.</p>
<p>• H1: The results are stable across all runs of the same model and same optimizer using the same hyper-parameters configurations but the same random seeds. • H2: The results are stable across all runs of the same model and same optimizer using the same random seeds but using different hyper-parameters configurations. • H3: The results are stable across all runs of the same model and same optimizer using the same hyper-parameters configurations and different random seeds.</p>
<p>Re-seeding framework The second step is to design experiments to generate the data and test our hypotheses. We sample hyper-parameters configurations at random for each given implementation of each algorithm. We set the seeds of the pseudo-random number generators (PNRG) to ensure similar streams for multiple runs 2 .</p>
<p>As illustrated in Figure 1, for each combination of model and optimizer we sample a fixed number of hyper-parameter configurations. For each configuration, we s and rerun the training a fixed number of times for each configuration. An experiment consists of a rerun for a given combination of model, optimizer, PRNG seed, and sampled hyper-parameter configuration. We use the term rerun to indicate the act of running an algorithm with exactly the same hyper-parameter settings and the same random seed multiple times in the context of an experiment.</p>
<p>Statistical tests The third step of the procedure is to select and use statistical tools to test our set of hypotheses. To do so we define a statistical model based on a linear mixed model suited to analyze clustered data. We then fit this model on the data and perform statistical tests on that model.</p>
<p>Linear Mixed Models</p>
<p>To measure the difference of means between groups, one can use linear models (Fisher, 1919;Bates &amp; DebRoy, 2004) or Bayesian linear models (Gelman &amp; Hill, 2006). Linear models is a wide family of statistical models to measure the impact of different factors on a target variable. Two sample t-tests are a simple form of Analysis of variance (ANOVA) with one binary factor. It can also be seen as linear regression with a binary explanatory variable and no bias. To compare more than two groups w.r.t the same target variable, it's possible to use a one-hot encoded categorical variable. In our setting, we can assume we are in presence of noisy clustered observations of the target variable.</p>
<p>Since we control almost completely the environment where the experiments are run and thus the data-generating process, we can define a specific design to reason about statistical reproducibility while comparing the results of different runs of different algorithms. For each sample, we retrieve the information about the experiment name, the hyper-parameter configuration, the random seed used, the rerun identifier and the test accuracy on the meta-test split.</p>
<p>In our setup, we have N × D features X corresponding to a contrast matrix (one-hot encoded vector for the couple model-optimizer). N is the total number of experiments, i.e. of leaves in the tree of Figure 1. D is the number of distinct experiments illustrated in Figure 2, i.e. the number of combinations of models and optimizers multiplied by the number of reruns per seed. In other words, D is the number of leaves in the tree integrated over hyper-parameters and seeds.</p>
<p>In the simple case where we assumed that our observations are i.i.d, we could estimate the effects of each experiment with the linear regression model 3 :
y = Xβ + α + ,(1)
where β ∈ R D is the slope vector, α ∈ R is the intercept, and ∼ N (0, I) is random noise. In our setup, β and α are "fixed effects": we want to measure the difference between groups with constant effects across our dataset (X, y). To achieve this, we can maximize the likelihood y ∼ N (Xβ + α, I) to find point estimates of β and α that fit the data.</p>
<p>However, with our design, we know that there is a structure in the data generating process and that the data (X, y) is therefore not i.i.d. To circumvent this modeling problem, we can rewrite our linear model by introducing normally-distributed "random effects" b that vary across the population:
b ∼ N (0, σ 2 I) (2) y = Xβ + Zb + α + ,(3)
The term Zb models the clusters, where Z is the N × Q model matrix for the Q-dimensional random-effects variable b. In this setting we can rewrite the conditional distribution:
y|b ∼ N (Xβ + α + Zb, σ 2 W −1 ).(4)
Since E[b] = 0, the dependent variable mean is captured by Xβ + α when we marginalize over all the samples. The random effects component Zb captures variations in the data. It can be interpreted as an individual deviation from the group-level fixed effect.</p>
<p>In our context, we can write the model as follows:
y ijk = βX i + α + ε ijk (5) ε ijk = b 0j + b 1k + i (6) b uj ∼ N (0, σ uj ), u ∈ {1, 2} (7) i ∼ N (0, σ )(8)
where A is the intercept representing the mean of the whole group, β is a vector of parameters representing the deviation of each experiment from the mean, and Experiment i is a one hot vector of experiments for the observation i. In this model, we can decompose the error into multiple structured components. To this end we compose it in three terms: b 0j is a random effect associated with an observation from a random seed j, b 1k is associated with an observation from a hyperparameters configuration k and i is Gaussian noise. It is possible to regroup all the random intercepts as nuisance parameters in ε ijk = b 0j + b 1k + i .</p>
<p>Hypotheses testing</p>
<p>From the configurations used in the random re-seeding framework, we fit the linear mixed model defined in equation 7. As defined in our three hypotheses, our goal is to quantify the variability in the error α 0j linked to the seeds, quantify the variability in the error α 1k linked to the hyper-parameters configuration and estimate the differences in performances β between the different experiments and algorithms.</p>
<p>We first perform likelihood ratio tests for each random effect added to the model to test H1 and H2, i.e. if adding any of the random effects significantly changes the likelihood of the model given the data. Rejecting H1 and H2 would indicate that the implementations' performances vary significantly with a change of seed or hyper-parameters configuration.</p>
<p>To address H3, we first need to test if a difference exists between all the reruns' mean for a given combination of model and optimizer. We can use an ANOVA with a correction for the degrees of freedom for the number of comparisons performed (Kenward &amp; Roger, 1997). We finally compare the means of reruns of the same combination of model and optimizer, by computing the means' difference and providing standard errors and 95% estimated confidence intervals of our estimators.</p>
<p>To estimate the parameters of the linear mixed model defined in equation 7, we use the R implementation provided by the lme4 package (Bates et al., 2014). The estimates for the random effects and the fixed effects estimated with lme4 can be augmented with the lmerTest package (Kuznetsova et al., 2017) to add corrected degrees of freedom for the p-values (Kenward &amp; Roger, 1997;Satterthwaite, 1946) in small samples settings to compare several groups.</p>
<p>Experiments</p>
<p>We selected three prominent articles which present three key few-shot learning algorithms using metric-based learning. We study and compare their performance on a specific task and a specific dataset, common to all three algorithms. We investigate how they exhibit different behaviors subject to different random seeds reruns and hyper-parameter changes.</p>
<p>Experimental protocol</p>
<p>To illustrate our methodology, we follow the steps of experimental design described in Section 2.</p>
<p>Dataset We use the miniImagenet dataset proposed by Vinyals et al. (2016) to perform our experiments. To construct the tasks, we sample 5 classes uniformly and 5 training samples per class uniformly. We use the (meta-) train, validation and test splits from Ravi &amp; Larochelle (2017).</p>
<p>Models For this review we have selected three metric-based few-shot learning models: Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), and TADAM (Oreshkin et al., 2018). These models represent the state of the art in the 5-shot case for 2016, 2017, and 2018, respectively. We identified the official or community-endorsed implementation for each model.</p>
<p>Optimizers We select two optimizers (Stochastic Gradient Descent, ADAM) that we use for every model. Using the same 10 PRNG seeds for each optimizer and model, we sample 15 hyper-parameters Figure 2: Distribution of results of reruns over models, optimizers, hyper-parameter, and PRNG seed. The color of the boxplots represents the model, the color of the markers the hyper-parameters configuration and the different markers the random seed used in each trial. We illustrate each combination of model/optimizer/hyper-parameters for two sub-sets of seeds.</p>
<p>configurations using the distributions in Table 1 for each seed. We rerun the training 10 times for each combination of model, optimizer, seed and hyper-parameters configuration. This amounts to 3,000 experiments per model. Overall, we evaluate 3 models for a total of 9,000 experiments.</p>
<p>From those experiments, we fit the linear mixed model defined in equation 7. Our goal is to quantify the variability in the error b 0j linked to the seeds, quantify the variability in the error b 1k linked to the rerun and estimate the differences in performances β between the different experiments and algorithms.</p>
<p>Experimental results</p>
<p>Following our methodology described in Section 2.3, we perform likelihood ratio tests for each random effect added to the model, see Table 2. We reject both H1 and H2 related respectively to the seed and the hyper-parameters configuration, and confirm that the implementations' performances vary significantly with those factors. Intuitively, it means there is enough correlation between the observations using the same random seed and the same configuration to estimate co-variance parameters, see Table 3. We then test if a difference exists between all the experiments, and find that there is significant difference in the experiments' accuracy means as presented in Table 4. We finally compare the means of reruns of given combinations of model and optimizer by computing the means' difference and providing standard errors and 95% estimated confidence intervals of our estimators: see Table  5. Among all the comparisons, we do not find any statistical difference. In that sense we do not reject H3 and confirm that rerunning the same combination or model and optimizer using the same hyper-parameters configuration and same random seed yields stable results.</p>
<p>Benefits</p>
<p>Based on the hypotheses discussed in Section 2.3 , our experimental design not only suggest a specific setup for reproducible results but also identifies the factors of variation explicitly in case of failure in reproducing the same results by giving quantified measurement for the effect of variation on the results of the model.   </p>
<p>Conclusion</p>
<p>We model effects dependencies (model, optimizer, hyper-parameters configuration, seed, rerun) on the performance in a hierarchical manner: see 1. Exploiting the co-variance structure of the linear mixed models allows to establish relations between different factors like random seed, hyper-parameters reruns. By studying this, we directly test for significant differences in performance between various architecture choices subject to factors variation. This for instance allows for more informed decisions on architecture choices and training regimes, and gives better clarity on the variability characteristics of a given model.</p>
<p>To enable variability studies following our methodology, we intend to release a ready-to-run notebook (along with its docker container for easy reproducibility) with all the statistical tests implemented and an easy-to-follow example. This release will also include the data gathered in our case study to examplify the methodology.</p>
<p>Figure 1 :
1Visualization of tree of experiments and PRNG seeding procedure.</p>
<p>Table 1 :
1Search space for the experiments' hyper-parameters for Adam and SGD optimizers.Algorithms 
TADAM 
Proto nets 
Matching nets </p>
<p>Learning rate 
U(0.1, 0.02) N (0.005, 0.0012) 
log U(0.0001, 0.1) 
LR decay rate 
N (10, 1) 
0.5 
log U(0.00001, 0.01) 
LR decay period (batch) 
2500 
U(500, 2000) 
1 
Query shots per class 
U{16, 64} 
15 
U(5, 30) 
Pre-train batch size 
U{32, 64} 
-
-</p>
<p>N-Way 
5 
5 
5 
N-Shot / support set 
5 
5 
5 
Number of tasks per batch 
2 
1 
1 
Batch size 
100 
100 
500 
Early stop (epochs) 
-
20 
Training steps (batches) 
21K 
10K 
75K 
Test episodes 
500 
600 
600 </p>
<p>Table 2 :
2Random effects ANOVA.npar 
logLik 
AIC 
LRT 
Df 
Pr(&gt;Chisq) </p>
<p>(1 | seeds) 
14 
2493.680 -4959.361 
24.41725 
1 
7.757113e-07 
(1 | hparams) 
14 
1854.749 -3681.498 1302.27988 1 3.612198e-285 </p>
<p>Table 3 :
3Random effects parameters.Groups 
Name 
Variance Std. Dev. </p>
<p>seeds 
(Intercept) 0.0000309 0.005559 
hparams 
(Intercept) 0.0017922 0.042334 
Residual 
0.0004338 0.020828 </p>
<p>Table 4 :
4Linear Mixed Model fixed effects results.Sum Sq 
Mean Sq NumDF DenDF F value 
Pr(&gt;F) 
experiments 0.133368 0.0121244 
11 
56.75 
27.95 
5.896146e-19 </p>
<p>Table 5 :
5Means comparisons, see Figure 2 for graphical representation.Estimate Std. Error 
lower 
upper 
Pr(&gt;|t|) 
m-net-adam 
-0.006338 
0.027132 
-0.06071979 0.048044548 8.161823e-01 
m-net-sgd 
0.006836 
0.0271328 -0.04754668 0.061219636 8.020148e-01 
protonet-adam 0.004232 
0.027331 
-0.05051218 0.05897616 8.774987e-01 
protonet-sgd 
-0.000794 
0.027342 
-0.05555950 0.05397145 9.769353e-01 
tadam-adam 
-0.020189 0.02308498 -0.06631741 0.02594024 3.851393e-01 
tadam-sgd 
-0.000014 0.02713232 -0.05439609 0.05436825 9.995925e-01 </p>
<p>In an ideal world we would use PRNGs designed for parallel pseudo-independent streams accross experiments. However for practical purposes, as such PRNGs are rarely available in common ML libraries, we deemed acceptable to simply use multiple seeds of quality PRNGs such as Mersenne Twister(Matsumoto &amp; Nishimura, 1998).
Note that this linear model is equivalent to a one way ANOVA with i.i.d samples.</p>
<p>Fitting linear mixed-effects models using lme4. Douglas Bates, Martin Mächler, Ben Bolker, Steve Walker, arXiv:1406.5823arXiv preprintDouglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. Fitting linear mixed-effects models using lme4. arXiv preprint arXiv:1406.5823, 2014.</p>
<p>Linear mixed models and penalized least squares. M Douglas, Saikat Bates, Debroy, Journal of Multivariate Analysis. 911Douglas M Bates and Saikat DebRoy. Linear mixed models and penalized least squares. Journal of Multivariate Analysis, 91(1):1-17, 2004.</p>
<p>Building a reproducible machine learning pipeline. Małgorzata Cebrat, Florian Hartl, abs/1810.04570CoRRMałgorzata Cebrat and Florian Hartl. Building a reproducible machine learning pipeline. CoRR, abs/1810.04570, 2018.</p>
<p>Open is not enough. Xiaoli Chen, Sünje Dallmeier-Tiessen, Robin Dasler, Sebastian Feger, Pamfilos Fokianos, Jose Benito Gonzalez, Harri Hirvonsalo, Dinos Kousidis, Artemis Lavasa, Salvatore Mele, Nature Physics. 1Xiaoli Chen, Sünje Dallmeier-Tiessen, Robin Dasler, Sebastian Feger, Pamfilos Fokianos, Jose Benito Gonzalez, Harri Hirvonsalo, Dinos Kousidis, Artemis Lavasa, Salvatore Mele, et al. Open is not enough. Nature Physics, pp. 1, 2018.</p>
<p>One-shot learning of object categories. Li Fei-Fei, Rob Fergus, Pietro Perona, IEEE Transaction on Pattern Analysis and Machine Intelligence. 28Li Fei-fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Transaction on Pattern Analysis and Machine Intelligence, 28:2006, 2006.</p>
<p>Xv.-the correlation between relatives on the supposition of mendelian inheritance. A Ronald, Fisher, Earth and Environmental Science Transactions of the Royal Society of Edinburgh. 522Ronald A Fisher. Xv.-the correlation between relatives on the supposition of mendelian inheritance. Earth and Environmental Science Transactions of the Royal Society of Edinburgh, 52(2):399-433, 1919.</p>
<p>Data analysis using regression and multilevel/hierarchical models. Andrew Gelman, Jennifer Hill, Cambridge university pressAndrew Gelman and Jennifer Hill. Data analysis using regression and multilevel/hierarchical models. Cambridge university press, 2006.</p>
<p>Deep reinforcement learning that matters. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, David Meger, AAAI. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In AAAI, 2018.</p>
<p>Small sample inference for fixed effects from restricted maximum likelihood. G Michael, James H Kenward, Roger, Biometrics. Michael G Kenward and James H Roger. Small sample inference for fixed effects from restricted maximum likelihood. Biometrics, pp. 983-997, 1997.</p>
<p>lmertest package: tests in linear mixed effects models. Alexandra Kuznetsova, B Per, Rune Haubo Bojesen Brockhoff, Christensen, Journal of Statistical Software. 8213Alexandra Kuznetsova, Per B Brockhoff, and Rune Haubo Bojesen Christensen. lmertest package: tests in linear mixed effects models. Journal of Statistical Software, 82(13), 2017.</p>
<p>Troubling trends in machine learning scholarship. C Zachary, Jacob Lipton, Steinhardt, arXiv:1807.03341arXiv preprintZachary C Lipton and Jacob Steinhardt. Troubling trends in machine learning scholarship. arXiv preprint arXiv:1807.03341, 2018.</p>
<p>Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. Makoto Matsumoto, Takuji Nishimura, ACM Transactions on Modeling and Computer Simulation (TOMACS). 81Makoto Matsumoto and Takuji Nishimura. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. ACM Transactions on Modeling and Computer Simulation (TOMACS), 8(1):3-30, 1998.</p>
<p>On the state of the art of evaluation in neural language models. Gábor Melis, Chris Dyer, Phil Blunsom, abs/1707.05589CoRRGábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. CoRR, abs/1707.05589, 2018.</p>
<p>Tadam: Task dependent adaptive metric for improved few-shot learning. Boris Oreshkin, Alexandre Pau Rodríguez López, Lacoste, Advances in Neural Information Processing Systems. Boris Oreshkin, Pau Rodríguez López, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in Neural Information Processing Systems, 2018.</p>
<p>Optimization as a model for few-shot learning. Sachin Ravi, Hugo Larochelle, International Conference on Learning Representations. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference on Learning Representations, 2017.</p>
<p>Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. Nils Reimers, Iryna Gurevych, Nils Reimers and Iryna Gurevych. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. In EMNLP, 2017.</p>
<p>An approximate distribution of estimates of variance components. E Franklin, Satterthwaite, Biometrics bulletin. 26Franklin E Satterthwaite. An approximate distribution of estimates of variance components. Biomet- rics bulletin, 2(6):110-114, 1946.</p>
<p>Prototypical networks for few-shot learning. Jake Snell, Kevin Swersky, Richard Zemel, Advances in Neural Information Processing Systems. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in Neural Information Processing Systems, 2017.</p>
<p>A practical taxonomy of reproducibility for machine learning research. Rachael Tatman, Rachael Tatman. A practical taxonomy of reproducibility for machine learning research. 2018.</p>
<p>Matching networks for one shot learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Kavukcuoglu Koray, Daan Wierstra, Advances in Neural Information Processing Systems. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. Advances in Neural Information Processing Systems, 2016.</p>            </div>
        </div>

    </div>
</body>
</html>
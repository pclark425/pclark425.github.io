<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4354 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4354</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4354</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-6c5c6f883604a3abaa829b83d2958de8c343beeb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6c5c6f883604a3abaa829b83d2958de8c343beeb" target="_blank">A Computational Inflection for Scientific Discovery</a></p>
                <p><strong>Paper Venue:</strong> Communications of the ACM</p>
                <p><strong>Paper TL;DR:</strong> Enabling researchers to leverage systems to overcome the limits of human cognitive capacity is a central challenge in the next generation of artificial intelligence.</p>
                <p><strong>Paper Abstract:</strong> Enabling researchers to leverage systems to overcome the limits of human cognitive capacity.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4354.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4354.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bridger</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bridger: novel author discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that extracts problem and method mentions from a researcher's papers and matches them to other authors whose work is both relevant and novel, surfacing author 'cards' with salient problems, methods and papers to stimulate new research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Bridger author-matching</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Inner knowledge is represented as mentions of problems and methods automatically extracted from a researcher's papers and weighted by term frequency. Outer knowledge is represented by other authors' papers using the same representation. The system matches these representations to identify authors who are relevant but methodologically novel, ranks salient problems/methods/papers per author, and displays author cards to the user to encourage analogical transfer and idea generation. The paper frames Bridger as an NLP-driven retrieval and ranking pipeline; specific model architectures (LLMs or otherwise) are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computer science / cross-domain scholarly literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>relationship patterns: author–problem–method associations (cross-domain methodological correspondences)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>structured facets (extracted problem and method mentions) presented as ranked author cards</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human-subject studies with CS researchers measuring creative search and inspiration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as a substantial boost in creative search and inspiration relative to Semantic Scholar's neural models (no numeric performance values provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively/experimentally against Semantic Scholar's state-of-the-art neural search models; Bridger produced more useful cross-domain connections (no numerical metrics reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on rough proxies of a researcher's inner knowledge (term-frequency weighted mentions); extraction accuracy and generalization across diverse scientific language are limited; interpretability/control hooks are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4354.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogy mining / Solvent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerating innovation through analogy mining (Solvent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval system that finds inventions or papers with partial structural/mechanistic similarity to a user-provided invention description to enable analogical transfer across distant domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating innovation through analogy mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Analogy-mining structural retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given a textual description of an invention, NLP-based methods extract mechanistic structure and retrieve candidate documents (inventions/papers) with partial structural similarity. The approach mines a corpus of technological inventions and ranks candidates that are structurally related to the query to provide inspiration for transferring mechanisms across domains. The pipeline emphasizes structural/mechanistic features rather than pure lexical similarity; exact model architectures are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Technology / engineering / inventive patents and papers</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>structural/mechanistic analogies and pattern correspondences (qualitative relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>ranked retrieved documents with extracted mechanism descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human ideation experiments where users generate ideas after viewing retrieved inspirations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as significantly boosting measures of human creativity in ideation experiments versus baseline IR methods (no precise numeric values provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to standard information retrieval baselines; analogy-mining retrieval produced more useful creative inspirations (no numeric comparisons in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Difficulty generalizing structural similarity across domains; extraction errors; reliance on surface vs. deep semantics in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4354.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Problem-hierarchy retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical problem graph-based retrieval for creative inspiration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that extracts problem mentions from a corpus, mines co-occurrence to construct hierarchical problem graphs, and traverses neighboring problems around a focal problem to surface diverse problem-perspectives and inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling creative inspiration with fine-grained functional facets of product ideas</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Hierarchical problem-graph traversal</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>NLP models extract mentions of problems from technological/invention texts. Co-occurrence statistics are used to build a hierarchical problem graph capturing which problems appear together; given a user-supplied focal problem, the system traverses nearby nodes in the graph to retrieve alternative problem perspectives and related problems as inspiration for abstraction or reformulation. The pipeline is extraction → graph construction → neighbor traversal → presentation of candidate inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Design / technological inventions / product ideas</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>co-occurrence-derived relational patterns and hierarchical associations among problem concepts</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>hierarchical graph structures and retrieved problem-perspective suggestions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>User studies measuring usefulness and novelty of retrieved inspirations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Over 60% of inspirations retrieved were found useful and novel; reported a relative boost of 50–60% over the best-performing baselines (no absolute baseline numbers provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against best-performing baseline retrieval methods (unspecified here); relative improvement of 50–60% in novelty/usefulness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Extraction accuracy limitations, lexical diversity and ambiguity, normalization across terms and domains; models tend to focus on surface lexical patterns rather than deeper semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4354.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Challenges search engine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search engine for discovery of scientific challenges and directions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval system that searches the literature for statements of uncertainty, open questions and hypotheses to guide researcher attention to areas with high uncertainty or opportunity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A search engine for discovery of scientific challenges and directions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Uncertainty-statement retrieval engine</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The system indexes scholarly text and targets linguistic expressions that indicate difficulties, uncertainties, open hypotheses and questions; queries (as proxies for user interests) are used to retrieve and rank documents and extracted uncertainty statements most relevant to the user's topic. The prototype is evaluated against PubMed for biomedical queries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (example: COVID-19 / ACE2 receptor)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>extracted statements of uncertainty / open hypotheses (qualitative patterns of discourse)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>retrieved documents with highlighted uncertainty/open-question statements</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human experiments with diverse participants (including clinicians) comparing outputs to PubMed search results</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to dramatically outperform PubMed at discovering important and interesting areas of challenges and directions for given queries; no numeric scores provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to PubMed; prototype focused more on uncertainty statements while PubMed returned well-studied results for the same queries.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependence on linguistic cues for uncertainty (which vary across domains); proxies for user inner knowledge are limited; extraction and normalization challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4354.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPI debiasing / bias-of-locality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>On biases of attention in scientific discovery (protein–protein interactions study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporal analysis of the growing graph of confirmed protein–protein interactions (PPIs) showing a locality bias in scientific attention and proposing reprioritization mechanisms to surface neglected candidate PPIs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On biases of attention in scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Temporal PPI attention analysis and reprioritization</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Constructed a temporal dataset of confirmed PPIs spanning decades, analyzed patterns of scientific attention (notably a bias towards recently studied proteins), and developed methods that reprioritize candidate PPIs based on protein properties to counter locality bias—demonstrating that discoveries could have been made earlier with debiasing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biology / proteomics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>empirical attention patterns (temporal locality biases) and reprioritization scoring functions for candidate PPIs</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>ranked candidate PPIs and analytical statistics over temporal graphs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Retrospective temporal analyses showing that debiasing would have prioritized earlier-discovered interactions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Data coverage limitations in PPI databases, systemic blindspots driven by attention dynamics; translating reprioritization into laboratory validation remains nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4354.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nye et al. extractor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Understanding clinical trial reports: Extracting medical entities and their relations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated system to extract treatment–outcome relations from clinical trial reports to assist tasks such as systematic review and drug repurposing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding clinical trial reports: Extracting medical entities and their relations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Treatment–outcome relation extractor</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An information-extraction pipeline that identifies biomedical entities (treatments, outcomes) and the relations between them in clinical trial reports; the extracted structured relations are intended to support downstream search and synthesis for tasks like drug repurposing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / clinical trial literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>structured treatment–outcome relations (potentially including effect descriptors)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>structured relation triples (entities + relation labels)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Task-oriented evaluation (assisting identification of drug repurposing opportunities); details not provided in this paper's text</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires semantically meaningful extraction aligned with researchers' reasoning; domain-specific complexities and variability in reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4354.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biomedical Evidence Enhanced Prediction (BEEP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-augmented clinical outcome prediction system that retrieves papers relevant to an ICU patient and synthesizes that literature with EMR features to improve predictive accuracy for outcomes such as mortality or length-of-stay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Literature-augmented clinical outcome prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-augmented clinical prediction (BEEP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each patient, BEEP retrieves medical papers relevant to patient-specific queries (extracted from clinical notes) and combines retrieved literature evidence with EMR-derived features in a prediction model to estimate outcomes (e.g., in-hospital mortality). The pipeline integrates retrieval, evidence synthesis, and predictive modeling; exact model architectures are not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Clinical medicine / intensive care</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>predictive relationships between patient features + literature evidence and clinical outcomes (statistical predictive models)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>predicted risk scores and alignments between patient aspects and cohorts described in papers</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison to state-of-the-art predictive models that do not use literature retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported 'large improvements' over state-of-the-art models that did not use literature retrieval; no numeric results included in this paper's text</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against state-of-the-art models without literature augmentation; BEEP improved performance (no numerical values provided here)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Aligning literature evidence with patient-specific contexts; accurate retrieval and synthesis; representational alignment with clinician reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4354.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACCORD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACCORD: multi-document generation of concept descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that generates personalized, grounded definitions of scientific concepts by retrieving instances where a target concept is explained in terms of user-familiar source concepts and then applying neural text generation to produce structured explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accord: A multi-document approach to generating diverse descriptions of scientific concepts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Personalized definition generation (ACCORD)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Input: a list of user-known source concepts (estimated from papers the user wrote/read) and a target concept. The system retrieves documents where the target is explained in terms of the known sources, then applies a neural text-generation model to rewrite the retrieved text into structured templates that ground the target concept in user-familiar concepts. Specific model architectures and sizes are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>neural text-generation model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>not targeted at extracting quantitative laws; produces grounded conceptual definitions</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>generated textual explanations conforming to templates</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not detailed in this paper's main text; presented as a prototype for personalized concept explanations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires accurate retrieval of explanatory passages; need for full-document understanding (tables, figures, equations) and alignment to user's knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4354.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-specific pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-specific pretraining for vertical search (biomedical case)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretraining neural language models on large, domain-specific corpora (biomedical papers) to improve literature search, support natural language queries, and retrieve aspectually similar documents for scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domainspecific pretraining for vertical search: Case study on biomedical literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-specific neural pretraining for literature search</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train/self-supervise language models on massive biomedical corpora to create representations better aligned with biomedical discourse; use these models to support natural-language queries, match discourse aspects (e.g., methods), and retrieve aspectually similar documents. The approach is a representation + retrieval pipeline; exact model sizes/architectures are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>domain-specific neural language models (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature / vertical search</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>document similarity and aspectual matching (not direct extraction of mathematical laws)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>ranked retrieved documents with higher precision for aspectual matches</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation on retrieval benchmarks and user-facing search features</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported state-of-the-art results across biomedical and computer science literature retrieval tasks (no numeric scores provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Improved over general pretrained models and classical retrieval baselines; specific numbers not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Full-document understanding (tables, equations, figures) still lacking; extraction accuracy for deeper semantic relations is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4354.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4354.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / LLM synthesis (aspirational)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 and large language models for scientific synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses GPT-4 and LLMs as promising tools that, when combined with retrieval over scientific corpora, could synthesize hypotheses, critique studies, recommend directions, and potentially generate structured scientific concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-augmented LLM synthesis for scientific discovery (aspirational)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An aspirational pipeline described in the paper: (1) retrieval of domain literature and evidence from large scholarly corpora; (2) conditioning or fine-tuning large language models (e.g., GPT-4) on retrieved documents to synthesize hypotheses, compose concepts, or generate structured representations; (3) present outputs aligned with a user's inner knowledge and tasks. The paper notes early experiments with GPT-4 indicating promising abilities for hypothesis formulation, critique, and recommending research directions, but provides no concrete architectural details, prompt recipes, or evaluation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (mentioned); LLMs in general (no sizes or specific fine-tuned variants specified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General / cross-disciplinary (aspirational for many scientific domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>potential to infer relationships/hypotheses and compose concepts (no concrete examples of extracted mathematical/scaling laws given in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>textual hypotheses, structured representations or templates (aspirational)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Early experiments and human evaluation are referenced at a high level; no formal validation protocol or metrics are reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Current LLM representations lack interpretability and 'hooks' for control; extraction accuracy from scientific texts (including tables, equations, figures) is limited; risk of focusing on surface lexical patterns rather than deep semantics; need for grounding, retrieval augmentation, and human-AI collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Computational Inflection for Scientific Discovery', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Accelerating innovation through analogy mining <em>(Rating: 2)</em></li>
                <li>Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery <em>(Rating: 2)</em></li>
                <li>Scaling creative inspiration with fine-grained functional facets of product ideas <em>(Rating: 2)</em></li>
                <li>A search engine for discovery of scientific challenges and directions <em>(Rating: 2)</em></li>
                <li>On biases of attention in scientific discovery <em>(Rating: 2)</em></li>
                <li>Literature-augmented clinical outcome prediction <em>(Rating: 2)</em></li>
                <li>Understanding clinical trial reports: Extracting medical entities and their relations <em>(Rating: 2)</em></li>
                <li>Accord: A multi-document approach to generating diverse descriptions of scientific concepts <em>(Rating: 2)</em></li>
                <li>Scico: Hierarchical cross-document coreference for scientific concepts <em>(Rating: 2)</em></li>
                <li>Domainspecific pretraining for vertical search: Case study on biomedical literature <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4354",
    "paper_id": "paper-6c5c6f883604a3abaa829b83d2958de8c343beeb",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Bridger",
            "name_full": "Bridger: novel author discovery",
            "brief_description": "A system that extracts problem and method mentions from a researcher's papers and matches them to other authors whose work is both relevant and novel, surfacing author 'cards' with salient problems, methods and papers to stimulate new research directions.",
            "citation_title": "Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery",
            "mention_or_use": "use",
            "method_name": "Bridger author-matching",
            "method_description": "Inner knowledge is represented as mentions of problems and methods automatically extracted from a researcher's papers and weighted by term frequency. Outer knowledge is represented by other authors' papers using the same representation. The system matches these representations to identify authors who are relevant but methodologically novel, ranks salient problems/methods/papers per author, and displays author cards to the user to encourage analogical transfer and idea generation. The paper frames Bridger as an NLP-driven retrieval and ranking pipeline; specific model architectures (LLMs or otherwise) are not specified in this paper.",
            "llm_model_used": null,
            "scientific_domain": "Computer science / cross-domain scholarly literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "relationship patterns: author–problem–method associations (cross-domain methodological correspondences)",
            "extraction_output_format": "structured facets (extracted problem and method mentions) presented as ranked author cards",
            "validation_method": "Human-subject studies with CS researchers measuring creative search and inspiration",
            "performance_metrics": "Reported as a substantial boost in creative search and inspiration relative to Semantic Scholar's neural models (no numeric performance values provided in this paper)",
            "baseline_comparison": "Compared qualitatively/experimentally against Semantic Scholar's state-of-the-art neural search models; Bridger produced more useful cross-domain connections (no numerical metrics reported here)",
            "challenges_limitations": "Relies on rough proxies of a researcher's inner knowledge (term-frequency weighted mentions); extraction accuracy and generalization across diverse scientific language are limited; interpretability/control hooks are limited.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.0",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Analogy mining / Solvent",
            "name_full": "Accelerating innovation through analogy mining (Solvent)",
            "brief_description": "A retrieval system that finds inventions or papers with partial structural/mechanistic similarity to a user-provided invention description to enable analogical transfer across distant domains.",
            "citation_title": "Accelerating innovation through analogy mining",
            "mention_or_use": "use",
            "method_name": "Analogy-mining structural retrieval",
            "method_description": "Given a textual description of an invention, NLP-based methods extract mechanistic structure and retrieve candidate documents (inventions/papers) with partial structural similarity. The approach mines a corpus of technological inventions and ranks candidates that are structurally related to the query to provide inspiration for transferring mechanisms across domains. The pipeline emphasizes structural/mechanistic features rather than pure lexical similarity; exact model architectures are not specified in this paper.",
            "llm_model_used": null,
            "scientific_domain": "Technology / engineering / inventive patents and papers",
            "number_of_papers": null,
            "type_of_quantitative_law": "structural/mechanistic analogies and pattern correspondences (qualitative relationships)",
            "extraction_output_format": "ranked retrieved documents with extracted mechanism descriptions",
            "validation_method": "Human ideation experiments where users generate ideas after viewing retrieved inspirations",
            "performance_metrics": "Reported as significantly boosting measures of human creativity in ideation experiments versus baseline IR methods (no precise numeric values provided in this paper)",
            "baseline_comparison": "Compared to standard information retrieval baselines; analogy-mining retrieval produced more useful creative inspirations (no numeric comparisons in-paper).",
            "challenges_limitations": "Difficulty generalizing structural similarity across domains; extraction errors; reliance on surface vs. deep semantics in some cases.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.1",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Problem-hierarchy retrieval",
            "name_full": "Hierarchical problem graph-based retrieval for creative inspiration",
            "brief_description": "A method that extracts problem mentions from a corpus, mines co-occurrence to construct hierarchical problem graphs, and traverses neighboring problems around a focal problem to surface diverse problem-perspectives and inspirations.",
            "citation_title": "Scaling creative inspiration with fine-grained functional facets of product ideas",
            "mention_or_use": "use",
            "method_name": "Hierarchical problem-graph traversal",
            "method_description": "NLP models extract mentions of problems from technological/invention texts. Co-occurrence statistics are used to build a hierarchical problem graph capturing which problems appear together; given a user-supplied focal problem, the system traverses nearby nodes in the graph to retrieve alternative problem perspectives and related problems as inspiration for abstraction or reformulation. The pipeline is extraction → graph construction → neighbor traversal → presentation of candidate inspirations.",
            "llm_model_used": null,
            "scientific_domain": "Design / technological inventions / product ideas",
            "number_of_papers": null,
            "type_of_quantitative_law": "co-occurrence-derived relational patterns and hierarchical associations among problem concepts",
            "extraction_output_format": "hierarchical graph structures and retrieved problem-perspective suggestions",
            "validation_method": "User studies measuring usefulness and novelty of retrieved inspirations",
            "performance_metrics": "Over 60% of inspirations retrieved were found useful and novel; reported a relative boost of 50–60% over the best-performing baselines (no absolute baseline numbers provided in this paper).",
            "baseline_comparison": "Compared against best-performing baseline retrieval methods (unspecified here); relative improvement of 50–60% in novelty/usefulness metrics.",
            "challenges_limitations": "Extraction accuracy limitations, lexical diversity and ambiguity, normalization across terms and domains; models tend to focus on surface lexical patterns rather than deeper semantics.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.2",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Challenges search engine",
            "name_full": "Search engine for discovery of scientific challenges and directions",
            "brief_description": "A retrieval system that searches the literature for statements of uncertainty, open questions and hypotheses to guide researcher attention to areas with high uncertainty or opportunity.",
            "citation_title": "A search engine for discovery of scientific challenges and directions",
            "mention_or_use": "use",
            "method_name": "Uncertainty-statement retrieval engine",
            "method_description": "The system indexes scholarly text and targets linguistic expressions that indicate difficulties, uncertainties, open hypotheses and questions; queries (as proxies for user interests) are used to retrieve and rank documents and extracted uncertainty statements most relevant to the user's topic. The prototype is evaluated against PubMed for biomedical queries.",
            "llm_model_used": null,
            "scientific_domain": "Biomedical literature (example: COVID-19 / ACE2 receptor)",
            "number_of_papers": null,
            "type_of_quantitative_law": "extracted statements of uncertainty / open hypotheses (qualitative patterns of discourse)",
            "extraction_output_format": "retrieved documents with highlighted uncertainty/open-question statements",
            "validation_method": "Human experiments with diverse participants (including clinicians) comparing outputs to PubMed search results",
            "performance_metrics": "Reported to dramatically outperform PubMed at discovering important and interesting areas of challenges and directions for given queries; no numeric scores provided in this paper.",
            "baseline_comparison": "Compared to PubMed; prototype focused more on uncertainty statements while PubMed returned well-studied results for the same queries.",
            "challenges_limitations": "Dependence on linguistic cues for uncertainty (which vary across domains); proxies for user inner knowledge are limited; extraction and normalization challenges.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.3",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "PPI debiasing / bias-of-locality",
            "name_full": "On biases of attention in scientific discovery (protein–protein interactions study)",
            "brief_description": "A temporal analysis of the growing graph of confirmed protein–protein interactions (PPIs) showing a locality bias in scientific attention and proposing reprioritization mechanisms to surface neglected candidate PPIs.",
            "citation_title": "On biases of attention in scientific discovery",
            "mention_or_use": "use",
            "method_name": "Temporal PPI attention analysis and reprioritization",
            "method_description": "Constructed a temporal dataset of confirmed PPIs spanning decades, analyzed patterns of scientific attention (notably a bias towards recently studied proteins), and developed methods that reprioritize candidate PPIs based on protein properties to counter locality bias—demonstrating that discoveries could have been made earlier with debiasing.",
            "llm_model_used": null,
            "scientific_domain": "Biology / proteomics",
            "number_of_papers": null,
            "type_of_quantitative_law": "empirical attention patterns (temporal locality biases) and reprioritization scoring functions for candidate PPIs",
            "extraction_output_format": "ranked candidate PPIs and analytical statistics over temporal graphs",
            "validation_method": "Retrospective temporal analyses showing that debiasing would have prioritized earlier-discovered interactions",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Data coverage limitations in PPI databases, systemic blindspots driven by attention dynamics; translating reprioritization into laboratory validation remains nontrivial.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.4",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Nye et al. extractor",
            "name_full": "Understanding clinical trial reports: Extracting medical entities and their relations",
            "brief_description": "An automated system to extract treatment–outcome relations from clinical trial reports to assist tasks such as systematic review and drug repurposing.",
            "citation_title": "Understanding clinical trial reports: Extracting medical entities and their relations",
            "mention_or_use": "mention",
            "method_name": "Treatment–outcome relation extractor",
            "method_description": "An information-extraction pipeline that identifies biomedical entities (treatments, outcomes) and the relations between them in clinical trial reports; the extracted structured relations are intended to support downstream search and synthesis for tasks like drug repurposing.",
            "llm_model_used": null,
            "scientific_domain": "Biomedical / clinical trial literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "structured treatment–outcome relations (potentially including effect descriptors)",
            "extraction_output_format": "structured relation triples (entities + relation labels)",
            "validation_method": "Task-oriented evaluation (assisting identification of drug repurposing opportunities); details not provided in this paper's text",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Requires semantically meaningful extraction aligned with researchers' reasoning; domain-specific complexities and variability in reporting.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.5",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "BEEP",
            "name_full": "Biomedical Evidence Enhanced Prediction (BEEP)",
            "brief_description": "A literature-augmented clinical outcome prediction system that retrieves papers relevant to an ICU patient and synthesizes that literature with EMR features to improve predictive accuracy for outcomes such as mortality or length-of-stay.",
            "citation_title": "Literature-augmented clinical outcome prediction",
            "mention_or_use": "use",
            "method_name": "Retrieval-augmented clinical prediction (BEEP)",
            "method_description": "For each patient, BEEP retrieves medical papers relevant to patient-specific queries (extracted from clinical notes) and combines retrieved literature evidence with EMR-derived features in a prediction model to estimate outcomes (e.g., in-hospital mortality). The pipeline integrates retrieval, evidence synthesis, and predictive modeling; exact model architectures are not described in this paper.",
            "llm_model_used": null,
            "scientific_domain": "Clinical medicine / intensive care",
            "number_of_papers": null,
            "type_of_quantitative_law": "predictive relationships between patient features + literature evidence and clinical outcomes (statistical predictive models)",
            "extraction_output_format": "predicted risk scores and alignments between patient aspects and cohorts described in papers",
            "validation_method": "Comparison to state-of-the-art predictive models that do not use literature retrieval",
            "performance_metrics": "Reported 'large improvements' over state-of-the-art models that did not use literature retrieval; no numeric results included in this paper's text",
            "baseline_comparison": "Compared against state-of-the-art models without literature augmentation; BEEP improved performance (no numerical values provided here)",
            "challenges_limitations": "Aligning literature evidence with patient-specific contexts; accurate retrieval and synthesis; representational alignment with clinician reasoning.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.6",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "ACCORD",
            "name_full": "ACCORD: multi-document generation of concept descriptions",
            "brief_description": "A system that generates personalized, grounded definitions of scientific concepts by retrieving instances where a target concept is explained in terms of user-familiar source concepts and then applying neural text generation to produce structured explanations.",
            "citation_title": "Accord: A multi-document approach to generating diverse descriptions of scientific concepts",
            "mention_or_use": "use",
            "method_name": "Personalized definition generation (ACCORD)",
            "method_description": "Input: a list of user-known source concepts (estimated from papers the user wrote/read) and a target concept. The system retrieves documents where the target is explained in terms of the known sources, then applies a neural text-generation model to rewrite the retrieved text into structured templates that ground the target concept in user-familiar concepts. Specific model architectures and sizes are not specified in this paper.",
            "llm_model_used": "neural text-generation model (unspecified)",
            "scientific_domain": "General scientific literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "not targeted at extracting quantitative laws; produces grounded conceptual definitions",
            "extraction_output_format": "generated textual explanations conforming to templates",
            "validation_method": "Not detailed in this paper's main text; presented as a prototype for personalized concept explanations",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Requires accurate retrieval of explanatory passages; need for full-document understanding (tables, figures, equations) and alignment to user's knowledge.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.7",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Domain-specific pretraining",
            "name_full": "Domain-specific pretraining for vertical search (biomedical case)",
            "brief_description": "Pretraining neural language models on large, domain-specific corpora (biomedical papers) to improve literature search, support natural language queries, and retrieve aspectually similar documents for scientific tasks.",
            "citation_title": "Domainspecific pretraining for vertical search: Case study on biomedical literature",
            "mention_or_use": "mention",
            "method_name": "Domain-specific neural pretraining for literature search",
            "method_description": "Train/self-supervise language models on massive biomedical corpora to create representations better aligned with biomedical discourse; use these models to support natural-language queries, match discourse aspects (e.g., methods), and retrieve aspectually similar documents. The approach is a representation + retrieval pipeline; exact model sizes/architectures are not specified in this paper.",
            "llm_model_used": "domain-specific neural language models (unspecified)",
            "scientific_domain": "Biomedical literature / vertical search",
            "number_of_papers": null,
            "type_of_quantitative_law": "document similarity and aspectual matching (not direct extraction of mathematical laws)",
            "extraction_output_format": "ranked retrieved documents with higher precision for aspectual matches",
            "validation_method": "Evaluation on retrieval benchmarks and user-facing search features",
            "performance_metrics": "Reported state-of-the-art results across biomedical and computer science literature retrieval tasks (no numeric scores provided in this paper)",
            "baseline_comparison": "Improved over general pretrained models and classical retrieval baselines; specific numbers not given in this paper.",
            "challenges_limitations": "Full-document understanding (tables, equations, figures) still lacking; extraction accuracy for deeper semantic relations is limited.",
            "requires_human_in_loop": false,
            "fully_automated": false,
            "uuid": "e4354.8",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "GPT-4 / LLM synthesis (aspirational)",
            "name_full": "GPT-4 and large language models for scientific synthesis",
            "brief_description": "The paper discusses GPT-4 and LLMs as promising tools that, when combined with retrieval over scientific corpora, could synthesize hypotheses, critique studies, recommend directions, and potentially generate structured scientific concepts.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "method_name": "Retrieval-augmented LLM synthesis for scientific discovery (aspirational)",
            "method_description": "An aspirational pipeline described in the paper: (1) retrieval of domain literature and evidence from large scholarly corpora; (2) conditioning or fine-tuning large language models (e.g., GPT-4) on retrieved documents to synthesize hypotheses, compose concepts, or generate structured representations; (3) present outputs aligned with a user's inner knowledge and tasks. The paper notes early experiments with GPT-4 indicating promising abilities for hypothesis formulation, critique, and recommending research directions, but provides no concrete architectural details, prompt recipes, or evaluation numbers.",
            "llm_model_used": "GPT-4 (mentioned); LLMs in general (no sizes or specific fine-tuned variants specified)",
            "scientific_domain": "General / cross-disciplinary (aspirational for many scientific domains)",
            "number_of_papers": null,
            "type_of_quantitative_law": "potential to infer relationships/hypotheses and compose concepts (no concrete examples of extracted mathematical/scaling laws given in this paper)",
            "extraction_output_format": "textual hypotheses, structured representations or templates (aspirational)",
            "validation_method": "Early experiments and human evaluation are referenced at a high level; no formal validation protocol or metrics are reported in this paper",
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Current LLM representations lack interpretability and 'hooks' for control; extraction accuracy from scientific texts (including tables, equations, figures) is limited; risk of focusing on surface lexical patterns rather than deep semantics; need for grounding, retrieval augmentation, and human-AI collaboration.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4354.9",
            "source_info": {
                "paper_title": "A Computational Inflection for Scientific Discovery",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Accelerating innovation through analogy mining",
            "rating": 2,
            "sanitized_title": "accelerating_innovation_through_analogy_mining"
        },
        {
            "paper_title": "Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery",
            "rating": 2,
            "sanitized_title": "bridger_toward_bursting_scientific_filter_bubbles_and_boosting_innovation_via_novel_author_discovery"
        },
        {
            "paper_title": "Scaling creative inspiration with fine-grained functional facets of product ideas",
            "rating": 2,
            "sanitized_title": "scaling_creative_inspiration_with_finegrained_functional_facets_of_product_ideas"
        },
        {
            "paper_title": "A search engine for discovery of scientific challenges and directions",
            "rating": 2,
            "sanitized_title": "a_search_engine_for_discovery_of_scientific_challenges_and_directions"
        },
        {
            "paper_title": "On biases of attention in scientific discovery",
            "rating": 2,
            "sanitized_title": "on_biases_of_attention_in_scientific_discovery"
        },
        {
            "paper_title": "Literature-augmented clinical outcome prediction",
            "rating": 2,
            "sanitized_title": "literatureaugmented_clinical_outcome_prediction"
        },
        {
            "paper_title": "Understanding clinical trial reports: Extracting medical entities and their relations",
            "rating": 2,
            "sanitized_title": "understanding_clinical_trial_reports_extracting_medical_entities_and_their_relations"
        },
        {
            "paper_title": "Accord: A multi-document approach to generating diverse descriptions of scientific concepts",
            "rating": 2,
            "sanitized_title": "accord_a_multidocument_approach_to_generating_diverse_descriptions_of_scientific_concepts"
        },
        {
            "paper_title": "Scico: Hierarchical cross-document coreference for scientific concepts",
            "rating": 2,
            "sanitized_title": "scico_hierarchical_crossdocument_coreference_for_scientific_concepts"
        },
        {
            "paper_title": "Domainspecific pretraining for vertical search: Case study on biomedical literature",
            "rating": 2,
            "sanitized_title": "domainspecific_pretraining_for_vertical_search_case_study_on_biomedical_literature"
        }
    ],
    "cost": 0.022628,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Computational Inflection for Scientific Discovery</h1>
<p>Tom Hope<br>tomh@allenai.org<br>The Allen Institute for AI<br>The Hebrew University of Jerusalem</p>
<p>Doug Downey<br>dougd@allenai.org<br>The Allen Institute for AI<br>Northwestern University</p>
<h2>Oren Etzioni</h2>
<p>oren@allenai.org
The Allen Institute for AI</p>
<h2>Daniel S. Weld</h2>
<p>danw@allenai.org
The Allen Institute for AI
The University of Washington</p>
<h2>Eric Horvitz</h2>
<p>horvitz@microsoft.com
Office of the Chief Scientific Officer
Microsoft
of possibility. The way we search through and reflect about information across the vast space-the areas we select to explore, and how we explore them-is hindered by cognitive biases [26] and lacks principled and scalable tools for guiding our attention [32]. "Unknowns" are not just holes in science, but important gaps in personal knowledge about the broader knowns across the sciences.</p>
<p>We thus face an imbalance between the treasure trove of scholarly information and our limited ability to reach into it. Despite technological advances, we require new paradigms and capabilities to address this widening gap. We see promise in developing new foundational capabilities that address the cognitive bottleneck, aimed at extending human performance on core tasks of researche.g., keeping abreast with developments, forming and prioritizing ideas, conducting experiments, reading and understanding papers (see Table 1). We focus on a research agenda we call task-guided scientific knowledge retrieval, in which systems counter humans' bounded capacity by ingesting corpora of scientific knowledge and retrieving inspirations, explanations, solutions and evidence synthesized to directly serve task-specific utility. We present key concepts of task-guided scientific knowledge retrieval, including work on prototypes that highlight the promise of the direction and bring into focus concrete steps forward for novel representations, tools, and services. In Section 4 we review systems that help researchers discover novel perspectives and inspirations [8, 9, 11, 29], help guide the attention of researchers toward opportunity areas rife with uncertainties and unknowns [18, 32], and models that leverage retrieval and synthesis of scientific knowledge as part of machine learning and prediction [6, 24]. We conclude in Section 5 with a discussion of opportunities ahead with computational approaches that have the potential to revolutionize science.</p>
<p>To set the stage, in the following section we begin by discussing some fundamental concepts and background for our research agenda.</p>
<h2>3 HUMAN-CENTRIC PERSPECTIVE</h2>
<p>Extraordinary developments at the convergence of AI and scientific discovery have emerged in specific areas, including new kinds of analytical tools, with the prominent example of AlphaFold, which harnesses deep neural models to dramatically improve the prediction of protein structure from amino acid sequence information [15]. Large language models (LLMs) have very recently made stellar progress in the ability to reason about complex tasks, including in the medical domain [25]. The most advanced LLM at presentemerging before the ink has dried on this paper-is GPT-4, which</p>
<table>
<thead>
<tr>
<th>Task/Activity</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Attention to areas of interest</td>
<td>A background process of keeping track of latest developments in relevant scientific communities.</td>
</tr>
<tr>
<td></td>
<td>Involves applying selective attention, perceiving relevance and utility.</td>
</tr>
<tr>
<td>Problem identification &amp; prioritization</td>
<td>Identifying new research questions and deciding on which ones to work. Involves factors such as</td>
</tr>
<tr>
<td></td>
<td>subjective preferences and assessment of feasibility.</td>
</tr>
<tr>
<td>Forming directions</td>
<td>Given a problem/question, forming ideas to address it. Involves cognitive processes such as</td>
</tr>
<tr>
<td></td>
<td>constructing mental models of a problem, problem reformulation, abstraction and decomposition,</td>
</tr>
<tr>
<td></td>
<td>adaptation of relevant knowledge to new scenarios, and assessing likelihood of success.</td>
</tr>
<tr>
<td>Literature search &amp; review</td>
<td>Accessing and ingesting knowledge in the literature. Involves many processes such as query</td>
</tr>
<tr>
<td></td>
<td>formulation, skimming and assessing relevance, positioning ideas with relations and contrasts to</td>
</tr>
<tr>
<td></td>
<td>existing work, and reading and summarization strategies.</td>
</tr>
<tr>
<td>Learning, understanding, sense-making</td>
<td>The cognitive processes and activities involved in assimilating new knowledge and concepts, and</td>
</tr>
<tr>
<td></td>
<td>making sense of complex scientific information spaces.</td>
</tr>
<tr>
<td>Experimentation, analysis, action</td>
<td>A broad category referring to the many processes and activities involved in formulating and</td>
</tr>
<tr>
<td></td>
<td>conducting experiments (e.g., planning data collection and measurements), performing analyses</td>
</tr>
<tr>
<td></td>
<td>(e.g., understanding a set of data points, modeling and extrapolation, prediction, evaluation), and</td>
</tr>
<tr>
<td></td>
<td>producing artifacts, techniques, theories, decisions, policies, actions.</td>
</tr>
<tr>
<td>Research communication</td>
<td>Writing research documents (papers, proposals, analyses), communicating with peers (feedback</td>
</tr>
<tr>
<td></td>
<td>and review, collaboration, presentation).</td>
</tr>
</tbody>
</table>
<p>Table 1: Research may be decomposed into salient tasks that are prime targets for computational augmentation (§ 4).</p>
<p>has exhibited jaw-dropping skill at handling clinical questions, mathematical problems and computer coding tasks [1].</p>
<p>We view these developments as tremendous research opportunities for building computational approaches that accelerate scientific discovery. We take a human-centered, cognitive perspective: augmenting researchers by taking into account the diversity of tasks, contexts, and cognitive processes involved in consuming and producing scientific knowledge. Collectively, we refer to these as the inner cognitive world of a researcher (see Figure 1). The researcher interacts with the scientific ecosystem—literature, resources, discussions—in order to inform decisions and actions. Researchers have different uses for scholarly information, depending on the task at hand and the stage of exploration (see Table 1 and discussion in Section 4). We pursue a research agenda around assisting researchers in their tasks, guided by two main desiderata:</p>
<p>(1) Systems for augmenting human capabilities in the sciences need to enhance the effective flow of knowledge from the outer world of scientific information and discourse to the researcher’s inner cognitive world—countering humans’ bounded capacity by retrieving and synthesizing information targeted to enhance performance on tasks. Achieving this goal requires methods that build and leverage rich representations of scientific content and that can align computational representations with human representations, in the context of specific tasks and backgrounds of researchers.</p>
<p>(2) Research on such systems should be rooted in conceptual models of the inner cognitive world of a researcher. Shining a spotlight on this inner world brings numerous factors and questions to the fore. How do researchers form ideas? How do they decide which problems to look into? How do they find and assimilate new information in the process of making decisions? What cognitive representations and bottlenecks are involved? What computing services would best augment these processes?</p>
<p>Figure 1: Information flows from the outer world into the inner cognitive world of researchers, constrained by cognitive capacity and biases. We see opportunities to support researchers by retrieving knowledge that helps with tasks across multiple phases of the scientific process (Table 1).</p>
<p>Background and Related Themes. We leverage research in natural language processing, information retrieval, data mining and human-computer interaction and draw concepts from multiple disciplines. For example, efforts in metascience focus on sociological factors that influence the evolution of science [17], e.g., analyses of information silos that impede mutual understanding and interaction [38], analyses of macro-scale ramifications of the rapid growth in scholarly publications [4], and of current metrics for measuring impact [5] — work enabled by digitization of scholarly corpora (see Section 3.1). Metascience research makes important observations about human biases (desideratum 2) but generally does not engage in building computational interventions to augment researchers (desideratum 1). Conversely, work in literature-based discovery [33] mines information from literature to generate new predictions (e.g., functions of materials or drug targets) but is typically done in isolation from cognitive considerations; however, these techniques have great promise in being used as part of human-augmentation</p>
<p>(2) Research on such systems should be rooted in conceptual models of the inner cognitive world of a researcher. Shining a spotlight on this inner world brings numerous factors and questions to the fore. How do researchers form ideas? How do they decide which problems to look into? How do they find and assimilate new information in the process of making decisions? What cognitive representations and bottlenecks are involved? What computing services would best augment these processes?</p>
<p>systems (see Sections 4-5). Other work uses machines to automate aspects of science. Pioneering work from Herbert Simon and Pat Langley automated discovery of empirical laws from data, with models inspired by cognitive mechanisms of discovery (see Section 3.2). More recent work has focused on developing robot scientists [16, 30] that run certain experiments in biology or chemistry-not only formulating hypotheses but "closing the loop" by automated tests in a physical laboratory-where robots may use narrow curated background knowledge (e.g., of a specific gene regulatory network) and machine learning to guide new experiments. Related work explores automating scientific data analysis [6], which we discuss in Section 4 as a case of retrieval from scientific repositories to augment aspects of experimentation and analysis (see Table 1).</p>
<p>We now turn to a discussion of central concepts: the ecosystem of science, and the cognitive world. This presentation lays the foundations for our exposition of task-guided retrieval in Section 4 and the research opportunities in Section 5.</p>
<h3>3.1 Outer World: Scientific Ecosystem</h3>
<p>We collectively name the scientific ecosystem and the digital representations of scientific knowledge as the outer world (see Figure 1). The outer world is comprised of scientific communities, a complex and shifting web of peers, concepts, methodologies, problems and directions revolving around shared interests, understandings and paradigms. This ecosystem generates digital information-digital "traces" of scientific thought and behavior-lying at the center of our attention as computer scientists interested in boosting human capacity to "reach into" the pool of scientific knowledge. This knowledge includes scholarly publications that appear in journals, conference proceedings, and online preprint repositories. Online publications are a main case of digital research artifacts; other examples of products of research include software, datasets, knowledge bases. Research artifacts are also associated typically with signals of quality and interest, such as citations to a specific paper or downloads of a dataset. The specific context of why a paper or resource was cited or used is often reflected in natural language descriptions. Different types of signals include peer review prior to publication (mostly not shared publicly), and social media discussions such as on Twitter, which has become a major virtual platform for academic dissemination and conversation. Along with the trend in society, private communication channels among researchers are also digital-mails, online calls and messages. Similarly, note taking and writing-important activities across the scientific workflow-are done in digital form. This information is siloed in different platforms under privacy restrictions, yet represents a treasure trove for tools for the augmentation of scientific reasoning and exploration.</p>
<h3>3.2 Inner World: Human Cognition in Science</h3>
<p>The way researchers decide to interact with information in the outer world and the way they process and use this information is governed by a complex array of cognitive processes, personal knowledge and preferences, biases and limitations, which are only partially understood. We collectively name these the inner world, and briefly discuss several salient aspects.</p>
<p>Early work in AI by Herbert Simon and Alan Newell and later efforts by Pat Langley and Paul Thagard focused on cognitive and
computational aspects of problem solving, creativity, decision making and scientific reasoning and discovery, seeking algorithmic representations to help understand and mimic human intelligence [19, 36]. Cognitive mechanisms that play important roles in scientific discovery include inductive and abductive reasoning, mental modeling of problems and situations, abstraction, decomposition, reformulation, analogical transfer and recombination; for example, in analogical transfer, given a situation or problem being considered in our working memory, we retrieve from our long-term memory prior analogous problems or situations.</p>
<p>This cognitive machinery powers humans' ingenuity. However, the human mind also has severe limitations-bounded rationality in the words of Simon-that impede these powerful mechanisms. Our limitations and capabilities have been studied for over a hundred years with cognitive psychology. Our limitations manifest in bounded cognitive capacity and knowledge, and biases that govern our behaviors and preferences. These limitations are all tightly interrelated. The ability to generate ideas, for instance, directly relies on prior knowledge; but, when a large volume of information from the outer world of science is met by insufficient cognitive capacity for processing and assimilating it, the result is information overload-a ubiquitous hindrance for researchers [29]. Information overload in science strains the attentional resources of researchers, and forces researchers to allocate attention to increasingly narrow areas. This effect, in turn, amplifies a host of biases which researchers, just like all humans, suffer from [26, 32]. For example, scientists can be limited by confirmation bias, aversion to information from novel domains, homophily, and fixation on specific directions and perspectives without consideration of alternative views [11, 26]. More broadly, selection of directions and areas to work on is a case of decision-making, and as such personal preference and subjective utility play fundamental roles. Our research decisions rely on subjective assessment of feasibility, long-term or short-term goals and interests, and even psychological factors (e.g., tendencies for risk aversion). These factors are of course also impacted by biases [26].</p>
<p>Clearly, the inner world of researchers is dauntingly complex. However, in the next section, we present encouraging results of applying computational methods to augment cognition in the sciences, helping to mitigate biases and limitations and enabling researchers to make better use of their powerful creative mechanisms.</p>
<h2>4 TASK-GUIDED RETRIEVAL</h2>
<p>How might we widen and deepen the connection between the outer world of science and researchers' limited cognitive worlds? We see a key bridge and research opportunity with developing tools for scientific task-guided knowledge retrieval. In this section, we discuss our vision and present initial work toward achieving it.</p>
<p>Drawing from discussions in literature on the process of scientific discovery, we enumerate in Table 1 salient scientific tasks and activities, such as problem identification, forming directions, learning, literature search and review, experimentation. These tasks could benefit from augmentation of human capabilities but remain underexplored in computer science. Existing computational technologies for assisting humans in discovering scientific knowledge are underinvested in important aspects of the intricate cognitive processes and goal-oriented contexts involved in scholarly endeavors.</p>
<p>The dominant approach to information retrieval research and systems can be summarized as "relevance first"-focusing on results that answer user queries as accurately as possible. Academic search engines assume users know what queries to explore and how to formulate them. For pinpointed literature search in familiar areas, this assumption may often suffice; but a broad array of other scholarly tasks, such as ideation or learning about a new topic, are very much underserved [9-11, 18, 29]. At the same time, many voices in the information retrieval community have discussed a different, broader view of utility-driven search situated in a wider context of information seeking by users with specific intents and tasks [31]. Here, we adapt ideas and principles from this general paradigm.</p>
<p>We envision methods for task-guided scientific knowledge retrieval: systems that retrieve and synthesize outer knowledge in a manner that directly serves a task-guided utility of a researcher, while taking into consideration the researcher's goals, state of inner knowledge, and preferences.</p>
<p>Consider the tasks in Table 1. For researchers engaged in experimentation or analysis, we envision systems that help users identify experiments and analyses in the literature to guide design choices and decisions. For researchers in early stages of selecting problems to work on, we picture systems that support this decision with information from literature and online discussions, synthesized to obtain estimated impact and feasibility. As part of forming directions to address a problem, systems will help users find inspirations for solutions. Researchers who are learning about a new topic will be provided with retrieved texts and discussions that explain the topic in a manner personally tailored to personal knowledge. Importantly, task-guided knowledge retrieval follows the two desiderata introduced in Section 3; namely, systems should enable users to find knowledge that directly assists them in core research tasks by augmenting their cognitive capacity and mitigating their biases, and computational representations and services should align with salient cognitive aspects of the inner world of researchers.</p>
<h3>4.1 Prototypes of Task-Guided Retrieval</h3>
<p>We present work on initial steps and prototypes, including representative work that we have done and the work of others, framed in alignment with task-guided knowledge retrieval and tasks enumerated in Table 1. The main aim of this brief review is to stimulate discussion in the computer science community on tools for extending human capabilities in the sciences. Existing methods are far from able to realize our vision. For example, we see major challenges in representation and inferences about the inner world of knowledge and preferences, and aligning these with representations and inferences drawn from the outer world knowledge. Today's prototypes are limited examples of our vision, using very rough proxies of inner knowledge and interest based on papers and documents written or read by the user, or in some cases only a set of keywords. We discuss these research challenges and others in Section 5.</p>
<p>Forming Directions. We have developed methods for helping researchers generate new directions. A fundamental pattern in the cognitive process of creativity involves detecting abstract connections across ideas and transferring ideas from one problem to another [36]. Grounded in this cognitive understanding, we have pursued several approaches for stimulating creativity powered by
retrieving outer knowledge. We developed and studied a system named Bridger that connects researchers to peers who inspire novel directions for research [29]. Bridger identifies matches among authors based on commonalities and contrasts, identifying peers who are both relevant and novel-working on similar problems but using very different methods, potentially inspiring new solutions. By doing so, Bridger helps mitigate the cognitive bias of fixation [11].</p>
<p>In this setting, inner knowledge is represented as mentions of problems and methods extracted automatically from a researcher's papers and weighted by term frequency. The outer knowledge being retrieved takes the form of other authors in computer science, following the same representation. For each retrieved author, the system displays salient problems, methods and papers, ranked by measures of relevance to the user. In studies with CS researchers, we found that Bridger dramatically boosted creative search and inspiration over state-of-art neural models employed by the Semantic Scholar search engine, surfacing useful connections across diverse areas; for example, one researcher drew novel connections between the mathematical area of graph theory and their own area of human-centered AI, by exploring a recommended author who applies graph theory to decision making. The studies also surfaced important challenges, discussed in Section 5.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Matching researchers to authors with whom they are unfamiliar, to help in generating directions. Author cards show key problems and methods extracted from their papers.</p>
<p>We have also explored retrieving outer knowledge to enhance the human ability to find opportunities for analogical transfer [3, 8]. Extensive work in cognitive studies has highlighted the human knack for "analogical retrieval" as a central function in creativitybringing together structurally related ideas and adapting them to a task at hand [36]. We developed a search method that enables researchers to search through a database of technological inventions and find mechanisms that can be transferred from distant domains to solve a given problem. Given as input from the user a textual description of an invention, we retrieve ideas (inventions, papers) that have partial structural similarity to the input (e.g., inventions with similar mechanisms), to facilitate discovery of analogical transfer opportunities. We found that the method could significantly boost measures of human creativity in ideation experiments, in which users were asked to formulate new ideas after viewing inspirations retrieved with our approach versus baseline information retrieval methods. For example, a biomechanical engineering lab working on polymer stretching/folding for creating novel structures found useful inspiration in a civil engineering paper on web crippling in steel beams-abstractly related to stretching and folding.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Using an extracted hierarchy of problems to retrieve new perspectives on a focal problem of interest.</p>
<p>Innovation may also involve traversing multiple levels of abstraction around a problem to "break out" of fixation on the details of a specific problem by exploring novel perspectives. Given as input a problem description written by the user (as a proxy summary of the user's inner world of knowledge and purpose), we have pursued mechanisms that can retrieve diverse problem perspectives that are related to the focal problem, with the goal of inspiring new ideas for problem abstraction and reformulation [11] (see Figure 3). Using NLP models to extract mentions of problems, we mine a corpus of technological invention texts to discover problems that often appear together, and use this information to form a hierarchical problem graph that supports automatic traversal of neighboring problems around a focal problem, surfacing novel inspirations to users. In a study of the efficacy of the methods, over $60 \%$ of "inspirations" retrieved this way were found to be useful and novel-a relative boost of $50-60 \%$ over the best-performing baselines. For example, given an input problem of reminding patients to take medication, our system retrieves related problems such as in patient health tracking and alerting devices.</p>
<p>Guiding attention and problem identification. We see great opportunity in developing methods for guiding the attention of researchers to important areas in the space of ideas where there exists less knowledge or certainty [18, 32] (Figure 4). In one direction, we built a search engine that allows users to retrieve outer knowledge in the form of difficulties, uncertainties and hypotheses in the literature. The key goals of this mode of search are to bolster attention to rising and standing challenges of relevance to the user so as to help overall with identification and selection of problems. We performed experiments with participants with diverse research backgrounds, including medical doctors working in a large hospital. Using query topics as a proxy for the inner world of participants' interests, we found the system could dramatically outperform PubMed search, the go-to biomedical search engine, at discovering important and interesting areas of challenges and directions. For example, while searching PubMed for the ACE2 receptor in the context of COVID19 returns well-studied results, the prototype system by contrast focuses on finding statements of uncertainty, open questions and initial hypotheses, like a paper noting the possibility that ACE2 plays a role in liver damage in COVID-19 patients.</p>
<p>Another direction on biases and blindspots considers the longterm effort to identify protein-protein interactions (PPIs). A dataset of the growing graph of confirmed PPIs over decades was constructed and leveraged to identify patterns of scientific attention [32]. A temporal analysis revealed a significant "bias of locality," where explorations of PPIs are launched more frequently from those that were most recently studied, rather than following more general prioritization of exploration. While locality reflects an understandable focus on adjacent and connected problems in the biosciences, the pattern of attention leads to systematic blindspots in large,</p>
<p>Input:
Items of interest (concepts, entities, topics...)
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Suggesting research opportunities for query concepts (e.g., medical topics) by identifying blindspots, gaps in collective knowledge and promising areas for exploration.
widely used PPI databases that are likely unappreciated-further exacerbating attentional biases. The study further demonstrated mechanisms for reprioritizing candidate PPIs based on properties of proteins, and showed how earlier discoveries could be made with use of the debiasing methods. The findings underscore the promise of tools that retrieve existing outer world knowledge to guide attention to worthwhile directions. In this case, the outer knowledge source is a PPI database, and a user-selected sub-graph provides a proxy for inner world knowledge and interests.</p>
<p>Literature search and review. A great body of work on literature search and review has deep relevance to task-guided retrieval in the sciences. In particular, we see great opportunity with building on recent advances in information retrieval to (1) help biomedical researchers with domain-specific representations and (2) enhance scientific search by building new neural models.</p>
<p>Specialized search systems have been developed for the biomedical domain, with the overall vision of harnessing natural language understanding technologies to help researchers discover relevant evidence and expedite the costly process of systematic literature review [27]. For example, Nye et al. [27] build a search and synthesis system based on automated extraction of biomedical treatmentoutcome relations from clinical trial reports. The system is found to assist in identification of drug repurposing opportunities. As another recent example, the SPIKE system enables researchers to extract and retrieve facts from a corpus using an expressive query language with biomedical entity types and new term classes that the user can define interactively [34]. Together, this work underscores the importance of extracting a semantically meaningful representation of outer world knowledge that aligns with core aspects of inner world reasoning by researchers (see Section 5).</p>
<p>In separate work, neural language models built via self-supervision on large corpora of biomedical publications have recently led to performance boosts and new features in literature search systems [39], such as support for natural language queries that provide users with a more natural way to formulate their informational goals. Neural models have also been trained to match abstract discourse aspects of pairs of papers (e.g., sentences referring to methodologies) and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Leveraging medical corpora to enhance the precision of AI models for inference about patient outcomes.
automatically retrieve documents that are aspectually similar [23]. By employing a representation that aligns with scientific reasoning across areas, this method achieves state-of-art results across biomedical and computer science literature.</p>
<p>Experimentation, analysis, and action. Beyond helping researchers via awareness and knowledge, we see great opportunities to use scientific corpora to construct task-centric inferential systems with automated models and tools for assisting with analysis, prediction and decisions. We demonstrate these ideas by casting two different lines of work as cases of task-guided retrieval.
(1) Workflows are multi-step computational pipelines used as part of scientific experimentation for data preparation, analysis and simulation [6]. Technically this includes execution of code scripts, services and tools, querying databases and submitting jobs to the cloud. In the life sciences, in areas such as genomics, there are specialized workflow management systems to help researchers find and use workflows, enabled by a community that creates and publicly shares repositories of workflows with standardised interfaces, metadata and functional annotations of tools and data. As discussed in Gil [6], machine learning algorithms can potentially use these resources to automate workflow construction, learning to retrieve and synthesize data analysis pipelines. In this setting, outer world knowledge takes the form of workflow repositories, from which systems retrieve and synthesize modular building blocks; users' inner world is reflected via analysis objectives and constraints.
(2) In our work on clinical predictions [24], the goal is to enhance prediction of medical outcomes of patients hospitalized in the intensive care unit (ICU), such as in-hospital mortality or prolonged length of stay. Our system, named BEEP (biomedical evidence enhanced prediction), learns to perform predictions by retrieving medical papers that are relevant to each specific ICU patient, and to synthesize this outer knowledge in combination with internal EMR knowledge to form a final prediction. The primary envisaged user is a practice-oriented researcher-a medical doctor, whose inner knowledge is given by a rough proxy in the form of internal clinical notes from which we extract "queries" issued over medical papers. We find BEEP to provide large improvements over state-of-art models that do not use retrieval from the literature. BEEP's output can
be aligned with inner world representations, e.g., matches between patient aspects and related cohorts in papers (see Figure 5).</p>
<p>Learning and understanding. We introduced a system [22] for helping users learn about new concepts by showing definitions grounded in familiar concepts; e.g., a new algorithm is explained as a variant of an algorithm familiar to the user. Cognitive studies have asserted that effective descriptions of a new concept ground it within the network of known concepts. Our system takes as input a list of source concepts reflecting the user's inner knowledge as obtained from papers that they have written or read. When the user seeks a definition of a new target concept, we retrieve outer knowledge in the form of definitions appearing in scientific papers in which the target concept is explained in terms of the source concepts; a neural text generation model then re-writes the text in a structured, template form that relates the target to the source.</p>
<h2>5. OPPORTUNITIES AHEAD</h2>
<p>The challenges of task-guided retrieval in support of researchers frame a host of problems and opportunities. We discuss select challenges and directions (see also Table 2). We begin with an illustrative example, imagining a futuristic system to motivate the discussion.</p>
<h3>5.1. Aspirations</h3>
<p>We envision tools that flow outer world knowledge to researchers based on inferences about their inner world-users' knowledge, past and present goals and difficulties, and the tasks from Table 1 they are engaged in. The systems would use multiple signals for making inferences, including users' papers, data, experiments and communication channels, and also converse with the user to understand needs and suggest solutions, hypotheses and experiments.</p>
<p>We foresee systems powered by rich representations of both inner and outer scientific knowledge. For a given concept, e.g., a certain algorithm or organism, an aspirational system would ingest all papers on the subject to form a multi-faceted representation of concepts as objects with associated properties and functions. Using this representation, the system could assist in literature search and review, enabling expressive queries to outer world information that target abstract aspects like functionalities, mechanisms, behaviors and designs in a manner that transcends field-specific jargon, abstracting away lexical differences that hindered historical search engines (e.g., Google Scholar). To help users learn and understand new concepts they encounter, the system would explain them in relation to other concepts the user already knows. A future system might also assist in automating experimentation, analysis and action and in forming directions, by forming compositions of concepts and predicting the resultant affordances; for example, matching a certain algorithm with a suitable problem based on the algorithm's properties and the problem's requirements, matching an organism with a specific method of measurement or modification, or recombining parts of two devices to form a new device. The system could help identify related problems in the literature, synthesizing from them useful suggestions for problem reformulations. Considering the huge combinatorial space of potential suggestions, a system could assist in prioritization using estimated measures of interestingness, feasibility and value by synthesizing historical and current signals in literature, online discussions and knowledge bases.</p>
<p>Envisioned systems would be designed as human-centric, focusing on the individual researcher. The systems would enable users to convey preferences, goals and interests, and mediate the presentation of suggested directions and problem solutions based on personal prior knowledge, proposing concrete new directions grounded in representations that researchers can follow, and assisting users in reading complex retrieved texts by editing their language to conform with concepts that users are familiar with.</p>
<h3>5.2 Research Directions</h3>
<p>While we have witnessed remarkable strides in AI, the journey towards actualizing our vision requires further advancement. Envisioning such capabilities, however, can serve as a compass for directing research endeavors. An encouraging development can be seen in the recent developments with large language models, which have demonstrated surprising capabilities with interpreting and generating complex texts and tackling technical tasks. The proficiencies demonstrated by these models instills confidence that many of the possibilities that we discussed are attainable. We now elaborate on challenges and directions ahead, including limitations in representing scientific knowledge and making inferences about the inner worlds of researchers (see Table 2).</p>
<p>Task-aligned representations and scientific NLP. Paul Thagard writes: "thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures". We seek representations that can be aligned with human thinking-for insight-building, decision making and communication. Can we go beyond textual representation toward representations that support such cognitive processes?</p>
<p>The quest for a universal schema representing scientific ideas goes back hundreds of years. Gottfried Leibniz and René Descartes were intrigued by the prospects of a universal codification of knowledge. Leibniz proposed the characteristica universalis, a hypothesized formal language of ideas enabling inferences with algebraic operators. While such a representation is not within reach, envisioning its existence-and how to even roughly approximate it-points to important research directions. One exciting direction is obtaining representations that support a "computational algebra of ideas"e.g., modeling compositions of concepts and the affordances that would be formed as a result. Early work on learning vector representations of natural language concepts supported rudimentary forms of addition, subtraction, and analogy (e.g., the Word2vec model).</p>
<p>Recently, large language models (LLM) [28] have made striking progress in generating new content and coherently combining concepts. Emerging evidence on GPT-4's ability to reason not only in unstructured language but also with logical structures grounded in code, suggests strong potential for generating novel ideas via compositionality and relational reasoning [1]. Our early experiments with GPT-4 have revealed a constellation of promising abilities to assist with the scientific process, such as formulating hypotheses, recommending future research directions, and critiquing studies. Equipped with training and retrieval with access to millions of scientific papers, descendants of today's models may have an ability to synthesize original scientific concepts with the in-depth technical detail at a level reported in high-quality scientific papers. We see great opportunity ahead to leverage LLMs to augment human scientific reasoning along the lines described in this paper.</p>
<p>One limitation with LLMs is that representations learned by these models are currently far from understood and lack "hooks" for control and interpretability, which are important in human-AI collaboration. In line with our focus on grounding representations of outer world knowledge with inner world cognitive aspects, we have pursued methods that "reverse engineer" scientific papers to automatically extract, using NLP, structured representations that balance three desiderata:
(1) Semantically meaningful representations, aligned with a salient task from the tasks in Table 1, grounded in cognitive research to guide us toward useful structures.
(2) Representations with sufficient level of abstraction to generalize across areas and topics.
(3) Representations expressive enough for direct utility in helping researchers as measured in human studies.</p>
<p>For example, we have extracted representations of causal mechanisms and hierarchical graphs of functional relationships. This kind of decomposition of ideas has enabled us to perform basic analogical inference in the space of technological and scientific ideas, helping researchers discover inspirations (see Section 4). However, many richer structures should be explored (e.g., of experimentation processes and methodologies, to enable tasks in Table 1).</p>
<p>A central challenge is that current models' extraction accuracy is limited, and the diversity of scientific language leads to problems in generalization and normalization of terms and concepts. We have pursued construction of new datasets, models and evaluations for identifying similarity between concepts and aspects across papers [2, 23], with fundamental problems in resolving diversity, ambiguity and hierarchy of language. As our results have highlighted, models tend to focus on surface-level lexical patterns, rather than deeper semantic relationships. Generally, substantial advances are needed to handle challenges posed by scientific documents. We require NLP models with full-document understanding, not only of text but of tables, equations, figures, and reference links. Open access corpora (e.g., S2ORC [20]) provide a foundation to address this challenge.</p>
<p>New modes of writing and reading. Perhaps the way we write can be dramatically different, using machine-actionable representations? Beyond reporting and documentation, writing represents a channel between the inner and outer worlds, forcing us to communicate ideas in concrete language; this process often begets new questions and perspectives. Can systems accompany different phases of writing, suggesting new ideas? In parallel, there is the task of reading what others have written; a recent interactive PDF reader offers, for example, customized concept definitions [7]. We imagine a future where every reader will see a different form of the same paper, re-written to align with readers' knowledge; e.g., our personalized concept definitions system [22] (§4) will insert new wording and explanations grounded in readers' knowledge.</p>
<p>Internal world of researchers. Grounding new concepts in readers' knowledge, suggests a wider and highly challenging problem. How can we enable researchers to specify their knowledge and preferences to direct systems to carry out tasks? Directly querying for these aspects burdens the researcher and may be prone to reporting biases. Digital traces present an opportunity for automatically estimating a researcher's knowledge, objectives, needs and interests-based on data. We are interested in using researchers'</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Challenge</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task-aligned representations, scientific NLP</td>
<td style="text-align: left;">How can we automatically and accurately extract conceptual representations of scientific <br> knowledge, that are aligned with tasks that comprise the endeavor of science (Table 1)? How <br> can we build NLP models that understand full scientific papers?</td>
</tr>
<tr>
<td style="text-align: left;">Computational algebra of ideas</td>
<td style="text-align: left;">Can we build representations of scientific knowledge that support composition of ideas? e.g., <br> inferring the result of recombining two concepts.</td>
</tr>
<tr>
<td style="text-align: left;">Identifying conceptual relationships across <br> literature</td>
<td style="text-align: left;">How do we detect important relationships across scientific ideas, across related discussions <br> in different communities and areas? How can we resolve challenges of diversity, ambiguity, <br> and multiple levels of detail in scientific language?</td>
</tr>
<tr>
<td style="text-align: left;">Estimation of personal knowledge</td>
<td style="text-align: left;">How can we estimate the knowledge of a given researcher? What are useful, practical models <br> of this knowledge? What concepts does a researcher know, which of their aspects, and to <br> what technical extent? How do we account for latent knowledge?</td>
</tr>
<tr>
<td style="text-align: left;">Addressing gaps in knowledge</td>
<td style="text-align: left;">Given an estimated model of a researcher's knowledge, and given a specific task in Table 1, <br> what new knowledge would be useful for the task at hand?</td>
</tr>
<tr>
<td style="text-align: left;">Estimation of preferences, goals, interests</td>
<td style="text-align: left;">How can we estimate key latent preferences, interests and subjective utilities of researchers? <br> Using information in papers and discussions to infer factors behind researchers' choices.</td>
</tr>
<tr>
<td style="text-align: left;">Prediction and prioritization</td>
<td style="text-align: left;">How might we identify promising sparse/unexplored areas in large "spaces of ideas" and <br> prioritize directions that are novel, plausible and valuable?</td>
</tr>
<tr>
<td style="text-align: left;">Developing new representations for learning <br> and communicating</td>
<td style="text-align: left;">Might the way we read and write papers change to be more effective? Might we communicate <br> with machine-actionable, interlinked representations of scholarly knowledge. Might we <br> create personalized "living" documents that tailor their content to readers' backgrounds.</td>
</tr>
</tbody>
</table>
<p>Table 2: Directions with formulating and leveraging computational representations of scientific knowledge.
papers to estimate what concepts users know and to what extent. We envision mixed-initiative interfaces [12] in which approximations of the inner world are presented to researchers and refined in human-machine collaboration, to identify and fill personal gaps in knowledge for a specific task. Representations of interest and preference are central in web commerce based on user activity histories. We are encouraged by results highlighting the feasibility of rich user models, e.g., in search personalization [31, 35] and dynamic inferences [14]. Paul Samuelson wrote of "revealed preferences"preferences revealed indirectly by the economic price people are willing to pay; while not equivalent, researchers' digital traces may reveal preferences, e.g., by working on one problem and not another.</p>
<p>Prediction and prioritization of directions. Whenever we decide to work on a research direction, we are implicitly making a prediction about an area in "idea space". Can automated systems help make these predictions? This involves identifying promising areas and generating directions-hypotheses, ideas-in either natural or structured language, under constraints on users' background knowledge; directions should be ranked by estimated likelihood (feasibility, plausibility), utility and novelty. Despite the great challenges involved, we are encouraged by advances in models trained for predicting specific targets (e.g., protein structures [15]); we see potential in building on these advances as part of our wider agenda that considers the inner world of cognitive aspects and tasks, and the outer world outside the context of a narrow dataset.</p>
<p>Pursuing challenges of translation. Finally, we note challenges for introducing new technologies into scientific workflows. In the context of systems for discovery, researchers interviewed in our studies [29] reported being limited in time and resources, making them less likely to enter new areas and learn unfamiliar concepts, preventing them discovering potentially promising ideas. More broadly, the sociotechnical environment in which AI models
are deployed has critical impact on their success [13, 21]. A pertinent example comes via reports on difficulties with translating IBM's Watson Health systems into practice. The vision of the effort included systems providing insights about patients by mining research papers to suggest, e.g., therapies or diagnostics [21]. A prototype system faced difficulties ranging from data processing and availability problems to deeper perceived gaps between the system's understanding of literature and that of physicians [37]. Challenges such as these are fundamental to the fielding of new applications not only in healthcare but in any setting where humans are required to interact with AI systems [40]. While issues such as data quality and privacy are orthogonal to our agenda, we see directions in modeling of human needs and limitations to inform the design of human-AI experiences within scientific workflows.</p>
<h2>6 SUMMARY</h2>
<p>As the terrain of science widens at a fast pace, researchers are constrained by the limits of human cognition, and lack principled methods to follow developments, guide attention, and formulate and prioritize directions. For the first time in history, essentially all of scientific knowledge and discourse has moved into the digital space. At the time of this writing, dramatic advances in AI with large language models are taking place at breathtaking speed. These shifts present tremendous opportunities for leveraging scientific corpora as databases from which solutions, insights, and inspirations can be gleaned. We see opportunity ahead for systems that can address the imbalance between the treasure trove of scholarly information and researchers' limited ability to reach into it, harnessing humankind's collective knowledge to revolutionize the scientific process. Numerous challenges stand in the way of the vision we have laid out. However, even small steps forward will unlock vast opportunities for making advances at the frontiers of science.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We thank the members of the Semantic Scholar team for stimulating discussions. Projects were supported by NSF Convergence Accelerator Grant 2132318, NSF RAPID grant 2040196, and ONR grant N00014-18-1-2193.</p>
<h2>REFERENCES</h2>
<p>[1] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.
[2] Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug Downey, and Tom Hope. Scico: Hierarchical cross-document coreference for scientific concepts. Automated Knowledge Base Construction (AKBC) 2021, 2021.
[3] Joel Chan, Joseph Chee Chang, Tom Hope, Dafna Shahaf, and Aniket Kittur. Solvent: A mixed initiative system for finding analogies between research papers. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW):1-21, 2018.
[4] Johan SG Chu and James A Evans. Slowed canonical progress in large fields of science. Proceedings of the National Academy of Sciences, 118(41), 2021.
[5] Cristina García-Villar. A critical review on altmetrics: can we measure the social impact factor? Insights into Imaging, 12(1):1-10, 2021.
[6] Yolanda Gil. Will AI write scientific papers in the future? AI Magazine, 2022.
[7] Andrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, Daniel S. Weld, and Marti A. Hearst. Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, 2021.
[8] Tom Hope, Joel Chan, Aniket Kittur, and Dafna Shahaf. Accelerating innovation through analogy mining. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017.
[9] Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric Horvitz, Daniel S Weld, Marti A Hearst, and Jevin West. Scisight: Combining faceted navigation and research group detection for covid-19 exploratory scientific search. In EMNLP, 2020.
[10] Tom Hope, Asla Amini, David Wadden, Madeleine van Zuylen, Sravanthi Parasa, Eric Horvitz, Daniel S Weld, Roy Schwartz, and Hannaneh Hajishirzi. Extracting a knowledge base of mechanisms from covid-19 papers. In NAACL, 2021.
[11] Tom Hope, Ronen Tamari, Hyeomu Kang, Daniel Hershcovich, Joel Chan, Aniket Kittur, and Dafna Shahaf. Scaling creative inspiration with fine-grained functional facets of product ideas. In CHI, 2022.
[12] Eric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 159-166, 1999.
[13] Eric Horvitz. The future of biomedical informatics: Bottlenecks and opportunities. In Biomedical Informatics: Computer Applications in Health Care and Biomedicine, E.H. Shortliffe, J.J. Cimino, et. al. Springer, 2021.
[14] Eric J Horvitz, John S Breese, David Heckerman, David Hovel, and Koos Rommelse. The Lumiere project: Bayesian user modeling for inferring the goals and needs of software users. In Proceedings of the Conference on Uncertainty in AI, pages 256-263, 1998.
[15] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Židek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
[16] Ross D King, Kenneth E Whelan, Ffion M Jones, Philip GK Reiser, Christopher H Bryant, Stephen H Muggleton, Douglas B Kell, and Stephen G Oliver. Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427(6971):247-252, 2004.
[17] Thomas S Kuhn. The structure of scientific revolutions, volume 111. Chicago University of Chicago Press, 1970.
[18] D Lahav, JS Falcon, B Kuehl, S Johnson, S Parasa, N Shomron, DH Chau, D Yang, E Horvitz, DS Weld, and T Hope. A search engine for discovery of scientific challenges and directions. In AAAL 2022.
[19] Pat Langley, Herbert A Simon, Gary L Bradshaw, and Jan M Zytkow. Scientific discovery: Computational explorations of the creative processes. MIT press, 1987.
[20] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of ACL, 2020.
[21] Steve Lohr. What ever happened to ibm's watson. The New York Times, 16(7):21, 2021.
[22] Sonia Murthy, Kyle Lo, Chandra Bhagavatula, Daniel King, Bailey Kuehl, Sophie Johnson, Jonathan Borchardt, Daniel S. Weld, Tom Hope, and Doug Downey. Accord: A multi-document approach to generating diverse descriptions of scientific concepts. In EMNLP, 2022.
[23] Sheshera Mysore, Arman Cohan, and Tom Hope. Multi-vector models with textual guidance for fine-grained scientific document similarity. NAACL, 2022.
[24] Aakanksha Naik, Sravanthi Parasa, Sergey Feldman, Lucy Lu Wang, and Tom Hope. Literature-augmented clinical outcome prediction. NAACL, 2022.
[25] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of GPT-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
[26] Regina Nuzzo et al. How scientists fool themselves-and how they can stop. Nature, 526(7572):182-185, 2015.
[27] Benjamin Nye, Jay DeYoung, Eric Lehman, Ani Nenkova, Iain J Marshall, and Byron C Wallace. Understanding clinical trial reports: Extracting medical entities and their relations. In AMIA Annual Symposium Proceedings, volume 2021, page 485. American Medical Informatics Association, 2021.
[28] OpenAI. Gpt-4 technical report, 2023.
[29] Jason Portenoy, Marissa Radensky, Jevin West, Eric Horvitz, Daniel Weld, and Tom Hope. Bridger: Toward bursting scientific filter bubbles and boosting innovation via novel author discovery. CHI, 2022.
[30] Edward O Pyser-Knapp, Jed W Pitera, Peter WJ Staar, Seiji Takeda, Teodoro Laino, Daniel P Sanders, James Sexton, John R Smith, and Alessandro Curioni. Accelerating materials discovery using artificial intelligence, high performance computing and robotics. npj Computational Materials, 8(1):1-9, 2022.
[31] Chirag Shah and Emily M Bender. Situating search. In ACM SIGIR Conference on Human Information Interaction and Retrieval, pages 221-232, 2022.
[32] Uriel Singer, Kira Radinsky, and Eric Horvitz. On biases of attention in scientific discovery. Bioinformatics, 12 2020. URL https://doi.org/10.1093/bioinformatics/ btaa1036.
[33] D. R. Swanson. Fish oil, raynaud's syndrome, and undiscovered public knowledge. Perspectives in Biology and Medicine, 30(1):7-18, 1986.
[34] Hillel Taub Tabib, Micali Shl aim, Shoval Sadde, Dan Lahav, Matan Eyal, Yaara Cohen, and Yoav Goldberg. Interactive extractive search over biomedical corpora. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pages 28-37, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1:2020.biomlp-1.3. URL https://aclanthology.org/2020.biomlp-1.3.
[35] Jaime Teevan, Susan T Dumais, and Eric Horvitz. Personalizing search via automated analysis of interests and activities. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 449-456, 2005.
[36] Paul Thagard. The cognitive science of science: Explanation, discovery, and conceptual change. Mit Press, 2012.
[37] Aish Thamba and Richard B Gunderman. For watson, solving cancer wasn't so elementary: Prospects for artificial intelligence in radiology. Academic Radiology, 29(2):312-314, 2022.
[38] Daril A Vilhena, Jacob G Foster, Martin Rosvall, Jevin D West, James Evans, and Carl T Bergstrom. Finding cultural holes: How structure and culture diverge in networks of scholarly communication. Sociological Science, 1:221, 2014.
[39] Yu Wang, Jinchao Li, Tristan Naumann, Chenyan Xiong, Hao Cheng, Robert Tinn, Cliff Wong, Naoto Usuyama, Richard Rogahn, Zhihong Shen, et al. Domainspecific pretraining for vertical search: Case study on biomedical literature. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining, pages 3717-3725, 2021.
[40] Daniel S Weld and Gagan Bansal. The challenge of crafting intelligible intelligence. Communications of the ACM, 62(6):70-79, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
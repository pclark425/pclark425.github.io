<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5832 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5832</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5832</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-f4df78183261538e718066331898ee5cad7cad05</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f4df78183261538e718066331898ee5cad7cad05" target="_blank">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper shows that ground truth demonstrations are in fact not required and that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of the label space, the distribution of the input text, and the overall format of the sequence.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5832.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5832.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-label demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrations with Random Labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replacing gold labels in k-shot demonstrations with randomly sampled labels (uniform over label set) to test whether correct input-label pairing is necessary for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Davinci), fairseq 13B, GPT-J (6B), GPT-2 Large (774M), fairseq 6.7B, MetaICL (774M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B, 13B, 6B, 774M, 6.7B, 774M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice (26 datasets: sentiment, paraphrase, NLI, hate speech, QA, sentence completion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard NLP classification and multiple-choice benchmarks (e.g., MRPC, RTE, Tweet_eval-hate, OpenBookQA, CommonsenseQA, COPA), evaluated in few-shot (k=16 default) in-context setting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>k-shot demonstrations where each demonstration is an (input, label) pair; labels replaced by random labels sampled uniformly from the task label set (keeping same demonstration format and input distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Gold-label demonstrations (standard k-shot) and No-demonstrations (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Relative performance: Using gold demonstrations gives large gains over no-demonstrations; using random-label demonstrations results in only marginal degradation compared to gold. Reported aggregate drops across models in the paper: 0–5% absolute; average ~2.6% absolute for classification, ~1.7% absolute for multi-choice (macro-F1 for classification, accuracy for multi-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to gold-label demos: drop typically 0–5% absolute; compared to no demonstrations: random-label demos still significantly better than no demonstrations for most models/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>0–5% absolute drop (aggregate); classification avg ≈2.6% absolute, multi-choice avg ≈1.7% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>minor reduction (i.e., random labels barely hurt performance)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The model does not primarily use the provided input-label correspondences in demonstrations; instead demonstrations provide signals about the label space, the input distribution, and the overall sequence format which the model exploits. Meta-trained models (e.g., MetaICL) especially tend to ignore correct pairings and rely on simpler aspects (format, label/input distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>MetaICL shows almost no degradation (0.1–0.9% absolute) when labels are randomized; some dataset-model pairs do show larger gaps (up to ≈14% in an extreme case for financial_phrasebank with GPT-J).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5832.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OOD-input demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrations with Out-of-Distribution (OOD) Inputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace example inputs in demonstrations with sentences sampled from an external corpus (CC-News) to test the importance of the input distribution in demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Channel MetaICL, Direct GPT-J, Channel GPT-J, others evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M (MetaICL), 6B (GPT-J), 13B (fairseq), 175B (GPT-3) etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice (subset evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>k-shot in-context evaluation where demonstration inputs are replaced by OOD sentences sampled from a news corpus (matched by length) while labels/format kept the same.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>k-shot demonstrations with OOD inputs + labels (keeping same pair format), compared to in-distribution input demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>In-distribution demonstrations (standard k-shot), No-demonstrations baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Substantial absolute drops reported for several models: 3–16% absolute decrease in performance on classification and multi-choice for Channel MetaICL, Direct GPT-J, Channel GPT-J; in a case (Direct GPT-J, multi-choice) performance became worse than no demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to in-distribution demonstrations, OOD-input demos significantly reduce performance (3–16% absolute); sometimes OOD demos < no demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>3–16% absolute performance drop (model- and dataset-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In-distribution demonstration inputs help by making the test-time conditioning closer to the distributions seen during pretraining; conditioning on similar input text makes the task resemble language modeling on familiar inputs, which the LM exploits. The input distribution in demonstrations is therefore an important signal.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Direct MetaICL was an exception and showed much smaller sensitivity to OOD inputs, likely due to meta-training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5832.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-English-labels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrations with Random English Word Labels (Altered Label Space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace demonstration labels with random English words (label space altered) while maintaining demonstration input distribution and paired format, to isolate effect of label vocabulary/space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Direct models (GPT-J, fairseq, GPT-2, GPT-3) and Channel models (MetaICL channel, GPT-J channel)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 13B, 774M, 175B (varies by model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>k-shot demonstrations where labels are replaced by random English words drawn from a large wordlist, keeping inputs and input-label pairing format intact.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>k-shot (input, label) pairs where labels are random words (label set size equal to original #labels), compared to original label set or random labels sampled from original label space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Gold labels; random labels sampled from original label space (uniform)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Direct models: large performance drop when label space replaced by random English words (≈5–16% absolute). Channel models: negligible drop (≈0–2% absolute) and sometimes slight increases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Direct models much more sensitive to removal of the original label space; channel models largely unaffected.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Direct: ~5–16% absolute drop; Channel: ~0–2% absolute change</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced for direct models; no meaningful effect for channel models</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Direct models must generate labels, so knowledge of the expected label vocabulary/distribution provides a strong cue; removing that (replacing with arbitrary words) hinders generation. Channel models condition on labels and score inputs given labels, so they are less dependent on the semantic content of the label tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some channel models showed tiny improvements in some settings when label space was replaced, suggesting model- and dataset-specific behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5832.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>No-labels (inputs-only, no pairing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrations Without Labels (Inputs Only — format removed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Provide demonstrations as a concatenation of example inputs only (no labels), i.e., remove the input-label pairing format to test whether the pairing format itself is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaICL, GPT-J, GPT-2, fairseq models, GPT-3 (varied across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M, 6B, 13B, 175B etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>k-shot prompt variant where demonstrations consist only of example inputs concatenated, with no labels; compares effect of removing the pairing format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Inputs-only demonstrations (no labels), compared to paired (input,label) demonstrations and labels-only demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Paired (input,label) demonstrations (standard), Labels-only demonstrations, No-demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Removing the pairing format generally yields performance close to or worse than the no-demonstrations baseline (i.e., the benefits of demonstrations largely disappear when the format is removed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paired demonstrations >> inputs-only (no labels) ≈ no-demonstrations; keeping the pairing format is crucial for retaining gains.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not quantified as a single number; described as 'close to or worse than no demonstrations' in aggregate (substantial loss relative to paired demos)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (format removal eliminates most gains)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The model uses the input-label pairing format as an explicit instruction to 'complete' or mimic the demonstrated mapping/format at test time; removing the pairing removes that signaling and thus the model cannot exploit format-based cues.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5832.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Labels-only demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrations Containing Labels Only (no input text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Condition the LM only on a concatenation of labels (y1 ... yk) without input examples, isolating the contribution of the label set/format absent input distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaICL, GPT-J, other direct and channel models evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Various; 774M to 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>k-shot variant where demonstrations are only the labels (no corresponding input text), testing whether seeing labels alone in the paired format provides signal.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Labels-only sequence conditioning, compared to paired demonstrations and inputs-only/no-label variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Paired (input,label) demonstrations; Inputs-only demonstrations; No-demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Labels-only (no input) is generally close to or worse than no demonstrations; however, when the label-only sequence is embedded in the paired format (i.e., labels randomly paired with inputs or placeholders to preserve pairing format), models can retain a substantial fraction of k-shot gains (varies by model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Naively providing labels-only (no format) does not help, but preserving the pairing format while using only labels (e.g., pairing random inputs with the true label set) can retain up to ~75–95% of improvements in some model/task combinations (reported examples: Direct MetaICL retained 95% of gains in classification when using inputs or label set with correct format).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>When format kept, retention of improvements ranges up to ~95% (varies by model/task); without format, performance ≈ no-demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent: labels-only without pairing yields no effect or reduced performance; labels embedded in pairing format can preserve much of the benefit</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The label set (label space) is a key component of demonstration information, but labels are only useful if presented in a format that cues the model to apply them to the test input (i.e., the input-label pairing format).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5832.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demo-size (k) effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Number of Demonstrations (k)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study sensitivity of in-context performance to the number of demonstration examples k (evaluated various k including 4, 8, 16).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models (best under 13B for each category reported: Channel MetaICL and Direct GPT-J used as examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>examples: MetaICL 774M, GPT-J 6B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>k-shot evaluation comparing k ∈ {4, 8, 16, ...} and measuring gains vs no-demonstrations and robustness to random vs gold labels across k.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Standard k-shot paired demonstrations; compare performance as k varies and when labels are random vs gold.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different k values; gold-label vs random-label demonstrations at each k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Demonstrations substantially improve performance even at small k (k=4); performance increases little beyond k ≥ 8; performance drop from gold to random labels consistent across k and small (0.8–1.6% absolute), with exception at k=4 showing higher variance (up to 4.4% in one setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Marginal returns for larger k (≥8); small effect size for swapping gold->random labels across k (≈0.8–1.6% abs).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Gold->random label drop across k: ~0.8–1.6% absolute (exception: 4.4% at k=4 in some cases)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>minor reduction when labels randomized; diminishing returns when increasing k beyond ~8</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Additional demonstration examples mainly help if they provide supervision of input-label correspondence; because models already exploit simpler aspects (format, label space, input distribution), gains saturate quickly and extra examples do not strongly improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Variance at small k can cause larger drops; in some models/datasets larger k may still offer gains (dataset-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5832.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template variants (minimal vs manual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minimal vs Manual Prompt Templates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between minimal, template-free input formatting and hand-crafted dataset-specific manual templates to evaluate sensitivity to prompt wording/phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various models including GPT-J, MetaICL, GPT-2, GPT-3 variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (774M to 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate in-context performance using 'minimal templates' (simple concatenations) versus 'manual templates' (dataset-specific phrasing taken from prior work) for forming input sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Minimal formatting by default; manual templates used as an alternative in an ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Minimal template vs manual ds-specific templates</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The key trend (random labels barely hurt) holds with both minimal and manual templates; manual templates do not consistently outperform minimal templates—results vary by dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No consistent improvement from manual templates vs minimal; both preserve overall trends regarding label/random-label effects.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not reported as a single aggregate number; effect varies by dataset/model and is not consistently positive for manual templates.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no consistent improvement (mixed/no effect)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Template wording/phrasing can change surface format but does not overturn the central findings: demonstrations primarily provide label space, input-distribution and format cues; thus some manual phrasings may help in specific cases but are not universally better.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5832.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct vs Channel sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differences Between Direct and Channel Inference Methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of two inference methods: direct (generate label given context) and channel (score P(x|y)*P(y)) to see how prompt/format variations affect models differently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Direct and Channel variants applied to GPT-J, MetaICL, GPT-2, fairseq, GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various: 774M–175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how sensitivity to label-space and input-distribution modifications differs between direct generation vs channel scoring approaches under the same demonstration formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same paired demonstrations, but inference performed via direct generation or channel method (flip x and y for channel).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct vs Channel inference on identical demonstration variants (random labels, random-English-labels, OOD inputs, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Direct models are more sensitive to removal of the original label space (5–16% drop when labels replaced by random English words), whereas channel models show little sensitivity to label-space changes (0–2% drop); channel models are more sensitive to input-distribution changes (OOD inputs cause 3–16% drops for some channel models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Direct: large sensitivity to label-space changes; Channel: larger sensitivity to input-distribution changes; numerical ranges reported in other entries.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Label-space change: Direct ~5–16% drop, Channel ~0–2% change. Input-distribution change (OOD): Channel & some direct models 3–16% drop.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-effect depends on inference method: direct more affected by label vocabulary; channel more affected by input distribution</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because direct models must generate output tokens, they rely on the label vocabulary cue; channel models score inputs given labels and so benefit from accurate conditioning on input distribution. This reflects which side of the conditional the model exploits more.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5832.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-training (MetaICL) impact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Meta-training with In-Context Learning Objective (MetaICL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Investigate how meta-training across supervised datasets (MetaICL) affects which aspects of demonstrations the model exploits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaICL (initialized from GPT-2 Large, meta-trained), compared to same-architecture non-meta-trained models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice (same suite)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Meta-trained model evaluated on few-shot demonstration variants to see whether meta-training changes sensitivity to label correctness, input distribution, label space, and format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>k-shot paired demonstrations under variants: random labels, OOD inputs, random-English-labels, no labels, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>MetaICL (meta-trained) vs other non-meta-trained LMs (e.g., GPT-2 Large baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MetaICL displays exaggerated trends: almost zero influence of input-label mapping and input distribution in some settings; near-zero degradation when labels randomized (0.1–0.9% absolute). MetaICL more strongly exploits format and label-space cues when available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>MetaICL shows smaller sensitivity to label correctness than other models; retains more benefit from format-only signals.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Gold->random label drop in MetaICL: ~0.1–0.9% absolute (very small); format-preserving manipulations retain large fraction (e.g., 82–95% of gains in examples).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format exploitation amplified; ground-truth label/input mapping matters even less</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Meta-training encourages models to exploit simpler, surface-level aspects of demonstrations (format, label/input distributions) because those aspects are easier to generalize across many tasks; thus meta-trained models tend to ignore harder signals like fine-grained input-label correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5832.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5832.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Negative/control variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Control Variants That Break Format (constant label/test-input repeated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Additional negative variants: use constant label for all demos ('answer') or replace all demonstration inputs with the test input, each paired with random labels; both change the sequence format and are used as counterexamples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various evaluated models (GPT-J, MetaICL, others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and Multi-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ablations where demonstrations are altered in ways that change the format (constant label token repeated or identical inputs repeated) to test whether format integrity matters.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Constant-label demonstrations (every label token = 'answer'); test-input-repeated demonstrations (each demo uses the test input paired with random labels).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard paired demonstrations and other ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Both variants performed consistently worse than other demonstration variants (worse than random-English-labels or OOD inputs), suggesting format perturbations that make separators ambiguous remove benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Constant-label and test-input-repeated generally underperform other ablations and often do worse than simpler random-label demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not summarized as a single number; described qualitatively as 'consistently worse' than other methods preserving format or distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (substantially worse)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>These manipulations changed the perceived format (e.g., constant label acts as a separator token) and thus destroy the format cue the model uses, demonstrating that not all label/text substitutions are neutral—their impact depends on whether they preserve the demonstration's format/structure.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>MetaICL: Learning to learn in context <em>(Rating: 2)</em></li>
                <li>Do prompt-based models really understand the meaning of their prompts? <em>(Rating: 2)</em></li>
                <li>Ground-truth labels matter: A deeper look into input-label demonstrations <em>(Rating: 2)</em></li>
                <li>An explanation of in-context learning as implicit bayesian inference <em>(Rating: 1)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 1)</em></li>
                <li>Text and patterns: For effective chain of thought, it takes two to tango <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5832",
    "paper_id": "paper-f4df78183261538e718066331898ee5cad7cad05",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Random-label demonstrations",
            "name_full": "Demonstrations with Random Labels",
            "brief_description": "Replacing gold labels in k-shot demonstrations with randomly sampled labels (uniform over label set) to test whether correct input-label pairing is necessary for in-context learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Davinci), fairseq 13B, GPT-J (6B), GPT-2 Large (774M), fairseq 6.7B, MetaICL (774M)",
            "model_size": "175B, 13B, 6B, 774M, 6.7B, 774M",
            "task_name": "Classification and Multi-choice (26 datasets: sentiment, paraphrase, NLI, hate speech, QA, sentence completion)",
            "task_description": "Standard NLP classification and multiple-choice benchmarks (e.g., MRPC, RTE, Tweet_eval-hate, OpenBookQA, CommonsenseQA, COPA), evaluated in few-shot (k=16 default) in-context setting.",
            "problem_format": "k-shot demonstrations where each demonstration is an (input, label) pair; labels replaced by random labels sampled uniformly from the task label set (keeping same demonstration format and input distribution).",
            "comparison_format": "Gold-label demonstrations (standard k-shot) and No-demonstrations (zero-shot).",
            "performance": "Relative performance: Using gold demonstrations gives large gains over no-demonstrations; using random-label demonstrations results in only marginal degradation compared to gold. Reported aggregate drops across models in the paper: 0–5% absolute; average ~2.6% absolute for classification, ~1.7% absolute for multi-choice (macro-F1 for classification, accuracy for multi-choice).",
            "performance_comparison": "Compared to gold-label demos: drop typically 0–5% absolute; compared to no demonstrations: random-label demos still significantly better than no demonstrations for most models/datasets.",
            "format_effect_size": "0–5% absolute drop (aggregate); classification avg ≈2.6% absolute, multi-choice avg ≈1.7% absolute",
            "format_effect_direction": "minor reduction (i.e., random labels barely hurt performance)",
            "explanation_or_hypothesis": "The model does not primarily use the provided input-label correspondences in demonstrations; instead demonstrations provide signals about the label space, the input distribution, and the overall sequence format which the model exploits. Meta-trained models (e.g., MetaICL) especially tend to ignore correct pairings and rely on simpler aspects (format, label/input distributions).",
            "counterexample_or_null_result": "MetaICL shows almost no degradation (0.1–0.9% absolute) when labels are randomized; some dataset-model pairs do show larger gaps (up to ≈14% in an extreme case for financial_phrasebank with GPT-J).",
            "uuid": "e5832.0",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "OOD-input demonstrations",
            "name_full": "Demonstrations with Out-of-Distribution (OOD) Inputs",
            "brief_description": "Replace example inputs in demonstrations with sentences sampled from an external corpus (CC-News) to test the importance of the input distribution in demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Channel MetaICL, Direct GPT-J, Channel GPT-J, others evaluated",
            "model_size": "774M (MetaICL), 6B (GPT-J), 13B (fairseq), 175B (GPT-3) etc.",
            "task_name": "Classification and Multi-choice (subset evaluated)",
            "task_description": "k-shot in-context evaluation where demonstration inputs are replaced by OOD sentences sampled from a news corpus (matched by length) while labels/format kept the same.",
            "problem_format": "k-shot demonstrations with OOD inputs + labels (keeping same pair format), compared to in-distribution input demonstrations.",
            "comparison_format": "In-distribution demonstrations (standard k-shot), No-demonstrations baseline.",
            "performance": "Substantial absolute drops reported for several models: 3–16% absolute decrease in performance on classification and multi-choice for Channel MetaICL, Direct GPT-J, Channel GPT-J; in a case (Direct GPT-J, multi-choice) performance became worse than no demonstrations.",
            "performance_comparison": "Compared to in-distribution demonstrations, OOD-input demos significantly reduce performance (3–16% absolute); sometimes OOD demos &lt; no demonstrations.",
            "format_effect_size": "3–16% absolute performance drop (model- and dataset-dependent)",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "In-distribution demonstration inputs help by making the test-time conditioning closer to the distributions seen during pretraining; conditioning on similar input text makes the task resemble language modeling on familiar inputs, which the LM exploits. The input distribution in demonstrations is therefore an important signal.",
            "counterexample_or_null_result": "Direct MetaICL was an exception and showed much smaller sensitivity to OOD inputs, likely due to meta-training.",
            "uuid": "e5832.1",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Random-English-labels",
            "name_full": "Demonstrations with Random English Word Labels (Altered Label Space)",
            "brief_description": "Replace demonstration labels with random English words (label space altered) while maintaining demonstration input distribution and paired format, to isolate effect of label vocabulary/space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Direct models (GPT-J, fairseq, GPT-2, GPT-3) and Channel models (MetaICL channel, GPT-J channel)",
            "model_size": "6B, 13B, 774M, 175B (varies by model)",
            "task_name": "Classification and Multi-choice",
            "task_description": "k-shot demonstrations where labels are replaced by random English words drawn from a large wordlist, keeping inputs and input-label pairing format intact.",
            "problem_format": "k-shot (input, label) pairs where labels are random words (label set size equal to original #labels), compared to original label set or random labels sampled from original label space.",
            "comparison_format": "Gold labels; random labels sampled from original label space (uniform)",
            "performance": "Direct models: large performance drop when label space replaced by random English words (≈5–16% absolute). Channel models: negligible drop (≈0–2% absolute) and sometimes slight increases.",
            "performance_comparison": "Direct models much more sensitive to removal of the original label space; channel models largely unaffected.",
            "format_effect_size": "Direct: ~5–16% absolute drop; Channel: ~0–2% absolute change",
            "format_effect_direction": "reduced for direct models; no meaningful effect for channel models",
            "explanation_or_hypothesis": "Direct models must generate labels, so knowledge of the expected label vocabulary/distribution provides a strong cue; removing that (replacing with arbitrary words) hinders generation. Channel models condition on labels and score inputs given labels, so they are less dependent on the semantic content of the label tokens.",
            "counterexample_or_null_result": "Some channel models showed tiny improvements in some settings when label space was replaced, suggesting model- and dataset-specific behavior.",
            "uuid": "e5832.2",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "No-labels (inputs-only, no pairing)",
            "name_full": "Demonstrations Without Labels (Inputs Only — format removed)",
            "brief_description": "Provide demonstrations as a concatenation of example inputs only (no labels), i.e., remove the input-label pairing format to test whether the pairing format itself is critical.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MetaICL, GPT-J, GPT-2, fairseq models, GPT-3 (varied across experiments)",
            "model_size": "774M, 6B, 13B, 175B etc.",
            "task_name": "Classification and Multi-choice",
            "task_description": "k-shot prompt variant where demonstrations consist only of example inputs concatenated, with no labels; compares effect of removing the pairing format.",
            "problem_format": "Inputs-only demonstrations (no labels), compared to paired (input,label) demonstrations and labels-only demonstrations.",
            "comparison_format": "Paired (input,label) demonstrations (standard), Labels-only demonstrations, No-demonstrations",
            "performance": "Removing the pairing format generally yields performance close to or worse than the no-demonstrations baseline (i.e., the benefits of demonstrations largely disappear when the format is removed).",
            "performance_comparison": "Paired demonstrations &gt;&gt; inputs-only (no labels) ≈ no-demonstrations; keeping the pairing format is crucial for retaining gains.",
            "format_effect_size": "Not quantified as a single number; described as 'close to or worse than no demonstrations' in aggregate (substantial loss relative to paired demos)",
            "format_effect_direction": "reduced (format removal eliminates most gains)",
            "explanation_or_hypothesis": "The model uses the input-label pairing format as an explicit instruction to 'complete' or mimic the demonstrated mapping/format at test time; removing the pairing removes that signaling and thus the model cannot exploit format-based cues.",
            "counterexample_or_null_result": null,
            "uuid": "e5832.3",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Labels-only demonstrations",
            "name_full": "Demonstrations Containing Labels Only (no input text)",
            "brief_description": "Condition the LM only on a concatenation of labels (y1 ... yk) without input examples, isolating the contribution of the label set/format absent input distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MetaICL, GPT-J, other direct and channel models evaluated",
            "model_size": "Various; 774M to 175B",
            "task_name": "Classification and Multi-choice",
            "task_description": "k-shot variant where demonstrations are only the labels (no corresponding input text), testing whether seeing labels alone in the paired format provides signal.",
            "problem_format": "Labels-only sequence conditioning, compared to paired demonstrations and inputs-only/no-label variants.",
            "comparison_format": "Paired (input,label) demonstrations; Inputs-only demonstrations; No-demonstrations",
            "performance": "Labels-only (no input) is generally close to or worse than no demonstrations; however, when the label-only sequence is embedded in the paired format (i.e., labels randomly paired with inputs or placeholders to preserve pairing format), models can retain a substantial fraction of k-shot gains (varies by model).",
            "performance_comparison": "Naively providing labels-only (no format) does not help, but preserving the pairing format while using only labels (e.g., pairing random inputs with the true label set) can retain up to ~75–95% of improvements in some model/task combinations (reported examples: Direct MetaICL retained 95% of gains in classification when using inputs or label set with correct format).",
            "format_effect_size": "When format kept, retention of improvements ranges up to ~95% (varies by model/task); without format, performance ≈ no-demonstrations",
            "format_effect_direction": "format-dependent: labels-only without pairing yields no effect or reduced performance; labels embedded in pairing format can preserve much of the benefit",
            "explanation_or_hypothesis": "The label set (label space) is a key component of demonstration information, but labels are only useful if presented in a format that cues the model to apply them to the test input (i.e., the input-label pairing format).",
            "counterexample_or_null_result": null,
            "uuid": "e5832.4",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Demo-size (k) effect",
            "name_full": "Effect of Number of Demonstrations (k)",
            "brief_description": "Study sensitivity of in-context performance to the number of demonstration examples k (evaluated various k including 4, 8, 16).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models (best under 13B for each category reported: Channel MetaICL and Direct GPT-J used as examples)",
            "model_size": "examples: MetaICL 774M, GPT-J 6B",
            "task_name": "Classification and Multi-choice",
            "task_description": "k-shot evaluation comparing k ∈ {4, 8, 16, ...} and measuring gains vs no-demonstrations and robustness to random vs gold labels across k.",
            "problem_format": "Standard k-shot paired demonstrations; compare performance as k varies and when labels are random vs gold.",
            "comparison_format": "Different k values; gold-label vs random-label demonstrations at each k.",
            "performance": "Demonstrations substantially improve performance even at small k (k=4); performance increases little beyond k ≥ 8; performance drop from gold to random labels consistent across k and small (0.8–1.6% absolute), with exception at k=4 showing higher variance (up to 4.4% in one setting).",
            "performance_comparison": "Marginal returns for larger k (≥8); small effect size for swapping gold-&gt;random labels across k (≈0.8–1.6% abs).",
            "format_effect_size": "Gold-&gt;random label drop across k: ~0.8–1.6% absolute (exception: 4.4% at k=4 in some cases)",
            "format_effect_direction": "minor reduction when labels randomized; diminishing returns when increasing k beyond ~8",
            "explanation_or_hypothesis": "Additional demonstration examples mainly help if they provide supervision of input-label correspondence; because models already exploit simpler aspects (format, label space, input distribution), gains saturate quickly and extra examples do not strongly improve performance.",
            "counterexample_or_null_result": "Variance at small k can cause larger drops; in some models/datasets larger k may still offer gains (dataset-dependent).",
            "uuid": "e5832.5",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Template variants (minimal vs manual)",
            "name_full": "Minimal vs Manual Prompt Templates",
            "brief_description": "Comparison between minimal, template-free input formatting and hand-crafted dataset-specific manual templates to evaluate sensitivity to prompt wording/phrasing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various models including GPT-J, MetaICL, GPT-2, GPT-3 variants",
            "model_size": "various (774M to 175B)",
            "task_name": "Classification and Multi-choice",
            "task_description": "Evaluate in-context performance using 'minimal templates' (simple concatenations) versus 'manual templates' (dataset-specific phrasing taken from prior work) for forming input sequences.",
            "problem_format": "Minimal formatting by default; manual templates used as an alternative in an ablation.",
            "comparison_format": "Minimal template vs manual ds-specific templates",
            "performance": "The key trend (random labels barely hurt) holds with both minimal and manual templates; manual templates do not consistently outperform minimal templates—results vary by dataset and model.",
            "performance_comparison": "No consistent improvement from manual templates vs minimal; both preserve overall trends regarding label/random-label effects.",
            "format_effect_size": "Not reported as a single aggregate number; effect varies by dataset/model and is not consistently positive for manual templates.",
            "format_effect_direction": "no consistent improvement (mixed/no effect)",
            "explanation_or_hypothesis": "Template wording/phrasing can change surface format but does not overturn the central findings: demonstrations primarily provide label space, input-distribution and format cues; thus some manual phrasings may help in specific cases but are not universally better.",
            "counterexample_or_null_result": null,
            "uuid": "e5832.6",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Direct vs Channel sensitivity",
            "name_full": "Differences Between Direct and Channel Inference Methods",
            "brief_description": "Comparison of two inference methods: direct (generate label given context) and channel (score P(x|y)*P(y)) to see how prompt/format variations affect models differently.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Direct and Channel variants applied to GPT-J, MetaICL, GPT-2, fairseq, GPT-3",
            "model_size": "various: 774M–175B",
            "task_name": "Classification and Multi-choice",
            "task_description": "Evaluate how sensitivity to label-space and input-distribution modifications differs between direct generation vs channel scoring approaches under the same demonstration formats.",
            "problem_format": "Same paired demonstrations, but inference performed via direct generation or channel method (flip x and y for channel).",
            "comparison_format": "Direct vs Channel inference on identical demonstration variants (random labels, random-English-labels, OOD inputs, etc.).",
            "performance": "Direct models are more sensitive to removal of the original label space (5–16% drop when labels replaced by random English words), whereas channel models show little sensitivity to label-space changes (0–2% drop); channel models are more sensitive to input-distribution changes (OOD inputs cause 3–16% drops for some channel models).",
            "performance_comparison": "Direct: large sensitivity to label-space changes; Channel: larger sensitivity to input-distribution changes; numerical ranges reported in other entries.",
            "format_effect_size": "Label-space change: Direct ~5–16% drop, Channel ~0–2% change. Input-distribution change (OOD): Channel & some direct models 3–16% drop.",
            "format_effect_direction": "format-effect depends on inference method: direct more affected by label vocabulary; channel more affected by input distribution",
            "explanation_or_hypothesis": "Because direct models must generate output tokens, they rely on the label vocabulary cue; channel models score inputs given labels and so benefit from accurate conditioning on input distribution. This reflects which side of the conditional the model exploits more.",
            "counterexample_or_null_result": null,
            "uuid": "e5832.7",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Meta-training (MetaICL) impact",
            "name_full": "Effect of Meta-training with In-Context Learning Objective (MetaICL)",
            "brief_description": "Investigate how meta-training across supervised datasets (MetaICL) affects which aspects of demonstrations the model exploits.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MetaICL (initialized from GPT-2 Large, meta-trained), compared to same-architecture non-meta-trained models",
            "model_size": "774M",
            "task_name": "Classification and Multi-choice (same suite)",
            "task_description": "Meta-trained model evaluated on few-shot demonstration variants to see whether meta-training changes sensitivity to label correctness, input distribution, label space, and format.",
            "problem_format": "k-shot paired demonstrations under variants: random labels, OOD inputs, random-English-labels, no labels, etc.",
            "comparison_format": "MetaICL (meta-trained) vs other non-meta-trained LMs (e.g., GPT-2 Large baseline).",
            "performance": "MetaICL displays exaggerated trends: almost zero influence of input-label mapping and input distribution in some settings; near-zero degradation when labels randomized (0.1–0.9% absolute). MetaICL more strongly exploits format and label-space cues when available.",
            "performance_comparison": "MetaICL shows smaller sensitivity to label correctness than other models; retains more benefit from format-only signals.",
            "format_effect_size": "Gold-&gt;random label drop in MetaICL: ~0.1–0.9% absolute (very small); format-preserving manipulations retain large fraction (e.g., 82–95% of gains in examples).",
            "format_effect_direction": "format exploitation amplified; ground-truth label/input mapping matters even less",
            "explanation_or_hypothesis": "Meta-training encourages models to exploit simpler, surface-level aspects of demonstrations (format, label/input distributions) because those aspects are easier to generalize across many tasks; thus meta-trained models tend to ignore harder signals like fine-grained input-label correspondences.",
            "counterexample_or_null_result": null,
            "uuid": "e5832.8",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Negative/control variants",
            "name_full": "Control Variants That Break Format (constant label/test-input repeated)",
            "brief_description": "Additional negative variants: use constant label for all demos ('answer') or replace all demonstration inputs with the test input, each paired with random labels; both change the sequence format and are used as counterexamples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various evaluated models (GPT-J, MetaICL, others)",
            "model_size": "various",
            "task_name": "Classification and Multi-choice",
            "task_description": "Ablations where demonstrations are altered in ways that change the format (constant label token repeated or identical inputs repeated) to test whether format integrity matters.",
            "problem_format": "Constant-label demonstrations (every label token = 'answer'); test-input-repeated demonstrations (each demo uses the test input paired with random labels).",
            "comparison_format": "Standard paired demonstrations and other ablations.",
            "performance": "Both variants performed consistently worse than other demonstration variants (worse than random-English-labels or OOD inputs), suggesting format perturbations that make separators ambiguous remove benefit.",
            "performance_comparison": "Constant-label and test-input-repeated generally underperform other ablations and often do worse than simpler random-label demonstrations.",
            "format_effect_size": "Not summarized as a single number; described qualitatively as 'consistently worse' than other methods preserving format or distribution.",
            "format_effect_direction": "reduced (substantially worse)",
            "explanation_or_hypothesis": "These manipulations changed the perceived format (e.g., constant label acts as a separator token) and thus destroy the format cue the model uses, demonstrating that not all label/text substitutions are neutral—their impact depends on whether they preserve the demonstration's format/structure.",
            "counterexample_or_null_result": null,
            "uuid": "e5832.9",
            "source_info": {
                "paper_title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "MetaICL: Learning to learn in context",
            "rating": 2
        },
        {
            "paper_title": "Do prompt-based models really understand the meaning of their prompts?",
            "rating": 2
        },
        {
            "paper_title": "Ground-truth labels matter: A deeper look into input-label demonstrations",
            "rating": 2
        },
        {
            "paper_title": "An explanation of in-context learning as implicit bayesian inference",
            "rating": 1
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 1
        },
        {
            "paper_title": "Text and patterns: For effective chain of thought, it takes two to tango",
            "rating": 1
        }
    ],
    "cost": 0.01767525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</h1>
<p>Sewon Min ${ }^{1,2}$ Xinxi Lyu ${ }^{1}$ Ari Holtzman ${ }^{1}$ Mikel Artetxe ${ }^{2}$<br>Mike Lewis ${ }^{2}$ Hannaneh Hajishirzi ${ }^{1,3}$ Luke Zettlemoyer ${ }^{1,2}$<br>${ }^{1}$ University of Washington ${ }^{2}$ Meta AI ${ }^{3}$ Allen Institute for AI<br>{sewon, alrope, ahai, hannaneh, lsz}@cs.washington.edu<br>{artetxe,mikelewis}@meta.com</p>
<h4>Abstract</h4>
<p>Large language models (LMs) are able to in-context learn-perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required-randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choice tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) have shown impressive performance on downstream tasks by simply conditioning on a few input-label pairs (demonstrations); this type of inference has been referred to as in-context learning (Brown et al., 2020). Despite in-context learning consistently outperforming zeroshot inference on a wide range of tasks (Zhao et al., 2021; Liu et al., 2021), there is little understanding of how it works and which aspects of the demonstrations contribute to end task performance.</p>
<p>In this paper, we show that ground truth demonstrations are in fact not required for effective in-context learning (Section 4). Specifically, replacing the labels in demonstrations with random labels barely hurts performance in a range of classification and multi-choice tasks (Figure 1). The result
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Results in classification (top) and multi-choice tasks (bottom), using three LMs with varying size. Reported on six datasets on which GPT-3 is evaluated; the channel method is used. See Section 4 for the full results. In-context learning performance drops only marginally when labels in the demonstrations are replaced by random labels.
is consistent over 12 different models including the GPT-3 family (Radford et al., 2019; Min et al., 2021b; Wang and Komatsuzaki, 2021; Artetxe et al., 2021; Brown et al., 2020). This strongly suggests, counter-intuitively, that the model does not rely on the input-label mapping in the demonstrations to perform the task.</p>
<p>Further analysis investigates which parts of demonstrations actually do contribute to the performance. We identify possible aspects of demonstrations (e.g., the label space and the distribution of the input text) and evaluate a series of variants of the demonstrations to quantify the impact of each (Section 5). We find that: (1) the label space and the distribution of the input text specified by the demonstrations are both key to in-context learning (regardless of whether the labels are correct for individual inputs); (2) specifying the overall format is also crucial, e.g., when the label space is unknown, using random English words as labels is significantly better than using no labels; and</p>
<p>(3) meta-training with an in-context learning objective (Min et al., 2021b) magnifies these effects—the models almost exclusively exploit simpler aspects of the demonstrations like the format rather than the input-label mapping.</p>
<p>In summary, our analysis provides a new way of understanding the role of the demonstrations in in-context learning. We empirically show that the model (1) counter-intuitively does not rely on the ground truth input-label mapping provided in the demonstrations as much as we thought (Section 4), and (2) nonetheless still benefits from knowing the label space and the distribution of inputs specified by the demonstrations (Section 5). We also include a discussion of broader implications, e.g., what we can say about the model learning at test time, and avenues for future work (Section 6).</p>
<h2>2 Related Work</h2>
<p>Large language models have been key to strong performance in a wide range of downstream tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020). While finetuning has been a popular approach to transfer to new tasks (Devlin et al., 2019), it is often impractical to finetune a very large model (e.g. $\geq 10 \mathrm{~B}$ parameters). Brown et al. (2020) propose in-context learning as an alternative way to learn a new task. As depicted in Figure 2, the LM learns a new task via inference alone by conditioning on a concatenation of the training data as demonstrations, without any gradient updates.</p>
<p>In-context learning has been the focus of significant study since its introduction. Prior work proposes better ways of formulating the problem (Zhao et al., 2021; Holtzman et al., 2021; Min et al., 2021a), better ways of choosing labeled examples for the demonstrations (Liu et al., 2021; Lu et al., 2021; Rubin et al., 2021), meta-training with an explicit in-context learning objective (Chen et al., 2021; Min et al., 2021b), and learning to follow instructions as a variant of in-context learning (Mishra et al., 2021b; Efrat and Levy, 2020; Wei et al., 2022a; Sanh et al., 2022). At the same time, some work reports brittleness and oversensitivity for in-context learning (Lu et al., 2021; Zhao et al., 2021; Mishra et al., 2021a).</p>
<p>Relatively less work has been done to understand why in-context learning works. Xie et al. (2022) provide theoretical analysis that in-context learning can be formalized as Bayesian inference that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of in-context learning. The demonstrations consist of $k$ input-label pairs from the training data ( $k=3$ in the figure).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th># Params</th>
<th>Public</th>
<th>Meta-trained</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-2 Large</td>
<td>774 M</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>MetaICL</td>
<td>774 M</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
<tr>
<td>GPT-J</td>
<td>6 B</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>fairseq 6.7B ${ }^{\dagger}$</td>
<td>6.7 B</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>fairseq 13B ${ }^{\dagger}$</td>
<td>13 B</td>
<td>$\checkmark$</td>
<td>$\boldsymbol{x}$</td>
</tr>
<tr>
<td>GPT-3</td>
<td>$175 B^{2}$</td>
<td>$\boldsymbol{x}$</td>
<td>$\boldsymbol{x}$</td>
</tr>
</tbody>
</table>
<p>Table 1: A list of LMs used in the experiments: GPT-2 (Radford et al., 2019), MetaICL (Min et al., 2021b), GPT-J (Wang and Komatsuzaki, 2021), fairseq LMs (Artetxe et al., 2021) and GPT-3 (Brown et al., 2020). 'Public' indicates whether the model weights are public; 'Meta-trained' indicates whether the model is meta-trained with an in-context learning objective. ${ }^{\dagger} \mathrm{We}$ use dense models in Artetxe et al. (2021) and refer them as fairseq LMs for convenience. ${ }^{2}$ We use the Davinci API (the base version, not the instruct version) and assume it to be 175B, following Gao et al. (2021) and Artetxe et al. (2021). uses the demonstrations to recover latent concepts. Razeghi et al. (2022) show that in-context learning performance is highly correlated with term frequencies in the pretraining data. To the best of our knowledge, this paper is the first that provides an empirical analysis that investigates why in-context learning achieves performance gains over zero-shot inference. We find that the ground truth input-label mapping in the demonstrations has only a marginal effect, and measure the impact of finer-grained aspects of the demonstrations.</p>
<h2>3 Experimental Setup</h2>
<p>We describe the experimental setup used in our analysis (Section 4 and 5).</p>
<p>Models. We experiment with 12 models in total. We include 6 language models (Table 1), all of which are decoder-only, dense LMs. We use each LM with two inference methods, direct and channel, following Min et al. (2021a). The sizes of LMs vary from 774 M to 175 B . We include the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Results when using no-demonstrations, demonstrations with gold labels, and demonstrations with random labels in classification (top) and multi-choice tasks (bottom). The first eight models are evaluated on 16 classification and 10 multi-choice datasets, and the last four models are evaluated on 3 classification and 3 multi-choice datasets. See Figure 11 for numbers comparable across all models. Model performance with random labels is very close to performance with gold labels (more discussion in Section 4.1).</p>
<p>Largest dense LM (GPT-3) and the largest publicly released dense LM (fairseq 13B) at the time of conducting experiments. We also include MetaICL, which is initialized from GPT-2 Large and then meta-trained on a collection of supervised datasets with an in-context learning objective, and ensure that our evaluation datasets do not overlap with those used at meta-training time.</p>
<p>Evaluation Data. We evaluate on 26 datasets, including sentiment analysis, paraphrase detection, natural language inference, hate speech detection, question answering, and sentence completion (full list and references provided in Appendix A).<sup>1</sup> All datasets are classification and multi-choice tasks.</p>
<p>We use these datasets because they (1) are true low-resource datasets with less than 10K training examples, (2) include well-studied benchmarks from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019a), and (3) cover diverse domains including science, social media, finance, and more.</p>
<p>Other Details. We use <em>k</em> = 16 examples as demonstrations by default for all experiments in the paper, unless otherwise specified. Examples are sampled at uniform from the training data. We choose a set of <em>k</em> training examples using 5 different random seeds and run experiments 5 times. For fairseq 13B and GPT-3, due to limited resources, we experiment with a subset of 6 datasets<sup>2</sup> and 3 random seeds. We report Macro-F1<sup>3</sup> for classification tasks and Accuracy for multi-choice tasks. We compute per-dataset average over seeds, and then report macro-average over datasets. We use the minimal templates in forming an input sequence from an example. We refer to Appendix B for more details. All experiments are reproducible from github.com/Alrope123/rethinking-demonstrations.</p>
<h1>4 Ground Truth Matters Little</h1>
<h3>4.1 Gold labels vs. random labels</h3>
<p>To see the impact of correctly-paired inputs and labels in the demonstrations—which we call the ground truth input-label mapping—we compare the following three methods.<sup>4</sup></p>
<p>No demonstrations is a typical zero-shot method that does not use any labeled data. A prediction is made via argmax<sub>p∈C</sub> <em>P</em>(<em>y</em>|<em>x</em>), where <em>x</em> is the test input and <em>C</em> is a small discrete set of possible labels.</p>
<p>Demonstrations w/ gold labels are used in a typical in-context learning method with <em>k</em> labeled examples (<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>)...(<em>x</em><sub><em>k</em></sub>, <em>y</em><sub><em>k</em></sub>). A concatenation of <em>k</em> input-label pairs is used to make a prediction via argmax<sub>p∈C</sub> <em>P</em>(<em>y</em>|<em>x</em><sub>1</sub>, <em>y</em><sub>1</sub>...<em>x</em><sub><em>k</em></sub>, <em>y</em><sub><em>k</em></sub>, <em>x</em>).</p>
<p><sup>1</sup>For convenience, we use 'labels' to refer to the output for the task, though our datasets include non-classification tasks.</p>
<p><sup>2</sup>Three classification and three multi-choice: MRPC, RTE, Tweet_eval-hate, OpenbookQA, CommonsenseQA, COPA.</p>
<p><sup>3</sup>Known to be better for imbalanced classes.</p>
<p><sup>4</sup>Without loss of generality, all methods in Section 4 and 5 are described based on the direct method, but can be trivially converted to the channel method by flipping <em>x</em> and <em>y</em>.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results with varying number of correct labels in the demonstrations. Channel and Direct used for classification and multi-choice, respectively. Performance with no demonstrations (blue) is reported as a reference.</p>
<p><strong>Demonstrations w/ random labels</strong> are formed with random labels, instead of gold labels from the labeled data. Each $x_i$ (1 ≤ i ≤ k) is paired with $\hat{y}<em _in="\in" _mathcal_C="\mathcal{C" y="y">i$ that is randomly sampled at uniform from $\mathcal{C}$. A concatenation of $(x_1, \hat{y}_1)...(x_k, \hat{y}_k)$ is then used to make a prediction via $\operatorname{argmax}</em>_k, x)$$.}} P(y|x_1, \hat{y}_1...x_k, \hat{y</p>
<p>Results are reported in Figure 3. First, using the demonstrations with gold labels significantly improves the performance over no demonstrations,<sup>5</sup> as it has been consistently found in much of prior work (Brown et al., 2020; Zhao et al., 2021; Liu et al., 2021). We then find that <strong>replacing gold labels with random labels only marginally hurts performance</strong>. The trend is consistent over nearly all models: models see performance drop in the range of 0–5% absolute. There is less impact in replacing labels in multi-choice tasks (1.7% on average) than in classification tasks (2.6% absolute).</p>
<p>This result indicates that the ground truth input-label pairs are not necessary to achieve performance gains. This is counter-intuitive, given that correctly paired training data is critical in typical supervised training—it informs the model of the expected input-label <em>correspondence</em> required to perform the downstream task. Nonetheless, the models <em>do</em> achieve non-trivial performance on the downstream tasks. This strongly suggests that the models are capable of recovering the expected input-label correspondence for the task; however, it is <em>not</em> directly from the pairings in the demonstrations.</p>
<p>It is also worth noting that there is particularly little performance drop in MetaICL: 0.1–0.9% absolute. This suggests that meta-training with an explicit in-context learning objective actually encourages the model to essentially ignore the input-label mapping and exploit other components of the demonstrations (more discussion in Section 5.4).</p>
<p>In Appendix C.2, we provide additional results showing that (1) selecting random labels from a true distribution of labels (instead of a uniform distribution) reduces the gap even further, and (2) the trends may depend on the dataset, although the overall trend is consistent over most datasets.</p>
<h3>4.2 Ablations</h3>
<p>For additional ablations, we experiment with 5 classification and 4 multi-choice datasets.<sup>6</sup></p>
<p><strong>Does the number of correct labels matter?</strong> To further examine the impact of correctness of labels in the demonstrations, we conduct an ablation study by varying the number of correct labels in the demonstrations. We evaluate "Demonstrations w/ a% correct labels" (0 ≤ a ≤ 100) which consist of k × a/100 correct pairs and k × (1 − a/100) incorrect pairs (see Algorithm 1 in Appendix B). Here, a = 100 is the same as typical in-context learning, i.e., demonstrations w/ gold labels.</p>
<p>Results are reported in Figure 4. Model performance is fairly insensitive to the number of correct labels in the demonstrations. In fact, always using incorrect labels significantly outperforms no-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ablations on varying numbers of examples in the demonstrations (k). Models that are the best under 13B in each task category (Channel MetaICL and Direct GPT-J, respectively) are used.</p>
<p><sup>5</sup>There are some exceptions, e.g., in the classification tasks, Direct GPT-2, Direct GPT-J and Direct fairseq 6.7B models are not significantly better than random guessing on many datasets; Channel fairseq 13B has significantly better no-demonstrations performance compared to demonstrations with gold labels. We thus discuss the results from these models less significantly for the rest of analysis.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Results with minimal templates and manual templates. ' + T' indicates that manual templates are used. Channel and Direct used for classification and multi-choice, respectively.
demonstrations, e.g., preserving $92 \%, 100 \%$ and $97 \%$ of improvements from using the demonstrations with MetaICL in classification, MetaICL in multi-choice, and GPT-J in multi-choice, respectively. In contrast, GPT-J in classification sees relatively significant performance drop with more incorrect labels, e.g., nearly $10 \%$ drop in performance when always using incorrect labels. Still, always using incorrect labels is significantly better than no demonstrations.</p>
<p>Is the result consistent with varying $k$ ? We study the impact of the number of input-label pairs $(k)$ in the demonstrations. Results are reported in Figure 5. First, using the demonstrations significantly outperforms the no demonstrations method even with small $k(k=4)$, and performance drop from using gold labels to using random labels is consistently small across varying $k$, in the range of $0.8-1.6 \% .^{7}$ Interestingly, model performance does not increase much as $k$ increases when $k \geq 8$, both with gold labels and with random labels. This is in contrast with typical supervised training where model performance rapidly increases as $k$ increases, especially when $k$ is small. We hypothesize that larger labeled data is beneficial mainly for supervising the input-label correspondence, and other components of the data like the example inputs, example labels and the data format are easier to recover from the small data, which is potentially a reason for minimal performance gains from larger $k$ (more discussion in Section 5).</p>
<h2>Is the result consistent with better templates?</h2>
<p>While we use minimal templates by default, we also explore manual templates, i.e., templates that are manually written in a dataset-specific manner, taken from prior work (details in Appendix B). Figure 6 shows that the trend-replacing gold labels with random labels barely hurting performanceholds with manual templates. It is worth noting</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Four different aspects in the demonstrations: the input-label mapping, the distribution of the input text, the label space, and the use of input-label pairing as the format of the demonstrations.
that using manual templates does not always outperform using minimal templates.</p>
<h2>5 Why does In-Context Learning work?</h2>
<p>Section 4 shows that the ground truth input-label mapping in the demonstrations has little impact to performance gains from in-context learning. This section further examines what other aspects of the demonstrations lead to good performance of incontext learning.</p>
<p>We identify four aspects of the demonstrations $\left(x_{1}, y_{1}\right) \ldots\left(x_{k}, y_{k}\right)$ that potentially provide learning signal (depicted in Figure 7).</p>
<ol>
<li>The input-label mapping, i.e., whether each input $x_{i}$ is paired with a correct label $y_{i}$.</li>
<li>The distribution of the input text, i.e., the underlying distribution that $x_{1} \ldots x_{k}$ are from.</li>
<li>The label space, i.e., the space covered by $y_{1} \ldots y_{k}$.</li>
<li>The format-specifically, the use of inputlabel pairing as the format.</li>
</ol>
<p>As Section 4 does for the input-label mapping, we design a series of variants of the demonstrations that quantify the impact of each aspect in isolation (Section 5.1-5.3). We then additionally discuss the trend of the models meta-trained with an in-context learning objective (Section 5.4). For all experiments, models are evaluated on five classification</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Impact of the distribution of the inputs. Evaluated in classification (top) and multi-choice (bottom). The impact of the distribution of the input text can be measured by comparing $\square$ and $\square$. The gap is substantial, with an exception in Direct MetaICL (discussion in Section 5.1).
and four multi-choice datasets as in Section 4.2. See Appendix B and Table 4 for implementation details and example demonstrations, respectively.</p>
<h3>5.1 Impact of the distribution of the input text</h3>
<p>We experiment with OOD demonstrations which include out-of-distribution (OOD) text instead of the inputs from unlabeled training data. Specifically, a set of $k$ sentences $\left{x_{i, \text { rand }}\right}<em 1="1">{i=1}^{k}$ are randomly sampled from an external corpus, and replace $x</em>$ in the demonstrations. This variant assesses the impact of the distribution of the input text, while keeping the label space and the format of the demonstrations.} \ldots x_{k</p>
<p>Results. Figure 8 shows that using out-ofdistribution inputs instead of the inputs from the training data significantly drops the performance when Channel MetaICL, Direct GPT-J or Channel GPT-J are used, both in classification and multichoice, by $3-16 \%$ in absolute. In the case of Direct GPT-J in multi-choice, it is even significantly worse than no demonstrations. Direct MetaICL is an exception, which we think is the effect of meta-training (discussion in Section 5.4).</p>
<p>This suggests that in-distribution inputs in the demonstrations substantially contribute to performance gains. This is likely because conditioning on the in-distribution text makes the task closer to language modeling, since the LM always conditioned on the in-distribution text during training.</p>
<h3>5.2 Impact of the label space</h3>
<p>We also experiment with demonstrations w/ random English words that use random English words as labels for all $k$ pairs. Specifically, we sample a random subset of English words $\mathcal{C}<em _rand="{rand" _text="\text">{\text {rand }}$
where $\left|\mathcal{C}</em>}}\right|=|\mathcal{C}|$, and randomly pair $\tilde{y<em _rand="{rand" _text="\text">{i} \in \mathcal{C}</em>$. This variant assesses the impact of the label space, while keeping the distribution of the input text and the format of the demonstrations.}}$ with $x_{i</p>
<p>Results. Based on Figure 9, direct models and channel models exhibit different patterns. With direct models, the performance gap between using random labels within the label space and using random English words is significant, ranging between $5-16 \%$ absolute. This indicates that conditioning on the label space significantly contributes to performance gains. This is true even for multi-choice tasks where there is no fixed set of labels-we hypothesize that multi-choice tasks still do have a particular distribution of the choices (e.g., objects like "Bolts" or "Screws" in the OpenBookQA dataset) that the model uses.</p>
<p>On the other hand, removing the output space does not lead to significant drop in the channel models: there is $0-2 \%$ drop in absolute, or sometimes even an increase. We hypothesize that this is because the channel models only condition on the labels, and thus are not benefiting from knowing the label space. This is in contrast to direct models which must generate the correct labels.</p>
<h3>5.3 Impact of input-label pairing</h3>
<p>Section 5.1 and 5.2 focus on variants which keep the format of the demonstrations as much as possible. This section explores variants that change the format. While there are many aspects of the format, we make minimal modifications to remove the pairings of inputs to labels. Specifically, we evaluate demonstrations with no labels where the LM is conditioned on the concatenation of $x_{1} \ldots x_{k}$, and</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Impact of the label space. Evaluated in classification (top) and multi-choice (bottom). The impact of the label space can be measured by comparing $\square$ and $\square$. The gap is significant in the direct models but not in the channel models (discussion in Section 5.2).
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Impact of the format, i.e., the use of the input-label pairs. Evaluated in classification (top) and multichoice (bottom). Variants of demonstrations without keeping the format ( $\square$ and $\square$ ) are overall not better than no demonstrations ( $\square$ ). Keeping the format is especially significant when it is possible to achieve substantial gains with the label space but without the inputs ( $\square$ vs. $\square$ in Direct MetaICL), or with the input distribution but without the labels ( $\square$ vs. $\square$ in Channel MetaICL and Channel GPT-J). More discussion in Section 5.3.
demonstrations with labels only where the LM is conditioned on the concatenation of $y_{1} \ldots y_{k}$. These ablations provide the no-format counterparts of the 'demonstrations with random English words' and 'demonstrations with OOD inputs', respectively.</p>
<p>Results. Based on Figure 10, removing the format is close to or worse than no demonstrations, indicating the importance of the format. This is likely because conditioning on a sequence of inputlabel pairs triggers the model to mimic the overall format and complete the new example as expected when the test input is given.</p>
<p>More interestingly, keeping the format plays a significant role in retaining a large portion of performance gains by only using the inputs or only using the labels. For instance, with Direct MetaICL, it is possible to retain $95 \%$ and $82 \%$ of improvements from in-context learning (demonstrations with gold labels) by simply sampling random sen-
tences from a corpus and randomly pairing them with the label set ( $\square$ in Figure 10) in classification and multi-choice, respectively. Similarly, with the channel models, it is possible to retain $82 \%, 87 \%$, $86 \%$ and $75 \%$ of improvements from in-context learning by simply pairing each input from the unlabeled training data with a random English word ( $\square$ in Figure 10) in MetaICL classification, GPTJ classification, MetaICL multi-choice and GPT-J multi-choice, respectively. For all of these cases, removing inputs instead of using OOD inputs, or removing labels instead of using random English words is significantly worse, indicating that keeping the format of the input-label pairs is key.</p>
<h3>5.4 Impact of meta-training</h3>
<p>Different from other models, MetaICL is trained with an in-context learning objective, in line with recent work that uses multi-task training on a</p>
<p>large collection of supervised datasets (called metatraining) for generalization to new tasks (Aghajanyan et al., 2021; Khashabi et al., 2020; Wei et al., 2022a; Sanh et al., 2022). We aim to better understand the role of this meta-training in relation with our findings by closely examining the result of MetaICL. In particular, we observe that the patterns we see so far are significantly more evident with MetaICL than with other models. For instance, the ground truth input-label mapping matters even less, and keeping the format of the demonstrations matters even more. There is nearly zero influence of the input-label mapping and the input distribution in Direct MetaICL, and the input-label mapping and the output space in Channel MetaICL.</p>
<p>Based on this observation, we hypothesize that meta-training encourages the model to exclusively exploit simpler aspects of the demonstrations and to ignore others. This is based on our intuition that (1) the input-label mapping is likely harder to exploit, (2) the format is likely easier to exploit, and (3) the space of the text that the model is trained to generate is likely easier to exploit than the space of the text that the model conditions on. ${ }^{8}$</p>
<h2>6 Discussion \&amp; Conclusion</h2>
<p>In this paper, we study the role of the demonstrations with respect to the success of in-context learning. We find that the ground truth input-label mapping in the demonstrations matters significantly less than one might think-replacing gold labels with random labels in the demonstrations only marginally lowers the performance. We then identify a series of aspects in the demonstrations and examine which aspect actually contributes to performance gains. Results reveal that (1) gains are mainly coming from independent specification of the input space and the label space, (2) the models can still retain up to $95 \%$ of performance gains by using either the inputs only or the label set only if the right format is used, and (3) meta-training with an in-context learning objective magnifies these trends. Together, our findings lead to a set of broader indications about in-context learning, as well as avenues for future work.</p>
<p>Does the model learn at test time? If we take a strict definition of learning: capturing the inputlabel correspondence given in the training data,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>then our findings suggest that LMs do not learn new tasks at test time. Our analysis shows that the model may ignore the task defined by the demonstrations and instead use prior from pretraining.</p>
<p>However, learning a new task can be interpreted more broadly: it may include adapting to specific input and label distributions and the format suggested by the demonstrations, and ultimately getting to make a prediction more accurately. With this definition of learning, the model does learn the task from the demonstrations. Our experiments indicate that the model does make use of aspects of the demonstrations and achieve performance gains.</p>
<p>Capacity of LMs. The model performs a downstream task without relying on the input-label correspondence from the demonstrations. This suggests that the model has learned the (implicit notion of) input-label correspondence from the language modeling objective alone, e.g., associating a positive review with the word 'positive'. This is in line with Reynolds and McDonell (2021) who claim that the demonstrations are for task location and the intrinsic ability to perform the task is obtained at pretraining time. ${ }^{9}$</p>
<p>On one hand, this suggests that the language modeling objective has led to great zero-shot capacity, even if it is not always evident from the naive zero-shot accuracy. On the other hand, this suggests that in-context learning may not work on a task whose input-label correspondence is not already captured in the LM. This leads to the research question of how to make progress in NLP problems that in-context learning does not solve: whether we need a better way of extracting the input-label mappings that are already stored in the LM, a better variant of the LM objective that learns a wider range of task semantics, or explicit supervision through fine-tuning on the labeled data.</p>
<h2>Connection to instruction-following models.</h2>
<p>Prior work has found it promising to train the model that reads the natural language description of the task (called instructions) and performs a new task at inference (Mishra et al., 2021b; Efrat and Levy, 2020; Wei et al., 2022a; Sanh et al., 2022). We think the demonstrations and instructions largely have the same role to LMs, and hypothesize that our findings hold for instruction-following models: the</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>instructions prompt the model to recover the capacity it already has, but do not supervise the model to learn novel task semantics. This has been partially verified by Webson and Pavlick (2022) who showed that the model performance does not degrade much with irrelevant or misleading instructions. We leave more analysis on instruction-following models for future work.</p>
<p>Significantly improved zero-shot performance. One of our key findings is that it is possible to achieve nearly $k$-shot performance without using any labeled data, by simply pairing each unlabeled input with a random label and using it as the demonstrations. This means our zero-shot baseline level is significantly higher than previously thought. ${ }^{10}$ Future work can further improve the zero-shot performance with relaxed assumptions in access to the unlabeled training data.</p>
<h2>Limitation</h2>
<p>Effect of types of tasks and datasets. This paper focuses on the tasks from established NLP benchmarks that have real natural language inputs. Synthetic tasks with more limited inputs may actually use the ground truth labels more, as observed by Rong (2021).</p>
<p>We report macro-level analysis by examining the average performance over multiple NLP datasets, but different datasets may behave differently. Appendix C. 2 discusses this aspect, including findings that there are larger gaps between using the ground truth labels and using the random labels in some dataset-model pairs (e.g., in the most extreme case, nearly $14 \%$ absolute on the financial_phrasebank dataset with GPT-J). Since the first version of our paper, Kim et al. (2022) showed that using negated labels substantially lowers the performance in classification. ${ }^{11}$ We believe it is important to understand to what extend the model needs the ground truth labels to successfully perform in-context learning.</p>
<p>Extensions to generation. Our experiments are limited to classification and multi-choice tasks. We hypothesize that ground truth output may not be necessary for in-context learning in the open-set</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tasks such as generation, but leave this to future work. Extending of our experiments to such tasks is not trivial, because it requires a variation of the output which has incorrect input-output correspondence while keeping the correct output distribution (which is important based on our analysis in Section 5).</p>
<p>Since the first version of our paper, Madaan and Yazdanbakhsh (2022) conducted a similar analysis with the chain of thought prompting (Wei et al., 2022b) which generates a rationale to perform complex tasks such as math problems. Madaan and Yazdanbakhsh (2022) show that, while simply using a random rationale in the demonstrations (e.g., pairing with a rationale from a different example) significantly degrades the performance, other types of counterfactual rationales (e.g., wrong equations) do not degrade the performance as much as we thought. We refer to Madaan and Yazdanbakhsh (2022) for more discussions on what aspects of the rationale matter or do not matter.</p>
<h2>Acknowledgements</h2>
<p>We thank Gabriel Ilharco, Julian Michael, Ofir Press, UW NLP members and anonymous reviewers for their comments in the paper. This research was supported by NSF IIS-2044660, ONR N00014-18-1-2826, a Sloan fellowship and gifts from AI2.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations with pre-finetuning. arXiv preprint arXiv:2101.11038.</p>
<p>Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. 2021. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684.</p>
<p>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment.</p>
<p>Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of EMNLP.</p>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge. In TAC.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS.</p>
<p>Michael Chen, Mike D'Arcy, Alisa Liu, Jared Fernandez, and Doug Downey. 2019. CODAH: An adversarially-authored question answering dataset for common sense. In Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP.</p>
<p>Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2021. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop.</p>
<p>Ona de Gibert, Naiara Perez, Aitor García-Pablos, and Montse Cuadros. 2018. Hate Speech Dataset from a White Supremacy Forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2).</p>
<p>Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL.
T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. 2020. Climate-fever: A dataset for verification of real-world climate claims. ArXiv.</p>
<p>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</p>
<p>Avia Efrat and Omer Levy. 2020. The turking test: Can language models understand instructions? arXiv preprint arXiv:2010.11982.
L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, et al. 2021. The pile: an 800gb dataset of diverse text for language modeling 2020. arXiv preprint arXiv:2101.00027.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing.</p>
<p>Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In The First Joint Conference on Lexical and Computational Semantics (SemEval).</p>
<p>Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In EMNLP.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing format boundaries with a single qa system. In Findings of EMNLP.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: A dataset for question answering via sentence composition. In AAAI.</p>
<p>Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. arXiv preprint arXiv:2205.12685.</p>
<p>Elyor Kodirov, Tao Xiang, Zhenyong Fu, and Shaogang Gong. 2015. Unsupervised domain adaptation for zero-shot learning. In Proceedings of the IEEE international conference on computer vision.</p>
<p>Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In $A C L$.</p>
<p>Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid,</p>
<p>Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In EMNLP: System Demonstrations.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Robert L Logan IV, Ivana Balaževic, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. 2021. Cutting down on prompts and parameters: Simple few-shot learning with language models. arXiv preprint arXiv:2106.13353.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686.</p>
<p>Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf. Sci. Technol.</p>
<p>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In LREC.</p>
<p>Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. 2020. Effective transfer learning for identifying similar questions: Matching user questions to covid-19 faqs. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2021a. Noisy channel language model prompting for few-shot text classification. arXiv preprint arXiv:2108.04106.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021b. MetaICL: Learning to learn in context. arXiv preprint.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2021a. Reframing instructional prompts to gptk's language. arXiv preprint arXiv:2109.07830.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021b. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773.</p>
<p>Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. 2020. Ethos: an online hate speech detection dataset. ArXiv.</p>
<p>Sebastian Nagel. 2016. CC-News. http: //web.archive.org/save/http: //commoncrawl.org/2016/10/ news-dataset-available.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems.</p>
<p>Frieda Rong. 2021. Extrapolating to unnatural language processing with gpt-3's in-context learning: The good, the bad, and the mysterious. https://ai.stanford.edu/blog/ in-context-learning.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teelun, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In ICLR.</p>
<p>Timo Schick and Hinrich Schütze. 2021. It's not just size that matters: Small language models are also few-shot learners. In NAACL-HLT.</p>
<p>Emily Sheng and David Uthus. 2020. Investigating societal biases in a poetry composition system. In Proceedings of the Second Workshop on Gender Bias in Natural Language Processing.</p>
<p>Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. 2019. DREAM: A challenge data set and models for dialogue-based reading comprehension. TACL.</p>
<p>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. 2019a. Quarel: A dataset and models for answering questions about qualitative relationships. In AAAI.</p>
<p>Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. 2019b. QuaRTz: An open-domain dataset of qualitative relationship questions. In EMNLP.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In NAACL-HLT.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. In NeurIPS.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax.</p>
<p>Wei Wang, Vincent W Zheng, Han Yu, and Chunyan Miao. 2019b. A survey of zero-shot learning: Settings, methods, and applications. ACM Transactions on Intelligent Systems and Technology (TIST).</p>
<p>Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts? In NAACL-HLT.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In $I C L R$.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In $I C L R$.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for crosstask generalization in nlp. In EMNLP.</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In ICML.</p>
<h2>A Full Datasets</h2>
<p>We include 26 datasets as follows: financial_phrasebank (Malo et al., 2014), poem_sentiment (Sheng and Uthus, 2020), medical_questions_pairs (McCreery et al., 2020), glue-mrpc (Dolan and Brockett, 2005), gluewnli (Levesque et al., 2012), climate_fever (Diggelmann et al., 2020), glue-rte (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), supergluecb (de Marneffe et al., 2019), sick (Marelli et al., 2014) , hate_speech18 (de Gibert et al., 2018), ethos-national_origin (Mollas et al., 2020), ethosrace (Mollas et al., 2020), ethos-religion (Mollas et al., 2020), tweet_eval-hate (Barbieri et al., 2020), tweet_eval-stance_atheism (Barbieri et al., 2020), tweet_eval-stance_feminist (Barbieri et al., 2020), quarel (Tafjord et al., 2019a), openbookqa (Mihaylov et al., 2018), qasc (Khot et al., 2020), commonsense_qa (Talmor et al., 2019), ai2_arc (Clark et al., 2018), codah (Chen et al., 2019), supergluecopa (Gordon et al., 2012), dream (Sun et al., 2019), quartz-with_knowledge (Tafjord et al., 2019b), quartz-no_knowledge (Tafjord et al., 2019b). The choice of datasets is made following low-resource datasets in Min et al. (2021b), with the exact same set of $k$-shot train data using 5 random seeds. We use the HuggingFace version of the data (Lhoest et al., 2021) and use the development data for evaluation, following Ye et al. (2021). See Table 2 for statistics.</p>
<h2>B Experimental Details</h2>
<p>Example template We follow Ye et al. (2021); Min et al. (2021b); Logan IV et al. (2021) in using the minimal format to transform the input to a sequence (e.g. a concatenation of multiple inputs) and using the label words from each dataset as it is. We also explore manual templates taken from prior work (Holtzman et al., 2021; Zhao et al., 2021) as reported in Section 4.2, although we find that using these templates is not consistently better than using minimal templates. We thus run main experiments with minimal templates. Example templates are provided in Table 3.</p>
<p>Format of the demonstrations We follow the standard of each model for formatting the demonstrations, either from exploration in prior work or the example code provided in the official tutorial. For GPT-2, we separate the input and the label,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># Train</th>
<th style="text-align: right;"># Eval</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task category: Sentiment analysis</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">financial_phrasebank</td>
<td style="text-align: center;">1,811</td>
<td style="text-align: right;">453</td>
</tr>
<tr>
<td style="text-align: left;">poem_sentiment</td>
<td style="text-align: center;">892</td>
<td style="text-align: right;">105</td>
</tr>
<tr>
<td style="text-align: left;">Task category: Paraphrase detection</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">medical_questions_pairs</td>
<td style="text-align: center;">2,438</td>
<td style="text-align: right;">610</td>
</tr>
<tr>
<td style="text-align: left;">glue-mrpc</td>
<td style="text-align: center;">3,668</td>
<td style="text-align: right;">408</td>
</tr>
<tr>
<td style="text-align: left;">Task category: Natural language inference</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">glue-wnli</td>
<td style="text-align: center;">635</td>
<td style="text-align: right;">71</td>
</tr>
<tr>
<td style="text-align: left;">climate_fever</td>
<td style="text-align: center;">1,228</td>
<td style="text-align: right;">307</td>
</tr>
<tr>
<td style="text-align: left;">glue-rte</td>
<td style="text-align: center;">2,490</td>
<td style="text-align: right;">277</td>
</tr>
<tr>
<td style="text-align: left;">superglue-cb</td>
<td style="text-align: center;">250</td>
<td style="text-align: right;">56</td>
</tr>
<tr>
<td style="text-align: left;">sick</td>
<td style="text-align: center;">4,439</td>
<td style="text-align: right;">495</td>
</tr>
<tr>
<td style="text-align: left;">Task category: Hate speech detection</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">hate_speech18</td>
<td style="text-align: center;">8,562</td>
<td style="text-align: right;">2,141</td>
</tr>
<tr>
<td style="text-align: left;">ethos-national_origin</td>
<td style="text-align: center;">346</td>
<td style="text-align: right;">87</td>
</tr>
<tr>
<td style="text-align: left;">ethos-race</td>
<td style="text-align: center;">346</td>
<td style="text-align: right;">87</td>
</tr>
<tr>
<td style="text-align: left;">ethos-religion</td>
<td style="text-align: center;">346</td>
<td style="text-align: right;">87</td>
</tr>
<tr>
<td style="text-align: left;">tweet_eval-hate</td>
<td style="text-align: center;">8,993</td>
<td style="text-align: right;">999</td>
</tr>
<tr>
<td style="text-align: left;">tweet_eval-stance_atheism</td>
<td style="text-align: center;">461</td>
<td style="text-align: right;">52</td>
</tr>
<tr>
<td style="text-align: left;">tweet_eval-stance_feminist</td>
<td style="text-align: center;">597</td>
<td style="text-align: right;">67</td>
</tr>
<tr>
<td style="text-align: left;">Task category: Question answering</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">quarel</td>
<td style="text-align: center;">1,941</td>
<td style="text-align: right;">278</td>
</tr>
<tr>
<td style="text-align: left;">openbookqa</td>
<td style="text-align: center;">4,957</td>
<td style="text-align: right;">500</td>
</tr>
<tr>
<td style="text-align: left;">qasc</td>
<td style="text-align: center;">8,134</td>
<td style="text-align: right;">926</td>
</tr>
<tr>
<td style="text-align: left;">commonsense_qa</td>
<td style="text-align: center;">9,741</td>
<td style="text-align: right;">1,221</td>
</tr>
<tr>
<td style="text-align: left;">ai2_arc</td>
<td style="text-align: center;">1,119</td>
<td style="text-align: right;">299</td>
</tr>
<tr>
<td style="text-align: left;">Task category: Sentence completion</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">codah</td>
<td style="text-align: center;">1665</td>
<td style="text-align: right;">556</td>
</tr>
<tr>
<td style="text-align: left;">superglue-copa</td>
<td style="text-align: center;">400</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">dream</td>
<td style="text-align: center;">6116</td>
<td style="text-align: right;">2040</td>
</tr>
<tr>
<td style="text-align: left;">quartz-with_knowledge</td>
<td style="text-align: center;">2696</td>
<td style="text-align: right;">384</td>
</tr>
<tr>
<td style="text-align: left;">quartz-no_knowledge</td>
<td style="text-align: center;">2696</td>
<td style="text-align: right;">384</td>
</tr>
</tbody>
</table>
<p>Table 2: 26 datasets used for experiments, classified into 6 task categories. # Train and # Test indicate the number of training and test examples of the dataset. Note that # train is based on the original training dataset but we use $k$ random samples for $k$-shot evaluation.
and each demonstration example with a space. For MetaICL, GPT-J and GPT-3, we separate the input and the label with a newline ( $\backslash \mathrm{n}$ ), and each demonstration example with three newlines. For fairseq models, we use a newline to separate the input and the label as well as each demonstration example.</p>
<p>Details in variants of the demonstrations For "demonstrations w/ $a \%$ accurate labels" ( $0 \leq$ $a \leq 100$ ), we use $k \times a / 100$ correct pairs and $k \times(1-a / 100)$ incorrect pairs in a random order, as described in Algorithm 1. For "OOD demonstrations", we use CC-News (Nagel, 2016) as an external corpus. We consider the length of the text during sampling, so that sampled sentences have similar length to the test input. For "demonstrations with random English words", we use pypi.org/ project/english-words for the set of En-</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Results of No-demonstration, Gold demonstration and Random demonstration on 3 classification datasets (top) and 3 multi-choice datasets (bottom). Details in Section 4.1. This figure is for providing numbers that are comparable across models—full results with more datasets are reported in Figure 3.</p>
<p>Algorithm 1 Forming the demonstrations with an accuracy of $a \%$.</p>
<div class="codehilite"><pre><span></span><code><span class="k">proc</span><span class="nv">edure</span><span class="w"> </span><span class="nv">FORmDEMONS</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="err">\</span><span class="nv">left</span><span class="err">\{\</span><span class="nv">left</span><span class="p">(</span><span class="nv">x_</span><span class="err">{</span><span class="nv">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">y_</span><span class="err">{</span><span class="nv">i</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="nv">right</span><span class="err">\}</span><span class="nv">_</span><span class="err">{</span><span class="nv">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="nv">k</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">a</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">D</span><span class="w"> </span><span class="err">\</span><span class="nv">leftarrow</span><span class="p">[]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">demonstration</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">formed</span>
<span class="w">    </span><span class="err">\(</span><span class="nf">n</span><span class="w"> </span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="nv">k</span><span class="w"> </span><span class="err">\</span><span class="nv">times</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">pairs</span>
<span class="w">    </span><span class="err">\(\</span><span class="nf">mathcal</span><span class="err">{</span><span class="nv">G</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nv">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">Sample</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">Range</span><span class="err">}</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nv">k</span><span class="p">),</span><span class="w"> </span><span class="nv">n</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nf">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">i</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">Range</span><span class="err">}</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nv">k</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">do</span>
<span class="w">        </span><span class="nf">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">i</span><span class="w"> </span><span class="err">\</span><span class="nv">in</span><span class="w"> </span><span class="err">\</span><span class="nv">mathcal</span><span class="err">{</span><span class="nv">G</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="nv">then</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">add</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">pair</span>
<span class="w">            </span><span class="err">\(</span><span class="nf">D</span><span class="w"> </span><span class="nv">.</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">append</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="nv">x_</span><span class="err">{</span><span class="nv">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="nv">y_</span><span class="err">{</span><span class="nv">i</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nf">else</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nv">add</span><span class="w"> </span><span class="nv">incorrect</span><span class="w"> </span><span class="nv">pair</span>
<span class="w">            </span><span class="err">\(</span><span class="nf">D</span><span class="w"> </span><span class="nv">.</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">append</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="err">\</span><span class="nv">left</span><span class="p">(</span><span class="nv">x_</span><span class="err">{</span><span class="nv">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nv">operatorname</span><span class="err">{</span><span class="nv">Sample</span><span class="err">}\</span><span class="nv">left</span><span class="p">(</span><span class="err">\</span><span class="nv">mathcal</span><span class="err">{</span><span class="nv">C</span><span class="err">}</span><span class="o">-</span><span class="nv">y_</span><span class="err">{</span><span class="nv">i</span><span class="err">}\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="nv">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nf">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nv">D</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>glish words, which consists of 61,569 words.
Table 4 provides a list of example demonstrations for each method used in Section 5.</p>
<h2>C More Experimental Results</h2>
<h2>C. 1 Gold labels vs. random labels</h2>
<p>Figure 11 shares the same interface as Figure 3, but all models are evaluated on 3 classification and 3 multi-choice datasets and are thus comparable to each other.</p>
<h2>C. 2 Random labels from true distribution of labels \&amp; Task breakdown</h2>
<p>In Section 4, random labels are sampled from the label space from a uniform distribution. We experiment with another variant of demonstrations in the classification tasks, where labels are randomly sampled from the true distribution of labels on the training data. This may have large impact if labels are far from uniform on the training data. Results indicate that performance drop from using gold
labels is further reduced compared to using uniformly random labels: with Channel MetaICL, the gap is reduced from $1.9 \%$ to $1.3 \%$ absolute, and with Channel GPT-J, the gap is reduced from 5.0\% to $3.5 \%$ absolute.</p>
<p>Figure 12 shows performance gap between using gold labels and using random labels per dataset. We find that the trend that the gap is smaller than previously thought is consistant across most datasets. Nonetheless, there are a few outlier datasets where performance gap is non-negligible, such as financial_phrasebank and a few hate speech detection datasets. Future work may investigate on which tasks the model makes more use of the correctly paired training data.</p>
<h2>C. 3 More variants of the demonstrations</h2>
<p>We explored demonstrations with a constant label where all labels in the demonstrations are replaced with a constant text, "answer". Specifically, a prediction is made via $\operatorname{argmax}<em 1="1">{y \in \mathcal{C}} P\left(y \mid x</em>\right.$, answer, $\left.x\right)$. This can be viewed as another way to remove the impact of the label space while keeping the impact of the distribution of the input text. However, results are consistently worse than the results of demonstrations with random English labels. We think this is because constant labels actually change the format of the demonstrations, since they can be viewed as part of a separator between different demonstration examples.}\right.$, answer... $\left.x_{k</p>
<p>We also explored demonstrations with the test input where all inputs in the demonstrations are replaced with the test input, each paired with a ran-</p>
<p>dom label. Specifically, a prediction is made via $\operatorname{argmax}<em 1="1">{y \in \mathcal{C}} P\left(y \mid x, \tilde{y}</em>} \ldots x, \tilde{y<em i="i">{k}, x\right)$, where $\tilde{y}</em>$. This variant is seemingly a reasonable choice given that it satisfies the condition that the inputs in the demonstrations come from the same distribution as the test input (since they are identical), and using random labels is as good as using gold labels. Nonetheless, we find that this variant is significantly worse than most other methods with demonstrations. We think this is because using the constant input for all demonstration example significantly changes the format of the sequence, since the input can be viewed as part of a separator between different demonstration examples.}(1 \leq$ $i \leq k$ ) is randomly sampled at uniform from $\mathcal{C</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Performance gap from using the demonstrations with gold labels to using the demonstrations with random labels. Datasets are sorted in descending order. The top two figures use random labels that are sampled at uniform, with Channel MetaICL and Channel GPT-J, respectively. The bottom two figures use random labels that are sampled from a true distribution of labels on the training data, with Channel MetaICL and Channel GPT-J, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MRPC</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">sentence 1: Cisco pared spending to compensate for sluggish sales . [SEP] sentence 2: In response to sluggish sales, Cisco pared spending. \n {equivalent]not_equivalent}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">Cisco pared spending to compensate for sluggish sales . \n The question is: In response to sluggish sales, Cisco pared spending. True or False? \n The answer is: ${$ True $\mid$ False $}$</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">sentence 1: The girl was found in Drummondville. [SEP] sentence 2: Drummondville contains the girl. \n {entailment]not_entailment}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">The girl was found in Drummondville. \n The question is: Drummondville contains the girl. True or False? \n The answer is: ${$ True $\mid$ False $}$</td>
</tr>
<tr>
<td style="text-align: center;">Tweet_eval-hate</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">The Truth about #Immigration \n {hate[non-hate}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">Tweet: The Truth about #Immigration \n Sentiment: {against[favor}</td>
</tr>
<tr>
<td style="text-align: center;">SICK</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">sentence 1: A man is screaming. [SEP] sentence 2: A man is scared. \n {contradiction</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">A man is screaming. \n The question is: A man is scared. True or False? \n The answer is: {False</td>
</tr>
<tr>
<td style="text-align: center;">poem-sentiment</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">willis sneered: \n {negative[no_impact</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">willis sneered: \n The sentiment is: {negative[no_impact</td>
</tr>
<tr>
<td style="text-align: center;">OpenbookQA</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">What creates a valley? \n {feet[rock</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">The question is: What creates a valley? \n The answer is: {feet]rock[water[sand}</td>
</tr>
<tr>
<td style="text-align: center;">CommonsenseQA</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">What blocks sunshine? \n {summer[park]desktop[sea]moon}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">The question is: What blocks sunshine? \n The answer is: {summer[park]desktop[sea]moon}</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">Effect: I coughed. \n {Cause: I inhaled smoke.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">I coughed because {I inhaled smoke.</td>
</tr>
<tr>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">Minimal</td>
<td style="text-align: center;">Which biome has the most vegetation? \n {desert[forest]grassland[tundra}</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;">The question is: Which biome has the most vegetation? \n The answer is: {desert[forest] grassland[tundra]</td>
</tr>
</tbody>
</table>
<p>Table 3: A list of minimal templates taken from Ye et al. (2021); Min et al. (2021b) and manual templates taken from Holtzman et al. (2021); Zhao et al. (2021). Details provided in Appendix B. See Figure 6 for discussion in empirical results. The input and the label are in the red text and in the blue text, respectively. Note that $\mid$ is used to separate different options for the labels.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Demos <br> w/ gold labels</th>
<th style="text-align: center;">(Format $\checkmark$ Input distribution $\checkmark$ Label space $\checkmark$ Input-label mapping $\checkmark$ ) Circulation revenue has increased by 5\% in Finland and 4\% in Sweden in 2008. \n positive Panostaja did not disclose the purchase price. \n neutral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Demos <br> w/ random labels</td>
<td style="text-align: center;">(Format $\checkmark$ Input distribution $\checkmark$ Label space $\checkmark$ Input-label mapping $\boldsymbol{X}$ ) Circulation revenue has increased by 5\% in Finland and 4\% in Sweden in 2008. \n neutral Panostaja did not disclose the purchase price. \n negative</td>
</tr>
<tr>
<td style="text-align: center;">OOD Demos <br> w/ random labels</td>
<td style="text-align: center;">(Format $\checkmark$ Input distribution $\boldsymbol{X}$ Label space $\checkmark$ Input-label mapping $\boldsymbol{X}$ ) <br> Colour-printed lithograph. Very good condition. Image size: $15 \times 231 / 2$ inches. \n neutral Many accompanying marketing claims of cannabis products are often well-meaning. \n negative</td>
</tr>
<tr>
<td style="text-align: center;">Demos <br> w/ random English words</td>
<td style="text-align: center;">(Format $\checkmark$ Input distribution $\checkmark$ Label space $\boldsymbol{X}$ Input-label mapping $\boldsymbol{X}$ ) Circulation revenue has increased by 5\% in Finland and 4\% in Sweden in 2008. \n unanimity Panostaja did not disclose the purchase price. \n wave</td>
</tr>
<tr>
<td style="text-align: center;">Demos <br> w/o labels</td>
<td style="text-align: center;">(Format $\boldsymbol{X}$ Input distribution $\checkmark$ Label space $\boldsymbol{X}$ Input-label mapping $\boldsymbol{X}$ ) Circulation revenue has increased by 5\% in Finland and 4\% in Sweden in 2008. Panostaja did not disclose the purchase price.</td>
</tr>
<tr>
<td style="text-align: center;">Demos <br> labels only</td>
<td style="text-align: center;">(Format $\boldsymbol{X}$ Input distribution $\boldsymbol{X}$ Label space $\checkmark$ Input-label mapping $\boldsymbol{X}$ ) positive neutral</td>
</tr>
</tbody>
</table>
<p>Table 4: Example demonstrations when using methods in Section 5. The financial_phrasebank dataset with $\mathcal{C}={$ "positive", "neutral", "negative" $}$ is used. Red text indicates the text is sampled from an external corpus; blue text indicates the labels are randomly sampled from the label set; purple text indicates a random English word.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ We take the perspective that using the unlabeled training data is permitted (Kodirov et al., 2015; Wang et al., 2019b; Schick and Schütze, 2021).
${ }^{11}$ Note that Kim et al. (2022) estimate the random label performance by interpolating with the performance using negated labels, while our paper samples the random labels at uniform.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ However, while Reynolds and McDonell (2021) claims that the demonstrations are thus unnecessary, we think using the demonstrations is actually the most unambiguous and the easiest way to prompt the model to perform a task.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
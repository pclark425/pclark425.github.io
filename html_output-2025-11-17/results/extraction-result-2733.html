<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2733 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2733</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2733</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-270870174</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.01093v1.pdf" target="_blank">IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline. In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable. The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective. To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent. Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama. Our codes and prompts are available at https://github.com/OpenDFM/ibsen.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2733.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2733.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IBSEN Actor Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IBSEN Actor Agent Memory Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The actor agent in the IBSEN framework uses a layered memory system (Actor Profile, Memory Database, Character Database) plus a short-term dialogue log; memories are stored as first‑person 'monologues', retrieved by a hybrid relevance score, and summarized from dialogue when the log grows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IBSEN Actor Agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Distributed actor agent within the IBSEN director-actor framework, implemented on gpt-3.5-turbo-1106; actors generate utterances given director instructions and a memory-backed context composed of profile, memory and character DBs and a short-term dialogue log.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Interactive drama / multi-agent dialogue generation (adapted Hedda Gabler scenario) with optional human player involvement; not a standard text-game benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hybrid: separated memory stores — Actor Profile (static persona), Memory Database (episodic memory / past events), Character Database (interpersonal relations) — plus a working/short-term dialogue log.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Document-oriented store with embedding indices (vector retrieval) and TF-IDF signals; short-term dialogue log as sequential buffer; periodic summarization converts older log segments into stored memory documents (monologues).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Character descriptions and persona, interpersonal relations and impressions, memory documents of past events/perceptions, summarized dialogue points (from earlier turns), and monologue-formatted documents (first-person perspective).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not numerically specified; bounded operationally by dialogue-log length (older turns are summarized and moved into memory) and a per-objective turn limit (e.g., forced completion after 9 turns); capacity not given as explicit token/entry count.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Retrieval ranking by a combination score: embedding (semantic similarity) + TF-IDF term-frequency + document recency; character documents retrieved when character names appear in instruction or dialogue history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>As the plot develops, actor summarizes older dialogue turns into concise points and adds them to the memory database as documents (monologues); actor profile and character DB entries are actively updated throughout the storyline; new event/narration turns appended to dialogue log and may be summarized later.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Provide contextual information for generating character-consistent utterances, preserve long-term character traits and interpersonal impressions, inform director's storyline/script generation and objective checking, and reduce irrelevant repetition by summarizing context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Full IBSEN (including monologue-formatted memory) achieved average storyline scores across 10 generated scripts: storyline logicality 2.96, storyline coherence 3.63, character consistency 2.77 (scale 1–4). Objective-check metrics reported: precision 0.82, recall 0.72, F1 0.77. (These are reported for the complete system that uses the memory design.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation removing first-person 'monologues' from memory/character DB: no short-term harm to storyline logicality reported, but a slight decrease in long-term storyline coherence and in maintaining character characteristics; exact numeric deltas for the ablation are not fully enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>First-person monologue framing (storing memory documents narrated in the character's voice) improves long-term character consistency and coherence; splitting memory into profile / memory DB / character DB avoids redundancy issues seen in monolithic memory modules; retrieval quality (embedding + TF-IDF + recency) and monologue formatting are key drivers of effective memory for role-specific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Paper notes risks of memory redundancy and retrieval failure if all content is stored in a single module; memory growth can make retrieval less likely to find valid info; hallucination and LLM safety behaviors require an output revision stage; high repetition still observed despite revision; capacity and quantitative scaling behavior not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2733.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2733.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that introduced LLM-based agents simulating human daily life with memory modules; referenced in IBSEN as a milestone for humanlike agent behavior and as an antecedent memory design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agents designed to simulate human daily behavior; they maintain memories of events and use retrieval to influence future actions and planning (referenced by IBSEN but not used experimentally in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Virtual-world simulation of agents' daily activities and multi-agent interactions (not a standard text-game benchmark in IBSEN).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Monolithic memory module (as characterized in IBSEN's related-work discussion): stores relationships, dialogue history, perceptions and events.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Single unified memory store (document-style) as described in the cited prior work (IBSEN references this architecture and notes its limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Character relationships, dialogue history, perceptions and recorded events (per IBSEN's description of prior works).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Support long-term, humanlike agent behavior and decisions by recalling past events and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>IBSEN cites that the monolithic memory approach used in prior generative-agent work can lead to redundancy and reduced retrieval validity as the number of memory entries increases.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Described in IBSEN as: when many memory entries accumulate, the memory becomes complex and retrieval may fail to access valid information (redundancy & retrieval degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2733.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2733.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-game LM studies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-trained language model agents for text-based games (Urbanek et al., 2019; Singh et al., 2022; Xu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior literature using pre-trained LMs to play text-based games is cited by IBSEN as related work; these references indicate use of LMs for environment-interactive tasks but IBSEN does not evaluate on those benchmarks or detail their memory architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Pre-trained LM agents (text-game studies)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Groups of works applying pre-trained language models to text-based games and interactive fiction (cited by IBSEN); these demonstrate LMs solving environment-interactive tasks but are not analyzed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>text-based games / interactive fiction (general — examples in citations include fantasy text adventure environments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure environments where agents must parse observations and produce textual actions to navigate, manipulate objects, and achieve goals (puzzle solving, exploration, task completion).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior. <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 2)</em></li>
                <li>Pre-trained language models as prior knowledge for playing text-based games <em>(Rating: 2)</em></li>
                <li>Humanoid agents: Platform for simulating human-like generative agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2733",
    "paper_id": "paper-270870174",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "IBSEN Actor Memory",
            "name_full": "IBSEN Actor Agent Memory Architecture",
            "brief_description": "The actor agent in the IBSEN framework uses a layered memory system (Actor Profile, Memory Database, Character Database) plus a short-term dialogue log; memories are stored as first‑person 'monologues', retrieved by a hybrid relevance score, and summarized from dialogue when the log grows.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "IBSEN Actor Agent",
            "agent_description": "Distributed actor agent within the IBSEN director-actor framework, implemented on gpt-3.5-turbo-1106; actors generate utterances given director instructions and a memory-backed context composed of profile, memory and character DBs and a short-term dialogue log.",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": "Interactive drama / multi-agent dialogue generation (adapted Hedda Gabler scenario) with optional human player involvement; not a standard text-game benchmark.",
            "uses_memory": true,
            "memory_type": "Hybrid: separated memory stores — Actor Profile (static persona), Memory Database (episodic memory / past events), Character Database (interpersonal relations) — plus a working/short-term dialogue log.",
            "memory_structure": "Document-oriented store with embedding indices (vector retrieval) and TF-IDF signals; short-term dialogue log as sequential buffer; periodic summarization converts older log segments into stored memory documents (monologues).",
            "memory_content": "Character descriptions and persona, interpersonal relations and impressions, memory documents of past events/perceptions, summarized dialogue points (from earlier turns), and monologue-formatted documents (first-person perspective).",
            "memory_capacity": "Not numerically specified; bounded operationally by dialogue-log length (older turns are summarized and moved into memory) and a per-objective turn limit (e.g., forced completion after 9 turns); capacity not given as explicit token/entry count.",
            "memory_retrieval_strategy": "Retrieval ranking by a combination score: embedding (semantic similarity) + TF-IDF term-frequency + document recency; character documents retrieved when character names appear in instruction or dialogue history.",
            "memory_update_strategy": "As the plot develops, actor summarizes older dialogue turns into concise points and adds them to the memory database as documents (monologues); actor profile and character DB entries are actively updated throughout the storyline; new event/narration turns appended to dialogue log and may be summarized later.",
            "memory_usage_purpose": "Provide contextual information for generating character-consistent utterances, preserve long-term character traits and interpersonal impressions, inform director's storyline/script generation and objective checking, and reduce irrelevant repetition by summarizing context.",
            "performance_with_memory": "Full IBSEN (including monologue-formatted memory) achieved average storyline scores across 10 generated scripts: storyline logicality 2.96, storyline coherence 3.63, character consistency 2.77 (scale 1–4). Objective-check metrics reported: precision 0.82, recall 0.72, F1 0.77. (These are reported for the complete system that uses the memory design.)",
            "performance_without_memory": "Ablation removing first-person 'monologues' from memory/character DB: no short-term harm to storyline logicality reported, but a slight decrease in long-term storyline coherence and in maintaining character characteristics; exact numeric deltas for the ablation are not fully enumerated in the paper.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "First-person monologue framing (storing memory documents narrated in the character's voice) improves long-term character consistency and coherence; splitting memory into profile / memory DB / character DB avoids redundancy issues seen in monolithic memory modules; retrieval quality (embedding + TF-IDF + recency) and monologue formatting are key drivers of effective memory for role-specific outputs.",
            "memory_limitations": "Paper notes risks of memory redundancy and retrieval failure if all content is stored in a single module; memory growth can make retrieval less likely to find valid info; hallucination and LLM safety behaviors require an output revision stage; high repetition still observed despite revision; capacity and quantitative scaling behavior not specified.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2733.0",
            "source_info": {
                "paper_title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents (Park et al., 2023)",
            "brief_description": "Prior work that introduced LLM-based agents simulating human daily life with memory modules; referenced in IBSEN as a milestone for humanlike agent behavior and as an antecedent memory design.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior.",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al., 2023)",
            "agent_description": "LLM-based agents designed to simulate human daily behavior; they maintain memories of events and use retrieval to influence future actions and planning (referenced by IBSEN but not used experimentally in this paper).",
            "base_model_size": null,
            "game_benchmark_name": null,
            "game_description": "Virtual-world simulation of agents' daily activities and multi-agent interactions (not a standard text-game benchmark in IBSEN).",
            "uses_memory": true,
            "memory_type": "Monolithic memory module (as characterized in IBSEN's related-work discussion): stores relationships, dialogue history, perceptions and events.",
            "memory_structure": "Single unified memory store (document-style) as described in the cited prior work (IBSEN references this architecture and notes its limitations).",
            "memory_content": "Character relationships, dialogue history, perceptions and recorded events (per IBSEN's description of prior works).",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": "Support long-term, humanlike agent behavior and decisions by recalling past events and relations.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "IBSEN cites that the monolithic memory approach used in prior generative-agent work can lead to redundancy and reduced retrieval validity as the number of memory entries increases.",
            "memory_limitations": "Described in IBSEN as: when many memory entries accumulate, the memory becomes complex and retrieval may fail to access valid information (redundancy & retrieval degradation).",
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2733.1",
            "source_info": {
                "paper_title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Text-game LM studies",
            "name_full": "Pre-trained language model agents for text-based games (Urbanek et al., 2019; Singh et al., 2022; Xu et al., 2023)",
            "brief_description": "Prior literature using pre-trained LMs to play text-based games is cited by IBSEN as related work; these references indicate use of LMs for environment-interactive tasks but IBSEN does not evaluate on those benchmarks or detail their memory architectures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Pre-trained LM agents (text-game studies)",
            "agent_description": "Groups of works applying pre-trained language models to text-based games and interactive fiction (cited by IBSEN); these demonstrate LMs solving environment-interactive tasks but are not analyzed in detail here.",
            "base_model_size": null,
            "game_benchmark_name": "text-based games / interactive fiction (general — examples in citations include fantasy text adventure environments)",
            "game_description": "Text-adventure environments where agents must parse observations and produce textual actions to navigate, manipulate objects, and achieve goals (puzzle solving, exploration, task completion).",
            "uses_memory": null,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": null,
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2733.2",
            "source_info": {
                "paper_title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior.",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 2,
            "sanitized_title": "learning_to_speak_and_act_in_a_fantasy_text_adventure_game"
        },
        {
            "paper_title": "Pre-trained language models as prior knowledge for playing text-based games",
            "rating": 2,
            "sanitized_title": "pretrained_language_models_as_prior_knowledge_for_playing_textbased_games"
        },
        {
            "paper_title": "Humanoid agents: Platform for simulating human-like generative agents",
            "rating": 1,
            "sanitized_title": "humanoid_agents_platform_for_simulating_humanlike_generative_agents"
        }
    ],
    "cost": 0.01424225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation
1 Jul 2024</p>
<p>Senyu Han 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Li-Min Lin 
Department of Cultural Industry Management
School of Media and Communication
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhengshan Xu 
Department of Cultural Industry Management
School of Media and Communication
Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation
1 Jul 20247572FBF13F93A2F0212128525EDC872BarXiv:2407.01093v1[cs.CL]
Large language models have demonstrated their capabilities in storyline creation and humanlike character role-playing.Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline.In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable.The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective.To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent.Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama.Our codes and prompts are available at https://github.com/OpenDFM/ibsen.</p>
<p>Introduction</p>
<p>Language models learn the commonsense knowledge and reasoning ability from pre-trained text data, and they are suitable for many generation tasks such as role-playing (Wang et al., 2023b) and story creation (See et al., 2019;Wang et al., 2023a).Utilizing the knowledge stored in model parameters, using language models for story creation can rapidly and efficiently generate diverse storylines, potentially inspiring creators with new ideas.Recently, the rapid progress of large language models (LLMs) greatly improved their reasoning ability,</p>
<p>Instruct Actors</p>
<p>Figure 1: IBSEN is an agent framework that uses director and actor agents to generate controlled drama scripts.</p>
<p>and capable LLMs like ChatGPT 1 and GPT-4 (Ope-nAI, 2023) could be used to simulate human intelligence.This not only provides creators with more powerful story creation abilities via language models, but also enables LLM to vividly role-play the characters in the story.A milestone to simulate believable human behavior using LLM is Generative Agents (Park et al., 2023).It created a virtual world where LLM-based characters make their day and interact with each other.By combining LLM's ability of storyline generation and agent role-playing, creators can easily bring a vivid story setting to life.</p>
<p>However, as the previous research on LLM agents concentrates on the framework of an individual agent, the multi-agent interactions could be greatly diverse and freestyle.The events between different characters are entirely based on their natural interactions, rather than a purposeful storyline.In some cases like drama playing, characters are expected to make certain decisions in a series of scenarios to normally push the plot de-velopment forward, and the interactions between agents should be constrained.Only improving the architecture of agents hardly solves this problem, as the distributed agents lack a centralized component to regulate their behaviors.In other cases like adventure games or script role-playing games, the involvement of human players would make the plot development more unpredictable, and it is more necessary to ensure the plot develops towards the correct direction and will not deviate from the original plot too much.Currently, the control of the generation process is mainly focused on controlling individual storyline LLMs (Prabhumoye et al., 2020;Zhang et al., 2023), while the work on controlling generation in the form of character agents is still lacking.</p>
<p>To realize both storyline generation and multiagent collaboration in the drama playing field, we propose the framework of IBSEN (Interactive-Based playhouSe for agENts).IBSEN focuses on the controllable generation of drama scripts in the form of agent dialogues.Inspired by the roles of director and actor in films and theatres, we propose the director agent to serve as that centralized component.While character role-playing agents called actors have the freedom to generate responses, they are instructed by the director to make their responses follow the plot development (Figure 1).The implementation of the IBSEN framework is largely prompt-based, and one can easily construct IBSEN agents on any publicly available general LLMs without fine-tuning.Although we test and evaluate IBSEN in a drama scenario, its framework design could be expanded to broader application cases, especially agent-based interactive games that contain specialized plot objectives.</p>
<p>Our contributions are summarized as follows:</p>
<p>• We propose the IBSEN, an LLM-based framework for generating controllable drama scripts with agent characters.• Our proposed framework allows human players to participate in the plot and dynamically adjust plot details according to player actions.• We conduct preliminary quantitative and qualitative analysis of the framework's performance, verifying its effectiveness in generating drama scripts.</p>
<p>Related Work</p>
<p>Story-telling Using Language Models Previous works have attempted to create stories and control the generated plots using language models.Users could use plain text to prompt the model, or inject story knowledge into model parameters to control the generation (Wang et al., 2023a).For better story coherency, Yao et al. (2019) proposed a hierarchical story generation strategy to maintain the logic of the whole storyline.Mirowski et al. (2023) used language models to build an interactive drama script co-writing tool, and creators could directly interfere with the plot generation by prompting the model.As storylines are largely driven by the characters, controlling character scripts is an effective method to control the plot development (Dirik et al., 2021;Xu et al., 2020).From this point, IBSEN mainly uses agent-style characters to automatically build personalized drama dialogues and push the plot forward.</p>
<p>Role-playing Using Language Models Language models require character-related knowledge to output personalized responses.If the character is derived from real-world or fictional work, its documents and profiles could be collected to serve as a reference to the language model (Li et al., 2023a;Wang et al., 2023b), or expand them into a training set to fine-tune the language model (Shao et al., 2023;Lu et al., 2024).Compared to simply using LLMs to generate character's dialogues, another way to role-play is to have LLM simulate the process of human thinking in the form of agents.Prior works have tried using pre-trained language model agents to play text-based games (Urbanek et al., 2019;Singh et al., 2022;Xu et al., 2023), or using multi-agent collaboration to accomplish certain tasks (Li et al., 2023b).Park et al. (2023) proposed Generative Agents to let LLM agents vividly simulate human's daily life.Later works mainly adopted and modified this architecture to enhance or broaden the ability of LLM agents (Wang et al., 2023c;Li et al., 2023c;Yan et al., 2023).In IB-SEN, we use a specialized agent architecture to build characters in the drama.Different from previous works, the behaviors of character agents are influenced and controlled by a storyline for a collaborated dialogue script generation.</p>
<p>3 Agent Architecture  personalized responses as actual drama scripts, and player agent ( §3.3) can interact with the characters without being controlled by the director.Among these agents, director and actor are essential for generating drama scripts, and the player agent is optional for possible human involvement.</p>
<p>Figure 2 illustrates the overall framework of IB-SEN.In this example, a player agent breaks into the dialogue between two actor agents.The director agent first generates a new storyline and script to handle player's involvement, then instructs the actor agent to make a response.The entire generation process is driven by pre-defined plot objectives, and the dialogues generated during reaching these objectives form the final drama script.</p>
<p>Director Agent</p>
<p>Director agent is the central component of IB-SEN.Just like the director in reality, the director agent reads the script settings and plot objectives as the basis of storyline generation, writes dialogue scripts to fill out the detailed plot and allocate the speaking order of characters, instructs actor agents to generate appropriate responses, and checks whether the current plot has reached the plot objective.</p>
<p>Storyline &amp; Script Generation</p>
<p>In our design, we treat the development of the storyline as a process of "accomplishing a series of plot objectives".To make the generated plot develop towards an expected direction, IBSEN pro-vides the director agent a predefined objective list ⟨G 1 , G 2 , . ..⟩, and the director builds the drama script according to the plot objectives.We adopt the hierarchical method (Yao et al., 2019) to generate the detailed plot.Director agent first writes a continuation story outline S G that adheres to the current plot objective G, then translates this plot outline S G into script format ⟨ T1 , T2 , . ..⟩G for several turns.Each turn T = (r, û) includes the role r and role's expected utterance û.When generating S G , the director agent also gets the information of characters in the scene from the script settings and corresponding actor agents.This information includes character descriptions, interpersonal relations and impressions, and memories related to the current plot objective.</p>
<p>Instructing Actors</p>
<p>Directly taking the generated script ⟨ T1 , T2 , . ..⟩ as the actual drama script lacks the involvement of the actor agent.In the design of the director agent, the generated script is mainly used to determine the order of speaking roles and the content outline of the utterance.The overhead of generating the actual response u is left for the actor agent.</p>
<p>In practice, however, providing the original script turn T = (r, û) to the actor agent often encourages the actor to directly take u = û as the output unchanged.On the other side, letting the actor agent independently generate responses may lead the plot out of the objective.To seek a balance between the autonomy of the actor and the control of the director, we use an "instruction" I generated by the director to hint the actor agent only necessary information about the plot.This high-level information includes the current story outline S G , a brief synopsis of the upcoming script line Synopsis(û), and several keywords to instruct the actor how to play out the dialogue that fits the script, the plot objective G and the character r.</p>
<p>Checking Plot Objectives</p>
<p>Naturally, the director needs to know whether the existing plot has reached the current plot objective.After the character gives its response in each dialogue turn T j = (r j , u j ), director will use the dialogue history ⟨. . ., T j−1 , T j ⟩ to query the LLM, and check whether current objective G i is completed.If completed, then the plot can be moved to the next objective G i+1 , and director will start a new round of generation under that new plot objective.Otherwise, the existing plot is determined as not completed, and director will still use the script Tj+1 generated before to instruct the actor in the next turn.</p>
<p>As a reminder, the process above is based on the hypothesis that no player agent would involve the plot.In the involvement case, the director will use newly generated scripts to adapt to the influence brought by the player (discuss later in Section 3.3).</p>
<p>Actor Agent</p>
<p>Actor agent is the distributed component of IB-SEN.It mostly preserves the architecture that other human-like LLM agents would typically own, while some parts are specially modified to better suit the drama role-playing scenario.</p>
<p>Actor Profiles &amp; Databases</p>
<p>Previous environment-interactive agents (Park et al., 2023;Wang et al., 2023c;Li et al., 2023c) mainly store all character relationships, dialogue history and other perceptions in one memory module.Although this method aligns with the human memory mechanism, as the number of memory entries increases, the memory content will become more complex, potentially making retrieval unable to access valid information.To avoid this redundancy, we explicitly divide the memory into Actor Profile, Memory Database and Character Database to store different categories of contents.Actor profile stores the basic information of the character that the actor agent is role-playing (e.g.name, overall description), memory database stores the memory documents about the character's past events and perceptions, and character database stores the interpersonal relationships in the script settings.These contents will serve as references for director and actor agents to generate storylines, scripts, dialogue responses and others.</p>
<p>Inspired by human's memorizing behaviors and Chain-of-Thought (Wei et al., 2022), for documents stored in memory database and character database, we adopt a first-person narrative approach called "monologue", allowing the actor agent to interpret the content stored in the document from the perspective and voice of the character it role-plays, as shown in Figure 3.While the memory retrieval still uses the embedding of the original memory content, the actual text provided to the actor is the monologue of the document.This human-like method enhances the performance of the actor with more characteristic features of the role.As the plot develops, we expect dynamic changes in the characters' personas and impressions towards others, thus the content in the profile and character database will be actively updated throughout the storyline.</p>
<p>Dialogue Paradigm</p>
<p>IBSEN uses a narrative dialogue format to tell the actors about all the events that occurred in the scene, and these events are stored in a dialogue log L = ⟨T 1 , T 2 , . ..⟩ of the actor agent.When the actor perceives the environment events, the utterances of characters normally enter the log as a dialogue turn (r, u).Other non-dialogue events will enter the log as "Narration" turn (narration, u).Contents in L serve as the short-term memory for the direct use of actor's response generation, and would not immediately be stored in the memory database.If the length of L is too long, later turns ⟨T k+1 , . ..⟩ of the dialogue are still preserved, and earlier turns ⟨T 1 , . . ., T k ⟩ will be summarized in points P 1 , P 2 , . ... Summarized dialogue points then rejoin L as the first turn T 1 = (narration, P 1 ||P 2 || . ..).Meanwhile, actor agent builds those summary points as memory documents and respectively stores them in the memory database for further retrieval.</p>
<p>Figure 3 shows an example of maintaining the dialogue log.The new dialogue turn will be appended to the end of the log (Index −1), and the content with a smaller index indicates an older turn.We note that the oldest turn (Index −16 in this case) itself is a summary of earlier dialogues, and its summarized points have already been added to the memory database before.When summarizing the dialogue log, this oldest turn is not included to avoid producing its summarized points again.</p>
<p>When the actor is required to generate the response r, it gets characteristics from the actor profile, and retrieves related documents from memory and character databases.Memory documents are retrieved and ordered by the combination score of embedding similarity, term frequency by TF-IDF and the document recency.Character documents are retrieved once the director's instruction or the dialogue log contains the character name stored in the database.Those documents, together with director's instruction, script background and dialogue history, build up the whole context to prompt the LLM.Due to the hallucination and safety strategies of the LLM, a revision stage is required to check whether the output response is abnormal.In that case, the prompt will be respectively modified to attempt to generate a normal response.</p>
<p>Player Agent</p>
<p>IBSEN also allows human players to participate in the plot and interact with other characters.Though the player agent is controlled by humans, it can be seen as an actor agent that is not controlled by the director.The director needs to dynamically adjust the plot to accommodate player actions while maintaining the development of the storyline.</p>
<p>The roadmap of this director-actor-player collaboration is illustrated in Figure 4.For each plot objective G, director agent generates a storyline S G and the corresponding dialogue script ⟨ T1 , T2 , . ..⟩G first.Dialogue script allocates the speaking order of the characters in the scene, and for each T in the script, the director checks its speaker r and waits for the speaker's action.The speaker could be classified into three types: Narration, actor character and player character.Narration content is directly written in T , and director only needs to check whether the dialogue has reached G after the narration turn T = T .Actor character is instructed by the director, and the response turn T = (r, u) it generated still follows S G in the script.For the two types above, the direction of the plot development is under director's control, and director just needs to continue the script if the plot objective is not completed.</p>
<p>Human player characters may act and speak in an out-of-script manner, and their behaviors are hard to predict and control.In the case of player involvement, the current existing storyline and dialogue script should be immediately updated to adapt to the uncertainties brought by the player.Although the director would also generate the script content for the player character, the player does not have to follow the speaking order and content in T .It can take any action at any turn of the dialogue script.Unless the player action completes the current plot objective, it will always lead to the re-generation of current storyline and dialogue script, and the actor characters thus have the chance to react to player actions in the new storyline.</p>
<p>Experiments</p>
<p>In this section, we introduce the experiments conducted on IBSEN.We first describe our preparations for the drama script generation ( §4.1), then evaluate the performance of IBSEN under different settings and scenarios ( §4.2).Lastly, we discuss some potential findings in our experiments ( §4.3).</p>
<p>Environment Setups</p>
<p>Script Settings</p>
<p>test and evaluate the framework of IBSEN, we invite a professional theatre industry writer to create a novel interactive drama scenario that is adapted from Hedda Gabler (Ibsen, 1890), originally a drama written by Henrik Ibsen.In our script setting, Hedda Gabler, the main character of the drama, is going to hold a press conference for the death of Eilert Lövborg.Player could control a journalist named Edward Helson to participate in the press conference.Considering that the LLM may have acquired related persona knowledge about the characters in the original drama, we modified the surnames of the characters in the play to prevent LLM from directly recognizing the character it is role-playing (e.g.Hedda Gabler to Hedda Gai).A brief introduction to our script setting is included in Appendix A.</p>
<p>We adopt the theatrical terminology act in the script.A script scene is divided into several acts, each with its own settings of characters and plot objectives.Different acts are controlled by different director agents to develop the plot, and the actions of characters will only affect other characters within the same act.Appendix A shows the content and flow of the acts (7 acts, 14 objectives in total) in the script setting.</p>
<p>Frontend Implementation</p>
<p>We build a simple text-based terminal frontend to interact with IBSEN.By default, the player is the audience outside the play.At the beginning of each dialogue turn, the player can prompt the terminal to enter one of the acts, speak in the current act or just do nothing.The player can also pause the play and interview the characters for a direct conversation.This interview will not affect any agent after the play is resumed, and we can use this way to probe character images during different acts of the play.</p>
<p>Evaluation</p>
<p>In our evaluation, We use gpt-3.5-turbo-1106as the backbone LLM of IBSEN.In this open-domain generation scenario, it is hard to evaluate the output of LLM in a unified benchmark, therefore we would conduct the evaluation and analysis in both quantitative and qualitative manners.</p>
<p>Basic Storyline Generation</p>
<p>We use IBSEN to generate 10 complete drama plays, and check their dialogue logs.Each storyline would generate 5 turns of the dialogue script, and to fully unfold the plot, we set the objective checking starting from the fifth turn for each objective.Sometimes the generated storyline may go beyond the objective too much, and to prevent the plot from getting stuck on the same objective, if an objective is not completed after 9 turns of dialogue, it will be forcibly completed.Objective Completion The experiment generates 785 turns of the dialogue in total.Table 1 shows statistics of the objective completion.As the maximum script length per objective is set to 9, most of the objectives could be successfully completed within this limit, and less than 10 percent of the objectives are not completed.Besides, we use the F1 metric to evaluate the correctness of objective checking: for each objective completion judgment the director agent has made, we compare it with human judgments.We find that the failed cases are often concentrated in certain specific, complex acts, where the LLM becomes overly fixated on the wording of the objective, leading to consecutive check failures.</p>
<p>Storyline Plausibility Utilizing LLM's textannotation ability (Gilardi et al., 2023), we use ChatGPT to evaluate and score the storylines from three dimensions: storyline logicality, storyline coherence and character consistency.Each dimension is scored by ChatGPT using a scale from 1 to 4 (significant disagreement, slight disagreement, general agreement, high agreement).Table 1 shows the average scores of the generated scripts.Benefit from the LLM's ability, generated storylines reach above average performances (2.5) in all dimensions.Even though the plot develops objective by objective, the whole storyline can still keep high coherence while generally maintaining characterization.</p>
<p>Case Study During the evaluation, we find some generated dialogue scripts highly match the expectation of the creator, while some other scripts evidently misunderstand our intention.Example cases are shown in Table 2. Most of the time, IBSEN follows the route of plot objectives and generates proper storylines (Case 1), while the actor agents maintain the characteristics in the dialogue (Case 2).On the other hand, some common faults occasionally arise during the generation.Although we use the revising step to prevent the actor from generating too similar responses (re-generate when the relative Levenshtein distance is higher than 0.4), the repetition is still rather high in practice (Case 5).When generating the dialogue script, director sometimes directly uses narration turns to substitute character turns (Case 6).In some certain acts, the actor agents are overly positive in their understanding of some negative events (Case 7), which might be caused by the safety strategies of the LLM.</p>
<p>Generation with Player Involvement</p>
<p>In this scenario, we role-play the character of Edward Helson to participate in the drama play.The force complete count is still set to 9, and the player is restricted from making consecutive actions to avoid frequent storyline rebuilds that disrupt the plot's development.We let the player act differently and generate 5 complete drama plays.Evaluations in this section mainly focus on the behaviors of agents in IBSEN.</p>
<p>Qualitative Analysis When the player rarely takes actions, director will manage to rebuild the storyline to reach the objective in most cases (Case 3).However, when the player acts frequently, it will be less likely to complete the objective in limited turns.New storylines generated by director tend to actively interact with the player, which increases the sense of immersion, making the player feel like it is indeed interacting with these humanlike agents.Moreover, player involvement may influence the behaviors of actors, thereby impacting the plot development at a deeper level (Case 4).</p>
<p>Ablation Study</p>
<p>In §3.2, we introduced the framework of the actor agent based on the original generative agent.</p>
<p>To validate the effectiveness of those modifications, we conduct the ablation study of IBSEN, and check its performance when certain components are absent: a) without director's instruction to the actor, b) without monologues in the memory and character database.We generate 5 complete drama plays for each case and evaluate their storyline performances.Comparison results are shown in Figure 5.</p>
<p>As the centralized component in IBSEN, removing director's instruction will lead to a performance drop in both storyline and characteristics.Removing first-person monologues will not harm storyline logicality in the short term, but it slightly decreases coherence and characteristics in the long-term storyline.Under these three conditions, the storyline scores are relatively close, but overall, the complete   IBSEN framework demonstrates better storyline performance.</p>
<p>Discussion</p>
<p>Influence of LLM IBSEN could be implemented on other LLMs besides GPT-3.5, and different LLMs would influence the style and quality of generated storylines and dialogues.On the more capable gpt-4-turbo, IBSEN becomes more chatty and has fewer hallucinations, but problems like being too positive still exist.Although IBSEN is designed on a general LLM, we believe that fine-tuning the language models of director and actor agents with specialized corpora could achieve better role-playing performances and overcome the shortcomings of commercial LLMs.</p>
<p>Moral Values</p>
<p>In both original Hedda Gabler and our script settings, Hedda is a character who is adept at manipulating others and has moral flaws.</p>
<p>During the experiment, we found that IBSEN tends to develop the plot with more upright ethical concepts.For example, Hedda may confess to her husband that she burned Eilert's notebook, or demonstrate a resolute attitude unafraid of authority in response to Brack's threats.Even though these plots do not align with the character image, considering that more aggressive role-playing may cause harm to AI users, we believe that IBSEN could be more suitable for the script settings with fewer moral conflicts.</p>
<p>Other Applications and Implementations The idea of "controllable plot generation" can be extended to broader scenarios besides traditional drama.One of the most notable application fields is interactive gaming.For example, in communica-tion games like script role-playing and table RPG, human players need to communicate with NPCs or each other to achieve certain tasks.In IBSEN, both the game master and NPCs can be role-played by AI generative agents, providing players with flexible gaming experiences at any place or time.</p>
<p>As a backend framework, IBSEN can also be implemented in more complicated environments like video games, where players can deeper immerse in human-like interactions with NPCs through audiovisual elements.</p>
<p>Conclusion</p>
<p>We present IBSEN, a framework for LLM-based interactive and controllable drama script generation in the form of agents.We propose the role of director to monitor the plot development and instruct actor agents to generate proper responses.By the collaboration between director and actor, IBSEN enables the human player to join the interactions between agents, while maintaining the plot development at the same time.We use a drama plot outline to test IBSEN, finding its practicability in controlled dialogue script generation with player involvement.Our work can be extended to other LLMs, script settings and application scenarios, and we expect that IBSEN could eventually bring us to achieve an immersive AI plot interaction experience in the future.</p>
<p>Limitations</p>
<p>IBSEN is initially designed for text-based interactive drama, therefore we adopt a simplified architecture of generative agents to build this framework.</p>
<p>In this text environment, agent perceptions and actions are represented in the dialogue format, which constrains agents from actually interacting with entities on the drama stage.In order to achieve plot objectives in limited turns, actor agents would actively develop the plot forward and engage less in daily life behaviors, making IBSEN not so suitable for overly detailed human behavior simulation.</p>
<p>To ensure consistent management of actor agents by the director, the interaction logic of IBSEN is based on dialogue turns rather than the actual passage of time, which might affect the storyline performance of time-related script settings.We plan to improve our work by implementing IBSEN in game engines like RPG Maker or Unity, and enhance its framework to meet the requirements for agent interactions in visual interfaces.</p>
<p>Figure 2 :
2
Figure 2: Overall framework of IBSEN and an illustration of director agent's controlling process in a certain scene.Director agent 1 ⃝ writes an outline to continue the previous plot and 2 ⃝ translate it into the dialogue script, then 3 ⃝ instructs the actor agent to generate a proper response.Actor agent in the next dialogue turn 4 ⃝ generates the response, and the director agent 5 ⃝ checks whether the dialogue has reached the current plot objective.</p>
<p>Figure3: An example of maintaining the dialogue log and updating the memory database during the play.Actor agent summarizes previous contexts in points, and adds them to the memory database.Each memory content is embedded as the retrieval index of the document, and has a monologue that is interpreted by the actor.</p>
<p>Figure 4 :
4
Figure 4: An interactive roadmap showing the collaboration between director agent and characters in the scene.Actor agents' actions are instructed by the director agent to follow the storyline generated by the director.When human players involve in the storyline, director agent will actively generate new storylines to adapt to player actions.</p>
<p>Figure 5 :
5
Figure 5: Effects of IBSEN unique components on the storyline performance.We use the same metric to score the storyline from 1 to 4.</p>
<p>Act. 1-1: Place: Lobby Characters: Hedda, George Background:</p>
<p>Before the press conference commemorating Eilert Luo begins, in the hotel lobby.
StageDirectorCurrent directing: Act. 1-1, Objective. 1Write outlineGenerate script"Following Helson's intrusive"HEDDA.question, Hedda deflects theWhile Eilert's passing is indeed a profound loss, today we honor his legacy andquestion with grace andcontributions. Let's focus on his life's work and the respect he deserves in thisauthority."1moment of remembrance."2Instruct actorsCheck objective[Suddenly, journalist Edward Helson enters the lobby, seeming to eavesdrop on their"Hedda should shifts the focus from the Eilert Luo's death to celebrating his life and achievements. The keywords are: Diplomatic, Reflective, …"3"Not completed: Hedda and George have not talking the past for enough turns. " 5conversation.]Helson (Player)Mrs. Gai, could you please share some inside stories about Eilert's death?Generate responseActor GeorgeActor ProfileMemory DBCharacter DBName: Hedda GaiTop memories:To Helson: an adversary in theDescription: Hedda Gai, well-1. Hedda once had a lingeringmedia landscapeeducated and hailing from a once-ex-lover named Eilert Luo, butTo Eilert: a lost opportunity forprominent but now declining family,they eventually broke up …control and influenceis vying for the presidency …2. …To George: …Characters on the stage
To integrate story-telling and role-playing in an agent style, we introduce three types of agent architectures in IBSEN: director, actor and player.Director agent ( §3.1) creates and checks the current drama storyline, actor agent ( §3.2) generates</p>
<p>Objectives: 1. Hedda Gai chatted casually with George Dai about past events. 2</p>
<p>. Berta rushed into the lobby, informing Hedda that the press conference was about to start.3.…HeddaEdward,now is neither the time nor the place for such inquiries.We are here to honor Eilert Luo's legacy and contributions to our academic community, not to delve into personal tragedies.
Actor Hedda4Player Helson</p>
<p>Table 1 :
1
Statistics of plot objective completeness, correctness and average storyline performance on 10 basic generated scripts.For storyline performance, scores are evaluated as an integer from 1 to 4. Higher score means higher performance.
Objective Completion StatusCheck count218Force completion11Turns per objective5.61Objective Check CorrectnessPrecision0.82Recall0.72F10.77Storyline PerformanceStoryline logicality2.96Storyline coherence3.63Character consistency2.77</p>
<p>Objective: Assistant Berta informed Hedda and George that the press conference was about to start and urged them to hurry over.HEDDA: Berta, what seems to be the urgency that requires your immediate attention?BERTA: The press conference is about to start.We need to hurry over.[Hedda,with a composed demeanor, acknowledges Berta's urgency and quickly collects her belongings, signaling for George to follow suit.]GEORGE:I'll join you shortly, Hedda.I just need a moment to gather my thoughts before we face the public.Brack also hinted to Hedda that he knew the gun Eilert used for suicide was given to him by her, implying a potential threat.BRACK: Oh, but I do, Hedda.In fact, it's precisely about Eilert that I have something... intriguing to share.Did you know the pistol he used was not just any gun?It seems there's more to its origin-a detail that might pique your interest and concern.HEDDA: Brack, while I'm sure your sleuthing has been most... enlightening, I must ask you to get to the point.What exactly are you insinuating about the pistol's origins?And more importantly, why should this concern me?BRACK: It's a small world, Hedda.The origins of that pistol tie back to someone very close to you.Closer than you might have imagined.I wonder, how would the public react if they knew?But, let's not get ahead of ourselves.Perhaps we should discuss this further, in private?Objective: Assistant Berta informed Hedda and George that the press conference was about to start and urged them to hurry over.BERTA: Hedda, George, the press conference is about to begin.We need to hurry over.HELSON: W-Wait!BERTA: Apologies, Mr. Helson, but we really must proceed to the press conference now.Thank you for your understanding.HEDDA: Thank you, Berta.I am aware of the press conference and appreciate your diligence in reminding me.Listen to me.I heard a news, Hedda Gai involved the death of Eilert Luo.She gave Eilert a pistol.JOHN: I...I don't know what to think.Hedda has always been a strong advocate for the people.But if there's even a hint of truth to this, we need to uncover it.(In Act 2-1, during the press conference) MARY: Mrs. Gai, can you comment on the nature of your relationship with Eilert Luo?There are rumors circulating about a pistol being involved, and the public is eager for clarity on this matter.HEDDA: I understand the public's interest, but let us not indulge in baseless rumors and accusations.This is a time for mourning and respect, not for unfounded speculations.I urge everyone to focus on honoring Eilert's memory and legacy.George, I sympathize with your concerns, but our priority right now is to present a united front to the public.The success of ... HEDDA: George, remember the importance of projecting a strong and confident image to the public.Our unity and determination are ... HEDDA: George, remember that time in university when we executed that unforgettable prank on the Dean?It's moments like those that ...She realizes the potential impact of George Dai's decision on their shared ambitions and attempts to regain control of the situation by subtly influencing George.][The press conference quiets down as the journalists absorb the shock of George Dai's sudden announcement.][Hedda Gai, visibly surprised and struggling to maintain her composed facade, quickly regains her composure and steps forward to address the journalists.]
CaseDescriptionScript or dialogue content1Following objective(without player)2 Objective: 3 Characterized dialogue Following objective(with player)4Player's influence(In Act 1-2, before the press conference)HELSON: 5 High repetition HEDDA: 6 Too many narrations [
7Too positive Intention of the creator: Hedda should feel devastated by George's decision to take over Eilert's work.[Atthis moment of confrontation between Brack and Hedda Gai, George concludes his phone call and walks towards the two of them.]GEORGE: Uh, H-Hedda, I-I wanted to share with you that Thea Fu has prepared a research room for me at Harvard.I'm e-excited to start my work there, but uh, don't miss me too much.HEDDA: George, that's wonderful news!I'm genuinely excited for your opportunity at Harvard and the research room.Your work there will undoubtedly bring great honor to us.</p>
<p>Table 2 :
2
Case study of the storylines and scripts produced by IBSEN.Contents highlighted in purple emphasize the features in the description column.
43.633.573.543.532.962.912.972.772.702.722.5Storyline logicalityStoryline coherenceCharacter consistencyIBSENw/o instructionw/o monologue
http://www.gutenberg.org
©Gotcha Gotcha Games Inc./YOJI OJIMA 2020
AcknowledgmentsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006), The National Social Science Fund of China (21Z300604700) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).Ethics PolicyThe derivative creation of Hedda Gabler(Ibsen, 1890)in this paper is based on a public domain print edition provided by Project Gutenberg 2 , and we follow the license to use this work.We use this drama only to demonstrate the application of IBSEN.We do not endorse the aggressive and unethical behavior of characters in the play, or the use of AI to generate similar harmful content.The writing of this paper partly uses image materials provided by RPG Maker MZ 3 .A Script Background and SettingsIn this section, we introduce our adoption from Hedda Gabler, character relationships (Figure6), the full script settings (Table3) and the plot flow (Figure7).Our script is adapted from the end of Act Fourth of the original play.The adaptation is set in a modern setting while generally maintaining the original character relationships.In our script setting, Hedda Gai (originally named Hedda Gabler) is a politician who is running for president.George Dai (originally named George Tesman) is Hedda's husband and also her advocate in academia.Thea Fu (originally named Thea Elvsted) is Hedda's rival in the election.Eilert Luo (originally named Eilert Lövborg) is Hedda's former lover, George's academic rival, and also a supporter of Thea's policies in academia.The script opens with Hedda preparing for a press conference regarding Eilert's suicide, and develops the plot through a series of events occurring before, during, and after the conference.To better align the script with the setting of press conference, and also to test the characteristics of different actor agents, we introduced several journalist characters that were not present in the original script.Table3shows the settings and plot outlines of our script.You can access the adoption script at our code repository.B Prompts used in IBSENWe provide the details of the prompts (Table4) in Figure2, which showcase the main features of IBSEN.Note that these prompts are all templates, and their specific contents will be subject to change at runtime.We follow the System-User-Assistant dialogue format to construct the prompt.The LLM takes instructions from system and user as the input, and outputs its responses in the role of the assistant.Original CharactersFormer loversDestroyed Eilert's notebook; gave a pistolHedda GaiPoliticianThea FuPoliticianEdward Helson (Player)JournalistJohnJournalistMaryJournalistPeterJournalistBertaAssistantBrackChief JudgeEilert LuoAcademiaGeorge DaiAcademiaAs the idea of the IBSEN framework is written clearly in the paper, users may modify the detailed prompts for other purposes; we just provide a possible prompt design in our implementation.You may also check all the prompts used in the implementation at our code repository.The press conference finally begins.Hedda Gai and George Dai walk up to the podium together.Objectives 1. Hedda Gai began to speak, revealing her shock and sadness over Eilert Luo's suicide.2. Journalists present, including John, Peter, Mary, etc., started asking Hedda Gai questions related to Eilert Luo's suicide.Hedda Gai responds to the journalists' questions, momentarily slipping and revealing emotions towards Eilert Luo, but quickly covered it up.3-1Place Press conference hall CharactersHedda Gai, George Dai, John, Peter, Mary Background Suddenly, the press conference is filled with the sound of ringing phones.It turns out that Thea Fu has issued a statement, stating that the research conducted by Eilert Luo is a collaborative effort, and she still possesses notes from the research process, making it highly likely to reconstruct Eilert Luo's work.Thea Fu has already provided the notes to Harvard University, and Harvard hopes that George Dai, with the closest expertise, can take over the research project.Objectives 1. Thea Fu's statement caused a commotion on the scene.Journalists like John, Peter, Mary, and others deviated from their original plan of questioning Hedda Gai, and directly confronted George Dai about the possibility of taking over Eilert Luo's research -that would mean a support for Thea.Internally, George Dai knew that this research contradicted Hedda Gai's policies, but he also recognized its societal value.Upon hearing the news, he initially appeared shocked, followed by a mix of joy and uncertainty, unsure whether to take on the research.2. In an attempt to maintain a facade of tranquility, Hedda Gai had to reassure the journalists that both she and her husband only served the public interest.However, George Dai took it at face value and immediately expressed his commitment to completing the research based on Thea Fu's notes, causing a stir in the room.Journalists all have astonished by the decision of George Dai.3. Hedda Gai reluctantly announced an end to today's press conference and left the scene.At that moment, journalist Mary suddenly asked if she was pregnant.Hedda Gai, rarely displaying anger, retorted that such a question was quite inappropriate.Act 4-1Place Press conference hall CharactersJohn, Peter, Mary BackgroundThe press conference rushes to the end.Objectives 1.The journalists remained at the scene exchanged thoughts on the just-concluded press conference.They continued their discussions until staff reminded them to clear the venue, at which point the journalists departed.Act 4-2Place Press conference backstage CharactersHedda Gai, Brack Background Hedda briskly enters the backstage of the press conference, only to find Brack waiting for her.Brack intercepts Hedda, with George Dai following closely behind, intending to join their conversation.However, George's phone rings, prompting him to answer it and walk to another corner alone.Objectives 1. Brack showed disdain for the developments at the press conference.Hedda, wanting to ignore Brack, was halted when he mentioned news about Eilert Luo. 2. Brack informed Hedda that Eilert Luo's death was not honorable -he shot himself in the abdomen during suicide.Brack also hinted to Hedda that he knew the gun Eilert used for suicide was given to him by her, implying a potential threat.Act 5-1Place Press conference backstage CharactersHedda Gai, George Dai, Brack Background At this moment of confrontation between Brack and Hedda Gai, George concludes his phone call and walks towards the two of them.Objectives 1. George Dai excitedly informed Hedda that Thea Fu had prepared a research room for him and he was eager to start his research immediately.Hedda asked, "What about me?" George affectionately told her not to miss him too much.2. Brack suggested taking care of Hedda Gai during this time, with George's approval.Unable to bear it any longer, Hedda Gai forcefully pushed away George and ran offstage.Table3: Script settings in the experiment.Director writes a story outlineSystemAssuming you are currently a director, guiding a scene in a drama.Given the characters and the existing script for this scene, please first summarize what has happened in the plot so far.Then, based on the relationships and impressions between characters, you are asked to write a detailed continuation for the upcoming script.Ensure that the combined plot of the current scene and the continuation adheres to the given plot objective, and the specific content of the script is more related to the characters' images.The existing script may have partially achieved the current plot objective.You must strictly follow the requirements of the plot objective, continuing the existing script and gradually developing the plot.Be cautious not to disregard the existing script or create plot developments beyond the specified plot objective.Your generated plot guidance should be descriptive about what will happen next, without using a dialogue script format.Do not include events that have already occurred in the existing script, and refrain from prematurely generating events beyond reaching the plot objective.Characters in the plot must be in the scene.You should summarize the existing script and give the continuation for the upcoming script in JSON format.Format example: {"previous_outline": "Summary of the existing script", "new_outline": "Continuation for the upcoming script"} User Characters in the scene: {{characters}} Your plot cannot include any characters that are not in the scene.Character descriptions:{{descriptions}} Relations between characters:{{relations}} Impressions between characters:{{impressions}} The existing script:{{dialogue_history}} Please summarize the plot of the existing script first.{{background}} Performance goal in the next: {{act_goal}} Character memories related to the plot objective:{{memories}} These memories above have already occurred in the past.You should refer to them to create the outline.Based on the information above, how should the plot develop next?Provide a detailed continuation for the upcoming plot, seamlessly connecting with the previous script to make the plot and character images relevant.Ensure the entire plot progresses towards the plot objective.You should output in JSON format.Director generates the dialogue scriptSystemAssuming you are currently a director, guiding a scene in a drama.Given the characters and the outline of the upcoming plot for this scene, please translate the upcoming plot outline into script format for up to {{num_lines}} lines, ensuring that it follows the storyline and seamlessly connects with the preceding script.You can gradually develop the script, enriching the details based on the upcoming plot outline.If you manage to cover all the outlined events before reaching {{num_lines}} lines, you can end your writing.Make sure your continuation smoothly integrates with the existing script.Use character dialogues to replace Narration wherever possible.You should output the script continuation in JSON format.Each line of the script includes the speaker "role" and his/her utterance "content".The speaker can only be chosen from Narration or one of the characters in the scene.Format example: {"scripts": [{"role": "Speaker 1", "content": "..."}, {"role": "Speaker 2", "content": "..."}, {"role": "Narration", "content": "..."}, ...]} User Characters in the scene: {{characters}} Relations between characters:{{relations}} Existing plot outline:{{prev_outline}} {{background}} Upcoming plot outline:{{act_outline}} Based on the above information, please translate the upcoming plot outline into script format up to {{num_lines}} lines in JSON format.Ensure that the extended script seamlessly integrates with the existing one and follows the upcoming plot outline.Note that the speaker can only be Narration or one of the characters in this scene.Use character dialogues to replace Narration wherever possible.Director instructs the actorSystemAssuming you are currently a director, guiding a scene in a drama.Given the characters, the plot objective of this scene and the existing script, please provide a brief synopsis of the upcoming line for the actor.However, do not directly provide the original script line.Then, use keywords to instruct the actor on how to role-play the character in the next line, so that the actor can play out the dialogue that fits the script, the characterization and the plot objective.UserCharacters in the scene: {{characters}} Relations between characters: {{relations}} Existing script: {{dialogue_history}} {{background}} Plot objective of this scene: {{act_goal}} According to the script, the character of the following line is {{actor_name}}, and the line content is: {{content}}.However, do not directly provide the original line for the actor that is role-playing this character.Description of the {{actor_name}}: {{description}} Based on the above information, please provide a brief synopsis of the upcoming line for the actor, but do not directly provide the original script line.Then, generate several keywords to instruct the actor how to play out the dialogue that fits the script, the plot objective and the characteristics of {{actor_name}}.Actor generates the responseSystemAssuming you are currently an actor performing in a drama play.Your role is {{name}}.Background of the drama script: {{background}} Character description for {{name}}: {{description}} Based on the information above, I will tell you the script that has unfolded so far in the play.Please role-play as {{name}} and respond with an appropriate line of the dialogue.
Controlled cue generation for play scripts. Alara Dirik, Hilal Donmez, Pinar Yanardag, arXiv:2112.069532021arXiv preprint</p>
<p>Chatgpt outperforms crowd-workers for textannotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, arXiv:2303.150562023arXiv preprint</p>
<p>. Henrik Ibsen, 1890Hedda Gabler</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, M I Weishi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Chatharuhi: Reviving anime character in reality via large language model. 2023aarXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Thirtyseventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, arXiv:2310.06500Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. 2023carXiv preprint</p>
<p>Large language models are superpositions of all characters: Attaining arbitrary role-play via selfalignment. Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou, arXiv:2401.124742024arXiv preprint</p>
<p>Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals. Piotr Mirowski, Kory W Mathewson, Jaylen Pittman, Richard Evans, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Exploring controllable text generation techniques. Shrimai Prabhumoye, Alan W Black, Ruslan Salakhutdinov, 10.18653/v1/2020.coling-main.1Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Do massively pretrained language models make better storytellers?. Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, Christopher D Manning, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)2019</p>
<p>Character-llm: A trainable agent for roleplaying. Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems. the 21st International Conference on Autonomous Agents and Multiagent Systems2022</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Open-world story generation with structured knowledge enhancement: A comprehensive survey. Yuxin Wang, Jieru Lin, Zhiwei Yu, Wei Hu, Börje F Karlsson, Neurocomputing. 1267922023a</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023barXiv preprint</p>
<p>Humanoid agents: Platform for simulating human-like generative agents. Zhilin Wang, Yu Ying Chiu, Yu Cheung Chiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2023c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Controllable multi-character psychology-oriented story generation. Feifei Xu, Xinpeng Wang, Yunpu Ma, Yuyi Volker Tresp, Wang, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020Shanlin Zhou, and Haizhou Du</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582023arXiv preprint</p>
<p>Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, Ji Yan, arXiv:2312.17653Larp: Language-agent role play for open-world games. 2023arXiv preprint</p>
<p>Planand-write: Towards better automatic storytelling. Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, Rui Yan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>A survey of controllable text generation using transformer-based pre-trained language models. Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, Dawei Song, ACM Computing Surveys. 5632023</p>            </div>
        </div>

    </div>
</body>
</html>
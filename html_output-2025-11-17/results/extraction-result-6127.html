<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6127 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6127</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6127</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-bd5deadc58ee45b5e004378ba1d54a96bc947b4a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bd5deadc58ee45b5e004378ba1d54a96bc947b4a" target="_blank">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> An automated model is introduced that estimates FACTSCORE using retrieval and a strong language model, and is used to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings.</p>
                <p><strong>Paper Abstract:</strong> Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6127.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6127.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FActScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FActScore (Factual precision in Atomicity Score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-grained evaluation metric that decomposes long-form LM generations into atomic facts and computes the fraction of those atomic facts that are supported by a designated knowledge source (e.g., Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Decompose each generation into atomic facts; label each atomic fact as Supported / Not-supported / Irrelevant against a specified knowledge source; aggregate by computing the mean fraction of supported facts per response.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary support label per atomic fact (Supported vs Not-supported) with Irrelevant filtering; aggregated metric = fraction of atomic facts labeled Supported (FActScore). Secondary metrics include abstention rate and average number of atomic facts per response.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to InstructGPT (text-davinci-003), ChatGPT, PerplexityAI; later used to compare GPT-4, Alpaca, Vicuna, Dolly, Oasst-pythia, StableLM, MPT-Chat and others.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Originally applied to people biographies (verifiable factual claims) using Wikipedia as the knowledge source; authors propose applicability to other domains including news and scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Not a scientific theory itself; a framework for evaluating factual precision of LM-generated statements, suitable for evaluating generated theories by verifying constituent factual claims against a reliable corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human-evaluated FActScores: InstructGPT 42.5%, ChatGPT 58.3%, PerplexityAI 71.5%. The automated estimator approximates human FActScore with <2% error rate in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>English Wikipedia (primary knowledge source), Wikidata (sampling entities), DBPedia (human-written biographies), small proof-of-concept on ACL Anthology for NLP summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human judgement is used as ground truth; human labeling costs ≈ $4 per generation and yields high inter-annotator agreement (88–96% depending on model). The automated estimator closely matches human FActScore (error <2%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Focuses on precision (supported facts) and does not measure recall/coverage; dependent on the chosen knowledge source (coverage and consistency). Judging support can be debatable for nuanced or inferential facts; cost/time for human annotation is high.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6127.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atomic Fact Decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atomic fact decomposition (unitization into atomic facts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Protocol to split long-form text into short statements each conveying a single piece of information (atomic facts) used as the primitive unit for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic sentence splitting followed by LM-assisted (InstructGPT) or human editing to break sentences into atomic facts; annotators then label each atomic fact against the knowledge source.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Each atomic fact is judged independently as Supported / Not-supported / Irrelevant; atomic facts are weighted equally in aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>InstructGPT used to propose atomic fact splits; human annotators revise splits.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Applied to people biographies (verifiable factual content) but generalizable to any long-form generation where atomic claims can be identified (including scientific claims).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A method to transform complex multi-assertion text into atomic assertions so that factual verification can be done at fine granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>In practice, InstructGPT's automatic splits required human edits in ~18% (split) and ~34% (merge) of cases; average facts per response vary by model (26–41).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Applied to LM-generated biography texts sampled from Wikidata entities with Wikipedia pages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human editors refine LM-produced atomic facts; inter-annotator agreement on labeling atomic facts was high (overall ~91% aggregate; per-model 88–96%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Defining atomicity can be subjective; over- or under-splitting affects subsequent verification; some facts depend on context or other facts leading to 'Irrelevant' labels if parent facts unsupported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6127.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human annotation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotation pipeline for FActScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step human annotation process: sample entities, collect LM generations, produce atomic facts (LM+human), and label each atomic fact against Wikipedia as Supported / Not-supported / Irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Stepwise pipeline: sample 183 Wikidata human entities; generate bios from evaluated LMs; auto-split sentences via InstructGPT and have annotators revise atomic facts; separate annotators label each atomic fact for support against Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Supported / Not-supported / Irrelevant labels per atomic fact; also measure % responding (abstentions), #tokens, #sentences, #facts per response, and inter-annotator agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>InstructGPT used for atomic fact proposals; evaluated LMs include InstructGPT, ChatGPT, PerplexityAI.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>People biographies (objective, verifiable domain); authors chose this domain to satisfy assumptions about knowledge-source consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Procedure to produce human-ground-truth FActScores for LM outputs by validating each atomic claim against Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Cost ≈ $4 per generation; agreement rates when two annotators labeled same generation: InstructGPT 96%, ChatGPT 90%, PerplexityAI 88%; found substantial unsupported content in evaluated LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Wikidata (entity sampling), English Wikipedia (knowledge source), DBPedia (human-written bios for larger-scale comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Human labeling is the gold standard in the paper; automated estimators are compared against this human ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Time-consuming and costly; some disagreements arise from debatability, inference, or ambiguous wording; not scalable for large evaluation sets without automated estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6127.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated Estimator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated estimator of FActScore (retrieval + LM +/- NP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based pipeline to predict FActScore automatically by (1) using atomic facts and (2) validating each via retrieval-augmented LM prompting and/or a nonparametric probability model, optionally ensembling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Break generation into atomic facts (using InstructGPT outputs), retrieve evidence passages from the knowledge source (k=5 from the entity page), and prompt an LM (Inst-LLAMA 7B or ChatGPT) to judge True/False (TF prompting). Optionally compute nonparametric probabilities (NPM) over masked tokens and ensemble decisions (Supported only if both methods support).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Aggregate per-atomic-fact predictions into estimated FActScore; evaluate estimator via Error Rate (absolute difference to human FActScore) and segment-level F1_MICRO (per-atomic-fact Not-supported detection).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Evaluators used: Inst-LLAMA (7B), LLAMA 65B, and ChatGPT as LM_EVAL; retrievers: GTR (large/xlarge), BM25.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Applied to people biography verification using Wikipedia; authors show a small proof-of-concept application to NLP paper summaries using ACL Anthology (indicative for scientific text).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Automated approximation of human FActScore that validates atomic claims using retrieved evidence plus LM judgment and/or nonparametric likelihood estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Best estimators achieved low system-level Error Rate (<2% reported overall) and strong ranking fidelity across LMs (Pearson r = 0.99 between two estimator variants over 13 subjects). Retrieve→LM + NP ensemble often reduces bias; best variant depends on the evaluated LM (LLAMA+NP best for InstructGPT/ChatGPT, ChatGPT best for PerplexityAI).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>English Wikipedia (04/01/2023 snapshot) as retrieval corpus; evaluation set = 183 entities (human labels) and larger unlabeled set of 6,500 generations across 13 LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Estimator approximates human FActScore closely in aggregate (ER <2%); segment-level decisions less perfect (F1_MICRO varies by method and evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Estimator performance depends on LM_EVAL choice and retrieval quality; retrieval sometimes fails to return direct evidence (≈70% of some error cases); some estimators biased toward over/underestimation; per-atomic-fact accuracy lags system-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6127.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieve→LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieve then LM prompting (Retrieve→LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validation variant that retrieves k passages relevant to the atomic fact from the knowledge source and concatenates them with the fact in a TF prompt to an LM_EVAL to decide Supported vs Not-supported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Retrieve top-k passages (k=5) from the entity's Wikipedia page using a dense retriever (GTR) or BM25, concatenate retrieved passages with the atomic fact and the prompt 'True or False?', then query an LM_EVAL to get a probability/answer.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-atomic-fact binary decision; aggregated into FActScore; evaluated with Error Rate and F1_MICRO.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LM_EVALs used: Inst-LLAMA 7B, LLAMA 65B, ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Used on people biographies (Wikipedia) and shown useful for other domains in small-scale checks (e.g., summaries of NLP papers against ACL Anthology).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Contextualizes the LM's factual judgment by providing retrieved evidence to reduce reliance on parametric memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Retrieval significantly improves per-fact detection (F1_MICRO) and reduces estimator Error Rate compared to no-context prompting; however, Retrieve→LM can overestimate FActScore in some settings (biased toward Supported).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Retriever candidates from English Wikipedia (per-topic page split into passages).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Retrieve→LM predictions correlate better with human labels than no-context prompting; combining with NP improves aggregated accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends on retrieval returning direct, relevant passages; distracting passages can mislead the LM; retrieval restricted to entity page may miss page-level evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6127.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NP / NPM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nonparametric Probability (NPM / NP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-free nonparametric likelihood method: mask each token in the atomic fact, compute token likelihoods using a nonparametric masked LM (NPM), average these, and threshold to decide support.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nonparametric masked language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute a nonparametric likelihood score for the atomic fact by masking tokens and scoring them with a retrieval-backed nonparametric model; threshold the averaged score to decide Supported vs Not-supported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-atomic-fact Supported / Not-supported decisions based on likelihood threshold (authors used threshold 0.3); F1_MICRO and Error Rate for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Method from Min et al. (2023) implemented alongside LM-based evaluators; used as an ensemble component (LLM+NP).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Applied to biography verification; conceptually applicable to other factual domains including scientific claims where retrieval may be noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A nonparametric statistical check of how likely a fact is given the corpus distribution, complementary to LM-based judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>NP alone yields competitive F1_MICRO for some models and, when ensembled with Retrieve→LM, often reduces overestimation bias and improves aggregate Error Rate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>NPM implementation used BM25 for passage selection in the original method; in this paper NP used NPM with English Wikipedia and threshold 0.3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>NP helps bring ensemble predictions closer to human-annotated aggregate FActScore by counteracting LM overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>NP depends on nonparametric masking and corpus coverage; may perform poorly when facts require page-level inference rather than local token likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6127.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieve→LM + NP ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble of Retrieve→LM and Nonparametric Probability (Retrieve→LM + NP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble that predicts Supported only if both Retrieve→LM and NP predict Supported, combining strengths of retrieval-augmented LM judgment and nonparametric likelihood to reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run both Retrieve→LM and NP on each atomic fact; label Supported only if both methods agree; otherwise label Not-supported. Aggregate to estimate FActScore.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Aggregate FActScore (system-level Error Rate) and per-fact F1_MICRO are used to evaluate ensemble performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LM_EVALs: Inst-LLAMA 7B, LLAMA 65B, ChatGPT; NP based on NPM.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Applied to people biographies; recommended as a robust estimator for long-form factual verification, and potentially transferable to scientific claim verification.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>A conservative ensemble that reduces false positives (overestimating support) by requiring agreement between a contextual LM judgment and a corpus-derived nonparametric signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Ensembling often reduces Error Rate compared to Retrieve→LM alone (notably for InstructGPT/ChatGPT cases), at the cost of sometimes underestimating FActScore for models closer to human-written text or for PerplexityAI.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated over human-labeled dataset (183 entities) and 6,500 unlabeled generations across 13 LMs using Wikipedia as retrieval corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Ensemble predictions maintain correct ranking across LMs and approach human aggregate scores more closely in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ensemble can under-estimate when NP is overly conservative or when retrieval+LM is correct but NP lacks sensitivity; best variant depends on the evaluated LM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6127.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error Rate (ER) & F1_MICRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Error Rate (ER) and F1_MICRO evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary evaluation metrics: ER measures system-level absolute difference between estimated and human FActScore; F1_MICRO measures segment-level per-atomic-fact detection performance (Not-supported detection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>ER = |estimated FActScore - human FActScore| aggregated over the evaluation set. F1_MICRO = micro F1 for the binary Not-supported class across atomic facts (precision/recall on predicted Not-supported facts).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>ER emphasizes system-level aggregate fidelity and correct system ranking; F1_MICRO emphasizes segment-level decision accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Applied to outputs of InstructGPT, ChatGPT, PerplexityAI and other evaluated LMs; used to compare estimator variants (Inst-LLAMA, LLAMA 65B, ChatGPT as LM_EVALs).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Designed for fine-grained factual verification in long-form generation; applicable to scientific claim verification by measuring both aggregate alignment and per-claim detection.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation framework to measure how well automatic estimators reproduce human judgments at both system and segment levels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Some estimators achieve low ER (<2% reported); F1_MICRO varies more widely (best Retrieve→LM+NP with Inst-LLAMA or ChatGPT reached F1_MICRO up to ~83% for some configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Human-labeled FActScore dataset (183 entities) and larger unlabeled set used for system comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>ER directly quantifies difference from human aggregate scores; F1_MICRO reports per-claim agreement rates with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>A method that optimizes F1_MICRO can still be biased (over- or underestimating) leading to poor ER, and vice versa; both metrics are needed for full picture.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6127.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TF Prompting vs QA Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>True/False (TF) Prompting versus Question-Answer (QA) Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting styles compared for atomic-fact validation: TF prompting directly asks the evaluator LM 'True or False?' about a fact, while QA prompting asks the LM to generate a question and expected answer then answer that question; TF prompting was empirically superior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>TF prompting: provide atomic fact + 'True or False?' and parse LM logits/answer. QA prompting: generate a question and expected answer from the fact, then query LM for an answer and compare equality; considered prone to vagueness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-atomic-fact Supported/Not-supported accuracy (F1_MICRO) and downstream effect on ER.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Evaluated with Inst-LLAMA 7B as LM_EVAL in ablations; results similar with other LM_EVALs.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Verification of factual claims in long-form text; approach generalizable to scientific claims verification.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Two prompting paradigms for LM-based fact verification; TF prompting yields more reliable direct binary judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>TF prompting significantly outperformed QA prompting across methods and LMs (notably TF Prompting + Retrieve→LM achieved higher F1_MICRO than QA prompting variants).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on the human-labeled biography dataset; retrieval from Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>TF prompting more closely aligns with human binary support judgments than QA prompting, which often produced vague/ambiguous questions leading to false negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>TF prompting relies on LM internals (logits) when available; when logits unavailable, output string detection (presence of 'True'/'False') is used and can be less robust.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6127.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-check LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-check LM (Manakul et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that validates facts by conditioning an LM_EVAL on multiple different outputs from the evaluated LM (LM_SUBJ) and aggregating judgments; zero-resource black-box hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate multiple samples from the LM_SUBJ, for each sample prompt the LM_EVAL to judge the fact, then aggregate (majority vote) to detect hallucinations. In this paper used as an additional baseline (requires LM_SUBJ access and nondeterministic outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-atomic-fact Not-supported detection (F1_MICRO) and effect on estimated FActScore.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Compared with Inst-LLAMA 7B as LM_EVAL; method requires access to LM_SUBJ to produce multiple outputs (not applicable to PerplexityAI semi-deterministic outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Mentioned as a concurrent baseline in hallucination detection; relevant to verification of factual claims including scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Black-box approach leveraging LM_SUBJ's output variability to detect model uncertainty/hallucination without external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Self-check LM outperforms no-context LM by a few percent on F1_MICRO but underperforms retrieval-based methods; not applicable to some models lacking nondeterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Evaluated on the same biography dataset as baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Improves over naive baselines but is inferior to retrieval-augmented approaches when strong retrieval is available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to LM_SUBJ and nondeterministic outputs; does not use external knowledge so limited when facts are outside LM's parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6127.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6127.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retriever systems (GTR, BM25)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Passage retrieval systems: GTR (Generalizable T5-based Retriever) and BM25 (Pyserini)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Passage retrieval backends used to provide context to LM_EVAL: dense retrievers (GTR variants) and traditional sparse retrieval (BM25) were compared and found to give similar results for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large dual encoders are generalizable retrievers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Retrieve top-k passages from the topic entity's Wikipedia page (page-scope retrieval) using GTR Large/xLarge or BM25; supply retrieved passages to Retrieve→LM pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-atomic-fact decision accuracy (F1_MICRO) and downstream estimator Error Rate.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Retriever is model-agnostic; used with Inst-LLAMA, LLAMA 65B, ChatGPT as LM_EVALs.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Retrieval of factual evidence from Wikipedia for biography verification; analogous retrieval pipelines can be built over scientific literature corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Retrievers aim to surface passages that contain direct evidence to support or contradict atomic facts to improve LM-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>All retrieval systems performed roughly equivalently in the paper (GTR xLarge slightly best in some settings); retrieval significantly improves validation performance over no-context baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>English Wikipedia split into passages (up to 256 tokens) used as retrieval corpus; GTR models and BM25 (Pyserini) compared.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Retrieval-augmented validation better matches human labels than no-context LM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval often fails to return direct evidence (≈70% of the estimator's error cases were due to lack of direct evidence in retrieved passages); some verification requires page-level or broader context beyond single passages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>FEVER: a large-scale dataset for fact extraction and VERification <em>(Rating: 2)</em></li>
                <li>SciFact-open: Towards open-domain scientific claim verification <em>(Rating: 2)</em></li>
                <li>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Generating fact checking briefs <em>(Rating: 1)</em></li>
                <li>Generating scientific claims for zeroshot scientific fact checking <em>(Rating: 2)</em></li>
                <li>Nonparametric masked language modeling <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6127",
    "paper_id": "paper-bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "FActScore",
            "name_full": "FActScore (Factual precision in Atomicity Score)",
            "brief_description": "A fine-grained evaluation metric that decomposes long-form LM generations into atomic facts and computes the fraction of those atomic facts that are supported by a designated knowledge source (e.g., Wikipedia).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Decompose each generation into atomic facts; label each atomic fact as Supported / Not-supported / Irrelevant against a specified knowledge source; aggregate by computing the mean fraction of supported facts per response.",
            "evaluation_criteria": "Binary support label per atomic fact (Supported vs Not-supported) with Irrelevant filtering; aggregated metric = fraction of atomic facts labeled Supported (FActScore). Secondary metrics include abstention rate and average number of atomic facts per response.",
            "llm_model_name": "Applied to InstructGPT (text-davinci-003), ChatGPT, PerplexityAI; later used to compare GPT-4, Alpaca, Vicuna, Dolly, Oasst-pythia, StableLM, MPT-Chat and others.",
            "theory_domain": "Originally applied to people biographies (verifiable factual claims) using Wikipedia as the knowledge source; authors propose applicability to other domains including news and scientific literature.",
            "theory_description": "Not a scientific theory itself; a framework for evaluating factual precision of LM-generated statements, suitable for evaluating generated theories by verifying constituent factual claims against a reliable corpus.",
            "evaluation_results": "Human-evaluated FActScores: InstructGPT 42.5%, ChatGPT 58.3%, PerplexityAI 71.5%. The automated estimator approximates human FActScore with &lt;2% error rate in aggregate.",
            "benchmarks_or_datasets": "English Wikipedia (primary knowledge source), Wikidata (sampling entities), DBPedia (human-written biographies), small proof-of-concept on ACL Anthology for NLP summaries.",
            "comparison_to_human": "Human judgement is used as ground truth; human labeling costs ≈ $4 per generation and yields high inter-annotator agreement (88–96% depending on model). The automated estimator closely matches human FActScore (error &lt;2%).",
            "limitations_or_challenges": "Focuses on precision (supported facts) and does not measure recall/coverage; dependent on the chosen knowledge source (coverage and consistency). Judging support can be debatable for nuanced or inferential facts; cost/time for human annotation is high.",
            "uuid": "e6127.0",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Atomic Fact Decomposition",
            "name_full": "Atomic fact decomposition (unitization into atomic facts)",
            "brief_description": "Protocol to split long-form text into short statements each conveying a single piece of information (atomic facts) used as the primitive unit for verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Automatic sentence splitting followed by LM-assisted (InstructGPT) or human editing to break sentences into atomic facts; annotators then label each atomic fact against the knowledge source.",
            "evaluation_criteria": "Each atomic fact is judged independently as Supported / Not-supported / Irrelevant; atomic facts are weighted equally in aggregation.",
            "llm_model_name": "InstructGPT used to propose atomic fact splits; human annotators revise splits.",
            "theory_domain": "Applied to people biographies (verifiable factual content) but generalizable to any long-form generation where atomic claims can be identified (including scientific claims).",
            "theory_description": "A method to transform complex multi-assertion text into atomic assertions so that factual verification can be done at fine granularity.",
            "evaluation_results": "In practice, InstructGPT's automatic splits required human edits in ~18% (split) and ~34% (merge) of cases; average facts per response vary by model (26–41).",
            "benchmarks_or_datasets": "Applied to LM-generated biography texts sampled from Wikidata entities with Wikipedia pages.",
            "comparison_to_human": "Human editors refine LM-produced atomic facts; inter-annotator agreement on labeling atomic facts was high (overall ~91% aggregate; per-model 88–96%).",
            "limitations_or_challenges": "Defining atomicity can be subjective; over- or under-splitting affects subsequent verification; some facts depend on context or other facts leading to 'Irrelevant' labels if parent facts unsupported.",
            "uuid": "e6127.1",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Human annotation pipeline",
            "name_full": "Human annotation pipeline for FActScore",
            "brief_description": "A multi-step human annotation process: sample entities, collect LM generations, produce atomic facts (LM+human), and label each atomic fact against Wikipedia as Supported / Not-supported / Irrelevant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Stepwise pipeline: sample 183 Wikidata human entities; generate bios from evaluated LMs; auto-split sentences via InstructGPT and have annotators revise atomic facts; separate annotators label each atomic fact for support against Wikipedia.",
            "evaluation_criteria": "Supported / Not-supported / Irrelevant labels per atomic fact; also measure % responding (abstentions), #tokens, #sentences, #facts per response, and inter-annotator agreement.",
            "llm_model_name": "InstructGPT used for atomic fact proposals; evaluated LMs include InstructGPT, ChatGPT, PerplexityAI.",
            "theory_domain": "People biographies (objective, verifiable domain); authors chose this domain to satisfy assumptions about knowledge-source consistency.",
            "theory_description": "Procedure to produce human-ground-truth FActScores for LM outputs by validating each atomic claim against Wikipedia.",
            "evaluation_results": "Cost ≈ $4 per generation; agreement rates when two annotators labeled same generation: InstructGPT 96%, ChatGPT 90%, PerplexityAI 88%; found substantial unsupported content in evaluated LMs.",
            "benchmarks_or_datasets": "Wikidata (entity sampling), English Wikipedia (knowledge source), DBPedia (human-written bios for larger-scale comparisons).",
            "comparison_to_human": "Human labeling is the gold standard in the paper; automated estimators are compared against this human ground truth.",
            "limitations_or_challenges": "Time-consuming and costly; some disagreements arise from debatability, inference, or ambiguous wording; not scalable for large evaluation sets without automated estimators.",
            "uuid": "e6127.2",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Automated Estimator",
            "name_full": "Automated estimator of FActScore (retrieval + LM +/- NP)",
            "brief_description": "A model-based pipeline to predict FActScore automatically by (1) using atomic facts and (2) validating each via retrieval-augmented LM prompting and/or a nonparametric probability model, optionally ensembling methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Break generation into atomic facts (using InstructGPT outputs), retrieve evidence passages from the knowledge source (k=5 from the entity page), and prompt an LM (Inst-LLAMA 7B or ChatGPT) to judge True/False (TF prompting). Optionally compute nonparametric probabilities (NPM) over masked tokens and ensemble decisions (Supported only if both methods support).",
            "evaluation_criteria": "Aggregate per-atomic-fact predictions into estimated FActScore; evaluate estimator via Error Rate (absolute difference to human FActScore) and segment-level F1_MICRO (per-atomic-fact Not-supported detection).",
            "llm_model_name": "Evaluators used: Inst-LLAMA (7B), LLAMA 65B, and ChatGPT as LM_EVAL; retrievers: GTR (large/xlarge), BM25.",
            "theory_domain": "Applied to people biography verification using Wikipedia; authors show a small proof-of-concept application to NLP paper summaries using ACL Anthology (indicative for scientific text).",
            "theory_description": "Automated approximation of human FActScore that validates atomic claims using retrieved evidence plus LM judgment and/or nonparametric likelihood estimates.",
            "evaluation_results": "Best estimators achieved low system-level Error Rate (&lt;2% reported overall) and strong ranking fidelity across LMs (Pearson r = 0.99 between two estimator variants over 13 subjects). Retrieve→LM + NP ensemble often reduces bias; best variant depends on the evaluated LM (LLAMA+NP best for InstructGPT/ChatGPT, ChatGPT best for PerplexityAI).",
            "benchmarks_or_datasets": "English Wikipedia (04/01/2023 snapshot) as retrieval corpus; evaluation set = 183 entities (human labels) and larger unlabeled set of 6,500 generations across 13 LMs.",
            "comparison_to_human": "Estimator approximates human FActScore closely in aggregate (ER &lt;2%); segment-level decisions less perfect (F1_MICRO varies by method and evaluator).",
            "limitations_or_challenges": "Estimator performance depends on LM_EVAL choice and retrieval quality; retrieval sometimes fails to return direct evidence (≈70% of some error cases); some estimators biased toward over/underestimation; per-atomic-fact accuracy lags system-level agreement.",
            "uuid": "e6127.3",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Retrieve→LM",
            "name_full": "Retrieve then LM prompting (Retrieve→LM)",
            "brief_description": "A validation variant that retrieves k passages relevant to the atomic fact from the knowledge source and concatenates them with the fact in a TF prompt to an LM_EVAL to decide Supported vs Not-supported.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Retrieve top-k passages (k=5) from the entity's Wikipedia page using a dense retriever (GTR) or BM25, concatenate retrieved passages with the atomic fact and the prompt 'True or False?', then query an LM_EVAL to get a probability/answer.",
            "evaluation_criteria": "Per-atomic-fact binary decision; aggregated into FActScore; evaluated with Error Rate and F1_MICRO.",
            "llm_model_name": "LM_EVALs used: Inst-LLAMA 7B, LLAMA 65B, ChatGPT.",
            "theory_domain": "Used on people biographies (Wikipedia) and shown useful for other domains in small-scale checks (e.g., summaries of NLP papers against ACL Anthology).",
            "theory_description": "Contextualizes the LM's factual judgment by providing retrieved evidence to reduce reliance on parametric memorization.",
            "evaluation_results": "Retrieval significantly improves per-fact detection (F1_MICRO) and reduces estimator Error Rate compared to no-context prompting; however, Retrieve→LM can overestimate FActScore in some settings (biased toward Supported).",
            "benchmarks_or_datasets": "Retriever candidates from English Wikipedia (per-topic page split into passages).",
            "comparison_to_human": "Retrieve→LM predictions correlate better with human labels than no-context prompting; combining with NP improves aggregated accuracy.",
            "limitations_or_challenges": "Performance depends on retrieval returning direct, relevant passages; distracting passages can mislead the LM; retrieval restricted to entity page may miss page-level evidence.",
            "uuid": "e6127.4",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "NP / NPM",
            "name_full": "Nonparametric Probability (NPM / NP)",
            "brief_description": "A retrieval-free nonparametric likelihood method: mask each token in the atomic fact, compute token likelihoods using a nonparametric masked LM (NPM), average these, and threshold to decide support.",
            "citation_title": "Nonparametric masked language modeling",
            "mention_or_use": "use",
            "evaluation_method": "Compute a nonparametric likelihood score for the atomic fact by masking tokens and scoring them with a retrieval-backed nonparametric model; threshold the averaged score to decide Supported vs Not-supported.",
            "evaluation_criteria": "Per-atomic-fact Supported / Not-supported decisions based on likelihood threshold (authors used threshold 0.3); F1_MICRO and Error Rate for evaluation.",
            "llm_model_name": "Method from Min et al. (2023) implemented alongside LM-based evaluators; used as an ensemble component (LLM+NP).",
            "theory_domain": "Applied to biography verification; conceptually applicable to other factual domains including scientific claims where retrieval may be noisy.",
            "theory_description": "A nonparametric statistical check of how likely a fact is given the corpus distribution, complementary to LM-based judgment.",
            "evaluation_results": "NP alone yields competitive F1_MICRO for some models and, when ensembled with Retrieve→LM, often reduces overestimation bias and improves aggregate Error Rate.",
            "benchmarks_or_datasets": "NPM implementation used BM25 for passage selection in the original method; in this paper NP used NPM with English Wikipedia and threshold 0.3.",
            "comparison_to_human": "NP helps bring ensemble predictions closer to human-annotated aggregate FActScore by counteracting LM overconfidence.",
            "limitations_or_challenges": "NP depends on nonparametric masking and corpus coverage; may perform poorly when facts require page-level inference rather than local token likelihoods.",
            "uuid": "e6127.5",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Retrieve→LM + NP ensemble",
            "name_full": "Ensemble of Retrieve→LM and Nonparametric Probability (Retrieve→LM + NP)",
            "brief_description": "An ensemble that predicts Supported only if both Retrieve→LM and NP predict Supported, combining strengths of retrieval-augmented LM judgment and nonparametric likelihood to reduce bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Run both Retrieve→LM and NP on each atomic fact; label Supported only if both methods agree; otherwise label Not-supported. Aggregate to estimate FActScore.",
            "evaluation_criteria": "Aggregate FActScore (system-level Error Rate) and per-fact F1_MICRO are used to evaluate ensemble performance.",
            "llm_model_name": "LM_EVALs: Inst-LLAMA 7B, LLAMA 65B, ChatGPT; NP based on NPM.",
            "theory_domain": "Applied to people biographies; recommended as a robust estimator for long-form factual verification, and potentially transferable to scientific claim verification.",
            "theory_description": "A conservative ensemble that reduces false positives (overestimating support) by requiring agreement between a contextual LM judgment and a corpus-derived nonparametric signal.",
            "evaluation_results": "Ensembling often reduces Error Rate compared to Retrieve→LM alone (notably for InstructGPT/ChatGPT cases), at the cost of sometimes underestimating FActScore for models closer to human-written text or for PerplexityAI.",
            "benchmarks_or_datasets": "Evaluated over human-labeled dataset (183 entities) and 6,500 unlabeled generations across 13 LMs using Wikipedia as retrieval corpus.",
            "comparison_to_human": "Ensemble predictions maintain correct ranking across LMs and approach human aggregate scores more closely in many settings.",
            "limitations_or_challenges": "Ensemble can under-estimate when NP is overly conservative or when retrieval+LM is correct but NP lacks sensitivity; best variant depends on the evaluated LM.",
            "uuid": "e6127.6",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Error Rate (ER) & F1_MICRO",
            "name_full": "Error Rate (ER) and F1_MICRO evaluation metrics",
            "brief_description": "Two complementary evaluation metrics: ER measures system-level absolute difference between estimated and human FActScore; F1_MICRO measures segment-level per-atomic-fact detection performance (Not-supported detection).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "ER = |estimated FActScore - human FActScore| aggregated over the evaluation set. F1_MICRO = micro F1 for the binary Not-supported class across atomic facts (precision/recall on predicted Not-supported facts).",
            "evaluation_criteria": "ER emphasizes system-level aggregate fidelity and correct system ranking; F1_MICRO emphasizes segment-level decision accuracy.",
            "llm_model_name": "Applied to outputs of InstructGPT, ChatGPT, PerplexityAI and other evaluated LMs; used to compare estimator variants (Inst-LLAMA, LLAMA 65B, ChatGPT as LM_EVALs).",
            "theory_domain": "Designed for fine-grained factual verification in long-form generation; applicable to scientific claim verification by measuring both aggregate alignment and per-claim detection.",
            "theory_description": "Evaluation framework to measure how well automatic estimators reproduce human judgments at both system and segment levels.",
            "evaluation_results": "Some estimators achieve low ER (&lt;2% reported); F1_MICRO varies more widely (best Retrieve→LM+NP with Inst-LLAMA or ChatGPT reached F1_MICRO up to ~83% for some configurations).",
            "benchmarks_or_datasets": "Human-labeled FActScore dataset (183 entities) and larger unlabeled set used for system comparisons.",
            "comparison_to_human": "ER directly quantifies difference from human aggregate scores; F1_MICRO reports per-claim agreement rates with human labels.",
            "limitations_or_challenges": "A method that optimizes F1_MICRO can still be biased (over- or underestimating) leading to poor ER, and vice versa; both metrics are needed for full picture.",
            "uuid": "e6127.7",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "TF Prompting vs QA Prompting",
            "name_full": "True/False (TF) Prompting versus Question-Answer (QA) Prompting",
            "brief_description": "Prompting styles compared for atomic-fact validation: TF prompting directly asks the evaluator LM 'True or False?' about a fact, while QA prompting asks the LM to generate a question and expected answer then answer that question; TF prompting was empirically superior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "TF prompting: provide atomic fact + 'True or False?' and parse LM logits/answer. QA prompting: generate a question and expected answer from the fact, then query LM for an answer and compare equality; considered prone to vagueness.",
            "evaluation_criteria": "Per-atomic-fact Supported/Not-supported accuracy (F1_MICRO) and downstream effect on ER.",
            "llm_model_name": "Evaluated with Inst-LLAMA 7B as LM_EVAL in ablations; results similar with other LM_EVALs.",
            "theory_domain": "Verification of factual claims in long-form text; approach generalizable to scientific claims verification.",
            "theory_description": "Two prompting paradigms for LM-based fact verification; TF prompting yields more reliable direct binary judgments.",
            "evaluation_results": "TF prompting significantly outperformed QA prompting across methods and LMs (notably TF Prompting + Retrieve→LM achieved higher F1_MICRO than QA prompting variants).",
            "benchmarks_or_datasets": "Evaluated on the human-labeled biography dataset; retrieval from Wikipedia.",
            "comparison_to_human": "TF prompting more closely aligns with human binary support judgments than QA prompting, which often produced vague/ambiguous questions leading to false negatives.",
            "limitations_or_challenges": "TF prompting relies on LM internals (logits) when available; when logits unavailable, output string detection (presence of 'True'/'False') is used and can be less robust.",
            "uuid": "e6127.8",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-check LM",
            "name_full": "Self-check LM (Manakul et al., 2023)",
            "brief_description": "A baseline method that validates facts by conditioning an LM_EVAL on multiple different outputs from the evaluated LM (LM_SUBJ) and aggregating judgments; zero-resource black-box hallucination detection.",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "mention_or_use": "use",
            "evaluation_method": "Generate multiple samples from the LM_SUBJ, for each sample prompt the LM_EVAL to judge the fact, then aggregate (majority vote) to detect hallucinations. In this paper used as an additional baseline (requires LM_SUBJ access and nondeterministic outputs).",
            "evaluation_criteria": "Per-atomic-fact Not-supported detection (F1_MICRO) and effect on estimated FActScore.",
            "llm_model_name": "Compared with Inst-LLAMA 7B as LM_EVAL; method requires access to LM_SUBJ to produce multiple outputs (not applicable to PerplexityAI semi-deterministic outputs).",
            "theory_domain": "Mentioned as a concurrent baseline in hallucination detection; relevant to verification of factual claims including scientific claims.",
            "theory_description": "Black-box approach leveraging LM_SUBJ's output variability to detect model uncertainty/hallucination without external retrieval.",
            "evaluation_results": "Self-check LM outperforms no-context LM by a few percent on F1_MICRO but underperforms retrieval-based methods; not applicable to some models lacking nondeterministic outputs.",
            "benchmarks_or_datasets": "Evaluated on the same biography dataset as baseline comparison.",
            "comparison_to_human": "Improves over naive baselines but is inferior to retrieval-augmented approaches when strong retrieval is available.",
            "limitations_or_challenges": "Requires access to LM_SUBJ and nondeterministic outputs; does not use external knowledge so limited when facts are outside LM's parametric knowledge.",
            "uuid": "e6127.9",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Retriever systems (GTR, BM25)",
            "name_full": "Passage retrieval systems: GTR (Generalizable T5-based Retriever) and BM25 (Pyserini)",
            "brief_description": "Passage retrieval backends used to provide context to LM_EVAL: dense retrievers (GTR variants) and traditional sparse retrieval (BM25) were compared and found to give similar results for this task.",
            "citation_title": "Large dual encoders are generalizable retrievers",
            "mention_or_use": "use",
            "evaluation_method": "Retrieve top-k passages from the topic entity's Wikipedia page (page-scope retrieval) using GTR Large/xLarge or BM25; supply retrieved passages to Retrieve→LM pipelines.",
            "evaluation_criteria": "Per-atomic-fact decision accuracy (F1_MICRO) and downstream estimator Error Rate.",
            "llm_model_name": "Retriever is model-agnostic; used with Inst-LLAMA, LLAMA 65B, ChatGPT as LM_EVALs.",
            "theory_domain": "Retrieval of factual evidence from Wikipedia for biography verification; analogous retrieval pipelines can be built over scientific literature corpora.",
            "theory_description": "Retrievers aim to surface passages that contain direct evidence to support or contradict atomic facts to improve LM-based validation.",
            "evaluation_results": "All retrieval systems performed roughly equivalently in the paper (GTR xLarge slightly best in some settings); retrieval significantly improves validation performance over no-context baselines.",
            "benchmarks_or_datasets": "English Wikipedia split into passages (up to 256 tokens) used as retrieval corpus; GTR models and BM25 (Pyserini) compared.",
            "comparison_to_human": "Retrieval-augmented validation better matches human labels than no-context LM prompting.",
            "limitations_or_challenges": "Retrieval often fails to return direct evidence (≈70% of the estimator's error cases were due to lack of direct evidence in retrieved passages); some verification requires page-level or broader context beyond single passages.",
            "uuid": "e6127.10",
            "source_info": {
                "paper_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "rating": 2
        },
        {
            "paper_title": "SciFact-open: Towards open-domain scientific claim verification",
            "rating": 2
        },
        {
            "paper_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2
        },
        {
            "paper_title": "Generating fact checking briefs",
            "rating": 1
        },
        {
            "paper_title": "Generating scientific claims for zeroshot scientific fact checking",
            "rating": 2
        },
        {
            "paper_title": "Nonparametric masked language modeling",
            "rating": 2
        }
    ],
    "cost": 0.020406999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</h1>
<p>Sewon Min ${ }^{11}$ Kalpesh Krishna ${ }^{12}$ Xinxi Lyu ${ }^{1}$ Mike Lewis ${ }^{4}$ Wen-tau Yih ${ }^{4}$<br>Pang Wei Koh ${ }^{1}$ Mohit Iyyer ${ }^{2}$ Luke Zettlemoyer ${ }^{1,4}$ Hannaneh Hajishirzi ${ }^{1,3}$<br>${ }^{1}$ University of Washington ${ }^{2}$ University of Massachusetts Amherst<br>${ }^{3}$ Allen Institute for AI ${ }^{4}$ Meta AI<br>{sewon, alrope, pangwei, lsz, hannaneh}@cs.washington.edu<br>{kalpesh,miyyer}@cs.umass.edu {mikelewis, scottyih}@meta.com</p>
<h4>Abstract</h4>
<p>Evaluating the factuality of long-form text generated by large language models (LMs) is nontrivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FActSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FActSCOREs of people biographies generated by several state-of-the-art commercial LMs-InstructGPT, ChatGPT, and the retrievalaugmented PerplexityAI—and report new analysis demonstrating the need for such a finegrained score (e.g., ChatGPT only achieves $58 \%$ ). Since human evaluation is costly, we also introduce an automated model that estimates FActSCORE using retrieval and a strong language model, with less than a $2 \%$ error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $\$ 26 \mathrm{~K}$ if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FActSCORE is available for public use via pip install factscore. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Long-form text generated by large language models (LMs) has widely been used (Brown et al., 2020; Ouyang et al., 2022); nonetheless, evaluating their factual precision-whether each piece of information conveyed in a generation is factually accurateremains challenging for two reasons. First, a generation consists of a large number of pieces of infor-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of FActSCORE, a fraction of atomic facts (pieces of information) supported by a given knowledge source. FActSCORE allows a more fine-grained evaluation of factual precision, e.g., in the figure, the top model gets a score of $66.7 \%$ and the bottom model gets $10.0 \%$, whereas prior work would assign 0.0 to both. FActSCORE can either be based on human evaluation, or be automated, which allows evaluation of a large set of LMs with no human efforts.
mation that are a mixture of true or false, ${ }^{2}$ making a binary judgment inadequate (Pagnoni et al., 2021). Second, validating every piece of information is time-consuming and costly.</p>
<p>In this paper, we introduce FActScore (Factual precision in Atomicity Score), a new evaluation of an LM that represents the percentage of atomic facts (pieces of information) supported by a given knowledge source. Computing FActScore involves (1) breaking a generation into a series of atomic facts-short statements that each contain one piece of information (Nenkova and Passonneau, 2004; Shapira et al., 2019; Zhang and Bansal, 2021; Liu et al., 2022), and (2) assigning a binary label</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>to each atomic fact, allowing a fine-grained evaluation of factual precision. We evaluate FActScore on the task of generating people biographies because generations consist of verifiable statements rather than debatable or subjective ones, and the scope is broad (i.e., covering diverse nationalities, professions, and levels of rarity).</p>
<p>We perform extensive human annotations to obtain FActScores of three state-of-the-art, commercially available LMs: InstructGPT (Ouyang et al., 2022), ChatGPT (OpenAI, 2022), and searchaugmented PerplexityAI. ${ }^{3}$ Our results indicate that commercially available LMs are riddled with errors, having FActScores of $42 \%, 58 \%$ and $71 \%$, respectively. Their FActScores significantly drop as the rarity of the entities increases, e.g., $80 \% \rightarrow 16 \%$ for ChatGPT.</p>
<p>Since human evaluation is costly, we next introduce an automatic evaluation of FActScore through a model that estimates a FActScore for a given LM. Our estimator decomposes generations into atomic facts and validates each based on a given knowledge source, leveraging retrieval from the given knowledge source and strong language models. Our estimator closely approximates FActScore with an error rate of $&lt;2 \%$ and can be applied to a range of new LMs at scale with no human effort. Our case study evaluates 6,500 generations from 13 LMs that could have cost $\$ 26 \mathrm{~K}$, with various findings: GPT-4 (OpenAI, 2023) and ChatGPT are far less factual than humans but are much better than public models, and there is a large variance between public models, with Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023) being some of the best.</p>
<p>In summary, our contributions are as follows.</p>
<ol>
<li>We introduce FActScore, a new evaluation of factual precision of LMs by breaking their generations into atomic facts and validating each against a given knowledge source. Human evaluation reveals that the state-of-the-art LMs with and without search have low FActScores.</li>
<li>We introduce a model that approximates FActScore with an error rate of $&lt;2 \%$, allowing evaluation of a large set of new LMs without manual human efforts.</li>
<li>We open-sourced FActScore and the annotated data for public use, available via pip install factscore. We suggest future work
<sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to extend FActSCORE for a broader set of generations (e.g., open-ended generation) and to further improve the estimator.</li>
</ol>
<h2>2 Related Work</h2>
<p>Factual precision in text generation. Factual precision in text generation has been an active area of research in NLP. Most prior work studies factual precision of models supervised for a specific problem such as dialogue (Shuster et al., 2021), or focuses on question answering with short answers (Kadavath et al., 2022; Kandpal et al., 2022; Mallen et al., 2023; Nori et al., 2023).</p>
<p>More recent work has studied factual precision of text generation beyond short answers. Lee et al. (2022) evaluates the factual precision with proxy metrics, e.g., whether named entities in a generation appear in an article of the topic. A series of concurrent work verifies the precision of the citations (attributions) provided by the model (Gao et al., 2022; Liu et al., 2023a; Yue et al., 2023; Gao et al., 2023). A concurrent work by Manakul et al. (2023) automates the identification of factual errors in LM generations without using any knowledge source; we use their method as a baseline estimator in Section 4. In contrast, our work (1) considers much longer text generation ${ }^{4}$ from a variety of state-of-the-art LMs with and without search, (2) provides their fine-grained evaluation both by human experts and through an automated evaluator that closely approaches humans, and (3) applies it to a large set of LMs at scale.</p>
<p>Fact Verification. Our work is closely related to prior work on fact verification (Thorne et al., 2018; Wadden et al., 2020) where claim sentences are automatically checked against a large knowledge source like Wikipedia or scientific literature. Most literature assumes a single, atomic claim, sometimes modeled with surrounding context (Nakov et al., 2018; Mihaylova et al., 2019; Shaar et al., 2022). There also has been work that verifies a longer sentence or text through decomposition to atomic facts (Fan et al., 2020; Wright et al., 2022; Chen et al., 2022; Kamoi et al., 2023) from which we take inspiration. The primary difference between fact verification literature and our work is that we focus on long-form model-generated text rather than sentence-level human-written claims.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Model-based Evaluation. Prior work has used learned models to define automated evaluation scores <em>Zhang et al. (2020); Liu et al. (2023b)</em>. This includes model-based evaluation in summarization that considers the consistency between a summary and a source document using QA or NLI <em>Kryscinski et al. (2020); Wang et al. (2020); Fabbri et al. (2022); Deutsch et al. (2021); Laban et al. (2022)</em>. We take inspiration from this work, and evaluate factual precision of LM generations by considering whether pieces of information are supported by a large text corpus.</p>
<h2>3 FActScore: Evaluating Factual Precision of Long-form Text Generation</h2>
<p>We introduce FActScore, a new evaluation of an LM that considers the factual precision of atomic facts generated by the LM. We perform human evaluations to calculate FActScores of the state-of-the-art LMs (Section 3.3) and discuss results (Section 3.4). FActScore allows rigorous and fine-grained evaluation of factual precision, but is time-consuming and costly, motivating automatic evaluation in Section 4.</p>
<h3>3.1 Definition</h3>
<p>FActScore is based on two key ideas.</p>
<p>Key idea 1: Atomic fact as a unit. Long-form text consists of many pieces of information that can each be either true or false. Prior work has explored using a sentence as a unit; however, even a single sentence is a mix of supported and unsupported facts, e.g., in 40% of the cases with ChatGPT. Previous and concurrent work either (1) defines an additional label of partial support <em>Manakul et al. (2023); Liu et al. (2023a)</em> whose definition may be subjective and can lead to low agreement, or (2) takes the strictest definition of support that requires every piece of information to be supported <em>Rashkin et al. (2021); Gao et al. (2022)</em>, which ignores the partial support cases, e.g., assigning 0.0 to both generations in Figure 1 even though the first generation is considerably more accurate than the second.</p>
<p>In this paper, we define an atomic fact as a short sentence conveying one piece of information (examples in Figure 1), similar to summarization content units <em>Nenkova and Passonneau (2004)</em>. An atomic fact is a more fundamental unit than a sentence for a piece of information and provides a more fine-grained evaluation, e.g., in Figure 1, rating the first generation higher than the second.</p>
<p>Key Idea 2: Factual precision as a function of a given knowledge source. Prior work often considers factual precision as a single global truth <em>Manakul et al. (2023)</em>. In contrast, we adopt a perspective that the truthfulness of a statement should depend on a particular knowledge source that end users consider to be trustworthy and reliable. Therefore, instead of whether an atomic fact is globally true or false, we consider whether it is supported by a given source of knowledge. This has been used in the fact verification literature <em>Wadden et al. (2022)</em> where conflict of information between different sources is relatively common.</p>
<p>Definition. Let $\mathcal{M}$ be a language model to be evaluated, $\mathcal{X}$ be a set of prompts, and $\mathcal{C}$ be a knowledge source. Consider a response $y=\mathcal{M}<em y="y">{x}$ for $x \in \mathcal{X}$ and $\mathcal{A}</em>$ is defined as follows.}$, a list of atomic facts in $y$. A FActScore of $\mathcal{M</p>
<p>$f(y)=\frac{1}{\left|\mathcal{A}<em _in="\in" _mathcal_A="\mathcal{A" a="a">{y}\right|} \sum</em>],$}_{y}} \mathbb{I}[a \text { is supported by } \mathcal{C</p>
<p>$\operatorname{FACTSCORE}(\mathcal{M})=\mathbb{E}<em x="x">{x \in \mathcal{X}}\left[f\left(\mathcal{M}</em>\right.$ responds $]$.}\right) \mid \mathcal{M}_{x</p>
<p>$\mathcal{M}_{x}$ responds means $\mathcal{M}$ did not abstain from responding to the prompt $x$. This definition assumes the following:</p>
<ol>
<li>Whether or not an atomic fact is supported by $\mathcal{C}$ is undebatable.</li>
<li>Every atomic fact in $A_{y}$ has an equal weight of importance, following <em>Krishna et al. (2023)</em>.</li>
<li>Pieces of information in $\mathcal{C}$ do not conflict or overlap with each other.</li>
</ol>
<p>In the rest of the paper, we propose to use people biographies as $\mathcal{X}$ and Wikipedia as $\mathcal{C}$ because they satisfy these assumptions to a reasonable degree (Section 3.3). We discuss in which cases these assumptions hold or may not hold in more detail in the Limitation section.</p>
<p>FActScore considers precision but not recall, e.g., a model that abstains from answering too often or generates text with fewer facts may have a higher FActScore, even if these are not desired. We leave the evaluation of factual recall for future work (more discussion in the Limitation section).</p>
<h3>3.2 Studied LMs</h3>
<p>We evaluate three LMs (referred to as $\mathrm{LM}_{\text {SUBJ }}$, an LM as a subject): (1) InstructGPT (text-davinci-003, updated from *Ouyang et al.</p>
<p>(2022)), (2) ChatGPT (OpenAI, 2022), and (3) PerplexityAI, ${ }^{3}$ which incorporates a search engine with a language model.</p>
<h3>3.3 Data</h3>
<p>We perform human evaluation of factual precision based on our definition. We prompt the $\mathrm{LM}_{\text {sUBJ }}$ to generate people biographies and evaluate them against Wikipedia for the following reasons.</p>
<ul>
<li>Biographies are objective (not subjective or debatable) and contain specific (not vague) information, satisfying Assumption 1 in Section 3.1.</li>
<li>Biographies allow evaluation across diverse nationalities, professions, and levels of rarities.</li>
<li>Wikipedia offers reasonable coverage of information about people and is reasonably selfconsistent, ${ }^{5}$ satisfying Assumption 3.</li>
</ul>
<p>Data collection. We carefully design an annotation pipeline to assign a factual precision to a long-form generation through the following steps.</p>
<p>Step 0: Sampling people entities. We sample 183 people entities from Wikidata who have corresponding Wikipedia pages. We sample entities to annotate from a uniform distribution over categories defined in Appendix A.1.</p>
<p>Step 1: Obtaining generations. We feed a prompt "Tell me a bio of <entity>" to the $\mathrm{LM}_{\text {sUBJ }}$ and take a generation as it is. We implement rules to identify generations that abstain from answering and filter them out.</p>
<p>Step 2: Atomic facts generation. Human annotators break a generation into a series of atomic facts. To save annotation time, we provide atomic facts broken down by InstructGPT which human annotators can take and revise. Details in Appendix A.2.</p>
<p>Step 3: Labeling factual precision \&amp; editing. We ask another set of human annotators to assign each atomic fact one of three labels. If the atomic fact is clearly not related to the prompt, and thus should be removed from the bio without a validation step, they assign Irrelevant. If the fact is relevant, they validate the fact based on the English Wikipedia, and label either Supported or Not-supported.</p>
<p>We recruit freelancers through Upwork and pay 15-25 USD per hour. Annotation requires extensive effort and time, leading to the cost of $\$ 4$ per generation. We assign two freelancers for the $10 \%$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  | InstGPT | ChatGPT | PPLAI |
| :-- | --: | --: | --: |
| Use search | $\boldsymbol{x}$ | $\boldsymbol{x}$ | $\boldsymbol{\checkmark}$ |
| \% responding | 99.5 | 85.8 | 90.7 |
| # tokens / response | 110.6 | 154.5 | 151.0 |
| # sentences / response | 6.2 | 7.9 | 9.8 |
| # facts / response | 26.3 | 34.7 | 40.8 |
| Statistics of the labels |  |  |  |
| Supported | 42.3 | 50.0 | 64.9 |
| Not-supported | 43.2 | 27.5 | 11.1 |
| Irrelevant | 14.0 | 8.3 | 14.8 |
| Abstains from answering | 0.5 | 14.2 | 9.3 |
| FActScore | $\mathbf{4 2 . 5}$ | $\mathbf{5 8 . 3}$ | $\mathbf{7 1 . 5}$ |</p>
<p>Table 1: Statistics of the data and FActSCORE results. InstGPT and PPLAI respectively refer to InstructGPT and PerplexityAI. \% responding indicates \% of generations that do not abstain from responding. # tokens is based on white space.
of the data and calculate the agreement rate: $96 \%$, $90 \%$ and $88 \%$ for InstructGPT, ChatGPT and PerplexityAI, respectively. More details are provided in Appendix A.3.</p>
<h3>3.4 Results</h3>
<p>Statistics of the data and results are reported in Table 1.</p>
<p>All $\mathbf{L M}_{\text {sUBJ }}$ s struggle with factual precision errors. InstructGPT and ChatGPT achieve FActSCOREs of $42.5 \%$ and $58.3 \%$, respectively. PerplexityAI, which uses a commercial search engine and thus should have a perfect FActSCORE if directly copying the text from the correct Wikipedia page, attains a FActSCORE of $71.5 \%$. We provide a qualitative analysis of its error cases in the last paragraph of this section.</p>
<p>ChatGPT and PerplexityAI often abstain from answering which presumably improves their factual precision. InstructGPT rarely abstains from answering, likely because it is not trained to do so.</p>
<p>Irrelevant facts either (a) have dependencies on previous facts in a generation that turn out to be unsupported, or (b) are irrelevant to the prompt independent from other facts in a generation (examples in Appendix A.4). We find that (b) rarely happens with InstructGPT and ChatGPT but happens considerably with PerplexityAI, because PerplexityAI often directly copies search results even if they are largely irrelevant to the input prompt. This is in agreement with a concurrent work from Liu et al. (2023a) that shows generative search engines like PerplexityAI copy incorrect search results and generate text that is irrelevant to the input query.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">$\%$</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single-sentence contradiction (words)</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">Gen On November 25th, 2023, Glover Teixeira became an American citizen. Wiki In November 2020, Teixeira became an American citizen. Gen [Eric Hacker] was named the International League Pitcher of the Year. Wiki [Eric Hacker] was named the IL Pitcher of the Week.</td>
</tr>
<tr>
<td style="text-align: center;">Single-sentence contradiction (beyond words)</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">Gen William Waldegrave's grandfather was James II and VII. Wiki His father's title was created ... for the diplomat and ambassador James Waldegrave, 1st Earl Waldegrave, whose grandfather was James II and VII. Gen She has appeared in several successful films such as (...) and Zero (2018). Wiki: Zero was a commercial failure.</td>
</tr>
<tr>
<td style="text-align: center;">Page-level contradiction</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">Gen Some of [Julia Faye's] notable films include ... "Cleopatra" (1934). Comment No mention of Cleopatra on the Julia Faye page, and no mention of Julia Faye on the Cleopatra page. Gen [Kang Ji-hwan] has donated money to various charities and organizations over the years. Comment No such mention on the Kang Ji-hwan page.</td>
</tr>
<tr>
<td style="text-align: center;">Subjective</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">Gen His achievements, as an actor and as a cultural force, will surely prove to be as heroic as those of the characters he portrayed. Wiki Culture writer Steve Rose, in The Guardian, wrote: "Chadwick Boseman began his career playing African American icons and pioneers; he ends it as one himself. His [...] achievements, as an actor and as a cultural force, will surely prove to be as heroic as those of the characters he portrayed."</td>
</tr>
<tr>
<td style="text-align: center;">Fact is irrelevant</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">Gen [Zamfir Arbore]'s life is not well-documented, and there is little information available about him.</td>
</tr>
<tr>
<td style="text-align: center;">Wiki is inconsistent \&amp; wrong</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">Gen Kick (2014) that brought [Sajid Nadiadwala] various debutant director awards. Wiki 2015, IIFA Award for Debut Director, Kick. (...) Kick brought him various debutant director awards. Comment The first text is from a table that indicates he won one award (accurate). The second is inaccurate, incorrectly citing a news article.</td>
</tr>
<tr>
<td style="text-align: center;">Annotation error</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">Gen [Zamfir Arbore] was part of the staff of Romānal. Wiki The Romānal staff came to include Zamfir Arbore. Comment Mentioned in the Romānal page but not in the Zamfir Arbore page.</td>
</tr>
</tbody>
</table>
<p>Table 2: Categorization of precision errors (Not-supported) from PerplexityAI (Section A.5). Gen indicates the generation from PerplexityAI, and Wiki indicates evidence text from Wikipedia. Comment indicates our comments.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: FACTSCORE across varying frequency levels of human entities (top) and relative positions in a generation (bottom). FACTSCOREs are lower as the rarity of the entities increases and the position of the fact is later.</p>
<p>Error rates are higher for rarer entities. Figure 2 (top) shows factual precision over varying frequency levels of topic entities (humans) in the pretraining corpora (see Appendix A.1). There is a notable decrease in FACTSCORE as the rarity of entities increases, consistently across all $\mathrm{LM}_{\text {subis }}$. This is in agreement with Kandpal et al. (2022) and Mallen et al. (2023) which show that short question answering (QA) accuracy is highly correlated with the entity frequencies in the pretraining data. However, in contrast to Kandpal et al. (2022)
and Mallen et al. (2023) who report QA accuracy of models with retrieval is robust to the rarity of entities, FACTSCORE of PerplexityAI still significantly drops as entities are rarer: a relative drop of $50 \%$ and $64 \%$ observed at the atomic-level and sentence-level, respectively.</p>
<h2>Error rates are higher for facts mentioned later</h2>
<p>in the generation. Figure 2 (bottom) reports factual precision over relative positions in a generation. Across all LMs, the later part of the generation has significantly worse precision. This is likely because (a) information mentioned earlier is more frequently mentioned in the pretraining data (e.g., nationality, profession), and (b) error propagation affects the later part of the generation. This also implies that evaluating LMs solely based on short answers may not provide an adequate assessment of their factual precision, as it fails to account for errors that arise in the later stages of generation.</p>
<p>Qualitative analysis of Not-supported. One of the surprising findings in our empricial analysis is that a FACTSCORE of PerplexityAI (71.5\%) is lower than expected despite having access to the search engine. To better understand its errors, we categorize 30 random samples whose label is Not-supported (Table 2).</p>
<ul>
<li>Single-sentence contradiction: A single sentence from Wikipedia provides direct contradic-</li>
</ul>
<p>tion to the generation, either at a word level (numbers, dates, or entities) or beyond.</p>
<ul>
<li>Page-level contradiction: Errors found after reading the entire page, often because a fact that should have been mentioned in Wikipedia if true is missing, e.g., whether the subject appears in a particular film.</li>
<li>Subjective: Generation is subjective, often because PerplexityAI copies subjective text from Wikipedia, e.g., directly copying a quote from a journalist without realizing it.</li>
<li>Fact is irrelevant: Generation is irrelevant to the subject due to a search error.</li>
<li>Wiki is inconsistent \&amp; wrong: In the example, Wikipedia indicates that the subject won one award from the film Kick, but also includes text that they won multiple awards from Kick, which is inaccurate and cited a news article that does not support the claim.</li>
<li>Annotation error: Annotators assign incorrect labels, typically because the information is not mentioned in the subject's Wikipedia page (likely because it is insignificant).</li>
</ul>
<p>We also find that, although PerplexityAI provides citations to the references, citations have little correlation with factual precision. $36.0 \%$ and $37.6 \%$ of supported and unsupported sentences have citations, respectively. Together with independent findings from Liu et al. (2023a), this indicates that commercial LMs that incorporate search and provide citations may not be as reliable as expected.</p>
<p>More analysis is provided in Appendix A.5.</p>
<h2>4 Estimating FACTSCORE for Automatic Evaluation</h2>
<p>Human evaluation of factual precision is costly ( $\$ 4$ per generation) (Bohnet et al., 2022; Krishna et al., 2023) because validating every atomic fact against a large knowledge source is time-consuming, and one generation contains many (26-41) atomic facts. This prevents LM developers and practitioners from evaluating the factual precision in long-form generation of a new $\mathrm{LM}<em _SUBJ="{SUBJ" _text="\text">{\text {SUBJ }}$ at scale. In this context, we introduce a model that estimates FACTSCORE. This estimator takes a set of generations and automatically computes a FACTSCORE, and can be applied to any $\mathrm{LM}</em>$.}</p>
<p>We describe our model (Section 4.1) and demonstrate its accuracy against human evaluation (Sec-
tion 4.2). FACTSCORE estimated by our model is then used to evaluate twelve LMs (Section 4.3).</p>
<h3>4.1 Model</h3>
<p>Our estimator of FACTSCORE first breaks a generation into a series of atomic facts and then validates each against the given knowledge source. We find taking atomic facts generated by InstructGPT (used in data collection in Section 3.3) effective and close to human, consistent with findings from prior work (Chen et al., 2022). This section thus focuses on how to validate each atomic fact against a given knowledge source.</p>
<p>The validation is based on zero-shot prompting of an LM referred to as an $\mathbf{L M}<em _SUBJ="{SUBJ" _text="\text">{\text {EVAL }}$ to distinguish from an $\mathrm{LM}</em>}}$. Specifically, a prompt-whose construction methods differ across four variantsis fed into an $\mathrm{LM<em _EVAL="{EVAL" _text="\text">{\text {EVAL }}$. The prediction is then made by comparing the conditional probability of True and False from the $\mathrm{LM}</em>$
The four variants we consider are as follows.
No-context LM uses <atomic-fact> True or False? as a prompt, closely resembling Kadavath et al. (2022). ${ }^{7}$
Retrieve $\rightarrow$ LM retrieves passages from the given knowledge source and then prompts the $\mathrm{LM}}}$. If the logit values are unavailable (e.g., commercial LMs like ChatGPT), the prediction is made based on whether the generated text contains True or False. ${ }^{6<em _EVAL="{EVAL" _text="\text">{\text {EVAL }}$. It first retrieves $k$ passages, constructs the prompt by concatenating retrieved passages, the given atomic fact, and "True or False?", and feeds it to the $\mathrm{LM}</em>$ to get the prediction.
Nonparametric Probability (NP) makes a judgment based on a nonparametric likelihood. It masks out each token in the atomic fact, computes its likelihood using a nonparametric masked LM (Min et al., 2023), averages probabilities over all tokens, and makes a prediction based on thresholding.
Retrieve $\rightarrow \mathbf{L M}+\mathbf{N P}$ is an ensemble of Retrieve $\rightarrow$ LM and NP which assigns Supported only if both methods assign Supported.}</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Evaluator</th>
<th style="text-align: center;">retrv</th>
<th style="text-align: center;">SUBJ: InstGPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SUBJ: ChatGPT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SUBJ: PPLAI</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ranking</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ER</td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">ER</td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;">ER</td>
<td style="text-align: center;">FS</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㄱ } \ &amp; \text { ㄷ } \end{aligned}$</td>
<td style="text-align: center;">Always Supported</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">$100.0+$</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">$100.0+$</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">$100.0+$</td>
<td style="text-align: center;">$\mathbf{X}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Always Not-supported</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">$0.0-$</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">$0.0-$</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">$0.0-$</td>
<td style="text-align: center;">$\mathbf{X}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Always Random</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">$50.0+$</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">$50.0-$</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">$50.0-$</td>
<td style="text-align: center;">$\mathbf{X}$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㄴ } \ &amp; \text { ㄴ } \ &amp; \text { ㄷ } \end{aligned}$</td>
<td style="text-align: center;">No-context LM</td>
<td style="text-align: center;">$\mathbf{X}$</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">$49.6+$</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">$50.5-$</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">$36.8-$</td>
<td style="text-align: center;">$\mathbf{X}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NP</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">$57.3+$</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">$72.0+$</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Retrieve $\rightarrow$ LM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">$56.6+$</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">$75.4+$</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">$\mathbf{X}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Retrieve $\rightarrow$ LM + NP</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">$61.6-$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No-context LM</td>
<td style="text-align: center;">$\mathbf{X}$</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">$82.1+$</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">$90.1+$</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">$\mathbf{X}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Retrieve $\rightarrow$ LM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">$47.6+$</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">$65.1+$</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Retrieve $\rightarrow$ LM + NP</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">$37.3-$</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">$62.8-$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on Error Rate (ER) along with FActScores estimated by each model (FS). 'retrv' indicates whether or not retrieval is being used, and 'ranking' $\checkmark$ indicates whether the ranking between three $\mathrm{LM}_{\text {SUBJ }}$ s rated by the model is consistent to the ground truth ranking. + and - respectively indicate the estimation is an overestimation and an underestimation by more than $5 \%$ in absolute. Red Bold indicates the best (lowest) ER. See Appendix B. 2 for the results in other metrics that consider individual judgments instead of aggregated ones.</p>
<p>We use LLAMA 7B trained on Super Natural Instructions (Inst-LLAMA, Touvron et al., 2023; Wang et al., 2022) and ChatGPT as an $\mathrm{LM}_{\text {EVAL }}$, and Generalizable T5-based Retrievers (GTR, Ni et al. (2022)) for passage retrieval. See Appendix B. 1 for more implementation details.</p>
<h3>4.2 Evaluation of Estimators</h3>
<p>Metrics. We report Error Rate (ER)—the difference between the ground truth and the estimated FActSCORE-as well as whether the estimated FActSCORES preserve the ranking between three $\mathrm{LM}_{\text {SUBJ }}$ s. Appendix B. 2 discusses results with other metrics that consider individual judgments instead of aggregated judgments. We use the data in Section 3.3 as evaluation data.</p>
<p>Results are reported in Table 3.
Retrieval significantly helps. Models that use retrieval are consistently better than No-context LM which either has a significantly high ER or does not preserve ranking between three $\mathrm{LM}<em _EVAL="{EVAL" _text="\text">{\text {SUBJ }}$ s. This is likely because the $\mathrm{LM}</em>}}$ has not memorized every factual information about the topic entity, thus benefiting from retrieval providing factual context. Nonetheless, just using Retrieve $\rightarrow$ LM may overestimate FActSCORE, e.g., by up to $17 \%$ with Inst-LLAMA, when a $\mathrm{LM<em _SUBJ="{SUBJ" _text="\text">{\text {SUBJ }}$ is InstructGPT or ChatGPT. In this case, ensembling Retrieve $\rightarrow \mathrm{LM}$ and NP reduces an error rate by a significant margin. When a $\mathrm{LM}</em>$ is PerplexityAI, single methods (either Retrieve $\rightarrow$ LM or NP) give a low ER, and ensemble methods have a higher ER due to an
underestimation of FActSCORE.
ChatGPT is not always the best. Our results show that ChatGPT is not necessarily better than Inst-LLAMA. We investigate this further in Appendix B.3. In summary, ChatGPT is better at validating each individual atomic fact. However, most errors from ChatGPT are incorrectly assigning Supported to unsupported facts, overestimating FActScore. In contrast, LLAMA+NP is not biased toward overestimation or underestimation of the factual precision, resulting in an aggregated factual precision to be closer to the ground truth. This is similar to the trade-off between systemlevel and segment-level correlations in summarization evaluation, which often produce different rankings (Bhandari et al., 2020; Deutsch et al., 2021).
The best estimator depends on the $\mathbf{L M}}<em _SUBJ="{SUBJ" _text="\text">{\text {SUBJ }}$. While using retrieval is consistently better than No-context LM, the best variant of estimator depends on a $\mathrm{LM}</em>}}$ : LLAMA+NP for InstructGPT and ChatGPT, and ChatGPT for PerplexityAI. Nevertheless, both evaluators give consistently correct ranking between three $\mathrm{LM<em _SUBJ="{SUBJ" _text="\text">{\text {SUBJ }}$ s, and Section 4.3 show scores from two estimators are largely correlated across $10+\mathrm{LM}</em>$ and report their correlation.}} \mathrm{s}(0.99$ Pearson's $r$ ). We recommend users try both variants of our estimator when evaluating a new $\mathrm{LM}_{\text {SUBJ }</p>
<h3>4.3 Evaluation of New LMs</h3>
<p>Our estimator allows evaluating factual precision of a large set of new LMs at scale with no human</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\mathrm{LM}_{\text {SUBJ }}$</th>
<th style="text-align: center;">Base LM</th>
<th style="text-align: center;">Use other LMs</th>
<th style="text-align: center;">Open</th>
<th style="text-align: center;">Release</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">InstructGPT</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">Nov 2022</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">Nov 2022</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">Mar 2023</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca [7B,13B,65B]</td>
<td style="text-align: center;">LLAMA</td>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Mar 2023</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna [7B,13B]</td>
<td style="text-align: center;">LLAMA</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Mar 2023</td>
</tr>
<tr>
<td style="text-align: left;">Dolly 12B</td>
<td style="text-align: center;">Pythia 12B</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Mar 2023</td>
</tr>
<tr>
<td style="text-align: left;">Oasst-pythia 12B</td>
<td style="text-align: center;">Pythia 12B</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Mar 2023</td>
</tr>
<tr>
<td style="text-align: left;">StableLM-tuned 7B</td>
<td style="text-align: center;">StableLM-base</td>
<td style="text-align: center;">ChatGPT, GPT-4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Apr 2023</td>
</tr>
<tr>
<td style="text-align: left;">MPT Chat 7B</td>
<td style="text-align: center;">MPT 7B</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">May 2023</td>
</tr>
</tbody>
</table>
<p>Table 4: A set of twelve LMs evaluated in Section 4.3. All models are tuned for instruction following or chat. Use other LMs indicates whether the model is trained on any data that includes outputs of another model. Open indicates model weights are publicly available.
efforts. As a case study, we evaluate ten new LMs that came out within two months at the time of conducting experiments (Table 4). These LMs were evaluated on many benchmarks but not in factual precision of long-form generation since such evaluation is costly. We aim to provide new insights on these LMs by estimating FACTSCORE of their long-form generations.</p>
<h3>4.3.1 Setup</h3>
<p>We evaluate 10 recently-released LMs as shown in Table 4. GPT-4 (OpenAI, 2023) is a multimodal LM released by OpenAI available through an API. Alpaca (Taori et al., 2023) is based on LLAMA (Touvron et al., 2023) fine-tuned on the instructions data based on InstructGPT following the recipe from Wang et al. (2022). Vicuna (Chiang et al., 2023) is based on LLAMA fine-tuned on the outputs from ChatGPT available through ShareGPT. ${ }^{8}$ Dolly $^{9}$ is Pythia 12B (Biderman et al., 2023) fine-tuned on DataBricks Dolly, human-written data created by Databricks. ${ }^{10}$ Oasst-pythia ${ }^{11}$ is Pythia 12B fine-tined on humanwritten data collected through Open Assistant. ${ }^{12}$ StableLM-tuned-alpha ${ }^{13}$ is based on StableLM-base-alpha ${ }^{14}$ fine-tuned on the data used in the Alpaca data, DataBricks Dolly, the ShareGPT data, the GPT4All data (Anand et al., 2023) and Anthropic HH (Bai et al., 2022). MPT Chat is based on MPT $7 \mathrm{~B}^{15}$ fine-tuned on the ShareGPT data, the Alpaca data, Anthropic HH, HC3 (Guo et al., 2023), and Evol-Instruct. ${ }^{16}$</p>
<p>We prompt each $\mathrm{LM}_{\text {SUBJ }}$ to generate biographies of 500 human entities as done in Section 3.3 but</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 5: Statistics of 500 model-generated bios in our unlabeled data from 12 LMs as well as human-written bios. \% responding indicates $\%$ of generations that do not abstain from responding. #facts / res indicates # of atomic facts per response. LMs are sorted based on # of facts per response. See Figure 3 for their FACTSCOREs.
with no overlap in entities. We additionally include InstructGPT, ChatGPT, and human-written biographies obtained through DBPedia. Human-written biographies were unavailable for $11 \%$ of entities which we consider as abstaining from responding. See Table 5 for their statistics. In total, we evaluate 6,500 generations from 13 subjects, which would have cost $\$ 26 \mathrm{~K}$ if they were evaluated by humans.</p>
<h3>4.3.2 Results</h3>
<p>Figure 3 shows the ranking between 13 subjects provided by the two best variants of our estimator whose scores are largely correlated, e.g., having a Pearson's $r$ of 0.99 . This evaluation allows a better understanding of these models, including:</p>
<ul>
<li>All LMs are substantially less factual than humans. This is in contrast to prior work that claims LMs approach human performance, even for complex tasks (Ding et al., 2022; Nori et al., 2023; Lee et al., 2023) even though the task of writing biographies is fairly easy.</li>
<li>GPT-4 and ChatGPT are comparable in factual precision. However, as reported in Table 5, GPT4 abstains from responding less ( $12 \%$ vs. $16 \%$ ) and generates significantly more facts ( 61 vs. 37 per response).</li>
<li>GPT-4 and ChatGPT are significantly more factual than public models.</li>
<li>Within the same family of models that differ in sizes, there is a clear correlation between the model size and factual precision, e.g., Alpaca $65 \mathrm{~B}&gt;13 \mathrm{~B}&gt;7 \mathrm{~B}$, and Vicuna $13 \mathrm{~B}&gt;7 \mathrm{~B}$.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Ranking between 13 subjects (human and 12 LMs), rated by the two best variants of our estimator: ChatGPT (<strong>left</strong>) and LLAMA+NP (<strong>right</strong>), both with retrieval. Scores from two metrics have a Pearson's <em>r</em> of 0.99. See Table 5 for % of responding and # of atomic facts per response of each LM. The variance in estimation based on different subsets of prompts is reported in Figure 5 of Appendix B.4.</p>
<ul>
<li>Alpaca and Vicuna achieve performance that is very close to each other within the same size of models, possibly because they share the same base model and similar training data. Nonetheless, as shown in Table 5, Vicuna generates significantly more atomic facts than Alpaca does (51 vs. 17 per response). Also, Alpaca never abstains from answering while Vicuna does.</li>
<li>Within public models, there are large gaps in factual precision even when the model size is similar, e.g., within the 7B models, Alpaca and Vicuna (~ 40%) are more factual than MPT-Chat (30%) and StableLM (17%). Possible factors include the choice of the base LM, the data, and the training recipe (Hoffmann et al., 2022).</li>
</ul>
<p>We highlight that this evaluation only considers factual precision, specifically in people biographies. A holistic evaluation of LMs should include other aspects of generations such as fluency, coherence, relevance, consistency and creativity, which is out of scope of this paper.</p>
<h2>5 Conclusion and Future Work</h2>
<p>We introduced FActScore, a new evaluation of the factual precision of long-form generation from LMs that breaks a generation down into a series of atomic facts and computes a fraction of facts supported by a given knowledge source. We first performed extensive human evaluation, finding that commercial, state-the-art-art LMs—InstructGPT, ChatGPT, and search engine augmented, PerplexityAI—make a substantial amount of errors, e.g., having a FActScore of 58% in the case of ChatGPT. Since human evaluation is time-consuming and costly, we proposed a model that estimates FActScore, allowing an automatic evaluation of FActScore. We found our estimator based on retrieval over a knowledge source and competitive language models estimates FActScore close to the ground truth, and showcased its application by evaluating 12 recently-released LMs that could have cost $65K if evaluated by humans and providing insights about them.</p>
<p>Within four months since its initial release, FActScore has actively been used in subsequent work, evaluating factual precision of recently-proposed models (Ye et al., 2023; Sun et al., 2023; Malaviya et al., 2023; Dhuliawala et al., 2023). As future work, we suggest: (1) considering other aspects of factuality such as recall (coverage of factual information); (2) further improving the estimator for a better approximation of factual precision; and (3) leveraging FActScore to correct model generations (briefly explored in Appendix C).</p>
<h2>Limitations</h2>
<p><strong>Scope of FActScore.</strong> All of our experiments focus on people biographies and Wikipedia, because many LMs can generate biographies with objective and specific facts (rather than subjective and vague ones) and Wikipedia has a high coverage for them. FActScore can be applied to a broader domain, e.g., text about recent events whose knowledge source can be a collection of news articles, or text about scientific findings whose knowledge source can be a collection of scientific literature. We present a proof of concept in Appendix B.5 and</p>
<p>leave further study for future work.
Due to the assumptions made in Section 3.1, FActSCORE is not applicable when the facts are more nuanced, open-ended, and debatable (Chen et al., 2019; Xu et al., 2023) or with a knowledge source whose text frequently conflicts with each other (Wadden et al., 2022). Moreover, FActSCORE may not be suitable for the humanwritten text that is nuanced and includes intentional or implicit deception.</p>
<p>Limitation in our estimator. While our estimator closely approximates humans and provides consistent ranking over a large set of LMs, it is not perfect in individual judgments, and the best variant depends on the degree of how close a generation is to human-written text and its linguistic complexity. Future work can investigate how the distribution of model generation affects the performance of the estimator and further improve the estimator.</p>
<p>Beyond factual precision. FActSCORE focuses on factual precision-whether each piece of information in a generation is factually supported by a reliable source of knowledge-which is only one aspect of the broader factuality problem. For instance, FActSCORE does not consider factual recall: the coverage of information in a generation. FActSCORE does not penalize a model that abstains from responding too frequently or generates fewer facts, which can be unfair since there is an inherent trade-off between precision and recall. Moreover, the boundary between precision and recall is often blurry, e.g., it is possible that, even if every piece of information in a generation is supported, it misses a significant piece of information that should have been mentioned in order to be considered as correctly responding to the input prompt (example in Table 6). We leave a more holistic evaluation of factuality for future work, and recommend reporting FActSCORE together with the $\%$ of abstention and the average number of atomic facts (as we did in Section 4.3).</p>
<h2>Acknowledgement</h2>
<p>We thank Yizhong Wang for sharing Instructiontuned LLAMA and Alpaca models with varying sizes, and for sharing feedback on the FActSCORE Python package. We thank experts in Upwork for annotating the data, and Dhruba Ghosh, Jiacheng Liu and Zeqiu Wu for participating in pilot annotation and sharing feedback. We thank Akari</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 6: An example whose factual precision is high but recall is low. The generation does not mention how Mary I of England got back to the line of succession and eventually became a queen.</p>
<p>Asai, Yanai Elazar, UW NLP members, UMass NLP members, FAIR lab members for feedback and discussion on the paper.</p>
<p>This research was supported by NSF IIS2046248, NSF IIS-2202506, NSF IIS-2044660, ONR N00014-18-1-2826, ONR MURI N00014-18-1-2670, DARPA under Contract No. FA8650-23-C-7316, an Allen Distinguished Award, and gifts from AI2. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. Sewon Min is supported by a J.P. Morgan fellowship, and Kalpesh Krishna was supported by the Google PhD Fellowship.</p>
<h2>References</h2>
<p>Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSurma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Bernd Bohnet, Vinh Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, Lierni Sestorain Saralegui, William Weston Cohen, Michael Collins, Dipanjan Das, Don Metzler, Slav Petrov, and Kellie Webster. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of Advances in Neural Information Processing Systems.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the Association for Computational Linguistics.</p>
<p>Jifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022. Generating literal and implied subquestions to fact-check complex claims. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021. Towards question-answering as an automatic metric for evaluating the content quality of a summary. Transactions of the Association for Computational Linguistics.</p>
<p>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495.</p>
<p>Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq Joty, and Boyang Li. 2022. Is gpt-3 a good data annotator? arXiv preprint arXiv:2212.10450.</p>
<p>Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QAbased factual consistency evaluation for summarization. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Angela Fan, Aleksandra Piktus, Fabio Petroni, Guillaume Wenzek, Marzieh Saeidi, Andreas Vlachos, Antoine Bordes, and Sebastian Riedel. 2020. Generating fact checking briefs. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2022. Attributed text generation via post-hoc research and revision. arXiv preprint arXiv:2210.08726.</p>
<p>Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations.</p>
<p>Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arxiv:2301.07597.</p>
<p>Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. 2018. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. Wice: Real-world entailment for claims in wikipedia. arXiv preprint arXiv:2303.01432.</p>
<p>Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411.</p>
<p>Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation</p>
<p>of faithfulness in long-form summarization. In Proceedings of the European Chapter of the Association for Computational Linguistics.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-visiting NLibased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics.</p>
<p>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. In Advances in Neural Information Processing Systems.</p>
<p>Peter Lee, Sebastien Bubeck, and Joseph Petro. 2023. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine.</p>
<p>Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval.</p>
<p>Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.</p>
<p>Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, et al. 2022. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. arXiv preprint arXiv:2212.07981.</p>
<p>Qingsong Ma, Johnny Wei, Ondřej Bojar, and Yvette Graham. 2019. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation.</p>
<p>Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2023. Expertqa: Expert-curated questions and attributed answers. arXiv preprint arXiv:2309.07852.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the Association for Computational Linguistics.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.</p>
<p>Tsvetomila Mihaylova, Georgi Karadzhov, Pepa Atanasova, Ramy Baly, Mitra Mohtarami, and Preslav Nakov. 2019. SemEval-2019 task 8: Fact checking in community question answering forums. In Proceedings of the 13th International Workshop on Semantic Evaluation.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2023. Nonparametric masked language modeling. In Findings of the Association for Computational Linguistics: ACL.</p>
<p>Preslav Nakov, Alberto Barrón-Cedeno, Tamer Elsayed, Reem Suwaileh, Lluís Màrquez, Wajdi Zaghouani, Pepa Atanasova, Spas Kyuchukov, and Giovanni Da San Martino. 2018. Overview of the clef-2018 checkthat! lab on automatic identification and verification of political claims. In Experimental IR Meets Multilinguality, Multimodality, and Interaction.</p>
<p>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375.</p>
<p>OpenAI. 2022. Chatgpt blog post. https://openai. com/blog/chatgpt.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of Advances in Neural Information Processing Systems.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870.</p>
<p>Shaden Shaar, Firoj Alam, Giovanni Da San Martino, and Preslav Nakov. 2022. The role of context in detecting previously fact-checked claims. In Findings of the Association for Computational Linguistics: NAACL 2022.</p>
<p>Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowdsourcing lightweight pyramids for manual summary evaluation. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021.</p>
<p>Simeng Sun, Dhawal Gupta, and Mohit Iyyer. 2023. Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of rlhf. arXiv preprint arXiv:2309.09055.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Conference of the North American Chapter of the Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,</p>
<p>Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022. SciFact-open: Towards open-domain scientific claim verification. In Findings of the Association for Computational Linguistics: EMNLP.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the Association for Computational Linguistics.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>John Wieting, Kevin Gimpel, Graham Neubig, and Taylor Berg-kirkpatrick. 2022. Paraphrastic representations at scale. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.</p>
<p>Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Lu Wang. 2022. Generating scientific claims for zeroshot scientific fact checking. In Proceedings of the Association for Computational Linguistics.</p>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In Proceedings of the Association for Computational Linguistics.</p>
<p>Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928.</p>
<p>Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311.</p>
<p>Shiyue Zhang and Mohit Bansal. 2021. Finding a balanced degree of automation for summary evaluation. In Proceedings of Empirical Methods in Natural Language Processing.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In Proceedings of the International Conference on Learning Representations.</p>
<h2>A Details in Data Collection</h2>
<h3>A.1 Sampling human entities</h3>
<p>We sample 183 human entities to be annotated as follows. We first choose entities from Wikidata whose instance of is human and have corresponding Wikipedia pages. We then categorize entities based on two dimensions: frequency and nationality, resulting in 20 categories. We then sample entities uniformly at random over all categories.</p>
<p>Frequency. We compute freqValue as a maximum of the entity occurrence in Wikipedia provided by Kandpal et al. (2022) and the pageview count of the Wikipedia page following Mallen et al. (2023). We found using one of them could lead to an underestimate of frequency levels due to failure in entity linking or mismatch in the Wikipedia page title, and taking a maximum of them provides a reasonable solution. We then assign one of five categories: 'Very rare' if freqValue $\in\left[0,10^{2}\right)$, 'Rare' if freqValue $\in$ $\left[10^{2}, 10^{3}\right)$, 'Medium' if freqValue $\in\left[10^{3}, 10^{4}\right)$, 'Frequent' if freqValue $\in\left[10^{4}, 10^{5}\right)$, and 'Very frequent' if freqValue $\in\left[10^{5}\right.$, $)$.</p>
<p>Nationality. We take country of citizenship from Wikidata and assign them one of four categories: 'North America', 'Europe \&amp; Middle East', 'Asia \&amp; Pacific' and 'Latin/South America \&amp; Africa'.</p>
<h3>A.2 Details in generating atomic facts</h3>
<p>We break out a generation automatically by splitting a generation into sentences, and feeding each sentence to InstructGPT (text-davinci-003) with a series of instructions to further break it down to a series of atomic facts. The prompt to InstructGPT is provided in Table 15. Outputs from InstructGPT are used (1) to human experts for revision (Section 3.3) and (2) for model-based evaluators (Section 4). We find human experts split and merged atomic facts from InstructGPT for $18 \%$ and $34 \%$ of the cases, respectively.</p>
<h3>A.3 More details on annotator recruitment</h3>
<p>We recruit freelancers through Upwork and pay 15-25 USD per hour. We recruit fact-checking experts-freelancers who mentioned fact-checking as their expertise-for Step 3. Every worker went through a qualification test of 2 hours and was tested to be highly qualified. We design one HIT to consist of three generations, one from each $\mathrm{LM}_{\text {SUBJ }}$,</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><code>Prompt: Tell me a bio of Ylona Garcia.
Sentence: [Ylona Garcia] has since appeared in various TV shows
such as ASAP (All-Star Sunday Afternoon Party), Wansapanataym
Presents: Annika PINTAsera and Maalaala Mo Kaya.
- Ylona Garcia has appeared in various TV shows. Supported
- She has appeared in ASAP. Supported
- ASAP stands for All-Star Sunday Afternoon Party. Supported
- ASAP is a TV show. Supported
- She has appeared in Wansapanataym Presents: Annika PINTAsera.
Not-supported
- Wansapanataym Presents: Annika PINTAsera is a TV show.
Irrelevant
- She has appeared in Maalaala Mo Kaya. Not-supported
- Maalaala Mo Kaya is a TV show. Irrelevant</code></p>
<p>Prompt: Tell me a bio of John Estes.
Sentence: William Estes is an American actor known for his role on CBS police drama Blue Bloods as Jameson JamieReagan.</p>
<ul>
<li>William Estes is an American. Irrelevant</li>
<li>William Estes is an actor. Irrelevant</li>
<li>William Estes is known for his role on CBS police drama Blue Bloods. Irrelevant</li>
<li>William Estes' role on Blue Bloods is Jameson "Jamie" Reagan. Irrelevant</li>
</ul>
<p>Table 7: Examples that contain Supported, Not-supported and Irrelevant. Sentences in bullet points indicate atomic facts.
for one prompt, because we find it saves annotation time in total. $10 \%$ of the HITs have two workers assigned to calculate the agreement rate; the rest have one worker assigned. The agreement rates are $96 \%, 90 \%$ and $88 \%$ for InstructGPT, ChatGPT and PerplexityAI, respectively. Appendix A. 5 discusses disagreement cases in more detail. The full instructions and the interface are provided in Figure 6 and Figure 7, respectively.</p>
<h2>A. 4 Examples in annotated data</h2>
<p>Table 7 provides examples of the human-annotated data, each atomic fact with an assigned label. Supported and Not-supported respectively indicate Wikipedia supports the fact and does not support the fact (either contradicts or does not contain any evidence). Irrelevant indicates the fact is irrelevant to the input prompt, which can further be divided into two cases: (1) the fact depends on other facts because it expands previous facts in a generation, and such other facts are Not-supported, e.g., in the first example in Table 7, and (2) the entire sentence is irrelevant to the prompt, independent from other facts in a generation, e.g., the second example in Table 7. The second case rarely happens with InstructGPT and ChatGPT, but happens considerably with PerplexityAI, i.e., $24.7 \%$ of generations of PerplexityAI have $\geq$ sentences marked as irrelevant without dependencies to other facts, compared to $0.5 \%$ and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">$\%$</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Different interpretations of the factual information</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Gen Gerhard Fischer is an inventor. Wiki Gerhard Fischer (inventor). ... was first patented by Dr. Gerhard Fischer in 1931. A metal detector had been invented some forty years earlier (1881) by Alexander Graham Bell ... <br> Gen Chadwick Boseman was a producer. Comment Chadwick Boseman is not known as a producer, but produced one music video.</td>
</tr>
<tr>
<td style="text-align: center;">Inferred (not directly mentioned but highly likely)</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">Gen Leach has since become a member of the England Test team. Comment Leach is a member of the England Test team, but since when is less clear.</td>
</tr>
<tr>
<td style="text-align: center;">Depends on how strict in judging the correctness</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Gen He made his Test debut for England in March 2018. Wiki On 16 March 2018, he was called up to England's Test squad (...) He made his debut in the second Test in Christchurch. Gen The building was the first LEED-certificated building in Edmonton. Wiki (..) became the first project in the City of Edmonton to achieve a LEED Gold status.</td>
</tr>
<tr>
<td style="text-align: center;">Subjective</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Gen Chadwick Boseman became an African American pioneer. Wiki Culture writer Steve Rose, in The Guardian, said that Boseman's career was revolutionary and he "leaves behind a gamechanging legacy" (...) Rose wrote: "Chadwick Boseman began his career playing African American icons and pioneers; he ends it as one himself."</td>
</tr>
<tr>
<td style="text-align: center;">Wikipedia not consistent</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Gen [Tim Fischer] was an Ambassador to the Holy See from 2009 to 2012. Wiki ... was later Ambassador to the Holy See from 2009 to 2012. (...) Australian Ambassador to the Holy See 2008-2012 Comment The plain text and the table of the Tim Fischer page as well as the Australian Ambassador to the Holy See page are inconsistent in his start year.</td>
</tr>
<tr>
<td style="text-align: center;">Two different entities</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Comment Carlos J. Alfonso vs. Carlos Alfonso</td>
</tr>
<tr>
<td style="text-align: center;">Mistakes in annotation</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">Gen Jack Leach is a left-handed batsman. Comment mentioned in the England cricket team page, Table Current Squad.</td>
</tr>
</tbody>
</table>
<p>Table 8: Categorization of disagreement cases. Gen indicates the generation from PerplexityAI, and Wiki indicates evidence text from Wikipedia. Comment indicates our comments.
$1.3 \%$ in InstructGPT and ChatGPT, respectively. This is because PerplexityAI often directly copies search results even if they are largely irrelevant to the input prompt. This is in agreement with a concurrent work from Liu et al. (2023a) that shows generative search engines like PerplexityAI copy incorrect search results and generate text that is irrelevant to the input query.</p>
<h2>A. 5 Qualitative Analysis</h2>
<p>Analysis of disagreement cases. We analyze the cases where two annotators assigned to a same generation disagree on a precision label for the same atomic fact. Categorization is provided in Table 8 The $70 \%$ is due to an inherent debatability on whether or not the fact is supported by a given source of knowledge, not satisfying Assumption 2 in Section 3.1. This is because there can be multiple interpretations of a fact, it is debatable whether or not an information can be inferred from a piece of text, or the atomic fact is subjective. For instance:</p>
<ul>
<li>Gerhard Fischer is an inventor: Gerhard Fischer is widely known as an inventor of a metal detector, and even the title of the Wikipedia article is "Gerhard Fischer (inventor)". However, it later turns out that he did not invent a metal detector; rather, he commercialized it.</li>
<li>Chadwick Boseman was a producer: Chadwick Boseman is widely known as another profession (singer) and there is no text that mentions him as a producer. However, he produced one music video.</li>
</ul>
<p>Nonetheless, since our agreement rate is fairly high ( $91 \%$ ), we think such cases are rare in our particular domain of people biographies. We include more discussion on other domains that such cases may be more frequent in the Limitation section.</p>
<p>Coverage of English Wikipedia. While factual prediction is inherently a function of a knowledge source given as part of the input, a potential concern is how representative using English Wikipedia as a knowledge source for evaluating people biographies with respect to its coverage. For instance, it is possible that, especially for rare entities, the coverage of information in Wikipedia is not high enough, and LMs may be penalized by generating information that is true even if not supported by Wikipedia (i.e., supported by other sources on the web).</p>
<p>To quantify the effect, we randomly sample 30 unsupported facts from ChatGPT on people whose categories are either 'rare' or 'very rare', and then validate them against the entire web. We found $10 \%$ ( 3 out of 30 facts) are in fact supported, even though they are not supported in Wikipedia. An example is [Hibo] Wardere published her memoir titled "Cut:</p>
<p>One Woman’s Fight Against FGM in Britain Today" which is not mentioned in Wikipedia but is found from Google Books.</p>
<p>Nonetheless, we found that Wikipedia has a high coverage and mentions most of the important information that we were able to find from any other sources on the web. This is in agreement with prior work that treated Wikipedia as a general knowledge source under the same reason (Chen et al., 2017; Petroni et al., 2021).</p>
<h2>B Details in Estimators</h2>
<h3>B.1 Implementation details</h3>
<p>As an $\mathrm{LM}_{\text {EVAL }}$, we use the best open LM and the best commercial LM at the time of conducting experiments: LLAMA 65B (Touvron et al., 2023) and LLAMA 7B trained on Super Natural Instructions (Inst-LLAMA, Wang et al., 2022) as the former, and ChatGPT (OpenAI, 2022) as the latter. For computing nonparametric probabilities, we use a single-mask variant of NPM with BM25 as in the original paper (Min et al., 2023), and use 0.3 as a thresholding hyperparameter.</p>
<p>For passage retrieval, we use Generalizable T5based Retrievers (GTR, a large variant), an unsupervised dense passage retrieval system (Ni et al., 2022). We restrict retrieved passages to be from the topic entity's page, and use $k=5$. We find our estimator is not sensitive to the choice of a retrieval system (ablations provided in Appendix B.3). As a retrieval corpus, we use the English Wikipedia from 04/01/2023 which is around the time the data annotation was completed, and split each page into passages with up to 256 tokens.</p>
<p>Additional baselines. We also compare with Self-check LM, a method from a concurrent work by Manakul et al. (2023). Self-check LM needs multiple samples generated from the $\mathrm{LM}<em _EVAL="{EVAL" _text="\text">{\text {SURJ }}$. It validates the given atomic fact by prompting $\mathrm{LM}</em>}}$ conditioning on each generated sample, ${ }^{17}$ making judgment (Supported or not) from each, and aggregates the results through a majority vote. This method assumes (1) the $\mathrm{LM<em _SURJ="{SURJ" _text="\text">{\text {SURJ }}$ is available at the time of evaluation and (2) the outputs from the $\mathrm{LM}</em>$ are nondeterministic, which makes it not applicable to PerplexityAI.}</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A case in which $\mathrm{F1}<em _MICRO="{MICRO" _text="\text">{\text {MICRO }}$ and Error Rate (ER) rank two evaluators differently. Evaluator A is better in $\mathrm{F1}</em>$, and Evaluator B is better in ER.}</p>
<h2>B. 2 Segment-level vs. system-level evaluation</h2>
<p>Besides how close the estimated FACTSCORE is to the ground truth FACTSCORE (Error Rate, as reported in Section 4), we also report $\mathbf{F 1}<em _MICRO="{MICRO" _text="\text">{\text {MICRO }}$. $\mathrm{F} 1</em>$ as follows.}}$ evaluates how well the model validates each individual atomic fact, assuming oracle atomic facts (atomic facts by human experts) are given, and evaluates how good the estimator is in identifying facts that are not Supported (NS). Formally, let $\mathcal{G}$ and $\mathcal{P}$ be sets of atomic facts in a set of generations that have Not-supported as a ground truth label and as a predicted label, respectively. We define $\mathrm{F} 1_{\text {MICRO }</p>
<p>$$
\mathrm{P}=\frac{\mathcal{P} \cap \mathcal{G}}{\mathcal{P}}, \mathrm{R}=\frac{\mathcal{P} \cap \mathcal{G}}{\mathcal{G}}, \quad \mathbf{F 1}_{\text {MICRO }}=\frac{2 \cdot \mathrm{P} \cdot \mathrm{R}}{\mathrm{P}+\mathrm{R}}
$$</p>
<p>We call them MICRO because they consider individual decisions rather than aggregated estimation.</p>
<p>ER vs. $\mathbf{F 1}<em _MICRO="{MICRO" _text="\text">{\text {MICRO }}$. $\mathrm{F} 1</em>$ and ER are also closely related to segment-level and system-level correlations to human judgments respectively, which have been extensively used in}}$ cares about the individual decision, while ER cares about the aggregated estimation. An evaluator that has a high (better) $\mathrm{F} 1_{\text {MICRO }}$ but always overestimates or underestimates factual precision may have a higher (worse) ER, e.g., Evaluator A in Figure 4. Conversely, an evaluator that has a lower (worse) $\mathrm{F} 1_{\text {MICRO }}$ but is not biased toward overestimation nor underestimation may have a lower (better) ER, e.g., Evaluator B in Figure 4. Prior work in model-based evaluation mainly reports aggregated scores since the goal is a comparison between different systems being evaluated (Zhang et al., 2020; Rashkin et al., 2021; Gao et al., 2022) while we report both to see the relationship between two types of metrics. $\mathrm{F} 1_{\text {MICRO }</p>
<table>
<thead>
<tr>
<th>Evaluator</th>
<th>retrv</th>
<th>$\mathrm{LM}_{\text {sско }}$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>InstGPT</td>
<td>ChatGPT</td>
<td>PPLAI</td>
</tr>
<tr>
<td>Always Supported</td>
<td>-</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>Always Not-supported</td>
<td>-</td>
<td>71.4</td>
<td>58.3</td>
<td>30.9</td>
</tr>
<tr>
<td>Random</td>
<td>-</td>
<td>52.2</td>
<td>45.0</td>
<td>25.7</td>
</tr>
<tr>
<td>No-context LM</td>
<td>$\boldsymbol{X}$</td>
<td>61.2</td>
<td>52.2</td>
<td>31.4</td>
</tr>
<tr>
<td>Self-check LM</td>
<td>$\boldsymbol{X}$</td>
<td>66.0</td>
<td>48.4</td>
<td>-</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM</td>
<td>$\checkmark$</td>
<td>78.7</td>
<td>61.9</td>
<td>51.1</td>
</tr>
<tr>
<td>NP</td>
<td>$\checkmark$</td>
<td>70.0</td>
<td>56.6</td>
<td>51.4</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM + NP</td>
<td>$\checkmark$</td>
<td>83.2</td>
<td>70.5</td>
<td>53.3</td>
</tr>
</tbody>
</table>
<p>Table 9: Results in $\mathbf{F 1}<em _EVAL="{EVAL" _text="\text">{\text {MICRO }}$ using Inst-LLAMA 7B as an $\mathrm{LM}</em>$. 'retrv' indicates whether or not retrieval is used. Self-check is not applicable to PerplexityAI whose outputs are semi-deterministic. Bold indicates the best performance.
developing evaluation metrics in machine translation (Ma et al., 2019; Thompson and Post, 2020) and summarization (Bhandari et al., 2020; Deutsch et al., 2021).}</p>
<p>Results. Results on $\mathrm{F} 1_{\text {MICRO }}$ are reported in Table 9. Self-check LM outperforms no-context LM by $4-11 \%$, which confirms findings from Manakul et al. (2023). However, both significantly underperform methods that use retrieval. This is in contrast to Manakul et al. (2023) that reports that Self-check without retrieval achieves performance that is close to that with retrieval, likely because the data in Manakul et al. (2023) contains more frequent entities. The fact that retrieval significantly helps is consistent with findings in Section 4.2 with an ER as a metric.</p>
<p>Adding NP improves Retrieve $\rightarrow$ LM by 2-9\%, again consistent with findings in Section 4.2. This is likely because Retrieve $\rightarrow$ LM often makes incorrect predictions when there is a strong bias from an LM or there are distracting passages, and considering nonparametric probabilities makes the model more robust to these factors. For instance, given an unsupported fact Samuel Oboh is Nigerian, Nocontext LM, Self-check LM and Retrieve $\rightarrow$ LM predict Supported due to a strong name-nationality bias. NPM correctly predicts Not-supported based on a passage Samuel Oboh ... is a Canadian architect, manager, .... It is also worth noting that this is different from findings in Section 4.2 that ChatGPT is not necessarily better than LLAMA+NP based on ER.</p>
<p>Using a stronger $\mathbf{L M}<em _MICRO="{MICRO" _text="\text">{\text {EVAL }}$ significantly improves $\mathbf{F 1}</em>$. Table 10 reports a comparison across}</p>
<table>
<thead>
<tr>
<th>Evaluator</th>
<th>retrv</th>
<th>$\mathrm{LM}_{\text {sско }}$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>InstGPT</td>
<td>ChatGPT</td>
<td>PPLAI</td>
</tr>
<tr>
<td>LLAMA 65B</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No-context LM</td>
<td>$\boldsymbol{X}$</td>
<td>22.2</td>
<td>20.0</td>
<td>18.6</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM</td>
<td>$\checkmark$</td>
<td>54.6</td>
<td>42.1</td>
<td>36.1</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM + NP</td>
<td>$\checkmark$</td>
<td>80.1</td>
<td>67.1</td>
<td>55.1</td>
</tr>
<tr>
<td>Inst-LLAMA 7B</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No-context LM</td>
<td>$\boldsymbol{X}$</td>
<td>61.2</td>
<td>52.2</td>
<td>31.4</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM</td>
<td>$\checkmark$</td>
<td>78.7</td>
<td>61.9</td>
<td>51.1</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM + NP</td>
<td>$\checkmark$</td>
<td>83.2</td>
<td>70.5</td>
<td>53.3</td>
</tr>
<tr>
<td>ChatGPT</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No-context LM</td>
<td>$\boldsymbol{X}$</td>
<td>40.0</td>
<td>25.4</td>
<td>25.4</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM</td>
<td>$\checkmark$</td>
<td>87.5</td>
<td>80.2</td>
<td>65.8</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM + NP</td>
<td>$\checkmark$</td>
<td>86.6</td>
<td>77.8</td>
<td>60.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Ablation in $\mathbf{F 1}<em _EVAL="{EVAL" _text="\text">{\text {MICRO }}$ on the choices of $\mathrm{LM}</em>$. 'retrv' indicates whether or not retrieval is used. Bold and Red bold indicate the best F1 within open-access LMs and commercial LMs, respectively.
different choices of an $\mathrm{LM}_{\text {EVAL }}$. Within the same method, Inst-LLAMA 7B outperforms LLAMA 65B, and ChatGPT outperforms both. Using retrieval is critical across all models, e.g., the best no-context model based on ChatGPT is underperformed by all models with retrieval. Using NP helps LLAMA-based models but not ChatGPT, likely because ChatGPT is less affected by incorrect prior from the LM or distracting passages.}</p>
<p>It is worth noting that these results are somewhat different from findings in Section 4.2 that ChatGPT is not necessarily better than LLAMA+NP. This is becauase, although ChatGPT is better in validating each individual atomic fact, most errors from ChatGPT are incorrectly assigning Supported to Not-supported facts, resulting in an overestimation of FACTSCORE. In contrast, LLAMA+NP is not biased toward overestimation or underestimation of the factual precision, resulting in an aggregated factual precision to be closer to the ground truth. This is similar to the trade-off between system-level and segment-level correlations in summarization evaluation (Bhandari et al., 2020; Deutsch et al., 2021).</p>
<h2>B. 3 Ablations</h2>
<p>QA Prompting vs. TF Prompting As described in Section 4.1, we use True or False as part of the prompt, so-called TF Prompting. An alternative is QA Prompting, which generates a question and the expected answer, obtains the answer for the generated question independent from the expected answer, and compares the expected answer</p>
<table>
<thead>
<tr>
<th>Evaluator</th>
<th>$\mathrm{LM}_{\text {sUG }}$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>InstGPT</td>
<td>ChatGPT</td>
<td>PPLAI</td>
</tr>
<tr>
<td>Always Supported</td>
<td>30.8</td>
<td>37.1</td>
<td>45.0</td>
</tr>
<tr>
<td>Always Not-supported</td>
<td>35.7</td>
<td>29.1</td>
<td>15.5</td>
</tr>
<tr>
<td>Random</td>
<td>50.5</td>
<td>50.2</td>
<td>43.2</td>
</tr>
<tr>
<td>QA Prompting</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No-context LM</td>
<td>56.5</td>
<td>48.8</td>
<td>32.5</td>
</tr>
<tr>
<td>Self-check LM</td>
<td>65.3</td>
<td>63.2</td>
<td>-</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM</td>
<td>65.3</td>
<td>58.2</td>
<td>47.3</td>
</tr>
<tr>
<td>TF Prompting</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>No-context LM</td>
<td>57.3</td>
<td>55.3</td>
<td>41.7</td>
</tr>
<tr>
<td>Self-check LM</td>
<td>68.0</td>
<td>61.9</td>
<td>-</td>
</tr>
<tr>
<td>Retrieve $\rightarrow$ LM</td>
<td>78.9</td>
<td>71.4</td>
<td>69.2</td>
</tr>
</tbody>
</table>
<p>Table 11: Results on $\mathrm{F1}<em _EVAL="{EVAL" _text="\text">{\text {MICRO }}$, comparing between the QA prompting and TF Prompting. We use Inst-LLAMA 7B as an $\mathrm{LM}</em>$.}}$. Self-check is not applicable to PerplexityAI since PerplexityAI outputs are semi-deterministic. Bold indicates the best $\mathrm{F1}_{\text {MICRO }</p>
<table>
<thead>
<tr>
<th>Retrieval</th>
<th>$\mathrm{LM}_{\text {sUG }}$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>InstGPT</td>
<td>ChatGPT</td>
<td>PPLAI</td>
</tr>
<tr>
<td>BM25</td>
<td>78.5</td>
<td>70.8</td>
<td>69.1</td>
</tr>
<tr>
<td>GTR Large</td>
<td>78.9</td>
<td>$\mathbf{7 1 . 4}$</td>
<td>$\mathbf{6 9 . 2}$</td>
</tr>
<tr>
<td>GTR xLarge</td>
<td>$\mathbf{7 9 . 2}$</td>
<td>71.3</td>
<td>69.0</td>
</tr>
</tbody>
</table>
<p>Table 12: Results on $\mathrm{F1}<em _MICRO="{MICRO" _text="\text">{\text {MICRO }}$, comparing different retrieval systems: BM25, GTR Large and GTR xLarge, all with Retrieve $\rightarrow$ LM based on Inst-LLAMA 7B. Bold indicates the best $\mathrm{F1}</em>$.
and the predicted answer. This approach has been widely studied in the summarization literature and recent work in factual precision }<em>Kryscinski et al. (2020); Wang et al. (2020); Gao et al. (2022); Manakul et al. (2023)</em>. Table 11 provides a comparison between two types of prompting. The TF approach significantly outperforms the QA approach, consistently over all methods. Our further analysis finds that this is due to generated questions often being overly vague or ambiguous. For instance, given a supported fact Samuel Oboh is an architect, the LM generates What is Samuel Oboh's job? as a question and Architect as an expected answer, and the obtained answer is Vice President. Although both Architect and Vice President are correct, they are not the same, thus the model incorrectly predicts Not-supported. Such cases make the model overpredict Not-supported, leading to many incorrect predictions.</p>
<p>Impact of the choice of retrieval. Table 12 compares Retrieve $\rightarrow$ LM methods based on a few passage retrieval systems, including BM25 <em>Lin et al. (2021)</em>, GTR Large and GTR xLarge. Results indi-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">$\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">No direct evidence from retrieved passages</td>
<td style="text-align: right;">70</td>
</tr>
<tr>
<td style="text-align: left;">Distracted by other passages</td>
<td style="text-align: right;">17</td>
</tr>
<tr>
<td style="text-align: left;">Atomic fact is context-dependent</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Wrong prediction even with the right passage</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Annotation error</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
<p>Table 13: Categorization of 30 samples incorrectly predicted by Retrieve $\rightarrow$ LM based on ChatGPT.
cate that all retrieval systems are equally good and Retrieve $\rightarrow$ LM is not sensitive to the choice of the retrieval system.</p>
<p>Qualitative analysis. Table 13 categories errors made by Retrieve $\rightarrow$ LM based on ChatGPT, the evaluator with the best $\mathrm{F1}_{\text {MICRO }}$. $70 \%$ of the errors are due to retrieved passages not providing direct evidence (either support or contradiction). These are difficult even for state-of-the-art retrieval systems and language models because validating facts often requires reading the entire page rather than a single passage, e.g., an actor not appearing in a particular film. $17 \%$ of errors are made because ChatGPT is being distracted by other passages, although it assigns a correct label if only a particular, correct passage is given.</p>
<h3>B. 4 More details in evaluation of new LMs (Section 4.3)</h3>
<p>Variance in estimation. Figure 5 reports FActScores estimated by two variants of our estimator as in Figure 3 but with 100 random subsets of the data. Specifically, we chose $N$ samples (out of 500) uniformly at random across 20 categories (defined in Appendix A.1) $M$ times and report the average and the standard deviation. We use $N={40,100,200}$ and $M=100$. Results indicate that the variance is overall low, preserving ranking between 13 subjects in most cases. As expected, the variance is lower as the sample size gets larger. Finally, the estimator based on ER based on LLAMA+NP (bottom) has an overall lower variance than the estimator based on ChatGPT (top).</p>
<h3>B. 5 Feasibility in applying FActScore to other domains</h3>
<p>As mentioned in the Limitation section, our paper mainly evaluates on people biographies using Wikipedia. Evaluating the generalizability of FActSCORE to other types of prompts and other domains is an avenue for future work.</p>
<p>As a proof of conept, we conduct small-scale</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Impact of different subsets of random samples in prompts. The FActScores to 13 subjects (human and 12 LMs) are rated by the two best variants of our estimator: ChatGPT (<strong>Top</strong>) and LLAMA+NP (<strong>Bottom</strong>), both with retrieval. The variance is overall low, and is lower as the sample size gets larger and with LLAMA+NP (bottom) than with ChatGPT (top).</p>
<p>Studies in the NLP domain. We first manually write 10 prompts asking about NLP papers: <em>Tell me a summary of</em> <paper-title>, and then obtain responses from ChatGPT. Next, we run FActScore against an ACL anthology as a knowledge source. Finally, we compute an error rate (ER)—a difference between humans' validation (labeled by authors) and the model's validation—as we do in Section 4. The ER is 7.41 (FActScore from humans being 66.20, and FActScore from the model being 73.61), which is comparable to ER values in people bios shown in Table 3.</p>
<p>This suggests that FActScore can generalize beyond people biographies. However, since this is a very small-scale experiment, we strongly encourage future research to explore the generalizability of FActScore to more domains at scale.</p>
<h3>C Editing Experiments</h3>
<p>Our experiments in Section 4 focus on automatically identifying factual precision errors in long-form generations by language models. Can these labels be used to actually correct errors in the long-form generations? In this section, we perform a preliminary exploration of methods to edit long-form LM generations to reflect factually correct information. We assume we have access to the human-annotated set of FActScore labels, and measure how good models are at editing incorrect sentences. In other words, we evaluate our editor models independent of the errors arising from the estimator.</p>
<h4>C.1 Methods</h4>
<p>We adopt a similar set of methods as Section 4.1 for our editing models. All methods below use four exemplar examples for in-context learning which were sampled from our dataset and removed for subsequent analysis. For all methods, we use OpenAI's ChatGPT [OpenAI2022GPT4V] as the base language model due to its generative capabilities.</p>
<p><strong>No-context LM.</strong> We feed language models the prompt Input: <sentence> Edit: and ask it to edit the text, without any retrieved context.</p>
<p><strong>Retrv→LM.</strong> To assist an editor model, we use a passage retrieval system to find supporting evidence from an external knowledge source (Wikipedia in our case). Our retrieval pipeline is</p>
<p>identical to Appendix B.1, but uses 3 retrieved passages instead of 5 due to context length restrictions.</p>
<ul>
<li>Atomic Facts. Additionally, we explore whether adding atomic facts and their labels assist a model with fine-grained editing. Specifically, after the input sentence we add information to the prompt of the form Fact 1 (True/False): <atomic fact 1> Fact 2 (True/False): <atomic fact 2> ... This data is also provided in the exemplars.
Non-edit baselines. Finally, we add some trivial baselines to lower-bound our editing metrics. Specifically, we measure the performance of input copying (no edits), as well as an editor with random token dropping / replacement on a random $25 \%$ subset of tokens.</li>
</ul>
<h2>C. 2 Evaluation</h2>
<p>In our data collection process (Section 3.3), along with our verification data we also collected goldstandard human written edits. Let $X=x_{1}, \ldots x_{N_{X}}$ be the input sentence and $G=g_{1}, \ldots g_{N_{G}}$ be the gold edited sentence. We evaluate the quality of the model-generated edit ( $E=e_{1}, \ldots, e_{N_{E}}$ ) using three automatic metrics,
(1) Error Localization (ErrLoc): Our first metric measures how well the editor identifies errors within the input sentence. Specifically, we first create a "token preservation string", marking token $x_{i}$ in the input sentence $X$ as "Preserved" or "Not Preserved". We then compute the macro-averaged F1 score between the token preservation strings derived from the gold edit and the model-generated edit. We remove stopwords, punctuation and lowercase all words before performing this calculation. To equally weigh every sentence, F1 scores are independently computed for each sentence before a final averaging.
(2) Edit Correctness (EditCorr): Our second metric assesses the quality of the additional tokens added by the model-generated edit. Specifically, we check the token-level F1 score (Rajpurkar et al., 2016) comparing the new tokens added by the gold edit $G$ and the new tokens added by the modelgenerated edit $E$. More concretely,</p>
<p>$$
\begin{aligned}
N_{\text {common }} &amp; =\sum_{e_{i} \in E, e_{i} \notin X} e_{i} \in G \
\text { precision } &amp; =N_{\text {common }} /\left|\left{e_{i} \in E, e_{i} \notin X\right}\right| \
\text { recall } &amp; =N_{\text {common }} /\left|\left{g_{i} \in G, g_{i} \notin X\right}\right|
\end{aligned}
$$</p>
<p>EditCorr (F1) $=\mathrm{HM}($ precision, recall $)$
where $|\cdot|$ is the set cardinality and HM denotes a harmonic mean. For this metric, we discard data points where the gold edit did not add new tokens. Similar to ErrLoc, we also remove stopwords, remove punctuation and lowercase strings before calculating EditCorr scores.
(3) SIM alignment (SimAl): Finally, due to the large output space of possible edits, we also adopt a metric which rewards paraphrases of the gold edits. We use semantic similarity embeddings from Wieting et al. (2022) which map paraphrases to a similar part of a vector space. We check the similarity between the model edit $E$ and the gold edit $G$, normalizing it by the similarity between $G$ and the original input $X .{ }^{18}$ Specifically,</p>
<p>$$
\operatorname{Sim}=\max \left(0, \frac{s(G, E)-s(G, X)}{1-s(G, X)}\right)
$$</p>
<p>where $s(A, B)$ is the semantic similarity score (normalized to $[0,1]$ ) from the model in Wieting et al. (2022). Intuitively, this metric measures how much closer $G$ and $E$ are compared to $G$ and $X$.</p>
<h2>C. 3 Results</h2>
<p>We present our editing results in Table 14. Overall, we find that:</p>
<p>All editing models perform better than trivial lower bounds. Overall, we find that all editor models outperform lower-bound baselines like random noise. This even happens in the no-context LM setting, where ChatGPT is editing its own output (or search engine augmented Perplexity AI's outputs), but can still perform non-trivial corrections (6.8 ErrCorr for ChatGPT correcting its own outputs vs 0.1 for a random noise editor baseline).</p>
<p>Retrieval significantly helps with editing performance. Across all base language models and metrics, augmenting the editor with retrieved paragraphs boosts performance ( $6.8 \rightarrow 16.8$ ErrCorr, $4.0 \rightarrow 9.5$ SimAl for ChatGPT correcting its own outputs). We hypothesize that the internal parametric knowledge in ChatGPT has insufficient information about the topic (as we also observed in Section 3.4) to perform fine-grained editing, and using external knowledge from Wikipedia greatly simplifies error localization and correction. This also corroborates with our findings in Section 4.2.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{18}$ We avoid taking the vector differences between the original / edited text since edit vectors (Guu et al., 2018) were not explicitly modeled in Wieting et al. (2022).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Consisting of 110-151 words (Table 1), in contrast to 18-29 in Gao et al. (2022) and 65 in Liu et al. (2023a).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3094 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3094</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3094</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267897903</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.15000v2.pdf" target="_blank">Divide-or-Conquer? Which Part Should You Distill Your LLM?</a></p>
                <p><strong>Paper Abstract:</strong> Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization. These results indicate that by using smaller, distilled problem decomposition models in combination with problem solving LLMs we can achieve reasoning with cost-efficient inference and local adaptation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3094.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3094.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo (teacher)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (teacher LLM used for demonstration generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as the black-box teacher to generate decomposition demonstrations (subquestions) and chain-of-thought style solving; serves as the reference for distillation of decomposition and solving capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A black-box OpenAI conversational/assistant model used as the teacher to generate decomposition instructions, subquestions and (optionally) answers for distillation; treated as a high-capacity reasoning oracle in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) / step-by-step prompting', 'Static Two-Stage decomposition (generate subquestions then solve)', 'Dynamic decomposition (self-ask style / iterative follow-up questions)', 'Least-to-Most style decomposition']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>GPT-3.5 is prompted to (a) directly reason via CoT (step-by-step chains), (b) generate a static list of subquestions (I_decomp) that decompose the main question and then answer them in a separate solving stage (I_ans), and (c) is also used as reference in a dynamic self-ask style where follow-up questions depend on earlier answers; the paper uses GPT-3.5-generated decompositions as training targets for student decomposers.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Uses multiple but related decomposition-centric reasoning styles (CoT, static two-stage, and dynamic self-ask); the paper elicits decomposition behavior through prompting rather than architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, DROP, Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GSM8K (math word problems, EM), DROP (reading-comprehension requiring discrete reasoning, F1), Bamboogle (compositional QA combining two questions into one, accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>As teacher decomposer+solver (Two-stage with teacher): GSM8K ~66.05 EM, DROP ~59.38 F1 (Table 4); single-stage teacher reported ~65.13 EM on GSM8K, ~55.73 F1 on DROP and ~54.40 on Bamboogle (Table 1 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Two-stage (decompose then solve) using GPT-3.5 generally outperforms single-stage (direct) GPT prompting; dynamic self-ask performed worse than static two-stage in evaluated tasks and had much higher token/inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3.5 provides high-quality decompositions that improve downstream solving when used in a static two-stage pipeline; those decompositions are effective targets for distillation into smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Dynamic decomposition (self-ask) with GPT-3.5 had poorer end-task performance and substantially higher inference cost than static two-stage decomposition on GSM8K and Bamboogle in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3094.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3094.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SD-T (distilled decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S_D-T (student decomposer distilled from teacher demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A student model (Vicuna-13B in main experiments) fine-tuned to imitate teacher-generated subquestion decompositions; intended as a cheap, general decomposer replacement for the teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SD-T (Vicuna-13B fine-tuned as decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-13B base model fine-tuned on teacher (GPT-3.5) decomposition demonstrations (instruction I_decomp) to output a set of up-to-three subquestions for each input query.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Static Two-Stage decomposition (generate subquestions only)', "Imitation of teacher's decomposition style via supervised distillation"]</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SD-T is trained with cross-entropy to map (I_decomp, Q) -> {S_i} using teacher outputs as ground truth; it only performs the decomposition (planning) stage and delegates solving to a separate solver (which can be a larger LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single, focused decomposition method (static subquestion generation) — it imitates the teacher's decomposition outputs rather than exploring diverse reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, DROP, Bamboogle (evaluated as decomposer plugged into solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above: math word problems (GSM8K), discrete reading-comprehension (DROP), compositional QA (Bamboogle). SD-T is evaluated by pairing with various solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>When paired with the teacher solver (GPT) and without oracle-answer screening: GSM8K 67.02 EM, DROP 48.98 F1, Bamboogle 55.19 accuracy (Table 1 'w/o oracle answer' row for SD-T + GPT solver). SD-T's decompositions closely match teacher decompositions on unseen tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Replacing the teacher decomposer with SD-T is at least comparable to using the teacher and significantly better than using the untuned backbone (Vicuna) as decomposer; pairing SD-T with different solvers yields improvements over weaker decomposers, especially helping weaker solvers more.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposition is easier to distill: a relatively small 13B model (SD-T) can learn teacher decomposition behavior and generalize across domains and solvers while reducing inference cost substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>SD-T trained on domain-specific teacher demonstrations shows only slight degradation out-of-domain, but if trained solely to improve decomposition without separation from solving, fine-tuning for decomposition can harm the model's own solving ability (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3094.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3094.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SD-R (screened distilled decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S_D-R (student decomposer distilled with rejection screening using oracle answers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A student decomposer like SD-T but distilled on a screened subset of teacher decompositions that lead to correct final answers (filtered using teacher answers and oracle A), yielding higher-quality decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SD-R (Vicuna-13B fine-tuned with rejection screening)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-13B distilled on teacher decompositions but filtered via an additional teacher-solving step and keeping only decomposition examples where the teacher's answer matches the oracle; used to further boost decomposer quality in domain-adapted settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Static Two-Stage decomposition (teacher-filtered)', 'Rejection-sampling-style screening of decomposition demonstrations']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>SD-R uses two-pass teacher outputs: first teacher produces subquestions, then teacher attempts to answer them; instances where teacher answer matches oracle A are kept, producing a screened training set used for fine-tuning the student decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single decomposition method (static) but trained with additional quality screening to focus on high-utility decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, DROP, Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same datasets; SD-R is evaluated plugged into different solvers and across domains to assess generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>SD-R paired with GPT-3.5 solver: GSM8K 66.56 EM, DROP 61.94 F1. Paired with GPT-4 solver: GSM8K 91.58 EM, DROP 81.02 F1 (Table 4). In Table 1 (w/ oracle answer), SD-R + GPT solver shows GSM8K 67.78 EM, DROP 51.55 F1, Bamboogle 57.97 accuracy and in DROP it even outperforms the teacher in F1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>SD-R consistently slightly improves over SD-T and substantially improves over the untuned backbone decomposer; screening with oracle answers can further increase downstream performance and sometimes exceed teacher performance on certain metrics/datasets (DROP F1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Screening decomposition training data with oracle answers (SD-R) yields a distilled decomposer that can outperform or match the teacher in some settings and generalizes well to other solvers and domains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Screening reduces data size (data loss), and the benefit depends on oracle availability; when oracle answers are not available, SD-T remains the practical alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3094.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3094.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B (backbone student)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-13B (open-source student backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 13B model used as both untuned baseline, student decomposer, and student solver (when fine-tuned); serves as the primary student architecture in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM (13B) used both as an untuned backbone baseline and as the basis for fine-tuned student decomposer (SD-T/SD-R) and student solvers (S_E-T, S_E-A).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Single-stage direct prompting (vanilla usage)', 'Static Two-Stage when paired with decompositions', 'Fine-tuned to imitate teacher decomposition or solving (as student)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>As backbone, untreated Vicuna produces decompositions or direct answers depending on prompts; as decomposer or solver it is fine-tuned on teacher outputs (decompositions or answers).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Primarily similar/single styles depending on how it is fine-tuned (either direct solving or decomposition imitation) — does not inherently employ diverse, heterogeneous reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, DROP, Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same datasets; Vicuna-13B is evaluated in three configurations: untuned backbone, as student decomposer, and as student solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Untuned backbone decomposer + untuned solver (Vicuna/Vicuna two-stage) reported GSM8K 28.0 EM, DROP 33.78 F1 (Table 4). Untuned single-stage Vicuna performs much worse (e.g., Table 1 baseline single-stage Vicuna-13B numbers are substantially lower).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Replacing Vicuna decomposer with SD-T/SD-R substantially improves downstream solving quality; fine-tuning Vicuna as a student solver (S_E variants) often harms performance compared to using the teacher or using Vicuna as solver with a stronger decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla Vicuna decomposer is weak and generates distracting/unhelpful subquestions; upgrading the decomposer (via SD-T / SD-R) yields clear gains even if the solver remains Vicuna.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Fine-tuning a model to specialize in decomposition (without separating solving) can hurt its solving ability (Table 7). Also, student-solvers distilled to mimic teacher solving (S_E-T) suffered large performance drops and poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3094.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3094.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S_E-T / S_E-A (student solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S_E-T (student solver distilled from teacher answers) and S_E-A (student solver fine-tuned with oracle answers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-based student solvers trained to imitate teacher solving (S_E-T) or trained using oracle final answers (S_E-A); evaluated as replacements for teacher solvers in two-stage pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>S_E-T / S_E-A (Vicuna-13B variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-13B fine-tuned: S_E-T uses teacher's (GPT) answers to subquestions and final answer as targets; S_E-A uses teacher subquestion answers but oracle final answers (A) when available.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Attempted imitation of teacher solving behavior (answering subquestions and final question)', 'Direct-final-answer training (S_E-A) in oracle-available settings']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>S_E models are trained to produce answers given subquestions {S_i} and the main question, either recreating teacher-generated intermediate answers ({Âs_i}) and final answer (S_E-T) or using oracle final answer (S_E-A); variants also trained to skip producing intermediate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single solving-focused approach (attempt to mimic teacher solving); does not employ multiple distinct reasoning styles beyond the supervision signal choice (teacher vs oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, DROP, Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used as solvers in the two-stage pipeline where the decomposer supplies subquestions; performance is compared to teacher solver and to the untuned backbone solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports that swapping in S_E-T as the solver significantly harms performance compared to teacher solvers; S_E-A (oracle-finetuned) improves in-domain but generalizes poorly out-of-domain. Exact per-dataset numeric drops are reported qualitatively: substantial reductions relative to teacher solver, and worse than SD-T decomposer + teacher solver combination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Distilling the solver capability into a small student (S_E-T) leads to large performance drops and poor generalization; in contrast distilling only the decomposer (SD-T/SD-R) is far more successful. Removing intermediate-answer targets sometimes helps on DROP but hurts on GSM8K, showing unstable behavior across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Solving is knowledge-intensive and hard to compress: student solvers trained with few demonstrations cannot match teacher solving quality and frequently confuse subquestion answers with the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>S_E models sometimes produce final answers that are actually intermediate-subquestion responses; excluding teacher-generated intermediate answers in training yields task-dependent mixed results (improvement on DROP but degradation on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3094.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3094.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Two-Stage vs Dynamic Self-Ask</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static two-stage decomposition (generate subquestions then solve) compared to dynamic self-ask (iterative follow-ups)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of a static pipeline that first produces all subquestions then solves them versus a dynamic pipeline that asks and answers follow-ups conditioned on previous answers; analyzed primarily with GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Static Two-Stage vs Dynamic Self-Ask (pipeline-level methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Static two-stage: decomposer generates up to three subquestions in one shot (I_decomp), then a solver answers them. Dynamic self-ask: the model appends 'Are follow-up questions needed here?' and iteratively generates and answers subquestions conditioned on intermediate results.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Static decomposition (one-shot subquestion list)', 'Dynamic iterative decomposition (self-ask)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Static method separates planning and solving and keeps them independent; dynamic method conditions generation of each subquestion on answers to previous subquestions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Two distinct pipeline styles compared: static (single-shot decomposition) vs dynamic (iterative, dependent steps).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K, Bamboogle</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to test whether interdependent iterative decomposition yields benefits on math and compositional QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported: dynamic planning performed worse than static on both datasets (exact numbers in Table 6) and incurred substantially higher token/inference cost (×3.96 and ×2.32 more input tokens on Bamboogle and GSM8K respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Static two-stage outperformed dynamic self-ask on the evaluated datasets; dynamic is more costly and more sensitive to intermediate-step errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For the tested math and QA datasets, static decomposition is preferable: it gives better accuracy and far lower inference cost; dynamic decomposition may be valuable in other settings but is brittle here.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Dynamic decomposition, despite its theoretical flexibility, produced poorer end-task performance and much higher cost in these experiments; errors in intermediate steps propagate and hurt final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3094.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3094.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (open-source 7B backbone used for Bamboogle experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller (7B) open-source backbone tested to validate that the observed patterns (distilled decomposer improves performance) generalize across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B model used as a backbone/solver in experiments on the Bamboogle dataset to verify that decomposer distillation benefits extend beyond Vicuna.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Static Two-Stage decomposition when paired with decomposers (GPT, SD-T, SD-R)', 'Backbone untuned single-stage answering']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Mistral-7B was used analogously to Vicuna: either as an untuned backbone solver or as a student solver; evaluated with various decomposers to check robustness of findings.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single methods similar to Vicuna experiments; no additional diverse reasoning styles introduced specifically for Mistral.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Bamboogle (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Compositional dataset combining two sub-questions into a single complex question; used to test cross-backbone consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports consistent patterns across backbones (Table 5): distilled decomposers (SD-T/SD-R) yield consistent improvements on Bamboogle when paired with Mistral-7B versus using the vanilla backbone decomposer; exact per-cell numbers are reported in Table 5 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Distilled decomposers improve solver performance for both Vicuna-13B and Mistral-7B backbones; weaker solvers see larger relative gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The advantage of distilling decomposition generalizes to different backbone families (Mistral-7B), indicating robustness of the decomposition-distillation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No specific negative results unique to Mistral-7B reported; the same caveats about solver distillation (S_E) apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide-or-Conquer? Which Part Should You Distill Your LLM?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-ask: Asking follow-up questions to improve multi-step question answering (Press et al.) <em>(Rating: 2)</em></li>
                <li>Symbolic chain-of-thought distillation: Small models can also "think" step-by-step <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3094",
    "paper_id": "paper-267897903",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-Turbo (teacher)",
            "name_full": "GPT-3.5-Turbo (teacher LLM used for demonstration generation)",
            "brief_description": "Used as the black-box teacher to generate decomposition demonstrations (subquestions) and chain-of-thought style solving; serves as the reference for distillation of decomposition and solving capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "A black-box OpenAI conversational/assistant model used as the teacher to generate decomposition instructions, subquestions and (optionally) answers for distillation; treated as a high-capacity reasoning oracle in the paper.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT) / step-by-step prompting",
                "Static Two-Stage decomposition (generate subquestions then solve)",
                "Dynamic decomposition (self-ask style / iterative follow-up questions)",
                "Least-to-Most style decomposition"
            ],
            "reasoning_methods_description": "GPT-3.5 is prompted to (a) directly reason via CoT (step-by-step chains), (b) generate a static list of subquestions (I_decomp) that decompose the main question and then answer them in a separate solving stage (I_ans), and (c) is also used as reference in a dynamic self-ask style where follow-up questions depend on earlier answers; the paper uses GPT-3.5-generated decompositions as training targets for student decomposers.",
            "diversity_of_methods": "Uses multiple but related decomposition-centric reasoning styles (CoT, static two-stage, and dynamic self-ask); the paper elicits decomposition behavior through prompting rather than architectural changes.",
            "reasoning_task_name": "GSM8K, DROP, Bamboogle",
            "reasoning_task_description": "GSM8K (math word problems, EM), DROP (reading-comprehension requiring discrete reasoning, F1), Bamboogle (compositional QA combining two questions into one, accuracy).",
            "performance_by_method": "As teacher decomposer+solver (Two-stage with teacher): GSM8K ~66.05 EM, DROP ~59.38 F1 (Table 4); single-stage teacher reported ~65.13 EM on GSM8K, ~55.73 F1 on DROP and ~54.40 on Bamboogle (Table 1 entries).",
            "comparison_of_methods": "Two-stage (decompose then solve) using GPT-3.5 generally outperforms single-stage (direct) GPT prompting; dynamic self-ask performed worse than static two-stage in evaluated tasks and had much higher token/inference cost.",
            "key_findings": "GPT-3.5 provides high-quality decompositions that improve downstream solving when used in a static two-stage pipeline; those decompositions are effective targets for distillation into smaller models.",
            "counter_examples_or_negative_results": "Dynamic decomposition (self-ask) with GPT-3.5 had poorer end-task performance and substantially higher inference cost than static two-stage decomposition on GSM8K and Bamboogle in this work.",
            "uuid": "e3094.0",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SD-T (distilled decomposer)",
            "name_full": "S_D-T (student decomposer distilled from teacher demonstrations)",
            "brief_description": "A student model (Vicuna-13B in main experiments) fine-tuned to imitate teacher-generated subquestion decompositions; intended as a cheap, general decomposer replacement for the teacher.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SD-T (Vicuna-13B fine-tuned as decomposer)",
            "model_description": "Vicuna-13B base model fine-tuned on teacher (GPT-3.5) decomposition demonstrations (instruction I_decomp) to output a set of up-to-three subquestions for each input query.",
            "model_size": "13B",
            "reasoning_methods": [
                "Static Two-Stage decomposition (generate subquestions only)",
                "Imitation of teacher's decomposition style via supervised distillation"
            ],
            "reasoning_methods_description": "SD-T is trained with cross-entropy to map (I_decomp, Q) -&gt; {S_i} using teacher outputs as ground truth; it only performs the decomposition (planning) stage and delegates solving to a separate solver (which can be a larger LLM).",
            "diversity_of_methods": "Single, focused decomposition method (static subquestion generation) — it imitates the teacher's decomposition outputs rather than exploring diverse reasoning strategies.",
            "reasoning_task_name": "GSM8K, DROP, Bamboogle (evaluated as decomposer plugged into solvers)",
            "reasoning_task_description": "Same as above: math word problems (GSM8K), discrete reading-comprehension (DROP), compositional QA (Bamboogle). SD-T is evaluated by pairing with various solvers.",
            "performance_by_method": "When paired with the teacher solver (GPT) and without oracle-answer screening: GSM8K 67.02 EM, DROP 48.98 F1, Bamboogle 55.19 accuracy (Table 1 'w/o oracle answer' row for SD-T + GPT solver). SD-T's decompositions closely match teacher decompositions on unseen tests.",
            "comparison_of_methods": "Replacing the teacher decomposer with SD-T is at least comparable to using the teacher and significantly better than using the untuned backbone (Vicuna) as decomposer; pairing SD-T with different solvers yields improvements over weaker decomposers, especially helping weaker solvers more.",
            "key_findings": "Decomposition is easier to distill: a relatively small 13B model (SD-T) can learn teacher decomposition behavior and generalize across domains and solvers while reducing inference cost substantially.",
            "counter_examples_or_negative_results": "SD-T trained on domain-specific teacher demonstrations shows only slight degradation out-of-domain, but if trained solely to improve decomposition without separation from solving, fine-tuning for decomposition can harm the model's own solving ability (Table 7).",
            "uuid": "e3094.1",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SD-R (screened distilled decomposer)",
            "name_full": "S_D-R (student decomposer distilled with rejection screening using oracle answers)",
            "brief_description": "A student decomposer like SD-T but distilled on a screened subset of teacher decompositions that lead to correct final answers (filtered using teacher answers and oracle A), yielding higher-quality decompositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SD-R (Vicuna-13B fine-tuned with rejection screening)",
            "model_description": "Vicuna-13B distilled on teacher decompositions but filtered via an additional teacher-solving step and keeping only decomposition examples where the teacher's answer matches the oracle; used to further boost decomposer quality in domain-adapted settings.",
            "model_size": "13B",
            "reasoning_methods": [
                "Static Two-Stage decomposition (teacher-filtered)",
                "Rejection-sampling-style screening of decomposition demonstrations"
            ],
            "reasoning_methods_description": "SD-R uses two-pass teacher outputs: first teacher produces subquestions, then teacher attempts to answer them; instances where teacher answer matches oracle A are kept, producing a screened training set used for fine-tuning the student decomposer.",
            "diversity_of_methods": "Single decomposition method (static) but trained with additional quality screening to focus on high-utility decompositions.",
            "reasoning_task_name": "GSM8K, DROP, Bamboogle",
            "reasoning_task_description": "Same datasets; SD-R is evaluated plugged into different solvers and across domains to assess generalization.",
            "performance_by_method": "SD-R paired with GPT-3.5 solver: GSM8K 66.56 EM, DROP 61.94 F1. Paired with GPT-4 solver: GSM8K 91.58 EM, DROP 81.02 F1 (Table 4). In Table 1 (w/ oracle answer), SD-R + GPT solver shows GSM8K 67.78 EM, DROP 51.55 F1, Bamboogle 57.97 accuracy and in DROP it even outperforms the teacher in F1.",
            "comparison_of_methods": "SD-R consistently slightly improves over SD-T and substantially improves over the untuned backbone decomposer; screening with oracle answers can further increase downstream performance and sometimes exceed teacher performance on certain metrics/datasets (DROP F1).",
            "key_findings": "Screening decomposition training data with oracle answers (SD-R) yields a distilled decomposer that can outperform or match the teacher in some settings and generalizes well to other solvers and domains.",
            "counter_examples_or_negative_results": "Screening reduces data size (data loss), and the benefit depends on oracle availability; when oracle answers are not available, SD-T remains the practical alternative.",
            "uuid": "e3094.2",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Vicuna-13B (backbone student)",
            "name_full": "Vicuna-13B (open-source student backbone)",
            "brief_description": "An open-source 13B model used as both untuned baseline, student decomposer, and student solver (when fine-tuned); serves as the primary student architecture in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna-13B",
            "model_description": "Open-source LLM (13B) used both as an untuned backbone baseline and as the basis for fine-tuned student decomposer (SD-T/SD-R) and student solvers (S_E-T, S_E-A).",
            "model_size": "13B",
            "reasoning_methods": [
                "Single-stage direct prompting (vanilla usage)",
                "Static Two-Stage when paired with decompositions",
                "Fine-tuned to imitate teacher decomposition or solving (as student)"
            ],
            "reasoning_methods_description": "As backbone, untreated Vicuna produces decompositions or direct answers depending on prompts; as decomposer or solver it is fine-tuned on teacher outputs (decompositions or answers).",
            "diversity_of_methods": "Primarily similar/single styles depending on how it is fine-tuned (either direct solving or decomposition imitation) — does not inherently employ diverse, heterogeneous reasoning styles.",
            "reasoning_task_name": "GSM8K, DROP, Bamboogle",
            "reasoning_task_description": "Same datasets; Vicuna-13B is evaluated in three configurations: untuned backbone, as student decomposer, and as student solver.",
            "performance_by_method": "Untuned backbone decomposer + untuned solver (Vicuna/Vicuna two-stage) reported GSM8K 28.0 EM, DROP 33.78 F1 (Table 4). Untuned single-stage Vicuna performs much worse (e.g., Table 1 baseline single-stage Vicuna-13B numbers are substantially lower).",
            "comparison_of_methods": "Replacing Vicuna decomposer with SD-T/SD-R substantially improves downstream solving quality; fine-tuning Vicuna as a student solver (S_E variants) often harms performance compared to using the teacher or using Vicuna as solver with a stronger decomposer.",
            "key_findings": "Vanilla Vicuna decomposer is weak and generates distracting/unhelpful subquestions; upgrading the decomposer (via SD-T / SD-R) yields clear gains even if the solver remains Vicuna.",
            "counter_examples_or_negative_results": "Fine-tuning a model to specialize in decomposition (without separating solving) can hurt its solving ability (Table 7). Also, student-solvers distilled to mimic teacher solving (S_E-T) suffered large performance drops and poor generalization.",
            "uuid": "e3094.3",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "S_E-T / S_E-A (student solvers)",
            "name_full": "S_E-T (student solver distilled from teacher answers) and S_E-A (student solver fine-tuned with oracle answers)",
            "brief_description": "Vicuna-based student solvers trained to imitate teacher solving (S_E-T) or trained using oracle final answers (S_E-A); evaluated as replacements for teacher solvers in two-stage pipelines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "S_E-T / S_E-A (Vicuna-13B variants)",
            "model_description": "Vicuna-13B fine-tuned: S_E-T uses teacher's (GPT) answers to subquestions and final answer as targets; S_E-A uses teacher subquestion answers but oracle final answers (A) when available.",
            "model_size": "13B",
            "reasoning_methods": [
                "Attempted imitation of teacher solving behavior (answering subquestions and final question)",
                "Direct-final-answer training (S_E-A) in oracle-available settings"
            ],
            "reasoning_methods_description": "S_E models are trained to produce answers given subquestions {S_i} and the main question, either recreating teacher-generated intermediate answers ({Âs_i}) and final answer (S_E-T) or using oracle final answer (S_E-A); variants also trained to skip producing intermediate answers.",
            "diversity_of_methods": "Single solving-focused approach (attempt to mimic teacher solving); does not employ multiple distinct reasoning styles beyond the supervision signal choice (teacher vs oracle).",
            "reasoning_task_name": "GSM8K, DROP, Bamboogle",
            "reasoning_task_description": "Used as solvers in the two-stage pipeline where the decomposer supplies subquestions; performance is compared to teacher solver and to the untuned backbone solver.",
            "performance_by_method": "Paper reports that swapping in S_E-T as the solver significantly harms performance compared to teacher solvers; S_E-A (oracle-finetuned) improves in-domain but generalizes poorly out-of-domain. Exact per-dataset numeric drops are reported qualitatively: substantial reductions relative to teacher solver, and worse than SD-T decomposer + teacher solver combination.",
            "comparison_of_methods": "Distilling the solver capability into a small student (S_E-T) leads to large performance drops and poor generalization; in contrast distilling only the decomposer (SD-T/SD-R) is far more successful. Removing intermediate-answer targets sometimes helps on DROP but hurts on GSM8K, showing unstable behavior across tasks.",
            "key_findings": "Solving is knowledge-intensive and hard to compress: student solvers trained with few demonstrations cannot match teacher solving quality and frequently confuse subquestion answers with the final answer.",
            "counter_examples_or_negative_results": "S_E models sometimes produce final answers that are actually intermediate-subquestion responses; excluding teacher-generated intermediate answers in training yields task-dependent mixed results (improvement on DROP but degradation on GSM8K).",
            "uuid": "e3094.4",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Static Two-Stage vs Dynamic Self-Ask",
            "name_full": "Static two-stage decomposition (generate subquestions then solve) compared to dynamic self-ask (iterative follow-ups)",
            "brief_description": "Comparison of a static pipeline that first produces all subquestions then solves them versus a dynamic pipeline that asks and answers follow-ups conditioned on previous answers; analyzed primarily with GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Static Two-Stage vs Dynamic Self-Ask (pipeline-level methods)",
            "model_description": "Static two-stage: decomposer generates up to three subquestions in one shot (I_decomp), then a solver answers them. Dynamic self-ask: the model appends 'Are follow-up questions needed here?' and iteratively generates and answers subquestions conditioned on intermediate results.",
            "model_size": null,
            "reasoning_methods": [
                "Static decomposition (one-shot subquestion list)",
                "Dynamic iterative decomposition (self-ask)"
            ],
            "reasoning_methods_description": "Static method separates planning and solving and keeps them independent; dynamic method conditions generation of each subquestion on answers to previous subquestions.",
            "diversity_of_methods": "Two distinct pipeline styles compared: static (single-shot decomposition) vs dynamic (iterative, dependent steps).",
            "reasoning_task_name": "GSM8K, Bamboogle",
            "reasoning_task_description": "Used to test whether interdependent iterative decomposition yields benefits on math and compositional QA tasks.",
            "performance_by_method": "Reported: dynamic planning performed worse than static on both datasets (exact numbers in Table 6) and incurred substantially higher token/inference cost (×3.96 and ×2.32 more input tokens on Bamboogle and GSM8K respectively).",
            "comparison_of_methods": "Static two-stage outperformed dynamic self-ask on the evaluated datasets; dynamic is more costly and more sensitive to intermediate-step errors.",
            "key_findings": "For the tested math and QA datasets, static decomposition is preferable: it gives better accuracy and far lower inference cost; dynamic decomposition may be valuable in other settings but is brittle here.",
            "counter_examples_or_negative_results": "Dynamic decomposition, despite its theoretical flexibility, produced poorer end-task performance and much higher cost in these experiments; errors in intermediate steps propagate and hurt final answers.",
            "uuid": "e3094.5",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral-7B (backbone)",
            "name_full": "Mistral-7B (open-source 7B backbone used for Bamboogle experiments)",
            "brief_description": "A smaller (7B) open-source backbone tested to validate that the observed patterns (distilled decomposer improves performance) generalize across architectures.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_description": "Open-source 7B model used as a backbone/solver in experiments on the Bamboogle dataset to verify that decomposer distillation benefits extend beyond Vicuna.",
            "model_size": "7B",
            "reasoning_methods": [
                "Static Two-Stage decomposition when paired with decomposers (GPT, SD-T, SD-R)",
                "Backbone untuned single-stage answering"
            ],
            "reasoning_methods_description": "Mistral-7B was used analogously to Vicuna: either as an untuned backbone solver or as a student solver; evaluated with various decomposers to check robustness of findings.",
            "diversity_of_methods": "Single methods similar to Vicuna experiments; no additional diverse reasoning styles introduced specifically for Mistral.",
            "reasoning_task_name": "Bamboogle (primary)",
            "reasoning_task_description": "Compositional dataset combining two sub-questions into a single complex question; used to test cross-backbone consistency.",
            "performance_by_method": "Paper reports consistent patterns across backbones (Table 5): distilled decomposers (SD-T/SD-R) yield consistent improvements on Bamboogle when paired with Mistral-7B versus using the vanilla backbone decomposer; exact per-cell numbers are reported in Table 5 of the paper.",
            "comparison_of_methods": "Distilled decomposers improve solver performance for both Vicuna-13B and Mistral-7B backbones; weaker solvers see larger relative gains.",
            "key_findings": "The advantage of distilling decomposition generalizes to different backbone families (Mistral-7B), indicating robustness of the decomposition-distillation strategy.",
            "counter_examples_or_negative_results": "No specific negative results unique to Mistral-7B reported; the same caveats about solver distillation (S_E) apply.",
            "uuid": "e3094.6",
            "source_info": {
                "paper_title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-ask: Asking follow-up questions to improve multi-step question answering (Press et al.)",
            "rating": 2,
            "sanitized_title": "selfask_asking_followup_questions_to_improve_multistep_question_answering_press_et_al"
        },
        {
            "paper_title": "Symbolic chain-of-thought distillation: Small models can also \"think\" step-by-step",
            "rating": 2,
            "sanitized_title": "symbolic_chainofthought_distillation_small_models_can_also_think_stepbystep"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.0194425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Divide-or-Conquer? Which Part Should You Distill Your LLM?
4 Oct 2024</p>
<p>Zhuofeng Wu zhuofeng@umich.edu 
University of Michigan
‡ Apple</p>
<p>He Bai ‡Aonan Zhang aonan_zhang@apple.com 
University of Michigan
‡ Apple</p>
<p>Jiatao Gu 
University of Michigan
‡ Apple</p>
<p>Vinod Vydiswaran vgvinodv@umich.edu 
University of Michigan
‡ Apple</p>
<p>Navdeep Jaitly njaitly@apple.com 
University of Michigan
‡ Apple</p>
<p>Yizhe Zhang yizhe_zhang@apple.com 
University of Michigan
‡ Apple</p>
<p>Divide-or-Conquer? Which Part Should You Distill Your LLM?
4 Oct 2024526D95D697A72DAD32B9C7B3B560EC37arXiv:2402.15000v2[cs.CL]
Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first.In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution.Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies.We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost.We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models.However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization.These results indicate that by using smaller, distilled problem decomposition models in combination with problem solving LLMs we can achieve reasoning with cost-efficient inference and local adaptation. 1</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023), demonstrate exceptional abilities in solving knowledge-intensive tasks like Open Domain QA (ODQA) (Zhu et al., 2021), math (Yue et al., 2023), science (Taylor et al., 2022) and autonomous agents (Yao et al., 2022;Significant Gravitas, 2023;Wang et al., 2024).However, the use of gigantic LLMs with hundreds of billions of parameters can be costly during inference, particularly when the reasoning chain generated is * Work done during internship at Apple 1 Accepted at Findings of the Association for Computational Linguistics: EMNLP 2024 lengthy.Additionally, due to the opaque nature of these black box LLMs, they offer limited adaption options.There is a need to use cheaper and more flexible models to leverage the power of these black box LLMs for local adaptation and cost-efficient inference.Distilling the large LLMs would seem like a reasonable strategy, but it often results in a significant drop in performance for downstream tasks (Chiang et al., 2023).</p>
<p>Previous studies (Weng, 2023;Wang et al., 2023a) have indicated that effectively addressing such tasks requires the model to proficiently perform two essential capabilities simultaneously: 1) planning and decomposition, which involves breaking down complex objectives into smaller, more manageable subgoals to facilitate efficient handling of intricate tasks; and 2) execution and solving, which involves memorizing vast amounts of knowledge from extensive web training data and effectively recalling this information when needed to execute the problem-solving process.The first capability, decomposition, typically requires the model to engage in self-reflection on the input query and generate a Chain-of-Thoughts (CoT)style reasoning chain (Wei et al., 2022) to tackle the problem.Usually, these two abilities are intertwined in a single monolithic model throughout the problem-solving process (Zhou et al., 2022).</p>
<p>In this paper, we first investigate whether it is possible to decouple the decomposition and solving capabilities, and how to distill these capabilities into smaller models for faster inference.We then test several hypotheses: 1) Is distilling decomposition easier than distilling solving?Decomposition primarily relies on semantic understanding and query parsing, while solving requires more domain expertise and knowledge.For example, decomposing the query "who is older, Messi or Ronaldo?" into "how old is Messi?", "how old is Ronaldo?", and "who is older?"only requires text comprehension, whereas solving the task necessi-Figure 1: Reasoning with a long thought chain using the black box LLM can be expensive and inflexible.We propose to dissect the decomposition and solving of the task, and distill only the decomposition capability to a less costly and more flexible student model, while still maintaining the original performance.</p>
<p>tates memorization, retrieval, and utilization of information.We speculate that compressing the less knowledge-intensive decomposition is easier.2) Is decomposition capability more generalizable than solving capability?We hypothesize that decomposition can sometimes be abstracted into symbolic principles, making it more universally applicable across tasks, datasets, and models.This enables tasks and models to share a common decomposition engine and benefit from each other's power, reducing the effort and costs involved in distilling a model for each individual task.</p>
<p>A natural question arises: is it possible to distill only the long reasoning chain, which accounts for most of the inference cost, but is relatively easier to distill?To this end, we propose and evaluate the distillation of only the decomposition capability from the LLM.We empirically verified our hypothesis using a teacher model, GPT-3.5-turbo, and two student models, Vicuna-13B (Chiang et al., 2023) and Mistral-7B (Jiang et al., 2023), on QA, mathematics, and compositional datasets (Dua et al., 2019;Cobbe et al., 2021;Press et al., 2022).</p>
<p>Our contributions include:</p>
<ol>
<li>
<p>We demonstrate that the decomposition capability is crucial for complex reasoning of LLMs.This capability can be dissected from the problem solving or task solving capability.</p>
</li>
<li>
<p>We show the feasibility and effectiveness of distilling only the query decomposition from the teacher model.The resulting distilled model maintains most of the performance while significantly reducing inference costs.</p>
</li>
</ol>
<p>In contrast, distilling the solving component of the LLM leads to a considerable decline in performance.</p>
<ol>
<li>
<p>We illustrate that the distilled query decomposition model exhibits good generalization across tasks, datasets, and models.However, the distilled solving ability does not generalize well.</p>
</li>
<li>
<p>We demonstrate that a static decomposition and solving framework provides more advantages compared to a dynamic approach in which planning and solving steps are interdependent.</p>
</li>
</ol>
<p>Decoupling Decomposition and Solving</p>
<p>As shown in Figure 1, a common approach to solving a reasoning task using an LLM involves directly generating a response to the instruction and question.This is referred to as the Single-Stage model.The conventional method for LLM, known as the Chain of Thought (CoT), instructs the model to "think step by step," allowing the model to take more computational steps for difficult tasks.</p>
<p>However, CoT-style reasoning has limitations as it often struggles to generalize to problems beyond the scope of the in-context examples.To address this drawback, the most notable work is the Least-to-Most approach (Zhou et al., 2022), where the model breaks down the original question into subquestions and answers them sequentially.These approaches have shown improved performance compared to CoT.</p>
<p>For QA tasks, typically, the next subquestion is less dependent on the answer to the previous subquestions.Conveniently, we propose a static strategy similar to HuggingGPT (Shen et al., 2023), where in the first Decomposition stage several decomposed subquestions are first generated to decompose the primary question.In the second Solving stage, these subquestions are then answered one by one to obtain the final answer.We refer to this line of models as the Two-Stage models.</p>
<p>Interactive vs static process Note that an interactive and dynamic process could be beneficial for certain reasoning tasks.In our experiments with math and QA datasets, the decomposition and solving stages are more independent, thus we did not observe gain by switching to an interactive process.Our primary focus lies in understanding the impact of distilling task decomposition and solving capabilities, rather than finding the optimal framework.Using a static approach would enable us to have a clearer separation of the decomposition and solving.The distilled decomposer can also potentially be integrated into more dynamic reasoning processes, enabling iterative solving and refinement based on intermediate outputs.We include a comparison in Section 6.1 to illustrate the benefits of adopting a static process.</p>
<p>Distill the Decomposition Capability</p>
<p>Generating decomposed questions can be computationally expensive when the reasoning chain is long while using a black box LLM.Moreover, it is challenging to optimize or customize the decomposition process as it is performed by the black box model.Our proposal aims to address these issues by utilizing a smaller trainable student model, as a drop-in replacement for the large black box LLM for decomposition.To achieve this, we distill the decomposition capability from the teacher LLM, referred to as T .</p>
<p>Generating Sub-questions from Teacher As shown in Figure 1, we begin by gathering demonstrations from T .Instead of requesting T to solve the problem, we ask it to break down a given question Q without providing the solution.Specifically, we provide T with an instruction for decomposition, denoted as I decomp , along with Q. T then generates a set of sub-questions {S i } i=1,2,3... .</p>
<p>Instruction for decomposition: I decomp</p>
<p>Your task is to break down a given complex question into the most relevant and helpful subquestions, ensuring that no more than three subquestions are formulated for each question.Both the context and the main question will be provided to you.If the question does not need breaking down to be answered, return "No decomposition"; otherwise, list the necessary subquestions.Only return subquestions that directly aid in answering the original question, avoiding any that could be harmful or unhelpful.</p>
<p>Question: Q Decomposer Distillation Given the subquestions {S i } generated from the teacher, we can finetune a student decomposer S by optimizing the cross-entropy loss for T (I decomp , Q) → {S i }.We denote the resulting student model as S D -T .</p>
<p>Subquestions Screening via Ground-truth Answer As an additional step, if the dataset comes with a corresponding ground-truth answer, denoted as A, we can optionally use this information to screen high-quality generated subquestions.To do this, we feed the same teacher model T with another instruction I ans that asks the model to solve the primary question Q by first solving the subquestions {S i }.We collect the generated answer T (I ans , P, {S i }, Q) → Â, where P represents the premise.I ans is provided as the following: Instruction for solving: I ans Solve a complex question by answering several related subquestions that would help me to answer it first.Answer the subquestions one by one and finally solve the original question.The final answer is supposed to attached in the end in the format of "The answer is: ".Now comes our primary question and its subquestions:
Premise: P Question: Q SubQuestion: {Si}
We assume that, statistically speaking, better {S i } will eventually lead to better resolving the tasks.Thus, we can optionally filter out training instances where Â ̸ = A.However, this will result in data loss.As this screening process is similar to the Rejection Sampling (Touvron et al., 2023), we denote the resulting model as S D -R.</p>
<p>In Section 5.2, we compare the performance of the distilled decomposer trained using the entire set of demonstrations S D -T against decomposer trained using a screened dataset S D -R.</p>
<p>Experiments</p>
<p>Datasets We assess the effectiveness of our pipeline on three distinct datasets.GSM8K (Cobbe et al., 2021) focuses on mathematical reasoning and is composed of 7.5K training instances alongside 1K test problems.DROP (Dua et al., 2019) caters to Question Answering, containing 77.4K training samples and a 9.5K validation set.Bamboogle (Press et al., 2022) is a manually crafted collection that integrates two questions into one complex question, consisting of 125 test samples.We use the GSM8K test set, the DROP development set, and the Bamboogle test set for evaluation.</p>
<p>Teacher/Student Models We use GPT-3.5-Turbo-0615model (Ouyang et al., 2022) as the teacher model throughout our experiments.After training we employ different levels of teacher models to ensure a comprehensive evaluation: two open sourced models ) and three black box models (textdavinci-003 (Brown et al., 2020), GPT-3.5-Turbo and GPT-4).</p>
<p>Student solver Models</p>
<p>To compare the performance of distilling decomposer with distilling solver, we conducted further training on several Vicuna models to mimic the behavior of the teacher as student solvers.Similar to the student decomposer, S E -T represents the model trained using the teacher's demonstrations of
T (I ans , {S i }, Q) → ({ Âs i }, Â)
, where { Âs i } represents the answers to the subquestions {S i } generated by T .</p>
<p>Furthermore, in scenarios where the oracle answer A is available, we fine-tuned the same vanilla Vicuna-13B model to obtain S E -A.This model was trained using (I ans , {S i }, Q) → ({ Âs i }, A), where the targets include answers to the subquestions {S i } from the T and the oracle answer A.</p>
<p>Training Details We use a batch size of 128, train for 3 epochs on DROP and train for 5 epochs on GSM8K and Bamboogle dataset (until convergence), and set the learning rate to 2 • 10 −5 for the distillation training.All the distillation fine-tuning can be finished in less than 12 hours on 8 × 80G A100 GPUs.</p>
<p>Inference Cost Estimation</p>
<p>We calculate the cost based on GPT-3.5-turbo-1106(175B -a vague estimation based on GPT-3), with a rate of $0.001 for 1000 input tokens and $0.002 for 1000 output tokens.OpenAI has made significant optimizations for inference time when serving GPT models.To ensure a fair comparison, we conservatively estimate the cost of the Vicuna-13B model by dividing the cost by the ratio of the model size.As a result, the cost for Vicuna-13B is approximately $7.42 * 10 −5 for 1000 input tokens and $1.48 * 10 −4 for 1000 output tokens.</p>
<p>Results</p>
<p>Decomposition is Essential for Reasoning</p>
<p>First, we explore the possibility of separating the Decomposition from Solving and assess the effectiveness of using an improved decomposition for complex reasoning tasks.</p>
<p>Previous studies (Press et al., 2022;Zhou et al., 2022) have demonstrated the utility of leveraging decomposed subquestions to enhance the questionanswering capabilities of black-box models.They adopt interactive planning strategies, where the generation of each subquestion is conditioned on the answer of the previous subquestions.</p>
<p>As discussed in Section 2, we instead use a static strategy by breaking down the reasoning process into two separate stages of Decomposition and Solving.Table 1 (Single-stage GPT/Vicuna vs Twostage GPT/Vicuna), shows that in general such a static strategy leads to performance gains over a Single-stage approach.This aligns with previous findings (Zhou et al., 2022).We demonstrate in Table 1 (Two-stage models) that replacing a stronger decomposer (GPT) with a weaker decomposer (Vicuna) mostly results in a noticeable decrease in performance, with an exception of using Vicuna as solver on GSM8K.We hypothesize that presumably the Vicuna solver is too erroneous to harness the improvement from the decomposition.We observe that the decrease is more significant when the solver is more powerful.This suggests that in order to achieve optimal performance, a stronger decomposer is essential.</p>
<p>Is Distilling Decomposition Easier than</p>
<p>Distilling Solving?</p>
<p>Next, we investigate distilling knowledge from T to S when the ground truth answer A is not available.This is the most common use case as ground truth annotations are typically expensive and rare.</p>
<p>The results are shown in Table 1 (w/o oracle answer A).It can be seen that swapping in S D -T for the decomposer is at least comparable to the performance using T .Moreover, the S D -T exhibits a noticeable improvement compared to using Vicuna as the decomposer.However, swapping in a student solver model S E -T significantly harms the performance.We also evaluated a single-stage student model distilled from single-stage GPT.The result, omitted, was even worse than the model where GPT was the decomposer and S E -T was the solver.In terms of inference cost, our S D -T approach results in significantly lower cost for the decomposition compared to using the teacher GPT model.The cost of the solver remains unchanged.We compare some decompositions from T , from Vicuna and from S D -T on the evaluation set in Table 2.It can be observed that the distilled S D -T model, which is obtained by using in-domain demonstration from T , exhibits a high degree of similarity to the teacher demonstration in the generated subquestions on the unseen test set.In contrast, the original Vicuna model often generates unhelpful questions that have the potential to distract the solver (Table 2, Vicuna-13B).</p>
<p>One might naturally wonder: If a smaller student model can quickly imitate the decomposition abilities of the teacher model, why is it challenging to acquire this skill directly through student model's initial pretraining?Our hypothesis is that the decomposition ability of a stronger teacher model is easy to distill but difficult to acquire.This skill is likely based on the thorough digestion and internalization of vast amounts of data during the intensive pretraining of the larger models.However, as it is more logical and abstract rather than being knowledge-intensive, a few demonstrations may already provide ample guidance to the student.To draw an imperfect analogy, finding a physics theorem from massive observations is much more challenging than learning the theorem in the class.</p>
<p>With available oracle answers Sometimes, we have access to the oracle answers A, which can be used to further enhance the model's performance on specific domains through local adaptation and additional finetuning.As a result, the performance on these target domain can be beyond the performance of the black-box teacher model.We explore the options to enhance the models via distillation or target domain finetuning.</p>
<p>In these scenarios, we can possibly use A to screen the training instance for distill the decom- Vicuna-13B</p>
<ol>
<li>What is the definition of a ""bridge"" for the purpose of determining the longest one in the world?X 2. What is the current longest bridge in the world and when was it opened?GPT-3.5 1.What is the name of the longest bridge in the world? 2. When was the longest bridge in the world opened?</li>
</ol>
<p>SD-T In-Domain</p>
<ol>
<li>What is the name of the bridge that holds the record for being the longest in the world? 2. What is the date on which this record-breaking bridge was officially opened to traffic?Table 2: Examples for decomposed subquestions from each method on GSM8K, DROP, and Bamboogle.S D -T (GSM) and S D -T (DROP) denote Vicuna student models that distilled from T 's demonstration on GSM8K and DROP datasets, respectively.X indicates not helpful subquestions.poser, similar to Rejection Sampling.The resulting student model S D -R achieved higher performance than using S D -T , as shown in Table 1 (w/ oracle answer A).Notably, on the DROP dataset, S D -R outperforms the teacher model in F1 score.</li>
</ol>
<p>We also finetune another Vicuna model for the solver using the ground-truth answers, referred to as S E -A.Our main findings remain consistent to the scenario where no oracle answers are available.Distilling the decomposer still yields better performance comparing with finetuning the solver.We omitted the single-stage Vicuna model finetuned using A, which yielded worse results than GPT(decomposer) + S E -A(solver).</p>
<p>Failure modes for S E models According to our observations, we hypothesize that there are two pri-mary failure modes of the S E -T and S E -A models.</p>
<p>First, answering either subquestions or primary questions would require extensive world knowledge, which can be difficult to compress into a student model that is hundreds of times smaller, using only a few demonstrations.In other words, a strong solving capability is knowledge-intensive.On the other hand, decomposition capability might be more compressible as it is typically more abstract, has lower information density, and is more universal than solving capability.</p>
<p>Second, since we used the teacher's answers to the subquestions { Âs i } as part of the target, the S E models could get confused and generate the final answers to one of the subquestions {S i }, rather than the primary question Q. (Examples are pro-vided in Appendix C.)</p>
<p>Based on above findings, we excluded the { Âs i } in the target when training the S E models.Specifically, we train the models to directly generate the answer by skipping answering subquestions, S E (I ′ ans , {S i }, Q) → Â or A. The resulting models are denoted as S E -T (direct) and S E -A(direct) in Table 8.We found that { Âs i } from the target yields improved results over the DROP dataset, but leads to a decrease in performance over the GSM8K dataset.Overall, the decrease observed in GSM8K is more prominent than the gain seen in the DROP dataset.Therefore, we still use the S E models with the { Âs i } in the target.We provide additional analysis, I ′ ans , and show the comparison results in Appendix A. in Table 3. Surprisingly, the distilled decomposer S D -R demonstrates good generalization and versatility to the other domain, as evidenced by only a slight decrease in performance compared to using the teacher GPT model as the decomposer.In contrast, when substituting the solver with S E -A, which is fine-tuned on the original domain, the generalization to the other domain is poor regardless of the decomposer used.Some examples of crossdomain subquestion decomposition are shown in Table 2.The results on the scenario with no oracle answer are consistent with Table 3.</p>
<p>Generalization to other solvers Next, we examine whether the distilled decomposer is compatible and universally suitable for different solvers.The results can be seen in Table 4.The performance of S D -R is comparable to that of the teacher decomposer (GPT), and it shows overall improvements over a weaker decomposer (Vicuna) when connected to different solvers.We found that weaker solvers receive more performance gain compared to strong solvers, through upgrading to a distilled decomposer.We hypothesize that the reason lies in the fact that the weaker solver may be incapable of fully utilizing the benefits of the decomposition.</p>
<p>Generalization to other backbones To verify whether our observations on the Vicuna-13B model hold for other backbone models, we conducted similar experiments on the Bamboogle dataset using the Mistral-7B model (Table 5).The results exhibited a consistent pattern across different backbones, suggesting the robustness of our conclusions.</p>
<p>Ablations</p>
<p>We provide an extensive evaluation of various instructions, and an exploration into the influence of the number of demonstrations in Appendix B.</p>
<p>Decomposition: Static or Dynamic?</p>
<p>Prior research, including self-ask (Press et al., 2023) and CoQ (Zhu et al., 2023), employed dynamic decomposition pipelines where each reasoning step depends on previous steps.In contrast, our approach adopts a static method that separates planning from question-solving.This section presents a direct comparison of both pipelines.</p>
<p>Dynamic Pipeline Our pipeline is compared to the self-ask dynamic decomposition method.</p>
<p>Self-ask appends "Are follow-up questions needed here?" to the original query, determining if subquestions are needed to assist in answering.When needed, the model generates and independently answers relevant subquestions.This process iterates until the model deems no additional subquestions are needed to answer the original query.</p>
<p>Setup GPT-3.5-turbo is used for both question decomposition and answer generation in both the dynamic and static model.We compare the two approaches on Bamboogle and GSM8K datasets.Results and Analysis From Table 6, we observe that: 1) Dynamic planning performs poorly on both datasets because dynamic processes require precise reasoning models.Errors in intermediate steps can influence subsequent steps and affect the final outcomes.</p>
<p>2) The cost of the dynamic pipeline is markedly higher than that of the static pipeline (×3.96 and ×2.32 for the number of input tokens on Bamboogle and GSM8K, respectively), highlighting the substantial inference costs associated with dynamic processes.We acknowledge the potential of dynamic decomposition and its advantages in specific application scenarios.However, our experiments on evaluated tasks show that static decomposition offers certain advantages.Nevertheless, our goal is to analyze whether decomposition or solver capability is easier and more generalizable to distill, rather than seeking the optimal strategy.</p>
<p>Impact of Learning Decomposition</p>
<p>Fine-tuning a model to enhance its decomposition performance might affect its problem-solving abilities.To evaluate this effect, we assess the performance of a fine-tuned decomposer against an untrained model in a question-answering context.In the initial question decomposing phase, both models tackle the same set of decomposed questions.Then, during the answering phase, one model leverages an untrained Vicuna model, while the other employs S D -R, which was initially trained for the decomposition task.We adhered to the evaluation settings in Table 1.From Table 7, it is evident that the trained specialized model performs significantly worse than the untuned model on other tasks (-22.36 on GSM8K, -18.76 on DROP, -15.2 on Bamboogle).This indicates that, without specific adjustments or designs, focusing solely on learning decomposition can negatively impact problem-solving abilities in reasoning tasks.These findings underscore our core design principle: separating reasoning into distinct decomposition and solving stages is advantageous.In our pipeline, decomposition process is isolated from the problem-solving stage, ensuring that solver's reasoning capability remains intact.
Decomposer Solver GSM8K DROP Bamb SD-R Vicuna-13B28</p>
<p>Related Work</p>
<p>LLM Distillation Tremendous progress (Jiao et al., 2020;Sun et al., 2019;Li et al., 2021) has been made in terms of compressing large-scale pretrained language models such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019).For generative models, compression is predominantly achieved by minimizing the K-L divergence between teacher and student distributions (Sanh et al., 2019;Gu et al., 2023a).A pivotal assumption underlying these methods is the full accessibility of the teacher model's components.However, most powerful LLMs are black boxes, revealing only limited outputs.Given these constraints, several methodologies have emerged that train directly on data generated by teacher models (Chiang et al., 2023;Taori et al., 2023).We follow a similar distil-lation strategy but focus on the decomposition capability distillation.More recently, researchers have explored aligning small models with large models by leveraging the reward signal from teachers or human preference feedback (Kwon et al., 2023;Gu et al., 2023b;Tunstall et al., 2023), using methods such as DPO (Rafailov et al., 2024) or PPO (Schulman et al., 2017).While some approaches, such as Symbolic CoT (Li et al., 2023) and SCOTT (Wang et al., 2023b), have focused on distilling reasoning (CoT) capabilities into one compact model, which diverges from our main focus.</p>
<p>LLM Reasoning Chain LLMs can benefit from explicit reasoning chains, as demonstrated by recent studies (Wei et al., 2022;Zheng et al., 2023).The Chain of Thought (CoT) (Wei et al., 2022) technique has become standard for enhancing model performance on complex tasks.Tree of Thoughts (Yao et al., 2023) decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure.The LLM+P approach (Liu et al., 2023) incorporates an external classical planner for long-horizon planning and translates the plan back into natural language.Theoretical work (Feng et al., 2023) has analyzed why CoT works by using circuit complexity theory.It shows that without CoT, the model size would need to be prohibitively large to achieve the same performance through direct reasoning.</p>
<p>However, CoT-style reasoning is limited by the fact that it often generalizes poorly to problems beyond the scope of the provided in-context examples (Zhou et al., 2022).To address this, some studies have asked LLMs to decompose complex questions into subquestions following the Least-to-Most prompt (Zhou et al., 2022).Others have used the self-ask method to elicit follow-up questions that aid in addressing the original inquiry (Press et al., 2022).Our work contributes to this line of research by extending the horizon to cost-efficient inference and generalization across tasks.</p>
<p>Question Decompostion Datasets and Approaches A widely recognized dataset for question decomposition in the literature is QDMR (Wolfson et al., 2020).It comprises an ordered list of sub-questions essential for addressing a primary question.Several previous works have been training question decomposers on the QDMR dataset (Guo et al., 2022;Zhu et al., 2023).In contrast, some research does not rely on QDMR but employs their uniquely labeled data.For instance, Min et al. (2019) recast question decomposition as a span prediction problem and trained their model on a set of 400 labeled questions.Recognizing the challenges associated with obtaining reliable decomposition data, Perez et al. (2020) introduced an unsupervised decomposition approach, capitalizing on the similarity between the primary question and 10M potential sub-questions mined for decomposition purposes.Our approach differs from the aforementioned methodologies because we extract the decomposition power solely from the teacher model, without relying on annotated subquestions.</p>
<p>Complement LLMs with Small models There have been studies that have emphasized the potential of smaller, task-specific models to complement the predictions of LLM.Xu et al. (2023) explored a framework in which candidates produced by these task-specific models are fed to an LM, with a primary focus on classification tasks.Welleck et al. (2022) train a smaller model to iteratively improve sequences generated by LMs.Vernikos et al. (2023) have demonstrated that collecting multiple erroneous outputs from LMs and using a small corrector model to unify the generation can significantly reduce errors.Our work can also be seen as developing a smaller decomposer model to activate the best performance of a large-scale LM.</p>
<p>Conclusion</p>
<p>Our investigation provides a fine-grained examination of the LLM's capability on reasoning tasks, by disentangling the decomposition and solving aspects.Although both capacities are vital for reasoning, we demonstrate that decomposition is less dependent on specific knowledge and thus easier to distill compared to distilling solving capabilities, regardless of the availability of ground truth labels.Additionally, the distilled decomposer shows strong generalization abilities across different tasks, datasets and executor/solvers.For future work, it would be interesting to train universal decomposer models using data from various tasks, and explore the use of reinforcement learning to further enhance the decomposer, leveraging the signal from the solver outcome.Another possible direction for future work is to assess the effectiveness of our method in other long-horizon planning tasks, including LLM-powered agent, tool use, and multiturn decision making.</p>
<p>Limitations</p>
<p>Our work is built upon several assumptions.First, we assume that the teacher model is capable of breaking down queries effectively.Second, we assume that the student model has the capacity to learn the distilled planning from the teacher model.Lastly, we assume that the tasks involved in our work require long horizon planning capability.If any of these assumptions do not hold true, it would impact the effectiveness of our proposed method.</p>
<p>It is important to note that we have only assessed the effectiveness of our model in the context of math and QA aspects.In order to fully complete our work, it would be necessary to evaluate our model on a broader range of planning tasks.This would include benchmarks related to tool use, LLM agents, and multiturn scenarios.Such evaluations would help verify the versatility and applicability of our proposed method.We hypothesize that for tasks involving mathematical reasoning, the answers typically necessitate some form of computation, making a step-by-step solution essential.Without this, setting a numerical value as the fine-tuning target almost invariably results in failure.Conversely, DROP, being a reading comprehension dataset, derives a significant portion of its answers directly from the provided text.In such scenarios, including answers to subquestions poses a risk of disrupting the answer distributions.</p>
<p>The instruction for solving, denoted as I ′ ans , remain identical to those specified in I ans .The only difference comes from the fine-tuning target.Prior research has demonstrated that incorporating demonstrations within prompts can significantly enhance the ability of Large Language Models to adhere to given instructions.Our findings in Table 9 further substantiate this, revealing that including a single-shot demonstration notably improves the quality of decomposed questions.This enhancement has been consistently observed across a variety of decomposers.</p>
<p>B Ablation Study over</p>
<p>Instruction for decomposition: I decomp Your task is to break down a given complex question into the most relevant and helpful subquestions, ensuring that no more than three subquestions are formulated for each question.Both the context and the main question will be provided to you.If the question does not need breaking down to be answered, return "No decomposition"; otherwise, list the necessary subquestions.Only return subquestions that directly aid in answering the original question, avoiding any that could be harmful or unhelpful.Question: Q</p>
<p>We have conducted an ablation study focusing on the instructions used for question decomposition.Our goal is for the resulting subquestions to act as useful cues for the executor, all the while ensuring they do not introduce unnecessary information.Central to our design rationale is determining the optimal number of subquestions the decomposer should produce.More specifically, we analyzed outcomes where no restrictions were applied (removing the highlighted part in I decomp ) and compared these against scenarios with varying maximum numbers of subquestions allowed.The results of these investigations are detailed in Table 10.Our findings succinctly reveal that a cap of "no more than three subquestions" yields the most effective results.</p>
<p>C Examples Where Solver Models Become Confounded by Subquestions</p>
<p>As illustrated in Figure 2, up to the second subquestion, the solver model accurately responds that "The robe requires 2 bolts of blue fiber" and "it would need 1 bolt of white fiber."Nevertheless, the introduction of the third subquestion, closely resembling the second, leads to confusion.Conse- quently, the model deviates from its initial accuracy, culminating in an incorrect answer following this subquestion.</p>
<p>D Extended Related Work</p>
<p>Planning and Task Decomposition of LLMpowered Agent Recent advances in LLMpowered systems have made it possible to create an end-to-end pipeline, opening up new possibilities for developing autonomous agents that can complete complex tasks using enhanced planning and memory capabilities.Promising works, such as ReAct (Yao et al., 2022), HuggingGPT (Shen et al., 2023), AutoGPT (Significant Gravitas, 2023), LangChain (Langchain-AI, 2023), GPT-Engineer (Anton Osika, 2023), BabyAGI (Nakajima, 2023) and CoMM (Chen et al., 2024), have demonstrated significant potential in this field.These agents rely on the LLM to decompose larger tasks into more manageable components.Among them, some approaches (e.g., HuggingGPT) use a static planning strategy by first generating the complete plan via LLM and subsequently tackling each subtask.</p>
<p>Other approaches (e.g., AutoGPT) adopt a dynamic and interactive planning strategy, where the generation of each action is conditioned on the outcome of the previous planning steps.</p>
<p>Figure 2 :
2
Figure 2: Solver models get lost sometimes.</p>
<p>Table 1 :
1
Comparison results on GSM8K, DROP, and Bamboogle datasets.Performance on GSM8K is assessed via the exact match score (EM), DROP is evaluated using the F1 score, Bamboogle (Bamb) is evaluated using accuracy.The inference expense is estimated by total sample cost.X/X indicates decomposition/solving cost.
DecomposerSolverPerformance↑Inference Expense ↓ModelModelGSM8K DROP Bamb GSM8K($) DROP($) Bamb($)Single-stage--GPT Vicuna-13B20.32 9.4046.51 26.6849.6 33.6-/0.01 -/0.03-/0.05 -/0.03-/7e-3 -/2e-3GPTGPT65.1355.7354.40.13/0.630.73/0.96 2e-3/9e-3Two-stageVicuna-13B GPTGPT Vicuna-13B62.93 28.1347.13 21.2948.8 32.80.02/0.67 0.13/0.070.07/0.96 1e-3/0.02 0.73/0.08 2e-3/3e-3Vicuna-13BVicuna-13B28.5120.9029.60.02/0.080.07/0.08 1e-3/5e-3w/o oracle answerSD-T GPTGPT SE-T67.02 48.9855.19 13.3752.0 31.20.01/0.62 0.13/0.090.06/0.96 1e-3/0.03 0.73/0.06 2e-3/2e-3w/ oracle answerSD-R GPTGPT SE-A67.78 51.5557.97 20.3452.0 40.80.01/0.60 0.13/0.090.06/1.11 8e-5/7e-3 0.73/0.04 2e-3/2e-3</p>
<p>What were the scores for each team during the game?X 3. Which team had the lead at the end of the game?X GPT-3.5 1.How many field goals did the Raiders kick in the first half?2. How many field goals did the Texans kick in the first half?3. What is the sum of the field goals kicked by both teams in the first half?SD-T In-Domain 1.How many field goals did the Raiders kick in the first half?2. How many field goals did the Texans kick in the first half?SD-T (GSM) Cross-Domain 1.How many field goals did the Raiders kick in the first half?2. How many field goals did the Texans kick in the first half?How many sentences does Mark edit for Publisher A in a week? 2. How many sentences does Mark edit for Publisher B in a week? 3. What is the rate per sentence paid by Publisher B? How much does Publisher A pay Mark per sentence?2: How much does Publisher B pay Mark per sentence?3: How many sentences does Mark edit in a week?
Dataset: DROPModelsDecomposed Sub-questions1. Which teams played against each other? X 2. Dataset: GSM8K Premise P : The Raiders stayed at home for a Week 16 duel with the Houston Texans. ... The Texans tried to rally in the fourth quarter as Brown nailed a 40-yard field goal, yet the Raiders' defense would shut down any possible attempt. Question Q: How many field goals did both teams kick in the first half? Vicuna-13B Models Decomposed Sub-questions1. What is the rate per sentence that Publisher B pays Mark? XPremise P : Mark is a copy-editor. He edits an equal number of sentences each week for two different publishers, who each pay him a different rate per sentence. Publisher B pays Mark twice what Publisher A pays. Mark edits aVicuna-13B GPT-3.52. What is the total amount Publisher A pays Mark for editing 1000 sentences? 3. What is the total amount Publisher B pays Mark for editing 1000 sentences? 1. How many sentences does Mark edit each week for Publisher A? 2. How many sentences does Mark edit each week for Publisher B? 3. How much does Mark make per sentence from Publisher B?total number of 1000 sentences each week, and Publisher A pays him 5 cents per sentence. Question Q: How much does Mark make in a week, in cents? 1: Dataset: Bamboogle SD-T In-Domain 1. SD-T (DROP) Cross-Domain Models Decomposed Sub-questionsQuestion Q: When was thelongest bridge in the worldopened?</p>
<p>Table 3 :
3
Distilled student decomposers demonstrate strong generalization over out-domain datasets.
BackboneVicuna-13B Mistral-7B-/Backbone33.640.0Backbone/GPT48.856.8GPT/Backbone32.840.0Backbone/Backbone29.638.4SD-T /GPT52.055.2GPT/SE-T31.240.8SD-R/GPT52.060.0GPT/SE-A40.847.2Table 5: Comparison results of Vicuna-13B and Mistral-7B on Bamboogle dataset. X/X denotes the Decom-poser/Solver model, where "Backbone" refers to thevanilla untuned backbone model.Decomposer GPTSD-RGPT-SolverGPTGPTSE-ASE-ATrained onEvaluation on DROPGSM8K55.7351.057.9817.22Trained onEvaluation on GSM8KDROP65.1363.1511.303.41DecomposorSolverGSM8K DROPVicuna-13B28.033.78GPT-3.5-TurboGPT-3.5-Turbo66.059.38GPT-490.577.60Vicuna-13B29.526.56Vicuna-13BGPT-3.5-Turbo57.047.31GPT-488.579.40Vicuna-13B31.533.38SD-RGPT-3.5-Turbo66.561.94GPT-491.581.02Table 4: Distilled student decomposers demonstrateconsistent improvements over different solvers. Weakersolvers receive more gain.5.3 Is Distilling Decomposition MoreGeneralizable than Distilling Solving?Generalization to other domains We then in-vestigate whether the distilled decomposer, whichis trained on a specific domain dataset, can be ap-plied to out-of-domain datasets with distinct ob-jectives. To test this, we perform a cross-domainevaluation on DROP and GSM8K, which requiredifferent expertise from the solver. The results,when the oracle answer is available, are presented</p>
<p>Table 6 :
6
Comparison between static planning and dynamic planning on Bamboogle and GSM8K datasets.</p>
<p>Table 7 :
7
Impact of learning question decomposition on model's solving capability.
.3526.6332.8</p>
<p>Table 8 :
8
Excluding answers to subquestions { Âs i } from the target yields improved results over the DROP dataset, but leads to a decrease in performance over the GSM8K dataset.</p>
<p>Table 9 :
9
Impact of including demonstration in decomposition instruction, examined on a subset of GSM8K dataset.</p>
<p>Table 10 :
10
Effect of limiting the maximum number of subquestions in decomposition instructions on a subset of the DROP dataset.
InstructionEMF1no restriction45.69 56.63no more than four46.40 57.19no more than three 50.00 59.88no more than two46.89 58.47
AcknowledgementsWe thank Wang Zhu, Lingxiao Zhao, Simone Conia, Pratyush Maini, Zhenhao Zhang, Jiazhao Li, Fei Sun, members of the University of Michigan's NLP4Health research group, and all anonymous reviewers for helpful discussion and valuable feedback.
Language models are few-shot learners. Anton Osika, ; Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, GPT Engineer. 2023. 202033</p>
<p>CoMM: Collaborative multi-agent, multi-reasoningpath prompting for complex problem solving. Pei Chen, Shuai Zhang, Boran Han, 10.18653/v1/2024.findings-naacl.112Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, Mexico2024Association for Computational Linguistics</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, Liwei Wang, arXiv:2305.15408Towards revealing the mystery behind chain of thought: a theoretical perspective. 2023arXiv preprint</p>
<p>Knowledge distillation of large language models. Yuxian Gu, Li Dong, Furu Wei, Minlie Huang, arXiv:2306.085432023aarXiv preprint</p>
<p>Minillm: Knowledge distillation of large language models. Yuxian Gu, Li Dong, Furu Wei, Minlie Huang, The Twelfth International Conference on Learning Representations. 2023b</p>
<p>Complex reading comprehension through question decomposition. Xiao-Yu Guo, Yuan-Fang Li, Gholamreza Haffari, Proceedings of the The 20th Annual Workshop of the Australasian Language Technology Association. the The 20th Annual Workshop of the Australasian Language Technology AssociationAdelaide2022Australia. Australasian Language Technology Association</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>TinyBERT: Distilling BERT for natural language understanding. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, 10.18653/v1/2020.findings-emnlp.372Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, arXiv:2303.00001Reward design with language models. 2023arXiv preprint</p>
<p>Langchain Github Repository. GitHub repository. Langchain-Ai , 2023</p>
<p>Dynamic knowledge distillation for pre-trained language models. Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun, 10.18653/v1/2021.emnlp-main.31Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Symbolic chain-of-thought distillation: Small models can also "think" step-by-step. Liunian Harold, Li , Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin Choi, 10.18653/v1/2023.acl-long.150Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Multi-hop reading comprehension through question decomposition and rescoring. Sewon Min, Victor Zhong, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.18653/v1/P19-1613Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Yohei Nakajima, ArXiv, abs/2303.08774GitHub repository. OpenAI. 2023. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Unsupervised question decomposition for question answering. Ethan Perez, Patrick Lewis, Wen-Tau Yih, Kyunghyun Cho, Douwe Kiela, 10.18653/v1/2020.emnlp-main.713Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, Mike Lewis, 10.18653/v1/2023.findings-emnlp.378arXiv:2210.03350Findings of the Association for Computational Linguistics: EMNLP 2023. Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike LewisAssociation for Computational Linguistics. Ofir Press2023. 2022arXiv preprintMeasuring and narrowing the compositionality gap in language models</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.01108Distilbert, a distilled version of bert: Smaller, faster, cheaper and lighter. 2019arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Auto-gpt: An Autonomous GPT-4 Experiment. Significant Gravitas, 2023GitHub repository</p>
<p>Patient knowledge distillation for BERT model compression. Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, 10.18653/v1/D19-1441Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Zephyr: Direct distillation of lm alignment. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, arXiv:2310.169442023arXiv preprint</p>
<p>Giorgos Vernikos, Arthur Bražinskas, Jakub Adamek, Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, arXiv:2305.13514Small language models improve giants by rewriting their outputs. 2023arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023aarXiv preprint</p>
<p>SCOTT: Selfconsistent chain-of-thought distillation. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren, 10.18653/v1/2023.acl-long.304Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b1</p>
<p>Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, arXiv:2402.01030Executable code actions elicit better llm agents. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, arXiv:2211.000532022arXiv preprint</p>
<p>Llm powered autonomous agents. Lilian Weng, 2023</p>
<p>Break it down: A question understanding benchmark. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, Jonathan Berant, 10.1162/tacl_a_00309Transactions of the Association for Computational Linguistics. 82020</p>
<p>Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, Julian Mcauley, arXiv:2305.08848Small models are valuable plug-ins for large language models. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, arXiv:2304.09797Progressive-hint prompting improves reasoning in large language models. 2023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>
<p>Retrieving and reading: A comprehensive survey on open-domain question answering. Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, Tat-Seng Chua, arXiv:2101.007742021arXiv preprint</p>
<p>Chain-of-questions training with latent answers for robust multistep question answering. Wang Zhu, Jesse Thomason, Robin Jia, 10.18653/v1/2023.emnlp-main.547Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingapore2023Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>